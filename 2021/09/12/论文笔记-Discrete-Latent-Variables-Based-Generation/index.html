<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>论文笔记-Discrete Latent Variables Based Generation - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="VQ-VAE: Neural Discrete Representation Learning (NIPS2017) VQ-VAE2: Generating Diverse High-Resolution Images with VQ-VAE-2 DALL-E: Zero-Shot Text-to-Image Generation VideoGPT: Video Generation using"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="VQ-VAE: Neural Discrete Representation Learning (NIPS2017) VQ-VAE2: Generating Diverse High-Resolution Images with VQ-VAE-2 DALL-E: Zero-Shot Text-to-Image Generation VideoGPT: Video Generation using"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2021-09-12T10:38:37.000Z"><meta property="article:modified_time" content="2021-12-08T06:30:12.302Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="generation"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/"},"headline":"论文笔记-Discrete Latent Variables Based Generation","image":["http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913162102233.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913164021096.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913172642920.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913173322976.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913195106457.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913201430732.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914102405202.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914092448703.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914092720814.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914093308467.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914095124312.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914095645194.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914111945553.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914112444311.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914153647534.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914154458963.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914164923274.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914164945741.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914165402615.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914165342645.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914173838681.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914173852849.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914173951653.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914174005480.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914180014657.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914180200167.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914210857358.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914214726470.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916151621009.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916151649926.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916162859221.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916162937840.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916163744821.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916195155055.png","http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916195414002.png"],"datePublished":"2021-09-12T10:38:37.000Z","dateModified":"2021-12-08T06:30:12.302Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"VQ-VAE: Neural Discrete Representation Learning (NIPS2017) VQ-VAE2: Generating Diverse High-Resolution Images with VQ-VAE-2 DALL-E: Zero-Shot Text-to-Image Generation VideoGPT: Video Generation using"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-12T10:38:37.000Z" title="2021/9/12 下午6:38:37">2021-09-12</time>发表</span><span class="level-item"><time dateTime="2021-12-08T06:30:12.302Z" title="2021/12/8 下午2:30:12">2021-12-08</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/generation/">generation</a></span><span class="level-item">37 分钟读完 (大约5593个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">论文笔记-Discrete Latent Variables Based Generation</h1><div class="content"><ul>
<li><a href="#vq-vae">VQ-VAE</a>: Neural Discrete Representation Learning (NIPS2017)</li>
<li>VQ-VAE2: Generating Diverse High-Resolution Images with VQ-VAE-2</li>
<li>DALL-E: Zero-Shot Text-to-Image Generation</li>
<li><a href="#videogpt">VideoGPT</a>: Video Generation using VQ-VAE and Transformers</li>
<li><a href="#lvt">LVT</a>: Latent Video Transformer</li>
<li>Feature Quantization Improves GAN Training (ICML2020)</li>
<li><a href="#DVT-NAT">DVT-NAT</a>: Fast Decoding in Sequence Models Using Discrete Latent Variables (ICML2018)</li>
<li>NWT: Towards natural audio-to-video generation with representation learning</li>
<li>NUWA: Visual Synthesis Pre-training for Neural visUal World creAtion</li>
</ul>
<span id="more"></span>

<h2 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE<a name="vq-vae"></a></h2><p>在认识VQ-VAE之前，先回顾一下AE、VAE。很早以前看李宏毅老师的视频学过一次，这里就直接使用之前整理的PPT笔记了. 以及对应的代码可以看这里 (<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/OlaWod/my-machine-learning/blob/master/VAE/vae.ipynb#scrollTo=nbSny5c5Bjzf">vae.ipynb - Colaboratory (google.com)</a>)</p>
<h3 id="Auto-Encoder"><a href="#Auto-Encoder" class="headerlink" title="Auto-Encoder"></a>Auto-Encoder</h3><p>我们在重构一个图像时，通常输入和输出都是图像本身。为了保证神经网络不是直接的copy，很自然会想到这种降维再升维的方式。这里的latent vector/codings 就是我们希望学到的一个原图像压缩后的低维特征表示。</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913162102233.png" style="zoom:60%;">

<p>模型代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">512</span>, latent_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        z = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        z = F.relu(self.linear1(z))</span><br><span class="line">        <span class="keyword">return</span> self.linear2(z)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(latent_dims, <span class="number">512</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">512</span>, <span class="number">784</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, z</span>):</span></span><br><span class="line">        x_hat = F.relu(self.linear1(z))</span><br><span class="line">        x_hat = torch.sigmoid(self.linear2(x_hat))</span><br><span class="line">        <span class="keyword">return</span> x_hat.reshape((-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Autoencoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Autoencoder, self).__init__()</span><br><span class="line">        self.encoder = Encoder(latent_dims)</span><br><span class="line">        self.decoder = Decoder(latent_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        z = self.encoder(x)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(z)</span><br></pre></td></tr></table></figure>

<p>看源代码可以直观的发现，对于每一个样本，我们学到了对应的压缩后的特征表示的维度是 latent_dims=2.（潜空间的维度当然也可以更大，这里为了可视化更方便，我们设置为2）。 </p>
<p>可视化代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_latent</span>(<span class="params">autoencoder, data, num_batches=<span class="number">100</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> i, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        z = autoencoder.encoder(x.to(device))</span><br><span class="line">        z = z.to(<span class="string">&#x27;cpu&#x27;</span>).detach().numpy()</span><br><span class="line">        plt.scatter(z[:, <span class="number">0</span>], z[:, <span class="number">1</span>], c=y, cmap=<span class="string">&#x27;tab10&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> i &gt; num_batches:</span><br><span class="line">            plt.colorbar()</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>将得到的这些2 维的 latent vectors可视化如下图：</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913164021096.png" style="zoom:70%;">

<p>通过上图我们可以发现，我们训练的每一个样本对应着潜空间中的一个点，这些点是离散的。这意味着，我们并不能生成新的图像，而只能“复制”原有的图像（也可以说，如果在潜空间采样得到的点在这些离散的点之间/外时，生成的样本会比较模糊）。但也有有趣的发现，降维确实是起到了一个聚类的效果，只是效果一般吧，类与类之间并没有完全的分开。这个应该是可以人为控制的。</p>
<p>我们可以试着去生成一些潜空间的vector，然后在此基础上去重构，看看会发现什么？</p>
<p>重构代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_reconstructed</span>(<span class="params">autoencoder, r0=(<span class="params">-<span class="number">10</span>, <span class="number">5</span></span>), r1=(<span class="params">-<span class="number">5</span>, <span class="number">10</span></span>), n=<span class="number">12</span></span>):</span></span><br><span class="line">    w = <span class="number">28</span></span><br><span class="line">    img = np.zeros((n*w, n*w))</span><br><span class="line">    <span class="keyword">for</span> i, y <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.linspace(*r1, n)):</span><br><span class="line">        <span class="keyword">for</span> j, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.linspace(*r0, n)):</span><br><span class="line">            z = torch.Tensor([[x, y]]).to(device)</span><br><span class="line">            x_hat = autoencoder.decoder(z)</span><br><span class="line">            x_hat = x_hat.reshape(<span class="number">28</span>, <span class="number">28</span>).to(<span class="string">&#x27;cpu&#x27;</span>).detach().numpy()</span><br><span class="line">            img[(n-<span class="number">1</span>-i)*w:(n-<span class="number">1</span>-i+<span class="number">1</span>)*w, j*w:(j+<span class="number">1</span>)*w] = x_hat</span><br><span class="line">    plt.imshow(img, extent=[*r0, *r1])</span><br></pre></td></tr></table></figure>

<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913172642920.png" style="zoom:70%;">

<p>代码中我们选取的潜空间的范围是 $x\in [-5, 10]$ , $y\in [10, 5]$ ,在结合可视化的图，可以看到主要是集中在数字 “0” 的区域。右上角是 (10, 5) 可以从可视化的图中也能看到，有部分接近“1”的区域，但生成的样本很模糊。</p>
<p>自编码除了潜空间是离散的，还有个缺点，就是当神经网络太强时，会overfitting。去噪自编码可以缓解这个问题。</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913173322976.png" style="zoom:50%;">



<h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>在前面提到AE的缺点是，其得到的潜空间是非连续的，导致采样发生在离散点之间/外时，会生成比较模糊的图片。而VAE就是我们假设先验 $p(z)$ 的每个维度就是一个连续的distribution.  AE是用潜空间中的一个点(fixed vector)来对应一个训练样本，VAE则是用一个连续的分布来对应一个训练样本。通过代码来理解这句话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VariationalEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VariationalEncoder, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">512</span>, latent_dims)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">512</span>, latent_dims)</span><br><span class="line">        </span><br><span class="line">        self.kl = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.linear1(x))</span><br><span class="line">        mu =  self.linear2(x)</span><br><span class="line">        sigma = torch.exp(self.linear3(x))</span><br><span class="line">        </span><br><span class="line">        z = mu + sigma*torch.randn_like(sigma)</span><br><span class="line">        self.kl = <span class="number">0.5</span>*(sigma**<span class="number">2</span> + mu**<span class="number">2</span> - torch.log(sigma) - <span class="number">1</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(latent_dims, <span class="number">512</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">512</span>, <span class="number">784</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, z</span>):</span></span><br><span class="line">        x_hat = F.relu(self.linear1(z))</span><br><span class="line">        x_hat = torch.sigmoid(self.linear2(x_hat))</span><br><span class="line">        <span class="keyword">return</span> x_hat.reshape((-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VariationalAutoencoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VariationalAutoencoder, self).__init__()</span><br><span class="line">        self.encoder = VariationalEncoder(latent_dims)</span><br><span class="line">        self.decoder = Decoder(latent_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        z = self.encoder(x)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(z)</span><br></pre></td></tr></table></figure>

<p>第16行代码 <code>z = mu + sigma*torch.randn_like(sigma)</code> 中得到的是潜空间的多维特征表示 z, 每个维度都是一个正态分布(均值为 mu, 方差为 sigma). 第17行代码的kl散度看着很疑惑，这里需要推导得到，先暂放一边。</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913195106457.png" style="zoom:50%;">

<ul>
<li><p>先验 $p_{\theta}(z)$</p>
</li>
<li><p>似然 $p_{\theta}(x|z)$</p>
</li>
<li><p>后验 $p_{\theta}(z|x)$</p>
</li>
</ul>
<p>右图中实线时生成模型（先验*似然），我们的优化目标可以有两种选择，一个是最大化似然概率(MLH)，另一个是最大化后验概率(MAP). 但是对于 $p(x|z)p(z)$ 这个的优化是很难的，我们不可能去采样所有的 $p(z)$。</p>
<p>于是为了解决这个问题，我们可以采用变分近似的方法，也就是我们通过另一个判别模型来代替这个后验概率。也就是 $q_{\phi}(z|x)\sim p_{\theta}(z|x)$ . 这个判别模型就是下图中的encoder。</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210913201430732.png" style="zoom:50%;">

<p>如代码中13-14行显示那样，我们通过encoder直接生成每一个维度的均值和方差，得到一个多维的正态分布，然后在此基础上去解码。</p>
<p>接下来我们再回过头来说说第17行代码的kl散度是咋回事。我们了解的kl散度通常作为一个loss来衡量两个分布是否接近，那这里我们也确实有两个分布需要拉近，就是下式中的第一项，先验和后验我们需要尽量一致：</p>
<p><img src="https://lh4.googleusercontent.com/3eBrdDtvg3VJfwA-Vts0Z0vsaK6I7CJmFUxTKP0wXzIFFfh-vqnNufqHcClPfMs2CmRbjOdfF59k5b2GroQExYR-omQ96xaVl2ULEQC9dXM1c4sgjkWVhhcBiixTnzk_RY8YZw0=s0" alt="img"></p>
<p>后验$q_{\phi}(z|x)$就是我们的encoder，先验$p_{\theta}(z|x)$ 就是我们的先验。除此之外，还有第二项极大似然（也就是我们希望最大化观测的样本的概率）。</p>
<p>对于如何通过极大似然推导得到我们的目标函数，李宏毅老师PPT中讲的我觉得没有下面这个清楚。这个就很直观了。</p>
<blockquote>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914102405202.png" style="zoom:60%;">
</blockquote>
<p>假设之前的我们懂了，我们总归是得到了这样的一个目标损失函数。一个是重构loss，一个是kl散度。</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914092448703.png" style="zoom:50%;">

<p>对于第一项kl散度，我们假设先验 $p(z)$ 是正态分布，通过推导可以得到：</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914092720814.png" alt="image-20210914092720814" style="zoom:50%;">

<p>这样就跟第17行中的代码一样了。也就是 lower bound的第一项。</p>
<p>对于第二项重构损失：</p>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914093308467.png" style="zoom:50%;">这里用到的就是一种在参数技巧。直接生成均值和方差，把后验分布这个转换成一个多维的正态分布。</p>
<p>至此，loss中的两项就讲完了。训练代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, data, epochs=<span class="number">20</span></span>):</span></span><br><span class="line">    opt = torch.optim.Adam(model.parameters())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data:</span><br><span class="line">            x = x.to(device) <span class="comment"># GPU</span></span><br><span class="line">            opt.zero_grad()</span><br><span class="line">            x_hat = model(x)</span><br><span class="line">            loss = ((x - x_hat)**<span class="number">2</span>).<span class="built_in">sum</span>() + model.encoder.kl</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line">      </span><br><span class="line">latent_dims = <span class="number">2</span></span><br><span class="line">vae = VariationalAutoencoder(latent_dims).to(device) <span class="comment"># GPU</span></span><br><span class="line">vae = train(vae, data)</span><br></pre></td></tr></table></figure>

<p>同样为了可视化方便，我们把latent_dims=2. 可视化代码与之前一致：</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914095124312.png" style="zoom:60%;">

<p>边缘的位置还是看起来并不连续，可能是可视化样本比较少。</p>
<p>重构代码与之前一致：</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914095645194.png" style="zoom:70%;">

<p>效果确实好了一些。。那么VAE的“后验崩塌”在这个图里有体现吗，好像确实更倾向于生成“1”和“7” ？</p>
<h3 id="VQ-VAE-1"><a href="#VQ-VAE-1" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h3><p>VAE生成的是连续的潜空间，AE生成的是离散的潜空间。那么问题来了，这不是回到原始AE了吗。确实，VQ-VAE本质上更像AE，而不是VAE。但是不同于AE的是，对于vanilla AE，一个训练样本/图像对应一个fixed vector(维度为latent_dims)，也就是潜空间中的一个点。而VQ-VAE则是将图像中的一个patch映射到一个codebook中的一个embedding vector(可视化这些codes可能会有有趣的发现)。可以这么说，一个样本/图像是由潜空间中的多个点构成的，这个潜空间就我们需要学习/维护的codebook。 </p>
<p> 我的理解，不同的潜空间中的点组合就可以生成新的样本/图像了？而且相比AE，潜空间明显更强大了，所以可以避免“后验崩塌”（decoder直接忽略潜向量去生成样本/图像）</p>
<blockquote>
<p>“Introducing the VQ-VAE model, which is simple, uses discrete latents, does not suffer from “posterior collapse” and has no variance issues. “ 这是论文中的一句话。</p>
</blockquote>
<blockquote>
<p>下面是reddit网友/大佬的解释：</p>
<p>Unlike VAEs with continuous latent spaces, VQVAEs use a discrete space by having a codebook containing a large number of continuous vectors. The VQVAE encoding process maps an image to a continuous space then for each spatial location changes each vector to the closest one in the codebook.</p>
<p>Brief recap of posterior collapse in case you’re not sure: my understanding is that VAE models struggle with posterior collapse when (a) the latents contain little information about the input data and (b) a powerful generative model (e.g. an autoregressive decoder) is used that can model the data distribution without any latents. At the start of training the latents often contain little information about the data so the generative model can ignore them and focus on modelling the data distribution on its own. This results in a lack of gradients to the encoder amplifying the problem and so the latents are never used (i.e. posterior collapse).</p>
</blockquote>
<blockquote>
<p>Specifically with VQVAEs the latents (although discrete) are pretty high dimensional so can store a LOT of information about the input, so this helps with (a). As for (b), the decoder tends to be a fairly simple conv net so the latents are definitely needed to reconstruct the input.</p>
<p>As for the variance issues since VAE encoders are probabilistic, training requires sampling the encoder outputs which can have high variance. Gaussian VAEs bypass this using the reparameterisation trick (here is a great discussion on this <a target="_blank" rel="noopener" href="https://ermongroup.github.io/cs228-notes/extras/vae/">https://ermongroup.github.io/cs228-notes/extras/vae/</a>). While VQVAEs have deterministic encoders, the discretisation process can introduce variance, however, they use the straight through estimator which is biased but has low variance, I believe. This leads to other issues such as codebook collapse, where some codes are never used. DALL-E on the other hand uses Gumbel Softmax.</p>
</blockquote>
<p>这段话解释了VQ-VAE为啥能解决“后验崩塌”的问题，但是也带来了新的问题，“codebook collapse”，也就是通过encoder压缩之后的vector映射到codebook中的向量时，只会使用到codebook中的很小一部分vectors。</p>
<p>模型前向的框架如下：</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914111945553.png" style="zoom:50%;">

<p>很好理解的一个过程，我们可以把 $z_e \rightarrow z_q$ 看作是一个聚类的过程（也确实可以用kmeans方法），甚至也可以看作是一个再参数化的过程，只不过这个过程不可导。</p>
<p>因为存在argmin所以不可导，那么这个就是我们需要解决的一个问题。</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914112444311.png" style="zoom:50%;">

<p>怎么解决这个问题呢，把 $z_q$ 的梯度传递到 $z_e$，这听起来太玄乎了。让我们来看看代码吧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims, pic_channels=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=pic_channels, out_channels=latent_dims//<span class="number">2</span>, kernel_size=<span class="number">4</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=latent_dims//<span class="number">2</span>, out_channels=latent_dims, kernel_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims, pic_channels=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.conv_trans1 = nn.ConvTranspose2d(</span><br><span class="line">          in_channels=latent_dims, out_channels=latent_dims//<span class="number">2</span>, kernel_size=<span class="number">4</span>)</span><br><span class="line">        self.conv_trans2 = nn.ConvTranspose2d(</span><br><span class="line">          in_channels=latent_dims//<span class="number">2</span>, out_channels=pic_channels, kernel_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv_trans1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv_trans2(x)</span><br><span class="line">        <span class="keyword">return</span> x </span><br><span class="line"></span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VectorQuantizer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims, num_codes=<span class="number">32</span>, beta=<span class="number">0.25</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VectorQuantizer, self).__init__()</span><br><span class="line">        self.K = num_codes</span><br><span class="line">        self.D = latent_dims</span><br><span class="line">        self.beta = beta</span><br><span class="line"></span><br><span class="line">        self.codebook = nn.Embedding(self.K, self.D)</span><br><span class="line">        self.codebook.weight.data.uniform_(-<span class="number">1</span> / self.K, <span class="number">1</span> / self.K)</span><br><span class="line"></span><br><span class="line">        self.vq_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, latents</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         latents: (batch, dim, height, width)</span></span><br><span class="line"><span class="string">         codebook: (K, dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># convert latents from BCHW -&gt; BHWC</span></span><br><span class="line">        latents = latents.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous() <span class="comment"># (B, H, W, dim)</span></span><br><span class="line">        latents_shape = latents.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Flatten latent</span></span><br><span class="line">        flat_latent = latents.view(-<span class="number">1</span>, self.D) <span class="comment"># (BHW, dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute L2 distance between latents and codes in codebook</span></span><br><span class="line">        dist = (flat_latent.unsqueeze(<span class="number">1</span>) - self.codebook.weight.unsqueeze(<span class="number">0</span>)) ** <span class="number">2</span> <span class="comment"># (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim)</span></span><br><span class="line">        dist = dist.<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># (BHW, K)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get the code index that has the min distance</span></span><br><span class="line">        nearest_idxs = torch.argmin(dist, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># (BHW, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert to one-hot</span></span><br><span class="line">        nearest_one_hot = torch.zeros(nearest_idxs.size(<span class="number">0</span>), self.K, device=latents.device) <span class="comment"># (BHW, K)</span></span><br><span class="line">        nearest_one_hot.scatter_(<span class="number">1</span>, nearest_idxs, <span class="number">1</span>)  <span class="comment"># .scatter(dim,index,src)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Quantize the latents</span></span><br><span class="line">        quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) <span class="comment"># (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the VQ Losses</span></span><br><span class="line">        commitment_loss = F.mse_loss(quantized_latents.detach(), latents)</span><br><span class="line">        codebook_loss = F.mse_loss(quantized_latents, latents.detach())</span><br><span class="line"></span><br><span class="line">        self.vq_loss = commitment_loss * self.beta + codebook_loss</span><br><span class="line"></span><br><span class="line">        <span class="comment"># convert quantized from BHWC -&gt; BCHW</span></span><br><span class="line">        <span class="keyword">return</span> quantized_latents.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VQVariationalAutoencoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims, ema=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VQVariationalAutoencoder, self).__init__()</span><br><span class="line">        self.encoder = Encoder(latent_dims)</span><br><span class="line">        <span class="keyword">if</span> ema:</span><br><span class="line">          self.vector_quantizer = VectorQuantizerEMA(latent_dims)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          self.vector_quantizer = VectorQuantizer(latent_dims)</span><br><span class="line">        self.decoder = Decoder(latent_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        z_e = self.encoder(x)</span><br><span class="line">        z_q = self.vector_quantizer(z_e) <span class="comment"># (batch, dim, 22, 22)</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(z_q)</span><br></pre></td></tr></table></figure>

<p>看代码其实也挺简单的：</p>
<ul>
<li><p>52-65行计算 l2 距离，然后选择距离最小的index，再通过这个index去codebook中选取对应的vector</p>
</li>
<li><p>这里的 quantized_latents 也就是我们想到得到的 $z_q$</p>
</li>
<li><p>68-69行代码对应下式中后两个loss</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914153647534.png" style="zoom:45%;"></li>
</ul>
<p>这里有个不太懂的问题，$z_q$的计算过程因为第58行argmin肯定是不可导的。可是代码中也没有见到做任何处理。在[苏剑林的博客](<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6760">VQ-VAE的简明介绍：量子化自编码器 - 科学空间|Scientific Spaces</a>)中讲解了这一点：</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914154458963.png" style="zoom:50%;">

<p>按照这个思路，我觉得第74行代码应该改成下面这样才对，这样确确实实做到了把 $z_q$ 的梯度传递给 $z_e$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">quantized_latents = latents + (quantized_latents - latents).detach()</span><br><span class="line"><span class="keyword">return</span> quantized_latents.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br></pre></td></tr></table></figure>

<p>提供的代码链接确实是有问题的，改成上述两行代码之后就可以跑出效果了～</p>
<p>接下来我们训练VQ-VAE,并可视化训练过程中的codebook和重构情况。为了方便可视化，我们同样设置latent_dims=2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_codebook</span>(<span class="params">autoencoder</span>):</span></span><br><span class="line">    codes = autoencoder.vector_quantizer.codebook.weight.detach().cpu().numpy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(codes.shape[<span class="number">0</span>]):</span><br><span class="line">        plt.scatter(codes[i][<span class="number">0</span>], codes[i][<span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_recon</span>(<span class="params">autoencoder, data, n=<span class="number">10</span></span>):</span></span><br><span class="line">    x = <span class="built_in">next</span>(<span class="built_in">iter</span>(data))[<span class="number">0</span>][:n]</span><br><span class="line">    x_hat = autoencoder(x.to(device))</span><br><span class="line">    x_hat = x_hat.to(<span class="string">&#x27;cpu&#x27;</span>).detach().numpy().squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    w = x_hat.shape[<span class="number">1</span>]</span><br><span class="line">    img = np.zeros((w, n*w))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;original:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x_hat.shape[<span class="number">0</span>]):</span><br><span class="line">      img[:, i*w:(i+<span class="number">1</span>)*w] = x[i]</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;reconstructed:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x_hat.shape[<span class="number">0</span>]):</span><br><span class="line">      img[:, i*w:(i+<span class="number">1</span>)*w] = x_hat[i]</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">autoencoder, data, epochs=<span class="number">20</span></span>):</span></span><br><span class="line">    opt = torch.optim.Adam(autoencoder.parameters())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data:</span><br><span class="line">            x = x.to(device) <span class="comment"># GPU</span></span><br><span class="line">            opt.zero_grad()</span><br><span class="line">            x_hat = autoencoder(x)</span><br><span class="line">            loss = ((x - x_hat)**<span class="number">2</span>).<span class="built_in">sum</span>() + autoencoder.vector_quantizer.vq_loss</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">        plot_codebook(autoencoder)</span><br><span class="line">        plot_recon(autoencoder, data)</span><br><span class="line">    <span class="keyword">return</span> autoencoder</span><br></pre></td></tr></table></figure>

<p>训练的可视化过程如下：</p>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914164923274.png" style="zoom:50%;"> <img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914164945741.png" style="zoom:50%;"> </p>
<p>这效果有点玄学，第一次跑出来效果挺好，之后跑出来就很模糊。。</p>
<p>把维度扩大到 latent_dims=32后效果会变好，效果如下：</p>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914165402615.png" style="zoom:50%;"> <img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914165342645.png" style="zoom:50%;"> </p>
<p>EMA update codebook：</p>
<p>对于loss function中的第二项 $||sg(E(x))- e_{k}||$ 只是用来更新codebook，这个可以用 EMA的方法来更新。也就是对于某一个code $e_i$，它是被 $z_{i,1},z_{i,2},…,z_{i,n}$ 选中的code vector，因此它应该于这些encoder output vector的均值 $\frac{1}{n}\sum_{j}^{n}z_{i,j}$ 更接近。</p>
<p>当使用小批量(minibatches) 训练时，由于数据量不足，直接用这个均值来更新 $e_i$是不准确的。所以用 指数滑动平均(exponential moving average) 来更新 $e_i$：</p>
<p><img src="https://lh3.googleusercontent.com/XregUPXxAP8LRUDsZcK8CaWu4yQc1v938HApWj6f3P4tveBPzBTFNSv3xJ6zytDWwShW41fgc6D89UYmgbcvQXKz0F2fyNLhNYw3nIZdQEQoGKwyLQnWZznQWdeBs8JjiP9eF3E=s0" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VectorQuantizerEMA</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_dims, num_codes=<span class="number">32</span>, beta=<span class="number">0.25</span>, gamma=<span class="number">0.99</span>, epsilon=<span class="number">1e-5</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VectorQuantizerEMA, self).__init__()</span><br><span class="line">        self.K = num_codes</span><br><span class="line">        self.D = latent_dims</span><br><span class="line">        self.beta = beta</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.epsilon = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">        self.codebook = nn.Embedding(self.K, self.D)</span><br><span class="line">        self.codebook.weight.data.normal_()</span><br><span class="line"></span><br><span class="line">        self.N = <span class="literal">None</span></span><br><span class="line">        self.m = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.vq_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, latents</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         latents: (batch, dim, height, width)</span></span><br><span class="line"><span class="string">         codebook: (K, dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># convert latents from BCHW -&gt; BHWC</span></span><br><span class="line">        latents = latents.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous() <span class="comment"># (B, H, W, dim)</span></span><br><span class="line">        latents_shape = latents.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Flatten latent</span></span><br><span class="line">        flat_latent = latents.view(-<span class="number">1</span>, self.D) <span class="comment"># (BHW, dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute L2 distance between latents and codes in codebook</span></span><br><span class="line">        dist = (flat_latent.unsqueeze(<span class="number">1</span>) - self.codebook.weight.unsqueeze(<span class="number">0</span>)) ** <span class="number">2</span> <span class="comment"># (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim)</span></span><br><span class="line">        dist = dist.<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># (BHW, K)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get the code index that has the min distance</span></span><br><span class="line">        nearest_idxs = torch.argmin(dist, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># (BHW, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert to one-hot</span></span><br><span class="line">        nearest_one_hot = torch.zeros(nearest_idxs.size(<span class="number">0</span>), self.K, device=latents.device) <span class="comment"># (BHW, K)</span></span><br><span class="line">        nearest_one_hot.scatter_(<span class="number">1</span>, nearest_idxs, <span class="number">1</span>)  <span class="comment"># .scatter(dim,index,src)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Quantize the latents</span></span><br><span class="line">        quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) <span class="comment"># (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the VQ Losses</span></span><br><span class="line">        commitment_loss = F.mse_loss(quantized_latents.detach(), latents)</span><br><span class="line">        self.vq_loss = commitment_loss * self.beta</span><br><span class="line"></span><br><span class="line">        <span class="comment"># EMA update cookbook</span></span><br><span class="line">        n = torch.<span class="built_in">sum</span>(nearest_one_hot, <span class="number">0</span>) <span class="comment"># (K)</span></span><br><span class="line">        self.N = self.N * self.gamma + (<span class="number">1</span> - self.gamma) * n <span class="keyword">if</span> self.N <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> n</span><br><span class="line">        N_ = torch.<span class="built_in">sum</span>(self.N.data) <span class="comment"># Laplace smoothing of the cluster size</span></span><br><span class="line">        self.N = (self.N + self.epsilon) / (N_ + self.K * self.epsilon) * N_</span><br><span class="line">        z = torch.matmul(nearest_one_hot.T, flat_latent) <span class="comment"># (K, BHW) * (BHW, dim) = (K, dim)</span></span><br><span class="line">        self.m = nn.Parameter(self.m * self.gamma + (<span class="number">1</span> - self.gamma) * z) <span class="keyword">if</span> self.m <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> nn.Parameter(z)</span><br><span class="line">        self.codebook.weight = nn.Parameter(self.m / self.N.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert quantized from BHWC -&gt; BCHW</span></span><br><span class="line">        quantized_latents = latents + (quantized_latents - latents).detach()</span><br><span class="line">        <span class="keyword">return</span> quantized_latents.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br></pre></td></tr></table></figure>

<p>潜空间维度 latent_dims=2， 可视化如下：</p>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914173838681.png" style="zoom:50%;"> <img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914173852849.png" style="zoom:50%;"> </p>
<p>潜空间维度为32， 可视化如下：</p>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914173951653.png" style="zoom:50%;"> <img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914174005480.png" style="zoom:50%;"> </p>
<p>前面这些可视化只是在训练过程中的重构，那么我们是否可以像VAE那样，丢掉encoder，直接从 latent space中采样去生成新的样本/图像呢？ 但是随机采样出来的codes组合起来的image大概乱七八糟的。。这个时候就在这个低维空间去训练一个自回归模型，来正确采样出有效有意义的codes</p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914180014657.png" style="zoom:50%;">



<p>在训练VQ-VAE完之后。我们并不能去生成新的样本/图像。怎么去从潜空间采样出这些离散的codes呢？现在我们需要训一个自回归的模型，让这些codes的排列变得有意义。我们可以把这些codes的序列看成一句话，然后训练这样一个自回归模型，也就是stage2 prior training。下图是VQ-VAE-2的伪代码（2.0版本可以看作是一个层次化的VQ-VAE). </p>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914180200167.png" style="zoom:50%;">

<p>简单解释下：</p>
<ul>
<li>$e_{top}\leftarrow Quantize(h_{top})$ 中 $h_{top}$ 就是encoder的输出，通过量化(映射)之后得到 $e_{top}$ </li>
<li>对于bottom也是一样的</li>
</ul>
<p>prior training: </p>
<ul>
<li><p>$T_{top}$ 就是 $e_{top}$ 的序列</p>
</li>
<li><p>$p_{top}=TrainPixelCNN(T_{top})$ 训练自回归模型</p>
</li>
</ul>
<h2 id="VideoGPT"><a href="#VideoGPT" class="headerlink" title="VideoGPT "></a>VideoGPT <a name="videogpt"></a></h2><p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914210857358.png" alt="image-20210914210857358"></p>
<p>左边就是原始的VQ-VAE. 右侧是训练一个自回归网络。这么看这篇paper确实没啥创新点啊。。就是把vq-vae应用在了video上，不过代码其实复杂了很多。</p>
<p>分两个阶段：</p>
<ul>
<li>VQ-VAE training</li>
<li>VideoGPT training</li>
</ul>
<p>看完代码，第二阶段forward过程如下：</p>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210914214726470.png"></p>
<p>需要注意的是，经过vqvae.encoder之后的维度变成了 [t/4, h/4,w/4], 这就是 latent vectors的个数，后续就是在这个上面做self-attention以及自回归 (注意自回归需要decoder input right-shift).</p>
<h2 id="LVT"><a href="#LVT" class="headerlink" title="LVT"></a>LVT<a name="lvt"></a></h2><p>这篇跟 VideoGPT很像，都是在潜空间内做自回归，但是这篇paper在VideoGPT之前发表。而且看了openviewer的审稿意见，reviewers 就怼VideoGPT跟这篇很像。。但是两者的模型结构是有区别的</p>
<h2 id="DVT-NAT"><a href="#DVT-NAT" class="headerlink" title="DVT-NAT "></a>DVT-NAT <a name="DVT-NAT"></a></h2><p>Fast Decoding in Sequence Models Using Discrete Latent Variables, (ICML2018)</p>
<p>这篇paper好呀好呀，是真好！！关键还是18年就发表的，是真的牛逼！反思一下为啥别人就能在紧跟前沿，而且还能从CV领域follow出这么厉害的NLP的工作呢？</p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul>
<li><p>提出了一种对目标序列进行离散建模的方法，能有效提高自回归建模的效率，从而提升解码速度</p>
<blockquote>
<p>To overcome this limitation, we propose to introduce a sequence of discrete latent variables $l_1 . . . l_m$, with $m &lt; n$, that summarizes the relevant information from the sequence $y_1 . . . y_n$. We will still generate $l_1 . . . l_m$ autoregressively, but it will be much faster as $m &lt; n$ (in our experiments we mostly use $m = n/8$ ). Then, we reconstruct each position in the sequence $y_1 . . . y_n$ from $l_1 . . . l_m$ in parallel.</p>
<p>以前的seq2seq方式是 $(x_1,…,x_L) \rightarrow (y_1,…,y_n)$ ，现在是 $(x_1,…,x_L) \rightarrow (l_1,..,l_m)\rightarrow (y_1,…,y_n)$ ,其中 $l_1,…,l_m$ 的生成依然是自回归的，但是长度更短， 所以提高了效率。</p>
</blockquote>
</li>
<li><p>将 latent transformer 应用在机器翻译上，提升翻译效率。但是在BLEU上仍然比自回归的方法要差很多。</p>
</li>
</ul>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ul>
<li>提出了一个基于离散潜变量的快速解码框架</li>
<li>提出新的离散化技术，能有效缓解VQ-VAE中的index collapse问题</li>
<li>将 <strong>latent transofmer</strong> 应用在机器翻译上，提升了翻译速度</li>
</ul>
<h4 id="Discretization-Techniques"><a href="#Discretization-Techniques" class="headerlink" title="Discretization Techniques"></a>Discretization Techniques</h4><ul>
<li><p>Gumbel-Softmax</p>
<ul>
<li>将argmax/argmin可微化。将采样过程用可导的softmax代替，同时加上gumbel noise使得最终的结果跟</li>
<li>PyTorch 32.Gumbel-Softmax Trick - 科技猛兽的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/166632315">https://zhuanlan.zhihu.com/p/166632315</a></li>
</ul>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916151621009.png" alt="image-20210916151621009" style="zoom:50%;"> <img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916151649926.png" alt="image-20210916151649926" style="zoom:50%;"> </p>
</li>
<li><p>Improved Semantic Hashing</p>
</li>
<li><p>Vector Quantization</p>
</li>
</ul>
<h4 id="Decomposed-Vector-Quantization"><a href="#Decomposed-Vector-Quantization" class="headerlink" title="Decomposed Vector Quantization"></a>Decomposed Vector Quantization</h4><h5 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h5><p>index collapse, where only a few of the embedding vectors get trained due to a rich getting richer phenomena</p>
<p><img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916162859221.png" style="zoom:50%;"> <img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916162937840.png" style="zoom:50%;"> </p>
<h5 id="Sliced-Vector-Quantization"><a href="#Sliced-Vector-Quantization" class="headerlink" title="Sliced Vector Quantization"></a>Sliced Vector Quantization</h5><ul>
<li>break up the encoder output <code>enc(y)</code> into <code>n_d</code> smaller slices, like multi-head attention in transformer (eq. 11)</li>
</ul>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916163744821.png" style="zoom:50%;">

<h4 id="Latent-Transformer"><a href="#Latent-Transformer" class="headerlink" title="Latent Transformer"></a>Latent Transformer</h4><ul>
<li><p>Main Steps</p>
<ul>
<li>VAE encoder encodes target sentence $y$ into shorter discrete latent variables $l$ (parallel)</li>
<li>Latent prediction model - Transformer, is trained to predict $l$ from source sentence $x$ (autoregressive)</li>
<li>VAE decoder decodes predicted $l$ back to sequence $y$ (parallel)</li>
</ul>
</li>
<li><p>Loss Function</p>
<ul>
<li>reconstruction loss $l_r$ from VAE</li>
<li>latent prediction loss $l_{lp}$ from Latent Transformer</li>
<li>in first 10k steps, true targets $y$ is given to transformer-decoder instead of decompressed latents $l$ which ensures self-attention part has reasonable gradients to train the whole architecture</li>
</ul>
</li>
<li><p>Architectures of  VAE</p>
<ul>
<li><p>Encoder</p>
<ul>
<li>conv residual blocks + attention + conv to scale down the dimension</li>
<li>$C = n/m, C=2^c$, in the setting, $C=8, c=3$</li>
</ul>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916195155055.png" style="zoom:50%;"></li>
<li><p>Decoder</p>
<ul>
<li>conv residual blocks + attention + up-conv to scale up the dimension</li>
<li>Transformer Decoder</li>
</ul>
<img src="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/image-20210916195414002.png" style="zoom:50%;"></li>
</ul>
</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>论文笔记-Discrete Latent Variables Based Generation</p><p><a href="http://www.panxiaoxie.cn/2021/09/12/论文笔记-Discrete-Latent-Variables-Based-Generation/">http://www.panxiaoxie.cn/2021/09/12/论文笔记-Discrete-Latent-Variables-Based-Generation/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-09-12</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-12-08</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/generation/">generation</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/09/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-generation/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">论文笔记-video generation</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/07/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Clip/"><span class="level-item">论文笔记-Clip!!!</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/';
            this.page.identifier = '2021/09/12/论文笔记-Discrete-Latent-Variables-Based-Generation/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#VQ-VAE"><span class="level-left"><span class="level-item">1</span><span class="level-item">VQ-VAE</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Auto-Encoder"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Auto-Encoder</span></span></a></li><li><a class="level is-mobile" href="#VAE"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">VAE</span></span></a></li><li><a class="level is-mobile" href="#VQ-VAE-1"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">VQ-VAE</span></span></a></li></ul></li><li><a class="level is-mobile" href="#VideoGPT"><span class="level-left"><span class="level-item">2</span><span class="level-item">VideoGPT </span></span></a></li><li><a class="level is-mobile" href="#LVT"><span class="level-left"><span class="level-item">3</span><span class="level-item">LVT</span></span></a></li><li><a class="level is-mobile" href="#DVT-NAT"><span class="level-left"><span class="level-item">4</span><span class="level-item">DVT-NAT </span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">4.1.1</span><span class="level-item">Abstract</span></span></a></li></ul><li><a class="level is-mobile" href="#Contribution"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Contribution</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Discretization-Techniques"><span class="level-left"><span class="level-item">4.2.1</span><span class="level-item">Discretization Techniques</span></span></a></li><li><a class="level is-mobile" href="#Decomposed-Vector-Quantization"><span class="level-left"><span class="level-item">4.2.2</span><span class="level-item">Decomposed Vector Quantization</span></span></a></li><li><a class="level is-mobile" href="#Latent-Transformer"><span class="level-left"><span class="level-item">4.2.3</span><span class="level-item">Latent Transformer</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>