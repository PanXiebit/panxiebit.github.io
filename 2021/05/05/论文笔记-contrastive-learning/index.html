<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>论文笔记-image-based contrastive learning - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="simCLR MoCo BYOL Swin-ssl BraVe   What Makes for Good Views for Contrastive Learning? BYOL works even without batch statistics   Understanding Self-Supervised Learning Dynamics without Contrastive Pa"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="simCLR MoCo BYOL Swin-ssl BraVe   What Makes for Good Views for Contrastive Learning? BYOL works even without batch statistics   Understanding Self-Supervised Learning Dynamics without Contrastive Pa"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2021-05-05T02:03:16.000Z"><meta property="article:modified_time" content="2021-08-01T01:41:14.735Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="contrastive learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/"},"headline":"论文笔记-image-based contrastive learning","image":["http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/simslr.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss_func.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/algorithm.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/random_crop.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/data_aug_method.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/data_augmentation.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/distortion.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/vs_supervised.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/projection.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/before_head.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss_res.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/normalization.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/batch_size.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/linear_evaluation.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/linear_eval.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/few_label.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/transfer_learn.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/wav2vec.png","http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/wav2vec_loss.png"],"datePublished":"2021-05-05T02:03:16.000Z","dateModified":"2021-08-01T01:41:14.735Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"simCLR MoCo BYOL Swin-ssl BraVe   What Makes for Good Views for Contrastive Learning? BYOL works even without batch statistics   Understanding Self-Supervised Learning Dynamics without Contrastive Pa"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-05-05T02:03:16.000Z" title="2021/5/5 上午10:03:16">2021-05-05</time>发表</span><span class="level-item"><time dateTime="2021-08-01T01:41:14.735Z" title="2021/8/1 上午9:41:14">2021-08-01</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/">constrast learning</a></span><span class="level-item">15 分钟读完 (大约2241个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">论文笔记-image-based contrastive learning</h1><div class="content"><ul>
<li><a href="#simCLR">simCLR</a></li>
<li><a href="#MoCo">MoCo</a></li>
<li><a href="#BYOL">BYOL</a></li>
<li><a href="#Swin-ssl">Swin-ssl</a></li>
<li><a href="#BraVe">BraVe</a>  </li>
<li>What Makes for Good Views for Contrastive Learning?</li>
<li>BYOL works even without batch statistics  </li>
<li>Understanding Self-Supervised Learning Dynamics without Contrastive Pairs   </li>
<li>Big Self-Supervised Models are Strong Semi-Supervised Learners  </li>
<li>Understanding contrastive representation learning through alignment and uniformity on the hypersphere.</li>
</ul>
<span id="more"></span>

<h2 id="simCLR"><a href="#simCLR" class="headerlink" title="simCLR"></a>simCLR<a name="simCLR"></a></h2><p>A Simple Framework for Contrastive Learning of Visual Representations</p>
<p>作者提出了一个简单的对比学习框架，不需要特殊的网络结构和memory bank.</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>现有的无监督视觉表示学习的方法主要分为两类：生成式和判别式。</p>
<p>生成式主要包括以下三类：    </p>
<ul>
<li><a href="#1">deep belief nets</a>  </li>
<li><a href="#2">Auto-encoding</a>   </li>
<li><a href="#3">Generative adversarial nets</a>    </li>
</ul>
<p>pixel-level 生成式算法非常消耗计算资源，因而对于有效的表示学习并不是必须的。</p>
<p>判别式方法的目标函数更接近监督学习，不过其对应的监督任务是从没有标签的数据集中自行构造的，因而学到的视觉表示能力受限于预定义的任务，而泛化能力有限。现有的达到sota的几篇paper<a href="#4">[4]</a><a href="#5">[5]</a><a href="#6">[6]</a>    </p>
<p>作者提出了一个简单的对比学习方法，不仅达到了sota，而且不需要复杂的网络结构<a href="#6">[6]</a><a href="#7">[7]</a>，也不需要memory bank<a href="#8">[8]</a><a href="#9">[9]</a><a href="#10">[10]</a><a href="#11">[11]</a>.    </p>
<p>为了系统的理解怎样才能获得有效的的对比学习，作者研究了以下几个重要组成部分：   </p>
<ul>
<li>data augmentation：相比有监督学习，对比学习更需要数据增强 $t\sim T$      </li>
<li>nonlinear projection：如图所示，在视觉表示和contrast loss之间增加一个非线性projection $g(\cdot)$ 很有必要  </li>
<li>normalized embeddings and an appropriately adjusted temperature parameter: 归一化的embedding和可调整的temperature parameter.      </li>
<li>larger batch size and more training steps   </li>
</ul>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/simslr.png"></p>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>如上图所示，simCLR 主要包括四部分：  </p>
<ul>
<li>A stochastic data augmentation module: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur    </li>
<li>A neural network base encoder $f(\cdot)$，作者采用的是 ResNet. $h_i = f(\tilde x_i) = ResNet(\tilde x_i)$   </li>
<li>A small neural network projection head $g(\cdot)$, $z_i = g(h_i) = W^{(2)}σ(W^{(1)}h_i)$. 作者发现在 $z_i$ 上计算 contrast loss，比 $h_i$ 效果更好。   </li>
<li>A contrastive loss function：NT-Xent (the normalized temperature-scaled cross entropy loss.  </li>
</ul>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss_func.png"></p>


<p>其中 $sim(u,v)=\dfrac{u^Tv}{\lVert u \rVert \lVert v\rVert}$.</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/algorithm.png"></p>
<h4 id="Training-with-Large-Batch-Size"><a href="#Training-with-Large-Batch-Size" class="headerlink" title="Training with Large Batch Size"></a>Training with Large Batch Size</h4><p>作者采用了更大的batch size(256 $\rightarrow$ 8192)，因而不需要memory bank. 这样一个batch有 ($8192\times 2=16382$) 个负样本。</p>
<p>在超大的batch size情况下，使用SGD/Momentum学习率不稳定，因此作者使用LARS optimizer.</p>
<p>作者使用 32-128 cores TPU进行训练。（这真的劝退。。。</p>
<h5 id="Global-BN"><a href="#Global-BN" class="headerlink" title="Global BN"></a>Global BN</h5><p>在分布式训练的场景下，BN的均值和方差是在单个device上计算的。而两个正样本是在同一个device上计算的，因此在拉进两个正样本之间的agreement时，BN会造成信息泄露。为了解决这个问题，作者采用的方法是在所有的device上计算BN的均值和方差。类似地解决这一问题的方法还有：shuffling data examples across devices[^10], replacing BN with layer norm[^7].</p>
<blockquote>
<p>这点其实不太理解，为啥BN会造成信息泄露？</p>
</blockquote>
<h4 id="Evaluation-Protocol"><a href="#Evaluation-Protocol" class="headerlink" title="Evaluation Protocol"></a>Evaluation Protocol</h4><h5 id="Dataset-and-Metrics"><a href="#Dataset-and-Metrics" class="headerlink" title="Dataset and Metrics."></a>Dataset and Metrics.</h5><p>作者先在CIFAR-10上进行试验，得到了94.0%的准确率（有监督的准确率是95.1%）.</p>
<p>为了验证学习得到的视觉表示，作者采用广泛使用的linear evaluation protocol[^5][^6]: a linear classifier is trained on top of the frozen base network, and test accuracy is used as a proxy for representation quality.</p>
<p>除了linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning.</p>
<h5 id="Default-setting"><a href="#Default-setting" class="headerlink" title="Default setting"></a>Default setting</h5><p>We use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space.</p>
<p>As the loss, we use NT-Xent, optimized using LARS with learning rate of <strong>4.8</strong> (= 0.3 × BatchSize/256) and weight decay of 10−6. We train at batch size 4096 for 100 epochs.</p>
<h3 id="Data-Augmentation-for-Contrastive-Representation-Learning"><a href="#Data-Augmentation-for-Contrastive-Representation-Learning" class="headerlink" title="Data Augmentation for Contrastive Representation Learning"></a>Data Augmentation for Contrastive Representation Learning</h3><h4 id="Data-augmentation-defines-predictive-tasks"><a href="#Data-augmentation-defines-predictive-tasks" class="headerlink" title="Data augmentation defines predictive tasks"></a>Data augmentation defines predictive tasks</h4><p>数据增强定义预预测任务。</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/random_crop.png"></p>
<p>随机裁剪既包括了 global and local views, 也包括了 adjacent views.</p>
<h4 id="Composition-of-data-augmentation-operations-is-crucial-for-learning-good-representations"><a href="#Composition-of-data-augmentation-operations-is-crucial-for-learning-good-representations" class="headerlink" title="Composition of data augmentation operations is crucial for learning good representations"></a>Composition of data augmentation operations is crucial for learning good representations</h4><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/data_aug_method.png"></p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/data_augmentation.png"></p>
<p>为了验证不同的数据增强对于表示学习的影响，作者进行了ablation实验，只对图2中的某一分支进行transformation. 实验结果如图5所示，对角线只有一种augmentation方法，非对角线是两种组合。</p>
<p>结果表明，单一的增强方法都不能学到好的表示。两种组合时，预测任务越难，学习到的表示能力越好。最好的组合是 random crop 和 color distortion. 但是只是单独用其中某一种效果都不好。</p>
<p>作者对只用单独一种数据增强方法不好的原因进行了解释：</p>
<blockquote>
<ol>
<li>We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution.</li>
<li>Figure 6 shows that color histograms alone suffice to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features.</li>
</ol>
</blockquote>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/distortion.png"></p>
<h4 id="Contrastive-learning-needs-stronger-data-augmentation-than-supervised-learning"><a href="#Contrastive-learning-needs-stronger-data-augmentation-than-supervised-learning" class="headerlink" title="Contrastive learning needs stronger data augmentation than supervised learning"></a>Contrastive learning needs stronger data augmentation than supervised learning</h4><p>相比监督学习，stronger数据增强对contrastive learning更为重要。</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/vs_supervised.png"></p>
<blockquote>
<p>When training supervised models with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning.</p>
</blockquote>
<h3 id="Architectures-for-Encoder-and-Head"><a href="#Architectures-for-Encoder-and-Head" class="headerlink" title="Architectures for Encoder and Head"></a>Architectures for Encoder and Head</h3><h4 id="Unsupervised-contrastive-learning-benefits-more-from-bigger-models"><a href="#Unsupervised-contrastive-learning-benefits-more-from-bigger-models" class="headerlink" title="Unsupervised contrastive learning benefits (more) from bigger models"></a>Unsupervised contrastive learning benefits (more) from bigger models</h4><p>对比学习在大模型下获益更多。</p>
<h4 id="A-nonlinear-projection-head-improves-the-representation-quality-of-the-layer-before-it"><a href="#A-nonlinear-projection-head-improves-the-representation-quality-of-the-layer-before-it" class="headerlink" title="A nonlinear projection head improves the representation quality of the layer before it"></a>A nonlinear projection head improves the representation quality of the layer before it</h4><p>作者探究了projection的三种方式：  </p>
<ul>
<li>identity mapping  </li>
<li>linear projection  </li>
<li>non-linear projection  </li>
</ul>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/projection.png"></p>
<p>结果表明，非线性projection更好，没有的话效果很差。</p>
<p>除此之外，使用project head之前的hidden layer $h(i)$ 比project layer之后的表示 $z(i)$ 效果更好, $\ge 10%$。</p>
<ol>
<li>为什么使用 non-linear projection head 之前的hidden layer效果更好？<br> 作者认为对比loss会损失信息。z = g(h) 被训练成transformation invariant 变换不变性（因为contrast loss要拉近两个不同变换的正样本）。因此，$g(\cdot)$ 会丢失信息。<br> 作者通过实验验证这一猜想，在保证最终的dimension不变的情况下，<br> <img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/before_head.png"></li>
</ol>
<h3 id="Loss-Functions-and-Batch-Size"><a href="#Loss-Functions-and-Batch-Size" class="headerlink" title="Loss Functions and Batch Size"></a>Loss Functions and Batch Size</h3><h4 id="Normalized-cross-entropy-loss-with-adjustable-temperature-works-better-than-alternatives"><a href="#Normalized-cross-entropy-loss-with-adjustable-temperature-works-better-than-alternatives" class="headerlink" title="Normalized cross entropy loss with adjustable temperature works better than alternatives"></a>Normalized cross entropy loss with adjustable temperature works better than alternatives</h4><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss.png"></p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss_res.png"></p>
<p>实验表明 NT-Xent 效果最好。这是因为其他的目标函数并没有衡量负样本的难度：unlike cross-entropy, other objective functions do not weight the negatives by their relative hardness.</p>
<p>$l_2$ normalization 和 temperture 很重要：$l_2$ normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives;</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/normalization.png"></p>
<ul>
<li>没有 $l_2$ normalization,尽管对比准确率很高，但是学习到的表示能力并不好。  </li>
<li>合适的temperture也很重要</li>
</ul>
<h4 id="Contrastive-learning-benefits-more-from-larger-batch-sizes-and-longer-training"><a href="#Contrastive-learning-benefits-more-from-larger-batch-sizes-and-longer-training" class="headerlink" title="Contrastive learning benefits (more) from larger batch sizes and longer training"></a>Contrastive learning benefits (more) from larger batch sizes and longer training</h4><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/batch_size.png"></p>
<p>实验表明，batch size很重要，越大收敛的越快，但最终效果也不是越大越好。随着训练的增加，batch size造成的表现差异也随着逐渐消失。</p>
<h3 id="Comparison-with-State-of-the-art"><a href="#Comparison-with-State-of-the-art" class="headerlink" title="Comparison with State-of-the-art"></a>Comparison with State-of-the-art</h3><p>作者采用了三种方法来验证performance。</p>
<h4 id="Linear-evaluation"><a href="#Linear-evaluation" class="headerlink" title="Linear evaluation"></a>Linear evaluation</h4><p>相比fine-tune，linear evaluation 的区别在于学习率的设置。</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/linear_evaluation.png"></p>
<blockquote>
<p>没搞懂为啥叫 linear evaluation？ 和fine-tune的区别就在于学习率的设置？</p>
</blockquote>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/linear_eval.png"></p>
<h4 id="Semi-supervised-learning"><a href="#Semi-supervised-learning" class="headerlink" title="Semi-supervised learning"></a>Semi-supervised learning</h4><p>sample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (∼12.8 and ∼128 images per class respectively).</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/few_label.png"></p>
<h4 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h4><p>在imageNet上训练，在其他数据集上测试。同上，采用了两种方式， Linear evaluation 和 fine-tune.</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/transfer_learn.png"></p>
<h1 id="speech-based"><a href="#speech-based" class="headerlink" title="### speech-based"></a>### speech-based</h1><h2 id="Representation-Learning-with-Contrastive-Predictive-Coding"><a href="#Representation-Learning-with-Contrastive-Predictive-Coding" class="headerlink" title="Representation Learning with Contrastive Predictive Coding"></a>Representation Learning with Contrastive Predictive Coding</h2><h2 id="wav2vec-Unsupervised-pre-training-for-speech-recognition"><a href="#wav2vec-Unsupervised-pre-training-for-speech-recognition" class="headerlink" title="wav2vec: Unsupervised pre-training for speech recognition"></a>wav2vec: Unsupervised pre-training for speech recognition</h2><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/wav2vec.png"></p>
<ul>
<li>encoder: $X \rightarrow Z$  </li>
<li>content-network: $C\rightarrow C$</li>
</ul>
<p><strong>noise contrastive binary classification task.</strong></p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/wav2vec_loss.png"></p>
<ul>
<li>$\sigma$ 是 sigmoid 函数</li>
<li>$c_i$ 是当前step的content feature  </li>
<li>$h_k(x_i)$ 是step-specific affine transformation $h_k(c_i) = W_kc_i+b_k$  </li>
<li>$z_{i+k}$ 是距离当前step为k的encoded feature, 为正样本  </li>
<li>$\hat z\sim p_n$是 (T-k) 帧中均匀选择10个负样本，得到对数概率的期望后，再乘以 $\lambda=10$. –&gt;</li>
</ul>
<hr>
<h2 id="MoCo"><a href="#MoCo" class="headerlink" title="MoCo"></a>MoCo<a name="MoCo"></a></h2><hr>
<h2 id="BYOL"><a href="#BYOL" class="headerlink" title="BYOL"></a>BYOL<a name="BYOL"></a></h2><p>paper: Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</p>
<hr>
<p>[1]<a name="1"></a> : A fast learning algorithm for deep belief nets.<br>[2]<a name="2"></a>: Auto-encoding variational bayes.<br>[3]<a name="3"></a>: Generative adversarial nets. NIPS2014<br>[4]<a name="4"></a>: Discriminative unsupervised feature learning with convolutional neural networks. NIPS2014<br>[5]<a name="5"></a>: Representation learning with contrastive predictive coding. arXiv2018<br>[6]<a name="6"></a>: Learning representations by maximizing mutual information across views. NIPS2019<br>[7]<a name="7"></a>: CPC: Data-efficient image recognition with contrastive predictive coding, arXiv2019<br>[8]<a name="8"></a>: Unsupervised feature learning via non-parametric instance discrimination. CVPR2018<br>[9]<a name="9"></a>: Contrastive multiview coding. arXiv2019<br>[10]<a name="10"></a>: MoCo: Momentum contrast for unsupervised visual representation learning, arXiv2019<br>[11]<a name="11"></a>: Self-supervised learning of pretext-invariant representations. arXiv2019   </p>
</div><div class="article-licensing box"><div class="licensing-title"><p>论文笔记-image-based contrastive learning</p><p><a href="http://www.panxiaoxie.cn/2021/05/05/论文笔记-contrastive-learning/">http://www.panxiaoxie.cn/2021/05/05/论文笔记-contrastive-learning/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-05-05</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-08-01</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/contrastive-learning/">contrastive learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/07/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-constrast-learning-in-NLP/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">论文笔记-constrast learning in NLP</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/"><span class="level-item">论文笔记-video transformer</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/';
            this.page.identifier = '2021/05/05/论文笔记-contrastive-learning/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>