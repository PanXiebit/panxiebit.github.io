<!doctype html>
<html lang="de"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="潘小榭"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘小榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="潘小榭"><meta property="og:url" content="http://www.panxiaoxie.cn/"><meta property="og:site_name" content="潘小榭"><meta property="og:image" content="http://www.panxiaoxie.cn/img/og_image.png"><meta property="article:author" content="Xie Pan"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/logo.svg"}},"description":null}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-28T03:00:35.000Z" title="2019/2/28 上午11:00:35">2019-02-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-29T08:12:09.141Z" title="2021/6/29 下午4:12:09">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/transfer-learning/">transfer learning</a></span><span class="level-item">23 minutes read (About 3449 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/02/28/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-0-NLP%20classification%20with%20transfer%20learning%20and%20weak%20supervision/">迁移学习系列-0-NLP classification with transfer learning and weak supervision</a></h1><div class="content"><p>paper:  </p>
<ul>
<li><p>Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision  </p>
</li>
<li><p>A Brief Introduction to Weakly Supervised Learning</p>
</li>
</ul>
<h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>现在的 state-of-the-art 技术都严重依赖于大量的数据，可以说数据是 NLP 应用的瓶颈(bottleneck)。比如，标注医学领域的电子健康记录需要大量的医学专业知识。</p>
<p>随着 transfer learning, multi-task learning 以及 weak supervision 的发展，NLP 可以尝试着去解决这些问题。</p>
<p>这里将介绍如何在没有公开数据集的情况下，使用少量的数据，来构建一个 dectect anti-smitic tweets 分类器。分为以下 3 个步骤：  </p>
<ul>
<li><p>Collect a small number of labeled examples (~600)  </p>
</li>
<li><p>Use weak supervision to build a training set from many unlabeled examples using weak supervision  </p>
</li>
<li><p>Use a large pre-trained language model for transfer learning</p>
</li>
</ul>
<h2 id="Weak-Supervision"><a href="#Weak-Supervision" class="headerlink" title="Weak Supervision"></a>Weak Supervision</h2><p>何为弱监督学习？</p>
<p><a target="_blank" rel="noopener" href="https://academic.oup.com/nsr/article/5/1/44/4093912">paper: A Brief Introduction to Weakly Supervised Learning</a>  </p>
<p><a target="_blank" rel="noopener" href="http://www.zhuanzhi.ai/document/ef293016d9acc5fd8b6c96e0d3d9b122">翻译版</a></p>
<p>弱监督学习是一个总括性的术语，它涵盖了试图通过较弱的监督来构建预测模型的各种研究。弱监督通常分为三种类型。  </p>
<ul>
<li><p>不完全监督(Incomplete Supervision): 只有训练数据集的一个（通常很小的）子集有标签，其它数据则没有标签。  </p>
</li>
<li><p>不确切监督(inexact supervision): 只有粗粒度的标签。以图像分类任务为例。我们希望图片中的每个物体都被标注；然而我们只有图片级的标签而没有物体级的标签。  </p>
</li>
<li><p>不准确监督(inaccurate supervision)，即给定的标签并不总是真值。</p>
</li>
</ul>
<p><img src="/2019/02/28/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-0-NLP%20classification%20with%20transfer%20learning%20and%20weak%20supervision/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-NLP-classification-with-transfer-learning-and-weak-supervision%5C01.png"></p>
<p>图1:三种弱监督学习的示意图。长方形表示特征向量；红色或蓝色表示标签；“？”表示标注可能是不准确的。中间的子图表示了几种弱监督的混合情形</p>
<h3 id="不完全监督"><a href="#不完全监督" class="headerlink" title="不完全监督"></a>不完全监督</h3><p>可以形式化为：$D={(x_1,y_1),…,(x_l,y_l),x_{l+1},…, x_m}$  即有 $l$ 个数据有标签（如 $y_i$ 所示），$u = m-l$ 个数据没有标签。</p>
<p>解决这类问题有两种技术：  </p>
<ul>
<li><p>主动学习(active learning), 也就是有个专家来标注 unlabeled 数据.</p>
</li>
<li><p>半监督学习(semi-supervision), 有一种特殊的半监督学习，叫 transductive learning(传导式学习)，它与（纯）半监督学习之间的差别在于，对测试数据（训练模型要预测的数据）的假设不同。传导式学习持有“封闭世界”的假设，即测试数据是事先给定的，且目标就是优化模型在测试数据上的性能；换句话说，未标注数据就是测试数据。纯半监督学习持有“开放世界”的假设，即测试数据是未知的，且未标注数据不一定是测试数据。实际中，绝大多数情况都是纯半监督学习。</p>
</li>
</ul>
<p><img src="/2019/02/28/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-0-NLP%20classification%20with%20transfer%20learning%20and%20weak%20supervision/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-NLP-classification-with-transfer-learning-and-weak-supervision%5C02.png"></p>
<h4 id="有人为干预"><a href="#有人为干预" class="headerlink" title="有人为干预"></a>有人为干预</h4><p>主动学习 active learning.</p>
<h4 id="无人为干预"><a href="#无人为干预" class="headerlink" title="无人为干预"></a>无人为干预</h4><p>半监督学习[3-5]是指在不询问人类专家的条件下挖掘未标注数据。为什么未标注数据对于构建预测模型也会有用？做一个简单的解释[19]，假设数据来自一个由 n 个高斯分布混合的 <a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/">高斯混合模型(参考以前的笔记)</a>，也就是说：</p>
<p>$$f(x | \theta) = \sum_{j=1}^n \alpha_j f(x | \theta_j)\quad\text{(1)}$$</p>
<p>其中 $\alpha_j$ 为混合系数，$\sum_{j=1}^n \alpha_j = 1$ 并且 $\theta = {\theta_j}$ 是模型参数。在这种情况下，标签 $y_i$ 可以看作一个随机变量，其分布 $P(y_i | x_i, g_i)$ 由混合成分 $g_i$ 和特征向量 $x_i$ 决定。最大化后验概率有：</p>
<p>$$h(x) = {argmax}<em>c \sum</em>{j=1}^n P(y_i = c | g_i = j, x_i) \times P(g_i = j | x_i)\quad(2)$$</p>
<p>其中：$P(g_i = j | x_i) = \dfrac{\alpha_j f(x_i | \theta_j)}  {\sum_{k=1}^n \alpha_k f(x_i | \theta_k)}\quad(3)$</p>
<p>$h(x)$ 可以通过用训练数据估计 $P(y_i = c | g_i = j, x_i)$ 和 $P(g_i = j | x_i)$ 来求得。很明显只有第一项需要需要标签信息。因此，未标注数据可以用来估计提升对第二项的估计，从而提升学习模型的性能。</p>
<p><img src="/2019/02/28/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-0-NLP%20classification%20with%20transfer%20learning%20and%20weak%20supervision/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-NLP-classification-with-transfer-learning-and-weak-supervision%5C03.png"></p>
<p>图中 $+,-$ 表示标注样本。而测试样本 $\bigcirc$ 正好在两者中间。</p>
<p>图3给出了一个直观的解释。如果我们只能根据唯一的正负样本点来预测，那我们就只能随机猜测，因为测试样本恰好落在了两个标注样本的中间位置；如果我们能够观测到一些未标注数据，例如图中的灰色样本点，我们就能以较高的置信度判定测试样本为正样本。在此处，尽管未标注样本没有明确的标签信息，它们却隐晦地包含了一些数据分布的信息，而这对于预测模型是有用的。</p>
<p>实际上，在半监督学习中有两个基本假设，即聚类假设（cluster assumption）和流形假设（manifold assumption）；两个假设都是关于数据分布的。前者假设数据具有内在的聚类结构，因此，落入同一个聚类的样本类别相同。后者假设数据分布在一个流形上，因此，相近的样本具有相似的预测。两个假设的本质都是相似的数据输入应该有相似的输出，而未标注数据有助于揭示出样本点之间的相似性</p>
<p>半监督学习有四种主要方法，即生成式方法（generative methods），基于图的方法（graph-based methods），低密度分割法（low-density separation methods）以及基于分歧的方法（disagreement methods）。</p>
<p><strong>生成式方法</strong>[19，20]假设标注数据和未标注数据都由一个固有的模型生成。因此，未标注数据的标签可以看作是模型参数的缺失，并可以通过EM算法（期望-最大化算法）等方法进行估计[21]。这类方法随着为拟合数据而选用的不同生成模型而有所差别。为了达到好的性能，通常需要相关领域的知识来选择合适的生成模型。也有一些将生成模型和判别模型的优点结合起来的尝试[22]。</p>
<p>基于图的方法构建一个图，其节点对应训练样本，其边对应样本之间的关系（通常是某种相似度或距离），而后依据某些准则将标注信息在图上进行扩散；例如标签可以在最小分割图算法得到的不同子图内传播[23]。很明显，模型的性能取决于图是如何构建的[26-28]。值得注意的是，对于m个样本点，这种方法通畅需要O(m^2)存储空间和O(m^3)计算时间复杂度。因此，这种方法严重受制于问题的规模；而且由于难以在不重建图的情况下增加新的节点，所以这种方法天生难以迁移。</p>
<p>基于分歧的方法[5，32，33]生成多个学习器，并让它们合作来挖掘未标注数据，其中不同学习器之间的分歧是让学习过程持续进行的关键。最为著名的典型方法——联合训练（co-traing），通过从两个不同的特征集合（或视角）训练得到的两个学习器来运作。在每个循环中，每个学习器选择其预测置信度最高的未标注样本，并将其预测作为样本的伪标签来训练另一个学习器。这种方法可以通过学习器集成来得到很大提升[34，35]。值得注意的是，基于分歧的方法提供了一种将半监督学习和主动学习自然地结合在一起的方式：它不仅可以让学习器相互学习，对于两个模型都不太确定或者都很确定但相互矛盾的未标注样本，还可以被选定询问“先知”。</p>
<h3 id="不确切监督"><a href="#不确切监督" class="headerlink" title="不确切监督"></a>不确切监督</h3><p>不确切监督是指在某种情况下，我们有一些监督信息，但是并不像我们所期望的那样精确。一个典型的情况是我们只有粗粒度的标注信息。例如，在药物活性预测中[40]，目标是建立一个模型学习已知分子的知识，来预测一种新的分子是否能够用于某种特殊药物的制造。一种分子可能有很多低能量的形态，这种分子能否用于制作该药物取决于这种分子是否有一些特殊形态。然而，即使对于已知的分子，人类专家也只知道其是否合格，而并不知道哪种特定形态是决定性的。</p>
<p>形式化表达为，这一任务是学习 $f: X\rightarrow Y$ ，其训练集为 $D = {(X_1, y_1), …, (X_m, y_m)}$，其中 $X_i = {x_{I, 1}, …, x_{I, m_i}}$, $X_i$ 属于X，且被称为一个包（bag），$x_{i, j}$ 属于 X，是一个样本（j属于 ${1, …, m_i}）$。$m_i$ 是 $X_i$ 中的样本个数，$y_i$ 属于 $Y = {Y, N}$。当存在 $x_{i, p}$ 是正样本时，$X_i$ 就是一个正包（positive bag），其中p是未知的且 p 属于 ${1, …, m_i}$。模型的目标就是预测未知包的标签。这被称为多示例学习（multi-instance learning）[40，41]</p>
<p>多示例学习已经成功应用于多种任务，例如图像分类、检索、注释[48-50]，文本分类[51，52]，垃圾邮件检测[53]，医疗诊断[54]，人脸、目标检测[55，56]，目标类别发现[57]，目标跟踪[58]等等。在这些任务中，我们可以很自然地将一个真实的目标（例如一张图片或一个文本文档）看作一个包；然而，不同于药物活性预测中包里有天然的示例（即分子的不同形态），这里的示例需要生成。一个包生成器明确如何生成示例来组成一个包。通常情况下，从一幅图像中提取的很多小图像块就作为可以这个图像的示例，而章节、段落甚至是句子可以作为一个文本文档的示例。尽管包生成器对于学习效果有重要的影响，但直到最近才出现关于图像包生成器的全面研究[59]；研究表明一些简单的密集取样包生成器要比复杂的生成器性能更好。图5显示了两个简单而有效的图像包生成器。</p>
<p><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume14/li13a/li13a.pdf">51.Convex and Scalable Weakly Labeled SVMs</a></p>
<p>[52.Towards making unlabeled data</p>
<p>never hurt](<a target="_blank" rel="noopener" href="http://www.icml-2011.org/papers/548_icmlpaper.pdf">http://www.icml-2011.org/papers/548_icmlpaper.pdf</a>)</p>
<h3 id="不准确监督"><a href="#不准确监督" class="headerlink" title="不准确监督"></a>不准确监督</h3><p>不准确监督关注监督信息不总是真值的情形；换句话说，有些标签信息可能是错误的。其形式化表示与概述结尾部分几乎完全相同，除了训练数据集中的y_i可能是错误的。</p>
<p>一个最近出现的不准确监督的情景发生在众包模式中(crowdsourcing)[74],即一个将工作外包给个人的流行模式。</p>
<p>在带有真值标签的大量训练样本的强监督条件下，监督学习技术已经取得了巨大的成功。然而，在真实的任务中，收集监督信息往往代价高昂，因此探索弱监督学习通常是更好的方式。</p>
<h2 id="Snorkel"><a href="#Snorkel" class="headerlink" title="Snorkel"></a>Snorkel</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.10160">paper:Snorkel: Rapid Training Data Creation with Weak Supervision</a></p>
<h3 id="motivation-1"><a href="#motivation-1" class="headerlink" title="motivation"></a>motivation</h3><p>deep learning 需要大量的标注数据。这对于一些大公司尚且能够雇佣标注人员，而很多小公司则将目标转向弱监督学习。尤其是，当数据的标注需要领域专家时(subject matter experts (SMEs))，标注数据变得更加困难。</p>
<p>弱监督学习包括一下形式：  </p>
<ul>
<li><p>distant supervision:  the records of an external knowledge base are heuristically aligned with data points to produce noisy labels [4,7,32]  </p>
</li>
<li><p>crowsourced labels[37,50]  </p>
</li>
<li><p>rules and heuristics for labeling data[39,52]  </p>
</li>
</ul>
<blockquote>
<p>Snorkel learns the accuracies of weak supervision sources withoust access to</p>
</blockquote>
<p>ground truth using a generative model [38]. Furthermore, it also learns correlations and other statistical dependencies among sources, correcting for dependencies in labeling functions that skew the estimated accuracies [5]  </p>
<p>Snorkel 生成训练数据的方法来自于作者的另外一篇论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1605.07723.pdf">Data programming: Creating large training sets, quickly, NIPS 2016</a>,不仅能给出弱监督得到的样本的置信度，还能学习得到样本之间的相关性和统计依赖。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-21T11:59:39.000Z" title="2019/2/21 下午7:59:39">2019-02-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-01-27T08:44:32.990Z" title="2021/1/27 下午4:44:32">2021-01-27</time></span><span class="level-item">a minute read (About 211 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/02/21/docker%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BD%BF%E7%94%A8/">docker学习和使用</a></h1><div class="content"><h3 id="docker-安装"><a href="#docker-安装" class="headerlink" title="docker 安装"></a>docker 安装</h3><h3 id="docker-tensorflow"><a href="#docker-tensorflow" class="headerlink" title="docker tensorflow"></a>docker tensorflow</h3><h3 id="docker-镜像迁移"><a href="#docker-镜像迁移" class="headerlink" title="docker 镜像迁移"></a>docker 镜像迁移</h3><p>有时候需要将镜像从一台服务器移动到另外一台服务器。可使用 nc 进行大文件传输</p>
<h3 id="docker-文件映射"><a href="#docker-文件映射" class="headerlink" title="docker 文件映射"></a>docker 文件映射</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sudo docker run -itd -p <span class="number">8888</span>:<span class="number">8888</span>  -v container_path:/usr/home/wuwei7/tmp_data   image_id /<span class="built_in">bin</span>/bash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker <span class="built_in">exec</span> -it container_id /<span class="built_in">bin</span>/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>-v 参数表示容器内文件与宿主机器文件映射</p>
<h3 id="docker-驻守状态"><a href="#docker-驻守状态" class="headerlink" title="docker 驻守状态"></a>docker 驻守状态</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sudo docker run -itd -p <span class="number">8888</span>:<span class="number">8888</span>  -v container_path:/usr/home/wuwei7/tmp_data   image_id /<span class="built_in">bin</span>/bash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker <span class="built_in">exec</span> -it container_id /<span class="built_in">bin</span>/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>-itd 表示驻守状态</p>
<p>exec 退出之后容器依然在运行</p>
<h3 id="docker-常用命令"><a href="#docker-常用命令" class="headerlink" title="docker 常用命令"></a>docker 常用命令</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sudo docker ps 查看正在运行的容器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker images  查看镜像</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker rm -rf Container_ID 删除正在运行的容器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker rmi -f Image_ID  删除镜像，-f 强制删除在容器中运行的镜像</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T08:41:32.000Z" title="2019/2/20 下午4:41:32">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-29T08:12:09.158Z" title="2021/6/29 下午4:12:09">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/">sentence embedding</a></span><span class="level-item">7 minutes read (About 985 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/02/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sentence-embedding/">论文笔记-sentence embedding</a></h1><div class="content"><p>句子的向量表示方法主要分为两类。 一类是通过无监督的方法得到 universal sentence embedding, 另一类是基于特定的任务，有标签的情况下，通过有监督学习得到 sentence embedding.</p>
<h2 id="supervised-learning"><a href="#supervised-learning" class="headerlink" title="supervised learning"></a>supervised learning</h2><h3 id="a-structured-self-attentive-sentence-embedding"><a href="#a-structured-self-attentive-sentence-embedding" class="headerlink" title="a structured self-attentive sentence embedding"></a>a structured self-attentive sentence embedding</h3><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03130">A Structured Self-attentive Sentence Embedding, ICLR 2017</a></p>
<p>传统的基于 RNN/LSTM 得到 sentence 的向量表示的方法通常是利用隐藏状态的最后一个状态，或者是所有时间步的隐藏状态，然后采用 sum/average/max pooling 的的方式得到 sentence embedding.</p>
<p>本文使用的方法就是在 LSTM 上加上一层 attention，通过注意力机制自动选择 sentence 中的某些方面，也就是赋予 sentence 中的每一个词一个权重，然后加权求和得到一个 vector. 本文另一个创新点在于，不仅仅得到一个 vector，而是一个 matrix，用来表示一个 sentence 中的不同方面。</p>
<h4 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture:"></a>Model Architecture:</h4><p><img src="/2019/02/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sentence-embedding/01.png"></p>
<p>模型也很简单，输入 sentence n tokens.</p>
<p>word embedding: $S\in R^{n\times d}$, d 表示词向量维度  </p>
<p>$$S=(w_1,w_2,…,w_n)$$</p>
<p>bidirection-LSTM: $H\in R^{n\times 2u}$, u 表示隐藏状态维度</p>
<p>$$H=(h_1,h_2,…,h_n)$$</p>
<p>single self-attention: $a\in R^n$, 表示 sentence 中对应位置的权重。与 encoder 之后的 sentence 加权求和得到 attention vector $m\in R^{2u}$.</p>
<p>$$a=softmax(w_{s2}tanh(W_{s1}H^T))$$</p>
<p>r-dim self-attention：有 r 个上述的 attention vector，并转换成矩阵形式，$A\in R^{n\times r}$ 与 encode 之后的句子表示 H 加权求和得到 embedding matrix $M\in R^{r\times 2u}$</p>
<p>$$A=softmax(W_{s2}tanh(W_{s1}H^T))$$</p>
<p>$$M=AH$$</p>
<h4 id="penalization-term"><a href="#penalization-term" class="headerlink" title="penalization term"></a>penalization term</h4><p>上面模型中使用 r 个 attention，很可能会出现冗余的情况，也就是得到的 r 个 attention vector( 论文中说的是 summation weight vectors) 可能得到的是同一个东西，所以需要 diversity.</p>
<blockquote>
<p>The best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors. However, we found that not very stable in our case. We conjecture it is because we are maximizing a set of KL divergence (instead of minimizing only one, which is the usual case), we are optimizing the annotation matrix A to have a lot of sufficiently small or even zero values at different softmax output units, and these vast amount of zeros is making the training unstable. There is another feature that KL doesn’t provide but we want, which is, we want each individual row to focus on a single aspect of semantics, so we want the probability mass in the annotation softmax output to be more focused. but with KL penalty we cant encourage that.</p>
</blockquote>
<p>一个最直观的方法是 Kullback Leibler divergence，也就是相对熵。因为得到的 attention vector 是一个概率分布 (distribution ), 任意两个分布差异越大，对应的相对熵越大。但是作者实验发现这种方法不稳定，原因作者推测是这里需要最大化的是多个 KL 散度的集合，并且在优化 annotation matrix A 时，在不同的softmax输出单元上有很多足够小甚至零值，而这些大量的零点使得训练不稳定. 另一方面，KL 散度不能 focus on 语义中的单个方面。</p>
<p>针对上面这两点，作者提出了一个新的正则项：</p>
<p>$$P=||(AA^T-I)||^2_{F}$$</p>
<p>$AA^T$ 是协方差矩阵，对角线元素是同一向量的内积，非对角线元素不同向量的内积。将其作为惩罚项加到 original loss 上，期望得到的是不同 vector 内积越小越好（内积越小，差异越大），并且向量的模长越大越好（概率分布更集中于某一两个词）。</p>
<p>最终得到矩阵级别的句子向量表示。</p>
<h4 id="training"><a href="#training" class="headerlink" title="training"></a>training</h4><p>3 different datasets:  </p>
<ul>
<li><p>the Age dataset  </p>
</li>
<li><p>the Yelp dataset  </p>
</li>
<li><p>the Stanford Natural Language Inference (SNLI) Corpus</p>
</li>
</ul>
<p>根据不同的任务有监督的训练。</p>
<h2 id="unsupervised-learning"><a href="#unsupervised-learning" class="headerlink" title="unsupervised learning"></a>unsupervised learning</h2><h3 id="sent2vec"><a href="#sent2vec" class="headerlink" title="sent2vec"></a>sent2vec</h3><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.02507.pdf">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-17T12:19:57.000Z" title="2019/2/17 下午8:19:57">2019-02-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-01-27T08:44:33.864Z" title="2021/1/27 下午4:44:33">2021-01-27</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/">ESA</a></span><span class="level-item">11 minutes read (About 1601 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/02/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Explicit-Semantic-Analysis/">论文笔记-Explicit Semantic Analysis</a></h1><div class="content"><p>paper:   </p>
<ul>
<li><p><a href>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysi, IJCAI2008</a>  </p>
</li>
<li><p><a href>Wikipedia-based Semantic Interpretation for Natural Language Processing</a></p>
</li>
</ul>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>对于自然语言的语义表示，需要的大量的 common sense 和 world knowledge. 前人的研究使用统计的方法，例如 WordNet，仅仅只利用了有限的词典知识（lexicographic knowledge），并不能有效的利用语言本身背后的背景知识（ background knowledge）。</p>
<p>作者提出了 Explicit Semantic Analysis (ESA)，能够对文本或单词进行可解释性的细粒度的语义表示。</p>
<blockquote>
<p>Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts.  </p>
</blockquote>
<p>显示的使用 wiki 中的概念（concepts）来表示任意长度的 text.  </p>
<p>作者通过文本分类和计算自然语言的文本片段之间的相似度来验证 ESA 的有效性。由于语义表示使用的是 natural concepts，ESA 模型的可解释性非常强。</p>
<h3 id="传统的方法："><a href="#传统的方法：" class="headerlink" title="传统的方法："></a>传统的方法：</h3><ol>
<li>词袋模型: 将 text 看作是 unordered bags of words, 每一个单词看作是一维特征。但是这并不能解决 NLP 中的两个主要问题： 一词多义和同义词（polysemy and synonymy）。  </li>
</ol>
<ol start="2">
<li>隐语义分析：Latent Semantic Analysis (LSA)</li>
</ol>
<blockquote>
<p> LSA is a purely statistical technique, which leverages word co-occurrence information from a large unlabeled corpus of text. LSA does not use any explicit human-organized knowledge; rather, it “learns” its representation by applying Singular Value Decomposition (SVD) to the words-by-documents co-occurrence matrix. LSA is essentially a dimensionality reduction technique that identifies a number of most prominent dimensions in the data, which are assumed to correspond to “latent concepts”. Meanings of words and documents are then represented in the space defined by these concepts.  </p>
</blockquote>
<p>LSA 是一种纯粹的统计技术，它利用来自大量未标记文本语料库的单词共现信息。 LSA不使用任何明确的人类组织知识; 相反，它通过将奇异值分解（SVD）应用于逐个文档的共现矩阵来“学习”其表示。 <strong>LSA本质上是一种降维技术</strong>，它识别数据中的许多最突出的维度，假设它们对应于“潜在概念”。 然后，在这些概念定义的空间中表示单词和文档的含义。</p>
<ol start="3">
<li>词汇数据库，WordNet.</li>
</ol>
<blockquote>
<p>However, lexical resources offer little information about the different word senses, thus making word sense disambiguation nearly impossible to achieve.Another drawback of such approaches is that creation of lexical resources requires lexicographic expertise as well as a lot of time and effort, and consequently such resources cover only a small fragment of the language lexicon. Specifically, such resources contain few proper names, neologisms, slang, and domain-specific technical terms. Furthermore, these resources have strong lexical orientation in that they predominantly contain information about individual words, but little world knowledge in general.  </p>
</blockquote>
<p>词汇资源几乎没有提供关于不同词义的信息，因此几乎不可能实现词义消歧。这种方法的另一个缺点是词汇资源的创建需要词典专业知识以及大量的时间和精力，因此 资源只涵盖语言词典的一小部分。 具体而言，此类资源包含很少的专有名称，新词，俚语和特定于域的技术术语。 此外，这些资源具有强烈的词汇取向，因为它们主要包含关于单个单词的信息，但总体上缺乏世界知识。</p>
<h3 id="concept-定义"><a href="#concept-定义" class="headerlink" title="concept 定义"></a>concept 定义</h3><blockquote>
<p>Observe that an encyclopedia consists of a large collection of articles, each of which provides a comprehensive exposition focused on a single topic. Thus, we view an encyclopedia as a collection of concepts (corresponding to articles), each accompanied with a large body of text (the article contents).  </p>
</blockquote>
<p>维基百科中每一个词条对应一个 concept.</p>
<p>example:  </p>
<p>对于文本：”Bernanke takes charge”  通过算法我们可以找到维基百科中相关的 concept:  </p>
<p>Ben Bernanke, Federal Reserve, Chairman of the Federal Reserve, Alan Greenspan (Bernanke’s predecessor), Monetarism (an economic theory of money supply and central banking), inflation and deflation.</p>
<p>对于文本：”Apple patents a Tablet Mac”  </p>
<p>相关的 concept:  Apple Computer 2 , Mac OS (the Macintosh operating system) Laptop (the general name for portable computers, of which Tablet Mac is a specific example), Aqua (the GUI of Mac OS X), iPod (another prominent product by Apple), and Apple Newton (the name of Apple’s early personal digital assistant).</p>
<p>ESA 对一个 texts 的表示是 wiki 中所有的 concept 的 weighted combination，这里为了展示方便，只列举了最相关的一些 concept.</p>
<h2 id="ESA-explicit-semantic-analysis"><a href="#ESA-explicit-semantic-analysis" class="headerlink" title="ESA(explicit semantic analysis)"></a>ESA(explicit semantic analysis)</h2><p>通过 wiki 得到一个 basic concepts: $C_1, C_2,…, C_n$, 其中 $C_k$ 都是来源于 wiki. 表示一个通用的 n 维语义空间。</p>
<p>然后将任意长度的文本 t 表示成与上述向量长度相同的 权重向量 $w_1, w_2,…, w_n$ 分别表示 t 与 $C_k$ 之间的相关程度。</p>
<p>接下来两个步骤就是：  </p>
<ol>
<li><p>the set of basic concepts  </p>
</li>
<li><p>the algorithm that maps text fragments into interpretation vectors</p>
</li>
</ol>
<h2 id="如何构建-concept-集合"><a href="#如何构建-concept-集合" class="headerlink" title="如何构建 concept 集合"></a>如何构建 concept 集合</h2><p>1.using Wikipedia as a Repository of Basic Concepts  </p>
<p>维基百科词条中的内容也很关键，用来计算 concept 与输入文本中单词的相似度。</p>
<p>2.building a semantic interpreter</p>
<p>根据 wiki 得到基本的 concept，以及对应的文档， $d_1,..,d_n$. 构建一个 sparse 表格 T， 其中，列表示 concept，行表示文档中的单词对应的 TDIDF 值。也就是计算文档中的单词与所有文档 $\bigcup_{i=1..n}d_i$ 的频率关系。</p>
<p>$$T[i,j]=tf(t_i, d_j)\cdot log\dfrac{n}{df_i}$$</p>
<p>其中，Term Frequency - Inverse Document Frequency：</p>
<p>TF 表示在文档 $d_j$ 中，单词 $t_i$ 出现的频率。</p>
<p>$$tf(t_i, d_j)=\begin{cases}</p>
<p>1 + log\ count(t_i, d_j), &amp;\text{if count(t_i, d_j) &gt; 0} \</p>
<p>0, &amp;\text{otherwise}</p>
<p>\end{cases}$$</p>
<p>IDF 表示逆文档频率。反应一个词在不同的文档中出现的频率越大，那么它的 IDF 值应该低，比如介词“to”。而反过来如果一个词在比较少的文本中出现，那么它的 IDF 值应该高。</p>
<p>$$IDF=log\dfrac{n}{df_i}$$</p>
<p>$df_i=|{d_k:t_i\in d_k}|$ 表示出现该单词的文档个数，n 表示总的文档个数。</p>
<p>正则化，cosine normalization:  </p>
<p>$$T[i,j]\leftarrow \dfrac{T[i,j]}{\sqrt{\sum_{l=1}^r T[i,j]^2}}$$</p>
<p>r 表示单词的总量。也就是除以所有单词对应的向量二范数之和平方。</p>
<p>得到 table T 之后，一个单词的向量 $t_i$ 表示就是第 i 行。一个文本片段 $&lt;t_1,..,t_k&gt;$ 的向量表示是文本中所有单词的质心。</p>
<h2 id="如何将文本片段映射成向量表示"><a href="#如何将文本片段映射成向量表示" class="headerlink" title="如何将文本片段映射成向量表示"></a>如何将文本片段映射成向量表示</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-07T02:08:35.000Z" title="2019/2/7 上午10:08:35">2019-02-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a></span><span class="level-item">a few seconds read (About 78 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/02/07/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%955-%E6%A0%91/">邓公数据结构与算法5-树</a></h1><div class="content"><h2 id="树"><a href="#树" class="headerlink" title="树"></a>树</h2><p>树：  </p>
<ul>
<li><p>无环连通图  </p>
</li>
<li><p>极小连通图  </p>
</li>
<li><p>极大无环图</p>
</li>
</ul>
<p>任一节点 v 与根节点 r 存在唯一的路径  </p>
<p>path(v, r) = path( r)</p>
<p><img src="/2019/02/07/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%955-%E6%A0%91/01.png"></p>
<h2 id="树的表示"><a href="#树的表示" class="headerlink" title="树的表示"></a>树的表示</h2><h2 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h2><h2 id="二叉树的实现"><a href="#二叉树的实现" class="headerlink" title="二叉树的实现"></a>二叉树的实现</h2><h3 id="先序遍历"><a href="#先序遍历" class="headerlink" title="先序遍历"></a>先序遍历</h3><h3 id="中序遍历"><a href="#中序遍历" class="headerlink" title="中序遍历"></a>中序遍历</h3><h3 id="层次遍历"><a href="#层次遍历" class="headerlink" title="层次遍历"></a>层次遍历</h3><h3 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h3></div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/5/">Previous</a></div><div class="pagination-next"><a href="/page/7/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/5/">5</a></li><li><a class="pagination-link is-current" href="/page/6/">6</a></li><li><a class="pagination-link" href="/page/7/">7</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/23/">23</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="潘小榭"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">潘小榭</p><p class="is-size-6 is-block">Blogging is happier than writing essays!</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">111</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-05T02:03:16.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-image-based contrastive learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:07:08.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:05:10.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-16T07:48:48.000Z">2021-04-16</time></p><p class="title"><a href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-12T08:16:35.000Z">2021-04-12</time></p><p class="title"><a href="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/">论文笔记-DETR and Deformable DETR</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">October 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">November 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESA/"><span class="tag">ESA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN%EF%BC%8CRL/"><span class="tag">GAN，RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MRC-and-QA/"><span class="tag">MRC and QA</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Translation/"><span class="tag">Machine Translation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/capsules/"><span class="tag">capsules</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-augmentation/"><span class="tag">data augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/open-set-recognition/"><span class="tag">open set recognition</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentence-embedding/"><span class="tag">sentence embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentiment-classification/"><span class="tag">sentiment classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language-recognition/"><span class="tag">sign language recognition</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-matching/"><span class="tag">text matching</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/transfer-learning/"><span class="tag">transfer learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="tag">数据结构与算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag">8</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("default");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>