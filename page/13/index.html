<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-12-06T01:10:46.000Z" title="2018/12/6 上午9:10:46">2018-12-06</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">14 分钟读完 (大约2150个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/">论文笔记-CoQA</a></h1><div class="content"><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.07042.pdf">CoQA: A Conversational Question Answering Challenge</a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>We introduce CoQA, a novel dataset for building Conversational Question Answering systems.1 Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains.  </p>
</blockquote>
<p>CoQA, 对话式阅读理解数据集。从 7 个不同领域的 8k 对话中获取的 127k 问答对。</p>
<blockquote>
<p>The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage.  </p>
</blockquote>
<blockquote>
<p>We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.  </p>
</blockquote>
<p>CoQA 跟传统的 RC 数据集所面临的挑战不一样，主要是指代和推理。</p>
<blockquote>
<p>We ask other people a question to either seek or test their knowledge about a subject. Depending on their answer, we follow up with another question and their answer builds on what has already been discussed. This incremental aspect makes human conversations succinct. An inability to build up and maintain common ground in this way is part of why virtual assistants usually don’t seem like competent conversational partners.  </p>
</blockquote>
<p>我们问其他人一个问题，来寻求或者测试他们对某一个主题的知识。然后依赖于他的答案，我们提出一个新的问题，他根据刚才我们讨论的来回答这个新的问题。  </p>
<p>这使得对话变得很简短。而正是这种建立和维持共同点的能力缺失，使得虚拟助手看起来并不是一个有能力的对话者。  </p>
<p>而 CoQA 就是要测试这种能力。</p>
<p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/01.png"></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote>
<p>In CoQA, a machine has to understand a text passage and answer a series of questions that appear in a conversation. We develop CoQA with three main goals in mind.  </p>
</blockquote>
<p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/02.png"></p>
<blockquote>
<p>The first concerns the nature of questions in a human conversation. Posing short questions is an effective human conversation strategy, but such questions are a pain in the neck for machines.  </p>
</blockquote>
<p>第一点：人类在对话时，会提出很简短的问题，但这对于机器来说却很难。比如 Q5 “Who?”</p>
<blockquote>
<p>The second goal of CoQA is to ensure the naturalness of answers in a conversation. Many existing QA datasets restrict answers to a contiguous span in a given passage, also known as extractive answers (Table 1). Such answers are not always natural, for example, there is no extractive answer for Q4 (How many?) in Figure 1. In CoQA, we propose that the answers can be free-form text (abstractive answers), while the extractive spans act as rationales for the actual answers. Therefore, the answer for Q4 is simply Three while its rationale is spanned across multiple sentences.  </p>
</blockquote>
<p>第二点：答案不是抽取式的 extractive，而是总结性的 abstractive, free-from text. 比如 Q4.  好难啊！！！</p>
<blockquote>
<p>The third goal of CoQA is to enable building QA systems that perform robustly across domains. The current QA datasets mainly focus on a single domain which makes it hard to test the generalization ability of existing models.  </p>
</blockquote>
<p>第三点：数据来自多种 domain，提高泛化性。</p>
<h2 id="Dataset-collection"><a href="#Dataset-collection" class="headerlink" title="Dataset collection"></a>Dataset collection</h2><p>数据集具体详情：</p>
<ol>
<li>It consists of 127k conversation turns collected from 8k conversations over text passages (approximately one conversation per</li>
</ol>
<p>passage). The average conversation length is 15 turns, and each turn consists of a question and an answer.</p>
<ol start="2">
<li>It contains free-form answers. Each answer has an extractive rationale highlighted in the passage.</li>
</ol>
<ol start="3">
<li>Its text passages are collected from seven diverse domains — five are used for in-domain evaluation and two are used for out-of-domain</li>
</ol>
<p>evaluation.</p>
<blockquote>
<p>Almost half of CoQA questions refer back to conversational history using coreferences, and a large portion requires pragmatic reasoning making it challenging for models that rely on lexical cues alone.  </p>
</blockquote>
<p>大部分涉及到对话历史的问题都用到了指代和逻辑推理，这对于仅仅是依赖于词汇提示（语义匹配）的模型来说会很难。</p>
<blockquote>
<p>The best-performing system, a reading comprehension model that predicts extractive rationales which are further fed into a sequence-to-sequence model that generates final answers, achieves a F1 score of 65.1%. In contrast, humans achieve 88.8% F1, a superiority of 23.7% F1, indicating that there is a lot of headroom for improvement.  </p>
</blockquote>
<p>Baseline 是将抽取式阅读理解模型转换成 seq2seq 形式，然后从 rationale 中获取答案，最终得到了 65.1% 的 F1 值。</p>
<h3 id="question-and-answer-collection"><a href="#question-and-answer-collection" class="headerlink" title="question and answer collection"></a>question and answer collection</h3><blockquote>
<p>We want questioners to avoid using exact words in the passage in order to increase lexical diversity. When they type a word that is already present in the passage, we alert them to paraphrase the question if possible.    </p>
</blockquote>
<p>questioner 提出的问题应尽可能避免使用出现在 passage 中的词，这样可以增加词汇的多样性。</p>
<blockquote>
<p>For the answers, we want answerers to stick to the vocabulary in the passage in order to limit the number of possible answers. We encourage this by automatically copying the highlighted text into the answer box and allowing them to edit copied text in order to generate a natural answer. We found 78% of the answers have at least one edit such as changing a word’s case or adding a punctuation.  </p>
</blockquote>
<p>对于答案呢，尽可能的使用 passage 中出现的词，从而限制出现很多中答案的可能性。作者通过复制 highlighted text(也就是 rationale 吧) 到 answer box，然后让 answerer 去生成相应的 answer. 其中 78% 的答案是需要一个编辑距离，比如一个词的大小写或增加标点符号。</p>
<h3 id="passage-collection"><a href="#passage-collection" class="headerlink" title="passage collection"></a>passage collection</h3><blockquote>
<p>Not all passages in these domains are equally good for generating interesting conversations. A passage with just one entity often result in questions that entirely focus on that entity. Therefore, we select passages with multiple entities, events and pronominal references using Stanford CoreNLP (Manning et al., 2014). We truncate long articles to the first few paragraphs that result in around 200 words.  </p>
</blockquote>
<p>如果一个 passage 只有一个 entity，那么根据它生成的对话都会是围绕这个 entity 的。显然这不是这个数据集想要的。因此，作者使用 Stanford CoreNLP 来对 passage 进行分析后选择多个 entity 和 event 的 passage.</p>
<p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/03.png"></p>
<blockquote>
<p>Table 2 shows the distribution of domains. We reserve the Science and Reddit domains for out-ofdomain evaluation. For each in-domain dataset, we split the data such that there are 100 passages in the development set, 100 passages in the test set, and the rest in the training set. For each out-of-domain dataset, we just have 100 passages in the test set.  </p>
</blockquote>
<p>In domain 中包含 Children, Literature, Mid/HIgh school, News, Wikipedia. 他们分出 100 passage 到开发集(dev dataset), 其余的在训练集 (train dataset).  out-of-diomain 包含 Science Reddit ，分别有 100 passage 在开发集中。  </p>
<p>test dataset:</p>
<h3 id="Collection-multiple-answers"><a href="#Collection-multiple-answers" class="headerlink" title="Collection multiple answers"></a>Collection multiple answers</h3><p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/04.png"></p>
<blockquote>
<p>Some questions in CoQA may have multiple valid answers. For example, another answer for Q4 in Figure 2 is A Republican candidate. In order to</p>
</blockquote>
<p>account for answer variations, we collect three additional answers for all questions in the development and test data.  </p>
<p>一个问题可能出现多种回答，因此在dev dataset 和 test dataset 中有三个候选答案。  </p>
<blockquote>
<p>In the previous example, if the original answer was A Republican Candidate, then the following question Which party does he</p>
</blockquote>
<p>belong to? would not have occurred in the first place. When we show questions from an existing conversation to new answerers, it is likely they will deviate from the original answers which makes the conversation incoherent. It is thus important to bring them to a common ground with the original answer.  </p>
<p>比如上图中 Q4, 如果回答是 A Republican candidate. 但是整个对话是相关的，所以接下来的问题就会使整个对话显得混乱了。</p>
<blockquote>
<p>We achieve this by turning the answer collection task into a game of predicting original answers. First, we show a question to a new answerer, and when she answers it, we show the original answer and ask her to verify if her answer matches the original. For the next question, we ask her to guess the original answer and verify again. We repeat this process until the conversation is complete. In our pilot experiment, the human F1 score is increased by 5.4% when we use this verification setup.  </p>
</blockquote>
<p>因为机器在学习的时候是有 original answer 进行对比的，同样的这个过程在人工阶段也是需要的，可以减少上诉的混乱情况，answerer 在给出一个答案后，作者会告诉他们是否与 original 匹配，然后直到整个过程完成。</p>
<h2 id="Dataset-Analysis"><a href="#Dataset-Analysis" class="headerlink" title="Dataset Analysis"></a>Dataset Analysis</h2><p>What makes the CoQA dataset conversational compared to existing reading comprehension datasets like SQuAD? How does the conversation flow from one turn to the other? <strong>What linguistic phenomena do the questions in CoQA exhibit?</strong> We answer these questions below.</p>
<p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/05.png"></p>
<p>在 question 中：  </p>
<ol>
<li><p>指代词(he, him, she, it, they)出现的更为频繁， SQuAD 则几乎没有。</p>
</li>
<li><p>SQuAD 中 what 几乎占了一半，CoQA 中问题类型则更为多样， 比如 did, was, is, does 的频率很高。  </p>
</li>
<li><p>CoQA 的问题更加简短。见图 3.   </p>
</li>
<li><p>answer 有 33% 的是 abstractive. 考虑到人工因素，抽取式的 answer 显然更好写，所以这高于作者预期了。yes/no 的答案也有一定比重。</p>
</li>
</ol>
<p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/06.png"></p>
<h3 id="Conversation-Flow"><a href="#Conversation-Flow" class="headerlink" title="Conversation Flow"></a>Conversation Flow</h3><p>A coherent conversation must have smooth transitions between turns.  </p>
<p>一段好的对话是具有引导性的，不断深入挖掘 passage 的信息。</p>
<p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/07.png"></p>
<p>作者将 passage 均匀分成 10 chunks，然后分析随着对话 turn 的变化，其对应的 passage chunks 变化的情况。</p>
<h3 id="Linguistic-Phenomena"><a href="#Linguistic-Phenomena" class="headerlink" title="Linguistic Phenomena"></a>Linguistic Phenomena</h3><p><img src="/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/09.png"></p>
<p>Relationship between a question and its passage：  </p>
<ul>
<li><p>lexical match: question 和 passage 中至少有一个词是匹配的。  </p>
</li>
<li><p>Paraphrasing: 解释型。虽然 question 没有与 passage 的词，但是确实对 rationale 的一种解释，也就是换了一种说法，当作问题提出了。通常这里面包含： synonymy(同义词), antonymy(反义词), hypernymy(上位词), hyponymy(下位词) and negation(否定词).  </p>
</li>
<li><p>Pragmatics: 需要推理的。</p>
</li>
</ul>
<p>Relationship between a question and its conversation history：  </p>
<ul>
<li><p>No coref  </p>
</li>
<li><p>Explicit coref.  </p>
</li>
<li><p>Implicit coref.</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-12-05T01:04:59.000Z" title="2018/12/5 上午9:04:59">2018-12-05</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.363Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">8 分钟读完 (大约1267个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/12/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dropblock/">论文笔记-dropblock</a></h1><div class="content"><p>paper:  </p>
<ul>
<li><p>DropBlock: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.12890.pdf">DropBlock: A regularization method for convolutional networks</a>  </p>
</li>
<li><p>Variational Dropout：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.05287.pdf">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a>  </p>
</li>
<li><p>Zoneout：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.01305">Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations</a></p>
</li>
</ul>
<p>dropblock 是关于 CNN 的，后两篇是关于 RNN 的正则化。</p>
<h1 id="DropBlock"><a href="#DropBlock" class="headerlink" title="DropBlock"></a>DropBlock</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that</p>
</blockquote>
<p>activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout.  </p>
<p>通常深度神经网络在过参数化，并在训练时加上大量的噪声和正则化，比如权重衰减和 dropout，这个时候神经网络能很好的 work. 但是 dropout 对于全链接网络是一个非常有效的正则化技术，它对于卷积神经网络却没啥效果。这可能是因为卷积神经网络的激活是空间相关的，即使 drop 掉部分 unit，信息仍然会传递到下一层网络中去。</p>
<blockquote>
<p>Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices.  </p>
</blockquote>
<p>作者为卷积神经网络提出了专门的正则化方式， dropblock. 同时 drop 掉一个连续的空间。作者发现将 dropblock 应用到 ResNet 能有效的提高准确率。同时增加 drop 的概率能提高参数的鲁棒性。</p>
<blockquote>
<p>回顾了一下 skip/shortcut connection: 目的是避免梯度消失。可以直接看 GRU 的公式：<a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/">参考笔记</a></p>
</blockquote>
<h2 id="dropblock"><a href="#dropblock" class="headerlink" title="dropblock"></a>dropblock</h2><blockquote>
<p>In this paper, we introduce DropBlock, a structured form of dropout, that is particularly effective to regularize convolutional networks. In DropBlock, features in a block, i.e., a contiguous region of a feature map, are dropped together. As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data (see Figure 1).</p>
</blockquote>
<p><img src="/2018/12/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dropblock/01.png"></p>
<p>具体的算法很简单，主要关注两个参数的设置： block_size 和 $\gamma$.  </p>
<ul>
<li><p>block_size is the size of the block to be dropped  </p>
</li>
<li><p>$\gamma$ controls how many activation units <strong>to drop</strong>.</p>
</li>
</ul>
<blockquote>
<p>We experimented with a shared DropBlock mask across different feature channels or each feature channel has its DropBlock mask. Algorithm 1 corresponds to the latter, which tends to work better in our experiments.  </p>
</blockquote>
<p>对于 channels， 不同的 feature map 具有不同的 dropblock 相比所有的 channels 共享 dropblock 效果要好。</p>
<p><img src="/2018/12/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dropblock/02.png"></p>
<blockquote>
<p>Similar to dropout we do not apply DropBlock during inference. This is interpreted as evaluating an averaged prediction across the exponentially-sized ensemble of sub-networks. These sub-networks include a special subset of sub-networks covered by dropout where each network does not see contiguous parts of feature maps.  </p>
</blockquote>
<p>关于 infer 时， dropblock 的处理和 dropout 类似。</p>
<p><strong>block_size</strong>:  </p>
<blockquote>
<p>In our implementation, we set a constant block_size for all feature maps, regardless the resolution of feature map. DropBlock resembles dropout [1] when block_size = 1 and resembles SpatialDropout [20] when block_size covers the full feature map.  </p>
</blockquote>
<p>block_size 设置为 1 时, 类似于 dropout. 当 block_size 设置为整个 feature map 的 size 大小时，就类似于 SpatialDropout.</p>
<p><strong>setting the value of $\gamma$</strong>:  </p>
<blockquote>
<p>In practice, we do not explicitly set $\gamma$. As stated earlier, $\gamma$ controls the number of features to drop. Suppose that we want to keep every activation unit with the probability of keep_prob, in dropout [1] the binary mask will be sampled with the Bernoulli distribution with mean 1 − keep_prob. However, to account for the fact that every zero entry in the mask will be expanded by block_size2 and the blocks will be fully contained in feature map, we need to adjust $\gamma$ accordingly when we sample the initial binary mask. In our implementation, $\gamma$ can be computed as  </p>
</blockquote>
<p>作者并没有显示的设置 $\gamma$. 对于 dropout，每一个 unit 满足概率为 keep_prob 的 Bernoulli 分布，但是对于 dropblock, 需要考虑到 block_size 的大小，以及其与 feature map size 的比例大小。</p>
<p><img src="/2018/12/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dropblock/03.png"></p>
<ul>
<li><p>keep_prob 是传统的 dropout 的概率，通常设置为 0.75-0.9.  </p>
</li>
<li><p>feat_size 是整个 feature map 的 size 大小。  </p>
</li>
<li><p>(feat_size - block_size + 1) 是选择 dropblock 中心位置的有效区域。  </p>
</li>
</ul>
<blockquote>
<p>The main nuance of DropBlock is that there will be some overlapped in the dropped blocks, so the above equation is only an approximation.  </p>
</blockquote>
<p>最主要的问题是，会出现 block_size 的重叠。所以上诉公式也只是个近似。  </p>
<p><strong>Scheduled DropBlock:</strong>  </p>
<blockquote>
<p>We found that DropBlock with a fixed keep_prob during training does not work well. Applying small value of keep_prob hurts learning at the beginning. Instead, gradually decreasing keep_prob over time from 1 to the target value is more robust and adds improvement for the most values of keep_prob.  </p>
</blockquote>
<p>定制化的设置 keep_prob, 在网络初期丢失特征会降低 preformance, 所以刚开始设置为 1,然后逐渐减小到 target value.  </p>
<p>所以是随着网络深度加深而变化，还是随着迭代步数变化，应该是后者吧，类似于 scheduled learning rate.</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><blockquote>
<p>In the following experiments, we study where to apply DropBlock in residual networks. We experimented with applying DropBlock only after convolution layers or applying DropBlock after both convolution layers and skip connections. To study the performance of DropBlock applying to different feature groups, we experimented with applying DropBlock to Group 4 or to both Groups 3 and 4.  </p>
</blockquote>
<p>实验主要在讨论在哪儿加 dropblock 以及 如何在 channels 中加 dropblock。</p>
<h1 id="Variational-Dropout"><a href="#Variational-Dropout" class="headerlink" title="Variational Dropout"></a>Variational Dropout</h1></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-12-01T04:56:14.000Z" title="2018/12/1 下午12:56:14">2018-12-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/pytorch/">pytorch</a></span><span class="level-item">37 分钟读完 (大约5510个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/12/01/pytorch-book-1-Tensor/">pytorch-Tensor</a></h1><div class="content"><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>从接口的角度来讲，对tensor的操作可分为两类：</p>
<ol>
<li><p><code>torch.function</code>，如<code>torch.save</code>等。</p>
</li>
<li><p>另一类是<code>tensor.function</code>，如<code>tensor.view</code>等。</p>
</li>
</ol>
<p>而从存储的角度来讲，对tensor的操作又可分为两类：</p>
<ol>
<li><p>不会修改自身的数据，如 <code>a.add(b)</code>， 加法的结果会返回一个新的tensor。</p>
</li>
<li><p>会修改自身的数据，如 <code>a.add_(b)</code>， 加法的结果仍存储在a中，a被修改了。</p>
</li>
</ol>
<p>表3-1: 常见新建tensor的方法</p>
<p>|函数|功能|</p>
<p>|:—:|:—:|</p>
<p>|Tensor(*sizes)|基础构造函数|</p>
<p>|ones(*sizes)|全1Tensor|</p>
<p>|zeros(*sizes)|全0Tensor|</p>
<p>|eye(*sizes)|对角线为1，其他为0|</p>
<p>|arange(s,e,step|从s到e，步长为step|</p>
<p>|linspace(s,e,steps)|从s到e，均匀切分成steps份|</p>
<p>|rand/randn(*sizes)|均匀/标准分布|</p>
<p>|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|</p>
<p>|randperm(m)|随机排列|</p>
<p>其中使用<code>Tensor</code>函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor.</p>
<ul>
<li><p>b.tolist() 把 tensor 转为 list</p>
</li>
<li><p>b.numel() b 中元素总数，等价于 b.nelement()</p>
</li>
<li><p>torch.Tensor(b.size()) 创建和 b 一样的 tensor</p>
</li>
<li><p>除了<code>tensor.size()</code>，还可以利用<code>tensor.shape</code>直接查看tensor的形状，<code>tensor.shape</code>等价于<code>tensor.size()</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 用list的数据创建tensor</span></span><br><span class="line"></span><br><span class="line">b = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.tolist())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.numel())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个和b形状一样的tensor</span></span><br><span class="line"></span><br><span class="line">c = torch.Tensor(b.size())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个元素为2和3的tensor</span></span><br><span class="line"></span><br><span class="line">d = torch.Tensor((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 1.,  2.,  3.],

        [ 4.,  5.,  6.]])

[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]

6

tensor(1.00000e-15 *

       [[-3.4942,  0.0000,  0.0000],

        [ 0.0000,  0.0000,  0.0000]])

tensor([ 2.,  3.])
</code></pre>
<h3 id="常用Tensor操作"><a href="#常用Tensor操作" class="headerlink" title="常用Tensor操作"></a>常用Tensor操作</h3><p><code>view</code>, <code>squeeze</code>, <code>unsqueeze</code>, <code>resize</code></p>
<ul>
<li>通过<code>tensor.view</code>方法可以调整tensor的形状，但必须保证调整前后元素总数一致。<code>view</code>不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候<code>squeeze</code>和<code>unsqueeze</code>两个函数就派上用场了。  </li>
</ul>
<p><code>tensorflow</code> 里面是 <code>tf.expand_dim</code> 和 <code>tf.squeeze</code>.</p>
<ul>
<li><code>resize</code>是另一种可用来调整<code>size</code>的方法，但与<code>view</code>不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.,  1.,  2.],

        [ 3.,  4.,  5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b = a.view(-<span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 当某一维为-1的时候，会自动计算它的大小</span></span><br><span class="line"></span><br><span class="line">b</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.,  1.,  2.],

        [ 3.,  4.,  5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b.unsqueeze(<span class="number">1</span>) <span class="comment"># 注意形状，在第1维（下标从0开始）上增加“１”</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 0.,  1.,  2.]],



        [[ 3.,  4.,  5.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b.unsqueeze(-<span class="number">2</span>) <span class="comment"># -2表示倒数第二个维度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 0.,  1.,  2.]],



        [[ 3.,  4.,  5.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c = b.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">c.squeeze(<span class="number">0</span>) <span class="comment"># 压缩第0维的“１”</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[[ 0.,  1.,  2.],

          [ 3.,  4.,  5.]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c.squeeze() <span class="comment"># 把所有维度为“1”的压缩</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.,  1.,  2.],

        [ 3.,  4.,  5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">b <span class="comment"># a修改，b作为view之后的，也会跟着修改</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[   0.,  100.,    2.],

        [   3.,    4.,    5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b.resize_(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">b</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[   0.,  100.,    2.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b.resize_(<span class="number">3</span>, <span class="number">3</span>) <span class="comment"># 旧的数据依旧保存着，多出的大小会分配新空间</span></span><br><span class="line"></span><br><span class="line">b</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[   0.0000,  100.0000,    2.0000],

        [   3.0000,    4.0000,    5.0000],

        [  -0.0000,    0.0000,    0.0000]])
</code></pre>
<h3 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h3><p>Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。</p>
<p>其它常用的选择函数如表3-2所示。</p>
<p>表3-2常用的选择函数</p>
<p>函数|功能|</p>
<p>:—:|:—:|</p>
<p>index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列</p>
<p>masked_select(input, mask)|例子如上，a[a&gt;0]，使用ByteTensor进行选取</p>
<p>non_zero(input)|非0元素的下标</p>
<p>gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样</p>
<p><code>gather</code>是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">out[i][j] = <span class="built_in">input</span>[index[i][j]][j]  <span class="comment"># dim=0</span></span><br><span class="line"></span><br><span class="line">out[i][j] = <span class="built_in">input</span>[i][index[i][j]]  <span class="comment"># dim=1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>三维tensor的<code>gather</code>操作同理，下面举几个例子。</p>
<h4 id="index-select-input-dim-index-指定维度上选取某些行和列-返回的是某行和某列"><a href="#index-select-input-dim-index-指定维度上选取某些行和列-返回的是某行和某列" class="headerlink" title="index_select(input, dim, index) 指定维度上选取某些行和列, 返回的是某行和某列"></a>index_select(input, dim, index) 指定维度上选取某些行和列, 返回的是某行和某列</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>,<span class="number">1</span>]) <span class="comment"># 第 0 行， 第 1 列</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 0.5948, -0.5760,  1.3726, -0.9664],

        [ 0.5705,  1.0374, -1.1780,  0.0635],

        [-0.1195,  0.6657,  0.9583, -1.8952]])

tensor(-0.5760)
</code></pre>
<h5 id="返回行的四种方式"><a href="#返回行的四种方式" class="headerlink" title="返回行的四种方式"></a>返回行的四种方式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[torch.LongTensor([<span class="number">1</span>,<span class="number">2</span>])]) <span class="comment"># 第 0 行 和 第 1 行</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 0.5705,  1.0374, -1.1780,  0.0635],

        [-0.1195,  0.6657,  0.9583, -1.8952]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">index = torch.LongTensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">a.index_select(dim=<span class="number">0</span>, index=index)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 0.5705,  1.0374, -1.1780,  0.0635],

        [-0.1195,  0.6657,  0.9583, -1.8952]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a[<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># 只能是连续的行</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>  tensor([[ 0.5705,  1.0374, -1.1780,  0.0635],</p>
<pre><code>        [-0.1195,  0.6657,  0.9583, -1.8952]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[torch.LongTensor([[<span class="number">1</span>],[<span class="number">2</span>]])]) <span class="comment"># 还是第 0 行 和 第 1 行</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[[ 0.5705,  1.0374, -1.1780,  0.0635]],



        [[-0.1195,  0.6657,  0.9583, -1.8952]]])
</code></pre>
<h5 id="返回列的两种方式"><a href="#返回列的两种方式" class="headerlink" title="返回列的两种方式"></a>返回列的两种方式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.index_select(dim=<span class="number">1</span>, index=index)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.5760,  1.3726],

        [ 1.0374, -1.1780],

        [ 0.6657,  0.9583]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a[:, <span class="number">1</span>:<span class="number">3</span>] <span class="comment"># 连续的列</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.5760,  1.3726],

        [ 1.0374, -1.1780],

        [ 0.6657,  0.9583]])
</code></pre>
<h4 id="masked-selected-input-mask-使用-ByteTensor-进行选取"><a href="#masked-selected-input-mask-使用-ByteTensor-进行选取" class="headerlink" title="masked_selected(input, mask) 使用 ByteTensor 进行选取"></a>masked_selected(input, mask) 使用 ByteTensor 进行选取</h4><p>mask is ByteTensor, 类似于 a[a&gt;1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[a&gt;<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">a.masked_select(a&gt;<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 0.3464,  1.4499,  0.7417, -1.9551],

        [-0.0042, -0.0141,  1.2861,  0.0691],

        [ 0.5843,  1.6635, -1.2771, -1.4623]])

tensor([ 0.3464,  1.4499,  0.7417,  1.2861,  0.0691,  0.5843,  1.6635])



tensor([ 0.3464,  1.4499,  0.7417,  1.2861,  0.0691,  0.5843,  1.6635])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a&gt;<span class="number">0</span>  <span class="comment"># 返回一个 ByteTensor</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 1,  1,  1,  0],

        [ 0,  0,  1,  1],

        [ 1,  1,  0,  0]], dtype=torch.uint8)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b = torch.ByteTensor(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">b</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[  80,  235,  127,  167],

        [ 199,   85,    0,    0],

        [   0,    0,    0,    0]], dtype=torch.uint8)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a[b]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([ 0.3464,  1.4499,  0.7417, -1.9551, -0.0042, -0.0141])
</code></pre>
<h4 id="gather-input-dim-index-根据-index-在-dim-维度上选取数据，输出-size-与-index-一样"><a href="#gather-input-dim-index-根据-index-在-dim-维度上选取数据，输出-size-与-index-一样" class="headerlink" title="gather(input, dim, index)  根据 index 在 dim 维度上选取数据，输出 size 与 index 一样."></a>gather(input, dim, index)  根据 index 在 dim 维度上选取数据，输出 size 与 index 一样.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">0</span>, <span class="number">20</span>).view(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">index = torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(index, index.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[  0.,   1.,   2.,   3.,   4.],

        [  5.,   6.,   7.,   8.,   9.],

        [ 10.,  11.,  12.,  13.,  14.],

        [ 15.,  16.,  17.,  18.,  19.]])

tensor([[ 0,  1,  2,  1,  3]]) torch.Size([1, 5])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.gather(dim=<span class="number">0</span>, index=index)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[  0.,   6.,  12.,   8.,  19.]])
</code></pre>
<p>所以 gather 就是 index 与 input 中某一个维度一致，比如这里 input.size()=[4,5].</p>
<p>那么 dim=0, index.size()=[1,5]. 然后在每列对应的 index 选取对应的数据。最后输出 size 与 index 一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">index2 = torch.LongTensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(index2.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([4, 1])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.gather(dim=<span class="number">1</span>, index=index2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[  1.],

        [  7.],

        [ 13.],

        [ 19.]])
</code></pre>
<h5 id="list-转换成-one-hot-向量"><a href="#list-转换成-one-hot-向量" class="headerlink" title="list 转换成 one-hot 向量"></a>list 转换成 one-hot 向量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### list 转换成 one-hot 向量</span></span><br><span class="line"></span><br><span class="line">label = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">label = torch.LongTensor(label).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">one_hot = torch.zeros(<span class="number">5</span>, <span class="number">10</span>).scatter_(dim=<span class="number">1</span>, index=label, value=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">one_hot</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],

        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],

        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])
</code></pre>
<h4 id="Tensor-类型"><a href="#Tensor-类型" class="headerlink" title="Tensor 类型"></a>Tensor 类型</h4><p>Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过<code>t.set_default_tensor_type</code> 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有<code>1000*1000*1000=10^9</code>个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。</p>
<p>^2: <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste">https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste</a></p>
<p>表3-3: tensor数据类型</p>
<p>数据类型|    CPU tensor    |GPU tensor|</p>
<p>:—:|:—:|:–:|</p>
<p>32-bit 浮点|    torch.FloatTensor    |torch.cuda.FloatTensor</p>
<p>64-bit 浮点|    torch.DoubleTensor|    torch.cuda.DoubleTensor</p>
<p>16-bit 半精度浮点|    N/A    |torch.cuda.HalfTensor</p>
<p>8-bit 无符号整形(0~255)|    torch.ByteTensor|    torch.cuda.ByteTensor</p>
<p>8-bit 有符号整形(-128~127)|    torch.CharTensor    |torch.cuda.CharTensor</p>
<p>16-bit 有符号整形  |    torch.ShortTensor|    torch.cuda.ShortTensor</p>
<p>32-bit 有符号整形     |torch.IntTensor    |torch.cuda.IntTensor</p>
<p>64-bit 有符号整形      |torch.LongTensor    |torch.cuda.LongTensor</p>
<p>各数据类型之间可以互相转换，<code>type(new_type)</code>是通用的做法，同时还有<code>float</code>、<code>long</code>、<code>half</code>等快捷方法。CPU tensor与GPU tensor之间的互相转换通过<code>tensor.cuda</code>和<code>tensor.cpu</code>方法实现。Tensor还有一个<code>new</code>方法，用法与<code>t.Tensor</code>一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。</p>
<ul>
<li>torch.set_sefault_tensor_type(‘torch.IntTensor)</li>
</ul>
<h4 id="逐元素操作"><a href="#逐元素操作" class="headerlink" title="逐元素操作"></a>逐元素操作</h4><p>这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。</p>
<p>表3-4: 常见的逐元素操作</p>
<p>|函数|功能|</p>
<p>|:–:|:–:|</p>
<p>|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|</p>
<p>|cos/sin/asin/atan2/cosh..|相关三角函数|</p>
<p>|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|</p>
<p>|clamp(input, min, max)|超过min和max部分截断|</p>
<p>|sigmod/tanh..|激活函数</p>
<p>对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如<code>a ** 2</code> 等价于<code>torch.pow(a,2)</code>, <code>a * 2</code>等价于<code>torch.mul(a,2)</code>。</p>
<p>其中<code>clamp(x, min, max)</code>的输出满足以下公式：</p>
<p>$$</p>
<p>y_i =</p>
<p>\begin{cases}</p>
<p>min,  &amp; \text{if  } x_i \lt min \</p>
<p>x_i,  &amp; \text{if  } min \le x_i \le max  \</p>
<p>max,  &amp; \text{if  } x_i \gt max\</p>
<p>\end{cases}</p>
<p>$$</p>
<p><code>clamp</code>常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">6</span>).view(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">torch.clamp(a, <span class="built_in">min</span>=<span class="number">3</span>, <span class="built_in">max</span>=<span class="number">5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 0.,  1.,  2.],

        [ 3.,  4.,  5.]])



tensor([[ 3.,  3.,  3.],

        [ 3.,  4.,  5.]])
</code></pre>
<h4 id="归并操作"><a href="#归并操作" class="headerlink" title="归并操作"></a>归并操作</h4><p>此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法<code>sum</code>，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。</p>
<p>表3-5: 常用归并操作</p>
<p>|函数|功能|</p>
<p>|:—:|:—:|</p>
<p>|mean/sum/median/mode|均值/和/中位数/众数|</p>
<p>|norm/dist|范数/距离|</p>
<p>|std/var|标准差/方差|</p>
<p>|cumsum/cumprod|累加/累乘|</p>
<p>以上大多数函数都有一个参数 **<code>dim</code>**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：</p>
<p>假设输入的形状是(m, n, k)</p>
<ul>
<li><p>如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)</p>
</li>
<li><p>如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)</p>
</li>
<li><p>如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)</p>
</li>
</ul>
<p>size中是否有”1”，取决于参数<code>keepdim</code>，<code>keepdim=True</code>会保留维度<code>1</code>。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如<code>cumsum</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 0.,  1.,  2.],

        [ 3.,  4.,  5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.norm(dim=<span class="number">0</span>, p=<span class="number">1</span>), a.norm(dim=<span class="number">0</span>, p=<span class="number">2</span>), a.norm(dim=<span class="number">0</span>, p=<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(tensor([ 3.,  5.,  7.]),

 tensor([ 3.0000,  4.1231,  5.3852]),

 tensor([ 3.0000,  4.0207,  5.1045]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.norm??</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>$||x||<em>{p} = \sqrt[p]{x</em>{1}^{p} + x_{2}^{p} + \ldots + x_{N}^{p}}$</p>
<h5 id="torch-dist"><a href="#torch-dist" class="headerlink" title="torch.dist??"></a>torch.dist??</h5><p>dist(input, other, p=2) -&gt; Tensor</p>
<p>Returns the p-norm of (:attr:<code>input</code> - :attr:<code>other</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.dist(torch.ones(<span class="number">4</span>), torch.zeros(<span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.var(torch.randn(<span class="number">10</span>,<span class="number">3</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([ 0.7617,  1.0060,  1.6778])
</code></pre>
<h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><p>比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。</p>
<p>表3-6: 常用比较函数</p>
<p>|函数|功能|</p>
<p>|:–:|:–:|</p>
<p>|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|</p>
<p>|topk|最大的k个数|</p>
<p>|sort|排序|</p>
<p>|max/min|比较两个tensor最大最小值|</p>
<p>表中第一行的比较操作已经实现了运算符重载，因此可以使用<code>a&gt;=b</code>、<code>a&gt;b</code>、<code>a!=b</code>、<code>a==b</code>，其返回结果是一个<code>ByteTensor</code>，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：</p>
<ul>
<li><p>t.max(tensor)：返回tensor中最大的一个数</p>
</li>
<li><p>t.max(tensor,dim)：指定维上最大的数，返回tensor和下标</p>
</li>
<li><p>t.max(tensor1, tensor2): 比较两个tensor相比较大的元素</p>
</li>
</ul>
<p>至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。</p>
<ul>
<li><p>max/min  </p>
</li>
<li><p>sort  </p>
</li>
<li><p>topk  </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.1845,  0.4101,  0.1470,  0.0083],

        [ 0.7520,  0.8871,  0.9494,  0.2504],

        [ 0.3879,  0.4554,  0.4080,  0.1703]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.<span class="built_in">max</span>(a, dim=<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(tensor([ 0.7326,  0.6784,  0.9791,  0.9011]), tensor([ 1,  2,  1,  1]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.sort(dim=<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(tensor([[ 0.1424,  0.5681,  0.1833,  0.1654],

         [ 0.4556,  0.6418,  0.3242,  0.5120],

         [ 0.7326,  0.6784,  0.9791,  0.9011]]), tensor([[ 2,  0,  0,  2],

         [ 0,  1,  2,  0],

         [ 1,  2,  1,  1]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.topk(k=<span class="number">2</span>, dim=<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(tensor([[ 0.7326,  0.6784,  0.9791,  0.9011],

         [ 0.4556,  0.6418,  0.3242,  0.5120]]), tensor([[ 1,  2,  1,  1],

         [ 0,  1,  2,  0]]))
</code></pre>
<h4 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h4><p>PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。</p>
<p>表3-7: 常用的线性代数函数</p>
<p>|函数|功能|</p>
<p>|:—:|:—:|</p>
<p>|trace|对角线元素之和(矩阵的迹)|</p>
<p>|diag|对角线元素|</p>
<p>|triu/tril|矩阵的上三角/下三角，可指定偏移量|</p>
<p>|mm/bmm|矩阵乘法，batch的矩阵乘法|</p>
<p>|addmm/addbmm/addmv/addr/badbmm..|矩阵运算</p>
<p>|t|转置|</p>
<p>|dot/cross|内积/外积</p>
<p>|inverse|求逆矩阵</p>
<p>|svd|奇异值分解</p>
<p>具体使用说明请参见官方文档<a target="_blank" rel="noopener" href="http://pytorch.org/docs/torch.html#blas-and-lapack-operations">^3</a>，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的<code>.contiguous</code>方法将其转为连续。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b.contiguous(), b.size()</span><br><span class="line"></span><br><span class="line">b.contiguous().is_contiguous()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.matmul(b.contiguous()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([[ 0.8260,  1.3392,  0.5944],

        [ 1.3392,  2.7192,  1.0062],

        [ 0.5944,  1.0062,  0.6130]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b = a.t()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.size(), b.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.mm(b))</span><br><span class="line"></span><br><span class="line">b.is_contiguous()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>torch.Size([3, 4]) torch.Size([4, 3])

tensor([[ 0.8260,  1.3392,  0.5944],

        [ 1.3392,  2.7192,  1.0062],

        [ 0.5944,  1.0062,  0.6130]])

False
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b, b.diag()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(tensor([[ 0.4556,  0.7326,  0.1424],

         [ 0.5681,  0.6418,  0.6784],

         [ 0.1833,  0.9791,  0.3242],

         [ 0.5120,  0.9011,  0.1654]]), tensor([ 0.4556,  0.6418,  0.3242]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">a.triu(<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.0000,  1.5959, -0.2253,  0.2349, -0.5151],

        [ 0.0000,  0.0000, -0.0366, -0.0867,  0.2737],

        [ 0.0000,  0.0000,  0.0000,  0.9904, -1.4889],

        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1053],

        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])
</code></pre>
<h3 id="Tensor和Numpy"><a href="#Tensor和Numpy" class="headerlink" title="Tensor和Numpy"></a>Tensor和Numpy</h3><p>Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。</p>
<p><strong>注意</strong>： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.dtype)</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>float64

array([[1., 1., 1.],

       [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b = torch.Tensor(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.<span class="built_in">type</span>())</span><br><span class="line"></span><br><span class="line">b</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>torch.FloatTensor

tensor([[   1.,  100.,    1.],

        [   1.,    1.,    1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.from_numpy??</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c = torch.from_numpy(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c.<span class="built_in">type</span>())</span><br><span class="line"></span><br><span class="line">c</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>torch.DoubleTensor

tensor([[ 1.,  1.,  1.],

        [ 1.,  1.,  1.]], dtype=torch.float64)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a[<span class="number">0</span>,<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">b  <span class="comment"># b与a不通向内存，所以即使a改变了，b也不变</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 1.,  1.,  1.],

        [ 1.,  1.,  1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c  <span class="comment"># c 与 a 共享内存</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[   1.,  100.,    1.],

        [   1.,    1.,    1.]], dtype=torch.float64)
</code></pre>
<h4 id="BroadCast"><a href="#BroadCast" class="headerlink" title="BroadCast"></a>BroadCast</h4><p>广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。</p>
<p>Numpy的广播法则定义如下：</p>
<ul>
<li><p>让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐</p>
</li>
<li><p>两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算</p>
</li>
<li><p>当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状</p>
</li>
</ul>
<p>PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：</p>
<ul>
<li><p><code>unsqueeze</code>或者<code>view</code>：为数据某一维的形状补1，实现法则1</p>
</li>
<li><p><code>expand</code>或者<code>expand_as</code>，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。</p>
</li>
</ul>
<p>注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">b = torch.zeros(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 自动广播法则</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#               可以利用广播法则扩展，两个形状都变成了（2，3，2）</span></span><br><span class="line"></span><br><span class="line">a+b</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 1.,  1.],

         [ 1.,  1.],

         [ 1.,  1.]],



        [[ 1.,  1.],

         [ 1.,  1.],

         [ 1.,  1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.unsqueeze(<span class="number">0</span>).expand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>) + b.expand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 1.,  1.],

         [ 1.,  1.],

         [ 1.,  1.]],



        [[ 1.,  1.],

         [ 1.,  1.],

         [ 1.,  1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存</span></span><br><span class="line"></span><br><span class="line">e = a.unsqueeze(<span class="number">0</span>).expand(<span class="number">10000000000000</span>, <span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h3><p>tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。</p>
<p>一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。</p>
<p><img src="/2018/12/01/pytorch-book-1-Tensor/tensor1.png" alt="图3-1: Tensor的数据结构"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([ 0.,  1.,  2.,  3.,  4.,  5.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.storage()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code> 0.0

 1.0

 2.0

 3.0

 4.0

 5.0

[torch.FloatStorage of size 6]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">b.storage()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code> 0.0

 1.0

 2.0

 3.0

 4.0

 5.0

[torch.FloatStorage of size 6]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 一个对象的id值可以看作它在内存中的地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># storage的内存地址一样，即是同一个storage</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span>(b.storage()) == <span class="built_in">id</span>(a.storage())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c = torch.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">c.storage()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code> 0.0

 1.0

 2.0

 3.0

 4.0

 5.0

[torch.FloatStorage of size 6]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 一个对象的id值可以看作它在内存中的地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># storage的内存地址一样，即是同一个storage</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span>(c.storage()) == <span class="built_in">id</span>(a.storage())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># a改变，b也随之改变，因为他们共享storage, 但是 c 没有改变啊，很神奇</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">b, c</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(tensor([[   0.,  100.,    2.],

         [   3.,    4.,    5.]]), tensor([ 0.,  1.,  2.,  3.,  4.,  5.]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 一个对象的id值可以看作它在内存中的地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># storage的内存地址一样，即是同一个storage</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span>(c[<span class="number">1</span>].storage()), <span class="built_in">id</span>(c.storage())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(139719200619016, 139719200619016)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c = a[<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line">c.storage()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([ 2.,  3.,  4.,  5.])

 0.0

 100.0

 2.0

 3.0

 4.0

 5.0

[torch.FloatStorage of size 6]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c.data_ptr(), a.data_ptr() <span class="comment"># data_ptr返回tensor首元素的内存地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(94551854283064, 94551854283056)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">c[<span class="number">0</span>]=-<span class="number">100</span> <span class="comment"># c[0]的内存地址对应 a[2] 的内存地址</span></span><br><span class="line"></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([   0.,  100., -100.,    3.,    4.,    5.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">d = torch.Tensor(c.storage())</span><br><span class="line"></span><br><span class="line">d[<span class="number">0</span>] = <span class="number">6666</span></span><br><span class="line"></span><br><span class="line">b</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 6666.,   100.,  -100.],

        [    3.,     4.,     5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 下面４个tensor共享storage</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span>(a.storage()) == <span class="built_in">id</span>(b.storage()) == <span class="built_in">id</span>(c.storage()) == <span class="built_in">id</span>(d.storage())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a.storage_offset(), c.storage_offset(), d.storage_offset()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(0, 2, 0)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">e = b[::<span class="number">2</span>, ::<span class="number">2</span>] <span class="comment"># 隔2行/列取一个元素</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span>(e.storage()) == <span class="built_in">id</span>(a.storage())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">e.is_contiguous()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>False
</code></pre>
<p>可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。</p>
<p>此外有些操作会导致tensor不连续，这时需调用<code>tensor.contiguous</code>方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。</p>
<p>另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。</p>
<h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p>Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的<code>pickle</code>模块，在load时还可将GPU tensor映射到CPU或其它GPU上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line"></span><br><span class="line">    a = a.cuda() <span class="comment"># 把a转为GPU1上的tensor,</span></span><br><span class="line"></span><br><span class="line">    torch.save(a,<span class="string">&#x27;a.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)</span></span><br><span class="line"></span><br><span class="line">    b = torch.load(<span class="string">&#x27;a.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载为c, 存储于CPU</span></span><br><span class="line"></span><br><span class="line">    c = torch.load(<span class="string">&#x27;a.pth&#x27;</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载为d, 存储于GPU0上</span></span><br><span class="line"></span><br><span class="line">    d = torch.load(<span class="string">&#x27;a.pth&#x27;</span>, map_location=&#123;<span class="string">&#x27;cuda:1&#x27;</span>:<span class="string">&#x27;cuda:0&#x27;</span>&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.load(<span class="string">&quot;a.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>tensor([ 6666.,   100.,  -100.,     3.,     4.,     5.], device=&#39;cuda:0&#39;)
</code></pre>
<h4 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h4><p>向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是<code>for</code>循环。在科学计算程序中应当极力避免使用Python原生的<code>for循环</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_loop_add</span>(<span class="params">x, y</span>):</span></span><br><span class="line"></span><br><span class="line">    result = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i,j <span class="keyword">in</span> <span class="built_in">zip</span>(x, y):</span><br><span class="line"></span><br><span class="line">        result.append(i + j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">y = torch.ones(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">%timeit -n <span class="number">10</span> for_loop_add(x, y)</span><br><span class="line"></span><br><span class="line">%timeit -n <span class="number">10</span> x + y</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>351 µs ± 9.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

The slowest run took 16.46 times longer than the fastest. This could mean that an intermediate result is being cached.

4.24 µs ± 7.12 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
<p>可见二者有超过40倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯。</p>
<p>此外还有以下几点需要注意：</p>
<ul>
<li><p>大多数<code>torch.function</code>都有一个参数<code>out</code>，这时候产生的结果将保存在out指定tensor之中。</p>
</li>
<li><p><code>torch.set_num_threads</code>可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。</p>
</li>
<li><p><code>torch.set_printoptions</code>可以用来设置打印tensor时的数值精度和格式。</p>
</li>
</ul>
<p>下面举例说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(precision=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.3306640089, -0.0507176071, -0.4223535955],

        [-0.8678948879, -0.0437202156, 0.0183448847]])
</code></pre>
<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><p>线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：</p>
<p>$$</p>
<p>loss = \sum_i^N \frac 1 2 ({y_i-(wx_i+b)})^2</p>
<p>$$</p>
<p>然后利用随机梯度下降法更新参数$\textbf{w}$和$\textbf{b}$来最小化损失函数，最终学得$\textbf{w}$和$\textbf{b}$的数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span></span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fake_data</span>(<span class="params">batch_size=<span class="number">8</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 产生随机数据：y=x*2+3，加上了一些噪声&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    x = t.rand(batch_size, <span class="number">1</span>) * <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    y = x * <span class="number">2</span> + (<span class="number">1</span> + t.randn(batch_size, <span class="number">1</span>))*<span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 来看看产生的x-y分布</span></span><br><span class="line"></span><br><span class="line">x, y = get_fake_data()</span><br><span class="line"></span><br><span class="line">plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/2018/12/01/pytorch-book-1-Tensor/output_106_1.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 随机初始化参数</span></span><br><span class="line"></span><br><span class="line">w = torch.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">b = torch.zeros(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.001</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line"></span><br><span class="line">    x, y = get_fake_data(batch_size=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line"></span><br><span class="line">    y_pred = x.mm(w) + b.expand_as(y)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0.5</span> * (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    loss = loss.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward: 手动计算梯度</span></span><br><span class="line"></span><br><span class="line">    dloss = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    dy_pred = dloss * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    dw = x.t().contiguous().mm(dy_pred)</span><br><span class="line"></span><br><span class="line">    db = dy_pred.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">    w.sub_(lr * dw)</span><br><span class="line"></span><br><span class="line">    b.sub_(lr * db)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;, loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 画图</span></span><br><span class="line"></span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        x = torch.arange(<span class="number">0</span>, <span class="number">20</span>).view(-<span class="number">1</span>, <span class="number">1</span>)    <span class="comment"># [20, 1]</span></span><br><span class="line"></span><br><span class="line">        y = x.mm(w) + b.expand_as(x)           <span class="comment"># predicted data</span></span><br><span class="line"></span><br><span class="line">        plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x2, y2 = get_fake_data(batch_size=<span class="number">20</span>)  <span class="comment"># true data</span></span><br><span class="line"></span><br><span class="line">        plt.scatter(x2.numpy(), y2.numpy())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        plt.xlim(<span class="number">0</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">        plt.ylim(<span class="number">0</span>,<span class="number">41</span>)</span><br><span class="line"></span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w.squeeze()[<span class="number">0</span>], b.squeeze()[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/2018/12/01/pytorch-book-1-Tensor/output_107_0.png" alt="png"></p>
<pre><code>tensor(2.0264241695) tensor(2.9323694706)
</code></pre>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-11-27T12:04:32.000Z" title="2018/11/27 下午8:04:32">2018-11-27</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.522Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">25 分钟读完 (大约3787个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">深度学习-优化算法</a></h1><div class="content"><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.04747.pdf">An overview of gradient descent optimization algorithms</a></p>
<h2 id="Gradient-descent-variants"><a href="#Gradient-descent-variants" class="headerlink" title="Gradient descent variants"></a>Gradient descent variants</h2><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h3><p>computes the gradient of the cost function to the parameters $\theta$ for the entire training dataset.</p>
<p>$$\theta= \theta - \delta_{\theta}J(\theta)$$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">for i in range ( nb_epochs ):</span><br><span class="line"></span><br><span class="line">  params_grad = evaluate_gradient ( loss_function , data , params )</span><br><span class="line"></span><br><span class="line">  params = params - learning_rate * params_grad</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.</p>
<h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><p>Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example x(i) and label y(i):</p>
<p>$$\theta= \theta - \delta_{\theta}J(\theta; x^{(i)}; y^{(i)})$$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">for i in range ( nb_epochs ):</span><br><span class="line"></span><br><span class="line">  np. random . shuffle ( data )</span><br><span class="line"></span><br><span class="line">  for example in data :</span><br><span class="line"></span><br><span class="line">    params_grad = evaluate_gradient ( loss_function , example , params )</span><br><span class="line"></span><br><span class="line">    params = params - learning_rate * params_grad</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn</p>
<p>online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily.  </p>
<p>批梯度下降的计算过于冗余，它在每一次参数更新之前的计算过程中会计算很多相似的样本。随机梯度下降则是每一次参数更新计算一个样本，因此更新速度会很快，并且可以在线学习。但是用于更新的梯度的方差会很大，导致 loss 曲线波动很大。</p>
<p>While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately</p>
<p>complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost</p>
<p>certainly converging to a local or the global minimum for non-convex and convex optimization respectively.  </p>
<p>批梯度下降收敛到的最小值与相应的参数关系很大（也就是说跟权重的初始化会有很大影响）。而 SGD 由于loss波动很大，更有效的跳出局部最优区域，从而获得更好的局部最优值。但另一方面，这也会使得 SGD 难以收敛。实验表明，缓慢的降低学习率， SGD 和 BatchGD 能获得同样的局部最优解。</p>
<h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples.</p>
<p>$$\theta= \theta - \delta_{\theta}J(\theta; x^{(i+n)}; y^{(i+n)})$$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">for i in range ( nb_epochs ):</span><br><span class="line"></span><br><span class="line">  np. random . shuffle ( data )</span><br><span class="line"></span><br><span class="line">  for batch in get_batches (data , batch_size =50):</span><br><span class="line"></span><br><span class="line">    params_grad = evaluate_gradient ( loss_function , batch , params )</span><br><span class="line"></span><br><span class="line">    params = params - learning_rate * params_grad</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>reduces the variance of the parameter updates, which can lead to more stable convergence;  </li>
</ul>
<p>减小参数更新的方差，使得收敛更稳定。</p>
<ul>
<li>can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient mini-batch very efficient.  </li>
</ul>
<p>能非常好的利用矩阵优化的方式来加速计算，这在各种深度学习框架里面都很常见。</p>
<h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><ul>
<li>Choosing a proper learning rate.  </li>
</ul>
<p>选择合适的学习率。</p>
<ul>
<li>Learning rate schedules.  try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics.  </li>
</ul>
<p>学习率计划。在训练过程中调整学习率，譬如退火，预先定义好的计划，当一个 epoch 结束后，目标函数（loss） 减小的值低于某个阈值时，可以调整学习率。  </p>
<ul>
<li>the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring</li>
</ul>
<p>features.  </p>
<p>对所有的参数使用相同的学习率。如果你的数据是稀疏的，并且不同的特征的频率有很大的不同，这个时候我们并不希望对所有的参数使用相同的学习率，而是对更罕见的特征执行更大的学习率。</p>
<ul>
<li>Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima.  Dauphin et al. [5] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.  </li>
</ul>
<p>对于非凸损失函数的优化问题，需要避免陷入其众多的次优局部极小值。Dauphin et al. [5] 则认为， 相比局部极小值，鞍点的是更难解决的问题。鞍点是一个维度上升，一个维度下降。详细的关于鞍点以及 SGD 如何逃离鞍点可参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29490637">知乎：如何逃离鞍点</a> .</p>
<h2 id="Gradient-descent-optimization-algorithms"><a href="#Gradient-descent-optimization-algorithms" class="headerlink" title="Gradient descent optimization algorithms"></a>Gradient descent optimization algorithms</h2><p>Momentum [17] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Figure 2b. It does this by padding a fraction $gamma$ of the update vector of the past time step to the current</p>
<p>update vector.</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>paper: [Neural networks :</p>
<p>the official journal of the International Neural Network Society]()</p>
<p><img src="/2018/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/sgd01.png"></p>
<p>without Momentum:</p>
<p>$$\theta += -lr * \nabla_{\theta}J(\theta)$$</p>
<p>with Momentum:</p>
<p>$$v_t=\gamma v_{t-1}+\eta \nabla_{\theta}J(\theta)$$</p>
<p>$$\theta=\theta-v_t$$</p>
<p>动量梯度下降的理解：</p>
<p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions.  </p>
<p>如上图中垂直方向的梯度方向是一致的，那么它的动量会累积，并在这个方向的速度越来越大。而在某个水平方向，其梯度方向总是变化，那么它的速度会减小，也就是在这个方向的波动幅度会得到抑制。</p>
<p>其实就是把梯度看做加速度，参数的更新量看做速度。速度表示一个step更新的大小。加速度总是朝着一个方向，速度必然越来越快。加速度方向总是变化，速度就会相对较小。</p>
<p>$\gamma$ 看做摩擦系数， 通常设置为 0.9。$\eta$ 是学习率。</p>
<h3 id="Nesterov-accelerate-gradient-NAG"><a href="#Nesterov-accelerate-gradient-NAG" class="headerlink" title="Nesterov accelerate gradient(NAG)"></a>Nesterov accelerate gradient(NAG)</h3><p>paper: [Yurii Nesterov. A method for unconstrained convex minimization problem</p>
<p>with the rate of convergence o(1/k2).]()</p>
<p>We would like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again. Nesterov accelerated gradient (NAG) [14] is a way to give our momentum term this kind of prescience.  </p>
<p>如果采用 momentum，在接近目标函数最优值时，由于速度在垂直方向是一直增加的，所以速度会很大，这个时候就会越过最小值，然后还得绕回来，增加了训练时间。所以我们需要参数的更新具有先见之明，知道在接近最优解时，降低参数更新的速度大小。</p>
<p>$$v_t=\gamma v_{t-1}+\eta \nabla_{\theta}J(\theta-\gamma v_{t-1})$$</p>
<p>$$\theta=\theta-v_t$$</p>
<p>在 momentum 中，我们用速度 $\gamma v_{t-1}$ 来更新参数。 事实上在接近局部最优解时，目标函数对于 $\theta$ 的梯度会越来越小，甚至接近于 0. 也就是说，尽管速度在增加，但是速度增加的程度越来越小。我们可以通过速度增加的程度来判断是否要接近局部最优解了。$\nabla_{\theta}J(\theta-\gamma v_{t-1})$ 就表示速度变化的程度，代替一直为正的 $\nabla_{\theta}J(\theta)$，在接近局部最优解时，这个值应该是负的，相应的参数更新的速度也会减小.</p>
<p>在代码实现时，对于 $J(\theta-\gamma v_{t-1})$ 的梯度计算不是很方便，可以令：</p>
<p>$$\phi = \theta-\gamma v_{t-1}$$</p>
<p>然后进行计算，具体可参考 tensorflow 或 pytorch 中代码。</p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>paper: [Adaptive Subgradient Methods for Online Learning</p>
<p>and Stochastic Optimization]()</p>
<p>Adagrad [8] is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.  </p>
<p>对于不同的参数，自适应的调整对应的梯度大小。对低频参数或特征，使其更新的梯度较大，对高频的参数或特征，使其更新的梯度较小。比如在训练 Glove 词向量时，低频词在某一步迭代中可能并没有参与 loss 的计算，所以更新的会相对较慢，所以需要人为的增大它的梯度。</p>
<p>不同的时间步 t,不同的参数 i 对应的梯度：</p>
<p>$$g_{t,i}=\nabla_{\theta_t}J(\theta_t,i)$$</p>
<p>$$\theta_{t+1,i}=\theta_{t,i}-\eta \cdot g_{t,i}$$</p>
<p>$$\theta_{t+1,i}=\theta_{t,i}-\dfrac{\eta}{\sqrt G_{t,ii}+\epsilon} g_{t,i}$$</p>
<p>$G_{t,ii}$ 是对角矩阵，对角元素是对应的梯度大小。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cache += dx**2</span><br><span class="line"></span><br><span class="line">x += -lr * dx/(np.sqrt(cache) + 1e-7)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p><a href>Geoff Hinton Lecture 6e</a></p>
<p>Adagrad 中随着 cache 的累积，最后的梯度会变为 0，RMSprop 在此基础上进行了改进，给了 cache 一个衰减率，相当于值考虑了最近时刻的梯度值，而很早之前的梯度值经过衰减后影响很小。</p>
<p>$$E[g^2]_ t=0.9E[g^2]_ {t-1}+0.1g^2_t$$</p>
<p>$$\theta_{t+1}=\theta_t-\dfrac{\eta}{E[g^2]_ t+\epsilon}g_t$$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cache = decay_rate*cache + (1-decay_rate)*dx**2</span><br><span class="line"></span><br><span class="line">x += -lr * dx/(np.sqrt(cache) + 1e-7)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用指数衰减的形式来保存 cache 能有效的节省内存，只需要记录当前的梯度值即可，而不用保存所有的梯度值。</p>
<h3 id="Adam-Adaptive-Moment-Estimation"><a href="#Adam-Adaptive-Moment-Estimation" class="headerlink" title="Adam(Adaptive Moment Estimation)"></a>Adam(Adaptive Moment Estimation)</h3><p><a href>Adam: a Method for Stochastic Optimization.</a></p>
<p>In addition to storing an exponentially decaying average of past squared gradients $v_t$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum:</p>
<p>similar like momentum:  </p>
<p>$$m_t=\beta_1m_{t-1}+(1-\beta_1)g_t$$</p>
<p>similar like autograd/RMSprop:  </p>
<p>$$v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2$$</p>
<p>$m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$ are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. β1 and β2 are close to 1). They counteract these biases by computing bias-corrected first and second moment estimates:</p>
<p>$$\hat m_t=\dfrac{m_t}{1-\beta^t_1}$$</p>
<p>$$\hat v_t=\dfrac{v_t}{1-\beta^t_2}$$</p>
<p>They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which</p>
<p>yields the Adam update rule:</p>
<p>$$\theta_{t+1}=\theta_t-\dfrac{\eta}{\sqrt{\hat a}+ \epsilon}{\hat m_t}$$</p>
<ul>
<li><p>$m_t$ 是类似于 Momentum 中参数更新量，是梯度的函数. $\beta_1$ 是摩擦系数，一般设为 0.9.  </p>
</li>
<li><p>$v_t$ 是类似于 RMSprop 中的 cache，用来自适应的改变不同参数的梯度大小。  </p>
</li>
<li><p>$\beta_2$ 是 cache 的衰减系数，一般设为 0.999.</p>
</li>
</ul>
<h3 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h3><p><a href>Adam: a Method for Stochastic Optimization.</a></p>
<p>在 Adam 中, 用来归一化梯度的因子 $v_t$ 与过去的梯度(包含在 $v_{t-1}$ 中)以及当前的梯度 $|g_t|^2$ 的 l2 范式成反比。</p>
<p>$$v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2$$</p>
<p>可以将其泛化到 $l_p$ 范式。同样的 $\beta_2$ 变为 $\beta_2^p$.</p>
<p>Norms for large p values generally become numerically unstable, which is why $l_1$ and $l_2$ norms are most common in practice. However, $l_{\infty}$ also generally exhibits stable behavior. For this reason, the authors propose AdaMax [10] and show that $v_t$ with $l_{\infty}$ converges to the following more stable value. To avoid confusion with Adam, we use ut to denote the infinity norm-constrained $v_t$:</p>
<p>$$\mu_t=\beta_2^{\infty}v_{t-1}+(1-\beta_2^{\infty})|g_t|^{\infty}$$</p>
<p>$$=max(\beta_2\cdot v_{t-1}, |g_t|)$$</p>
<p>然后用 $\mu_t$ 代替 Adam 中的 $\sqrt(v_t)+\epsilon$:</p>
<p>$$\theta_{t+1}=\theta_t-\dfrac{\eta}{\mu_t}{\hat m_t}$$</p>
<p>Note that as $\mu_t$ relies on the max operation, it is not as suggestible to bias towards zero as $m_t$ and $v_t$ in Adam, which is why we do not need to compute a bias correction for ut. Good default values are again:</p>
<p>$$\eta = 0.002, \beta_1 = 0.9, \beta_2 = 0.999.$$</p>
<h2 id="Visualization-of-algorithms"><a href="#Visualization-of-algorithms" class="headerlink" title="Visualization of algorithms"></a>Visualization of algorithms</h2><p><img src="/2018/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/opt2.gif"></p>
<blockquote>
<p>we see the path they took on the contours of a loss surface (the Beale function). All started at the same point and took different paths to reach the minimum. Note that Adagrad, Adadelta, and RMSprop headed off immediately in the right direction and converged similarly fast, while Momentum and NAG were led off-track, evoking the image of a ball rolling down the hill. NAG, however, was able to correct its course sooner due to its increased responsiveness by looking ahead and headed to the minimum.  </p>
</blockquote>
<p>如果目标函数是 Beale 这种类型的函数，自适应优化算法能更直接的收敛到最小值。而 Momentum 和 NAG 则偏离了轨道，就像球从山上滚下一样，刹不住车。但是 NAG 因为对未来具有一定的预见性，所以能更早的纠正从而提高其响应能力。</p>
<p><img src="/2018/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/opt1.gif"></p>
<blockquote>
<p>shows the behaviour of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope, which pose a difficulty for SGD as we mentioned before. Notice here that SGD, Momentum, and NAG find it difficulty to break symmetry, although the latter two eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope, with Adadelta leading the charge.  </p>
</blockquote>
<p>各种优化算法鞍点的表现。 Momentum, SGD, NAG 很难打破平衡，而自适应性的算法 Adadelta, RMSprop, Adadelta 能很快的逃离鞍点。</p>
<h2 id="example"><a href="#example" class="headerlink" title="example"></a>example</h2><h3 id="model"><a href="#model" class="headerlink" title="model"></a>model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(TestNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, label</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        x: [batch, 10]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        label: [batch]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        out = self.linear2(self.linear1(x)).squeeze()</span><br><span class="line"></span><br><span class="line">        loss = self.loss(out, label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, loss</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model = TestNet()</span><br><span class="line"></span><br><span class="line">model</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>TestNet(

  (linear1): Linear(in_features=10, out_features=5, bias=True)

  (linear2): Linear(in_features=5, out_features=1, bias=True)

  (loss): BCELoss()

)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">list</span>(model.named_parameters())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;linear1.weight&#39;, Parameter containing:

  tensor([[ 0.2901, -0.0022, -0.1515, -0.1064, -0.0475, -0.0324,  0.0404,  0.0266,

           -0.2358, -0.0433],

          [-0.1588, -0.1917,  0.0995,  0.0651, -0.2948, -0.1830,  0.2356,  0.1060,

            0.2172, -0.0367],

          [-0.0173,  0.2129,  0.3123,  0.0663,  0.2633, -0.2838,  0.3019, -0.2087,

           -0.0886,  0.0515],

          [ 0.1641, -0.2123, -0.0759,  0.1198,  0.0408, -0.0212,  0.3117, -0.2534,

           -0.1196, -0.3154],

          [ 0.2187,  0.1547, -0.0653, -0.2246, -0.0137,  0.2676,  0.1777,  0.0536,

           -0.3124,  0.2147]], requires_grad=True)),

 (&#39;linear1.bias&#39;, Parameter containing:

  tensor([ 0.1216,  0.2846, -0.2002, -0.1236,  0.2806], requires_grad=True)),

 (&#39;linear2.weight&#39;, Parameter containing:

  tensor([[-0.1652,  0.3056,  0.0749, -0.3633,  0.0692]], requires_grad=True)),

 (&#39;linear2.bias&#39;, Parameter containing:

  tensor([0.0450], requires_grad=True))]
</code></pre>
<h3 id="add-model-parameters-to-optimizer"><a href="#add-model-parameters-to-optimizer" class="headerlink" title="add model parameters to optimizer"></a>add model parameters to optimizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># parameters = model.parameters()</span></span><br><span class="line"></span><br><span class="line">parameters_filters = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">optimizer = optim.Adam(</span><br><span class="line"></span><br><span class="line">    params=parameters_filters,</span><br><span class="line"></span><br><span class="line">    lr=<span class="number">0.001</span>,</span><br><span class="line"></span><br><span class="line">    betas=(<span class="number">0.8</span>, <span class="number">0.999</span>),</span><br><span class="line"></span><br><span class="line">    eps=<span class="number">1e-8</span>,</span><br><span class="line"></span><br><span class="line">    weight_decay=<span class="number">3e-7</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">optimizer.state_dict</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&lt;bound method Optimizer.state_dict of Adam (

Parameter Group 0

    amsgrad: False

    betas: (0.8, 0.999)

    eps: 1e-08

    lr: 0.001

    weight_decay: 3e-07

)&gt;
</code></pre>
<h3 id="不同的模块设置不同的参数"><a href="#不同的模块设置不同的参数" class="headerlink" title="不同的模块设置不同的参数"></a>不同的模块设置不同的参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">parameters = [&#123;<span class="string">&quot;params&quot;</span>: model.linear1.parameters()&#125;,</span><br><span class="line"></span><br><span class="line">             &#123;<span class="string">&quot;params&quot;</span>:model.linear2.parameters(), <span class="string">&quot;lr&quot;</span>: <span class="number">3e-4</span>&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">optimizer2 = optim.Adam(</span><br><span class="line"></span><br><span class="line">    params=parameters,</span><br><span class="line"></span><br><span class="line">    lr=<span class="number">0.001</span>,</span><br><span class="line"></span><br><span class="line">    betas=(<span class="number">0.8</span>, <span class="number">0.999</span>),</span><br><span class="line"></span><br><span class="line">    eps=<span class="number">1e-8</span>,</span><br><span class="line"></span><br><span class="line">    weight_decay=<span class="number">3e-7</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">optimizer2.state_dict</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&lt;bound method Optimizer.state_dict of Adam (

Parameter Group 0

    amsgrad: False

    betas: (0.8, 0.999)

    eps: 1e-08

    lr: 0.001

    weight_decay: 3e-07



Parameter Group 1

    amsgrad: False

    betas: (0.8, 0.999)

    eps: 1e-08

    lr: 0.0003

    weight_decay: 3e-07

)&gt;
</code></pre>
<h3 id="zero-grad"><a href="#zero-grad" class="headerlink" title="zero_grad"></a>zero_grad</h3><p>在进行反向传播之前，如果不需要梯度累加的话，必须要用zero_grad()清空梯度。具体的方法是遍历self.param_groups中全部参数，根据grad属性做清除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Clears the gradients of all optimized :class:`torch.Tensor` s.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">                p.grad.detach_()</span><br><span class="line"></span><br><span class="line">                p.grad.zero_()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">group_parameters = [&#123;<span class="string">&quot;params&quot;</span>: model.linear1.parameters()&#125;,</span><br><span class="line"></span><br><span class="line">             &#123;<span class="string">&quot;params&quot;</span>:model.linear2.parameters(), <span class="string">&quot;lr&quot;</span>: <span class="number">3e-4</span>&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">label = torch.Tensor([<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">out, loss = model(x, label)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">optimizer2.zero_grad()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">for</span> group <span class="keyword">in</span> group_parameters:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">            p.grad.detach_()</span><br><span class="line"></span><br><span class="line">            p.grad.zero_()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>这里并没有使用 backward() 所以暂时不存在梯度。</p>
<p>在反向传播 backward() 计算出梯度之后，就可以调用step()实现参数更新。不过在 Optimizer 类中，step()函数内部是空的，并且用raise NotImplementError 来作为提醒。后面会根据具体的优化器来分析step()的实现思路。</p>
<h3 id="辅助类lr-scheduler"><a href="#辅助类lr-scheduler" class="headerlink" title="辅助类lr_scheduler"></a>辅助类lr_scheduler</h3><p>lr_scheduler用于在训练过程中根据轮次灵活调控学习率。调整学习率的方法有很多种，但是其使用方法是大致相同的：用一个Schedule把原始Optimizer装饰上，然后再输入一些相关参数，然后用这个Schedule做step()。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># lambda1 = lambda epoch: epoch // 30</span></span><br><span class="line"></span><br><span class="line">lambda1 = <span class="keyword">lambda</span> epoch: <span class="number">0.95</span> ** epoch</span><br><span class="line"></span><br><span class="line">scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scheduler.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="warm-up-scheduler"><a href="#warm-up-scheduler" class="headerlink" title="warm up scheduler"></a>warm up scheduler</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">parameters = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr_warm_up_num = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(</span><br><span class="line"></span><br><span class="line">    params=parameters,</span><br><span class="line"></span><br><span class="line">    lr=<span class="number">0.001</span>,</span><br><span class="line"></span><br><span class="line">    betas=(<span class="number">0.8</span>, <span class="number">0.999</span>),</span><br><span class="line"></span><br><span class="line">    eps=<span class="number">1e-8</span>,</span><br><span class="line"></span><br><span class="line">    weight_decay=<span class="number">3e-7</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cr = <span class="number">1.0</span> / math.log(lr_warm_up_num)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scheduler = optim.lr_scheduler.LambdaLR(</span><br><span class="line"></span><br><span class="line">    optimizer,</span><br><span class="line"></span><br><span class="line">    lr_lambda=<span class="keyword">lambda</span> ee: cr * math.log(ee + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ee &lt; lr_warm_up_num <span class="keyword">else</span> <span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-11-04T03:04:35.000Z" title="2018/11/4 上午11:04:35">2018-11-04</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.158Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/">text matching</a></span><span class="level-item">24 分钟读完 (大约3533个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/">论文笔记-Multi-cast Attention Networks</a></h1><div class="content"><p>paper: [Multi-Cast Attention Networks for Retrieval-based Question</p>
<p>Answering and Response Prediction](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.00778">https://arxiv.org/abs/1806.00778</a>)</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Our approach performs a series of soft attention</p>
</blockquote>
<p>operations, each time casting a scalar feature upon the inner word embeddings. The key idea is to provide a real-valued hint (feature) to a subsequent encoder layer and is targeted at improving the representation learning process.  </p>
<p>在 encoder layer 之前将 document 和 query 进行交互，然后将权重赋予到 document 和 query 之后，在通过 contextual/encoder layer 编码融合了上下文信息的向量表示。这样做的目地，是为后续层提供提示（特征），提升表示学习的性能。</p>
<blockquote>
<p>The key idea of attention is to extract only the most relevant information that is useful for prediction. In the context of textual data, attention learns to weight words and sub-phrases within documents based on how important they are. In the same vein, co-attention mechanisms [5, 28, 50, 54] are a form of attention mechanisms that learn joint pairwise attentions, with respect to both document and query.  </p>
</blockquote>
<p>attention 注意力的关键思想是仅提取对预测有用的最相关信息。在文本数据的上下文中，注意力学习根据文档中的单词和子短语的重要性来对它们进行加权。</p>
<blockquote>
<p>Attention is traditionally used and commonly imagined as a feature extractor. It’s behavior can be thought of as a dynamic form of pooling as it learns to select and compose different words to form the final document representation.  </p>
</blockquote>
<p>传统的 attention 可以看做为一个特征提取器。它的行为可以被认为是一种动态的pooing形式，因为它学习选择和组合不同的词来形成最终的文档表示，。</p>
<blockquote>
<p>This paper re-imagines attention as a form of feature augmentation method. Attention is casted with the purpose of not compositional learning or pooling but to provide hints for subsequent layers. To the best of our knowledge, this is a new way to exploit attention in neural ranking models.  </p>
</blockquote>
<p>这篇 paper 将 attention 重新设想为一种特征增强的方式，Attention的目的不是组合学习或汇集，而是为后续层提供提示（特征）。这是一种在神经排序模型中的新方法。</p>
<p>不管这篇paper提供的新的 attention 使用方式是否是最有效的，但这里对 attention 的很多解释让人耳目一新，可以说理解的很透彻了。</p>
<blockquote>
<p> An obvious drawback which applies to many existing models is that they are generally restricted to one attention variant. In the case where one or more attention calls are used (e.g., co-attention and intra-attention, etc.), concatenation is generally used to fuse representations [20, 28]. Unfortunately, this incurs cost in subsequent layers by doubling the representation size per call.  </p>
</blockquote>
<p>很多 paper 中只受限于一种 attention，这明显是不够好的。也有使用多种 attention 的，比如 co-attention 和 intra-attention， 然后直接拼接起来。这样会使得接下来的 modeling layer 维度加倍。。（在 ai challenge 的比赛中就是这么干的。。没啥不好啊。。）</p>
<blockquote>
<p>The rationale for desiring more than one attention call is intuitive. In [20, 28], Co-Attention and Intra-Attention are both used because each provides a different view of the document pair, learning high quality representations that could be used for prediction. Hence, this can significantly improve performance.  </p>
</blockquote>
<p>直觉上，使用多种 multi-attention 是靠谱的。 co-attention 和 intra-attention 提供了不同的视角去审视 document,用以学习高质量的向量表示。    </p>
<p>关于各种 attention：  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35041012">https://zhuanlan.zhihu.com/p/35041012</a>  </p>
</li>
<li><p>[50] co-attention: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.01604.pdf">dynamic coattention networks for question answering</a>  </p>
</li>
<li><p>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.03609">Attentive Pooling Networks</a>  </p>
</li>
<li><p>[28] [Inter-Weighted Alignment</p>
</li>
</ul>
<p>Network for Sentence Pair Modeling](<a target="_blank" rel="noopener" href="https://aclanthology.info/pdf/D/D17/D17-1122.pdf">https://aclanthology.info/pdf/D/D17/D17-1122.pdf</a>)   </p>
<ul>
<li><p>[54] <a target="_blank" rel="noopener" href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14611">Attentive Interactive Neural Networks for Answer Selection in Community Question</a></p>
</li>
<li><p>intra-attention:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all your need</a></p>
</li>
</ul>
<blockquote>
<p>Moreover, Co-Attention also comes in different flavors and can either be used with extractive max-mean pooling [5, 54] or alignment-based pooling [3, 20, 28]. Each co-attention type produces different document representations. In max-pooling, signals are extracted based on a word’s largest contribution to the other text sequence. Mean-pooling calculates its contribution to the overall sentence. Alignment-pooling is another flavor of co-attention, which aligns semantically similar sub-phrases together.  </p>
</blockquote>
<p>co-attention可以用于提取max-mean pooling或alignment-based pooling。每种co-attention都会产生不同的文档表示。在max-pooling中，基于单词对另一文本序列的最大贡献来提取特征；mean-pooling计算其对整个句子的贡献；alignment-based pooling是另一种协同注意力机制，它将语义相似的子短语对齐在一起。因此，不同的pooling操作提供了不同的句子对视图。</p>
<ul>
<li><p>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.06038">Enhanced LSTM for Natural Language Inference</a>  </p>
</li>
<li><p>[20] [A</p>
</li>
</ul>
<p>Decomposable Attention Model for Natural Language Inference](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.01933">https://arxiv.org/abs/1606.01933</a>)  </p>
<blockquote>
<p>Our approach is targeted at serving two important purposes: (1) It removes the need for architectural engineering of this component by enabling attention to be called for an arbitrary k times with hardly any consequence and (2) concurrently it improves performance by modeling multiple views via multiple attention calls. As such, our method is in similar spirit to multi-headed attention, albeit efficient. To this end, we introduce Multi-Cast Attention Networks (MCAN), a new deep learning architecture for a potpourri of tasks in the question answering and conversation modeling domains.  </p>
</blockquote>
<p>两个方面的贡献：  </p>
<p>（1）消除调用任意k次注意力机制所需架构工程的需要，且不会产生任何后果。  </p>
<p>（2）通过多次注意力调用建模多个视图以提高性能，与multi-headed attention相似。</p>
<blockquote>
<p> In our approach, attention is casted, in contrast to the most other works that use it as a pooling operation. We cast co-attention multiple times, each time returning a compressed scalar feature that is re-attached to the original word representations. The key intuition is that compression enables scalable casting of multiple attention calls, aiming to provide subsequent layers with a hint of not only global knowledge but also cross sentence knowledge.  </p>
</blockquote>
<p>与大多数其他用作池化操作的工作相反，在我们的方法中，注意力被投射。通过多次投射co-attention，每次返回一个压缩的标量特征，重新附加到原始的单词表示上。压缩函数可以实现多个注意力调用的可扩展投射，旨在不仅为后续层提供全局知识而且还有跨句子知识的提示（特征）。</p>
<h2 id="Model-Architecture-Multi-cast-Attention-Networks"><a href="#Model-Architecture-Multi-cast-Attention-Networks" class="headerlink" title="Model Architecture: Multi-cast Attention Networks"></a>Model Architecture: Multi-cast Attention Networks</h2><p><img src="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/01.png"></p>
<p>Figure 1: Illustration of our proposed Multi-Cast Attention Networks (Best viewed in color). MCAN is a wide multi-headed attention architecture that utilizes compression functions and attention as features.</p>
<p>模型输入是 document/query 语句对。</p>
<h3 id="Input-Encoder"><a href="#Input-Encoder" class="headerlink" title="Input Encoder"></a>Input Encoder</h3><h4 id="embedding-layer"><a href="#embedding-layer" class="headerlink" title="embedding layer"></a>embedding layer</h4><p>映射到向量空间： $w\in W^d$</p>
<h4 id="Highway-Encoder："><a href="#Highway-Encoder：" class="headerlink" title="Highway Encoder："></a>Highway Encoder：</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.00387.pdf">Highway Networks</a>可以对任意深度的网络进行优化。这是通过一种控制穿过神经网络的信息流的闸门机制所实现的。通过这种机制，神经网络可以提供通路，让信息穿过后却没有损失，将这种通路称为information highways。即highway networks主要解决的问题是网络深度加深、梯度信息回流受阻造成网络训练困难的问题。</p>
</blockquote>
<blockquote>
<p>highway encoders can be interpreted as data-driven word filters. As such, we can imagine them to parametrically learn which words have an inclination to be important and not important to the task at hand. For example, filtering stop words and words that usually do not contribute much to the prediction. Similar to recurrent models that are gated in nature, this highway encoder layer controls how much information (of each word) is flowed to the subsequent layers.  </p>
</blockquote>
<p>在本文模型中，每个词向量都通过highway编码器层。highway网络是门控非线性变换层，它控制后续层的信息流。许多工作都采用一种训练过的投影层来代替原始词向量。这不仅节省了计算成本，还减少了可训练参数的数量。本文将此投影层扩展为使用highway编码器，可以解释为数据驱动的词滤波器，它们可以参数化地了解哪些词对于任务具有重要性和重要性。例如，删除通常对预测没有多大贡献的停用词和单词。与自然门控的循环模型类似，highway编码器层控制每个单词流入下一层多少信息。  </p>
<p>$$y=H(x,W_H)\cdot T(x,W_T) + (1-T(x,W_T))\cdot x$$</p>
<p>其中 $W_H, W_T\in R^{r\times d}$ 是可学习参数. H(.) 和 T(.) 分别是全连接加上 relu 和 sigmoid 的函数，用以控制信息的流向下一层。</p>
<h3 id="co-attention"><a href="#co-attention" class="headerlink" title="co-attention"></a>co-attention</h3><blockquote>
<p>Co-Attention [50] is a pairwise attention mechanism that enables</p>
</blockquote>
<p>attending to text sequence pairs jointly. In this section, we introduce four variants of attention, i.e., (1) max-pooling, (2) mean-pooling, (3) alignment-pooling, and finally (4) intra-attention (or self attention).  </p>
<p>协同注意力机制是成对的注意力机制，能够同时关注文本序列对。作者引入了 4 中注意力机制。</p>
<h4 id="1-affinity-similarity-matrix"><a href="#1-affinity-similarity-matrix" class="headerlink" title="1.affinity/similarity matrix"></a>1.affinity/similarity matrix</h4><p>$$s_{ij}=F(q_i)^TF(d_j)$$</p>
<p>其中，F(.) 是多层感知机，通常可选择的计算相似矩阵的方式有：</p>
<p>$$s_{ij}=q_i^TMd_j, s_{ij}=F[q_i;d_j]$$</p>
<p>以及在 BiDAF 和 QANet 中使用的 $s_{ij}=F[q_i;d_j;q_i\circ d_j]$.</p>
<h4 id="2-Extractive-pooling"><a href="#2-Extractive-pooling" class="headerlink" title="2. Extractive pooling"></a>2. Extractive pooling</h4><h5 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max-pooling"></a>max-pooling</h5><p>关注于另一个序列交互后，最匹配的那个词。</p>
<p>$$q’=soft(max_{col}(s))^Tq, d’=soft(max_{row}(s))^Td$$</p>
<p>soft(.) 是 softmax 函数。$q’,d’$ 是 co-attentive representations of q and d respectively.</p>
<h5 id="mean-pooling"><a href="#mean-pooling" class="headerlink" title="mean-pooling"></a>mean-pooling</h5><p>关注另一个句子的全部，取平均值。</p>
<p>$$q’=soft(mean_{col}(s))^Tq, d’=soft(mean_{row}(s))^Td$$</p>
<blockquote>
<p>each pooling operator has different impacts and can be intuitively understood as follows: max-pooling selects each word based on its maximum importance of all words in the other text. Mean-pooling is a more wholesome comparison, paying attention to a word based on its overall influence on the other text. This is usually dataset-dependent, regarded as a hyperparameter and is tuned to see which performs best on the held out set.  </p>
</blockquote>
<p>不同的 pooling 操作有不同的影响，获取的信息也不一样。 max-pooling根据每个单词在其他文本中所有单词的最大重要性选择每个单词。mean-pooling是基于每个词在其他文本上的总体影响来关注每个词。  </p>
<p>这其实是与数据集和任务相关的。可以看作超参数，然后调整看哪个在对应的任务和数据集上表现更佳。</p>
<h4 id="3-Alignment-Pooling"><a href="#3-Alignment-Pooling" class="headerlink" title="3. Alignment-Pooling"></a>3. Alignment-Pooling</h4><p>$$d_i’:=\sum^{l_q}<em>{j=1}\dfrac{exp(s</em>{ij})}{\sum_{k=1}^{l_q}exp(s_{ik})}q_j$$</p>
<p>其中 $d_i’$ 是 q 和 $d_i$ 的软对齐。直观的说，$d_i’$ 是关于 $d_i$ 的 ${q_j}^{l_q}_{j=1}$ 的加权和。</p>
<p>$$q_i’:=\sum^{l_d}<em>{j=1}\dfrac{exp(s</em>{ij})}{\sum_{k=1}^{l_d}exp(s_{ik})}d_j$$</p>
<p>$q_i’$ 是 $q_i$ 和 d 的软对齐。也就是，$q_i’$ 是 关于 $q_i$ 的 ${d_j}^{l_d}_{j=1}$ 的加权和。</p>
<h4 id="4-intra-Attention"><a href="#4-intra-Attention" class="headerlink" title="4. intra-Attention"></a>4. intra-Attention</h4><p>$$x_i’:=\sum^{l}<em>{j=1}\dfrac{exp(s</em>{ij})}{\sum_{k=1}^{l}exp(s_{ik})}x_j$$</p>
<p>也就是自注意力机制。相比 attention is all your need,可能就是相似矩阵不一样吧。</p>
<h3 id="Multi-Cast-Attention"><a href="#Multi-Cast-Attention" class="headerlink" title="Multi-Cast Attention"></a>Multi-Cast Attention</h3><h4 id="Casted-Attention"><a href="#Casted-Attention" class="headerlink" title="Casted Attention"></a>Casted Attention</h4><p>用 $x$ 来表示 q 或 d，$\overline x$ 表示 经过 co-attention 和 soft-attention alignment 后的序列表示 $q’, d’$.</p>
<p>$$f_c=F_c[\overline x, x]$$</p>
<p>$$f_m=F_m[\overline x \circ x]$$</p>
<p>$$f_s=F_m[\overline x-x]$$</p>
<p>其中 $\circ$ 是 Hadamard product. $F(.)$ 是压缩函数，将特征压缩到 scalar.</p>
<blockquote>
<p>Intuitively, what is achieved here is that we are modeling the influence of co-attention by comparing representations before and after co-attention. For soft-attention alignment, a critical note here is that x and $\overline x$ (though of equal lengths) have ‘exchanged’ semantics. In other words, in the case of q, $\overline q$ actually contains the aligned representation of d.</p>
</blockquote>
<h4 id="Compression-Function"><a href="#Compression-Function" class="headerlink" title="Compression Function"></a>Compression Function</h4><blockquote>
<p>The rationale for compression is simple and intuitive - we do not want to bloat subsequent layers with a high dimensional vector which consequently incurs parameter costs in subsequent layers. We investigate the usage of three compression functions, which are capable of reducing a n dimensional vector to a scalar.  </p>
</blockquote>
<p>本节定义了Fc(.) 使用的压缩函数，不希望使用高维向量膨胀后续层，这会在后续层中会产生参数成本。因此本文研究了三种压缩函数的用法，它们能够将n维向量减少到标量。</p>
<ul>
<li>Sum  </li>
</ul>
<p>Sum（SM）函数是一个非参数化函数，它对整个向量求和，并输出标量</p>
<p>$$F(x)=\sum_i^nx_i, x_i\in x$$</p>
<ul>
<li>Neural networks</li>
</ul>
<p>$$F(x)=RELU(W_cx+b)$$</p>
<p>其中 $W_c\in R^{n\times 1}$</p>
<ul>
<li>Factorization Machines</li>
</ul>
<p>因子分解机是一种通用机器学习技术，接受实值特征向量 $x\in R^n$ 并返回标量输出。</p>
<p><img src="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/04.png"></p>
<p>FM是表达模型，使用分解参数捕获特征之间的成对相互作用。 k是FM模型的因子数。</p>
<h4 id="Multi-Cast"><a href="#Multi-Cast" class="headerlink" title="Multi-Cast"></a>Multi-Cast</h4><p>我们的架构背后的关键思想是促进k个注意力投射，每个投射都用一个实值注意力提示来增强原始词向量。 对于每个query-document对，应用Co-Attention with mean-pooling，Co-Attention with max-Pooling和Co-Attention with alignment-pooling。 此外，将Intra-Attention分别单独应用于query和document。 每个注意力投射产生三个标量（每个单词），它们与词向量连接在一起。最终的投射特征向量是 $z\in R^{12}$。</p>
<p>因此，对于每个单词 w_{i} ，新的表示成为 $\bar{w_{i}}=[w_{i};z_{i}]$</p>
<h3 id="Long-Short-Term-Memory-Encoder"><a href="#Long-Short-Term-Memory-Encoder" class="headerlink" title="Long Short-Term Memory Encoder"></a>Long Short-Term Memory Encoder</h3><p>接下来，将带有casted attetnion的单词表示 $\bar{w_{1}},\bar{w_{2}},…,\bar{w_{l}}$ 传递到序列编码器层。采用标准的长短期记忆（LSTM）编码器.</p>
<blockquote>
<p>As such, the key idea behind casting attention as features right before this layer is that it provides the LSTM encoder with hints that provide information such as (1) longterm and global sentence knowledge and (2) knowledge between sentence pairs (document and query).  </p>
</blockquote>
<p><strong>LSTM在document和query之间共享权重。</strong> 关键思想是LSTM编码器通过使用非线性变换作为门控函数来学习表示序列依赖性的表示。因此，在该层之前引人注意力作为特征的关键思想是它为LSTM编码器提供了带有信息的提示，例如长期和全局句子知识和句子对（文档和查询）之间的知识。</p>
<ul>
<li>Pooling Operation  </li>
</ul>
<p>最后，在每个句子的隐藏状态 $h_{1},…h_{l}$ 上应用池化函数。将序列转换为固定维度的表示。</p>
<p>$$h=MeanMax[h_1,…,h_l]$$</p>
<p>所以得到的 q 和 d 的最终表示是 [1, hiden_size]?</p>
<h3 id="Prediction-Layer-and-Optimization"><a href="#Prediction-Layer-and-Optimization" class="headerlink" title="Prediction Layer and Optimization"></a>Prediction Layer and Optimization</h3><p>$$y_{out} = H_2(H_1([x_q; x_d ; x_q \circ x_d ; x_q − x_d ]))$$</p>
<p>其中 $H_1,H_2$ 是具有 ReLU 激活的 highway 网络层。然后将输出传递到最终线性 softmax 层。</p>
<p>$$y_{pred} = softmax(W_F · y_{out} + b_F )$$</p>
<p>其中 $W_F\in R^{h\times 2}, b_F\in R^2$.</p>
<p>使用 multi-class cross entropy，并带有 L2 正则化。</p>
<p><img src="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/05.png"></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/12/">上一页</a></div><div class="pagination-next"><a href="/page/14/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/12/">12</a></li><li><a class="pagination-link is-current" href="/page/13/">13</a></li><li><a class="pagination-link" href="/page/14/">14</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>