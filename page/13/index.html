<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-11-04T03:04:35.000Z" title="2018/11/4 上午11:04:35">2018-11-04</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.158Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/">text matching</a></span><span class="level-item">24 分钟读完 (大约3533个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/">论文笔记-Multi-cast Attention Networks</a></h1><div class="content"><p>paper: [Multi-Cast Attention Networks for Retrieval-based Question</p>
<p>Answering and Response Prediction](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.00778">https://arxiv.org/abs/1806.00778</a>)</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Our approach performs a series of soft attention</p>
</blockquote>
<p>operations, each time casting a scalar feature upon the inner word embeddings. The key idea is to provide a real-valued hint (feature) to a subsequent encoder layer and is targeted at improving the representation learning process.  </p>
<p>在 encoder layer 之前将 document 和 query 进行交互，然后将权重赋予到 document 和 query 之后，在通过 contextual/encoder layer 编码融合了上下文信息的向量表示。这样做的目地，是为后续层提供提示（特征），提升表示学习的性能。</p>
<blockquote>
<p>The key idea of attention is to extract only the most relevant information that is useful for prediction. In the context of textual data, attention learns to weight words and sub-phrases within documents based on how important they are. In the same vein, co-attention mechanisms [5, 28, 50, 54] are a form of attention mechanisms that learn joint pairwise attentions, with respect to both document and query.  </p>
</blockquote>
<p>attention 注意力的关键思想是仅提取对预测有用的最相关信息。在文本数据的上下文中，注意力学习根据文档中的单词和子短语的重要性来对它们进行加权。</p>
<blockquote>
<p>Attention is traditionally used and commonly imagined as a feature extractor. It’s behavior can be thought of as a dynamic form of pooling as it learns to select and compose different words to form the final document representation.  </p>
</blockquote>
<p>传统的 attention 可以看做为一个特征提取器。它的行为可以被认为是一种动态的pooing形式，因为它学习选择和组合不同的词来形成最终的文档表示，。</p>
<blockquote>
<p>This paper re-imagines attention as a form of feature augmentation method. Attention is casted with the purpose of not compositional learning or pooling but to provide hints for subsequent layers. To the best of our knowledge, this is a new way to exploit attention in neural ranking models.  </p>
</blockquote>
<p>这篇 paper 将 attention 重新设想为一种特征增强的方式，Attention的目的不是组合学习或汇集，而是为后续层提供提示（特征）。这是一种在神经排序模型中的新方法。</p>
<p>不管这篇paper提供的新的 attention 使用方式是否是最有效的，但这里对 attention 的很多解释让人耳目一新，可以说理解的很透彻了。</p>
<blockquote>
<p> An obvious drawback which applies to many existing models is that they are generally restricted to one attention variant. In the case where one or more attention calls are used (e.g., co-attention and intra-attention, etc.), concatenation is generally used to fuse representations [20, 28]. Unfortunately, this incurs cost in subsequent layers by doubling the representation size per call.  </p>
</blockquote>
<p>很多 paper 中只受限于一种 attention，这明显是不够好的。也有使用多种 attention 的，比如 co-attention 和 intra-attention， 然后直接拼接起来。这样会使得接下来的 modeling layer 维度加倍。。（在 ai challenge 的比赛中就是这么干的。。没啥不好啊。。）</p>
<blockquote>
<p>The rationale for desiring more than one attention call is intuitive. In [20, 28], Co-Attention and Intra-Attention are both used because each provides a different view of the document pair, learning high quality representations that could be used for prediction. Hence, this can significantly improve performance.  </p>
</blockquote>
<p>直觉上，使用多种 multi-attention 是靠谱的。 co-attention 和 intra-attention 提供了不同的视角去审视 document,用以学习高质量的向量表示。    </p>
<p>关于各种 attention：  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35041012">https://zhuanlan.zhihu.com/p/35041012</a>  </p>
</li>
<li><p>[50] co-attention: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.01604.pdf">dynamic coattention networks for question answering</a>  </p>
</li>
<li><p>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.03609">Attentive Pooling Networks</a>  </p>
</li>
<li><p>[28] [Inter-Weighted Alignment</p>
</li>
</ul>
<p>Network for Sentence Pair Modeling](<a target="_blank" rel="noopener" href="https://aclanthology.info/pdf/D/D17/D17-1122.pdf">https://aclanthology.info/pdf/D/D17/D17-1122.pdf</a>)   </p>
<ul>
<li><p>[54] <a target="_blank" rel="noopener" href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14611">Attentive Interactive Neural Networks for Answer Selection in Community Question</a></p>
</li>
<li><p>intra-attention:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all your need</a></p>
</li>
</ul>
<blockquote>
<p>Moreover, Co-Attention also comes in different flavors and can either be used with extractive max-mean pooling [5, 54] or alignment-based pooling [3, 20, 28]. Each co-attention type produces different document representations. In max-pooling, signals are extracted based on a word’s largest contribution to the other text sequence. Mean-pooling calculates its contribution to the overall sentence. Alignment-pooling is another flavor of co-attention, which aligns semantically similar sub-phrases together.  </p>
</blockquote>
<p>co-attention可以用于提取max-mean pooling或alignment-based pooling。每种co-attention都会产生不同的文档表示。在max-pooling中，基于单词对另一文本序列的最大贡献来提取特征；mean-pooling计算其对整个句子的贡献；alignment-based pooling是另一种协同注意力机制，它将语义相似的子短语对齐在一起。因此，不同的pooling操作提供了不同的句子对视图。</p>
<ul>
<li><p>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.06038">Enhanced LSTM for Natural Language Inference</a>  </p>
</li>
<li><p>[20] [A</p>
</li>
</ul>
<p>Decomposable Attention Model for Natural Language Inference](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.01933">https://arxiv.org/abs/1606.01933</a>)  </p>
<blockquote>
<p>Our approach is targeted at serving two important purposes: (1) It removes the need for architectural engineering of this component by enabling attention to be called for an arbitrary k times with hardly any consequence and (2) concurrently it improves performance by modeling multiple views via multiple attention calls. As such, our method is in similar spirit to multi-headed attention, albeit efficient. To this end, we introduce Multi-Cast Attention Networks (MCAN), a new deep learning architecture for a potpourri of tasks in the question answering and conversation modeling domains.  </p>
</blockquote>
<p>两个方面的贡献：  </p>
<p>（1）消除调用任意k次注意力机制所需架构工程的需要，且不会产生任何后果。  </p>
<p>（2）通过多次注意力调用建模多个视图以提高性能，与multi-headed attention相似。</p>
<blockquote>
<p> In our approach, attention is casted, in contrast to the most other works that use it as a pooling operation. We cast co-attention multiple times, each time returning a compressed scalar feature that is re-attached to the original word representations. The key intuition is that compression enables scalable casting of multiple attention calls, aiming to provide subsequent layers with a hint of not only global knowledge but also cross sentence knowledge.  </p>
</blockquote>
<p>与大多数其他用作池化操作的工作相反，在我们的方法中，注意力被投射。通过多次投射co-attention，每次返回一个压缩的标量特征，重新附加到原始的单词表示上。压缩函数可以实现多个注意力调用的可扩展投射，旨在不仅为后续层提供全局知识而且还有跨句子知识的提示（特征）。</p>
<h2 id="Model-Architecture-Multi-cast-Attention-Networks"><a href="#Model-Architecture-Multi-cast-Attention-Networks" class="headerlink" title="Model Architecture: Multi-cast Attention Networks"></a>Model Architecture: Multi-cast Attention Networks</h2><p><img src="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/01.png"></p>
<p>Figure 1: Illustration of our proposed Multi-Cast Attention Networks (Best viewed in color). MCAN is a wide multi-headed attention architecture that utilizes compression functions and attention as features.</p>
<p>模型输入是 document/query 语句对。</p>
<h3 id="Input-Encoder"><a href="#Input-Encoder" class="headerlink" title="Input Encoder"></a>Input Encoder</h3><h4 id="embedding-layer"><a href="#embedding-layer" class="headerlink" title="embedding layer"></a>embedding layer</h4><p>映射到向量空间： $w\in W^d$</p>
<h4 id="Highway-Encoder："><a href="#Highway-Encoder：" class="headerlink" title="Highway Encoder："></a>Highway Encoder：</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.00387.pdf">Highway Networks</a>可以对任意深度的网络进行优化。这是通过一种控制穿过神经网络的信息流的闸门机制所实现的。通过这种机制，神经网络可以提供通路，让信息穿过后却没有损失，将这种通路称为information highways。即highway networks主要解决的问题是网络深度加深、梯度信息回流受阻造成网络训练困难的问题。</p>
</blockquote>
<blockquote>
<p>highway encoders can be interpreted as data-driven word filters. As such, we can imagine them to parametrically learn which words have an inclination to be important and not important to the task at hand. For example, filtering stop words and words that usually do not contribute much to the prediction. Similar to recurrent models that are gated in nature, this highway encoder layer controls how much information (of each word) is flowed to the subsequent layers.  </p>
</blockquote>
<p>在本文模型中，每个词向量都通过highway编码器层。highway网络是门控非线性变换层，它控制后续层的信息流。许多工作都采用一种训练过的投影层来代替原始词向量。这不仅节省了计算成本，还减少了可训练参数的数量。本文将此投影层扩展为使用highway编码器，可以解释为数据驱动的词滤波器，它们可以参数化地了解哪些词对于任务具有重要性和重要性。例如，删除通常对预测没有多大贡献的停用词和单词。与自然门控的循环模型类似，highway编码器层控制每个单词流入下一层多少信息。  </p>
<p>$$y=H(x,W_H)\cdot T(x,W_T) + (1-T(x,W_T))\cdot x$$</p>
<p>其中 $W_H, W_T\in R^{r\times d}$ 是可学习参数. H(.) 和 T(.) 分别是全连接加上 relu 和 sigmoid 的函数，用以控制信息的流向下一层。</p>
<h3 id="co-attention"><a href="#co-attention" class="headerlink" title="co-attention"></a>co-attention</h3><blockquote>
<p>Co-Attention [50] is a pairwise attention mechanism that enables</p>
</blockquote>
<p>attending to text sequence pairs jointly. In this section, we introduce four variants of attention, i.e., (1) max-pooling, (2) mean-pooling, (3) alignment-pooling, and finally (4) intra-attention (or self attention).  </p>
<p>协同注意力机制是成对的注意力机制，能够同时关注文本序列对。作者引入了 4 中注意力机制。</p>
<h4 id="1-affinity-similarity-matrix"><a href="#1-affinity-similarity-matrix" class="headerlink" title="1.affinity/similarity matrix"></a>1.affinity/similarity matrix</h4><p>$$s_{ij}=F(q_i)^TF(d_j)$$</p>
<p>其中，F(.) 是多层感知机，通常可选择的计算相似矩阵的方式有：</p>
<p>$$s_{ij}=q_i^TMd_j, s_{ij}=F[q_i;d_j]$$</p>
<p>以及在 BiDAF 和 QANet 中使用的 $s_{ij}=F[q_i;d_j;q_i\circ d_j]$.</p>
<h4 id="2-Extractive-pooling"><a href="#2-Extractive-pooling" class="headerlink" title="2. Extractive pooling"></a>2. Extractive pooling</h4><h5 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max-pooling"></a>max-pooling</h5><p>关注于另一个序列交互后，最匹配的那个词。</p>
<p>$$q’=soft(max_{col}(s))^Tq, d’=soft(max_{row}(s))^Td$$</p>
<p>soft(.) 是 softmax 函数。$q’,d’$ 是 co-attentive representations of q and d respectively.</p>
<h5 id="mean-pooling"><a href="#mean-pooling" class="headerlink" title="mean-pooling"></a>mean-pooling</h5><p>关注另一个句子的全部，取平均值。</p>
<p>$$q’=soft(mean_{col}(s))^Tq, d’=soft(mean_{row}(s))^Td$$</p>
<blockquote>
<p>each pooling operator has different impacts and can be intuitively understood as follows: max-pooling selects each word based on its maximum importance of all words in the other text. Mean-pooling is a more wholesome comparison, paying attention to a word based on its overall influence on the other text. This is usually dataset-dependent, regarded as a hyperparameter and is tuned to see which performs best on the held out set.  </p>
</blockquote>
<p>不同的 pooling 操作有不同的影响，获取的信息也不一样。 max-pooling根据每个单词在其他文本中所有单词的最大重要性选择每个单词。mean-pooling是基于每个词在其他文本上的总体影响来关注每个词。  </p>
<p>这其实是与数据集和任务相关的。可以看作超参数，然后调整看哪个在对应的任务和数据集上表现更佳。</p>
<h4 id="3-Alignment-Pooling"><a href="#3-Alignment-Pooling" class="headerlink" title="3. Alignment-Pooling"></a>3. Alignment-Pooling</h4><p>$$d_i’:=\sum^{l_q}<em>{j=1}\dfrac{exp(s</em>{ij})}{\sum_{k=1}^{l_q}exp(s_{ik})}q_j$$</p>
<p>其中 $d_i’$ 是 q 和 $d_i$ 的软对齐。直观的说，$d_i’$ 是关于 $d_i$ 的 ${q_j}^{l_q}_{j=1}$ 的加权和。</p>
<p>$$q_i’:=\sum^{l_d}<em>{j=1}\dfrac{exp(s</em>{ij})}{\sum_{k=1}^{l_d}exp(s_{ik})}d_j$$</p>
<p>$q_i’$ 是 $q_i$ 和 d 的软对齐。也就是，$q_i’$ 是 关于 $q_i$ 的 ${d_j}^{l_d}_{j=1}$ 的加权和。</p>
<h4 id="4-intra-Attention"><a href="#4-intra-Attention" class="headerlink" title="4. intra-Attention"></a>4. intra-Attention</h4><p>$$x_i’:=\sum^{l}<em>{j=1}\dfrac{exp(s</em>{ij})}{\sum_{k=1}^{l}exp(s_{ik})}x_j$$</p>
<p>也就是自注意力机制。相比 attention is all your need,可能就是相似矩阵不一样吧。</p>
<h3 id="Multi-Cast-Attention"><a href="#Multi-Cast-Attention" class="headerlink" title="Multi-Cast Attention"></a>Multi-Cast Attention</h3><h4 id="Casted-Attention"><a href="#Casted-Attention" class="headerlink" title="Casted Attention"></a>Casted Attention</h4><p>用 $x$ 来表示 q 或 d，$\overline x$ 表示 经过 co-attention 和 soft-attention alignment 后的序列表示 $q’, d’$.</p>
<p>$$f_c=F_c[\overline x, x]$$</p>
<p>$$f_m=F_m[\overline x \circ x]$$</p>
<p>$$f_s=F_m[\overline x-x]$$</p>
<p>其中 $\circ$ 是 Hadamard product. $F(.)$ 是压缩函数，将特征压缩到 scalar.</p>
<blockquote>
<p>Intuitively, what is achieved here is that we are modeling the influence of co-attention by comparing representations before and after co-attention. For soft-attention alignment, a critical note here is that x and $\overline x$ (though of equal lengths) have ‘exchanged’ semantics. In other words, in the case of q, $\overline q$ actually contains the aligned representation of d.</p>
</blockquote>
<h4 id="Compression-Function"><a href="#Compression-Function" class="headerlink" title="Compression Function"></a>Compression Function</h4><blockquote>
<p>The rationale for compression is simple and intuitive - we do not want to bloat subsequent layers with a high dimensional vector which consequently incurs parameter costs in subsequent layers. We investigate the usage of three compression functions, which are capable of reducing a n dimensional vector to a scalar.  </p>
</blockquote>
<p>本节定义了Fc(.) 使用的压缩函数，不希望使用高维向量膨胀后续层，这会在后续层中会产生参数成本。因此本文研究了三种压缩函数的用法，它们能够将n维向量减少到标量。</p>
<ul>
<li>Sum  </li>
</ul>
<p>Sum（SM）函数是一个非参数化函数，它对整个向量求和，并输出标量</p>
<p>$$F(x)=\sum_i^nx_i, x_i\in x$$</p>
<ul>
<li>Neural networks</li>
</ul>
<p>$$F(x)=RELU(W_cx+b)$$</p>
<p>其中 $W_c\in R^{n\times 1}$</p>
<ul>
<li>Factorization Machines</li>
</ul>
<p>因子分解机是一种通用机器学习技术，接受实值特征向量 $x\in R^n$ 并返回标量输出。</p>
<p><img src="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/04.png"></p>
<p>FM是表达模型，使用分解参数捕获特征之间的成对相互作用。 k是FM模型的因子数。</p>
<h4 id="Multi-Cast"><a href="#Multi-Cast" class="headerlink" title="Multi-Cast"></a>Multi-Cast</h4><p>我们的架构背后的关键思想是促进k个注意力投射，每个投射都用一个实值注意力提示来增强原始词向量。 对于每个query-document对，应用Co-Attention with mean-pooling，Co-Attention with max-Pooling和Co-Attention with alignment-pooling。 此外，将Intra-Attention分别单独应用于query和document。 每个注意力投射产生三个标量（每个单词），它们与词向量连接在一起。最终的投射特征向量是 $z\in R^{12}$。</p>
<p>因此，对于每个单词 w_{i} ，新的表示成为 $\bar{w_{i}}=[w_{i};z_{i}]$</p>
<h3 id="Long-Short-Term-Memory-Encoder"><a href="#Long-Short-Term-Memory-Encoder" class="headerlink" title="Long Short-Term Memory Encoder"></a>Long Short-Term Memory Encoder</h3><p>接下来，将带有casted attetnion的单词表示 $\bar{w_{1}},\bar{w_{2}},…,\bar{w_{l}}$ 传递到序列编码器层。采用标准的长短期记忆（LSTM）编码器.</p>
<blockquote>
<p>As such, the key idea behind casting attention as features right before this layer is that it provides the LSTM encoder with hints that provide information such as (1) longterm and global sentence knowledge and (2) knowledge between sentence pairs (document and query).  </p>
</blockquote>
<p><strong>LSTM在document和query之间共享权重。</strong> 关键思想是LSTM编码器通过使用非线性变换作为门控函数来学习表示序列依赖性的表示。因此，在该层之前引人注意力作为特征的关键思想是它为LSTM编码器提供了带有信息的提示，例如长期和全局句子知识和句子对（文档和查询）之间的知识。</p>
<ul>
<li>Pooling Operation  </li>
</ul>
<p>最后，在每个句子的隐藏状态 $h_{1},…h_{l}$ 上应用池化函数。将序列转换为固定维度的表示。</p>
<p>$$h=MeanMax[h_1,…,h_l]$$</p>
<p>所以得到的 q 和 d 的最终表示是 [1, hiden_size]?</p>
<h3 id="Prediction-Layer-and-Optimization"><a href="#Prediction-Layer-and-Optimization" class="headerlink" title="Prediction Layer and Optimization"></a>Prediction Layer and Optimization</h3><p>$$y_{out} = H_2(H_1([x_q; x_d ; x_q \circ x_d ; x_q − x_d ]))$$</p>
<p>其中 $H_1,H_2$ 是具有 ReLU 激活的 highway 网络层。然后将输出传递到最终线性 softmax 层。</p>
<p>$$y_{pred} = softmax(W_F · y_{out} + b_F )$$</p>
<p>其中 $W_F\in R^{h\times 2}, b_F\in R^2$.</p>
<p>使用 multi-class cross entropy，并带有 L2 正则化。</p>
<p><img src="/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/05.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-10-15T11:34:48.000Z" title="2018/10/15 下午7:34:48">2018-10-15</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.158Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">21 分钟读完 (大约3214个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">论文笔记-CNN与自然语言处理</a></h1><div class="content"><p>最近在参加 AI challenge 观点型阅读理解的比赛。数据集形式如下：</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/01.png"></p>
<p>最开始尝试的模型主要包括几个部分：</p>
<ul>
<li><p><strong>Embedding:</strong> 使用预训练的中文词向量。  </p>
</li>
<li><p><strong>Encoder:</strong> 基于 Bi-GRU 对 passage,query 和 alternatives 进行编码处理。  </p>
</li>
<li><p><strong>Attention:</strong> 用 trilinear 的方式，并 mask 之后得到相似矩阵，然后采用类似于 <a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/08/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QA%20BiDAF/">BiDAF</a> 中的形式 bi-attention flow 得到 attened passage.  </p>
</li>
<li><p><strong>contextual:</strong> 用 Bi-GRU 对 attened passage 进行编码，得到 fusion.  </p>
</li>
<li><p><strong>match</strong> 使用 attention pooling 的方式将 fusion 和 enc_answer 转换为单个 vector. 然后使用 cosin 进行匹配计算得到最相似的答案。  </p>
</li>
</ul>
<p>目前能得到的准确率是 0.687. 距离第一名差了 0.1…其实除了换模型，能提升和改进的地方是挺多的。</p>
<ul>
<li><p>可以用 ELMO 或 wordvec 先对训练集进行预训练得到自己的词向量。  </p>
</li>
<li><p>attention 层可以使用更丰富的方式，很多paper 中也有提到。甚至可以加上人工提取的特征。比如苏剑林 <a target="_blank" rel="noopener" href="https://kexue.fm/archives/5409">blog</a> 中提到的。</p>
</li>
<li><p>还有个很重要的就是 match 部分， attention pooling 是否可以换成其他更好的方式？</p>
</li>
</ul>
<p>但是，不断尝试各种模型的前提也要考虑速度吧。。rnn 实在是太慢了，所以决定试试 CNN 的方式来处理 NLP 的任务。</p>
<p>关于使用 CNN 来处理阅读理解的任务的大作还是挺多的，这里主要介绍这两篇：  </p>
<ul>
<li><p>Facebook: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.03122">Convolutional Sequence to Sequence Learning</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.04352">Fast Reading Comprehension with ConvNets</a></p>
</li>
</ul>
<h1 id="ConvS2S"><a href="#ConvS2S" class="headerlink" title="ConvS2S"></a>ConvS2S</h1><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.03122">Convolutional Sequence to Sequence Learning</a></p>
<p>这篇 paper 对应的 NLP 任务是机器翻译，除了用 CNN 对 sentence 进行编码之外，其核心是在 decoder 的时候也使用 CNN. 对于阅读理解来说，能够借用的是其编码 sentence 的方式。但这里作为学习，也多了解一下 decoder 吧～</p>
<p>对文本来说，看到 CNN 我们首先想到的是 cnn 能有效利用局部信息，提取出局部特征，所以适合做文本分类。但是对于 机器翻译、阅读理解这样的需要考虑全局信息的任务，CNN 似乎看起来并不那么有效。而且在 decoder 的时候，词的生成是 one by one 的，下一个词的生成是依赖于上一个词的。所以在 decoder 中使用 RNN 也是很自然而然的。</p>
<p>Facebook 的这篇 paper 就改变了这些传统的思维，不仅用 CNN 编码全局信息，而且还能 decoder.</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher layers.  </p>
</blockquote>
<p>多层 CNN 具有层级表示结构，相邻的词之间在较低层的 layer 交互，距离较远的词在较高层的 layer 交互（交互的目的就是语义消歧）。</p>
<blockquote>
<p>Hierarchical structure provides a shorter path to capture long-range dependencies compared to the chain structure modeled by recurrent networks, e.g. we can obtain a feature representation capturing relationships within a window of n words by applying only O(n/k) convolutional operations for kernels of width k, compared to a linear number O(n) for recurrent neural networks.  </p>
</blockquote>
<p>层级结构提供了一个更短的路径来获取长期依赖。比如相距为 n 的两个词，在 rnn 中交互需要的步数是 O(n),在层级 CNN 中需要 O(n/k).这样减少了非线性的操作，降低了梯度消失的情况。所以这两个词的交互效果会更好～</p>
<blockquote>
<p>Inputs to a convolutional network are fed through a constant number of kernels and non-linearities, whereas recurrent networks apply up to n operations and non-linearities to the first word and only a single set of operations to the last word. Fixing the number of nonlinearities applied to the inputs also eases learning.  </p>
</blockquote>
<p>输入到 CNN 中每个词都会经历固定的 kernel 和 非线性操作。而输入到 RNN 的，第一个词需要经过 n 个 operations，最后一个词只经历了一个 operations. 作者认为固定的操作更容易学习。  </p>
<p>这一点我个人认为并不一定就是合理的，本来一个句子中不同词的重要性就是不一样的。  </p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Ｍodel Architecture"></a>Ｍodel Architecture</h2><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/02.png"></p>
<p>模型分为以下几个部分：  </p>
<ul>
<li><p>position embedding  </p>
</li>
<li><p>convolution block structure  </p>
</li>
<li><p>Multi-step attention  </p>
</li>
</ul>
<h3 id="position-encoding"><a href="#position-encoding" class="headerlink" title="position encoding"></a>position encoding</h3><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/05.png"></p>
<p>这部分在很多地方都出现过了，在没有 rnn 的情况下，都会用 PE 来编码位置信息。但是在这篇 paper 中，作者通过实验发现，PE 作用似乎并不是很重要。  </p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/03.png"></p>
<h3 id="convolution-blocks"><a href="#convolution-blocks" class="headerlink" title="convolution blocks"></a>convolution blocks</h3><p>作者使用的是门激活机制， GLU, gate linear units.  </p>
<blockquote>
<p>来自于 paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.08083">Language modeling with gated convolutional networks</a>  </p>
</blockquote>
<p>在这篇 paper 中，作者用无监督的方式，来训练语言模型，将 CNN 得到的语言模型与 LSTM 进行对比。</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/06.png"></p>
<p>也就是:</p>
<p>$$h_l=(X<em>W+b)\otimes \sigma(X</em>V+c)$$</p>
<blockquote>
<p>The output of each layer is a linear projection X ∗ W + b modulated by the gates σ(X ∗ V + c). Similar to LSTMs, these gates multiply each element of the matrix X ∗W+b</p>
</blockquote>
<p>and control the information passed on in the hierarchy.</p>
<p>如果是 LSTM-style，应该是 GTU：</p>
<p>$$h_i^l=tanh(X<em>W+b)\otimes \sigma(X</em>V+c)$$</p>
<p>作者将两者进行了对比，发现 GLU 效果更好。</p>
<p><strong>residual connection:</strong> 为了得到更 deep 的卷积神经网络，作者增加了残差链接。</p>
<p>$$h_i^l=v(W^l[h_{i-k/2}^{l-1},…,h_{i+k/2}^{l-1}]+b_w^l)+h_i^{l-1}$$</p>
<p>卷积的整个过程：  </p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/07.png"></p>
<p>论文中举了这样一个例子：  </p>
<blockquote>
<p>For instance, stacking 6 blocks with k = 5 results in an input field of 25 elements, i.e. each output depends on 25 inputs. Non-linearities allow the networks to exploit the full input field, or to focus on fewer elements if needed.  </p>
</blockquote>
<p>这个怎么算的呢？看下图：</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/09.jpg"></p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/10.jpg"></p>
<p>从上图中可以看到，当 k=3 时，3 个 blocks，第三层中的每一个输入都与输入中的 7 列有关。所以计算方式是 k + (k-1)* (blocks-1).</p>
<p>一维卷积和二维卷积的区别：</p>
<ul>
<li><p>ConvS2S 是 1D 卷积，kernel 只是在时间维度上平移，且 stride 的固定 size 为1,这是因为语言不具备图像的可伸缩性，图像在均匀的进行降采样后不改变图像的特征，而一个句子间隔着取词，意思就会改变很多了。  </p>
</li>
<li><p>在图像中一个卷积层往往有多个 filter，以获取图像不同的 pattern，但是在 ConvS2S 中，每一层只有一个 filter。一个句子进入 filter 的数据形式是 [1, n, d]. 其中 n 为句子长度， filter 对数据进行 n 方向上卷积，而 d 是词的向量维度，可以理解为 channel，与彩色图片中的 rgb 三个 channel 类似。</p>
</li>
</ul>
<blockquote>
<p>Facebook 在设计时，并没有像图像中常做的那样，每一层只设置一个 filter。这样做的原因，一是为了简化模型，加速模型收敛，二是他们认为一个句子的 pattern 要较图像简单很多，通过每层设置一个 filter，逐层堆叠后便能抓到所有的 pattern. 更有可能的原因是前者。因为在 transorfmer 中，multi-head attention 多头聚焦取得了很好的效果，说明一个句子的 pattern 是有多个的.  </p>
</blockquote>
<p>这段话是有问题的吧？ filter 的个数难道不是 2d吗？ 只不过这里说的 transorfmer 的多头聚焦是值得聚焦到一个词向量中的部分维度。记得在 cs224d 中 manning 曾经讲过一个例子，经过训练或词与词之间的交互后，词向量中的部分维度发生了变化。</p>
<p>在 paper 中，卷积核的尺寸大小是 $W\in R^{2d\times kd}$.</p>
<blockquote>
<p>For encoder networks we ensure that the output of the convolutional layers matches the input length by padding the input at each layer. However, for decoder networks we have to take care that no future information is available to the decoder (Oord et al., 2016a). Specifically, we pad the input by k − 1 elements on both the left and right side by zero vectors, and then remove k elements from the end of the convolution output.</p>
</blockquote>
<p>在 encoder 和 decoder 网络中，padding 的方式是不一样的。因为在 decoder 的时候不能考虑未来信息.  </p>
<p>在 encoder 时，将 (k-1) pad 到左右两边，保证卷积层的长度不变。  </p>
<p>在 decoder 中，将 (k-1) pad 到句子的左边。因此生成的词依旧是 one by one.</p>
<h3 id="Multi-step-Attention"><a href="#Multi-step-Attention" class="headerlink" title="Multi-step Attention"></a>Multi-step Attention</h3><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/08.gif"></p>
<p>$$d_i^l=W_d^lh_i^l+b_d^l+g_i$$</p>
<p>$$a_{ij}^l=\dfrac{exp(d_i^l\cdot z_j^u)}{\sum_{t=1}^mexp(d_i^l\cdot z_j^u)}$$</p>
<p>$$c_i^l=\sum_{j=1}^ma_{ij}^l(z_j^u+e_j)$$</p>
<p>上式中，l 表示 decoder 中卷积层的层数，i 表示时间步。  </p>
<p>实际上跟 rnn 的 decoder 还是比较接近的。  </p>
<ul>
<li><p>在训练阶段是 teacher forcing, 卷积核 $W_d^l$ 在 target sentence $h^l$ 上移动做卷积得到 $(W_d^lh_i^l + b_d^l)$，类似与 rnn-decoder 中的隐藏状态。然后加上上一个词的 embedding $g_i$,得到 $d_i^l$.  </p>
</li>
<li><p>与 encdoer 得到的 source sentence 做交互，通过 softmax 得到 attention weights $a_{ij}^l$.  </p>
</li>
<li><p>得到 attention vector 跟 rnn-decoder 有所不同，这里加上了 input element embedding $e_j$.</p>
</li>
</ul>
<p><strong>至于这里为什么要加 $e_j$?</strong>  </p>
<blockquote>
<p>We found adding e_j to be beneficial and it resembles key-value memory networks where the keys are the z_j^u and the values are the z^u_j + e_j (Miller et al., 2016). Encoder outputs z_j^u represent potentially large input contexts and e_j provides point information about a specific input element that is useful when making a prediction. Once c^l_i has been computed, it is simply added to the output of the corresponding decoder layer h^l_i.</p>
</blockquote>
<p>$z_j^u$ 表示更丰富的信息，而 $e_j$ 能够能具体的指出输入中对预测有用的信息。还是谁用谁知道吧。。</p>
<p><strong>关于 multi-hop attention:</strong>  </p>
<blockquote>
<p>This can be seen as attention with multiple ’hops’ (Sukhbaatar et al., 2015) compared to single step attention (Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016). In particular, the attention of the first layer determines a useful source context which is then fed to the second layer that takes this information into account when computing attention etc. The decoder also has immediate access to the attention history of the k − 1 previous time steps because the conditional inputs $c^{l-1}_{i−k}, . . . , c^{l-1}<em>i$ are part of $h^{l-1}</em>{i-k}, . . . , h^{l-1}_i$ which are input to $h^l_i$. This makes it easier for the model to take into account which previous inputs have been attended to already compared to recurrent nets where this information is in the recurrent state and needs to survive several non-linearities. Overall, our attention mechanism considers which words we previously attended to (Yang et al., 2016) and performs multiple attention ’hops’ per time step. In Appendix §C, we plot attention scores for a deep decoder and show that at different layers, different portions of the source are attended to.</p>
</blockquote>
<p>这个跟 memory networks 中的 multi-hop 是有点类似。</p>
<h1 id="FAST-READING-COMPREHENSION-WITH-CONVNETS"><a href="#FAST-READING-COMPREHENSION-WITH-CONVNETS" class="headerlink" title="FAST READING COMPREHENSION WITH CONVNETS"></a>FAST READING COMPREHENSION WITH CONVNETS</h1><p>Gated Linear Dilated Residual Network (GLDR):   </p>
<p>a combination of <strong>residual networks</strong> (He et al., 2016), <strong>dilated convolutions</strong> (Yu &amp; Koltun, 2016) and <strong>gated linear units (Dauphin et al., 2017)</strong>.</p>
<h2 id="text-understanding-with-dilated-convolution"><a href="#text-understanding-with-dilated-convolution" class="headerlink" title="text understanding with dilated convolution"></a>text understanding with dilated convolution</h2><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/11.png"></p>
<p>kernel:$k=[k_{-l},k_{-l+1},…,k_l]$, size=$2l+1$    </p>
<p>input: $x=[x_1,x_2,…,x_n]$  </p>
<p>dilation: d</p>
<p>卷积可以表示为：</p>
<p>$$(k*x)_ t=\sum_{i=-l}^lk_i\cdot x_{t + d\cdot i}$$</p>
<p><strong>为什么要使用膨胀卷积呢？ Why Dilated convolution?</strong>  </p>
<blockquote>
<p>Repeated dilated convolution (Yu &amp; Koltun, 2016) increases the receptive region of ConvNet outputs exponentially with respect to the network depth, which results in drastically shortened computation paths.   </p>
</blockquote>
<p>能够显著缩短两个词之间的计算路径。</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/12.png"></p>
<p>作者将 GLDR 和 self-attention,以及 RNN 进行了对比，<strong>input sequence length n, network width w, kernel size k, and network depth D</strong>.</p>
<h2 id="model-Architecture"><a href="#model-Architecture" class="headerlink" title="model Architecture"></a>model Architecture</h2><p>作者与 BiDAF 和 DrQA 进行了对比，将 BiDAF 和 DrQA 中的 BiLSTM 部分替换成 GLDR Convolution.</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/13.png"></p>
<blockquote>
<p>The receptive field of this convolutional network grows</p>
</blockquote>
<p>exponentially with depth and soon encompasses a long sequence, essentially enabling it to capture</p>
<p>similar long-term dependencies as an actual sequential model.  </p>
<p>感受野的尺寸大小指数增加，能够迅速压缩 long sentence,并 capture 长期依赖。</p>
<blockquote>
<p>Convolutional BiDAF. In our convolutional version of BiDAF, we replaced all bidirectional LSTMs with GLDRs . We have two 5-layer GLDRs in the contextual layer whose weights are un-tied. In the modeling layer, a 17-layer GLDR with dilation 1, 2, 4, 8, 16 in the first 5 residual blocks is used, which results in a reception region of 65 words. A 3-layer GLDR replaces the bidirectional LSTM in the output layer. For simplicity, we use same-padding and kernel size 3 for all convolutions unless specified. The hidden size of all GLDRs is 100 which is the same as the LSTMs in BiDAF.  </p>
</blockquote>
<p>具体网络结构，实际参数可以看 paper 实验部分。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-10-01T01:50:57.000Z" title="2018/10/1 上午9:50:57">2018-10-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.140Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">16 分钟读完 (大约2451个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/">论文笔记-batch,layer,weights normalization</a></h1><div class="content"><p>paper:  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">Batch Normalization</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1602.07868.pdf">weights Normalization</a>  </p>
</li>
</ul>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>在之前的笔记已经详细看过了:<a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/">深度学习-Batch Normalization</a></p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.  </p>
</blockquote>
<p>关于 batch normalisztion.</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/01.png"></p>
<p>从 Ng 的课上截来的一张图，全链接层相比卷积层更容易理解点，但形式上是一样的.  </p>
<p>样本数量是 m，第 l 层经过激活函数输出是第 l+1 层的输入，其中第 i 个神经元的值:  </p>
<p>线性输出： $z_i^l={w_i^l}^Th^l$.  </p>
<p>非线性输出： $h_i^{l+1} = a_i^l=f(z_i^l+b_i^l)$</p>
<p>其中 f 是非线性激活函数，$a_i^l$ 是下一层的 summed inputs. 如果 $a_i^l$ 的分布变化较大（change in a highly correlated way）,下一层的权重 $w^{l+1}$ 的梯度也会相应变化很大（反向传播中 $w^{l+1}$ 的梯度就是 $a_i^l$）。</p>
<p>Batch Normalization 就是将线性输出归一化。  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/02.png"></p>
<p>其中 $u_i^l$ 是均值，$\sigma_i^l$ 是方差。 $\overline a_i^l$ 是归一化之后的输出。 $g_i^l$ 是需要学习的参数，也就是 scale.</p>
<blockquote>
<p>有个疑问？为什么 BN 要在激活函数之前进行，而不是之后进行呢？</p>
</blockquote>
<p>上图中是单个样本，而所有的样本其实是共享层与层之间的参数的。样本与样本之间也存在差异，所以在某一个特征维度上进行归一化，（每一层其中的一个神经元可以看作一个特征维度）。</p>
<blockquote>
<p>batch normalization requires running averages of the summed input statistics. In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps.  </p>
</blockquote>
<p>BN 不是用于 RNN 是因为 batch 中的 sentence 长度不一致。我们可以把每一个时间步看作一个维度的特征提取，如果像 BN 一样在这个维度上进行归一化，显然在 RNN 上是行不通的。比如这个 batch 中最长的序列的最后一个时间步，他的均值就是它本身了，岂不是出现了 BN 在单个样本上训练的情况。</p>
<blockquote>
<p>In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case.  </p>
</blockquote>
<p>所以作者在这篇 paper 中提出了 Layer Normalization. 在单个样本上计算均值和方差进行归一化。然而是怎么进行的呢？</p>
<h3 id="Layer-Normalization-1"><a href="#Layer-Normalization-1" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>layer normalization 并不是在样本上求平均值和方差，而是在 hidden units 上求平均值和方差。</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/03.png"></p>
<p>其中 H 是 hidden units 的个数。</p>
<p>BN 和 LN 的差异：  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/04.jpg"></p>
<p>Layer normalisztion 在单个样本上取均值和方差，所以在训练和测试阶段都是一致的。</p>
<p>并且，尽管求均值和方差的方式不一样，但是在转换成 beta 和 gamma 的方式是一样的，都是在 channels 或者说 hidden_size 上进行的。</p>
<h3 id="Layer-normalized-recurrent-neural-networks"><a href="#Layer-normalized-recurrent-neural-networks" class="headerlink" title="Layer normalized recurrent neural networks"></a>Layer normalized recurrent neural networks</h3><blockquote>
<p>RNN is common among the NLP tasks to have different sentence lengths for different training cases. This is easy to deal with in an RNN because the same weights are used at every time-step. But when we apply batch normalization to an RNN in the obvious way, we need to to compute and store separate statistics for each time step in a sequence. This is problematic if a test sequence is longer than any of the training sequences. Layer normalization does not have such problem because its normalization terms depend only on the summed inputs to a layer at the current time-step. It also has only one set of gain and bias parameters shared over all time-steps.  </p>
</blockquote>
<p>这一部分也解释了 BN 不适用于 RNN 的原因，从 test sequence longer 的角度。RNN 的每个时间步计算共享参数权重.</p>
<p>$a^t=W_{hh}h^{t-1}+W_{xh}x^t$</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/05.png"></p>
<p>其中 b 和 g 是可学习的参数。</p>
<p><strong>layer normalize 在 LSTM 上的使用：</strong>  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/06.png"></p>
<h2 id="tensorflow-实现"><a href="#tensorflow-实现" class="headerlink" title="tensorflow 实现"></a>tensorflow 实现</h2><h3 id="batch-Normalization"><a href="#batch-Normalization" class="headerlink" title="batch Normalization"></a>batch Normalization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.training.moving_averages <span class="keyword">import</span> assign_moving_average</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.layers <span class="keyword">import</span> batch_norm</span><br><span class="line"></span><br><span class="line"><span class="comment">### batch normalization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span>(<span class="params">inputs, decay=<span class="number">0.9</span>, is_training=<span class="literal">True</span>, epsilon=<span class="number">1e-6</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs:  [batch, length, width, channels]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param is_training:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param eplison:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    pop_mean = tf.Variable(tf.zeros(inputs.shape[-<span class="number">1</span>]), trainable=<span class="literal">False</span>, name=<span class="string">&quot;pop_mean&quot;</span>)</span><br><span class="line"></span><br><span class="line">    pop_var = tf.Variable(tf.ones(inputs.shape[-<span class="number">1</span>]), trainable=<span class="literal">False</span>, name=<span class="string">&quot;pop_variance&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mean_and_var</span>():</span></span><br><span class="line"></span><br><span class="line">        axes = <span class="built_in">list</span>(<span class="built_in">range</span>(inputs.shape.ndims))</span><br><span class="line"></span><br><span class="line">        batch_mean, batch_var = tf.nn.moments(inputs, axes=axes)</span><br><span class="line"></span><br><span class="line">        moving_average_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (<span class="number">1</span>-decay))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 也可用 assign_moving_average(pop_mean, batch_mean, decay)</span></span><br><span class="line"></span><br><span class="line">        moving_average_var = tf.assign(pop_var, pop_var * decay + batch_var * (<span class="number">1</span>-decay))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 也可用 assign_moving_average(pop_var, batch_var, decay)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.control_dependencies([moving_average_mean, moving_average_var]):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> tf.identity(batch_mean), tf.identity(batch_var)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    mean, variance = tf.cond(tf.equal(is_training, <span class="literal">True</span>), update_mean_and_var,</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">lambda</span>: (pop_mean, pop_var))</span><br><span class="line"></span><br><span class="line">    beta = tf.Variable(initial_value=tf.zeros(inputs.get_shape()[-<span class="number">1</span>]), name=<span class="string">&quot;shift&quot;</span>)</span><br><span class="line"></span><br><span class="line">    gamma = tf.Variable(initial_value=tf.ones(inputs.get_shape()[-<span class="number">1</span>]), name=<span class="string">&quot;scale&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="layer-normalization"><a href="#layer-normalization" class="headerlink" title="layer normalization"></a>layer normalization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">hidden_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">whh = tf.random_normal(shape=[batch, hidden_size], mean=<span class="number">5.0</span>, stddev=<span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">whh_norm = tf.contrib.layers.layer_norm(inputs=whh, center=<span class="literal">True</span>, scale=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(whh)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(whh_norm)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh[<span class="number">0</span>]), tf.reduce_mean(whh[<span class="number">1</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh_norm[<span class="number">0</span>]), tf.reduce_mean(whh_norm[<span class="number">5</span>]), tf.reduce_mean(whh_norm[<span class="number">59</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh_norm[:,<span class="number">0</span>]), tf.reduce_mean(whh_norm[:,<span class="number">1</span>]), tf.reduce_mean(whh_norm[:,<span class="number">63</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(var)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(sess.run(var))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Tensor(&quot;random_normal:0&quot;, shape=(60, 64), dtype=float32)</span><br><span class="line"></span><br><span class="line">Tensor(&quot;LayerNorm/batchnorm/add_1:0&quot;, shape=(60, 64), dtype=float32)</span><br><span class="line"></span><br><span class="line">[5.3812757, 4.607581]</span><br><span class="line"></span><br><span class="line">[-1.4901161e-08, -2.9802322e-08, -3.7252903e-09]</span><br><span class="line"></span><br><span class="line">[-0.22264712, 0.14112064, -0.07268284]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;tf.Variable &#x27;LayerNorm/beta:0&#x27; shape=(64,) dtype=float32_ref&gt;</span><br><span class="line"></span><br><span class="line">[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"></span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"></span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"></span><br><span class="line">&lt;tf.Variable &#x27;LayerNorm/gamma:0&#x27; shape=(64,) dtype=float32_ref&gt;</span><br><span class="line"></span><br><span class="line">[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.</span><br><span class="line"></span><br><span class="line"> 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.</span><br><span class="line"></span><br><span class="line"> 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p> 发现一个很奇怪的问题， layer norm 是在每一个训练样本上求均值和方差，为啥 beta 和 gamma 的shape却是 [hidden_size]. 按理说不应该是 [batch,] 吗？ 带着疑问去看了源码，原来是这样的。。</p>
<p> 将源码用简介的方式写出来了：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm_mine</span>(<span class="params">inputs, epsilon=<span class="number">1e-12</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    inputs: [batch, sequence_len, hidden_size] or [batch, hidden_size]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    inputs_shape = inputs.shape</span><br><span class="line"></span><br><span class="line">    inputs_rank = inputs_shape.ndims</span><br><span class="line"></span><br><span class="line">    params_shape = inputs_shape[-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    beta, gamma = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> center:</span><br><span class="line"></span><br><span class="line">        beta = tf.get_variable(</span><br><span class="line"></span><br><span class="line">            name=<span class="string">&quot;beta&quot;</span>,</span><br><span class="line"></span><br><span class="line">            shape=params_shape,</span><br><span class="line"></span><br><span class="line">            initializer=tf.zeros_initializer(),</span><br><span class="line"></span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">        gamma = tf.get_variable(</span><br><span class="line"></span><br><span class="line">            name=<span class="string">&quot;gamma&quot;</span>,</span><br><span class="line"></span><br><span class="line">            shape=params_shape,</span><br><span class="line"></span><br><span class="line">            initializer=tf.ones_initializer(),</span><br><span class="line"></span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    norm_axes = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, inputs_rank))</span><br><span class="line"></span><br><span class="line">    mean, variance = tf.nn.moments(inputs, norm_axes, keep_dims=<span class="literal">True</span>)      <span class="comment"># [batch]</span></span><br><span class="line"></span><br><span class="line">    inv = tf.rsqrt(variance + epsilon)</span><br><span class="line"></span><br><span class="line">    inv *= gamma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inputs*inv + ((beta-mean)*inv <span class="keyword">if</span> beta <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> - mean * inv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">hidden_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">whh = tf.random_normal(shape=[batch, hidden_size], mean=<span class="number">5.0</span>, stddev=<span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">whh_norm = layer_norm_mine(whh)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>layer_norm_mine 得到的结果与源码一致。可以发现 计算均值和方差时， <code>tf.nn.moments</code> 中 <code>axes=[1:-1]</code>. （tf.nn.moments 中 axes 的含义是在这些维度上求均值和方差）. 也就是说得到的均值和方差确实是 [batch,]. 只是在转换成 beta 和 gamma 的分布时，依旧是在最后一个维度上进行的。有意思，所以最终的效果应该和 batch normalization 效果是一致的。只不过是否符合图像或文本的特性就另说了。</p>
<h3 id="LayerNormBasicLSTMCell"><a href="#LayerNormBasicLSTMCell" class="headerlink" title="LayerNormBasicLSTMCell"></a>LayerNormBasicLSTMCell</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNormBasicLSTMCell</span>(<span class="params">rnn_cell_impl.RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;LSTM unit with layer normalization and recurrent dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This class adds layer normalization and recurrent dropout to a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  basic LSTM unit. Layer normalization implementation is based on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1607.06450.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Layer Normalization&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  and is applied before the internal nonlinearities.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Recurrent dropout is base on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1603.05118</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Recurrent Dropout without Memory Loss&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               forget_bias=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               input_size=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=math_ops.tanh,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               layer_norm=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               norm_gain=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               norm_shift=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_keep_prob=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_prob_seed=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initializes the basic LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: int, The number of units in the LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      forget_bias: float, The bias added to forget gates (see above).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      input_size: Deprecated and unused.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      activation: Activation function of the inner states.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      layer_norm: If `True`, layer normalization will be applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      norm_gain: float, The layer normalization gain initial value. If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `layer_norm` has been set to `False`, this argument will be ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      norm_shift: float, The layer normalization shift initial value. If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `layer_norm` has been set to `False`, this argument will be ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_keep_prob: unit Tensor or float between 0 and 1 representing the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        recurrent dropout probability value. If float and 1.0, no dropout will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        be applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_prob_seed: (optional) integer, the randomness seed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LayerNormBasicLSTMCell, self).__init__(_reuse=reuse)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      logging.warn(<span class="string">&quot;%s: The input_size parameter is deprecated.&quot;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._activation = activation</span><br><span class="line"></span><br><span class="line">    self._forget_bias = forget_bias</span><br><span class="line"></span><br><span class="line">    self._keep_prob = dropout_keep_prob</span><br><span class="line"></span><br><span class="line">    self._seed = dropout_prob_seed</span><br><span class="line"></span><br><span class="line">    self._layer_norm = layer_norm</span><br><span class="line"></span><br><span class="line">    self._norm_gain = norm_gain</span><br><span class="line"></span><br><span class="line">    self._norm_shift = norm_shift</span><br><span class="line"></span><br><span class="line">    self._reuse = reuse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rnn_cell_impl.LSTMStateTuple(self._num_units, self._num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_norm</span>(<span class="params">self, inp, scope, dtype=dtypes.float32</span>):</span></span><br><span class="line"></span><br><span class="line">    shape = inp.get_shape()[-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    gamma_init = init_ops.constant_initializer(self._norm_gain)</span><br><span class="line"></span><br><span class="line">    beta_init = init_ops.constant_initializer(self._norm_shift)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(scope):</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Initialize beta and gamma for use by layer_norm.</span></span><br><span class="line"></span><br><span class="line">      vs.get_variable(<span class="string">&quot;gamma&quot;</span>, shape=shape, initializer=gamma_init, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      vs.get_variable(<span class="string">&quot;beta&quot;</span>, shape=shape, initializer=beta_init, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    normalized = layers.layer_norm(inp, reuse=<span class="literal">True</span>, scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> normalized</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_linear</span>(<span class="params">self, args</span>):</span></span><br><span class="line"></span><br><span class="line">    out_size = <span class="number">4</span> * self._num_units</span><br><span class="line"></span><br><span class="line">    proj_size = args.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dtype = args.dtype</span><br><span class="line"></span><br><span class="line">    weights = vs.get_variable(<span class="string">&quot;kernel&quot;</span>, [proj_size, out_size], dtype=dtype)</span><br><span class="line"></span><br><span class="line">    out = math_ops.matmul(args, weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      bias = vs.get_variable(<span class="string">&quot;bias&quot;</span>, [out_size], dtype=dtype)</span><br><span class="line"></span><br><span class="line">      out = nn_ops.bias_add(out, bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;LSTM cell with layer normalization and recurrent dropout.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    c, h = state</span><br><span class="line"></span><br><span class="line">    args = array_ops.concat([inputs, h], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    concat = self._linear(args)</span><br><span class="line"></span><br><span class="line">    dtype = args.dtype</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=<span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      i = self._norm(i, <span class="string">&quot;input&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      j = self._norm(j, <span class="string">&quot;transform&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      f = self._norm(f, <span class="string">&quot;forget&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      o = self._norm(o, <span class="string">&quot;output&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    g = self._activation(j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">not</span> <span class="built_in">isinstance</span>(self._keep_prob, <span class="built_in">float</span>)) <span class="keyword">or</span> self._keep_prob &lt; <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">      g = nn_ops.dropout(g, self._keep_prob, seed=self._seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_c = (</span><br><span class="line"></span><br><span class="line">        c * math_ops.sigmoid(f + self._forget_bias) + math_ops.sigmoid(i) * g)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      new_c = self._norm(new_c, <span class="string">&quot;state&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    new_h = self._activation(new_c) * math_ops.sigmoid(o)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_h, new_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-24T03:06:40.000Z" title="2018/9/24 上午11:06:40">2018-09-24</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.159Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/">language model</a></span><span class="level-item">13 分钟读完 (大约1992个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/">论文笔记-character embedding and ELMO</a></h1><div class="content"><ul>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1508.06615.pdf">Character-Aware Neural Language Models</a>  </p>
</li>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a></p>
</li>
</ul>
<h1 id="character-embedding"><a href="#character-embedding" class="headerlink" title="character embedding"></a>character embedding</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>A language model is formalized as a probability distribution over a sequence of strings (words), and traditional methods usually involve making an n-th order Markov assumption and estimating n-gram probabilities via counting and subsequent smoothing (Chen and Goodman 1998). The count-based models are simple to train, but probabilities of rare n-grams can be poorly estimated due to data sparsity (despite smoothing techniques).  </p>
</blockquote>
<p>对语言模型的描述：语言模型是 一个单词序列的概率分布 的形式化描述（什么意思？就是比如这个句子长度为 10, 那么每个位置可能是词表中的任意一个词，而出现当前词是有一个概率的, 这个概率是依赖于之前的词的）。  </p>
<p>在传统的方法主要是运用 n阶马尔可夫假设来估计 n-gram 的概率，通过统计计数，以及子序列平滑的方式。这种基于计数的模型虽然简单，但是在数据稀疏的情况下，对不常见的 n-gram 的概率估计会很差。  </p>
<blockquote>
<p>While NLMs have been shown to outperform count-based n-gram language models (Mikolov et al. 2011), they are blind to subword information (e.g. morphemes). For example, they do not know, a priori, that eventful, eventfully, uneventful, and uneventfully should have structurally related embeddings in the vector space. Embeddings of rare words can thus be poorly estimated, leading to high perplexities for rare words (and words surrounding them). This is especially problematic in morphologically rich languages with long-tailed frequency distributions or domains with dynamic vocabularies (e.g. social media).  </p>
</blockquote>
<p>neural language models 将词嵌入到低维的向量中，使得语义相似的词在向量空间的位置也是相近的。然后 Mikolov word2vec 这种方式不能有效的解决子单词的信息问题，比如一个单词的各种形态，也不能认识前缀。这种情况下，不可避免的会造成不常见词的向量表示估计很差，对于不常见词会有较高的困惑度。这对于词语形态很丰富的语言是一个难题，同样这种问题也是动态词表的问题所在（比如社交媒体）。</p>
<h2 id="Recurrent-Neural-Network-Language-Model"><a href="#Recurrent-Neural-Network-Language-Model" class="headerlink" title="Recurrent Neural Network Language Model"></a>Recurrent Neural Network Language Model</h2><p>给定词表为 V，之前的序列是 $w_{1:t}=[w_1,..,w_t]$,在 RNN-LM 中通过全链接 affine transformation 计算 $w_{t+1}$ 个词的概率分布：  </p>
<p>$$Pr(w_{t+1}=j|w_{1:t})=\dfrac{exp(h_t\cdot p^j+q^j)}{\sum_{j’\in V}exp(h_t\cdot p^{j’}+q^{j’})}$$</p>
<p>其中 $h_t$ 是当前 t 时刻的隐藏状态。也就是先通过全链接映射到词表的 V 的维度，然后通过 softmax 计算其是词表中第 j 个词的概率。</p>
<p>然后假设训练预料库的 sentence 是 $w_{1:T}=[w_1,…,w_T]$,那么训练也就是最小化这个序列的 似然概率的负对数：</p>
<p>$$NLL=-\sum_{T}^{t=1}logPr(w_t|w_{1:t-1})$$</p>
<h2 id="Chracter-level-Convolution-Neural-Network"><a href="#Chracter-level-Convolution-Neural-Network" class="headerlink" title="Chracter-level Convolution Neural Network"></a>Chracter-level Convolution Neural Network</h2><p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/01.png"></p>
<p>以单词 absurdity 为例，有 l 个字符（通常会 padded 到一个固定size），通过 character embedding 映射成矩阵 $C\in R^{d\times l}$. d 是 embedding size. 图中 embedding size 为 4.</p>
<p>然后使用卷积核 kernel H 做卷积运算, $H\in R^{d\times w}$，所以得到的 feature map $f^k\in R^{l-w+1}$. 跟之前 CNN 做文本分类其实挺像的, kernel 的长是 embedding size d, 宽度 w 分别是 2,3,4. 上图中蓝色区域为例，filter 宽度为 2 的个数是3, 那么卷积得到的 featur map 是 $3 \times (9-2+1) = 3\times 8$.</p>
<p>$$f^k[i]=tanh(&lt;C^k[* ,i:i-w+1], H&gt; +b)$$</p>
<p>&lt;&gt;表示做卷积运算(Frobenius inner product). 然后加上 bias 和 非线性激活函数 tanh.</p>
<p>接着基于 times 维度做 max pooling. 上图中 filter 宽度为 3,2,4 的个数分别为 4,3,5.所以得到长度为 4+3+5=12 的向量。</p>
<p>这里每一个 filter matrix 得到一个相应的特征 feature. 在通常的 NLP 任务中这些 filter 的总数 $h\in[100, 1000]$</p>
<h2 id="Highway-Network"><a href="#Highway-Network" class="headerlink" title="Highway Network"></a>Highway Network</h2><p>通过卷积层得到单词 k 的向量表示为 $y^k$.</p>
<p>Highway Network 分为两层 layer.</p>
<ul>
<li>one layer of an MLP applies an affine transformation:</li>
</ul>
<p>$$z=g(W_y+b)$$</p>
<ul>
<li>one layer 有点类似 LSTM 中的 gate 机制：</li>
</ul>
<p>$$z=t\circ g(W_Hy+b_H)+(1-t)\circ y$$</p>
<p>其中  g 是非线性函数。$t=\sigma(W_Ty+b_T)$. t 成为 transform gate, (1-t) 是 carry gate. 同 LSTM 类似， highway network 允许输出能自适应的从 $y^k$ 中直接获取信息。</p>
<h1 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h1><p>传统的提取 word embedding 的方法，比如 word2vec 和 language model， 前者是通过词与词之间的共现，后者是 contextual，但他们都是获得固定的 embedding，也就是每一个词对应一个单一的 embedding.  而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练 language model, 而 word embedding 是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 且 ELMo 输出多个层的 embedding 表示, 试验中已经发现每层 LM 输出的信息对于不同的任务效果不同, 因此对每个 token 用不同层 embedding 表示会提升效果.</p>
<p>个人觉得，可以从这个角度去理解。RNN 可以看做一个高阶马尔可夫链，而不同于 马尔可夫模型，RNN 中的状态转移矩阵是用神经网络来模拟的，也就是我们计算隐藏层所用的 $h_t=tanh(w_{hh}h_{t-1}+w_{hx}x_t)$. 这个状态转移是动态的，也是不断更新的。而使用 语言模型 来训练 RNN/LSTM 目的就是得到这样的一套参数，使得它能学习到任何 合理的，自然的 sentence. 所以，这个语料库越大越好。事实上，有监督的训练也可以达到这个目的，但是有监督的数据有限，并且整个模型是有偏置的，比如文本分类的任务去训练，那么它更倾向于 局部信息。相比之下，机器翻译作为有监督的效果会更好，最好的还是语言模型呢，不仅可用的数据量很大，而且因为要预测每一个词的信息，它会努力结合每一个词的上下文去学习这个词的表示。这也正是我们需要的。ELMo 和 BERT 都是这样的道理，而 BERT 的优势前一篇 blog 说过了。</p>
<h2 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h2><p>给定 sentence $t_1, t_2,…,t_N$, 通过前面的词 $t_1,..,t_{k-1}$ 计算 token $t_k$ 的概率分布:</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/06.png"></p>
<p>反向：</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/02.png"></p>
<p>语言模型的训练就是采用极大似然估计，最大化这个概率：</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/03.png"></p>
<p>传统的方法就是 提取出对应位置的向量表示作为对应位置的词向量 context-independent token representation $x_k^{LM}$.</p>
<h2 id="ELMo-1"><a href="#ELMo-1" class="headerlink" title="ELMo"></a>ELMo</h2><blockquote>
<p>ELMo is a task specific combination of the intermediate layer representations in the biLM.</p>
</blockquote>
<p>ELMo 实际上只是下游任务的中间层，跟 BERT 一样。但也有不同的是， ELMo 每一层的向量表示会获得不同的 信息。底层更能捕捉 syntax and semantics 信息，更适用于 part-of-speech tagging 任务，高层更能获得 contextual 信息，更适用于 word sense disambiguation 任务。所以对不同的任务，会对不同层的向量表示的利用不同。</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/05.png"></p>
<p>在使用 ELMo 进行下游有监督训练时，通常是这样 $[x_k; ELMo_k^{task}]$. 对于 SQuAD 这样的任务，$[h_k, ELMo_k^{task}]$.</p>
<h2 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h2><blockquote>
<p>The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second</p>
</blockquote>
<p>layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation.</p>
<p>具体模型还是得看代码。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-22T10:56:03.000Z" title="2018/9/22 下午6:56:03">2018-09-22</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">12 分钟读完 (大约1777个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/">论文笔记-QANet</a></h1><div class="content"><p>paper:</p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.09541">combining local convolution with local self-attention for reading comprehension</a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.   </p>
</blockquote>
<p> encoder 编码方式仅仅由 卷积 和 自注意力 机制构成，没了 rnn 速度就是快。</p>
<blockquote>
<p>The key motivation behind the design of our model is the following: convolution captures the local structure of the text, while the self-attention learns the global interaction between each pair of words.  </p>
</blockquote>
<p> 这篇论文最主要的创新点：使用 CNN 来捕捉文本结构的局部信息，使用 self-attention 来学习全局中每两个词之间的交互信息，使得其能耦合上下文信息。相比 RNN，attention 能够有效的解决长期依赖问题。只是相比少了词序信息。说到底，也是一种 contextualize 的 encoder 方式。</p>
<blockquote>
<p>we propose a complementary data augmentation technique to enhance the training data. This technique paraphrases the examples by translating the original sentences from English to another language and then back to English, which not only enhances the number of training instances but also diversifies the phrasing.  </p>
</blockquote>
<p>使用了一种数据增强的方式，先将源语言转换成另一种语言，然后再翻译回英语。这样能有效增加训练样本，同时也丰富了短语的多样性。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa01.png"></p>
<p>模型分为5部分:  </p>
<ul>
<li><p>an embedding layer  </p>
</li>
<li><p>an embedding encoder layer  </p>
</li>
<li><p>a context-query attention layer  </p>
</li>
<li><p>a model encoder layer  </p>
</li>
<li><p>an output layer.</p>
</li>
</ul>
<blockquote>
<p>the combination of convolutions and self-attention is novel, and is significantly better than self-attention alone and gives 2.7 F1 gain in our experiments. The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth (layer dropout) (Huang et al., 2016), which gives an additional gain of 0.2 F1 in our experiments.  </p>
</blockquote>
<p>CNN 和 self-attention 的结合比单独的 self-attention 效果要好。同时使用了 CNN 之后能够使用常用的正则化方式 dropout, 这也能带来一点增益。</p>
<h3 id="Input-embedding-layer"><a href="#Input-embedding-layer" class="headerlink" title="Input embedding layer"></a>Input embedding layer</h3><blockquote>
<p>obtain the embedding of each word w by concatenating its word embedding and character embedding.</p>
</blockquote>
<p>由词向量和字符向量拼接而成。其中词向量采用预训练的词向量 Glove，并且不可训练，fixed. 只有 OOV (out of vocabulary) 是可训练的，用来映射所有不在词表内的词。</p>
<blockquote>
<p>Each character is represented as a trainable vector of dimension p2 = 200, meaning each word can be viewed as the concatenation of the embedding vectors for each of its characters. The length of each word is either truncated or padded to 16. We take maximum value of each row of this matrix to get a fixed-size vector representation of each word.  </p>
</blockquote>
<p>字符向量的处理。每个字母是可训练的，对应的维度是 200 维。然后每个词都 truncated 或者 padded 成16个字母，保证每个词的向量维度是一样大小。  </p>
<p>所以一个词的向量维度是 $300+200=500$.</p>
<h3 id="Embedding-encoding-layer"><a href="#Embedding-encoding-layer" class="headerlink" title="Embedding encoding layer"></a>Embedding encoding layer</h3><blockquote>
<p>The encoder layer is a stack of the following basic building block: [convolution-layer × # + self-attention-layer + feed-forward-layer]</p>
</blockquote>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa02.png"></p>
<p>其中：  </p>
<ul>
<li>convolution: 使用 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02357">depthwise separable convolutions</a> 而不是用传统的 convolution，因为作者发现 <strong>it is memory efficient and has better generalization.</strong> 怎么理解这个，还得看原 paper. The kernel size is 7, the number of filters is d = 128.</li>
</ul>
<ul>
<li>self-attention: the multi-head attention mechanism <a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/">论文笔记, Attention Is All You Need</a></li>
</ul>
<blockquote>
<p>Each of these basic operations (conv/self-attention/ffn) is placed inside a residual block, shown lower-right in Figure 1. For an input x and a given operation f, the output is f(layernorm(x))+x.  </p>
</blockquote>
<p>在 cnn/self-attention/ffn 层都有 layer normalization.</p>
<h4 id="为什么要用-CNN："><a href="#为什么要用-CNN：" class="headerlink" title="为什么要用 CNN："></a>为什么要用 CNN：</h4><p>用来获取局部信息 k-gram features</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa03.png"></p>
<p>相信看了这个图能对 QANet 中的 cnn 怎么实现的更清楚了。上图中每个卷积核的尺寸分别是 [2, embed_size], [3, embed_size], [3, embed_size]. padding参数 使用的是 “SAME”. 得到 3 个 [1, sequence_len]，然后拼接起来, 得到最终结果 [filters_num, sequence_len].</p>
<p>在 QANet 的实现中，kernel_size 都设置为7, num_filters=128.</p>
<h4 id="为什么要用-self-attention"><a href="#为什么要用-self-attention" class="headerlink" title="为什么要用 self-attention"></a>为什么要用 self-attention</h4><p>用来获取全局信息。</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa04.png"></p>
<p>上图中的这种方式显然不太好，复杂度高且效果不好。于是有了 self-attention.</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa05.png"></p>
<p>矩阵内部向量之间作內积，并通过 softmax 得到其他词对于 “The” 这个词的权重大小（权重比例与相似度成正比，这里看似不太合理 similarity == match??，但实际上效果很不错，可能跟词向量的训练有关）。</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa06.png"></p>
<p>然后将对应的权重大小 $[w_1,w_2,w_3,w_4,w_5]$ 与对应的词相乘，累和得到蕴含了上下文信息的 contextualized “The”.</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa07.png">  </p>
<p>并且，这是可以并行化的。大大加速了训练速度。</p>
<h3 id="Context-Query-Attention-Layer"><a href="#Context-Query-Attention-Layer" class="headerlink" title="Context-Query Attention Layer"></a>Context-Query Attention Layer</h3><p>跟 BIDAF 是一样的。来，不看笔记把公式过一遍。  </p>
<p>content: $C={c_1, c_2,…,c_n}$   </p>
<p>query: $Q={q_1,q_2,…q_m}$.</p>
<p>所以 embeded 之后，   </p>
<ul>
<li><p>content: [batch, content_n, embed_size]    </p>
</li>
<li><p>query: [batch, query_m, embed_size]  </p>
</li>
</ul>
<p>做矩阵相乘得到相似矩阵 similarity matrix $S\in R^{n\times m}$:    </p>
<p>sim_matrix: [batch, content_n, query_m]</p>
<blockquote>
<p>The similarity function used here is the trilinear function (Seo et al., 2016). $f(q,c)=W_0[q,c,q\circ c]$.  </p>
</blockquote>
<p>相似矩阵的计算可以不是直接矩阵相乘，而是加个前馈神经网络。毕竟 similarity 不一定等于 match.</p>
<h4 id="content-to-query"><a href="#content-to-query" class="headerlink" title="content-to-query"></a>content-to-query</h4><p>对 S 每一行 row 做 softmax 得到对应的概率，得到权重矩阵 $\tilde S\in R^{n\times m}$, shape = [batch, content_n, query_m].</p>
<p>然后与 query $Q^T$ [batch, query_m, embed_size] 矩阵相乘得到编码了 query 信息的 content:  </p>
<p>$A = \tilde SQ^T$, shape = [batch, content_n, embed_size]</p>
<h4 id="query-to-content"><a href="#query-to-content" class="headerlink" title="query_to_content"></a>query_to_content</h4><blockquote>
<p>Empirically, we find that, the DCN attention can provide a little benefit over simply applying context-to-query attention, so we adopt this strategy.  </p>
</blockquote>
<p>这里没有采用 BiDAF 里面的方法，而是采用 DCN 中的方式，利用了 $\tilde S$.</p>
<p>对 S 每一列 column 做 softmax 得到矩阵 $\overline S$, shape = [batch, content_n, query_n].</p>
<p>然后矩阵相乘得到 $B=\tilde S \overline S^T C^T$.  </p>
<p>$\tilde S$.shape=[batch, content_n, query_m]  </p>
<p>$\overline S^T$.shape=[batch, query_m, content_n]  </p>
<p>$C^T$.shape=[batch, query_m, embed_size]  </p>
<p>所以最后 B.shape=[batch, content_n, embed_size]</p>
<h3 id="Model-Encoder-Layer"><a href="#Model-Encoder-Layer" class="headerlink" title="Model Encoder Layer"></a>Model Encoder Layer</h3><p>同 BiDAF 一样输入是 $[c,a,c\circ a,c\circ b]$， 其中 a, b 分别是 attention matrix A，B 的行向量。不过不同的是，这里不同 bi-LSTM，而是类似于 encoder 模块的 [conv + self-attention + ffn]. 其中 conv 层数是 2, 总的 blocks 是7.</p>
<h3 id="Ouput-layer"><a href="#Ouput-layer" class="headerlink" title="Ouput layer"></a>Ouput layer</h3><p>$$p^1=softmax(W_1[M_0;M_1]), p^2=softmax(W_2[M_0;M_2])$$</p>
<p>其中 $W_1, w_2$ 是可训练的参数矩阵，$M_0, M_1, M_2$ 如图所示。</p>
<p>然后计算交叉熵损失函数：  </p>
<p>$$L(\theta)=-\dfrac{1}{N}\sum_i^N[log(p^1_{y^1})+log(p^2_{y^2})]$$</p>
<h3 id="QANet-哪里好，好在哪儿？"><a href="#QANet-哪里好，好在哪儿？" class="headerlink" title="QANet 哪里好，好在哪儿？"></a>QANet 哪里好，好在哪儿？</h3><p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa08.png"></p>
<ul>
<li><p>separable conv 不仅参数量少，速度快，还效果好。将 sep 变成传统 cnn, F1 值减小 0.7.  </p>
</li>
<li><p>去掉 CNN， F1值减小 2.7.  </p>
</li>
<li><p>去掉 self-attention, F1值减小 1.3.</p>
</li>
</ul>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa09.png">  </p>
<ul>
<li><p>layer normalization  </p>
</li>
<li><p>residual connections  </p>
</li>
<li><p>L2 regularization  </p>
</li>
</ul>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa10.png"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01604">Dynamic Coattention Networks For Question Answering</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02357">Xception: Deep Learning with Depthwise Separable Convolutions</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://thinklab.com/content/2940784">qanet_talk_v1.pdf</a></p>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/12/">上一页</a></div><div class="pagination-next"><a href="/page/14/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/12/">12</a></li><li><a class="pagination-link is-current" href="/page/13/">13</a></li><li><a class="pagination-link" href="/page/14/">14</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">116</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>