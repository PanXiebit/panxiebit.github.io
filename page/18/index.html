<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-02T12:34:18.000Z" title="2018/6/2 下午8:34:18">2018-06-02</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.540Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">9 分钟读完 (大约1365个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Dropout/">深度学习-Dropout</a></h1><div class="content"><p>dropout的数学原理。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><h4 id="随机失活（Dropout）"><a href="#随机失活（Dropout）" class="headerlink" title="随机失活（Dropout）"></a>随机失活（Dropout）</h4><p>是一个简单又极其有效的正则化方法。该方法由Srivastava在论文<a href="http://link.zhihu.com/?target=http://www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。</p>
<p><img src="/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Dropout/03.png"></p>
<p>在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。</p>
<p><strong>关于dropout的理解:</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23178423">知乎上的回答</a></p>
<p>python代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; 普通版随机失活: 不推荐实现 (看下面笔记) &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">X</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot; X中是输入数据 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line"></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line"></span><br><span class="line">  U1 = np.random.rand(*H1.shape) &lt; p <span class="comment"># 第一个随机失活遮罩,rand() [0,1)的随机数</span></span><br><span class="line"></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line"></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line"></span><br><span class="line">  U2 = np.random.rand(*H2.shape) &lt; p <span class="comment"># 第二个随机失活遮罩</span></span><br><span class="line"></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line"></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">X</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line"></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line"></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line"></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>在上面的代码中，train_step函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据X创建一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩U1和U2加入进去。</p>
<p>注意：在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以p，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以p=0.5为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元x的输出，那么进行随机失活的时候，该神经元的输出就是px+(1-p)0，这是有1-p的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整x\to px来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。</p>
<h4 id="反向随机失活"><a href="#反向随机失活" class="headerlink" title="反向随机失活"></a>反向随机失活</h4><p>它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">反向随机失活: 推荐实现方式.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">在训练的时候drop和调整数值范围，测试时不做任何事.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">X</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line"></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line"></span><br><span class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 第一个随机失活遮罩. 注意/p!</span></span><br><span class="line"></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line"></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line"></span><br><span class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># 第二个随机失活遮罩. 注意/p!</span></span><br><span class="line"></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line"></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">X</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line"></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># 不用数值范围调整了</span></span><br><span class="line"></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line"></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献：</p>
<p><a href="http://link.zhihu.com/?target=http://www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf">Dropout paper</a> by Srivastava et al. 2014.</p>
<p><a href="http://link.zhihu.com/?target=http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf">Dropout Training as Adaptive Regularization</a>：“我们认为：在使用费希尔信息矩阵（<a href="http://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Fisher_information_metric">fisher information matrix</a>）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-01T06:09:10.000Z" title="2018/6/1 下午2:09:10">2018-06-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.522Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">7 分钟读完 (大约1107个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/01/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%974-textRCNN/">文本分类系列4-textRCNN</a></h1><div class="content"><h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>paper: <a target="_blank" rel="noopener" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552">Recurrent Convolutional Neural Networks for Text Classification</a></p>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>先对之前的研究进行一番批判(0.0).</p>
<ol>
<li>传统的文本分类方法都是基于特征工程 feature representation，主要包括：</li>
</ol>
<ul>
<li><p>词袋模型 bag-of-words(BOW)model，用于提取unigram, bigram, n-grams的特征。</p>
</li>
<li><p>常见的特征选择的方法：frequency, MI (Cover and Thomas 2012), pLSA (Cai and Hofmann 2003), LDA (Hingmire et al. 2013)，用于选择具有更好的判别效果的特征。</p>
</li>
</ul>
<p>其原理就是去噪声来提高分类效果。比如去掉停用词，使用信息增益，互信息，或者L1正则化来获取有用的特征。但传统的特征表示的方法通常忽视了上下文信息和词序信息。</p>
<ol start="2">
<li>Richard Socher 提出的 <strong>Recursive Neural Network</strong></li>
</ol>
<p>RecusiveNN 通过语言的tree结构来获取句子的语义信息。但是分类的准确率太依赖文本的树结构。在文本分类之前建立一个树结构需要的计算复杂度就是 $O(n^2)$ （n是句子的长度）。所以对于很长的句子并不适用。</p>
<ol start="3">
<li>循环神经网络 Recurrent Neural Network</li>
</ol>
<p>计算复杂度是 $O(n)$，优点是能够很好的捕获长文本的语义信息，但是在rnn模型中，later words are more dominatant than earlier words. 但是如果对与某一个文本的分类，出现在之前的word影响更大的话，RNN的表现就不会很好。</p>
<p>为解决RNN这个问题，可以将CNN这个没有偏见的模型引入到NLP的工作中来，CNN能公平的对待句子中的每一个短语。 To tackle the bias problem, the Convolutional Neural Network (CNN), an unbiased model is introduced to NLP tasks, which can fairly determine discriminative phrases in a text with a max-pooling layer.</p>
<p>但是呢，通过前面的学习我们知道CNN的filter是固定尺寸的（fixed window），如果尺寸太短，会丢失很多信息，如果尺寸过长，计算复杂度又太大。所以作者提出个问题：能不能通过基于窗口的神经网络（CNN）学到更多的上下文信息，更好的表示文本的语义信息呢？ Therefore, it raises a question: can we learn more contextual information than conventional window-based neural networks and represent the semantic of texts more precisely for text classification.</p>
<p>于是，这篇论文提出了 Recurrent Concolution Neural Network(RCNN).</p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p><img src="/2018/06/01/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%974-textRCNN/model.png"></p>
<p>$$c_l{(w_i)} = f(W^{(l)}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))$$</p>
<p>$$c_r{(w_i)} = f(W^{(r)}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))$$</p>
<p>这两个公式类似于双向RNN，将 $c_l(w_i)$ 看作前一个时刻的隐藏状态 $h_{t-1}$, $c_l(w_{i-1})$ 就是 t-2 时刻的隐藏状态 $h_{t-2}$… 所以这就是个双向RNN…. 然后比较有创新的是，作者将隐藏状态 $h_{t-1}$ 和 $\tilde h_{t+1}$ ($\tilde h$ 表示反向), 以及当前word的词向量堆在一起，作为当前词以及获取了上下文信息的向量表示。</p>
<p>$$x_i = [c_l(w_i);e(w_i);c_r(w_i)]$$</p>
<p>然后是一个全连接层，这个可以看做textCNN中的卷积层,只是filter_size=1：</p>
<p>$$y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})$$</p>
<p>接着是最大池化层：</p>
<p>$$y^{(3)} = max_{i=1}^ny_i^{(2)}$$</p>
<p>然后是全连接层+softmax：</p>
<p>$$y^{(4)} = W^{(4)}y^{(3)}+b^{(4)}$$</p>
<p>$$p_i=\dfrac{exp(y_i^{(4)})}{\sum_{k=1}^nexp(y_k^{(4)})}$$</p>
<p>感觉就是双向rnn呀，只不过之前的方法是用最后一个隐藏层的输出作为整个sentence的向量表示，但这篇论文是用每一个时刻的向量表示(叠加了上下时刻的隐藏状态)，通过卷积层、maxpool后得到的向量来表示整个sentence.</p>
<p>确实是解决了RNN过于重视句子中靠后的词的问题，但是RNN训练慢的问题还是没有解决呀。但是在这里 <a target="_blank" rel="noopener" href="https://github.com/brightmart/text_classification#3textrnn">brightmart/text_classification</a> 中textCNN 和 RCNN的训练时间居然是一样的。why？</p>
<h4 id="Results-and-Discussion"><a href="#Results-and-Discussion" class="headerlink" title="Results and Discussion"></a>Results and Discussion</h4><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h3 id="需要注意的问题："><a href="#需要注意的问题：" class="headerlink" title="需要注意的问题："></a>需要注意的问题：</h3><ul>
<li>tf.nn.rnn_cell.DropoutWrapper</li>
</ul>
<ul>
<li>tf.nn.bidirectional_dynamic_rnn</li>
</ul>
<ul>
<li>tf.einsum</li>
</ul>
<ul>
<li>损失函数的对比 tf.nn.softmax_cross_entropy_with_logits</li>
</ul>
<ul>
<li>词向量是否需要正则化</li>
</ul>
<ul>
<li>tensorflow.contrib.layers.python.layers import optimize_loss 和 tf.train.AdamOptimizer(learning_rate).minimize(self.loss, self.global_steps) 的区别</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-31T13:25:54.000Z" title="2018/5/31 下午9:25:54">2018-05-31</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.479Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">4 分钟读完 (大约625个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/05/31/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%973-TextRNN/">文本分类系列3-TextRNN</a></h1><div class="content"><h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。</p>
<p>RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，<a target="_blank" rel="noopener" href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf">Recurrent Neural Network for Text Classification with Multi-Task Learning</a>文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。</p>
<h3 id="关于解决RNN无法并行化，计算效率低的问题"><a href="#关于解决RNN无法并行化，计算效率低的问题" class="headerlink" title="关于解决RNN无法并行化，计算效率低的问题"></a>关于解决RNN无法并行化，计算效率低的问题</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.10722">Factorization tricks for LSTM networks</a></p>
<ul>
<li>We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is “matrix factorization by design” of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></p>
<ul>
<li>The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-30T07:37:47.000Z" title="2018/5/30 下午3:37:47">2018-05-30</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.540Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">8 分钟读完 (大约1222个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/05/30/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%972-textCNN/">文本分类系列2-textCNN</a></h1><div class="content"><h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>主要框架和使用CNN进行文本分类的意图参考paper: <a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/D14-1181">Convolutional Neural Networks for Sentence Classification</a> 可参考cs224d中的课堂笔记，这堂课就是讲的这篇paper：</p>
<p><a href="http://www.panxiaoxie.cn/2018/05/14/cs224d-lecture13-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/#more">cs224d-lecture13 卷积神经网络</a></p>
<p><img src="/2018/05/30/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%972-textCNN/TextCNN.JPG"></p>
<p><strong>TextCNN详细过程</strong>：</p>
<ul>
<li>第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了,然后在图像中图像的表示是[length, width, channel],这里将文本的表示[sequence_len, embed_size, 1]。可以看到下面代码中：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">embeded_words = tf.nn.embedding_lookup(self.embedding, self.input_x) <span class="comment"># [None, sentence_len, embed_size]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># three channels similar to the image. using the tf.nn.conv2d</span></span><br><span class="line"></span><br><span class="line">self.sentence_embedding_expanded = tf.expand_dims(embeded_words, axis=-<span class="number">1</span>) <span class="comment"># [None, sentence_len, embed_size, 1]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。上图中的filter有3个，分别为：</p>
<ul>
<li><p>filter:[2, 5, 2] ==&gt; feature map:[6,1,2]</p>
</li>
<li><p>filter:[3, 5, 2] ==&gt; feature map:[5,1,2]</p>
</li>
<li><p>filter:[4, 5, 2] ==&gt; feature map:[4,1,2]</p>
</li>
<li><p>第三维表示channels，卷积后得到两个feature maps.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成scale，这里每个fiter_size的channel为2，所以输入的pooling.shape=[batch_size,1,1,2], 然后concat为一个flatten向量。</li>
</ul>
<ul>
<li>最后接一层全连接的 softmax 层，输出每个类别的概率。</li>
</ul>
<p><strong>特征</strong>：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。</p>
<p><strong>通道（Channels）</strong>：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。下面代码中的通道为1.</p>
<p><strong>一维卷积（conv-1d）</strong>：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。</p>
<p><strong>Pooling层</strong>：利用CNN解决文本分类问题的文章还是很多的，比如这篇 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a> 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：</p>
<pre><code>        “ 我觉得这个地方景色还不错，但是人也实在太多了 ”
</code></pre>
<p>虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>参数设置和模型具体实现参考paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1510.03820">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a></p>
<p>设计一个模型需要考虑的：  </p>
<ul>
<li><p>input word vector representations 输入的词向量表示  </p>
</li>
<li><p>filter region size(s); 卷积核的大小  </p>
</li>
<li><p>the number of feature maps; 特征图的通道数  </p>
</li>
<li><p>the activation function 激活函数  </p>
</li>
<li><p>the pooling strategy 池化的方式  </p>
</li>
<li><p>regularization terms (dropout/l2) 正则化项（dropout/l2）  </p>
</li>
</ul>
<h3 id="需要注意的问题"><a href="#需要注意的问题" class="headerlink" title="需要注意的问题"></a>需要注意的问题</h3><h4 id="一个单本对应单个标签和多个标签的区别？"><a href="#一个单本对应单个标签和多个标签的区别？" class="headerlink" title="一个单本对应单个标签和多个标签的区别？"></a>一个单本对应单个标签和多个标签的区别？</h4><p>关于多标签分类，应该看看周志华老师的这篇文章<a target="_blank" rel="noopener" href="http://cse.seu.edu.cn/people/zhangml/files/TKDE%2713.pdf">A Review on Multi-Label Learning Algorithms</a>, 知乎上还有其他资料<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/35486862">多标签（multi-label）数据的学习问题，常用的分类器或者分类策略有哪些？</a></p>
<p>本文代码中的方法：</p>
<ul>
<li>真实值labels的输入：单个标签的真实值是 <code>input_y.shape=[batch_size]</code>, 多个标签的真实值是 <code>input_y_multilabels.shape=[batch_size, label_size]</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">self.input_y = tf.placeholder(dtype=tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;input_y&#x27;</span>)</span><br><span class="line"></span><br><span class="line">self.input_y_multilabels = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>, num_classes], name=<span class="string">&quot;input_y_multilabels&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>损失函数的选择：</li>
</ul>
<ul>
<li>评价指标的区别：</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-25T07:12:40.000Z" title="2018/5/25 下午3:12:40">2018-05-25</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.522Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">19 分钟读完 (大约2870个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">文本分类系列0：NLTK学习和特征工程</a></h1><div class="content"><h3 id="计算语言：简单的统计"><a href="#计算语言：简单的统计" class="headerlink" title="计算语言：简单的统计"></a>计算语言：简单的统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.book <span class="keyword">import</span> *</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>*** Introductory Examples for the NLTK Book ***

Loading text1, ..., text9 and sent1, ..., sent9

Type the name of the text or sentence to view it.

Type: &#39;texts()&#39; or &#39;sents()&#39; to list the materials.

text1: Moby Dick by Herman Melville 1851

text2: Sense and Sensibility by Jane Austen 1811

text3: The Book of Genesis

text4: Inaugural Address Corpus

text5: Chat Corpus

text6: Monty Python and the Holy Grail

text7: Wall Street Journal

text8: Personals Corpus

text9: The Man Who Was Thursday by G . K . Chesterton 1908
</code></pre>
<p>找出text1,《白鲸记》中的词monstrous，以及其上下文</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text1.concordance(<span class="string">&quot;monstrous&quot;</span>, width=<span class="number">40</span>, lines=<span class="number">10</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>Displaying 10 of 11 matches:

 was of a most monstrous size . ... Thi

 Touching that monstrous bulk of the wh

enish array of monstrous clubs and spea

 wondered what monstrous cannibal and s

e flood ; most monstrous and most mount

Moby Dick as a monstrous fable , or sti

PTER 55 Of the Monstrous Pictures of Wh

exion with the monstrous pictures of wh

ose still more monstrous stories of the

ed out of this monstrous cabinet there
</code></pre>
<p>找出text1中与monstrous具有相同语境的词。比如monstrous的上下文 the __ pictures, the __ size. 同样在text1中与monstrous类似的上下文的词。很好奇这个是怎么实现的？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similar</span>(<span class="params">self, word, num=<span class="number">20</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Distributional similarity: find other words which appear in the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    same contexts as the specified word; list most similar words first.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text1.similar(<span class="string">&quot;monstrous&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>true contemptible christian abundant few part mean careful puzzled

mystifying passing curious loving wise doleful gamesome singular

delightfully perilous fearless
</code></pre>
<p>共用两个或两个以上词汇的上下文，如monstrous和very</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text2.common_contexts([<span class="string">&quot;monstrous&quot;</span>, <span class="string">&quot;very&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>a_pretty am_glad a_lucky is_pretty be_glad
</code></pre>
<p>自动检测出现在文本中的特定词，并显示同一上下文中出现的其他词。text4是《就职演说语料》，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    text4.dispersion_plot([<span class="string">&quot;citizens&quot;</span>, <span class="string">&quot;liberty&quot;</span>, <span class="string">&quot;freedom&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<pre><code>&lt;matplotlib.figure.Figure at 0x7f3794818588&gt;
</code></pre>
<p>如果不使用 if <strong>name</strong>==”<strong>main</strong>“ 的话会报错``` ‘NoneType’ object has no attribute ‘show’</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"></span><br><span class="line">fdist1 = FreqDist(text1)</span><br><span class="line"></span><br><span class="line">vocabulary1 = list(fdist1.keys())  # keys() 返回key值组成的list</span><br><span class="line"></span><br><span class="line">print(vocabulary1[:10])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;[&#39;, &#39;Moby&#39;, &#39;Dick&#39;, &#39;by&#39;, &#39;Herman&#39;, &#39;Melville&#39;, &#39;1851&#39;, &#39;]&#39;, &#39;ETYMOLOGY&#39;, &#39;.&#39;]
</code></pre>
<p>需要加list，不然回报错，<code>“TypeError: &#39;dict_keys&#39; object is not subscriptable”</code></p>
<p>dict.keys() returns an iteratable but not indexable object. The most simple (but not so efficient) solution would be:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 同样的道理这里也需要加list，因为生成的&lt;class &#x27;dict_items&#x27;&gt;z在python3中是迭代器</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(fdist1.items()))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(fdist1.items())[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&lt;class &#39;dict_items&#39;&gt;

[(&#39;[&#39;, 3), (&#39;Moby&#39;, 84), (&#39;Dick&#39;, 84), (&#39;by&#39;, 1137), (&#39;Herman&#39;, 1), (&#39;Melville&#39;, 1), (&#39;1851&#39;, 3), (&#39;]&#39;, 1), (&#39;ETYMOLOGY&#39;, 1), (&#39;.&#39;, 6862)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># dict.items() 实际上是将dict转换为可迭代对象list，list的对象是 (&#x27;[&#x27;, 3), (&#x27;Moby&#x27;, 84), (&#x27;Dick&#x27;, 84), (&#x27;by&#x27;, 1137)这样的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这下总能记住dict按照value排序了吧。。。尴尬，以前居然没弄懂？？</span></span><br><span class="line"></span><br><span class="line">fdist_sorted = <span class="built_in">sorted</span>(fdist1.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(fdist_sorted[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;,&#39;, 18713), (&#39;the&#39;, 13721), (&#39;.&#39;, 6862), (&#39;of&#39;, 6536), (&#39;and&#39;, 6024), (&#39;a&#39;, 4569), (&#39;to&#39;, 4542), (&#39;;&#39;, 4072), (&#39;in&#39;, 3916), (&#39;that&#39;, 2982)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 这个就是按照key排序。</span></span><br><span class="line"></span><br><span class="line">fdist_sorted2 = <span class="built_in">sorted</span>(fdist1.keys(), reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(fdist_sorted2[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;zoology&#39;, &#39;zones&#39;, &#39;zoned&#39;, &#39;zone&#39;, &#39;zodiac&#39;, &#39;zig&#39;, &#39;zephyr&#39;, &#39;zeal&#39;, &#39;zay&#39;, &#39;zag&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">fdist1.plot(<span class="number">20</span>, cumulative=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><img src="/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/output_16_0.png" alt="png"></p>
<p>可以看到高频词大都是无用的停用词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 低频词 fdist.hapaxes() 出现次数为1的词</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(fdist1.hapaxes()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> fdist1.hapaxes():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> fdist1[i] <span class="keyword">is</span> <span class="keyword">not</span> <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;hh&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>9002
</code></pre>
<p>可以看到低频词也很多，而且大都也是很无用的词。</p>
<h4 id="词语搭配"><a href="#词语搭配" class="headerlink" title="词语搭配"></a>词语搭配</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">list</span>(bigrams([<span class="string">&#x27;more&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;sad&#x27;</span>, <span class="string">&#x27;than&#x27;</span>, <span class="string">&#x27;done&#x27;</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>[(&#39;more&#39;, &#39;is&#39;), (&#39;is&#39;, &#39;sad&#39;), (&#39;sad&#39;, &#39;than&#39;), (&#39;than&#39;, &#39;done&#39;)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text4.collocations(window_size=<span class="number">4</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>United States; fellow citizens; four years; years ago; men women;

Federal Government; General Government; self government; Vice

President; American people; every citizen; within limits; Old World;

Almighty God; Fellow citizens; Chief Magistrate; Chief Justice; one

another; Declaration Independence; protect defend
</code></pre>
<p>文本4是就职演说语料，可以看到n-grams能够很好的展现出文本的特性，说明n-grams是不错的特征。</p>
<p>collections()源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collocations</span>(<span class="params">self, num=<span class="number">20</span>, window_size=<span class="number">2</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Print collocations derived from the text, ignoring stopwords.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :seealso: find_collocations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num: The maximum number of collocations to print.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :type num: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param window_size: The number of tokens spanned by a collocation (default=2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :type window_size: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (<span class="string">&#x27;_collocations&#x27;</span> <span class="keyword">in</span> self.__dict__ <span class="keyword">and</span> self._num == num <span class="keyword">and</span> self._window_size == window_size):</span><br><span class="line"></span><br><span class="line">        self._num = num</span><br><span class="line"></span><br><span class="line">        self._window_size = window_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#print(&quot;Building collocations list&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">        ignored_words = stopwords.words(<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        finder = BigramCollocationFinder.from_words(self.tokens, window_size)</span><br><span class="line"></span><br><span class="line">        finder.apply_freq_filter(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        finder.apply_word_filter(<span class="keyword">lambda</span> w: <span class="built_in">len</span>(w) &lt; <span class="number">3</span> <span class="keyword">or</span> w.lower() <span class="keyword">in</span> ignored_words)</span><br><span class="line"></span><br><span class="line">        bigram_measures = BigramAssocMeasures()</span><br><span class="line"></span><br><span class="line">        self._collocations = finder.nbest(bigram_measures.likelihood_ratio, num)</span><br><span class="line"></span><br><span class="line">    colloc_strings = [w1+<span class="string">&#x27; &#x27;</span>+w2 <span class="keyword">for</span> w1, w2 <span class="keyword">in</span> self._collocations]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(tokenwrap(colloc_strings, separator=<span class="string">&quot;; &quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="自动理解自然语言"><a href="#自动理解自然语言" class="headerlink" title="自动理解自然语言"></a>自动理解自然语言</h3><ul>
<li><p>词义消歧 Ambiguity 关于词义消歧的理解可以看之前的笔记<a href="http://www.panxiaoxie.cn/2018/04/20/chapter12-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/">chapter12-句法分析</a></p>
</li>
<li><p>指代消解 anaphora resolution</p>
</li>
<li><p>自动问答</p>
</li>
<li><p>机器翻译</p>
</li>
<li><p>人机对话系统</p>
</li>
</ul>
<h3 id="获得文本语料和词汇资源"><a href="#获得文本语料和词汇资源" class="headerlink" title="获得文本语料和词汇资源"></a>获得文本语料和词汇资源</h3><h4 id="布朗语料库"><a href="#布朗语料库" class="headerlink" title="布朗语料库"></a>布朗语料库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>有以下这些类别的文本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(brown.categories())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;adventure&#39;, &#39;belles_lettres&#39;, &#39;editorial&#39;, &#39;fiction&#39;, &#39;government&#39;, &#39;hobbies&#39;, &#39;humor&#39;, &#39;learned&#39;, &#39;lore&#39;, &#39;mystery&#39;, &#39;news&#39;, &#39;religion&#39;, &#39;reviews&#39;, &#39;romance&#39;, &#39;science_fiction&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">news_text = brown.words(categories=<span class="string">&quot;news&quot;</span>)</span><br><span class="line"></span><br><span class="line">fdist_news = nltk.FreqDist([w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> news_text])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(fdist_news))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>13112
</code></pre>
<h3 id="标注文本语料库"><a href="#标注文本语料库" class="headerlink" title="标注文本语料库"></a>标注文本语料库</h3><p>经过了标注的语料库，有词性标注、命名实体、句法结构、语义角色等。</p>
<h4 id="分类和标注词汇"><a href="#分类和标注词汇" class="headerlink" title="分类和标注词汇"></a>分类和标注词汇</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text = nltk.word_tokenize(<span class="string">&quot;and now for something completely differences!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(text)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.pos_tag(text))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;and&#39;, &#39;now&#39;, &#39;for&#39;, &#39;something&#39;, &#39;completely&#39;, &#39;differences&#39;, &#39;!&#39;]

[(&#39;and&#39;, &#39;CC&#39;), (&#39;now&#39;, &#39;RB&#39;), (&#39;for&#39;, &#39;IN&#39;), (&#39;something&#39;, &#39;NN&#39;), (&#39;completely&#39;, &#39;RB&#39;), (&#39;differences&#39;, &#39;VBZ&#39;), (&#39;!&#39;, &#39;.&#39;)]
</code></pre>
<h5 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h5><p>NLTK中采用的方法可参考：<a target="_blank" rel="noopener" href="https://explosion.ai/blog/part-of-speech-pos-tagger-in-python">A Good Part-of-Speech Tagger in about 200 Lines of Python</a></p>
<p>对于一些同形同音异义词，通过词性标注能消除歧义.很多文本转语音系统通常需要进行词性标注，因为不同意思发音会不太一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text1 = nltk.word_tokenize(<span class="string">&quot;They refuse to permit us tpo obtain the refuse permit&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.pos_tag(text1))</span><br><span class="line"></span><br><span class="line">text2 = nltk.word_tokenize(<span class="string">&quot;They refuse to permit us to obtain the refuse permit&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.pos_tag(text2))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;They&#39;, &#39;PRP&#39;), (&#39;refuse&#39;, &#39;VBP&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;permit&#39;, &#39;VB&#39;), (&#39;us&#39;, &#39;PRP&#39;), (&#39;tpo&#39;, &#39;VB&#39;), (&#39;obtain&#39;, &#39;VB&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;refuse&#39;, &#39;NN&#39;), (&#39;permit&#39;, &#39;NN&#39;)]

[(&#39;They&#39;, &#39;PRP&#39;), (&#39;refuse&#39;, &#39;VBP&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;permit&#39;, &#39;VB&#39;), (&#39;us&#39;, &#39;PRP&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;obtain&#39;, &#39;VB&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;refuse&#39;, &#39;NN&#39;), (&#39;permit&#39;, &#39;NN&#39;)]
</code></pre>
<h5 id="获取已经标注好的语料库"><a href="#获取已经标注好的语料库" class="headerlink" title="获取已经标注好的语料库"></a>获取已经标注好的语料库</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.brown.tagged_words())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;The&#39;, &#39;AT&#39;), (&#39;Fulton&#39;, &#39;NP-TL&#39;), ...]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.treebank.tagged_words())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.treebank.tagged_sents()[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;Pierre&#39;, &#39;NNP&#39;), (&#39;Vinken&#39;, &#39;NNP&#39;), (&#39;,&#39;, &#39;,&#39;), ...]

[(&#39;Pierre&#39;, &#39;NNP&#39;), (&#39;Vinken&#39;, &#39;NNP&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;61&#39;, &#39;CD&#39;), (&#39;years&#39;, &#39;NNS&#39;), (&#39;old&#39;, &#39;JJ&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;will&#39;, &#39;MD&#39;), (&#39;join&#39;, &#39;VB&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;board&#39;, &#39;NN&#39;), (&#39;as&#39;, &#39;IN&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;nonexecutive&#39;, &#39;JJ&#39;), (&#39;director&#39;, &#39;NN&#39;), (&#39;Nov.&#39;, &#39;NNP&#39;), (&#39;29&#39;, &#39;CD&#39;), (&#39;.&#39;, &#39;.&#39;)]
</code></pre>
<p>查看brown语料库中新闻类最常见的词性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">brown_news_tagged = brown.tagged_words(categories=<span class="string">&#x27;news&#x27;</span>, tagset=<span class="string">&#x27;universal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tag_fd = nltk.FreqDist(tag <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> brown_news_tagged)</span><br><span class="line"></span><br><span class="line">tag_fd.keys()</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>dict_keys([&#39;DET&#39;, &#39;NOUN&#39;, &#39;ADJ&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;.&#39;, &#39;ADV&#39;, &#39;CONJ&#39;, &#39;PRT&#39;, &#39;PRON&#39;, &#39;NUM&#39;, &#39;X&#39;])
</code></pre>
<h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><h4 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h4><p>选取特征，将名字的最后一个字母作为特征. 返回的字典称为特征集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features</span>(<span class="params">word</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;last_letter&#x27;</span>:word[-<span class="number">1</span>]&#125;</span><br><span class="line"></span><br><span class="line">gender_features(<span class="string">&#x27;Shrek&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&#123;&#39;last_letter&#39;: &#39;k&#39;&#125;
</code></pre>
<p>定义一个特征提取器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> names</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">names = ([(name, <span class="string">&#x27;male&#x27;</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">&#x27;male.txt&#x27;</span>)] +</span><br><span class="line"></span><br><span class="line">        [(name, <span class="string">&#x27;female&#x27;</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">&#x27;female.txt&#x27;</span>)])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.names.words(<span class="string">&#x27;male.txt&#x27;</span>)[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(names[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;Aamir&#39;, &#39;Aaron&#39;, &#39;Abbey&#39;, &#39;Abbie&#39;, &#39;Abbot&#39;, &#39;Abbott&#39;, &#39;Abby&#39;, &#39;Abdel&#39;, &#39;Abdul&#39;, &#39;Abdulkarim&#39;]

[(&#39;Aamir&#39;, &#39;male&#39;), (&#39;Aaron&#39;, &#39;male&#39;), (&#39;Abbey&#39;, &#39;male&#39;), (&#39;Abbie&#39;, &#39;male&#39;), (&#39;Abbot&#39;, &#39;male&#39;), (&#39;Abbott&#39;, &#39;male&#39;), (&#39;Abby&#39;, &#39;male&#39;), (&#39;Abdel&#39;, &#39;male&#39;), (&#39;Abdul&#39;, &#39;male&#39;), (&#39;Abdulkarim&#39;, &#39;male&#39;)]
</code></pre>
<p>使用特征提取器处理names数据，并把数据集分为训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 二分类</span></span><br><span class="line"></span><br><span class="line">features = [(gender_features(n), g) <span class="keyword">for</span> (n, g) <span class="keyword">in</span> names]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">train_set, test_set = features[<span class="number">500</span>:], features[:<span class="number">500</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_set[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#123;&#39;last_letter&#39;: &#39;n&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;b&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;b&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;y&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;y&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;t&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 预测一个未出现的名字</span></span><br><span class="line"></span><br><span class="line">classifier.classify(gender_features(<span class="string">&#x27;Pan&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>&#39;male&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 测试集上的准确率</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, test_set))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>0.602
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">classifier.show_most_informative_features(<span class="number">5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>Most Informative Features

             last_letter = &#39;a&#39;            female : male   =     35.5 : 1.0

             last_letter = &#39;k&#39;              male : female =     34.1 : 1.0

             last_letter = &#39;f&#39;              male : female =     15.9 : 1.0

             last_letter = &#39;p&#39;              male : female =     13.5 : 1.0

             last_letter = &#39;v&#39;              male : female =     12.7 : 1.0
</code></pre>
<p>构建包含所有实例特征的单独list会占用大量内存，所有应该把这些特征集成起来。</p>
<h4 id="定义一个特征提取器包含多个特征"><a href="#定义一个特征提取器包含多个特征" class="headerlink" title="定义一个特征提取器包含多个特征"></a>定义一个特征提取器包含多个特征</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 添加多个特征</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> apply_features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features2</span>(<span class="params">word</span>):</span></span><br><span class="line"></span><br><span class="line">    features = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    features[<span class="string">&#x27;firstletter&#x27;</span>] = word[<span class="number">0</span>].lower()</span><br><span class="line"></span><br><span class="line">    features[<span class="string">&#x27;lastletter&#x27;</span>] = word[-<span class="number">1</span>].lower()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        features[<span class="string">&quot;count(%s)&quot;</span>%letter] = word.lower().count(letter)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gender_features2(<span class="string">&#x27;xiepan&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(gender_features2(<span class="string">&#x27;xiepan&#x27;</span>))) <span class="comment"># 有28个特征， 2+26=28</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&#123;&#39;firstletter&#39;: &#39;x&#39;, &#39;lastletter&#39;: &#39;n&#39;, &#39;count(a)&#39;: 1, &#39;count(b)&#39;: 0, &#39;count(c)&#39;: 0, &#39;count(d)&#39;: 0, &#39;count(e)&#39;: 1, &#39;count(f)&#39;: 0, &#39;count(g)&#39;: 0, &#39;count(h)&#39;: 0, &#39;count(i)&#39;: 1, &#39;count(j)&#39;: 0, &#39;count(k)&#39;: 0, &#39;count(l)&#39;: 0, &#39;count(m)&#39;: 0, &#39;count(n)&#39;: 1, &#39;count(o)&#39;: 0, &#39;count(p)&#39;: 1, &#39;count(q)&#39;: 0, &#39;count(r)&#39;: 0, &#39;count(s)&#39;: 0, &#39;count(t)&#39;: 0, &#39;count(u)&#39;: 0, &#39;count(v)&#39;: 0, &#39;count(w)&#39;: 0, &#39;count(x)&#39;: 1, &#39;count(y)&#39;: 0, &#39;count(z)&#39;: 0&#125;

28
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 对每个样本进行特征处理</span></span><br><span class="line"></span><br><span class="line">features = [(gender_features(n), g) <span class="keyword">for</span> (n,g) <span class="keyword">in</span> names]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(features))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>7944
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 训练集，开发集和测试集</span></span><br><span class="line"></span><br><span class="line">train_set = features[<span class="number">1500</span>:]</span><br><span class="line"></span><br><span class="line">dev_set = apply_features(gender_features2, names[<span class="number">500</span>:<span class="number">1500</span>])</span><br><span class="line"></span><br><span class="line">test_set = apply_features(gender_features2, names[:<span class="number">500</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, dev_set))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, test_set))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, train_set))  <span class="comment">## 明显过拟合了～</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>0.007

0.008

0.883302296710118
</code></pre>
<h3 id="文档分类"><a href="#文档分类" class="headerlink" title="文档分类"></a>文档分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> movie_reviews</span><br><span class="line"></span><br><span class="line">documents = [(<span class="built_in">list</span>(movie_reviews.words(fileid)), category) <span class="keyword">for</span> category <span class="keyword">in</span> movie_reviews.categories()</span><br><span class="line"></span><br><span class="line">             <span class="keyword">for</span> fileid <span class="keyword">in</span> movie_reviews.fileids(category) ]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">movie_reviews.categories()</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>[&#39;neg&#39;, &#39;pos&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">neg_docu = movie_reviews.fileids(<span class="string">&#x27;neg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(neg_docu))   <span class="comment"># neg类别的文档数　1000</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))  <span class="comment">#　总的文档数    1000</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(movie_reviews.words(neg_docu[<span class="number">0</span>])) <span class="comment"># 第一个文件中单词数  879</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>1000

2000

879
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">random.shuffle(documents)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="文档分类的特征提取器"><a href="#文档分类的特征提取器" class="headerlink" title="文档分类的特征提取器"></a>文档分类的特征提取器</h4><p>所谓特征提取器实际上就是将文档原本的内容用认为选定的特征来表示。然后用分类器找出这些特征和对应类标签的映射关系。</p>
<p>那么什么样的特征才是好的特征，这就是特征工程了吧。</p>
<h4 id="文本分类概述"><a href="#文本分类概述" class="headerlink" title="文本分类概述"></a>文本分类概述</h4><p>文本分类，顾名思义，就是根据文本内容本身将文本归为不同的类别，通常是有监督学习的任务。根据文本内容的长短，有做句子、段落或者文章的分类；文本的长短不同可能会导致文本可抽取的特征上的略微差异，<strong>但是总体上来说，文本分类的核心都是如何从文本中抽取出能够体现文本特点的关键特征，抓取特征到类别之间的映射。</strong> 所以，特征工程就显得非常重要，特征找的好，分类效果也会大幅提高（当然前提是标注数据质量和数量也要合适，数据的好坏决定效果的下限，特征工程决定效果的上限）。</p>
<p>也许会有人问最近的深度学习技术能够避免我们构造特征这件事，为什么还需要特征工程？深度学习并不是万能的，在NLP领域深度学习技术取得的效果有限（毕竟语言是高阶抽象的信息，深度学习在图像、语音这些低阶具体的信息处理上更适合，因为在低阶具体的信息上构造特征是一件费力的事情），并不是否认深度学习在NLP领域上取得的成绩，工业界现在通用的做法都是会把深度学习模型作为系统的一个子模块（也是一维特征），和一些传统的基于统计的自然语言技术的特征，还有一些针对具体任务本身专门设计的特征，一起作为一个或多个模型（也称Ensemble，即模型集成）的输入，最终构成一个文本处理系统。</p>
<h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><p>那么，对于文本分类任务而言，工业界常用到的特征有哪些呢？下面用一张图以概括：</p>
<p><img src="/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/01.png"></p>
<p>我主要将这些特征分为四个层次，由下往上，特征由抽象到具体，粒度从细到粗。我们希望能够从不同的角度和纬度来设计特征，以捕捉这些特征和类别之间的关系。下面详细介绍这四个层次上常用到的特征表示。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/17/">上一页</a></div><div class="pagination-next"><a href="/page/19/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/17/">17</a></li><li><a class="pagination-link is-current" href="/page/18/">18</a></li><li><a class="pagination-link" href="/page/19/">19</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">117</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>