<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-01T09:49:53.000Z" title="2018/9/1 下午5:49:53">2018-09-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.141Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/TensorFlow/">TensorFlow</a></span><span class="level-item">44 分钟读完 (大约6574个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/01/tensorflow-rnn-api-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/">Tensorflow RNN API 源码阅读</a></h1><div class="content"><p>在三星研究院实习一段时间发现在公司写代码和在学校还是有差别的。一是在公司要追求效率，会使用很多官方封装好的api，而在学校的时候因为要去理解内部原理，更多的是在造轮子，导致对很多 api 不是很熟悉。但实际上官方api不仅在速度，以及全面性上都比自己写的还是好很多的。二是，在公司对代码的复用率要求比较高，模型跑到哪一个版本了，对应的参数都要留下来，随时可以跑起来，而不是重新训练，这对模型、参数的保存要求很重要。以及在测试集上的性能指标都要在代码上很完整，而不是仅仅看看 loss 和 accuracy 就可以的。</p>
<p>这节内容主要是详细过一遍 tensorflow 里面的 rnn api，根据<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods">RNN and Cells (contrib)</a>这里的顺序逐步深入研究</p>
<h2 id="先回顾一下-RNN-LSTM-GRU"><a href="#先回顾一下-RNN-LSTM-GRU" class="headerlink" title="先回顾一下 RNN/LSTM/GRU:"></a>先回顾一下 RNN/LSTM/GRU:</h2><p>参考之前 cs224d 的笔记  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/05/07/cs224d-lecture9-machine-translation/">cs224d-lecture9 机器翻译</a>   </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/">cs224d-lecture8-RNN</a> 发现有些小错误，但不影响自己复习。。</p>
</li>
</ul>
<p>basic rnn：  </p>
<p><img src="https://panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn16.png"></p>
<p>$$h_t = \sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$</p>
<h2 id="先看-tf-contrib-rnn-RNNCell"><a href="#先看-tf-contrib-rnn-RNNCell" class="headerlink" title="先看 tf.contrib.rnn.RNNCell"></a>先看 tf.contrib.rnn.RNNCell</h2><p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/ops/rnn_cell_impl.py#L170">https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/ops/rnn_cell_impl.py#L170</a>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.RNNCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNCell</span>(<span class="params">base_layer.Layer</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Abstract object representing an RNN cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Every `RNNCell` must have the properties below and implement `call` with</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  the signature `(output, next_state) = call(input, state)`.  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  RNNCell 是一个抽象的父类，之后更复杂的 RNN/LSTM/GRU 都是重新实现 call 函数，也就是更新隐藏状态</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  的方式改变了。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The optional third input argument, `scope`, is allowed for backwards compatibility</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  purposes; but should be left off for new subclasses.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  scope 这个参数管理变量，在反向传播中变量是否可训练。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This definition of cell differs from the definition used in the literature.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  In the literature, &#x27;cell&#x27; refers to an object with a single scalar output.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This definition refers to a horizontal array of such units.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  这里的 cell 的概念和一些论文中是不一样的。在论文中，cell 表示一个神经元，也就是单个值。而这里表示的是</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  一组神经元，比如隐藏状态[batch, num_units].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  An RNN cell, in the most abstract setting, is anything that has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  a state and performs some operation that takes a matrix of inputs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This operation results in an output matrix with `self.output_size` columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  If `self.state_size` is an integer, this operation also results in a new</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  state matrix with `self.state_size` columns.  If `self.state_size` is a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  (possibly nested tuple of) TensorShape object(s), then it should return a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  matching structure of Tensors having shape `[batch_size].concatenate(s)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  for each `s` in `self.batch_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  rnn cell 的输入是一个状态 state 和 input 矩阵，参数有 self.output_size 和 self.state_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  分别表示输出层和隐藏层的维度。其中 state_size 可能是 tuple，这个之后在看。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, inputs, state, scope=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run this RNN cell on inputs, starting from the given state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      inputs: `2-D` tensor with shape `[batch_size, input_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state: if `self.state_size` is an integer, this should be a `2-D Tensor`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        with shape `[batch_size, self.state_size]`.  Otherwise, if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `self.state_size` is a tuple of integers, this should be a tuple</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        with shapes `[batch_size, s] for s in self.state_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      scope: VariableScope for the created subgraph; defaults to class name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A pair containing:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      - New state: Either a single `2-D` tensor, or a tuple of tensors matching</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the arity and shapes of `state`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> vs.variable_scope(scope,</span><br><span class="line"></span><br><span class="line">                             custom_getter=self._rnn_get_variable) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(RNNCell, self).__call__(inputs, state, scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      scope_attrname = <span class="string">&quot;rnncell_scope&quot;</span></span><br><span class="line"></span><br><span class="line">      scope = <span class="built_in">getattr</span>(self, scope_attrname, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">        scope = vs.variable_scope(vs.get_variable_scope(),</span><br><span class="line"></span><br><span class="line">                                  custom_getter=self._rnn_get_variable)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">setattr</span>(self, scope_attrname, scope)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> scope:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(RNNCell, self).__call__(inputs, state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_rnn_get_variable</span>(<span class="params">self, getter, *args, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">    variable = getter(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> context.executing_eagerly():</span><br><span class="line"></span><br><span class="line">      trainable = variable._trainable  <span class="comment"># pylint: disable=protected-access</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      trainable = (</span><br><span class="line"></span><br><span class="line">          variable <span class="keyword">in</span> tf_variables.trainable_variables() <span class="keyword">or</span></span><br><span class="line"></span><br><span class="line">          (<span class="built_in">isinstance</span>(variable, tf_variables.PartitionedVariable) <span class="keyword">and</span></span><br><span class="line"></span><br><span class="line">           <span class="built_in">list</span>(variable)[<span class="number">0</span>] <span class="keyword">in</span> tf_variables.trainable_variables()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> trainable <span class="keyword">and</span> variable <span class="keyword">not</span> <span class="keyword">in</span> self._trainable_weights:</span><br><span class="line"></span><br><span class="line">      self._trainable_weights.append(variable)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> <span class="keyword">not</span> trainable <span class="keyword">and</span> variable <span class="keyword">not</span> <span class="keyword">in</span> self._non_trainable_weights:</span><br><span class="line"></span><br><span class="line">      self._non_trainable_weights.append(variable)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;size(s) of state(s) used by this cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It can be represented by an Integer, a TensorShape or a tuple of Integers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    or TensorShapes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Abstract method&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Integer or TensorShape: size of outputs produced by this cell.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Abstract method&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, _</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># This tells the parent Layer object that it&#x27;s OK to call</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># self.add_variable() inside the call() method.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">zero_state</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return zero-filled state tensor(s).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: int, float, or unit Tensor representing the batch size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: the data type to use for the state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      If `state_size` is an int or TensorShape, then the return value is a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `N-D` tensor of shape `[batch_size, state_size]` filled with zeros.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      If `state_size` is a nested list or tuple, then the return value is</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      a nested list or tuple (of the same structure) of `2-D` tensors with</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      the shapes `[batch_size, s]` for each s in `state_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Try to use the last cached zero_state. This is done to avoid recreating</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># zeros, especially when eager execution is enabled.</span></span><br><span class="line"></span><br><span class="line">    state_size = self.state_size</span><br><span class="line"></span><br><span class="line">    is_eager = context.executing_eagerly()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_eager <span class="keyword">and</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;_last_zero_state&quot;</span>):</span><br><span class="line"></span><br><span class="line">      (last_state_size, last_batch_size, last_dtype,</span><br><span class="line"></span><br><span class="line">       last_output) = <span class="built_in">getattr</span>(self, <span class="string">&quot;_last_zero_state&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (last_batch_size == batch_size <span class="keyword">and</span></span><br><span class="line"></span><br><span class="line">          last_dtype == dtype <span class="keyword">and</span></span><br><span class="line"></span><br><span class="line">          last_state_size == state_size):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> last_output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ops.name_scope(<span class="built_in">type</span>(self).__name__ + <span class="string">&quot;ZeroState&quot;</span>, values=[batch_size]):</span><br><span class="line"></span><br><span class="line">      output = _zero_state_tensors(state_size, batch_size, dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_eager:</span><br><span class="line"></span><br><span class="line">      self._last_zero_state = (state_size, batch_size, dtype, output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p>两个属性 output_size, state_size 分别表示输出层的维度和隐藏层的维度。call 函数用来表示计算下一个时间步的隐藏状态和输出，zero_state 函数用来初始化初始状态全为 0, 这里 state_size 有两种情况，一种是 int 或 tensorshape,那么 [batch, state_size]. 如果是多层嵌套 rnn, 那么初始状态 <code>[batch, s] for s in state_size</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerRNNCell</span>(<span class="params">RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Subclass of RNNCells that act like proper `tf.Layer` objects.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __call__(self, inputs, state, scope=None, *args, **kwargs):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>Run this RNN cell on inputs, starting <span class="keyword">from</span> the given state.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line"></span><br><span class="line">      inputs: `<span class="number">2</span>-D` tensor <span class="keyword">with</span> shape `[batch_size, input_size]`.</span><br><span class="line"></span><br><span class="line">      state: <span class="keyword">if</span> `self.state_size` <span class="keyword">is</span> an integer, this should be a `<span class="number">2</span>-D Tensor`</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> shape `[batch_size, self.state_size]`.  Otherwise, <span class="keyword">if</span></span><br><span class="line"></span><br><span class="line">        `self.state_size` <span class="keyword">is</span> a <span class="built_in">tuple</span> of integers, this should be a <span class="built_in">tuple</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> shapes `[batch_size, s] <span class="keyword">for</span> s <span class="keyword">in</span> self.state_size`.</span><br><span class="line"></span><br><span class="line">      scope: optional cell scope.</span><br><span class="line"></span><br><span class="line">      *args: Additional positional arguments.</span><br><span class="line"></span><br><span class="line">      **kwargs: Additional keyword arguments.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line"></span><br><span class="line">      A pair containing:</span><br><span class="line"></span><br><span class="line">      - Output: A `<span class="number">2</span>-D` tensor <span class="keyword">with</span> shape `[batch_size, self.output_size]`.</span><br><span class="line"></span><br><span class="line">      - New state: Either a single `<span class="number">2</span>-D` tensor, <span class="keyword">or</span> a <span class="built_in">tuple</span> of tensors matching</span><br><span class="line"></span><br><span class="line">        the arity <span class="keyword">and</span> shapes of `state`.</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Bypass RNNCell&#x27;s variable capturing semantics for LayerRNNCell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Instead, it is up to subclasses to provide a proper build</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # method.  See the class docstring for more details.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return base_layer.Layer.__call__(self, inputs, state, scope=scope,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                                     *args, **kwargs)</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>



<h2 id="再看-Core-RNN-Cells"><a href="#再看-Core-RNN-Cells" class="headerlink" title="再看 Core RNN Cells"></a>再看 Core RNN Cells</h2><ul>
<li><p>tf.contrib.rnn.BasicRNNCell</p>
</li>
<li><p>tf.contrib.rnn.BasicLSTMCell</p>
</li>
<li><p>tf.contrib.rnn.GRUCell</p>
</li>
<li><p>tf.contrib.rnn.LSTMCell</p>
</li>
<li><p>tf.contrib.rnn.LayerNormBasicLSTMCell</p>
</li>
</ul>
<h3 id="tf-contrib-rnn-BasicRNNCell"><a href="#tf-contrib-rnn-BasicRNNCell" class="headerlink" title="tf.contrib.rnn.BasicRNNCell"></a>tf.contrib.rnn.BasicRNNCell</h3><p>直接扒源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.BasicRNNCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicRNNCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;The most basic RNN cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the RNN cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    name: String, the name of the layer. Layers with the same name will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      share weights, but to avoid mistakes we require reuse=True in such</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cases.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dtype: Default dtype of the layer (default of `None` means use the type</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      of the first input). Required when `build` is called before `call`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(BasicRNNCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Inputs must be 2-dimensional.</span></span><br><span class="line"></span><br><span class="line">    self.input_spec = base_layer.InputSpec(ndim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, inputs_shape</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> inputs_shape[<span class="number">1</span>].value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                       % inputs_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    input_depth = inputs_shape[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    self._kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + self._num_units, self._num_units])</span><br><span class="line"></span><br><span class="line">    self._bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.zeros_initializer(dtype=self.dtype))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Most basic RNN: output = new_state = act(W * input + U * state + B).&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    gate_inputs = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, state], <span class="number">1</span>), self._kernel)</span><br><span class="line"></span><br><span class="line">    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line"></span><br><span class="line">    output = self._activation(gate_inputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以发现 state_size = output_size = num_units, 以及输出就是下一个隐藏状态 output = new_state = act(W * input + U * state + B) = act(W[input, state] + b) 其中 self._kernel 表示 W, 其维度是 [input_depth + num_units, num_units]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tfe.executing_eagerly())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell = tf.contrib.rnn.BasicRNNCell(num_units=<span class="number">128</span>, activation=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cell.state_size, cell.output_size)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>128 128
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">32</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">h0 = cell.zero_state(batch_size=<span class="number">32</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = cell(inputs=inputs, state=h0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output.shape, state.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(32, 128) (32, 128)
</code></pre>
<h3 id="tf-contrib-rnn-BasicLSTMCell"><a href="#tf-contrib-rnn-BasicLSTMCell" class="headerlink" title="tf.contrib.rnn.BasicLSTMCell"></a>tf.contrib.rnn.BasicLSTMCell</h3><p>先回顾下 LSTM:</p>
<p><img src="https://panxiaoxie.cn/2018/05/07/cs224d-lecture9-machine-translation/lstm.png"></p>
<p>自己试着手敲公式～ 看着图还是简单，不看图是否也可以呢？</p>
<p>三个gate：遗忘门，输入/更新门，输出门</p>
<p>$$f_t=\sigma(W^{f}x_t + U^{f}h_{t-1})$$</p>
<p>$$i_t=\sigma(W^{i}x_t + U^{i}h_{t-1})$$</p>
<p>$$o_t=\sigma(W^{o}x_t + U^{o}h_{t-1})$$</p>
<p>new memory cell:</p>
<p>$$\hat c=tanh(W^cx_t + U^ch_{t-1})$$</p>
<p>输入门作用于新的记忆细胞,遗忘门作用于上一个记忆细胞，并得到最终的记忆细胞:</p>
<p>$$c_t=f_t\circ c_{t-1} + i_t\circ\hat c$$</p>
<p>用新的memory cell 和输出门得到新的隐藏状态：</p>
<p>$$h_t = tanh(o_t\circ c_t)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicLSTMCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Basic LSTM recurrent network cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The implementation is based on: http://arxiv.org/abs/1409.2329.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  We add forget_bias (default: 1) to the biases of the forget gate in order to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  reduce the scale of forgetting in the beginning of the training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  It does not allow cell clipping, a projection layer, and does not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  use peep-hole connections: it is the basic baseline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  For advanced models, please use the full @&#123;tf.nn.rnn_cell.LSTMCell&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  that follows.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               forget_bias=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               state_is_tuple=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize the basic LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: int, The number of units in the LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      forget_bias: float, The bias added to forget gates (see above).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Must set to `0.0` manually when restoring from CudnnLSTM-trained</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        checkpoints.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state_is_tuple: If True, accepted and returned states are 2-tuples of</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the `c_state` and `m_state`.  If False, they are concatenated</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        along the column axis.  The latter behavior will soon be deprecated.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      activation: Activation function of the inner states.  Default: `tanh`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: String, the name of the layer. Layers with the same name will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        share weights, but to avoid mistakes we require reuse=True in such</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        cases.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: Default dtype of the layer (default of `None` means use the type</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        of the first input). Required when `build` is called before `call`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      When restoring from CudnnLSTM-trained checkpoints, must use</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `CudnnCompatibleLSTMCell` instead.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(BasicLSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> state_is_tuple:</span><br><span class="line"></span><br><span class="line">      logging.warn(<span class="string">&quot;%s: Using a concatenated state is slower and will soon be &quot;</span></span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;deprecated.  Use state_is_tuple=True.&quot;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Inputs must be 2-dimensional.</span></span><br><span class="line"></span><br><span class="line">    self.input_spec = base_layer.InputSpec(ndim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._forget_bias = forget_bias</span><br><span class="line"></span><br><span class="line">    self._state_is_tuple = state_is_tuple</span><br><span class="line"></span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (LSTMStateTuple(self._num_units, self._num_units)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self._state_is_tuple <span class="keyword">else</span> <span class="number">2</span> * self._num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, inputs_shape</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> inputs_shape[<span class="number">1</span>].value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                       % inputs_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    input_depth = inputs_shape[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    h_depth = self._num_units</span><br><span class="line"></span><br><span class="line">    self._kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + h_depth, <span class="number">4</span> * self._num_units])</span><br><span class="line"></span><br><span class="line">    self._bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[<span class="number">4</span> * self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.zeros_initializer(dtype=self.dtype))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Long short-term memory cell (LSTM).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      inputs: `2-D` tensor with shape `[batch_size, input_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state: An `LSTMStateTuple` of state tensors, each shaped</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `[batch_size, num_units]`, if `state_is_tuple` has been set to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `True`.  Otherwise, a `Tensor` shaped</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `[batch_size, 2 * num_units]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A pair containing the new hidden state, and the new state (either a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `LSTMStateTuple` or a concatenated state, depending on</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `state_is_tuple`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    sigmoid = math_ops.sigmoid</span><br><span class="line"></span><br><span class="line">    one = constant_op.constant(<span class="number">1</span>, dtype=dtypes.int32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parameters of gates are concatenated into one multiply for efficiency.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">      c, h = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      c, h = array_ops.split(value=state, num_or_size_splits=<span class="number">2</span>, axis=one)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    gate_inputs = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, h], <span class="number">1</span>), self._kernel)</span><br><span class="line"></span><br><span class="line">    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span></span><br><span class="line"></span><br><span class="line">    i, j, f, o = array_ops.split(</span><br><span class="line"></span><br><span class="line">        value=gate_inputs, num_or_size_splits=<span class="number">4</span>, axis=one)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note that using `add` and `multiply` instead of `+` and `*` gives a</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># performance improvement. So using those at the cost of readability.</span></span><br><span class="line"></span><br><span class="line">    add = math_ops.add</span><br><span class="line"></span><br><span class="line">    multiply = math_ops.multiply</span><br><span class="line"></span><br><span class="line">    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),</span><br><span class="line"></span><br><span class="line">                multiply(sigmoid(i), self._activation(j)))</span><br><span class="line"></span><br><span class="line">    new_h = multiply(self._activation(new_c), sigmoid(o))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">      new_state = LSTMStateTuple(new_c, new_h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      new_state = array_ops.concat([new_c, new_h], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_h, new_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>阅读源码可以发现具体实现与上面的公式还是有点差别的。  </p>
<ul>
<li><p>先 concat[input, h], 然后 gate_input = matmul(concat[input, h], self._kernel)+self._bias,多了偏置项,这里的矩阵维度 [input_depth + h_depth,4*num_units]. 然后 i,j,f,o = split(gate_input, 4, axis=1). 其中 j 表示 new memory cell. 然后计算 new_c，其中 i,f,o 对应的激活函数确定是 sigmoid,因为其范围只能在(0,1)之间。但是 j 的激活函数self._activation 可以选择，默认是 tanh.  </p>
</li>
<li><p>与公式的差别之二在于 self._forget_bias.遗忘门在激活函数 $\sigma$ 之前加了偏置，目的是避免在训练初期丢失太多信息。    </p>
</li>
<li><p>要注意 state 的形式，取决于参数 self._state_is_tuple. 其中 c,h=state，表示 $c_{t-1},h_{t-1}$</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=<span class="number">128</span>, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>WARNING:tensorflow:From &lt;ipython-input-9-3f4ca183c5d7&gt;:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.

Instructions for updating:

This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name=&#39;basic_lstm_cell&#39;).
</code></pre>
<p>提示要更新了，那就换成最新的吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=<span class="number">128</span>, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lstm_cell.output_size, lstm_cell.state_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(128, LSTMStateTuple(c=128, h=128))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = lstm_cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>我们发现 lstm 的状态 state 是一个tuple,分别对应 c_t 和 h_t.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMStateTuple</span>(<span class="params">_LSTMStateTuple</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    and `h` is the output.</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<p>这里的解释感觉是有点问题的，<code>c</code> is the hidden state and <code>h</code> is the output. 看源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),</span><br><span class="line"></span><br><span class="line">            multiply(sigmoid(i), self._activation(j)))</span><br><span class="line"></span><br><span class="line">new_h = multiply(self._activation(new_c), sigmoid(o))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">  new_state = LSTMStateTuple(new_c, new_h)</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">  new_state = array_ops.concat([new_c, new_h], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> new_h, new_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>发现 c 表示的就是 new memory cell, 而 h 表示的是最后的隐藏状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 计算下一步的 output 和 state</span></span><br><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">30</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = lstm_cell(inputs, h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape, state[<span class="number">0</span>].shape, state[<span class="number">1</span>].shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">state.c, state.h  <span class="comment"># c 和 h 的值是不一样的</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(&lt;tf.Tensor: id=108, shape=(30, 128), dtype=float32, numpy=

 array([[ 0.08166471,  0.14020835,  0.07970127, ..., -0.1540019 ,

          0.38848224, -0.0842322 ],

        [-0.03643086, -0.20558938,  0.1503458 , ...,  0.01846285,

          0.15610473,  0.04408235],

        [-0.0933667 ,  0.03454542, -0.09073547, ..., -0.12701994,

         -0.34669587,  0.09373946],

        ...,

        [-0.00752909,  0.22412673, -0.270195  , ...,  0.09341058,

         -0.20986181, -0.18622127],

        [ 0.18778914,  0.37687936, -0.24727295, ..., -0.06409463,

          0.00218048,  0.5940756 ],

        [ 0.04073388, -0.08431841,  0.35944715, ...,  0.14135318,

          0.08472287, -0.11058106]], dtype=float32)&gt;,

 &lt;tf.Tensor: id=111, shape=(30, 128), dtype=float32, numpy=

 array([[ 0.04490132,  0.07412361,  0.03662094, ..., -0.07611651,

          0.17290959, -0.0277745 ],

        [-0.02212535, -0.13554382,  0.08272093, ...,  0.00918258,

          0.0861209 ,  0.02614526],

        [-0.05723168,  0.01372226, -0.02919216, ..., -0.06374882,

         -0.1918035 ,  0.03912015],

        ...,

        [-0.00377504,  0.15181372, -0.14555399, ...,  0.06073361,

         -0.09804281, -0.07492835],

        [ 0.10244624,  0.17440473, -0.09896267, ..., -0.03794969,

          0.00123257,  0.21985768],

        [ 0.01832823, -0.03795732,  0.1654894 , ...,  0.05827027,

          0.02769112, -0.05957894]], dtype=float32)&gt;)
</code></pre>
<h3 id="tf-nn-rnn-cell-GRUCell"><a href="#tf-nn-rnn-cell-GRUCell" class="headerlink" title="tf.nn.rnn_cell.GRUCell"></a>tf.nn.rnn_cell.GRUCell</h3><p>先回顾下 GRU.</p>
<p><img src="https://panxiaoxie.cn/2018/05/07/cs224d-lecture9-machine-translation/mt15.png"></p>
<p>手敲 GRU 公式：</p>
<p>$$r_t=\sigma(W^rx_t + U^rh_{t-1})$$</p>
<p>$$z_t=\sigma(W^zx_t + U^zh_{t-1})$$</p>
<p>$$\tilde h_t = tanh(Wx_t + r_t\circ h_{t-1})$$</p>
<p>$$h_t=(1-z_t)\circ\tilde h_t + z_t\circ h_{t-1}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.GRUCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GRUCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the GRU cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    kernel_initializer: (optional) The initializer to use for the weight and</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    projection matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    bias_initializer: (optional) The initializer to use for the bias.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    name: String, the name of the layer. Layers with the same name will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      share weights, but to avoid mistakes we require reuse=True in such</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cases.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dtype: Default dtype of the layer (default of `None` means use the type</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      of the first input). Required when `build` is called before `call`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               kernel_initializer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               bias_initializer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(GRUCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Inputs must be 2-dimensional.</span></span><br><span class="line"></span><br><span class="line">    self.input_spec = base_layer.InputSpec(ndim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line"></span><br><span class="line">    self._kernel_initializer = kernel_initializer</span><br><span class="line"></span><br><span class="line">    self._bias_initializer = bias_initializer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, inputs_shape</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> inputs_shape[<span class="number">1</span>].value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                       % inputs_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    input_depth = inputs_shape[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    self._gate_kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;gates/%s&quot;</span> % _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + self._num_units, <span class="number">2</span> * self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=self._kernel_initializer)</span><br><span class="line"></span><br><span class="line">    self._gate_bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;gates/%s&quot;</span> % _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[<span class="number">2</span> * self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=(</span><br><span class="line"></span><br><span class="line">            self._bias_initializer</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self._bias_initializer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span> init_ops.constant_initializer(<span class="number">1.0</span>, dtype=self.dtype)))</span><br><span class="line"></span><br><span class="line">    self._candidate_kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;candidate/%s&quot;</span> % _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + self._num_units, self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=self._kernel_initializer)</span><br><span class="line"></span><br><span class="line">    self._candidate_bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;candidate/%s&quot;</span> % _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=(</span><br><span class="line"></span><br><span class="line">            self._bias_initializer</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self._bias_initializer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span> init_ops.zeros_initializer(dtype=self.dtype)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Gated recurrent unit (GRU) with nunits cells.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    gate_inputs = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, state], <span class="number">1</span>), self._gate_kernel)</span><br><span class="line"></span><br><span class="line">    gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    value = math_ops.sigmoid(gate_inputs)</span><br><span class="line"></span><br><span class="line">    r, u = array_ops.split(value=value, num_or_size_splits=<span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    r_state = r * state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    candidate = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, r_state], <span class="number">1</span>), self._candidate_kernel)</span><br><span class="line"></span><br><span class="line">    candidate = nn_ops.bias_add(candidate, self._candidate_bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    c = self._activation(candidate)</span><br><span class="line"></span><br><span class="line">    new_h = u * state + (<span class="number">1</span> - u) * c</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_h, new_h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_LSTMStateTuple = collections.namedtuple(<span class="string">&quot;LSTMStateTuple&quot;</span>, (<span class="string">&quot;c&quot;</span>, <span class="string">&quot;h&quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>仔细阅读源码发现在 $sigma$ 计算 gate，以及 tanh 计算 candidate 之前都有偏置项，不过公式中都没写出来。而且在不设置 bias 的初始值时，默认的 GRU 中 gate_bias 的初始值是 1, 而 LSTM 中 gate_bias 的初始值是 0.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">gru_cell = tf.nn.rnn_cell.GRUCell(num_units=<span class="number">128</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">gru_cell.state_size, gru_cell.output_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<pre><code>(128, 128)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = gru_cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">30</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = gru_cell(inputs, h0)</span><br><span class="line"></span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<p>出现个很神奇的现象，如果我写成这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output, state = gru_cell.call(inputs, h0) <span class="comment"># 会报错的，gru_cell 没有 self._gate_kernel 这个属性，很神奇，</span></span><br><span class="line"></span><br><span class="line">                                          <span class="comment"># 不过这里先运行上面那行代码，所以没有出现报错</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="tf-nn-rnn-cell-LSTMCell-tf-contrib-rnn-LSTMCell"><a href="#tf-nn-rnn-cell-LSTMCell-tf-contrib-rnn-LSTMCell" class="headerlink" title="tf.nn.rnn_cell.LSTMCell, tf.contrib.rnn.LSTMCell"></a>tf.nn.rnn_cell.LSTMCell, tf.contrib.rnn.LSTMCell</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.LSTMCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Long short-term memory unit (LSTM) recurrent network cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The default non-peephole implementation is based on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    http://www.bioinf.jku.at/publications/older/2604.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  S. Hochreiter and J. Schmidhuber.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Long Short-Term Memory&quot;. Neural Computation, 9(8):1735-1780, 1997.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The peephole implementation is based on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://research.google.com/pubs/archive/43905.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Hasim Sak, Andrew Senior, and Francoise Beaufays.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Long short-term memory recurrent neural network architectures for</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   large scale acoustic modeling.&quot; INTERSPEECH, 2014.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The class uses optional peep-hole connections, optional cell clipping, and</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  an optional projection layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               use_peepholes=<span class="literal">False</span>, cell_clip=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               initializer=<span class="literal">None</span>, num_proj=<span class="literal">None</span>, proj_clip=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_unit_shards=<span class="literal">None</span>, num_proj_shards=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>, reuse=<span class="literal">None</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize the parameters for an LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>相比 BasicLSTMCell 多了这四个参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Args:</span><br><span class="line"></span><br><span class="line">    use_peepholes: <span class="built_in">bool</span>, <span class="built_in">set</span> <span class="literal">True</span> to enable diagonal/peephole connections.</span><br><span class="line"></span><br><span class="line">    cell_clip: (optional) A <span class="built_in">float</span> value, <span class="keyword">if</span> provided the cell state <span class="keyword">is</span> clipped</span><br><span class="line"></span><br><span class="line">        by this value prior to the cell output activation.</span><br><span class="line"></span><br><span class="line">    num_proj: (optional) <span class="built_in">int</span>, The output dimensionality <span class="keyword">for</span> the projection</span><br><span class="line"></span><br><span class="line">        matrices.  If <span class="literal">None</span>, no projection <span class="keyword">is</span> performed.</span><br><span class="line"></span><br><span class="line">    proj_clip: (optional) A <span class="built_in">float</span> value.  If `num_proj &gt; <span class="number">0</span>` <span class="keyword">and</span> `proj_clip` <span class="keyword">is</span></span><br><span class="line"></span><br><span class="line">        provided, then the projected values are clipped elementwise to within</span><br><span class="line"></span><br><span class="line">        `[-proj_clip, proj_clip]`.</span><br><span class="line"></span><br><span class="line">````</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">其中 cell_clip 很好理解，就是限制隐藏状态的大小，也就是 output 和 state 的大小。 而 num_proj 呢？</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> num_proj:</span><br><span class="line"></span><br><span class="line">      self._state_size = (</span><br><span class="line"></span><br><span class="line">          LSTMStateTuple(num_units, num_proj)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> state_is_tuple <span class="keyword">else</span> num_units + num_proj)</span><br><span class="line"></span><br><span class="line">      self._output_size = num_proj</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      self._state_size = (</span><br><span class="line"></span><br><span class="line">          LSTMStateTuple(num_units, num_units)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> state_is_tuple <span class="keyword">else</span> <span class="number">2</span> * num_units)</span><br><span class="line"></span><br><span class="line">      self._output_size = num_units</span><br><span class="line"></span><br><span class="line"> ````</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">通过源码可以发现，如果有 num_proj 那么 state 还要加一个全链接， state_size = num_units + num_proj. 而 proj_clip 是限制这个全链接的输出的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BacisLSTMCell 和 LSTMCell 区别还在于后者增加了 peephole 和 cell_clip</span><br><span class="line"></span><br><span class="line">![](https://img-blog.csdn.net/<span class="number">20171201095120010</span>?watermark/<span class="number">2</span>/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWJjbGhxMjAwNQ==/font/5a6L5L2T/fontsize/<span class="number">400</span>/fill/I0JBQkFCMA==/dissolve/<span class="number">70</span>/gravity/SouthEast)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">输入输出的shape，以及状态都是一样的，只不过内部计算方式更加复杂了。对于 peephole 是值得计算 gate 时，考虑到了 $c_&#123;t-<span class="number">1</span>&#125;$ 和 $c_t$.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"></span><br><span class="line">cell = tf.nn.rnn_cell.LSTMCell(num_units=<span class="number">64</span>,cell_clip=<span class="number">0.000000001</span>, num_proj=<span class="number">128</span>, proj_clip=<span class="number">0.001</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell.state_size, cell.output_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(LSTMStateTuple(c=64, h=128), 128)
</code></pre>
<p>发现 state_size 中的 h 维度发生了变化，相当于在每一个时间步得到的 state.h 之后再添加一个全链接。 在 decoder 中可以将 num_proj 设置为词表的大小，那么输出就是对应时间步的词表的分布。在此基础上在 softmax 就可以吧？ 但是下一个时间步的输入的隐藏状态 $h_{t-1}$ 岂不是维度为词表大小。。。感觉最好还是不用这个参数吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">h0.c.shape, h0.h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(TensorShape([Dimension(30), Dimension(64)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.ones(shape=[<span class="number">30</span>,<span class="number">50</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output, state = cell(inputs=inputs, state=h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape, state.c.shape, state.h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(64)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<h2 id="封装了-RNN-的其他组件"><a href="#封装了-RNN-的其他组件" class="headerlink" title="封装了 RNN 的其他组件"></a>封装了 RNN 的其他组件</h2><p>Core RNN Cell wrappers (RNNCells that wrap other RNNCells)</p>
<ul>
<li><p>tf.contrib.rnn.MultiRNNCell  </p>
</li>
<li><p>tf.contrib.rnn.LSTMBlockWrapper  </p>
</li>
<li><p>tf.contrib.rnn.DropoutWrapper  </p>
</li>
<li><p>tf.contrib.rnn.EmbeddingWrapper  </p>
</li>
<li><p>tf.contrib.rnn.InputProjectionWrapper  </p>
</li>
<li><p>tf.contrib.rnn.OutputProjectionWrapper  </p>
</li>
<li><p>tf.contrib.rnn.DeviceWrapper  </p>
</li>
<li><p>tf.contrib.rnn.ResidualWrapper  </p>
</li>
</ul>
<p>主要看 <code>tf.contrib.rnn.MultiRNNCell</code> 和 <code>tf.contrib.rnn.DropoutWrapper</code>吧，其他的封装的太好了也不好，用的其实也少。</p>
<h3 id="tf-contrib-rnn-MultiRNNCell"><a href="#tf-contrib-rnn-MultiRNNCell" class="headerlink" title="tf.contrib.rnn.MultiRNNCell"></a>tf.contrib.rnn.MultiRNNCell</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiRNNCell</span>(<span class="params">RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  RNN cell composed sequentially of multiple simple cells.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cells, state_is_tuple=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a RNN cell composed sequentially of a number of RNNCells.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cells: list of RNNCells that will be composed in this order.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state_is_tuple: If True, accepted and returned states are n-tuples, where</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `n = len(cells)`.  If False, the states are all</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        concatenated along the column axis.  This latter behavior will soon be</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        deprecated.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      ValueError: if cells is empty (not allowed), or at least one of the cells</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        returns a state tuple but the flag `state_is_tuple` is `False`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(MultiRNNCell, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> cells:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Must specify at least one cell for MultiRNNCell.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nest.is_sequence(cells):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;cells must be a list or tuple, but saw: %s.&quot;</span> % cells)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._cells = cells</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> cell_number, cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(self._cells):</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Add Checkpointable dependencies on these cells so their variables get</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># saved with this object when using object-based saving.</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(cell, checkpointable.CheckpointableBase):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># TODO(allenl): Track down non-Checkpointable callers.</span></span><br><span class="line"></span><br><span class="line">        self._track_checkpointable(cell, name=<span class="string">&quot;cell-%d&quot;</span> % (cell_number,))</span><br><span class="line"></span><br><span class="line">    self._state_is_tuple = state_is_tuple</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> state_is_tuple:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">any</span>(nest.is_sequence(c.state_size) <span class="keyword">for</span> c <span class="keyword">in</span> self._cells):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Some cells return tuples of states, but the flag &quot;</span></span><br><span class="line"></span><br><span class="line">                         <span class="string">&quot;state_is_tuple is not set.  State sizes are: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                         % <span class="built_in">str</span>([c.state_size <span class="keyword">for</span> c <span class="keyword">in</span> self._cells]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">tuple</span>(cell.state_size <span class="keyword">for</span> cell <span class="keyword">in</span> self._cells)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">sum</span>([cell.state_size <span class="keyword">for</span> cell <span class="keyword">in</span> self._cells])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._cells[-<span class="number">1</span>].output_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">zero_state</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ops.name_scope(<span class="built_in">type</span>(self).__name__ + <span class="string">&quot;ZeroState&quot;</span>, values=[batch_size]):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">tuple</span>(cell.zero_state(batch_size, dtype) <span class="keyword">for</span> cell <span class="keyword">in</span> self._cells)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We know here that state_size of each cell is not a tuple and</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># presumably does not contain TensorArrays or anything else fancy</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(MultiRNNCell, self).zero_state(batch_size, dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run this multi-layer cell on inputs, starting from state.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    cur_state_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    cur_inp = inputs</span><br><span class="line"></span><br><span class="line">    new_states = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(self._cells):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">&quot;cell_%d&quot;</span> % i):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> <span class="keyword">not</span> nest.is_sequence(state):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line"></span><br><span class="line">                <span class="string">&quot;Expected state to be a tuple of length %d, but received: %s&quot;</span> %</span><br><span class="line"></span><br><span class="line">                (<span class="built_in">len</span>(self.state_size), state))</span><br><span class="line"></span><br><span class="line">          cur_state = state[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">          cur_state = array_ops.<span class="built_in">slice</span>(state, [<span class="number">0</span>, cur_state_pos],</span><br><span class="line"></span><br><span class="line">                                      [-<span class="number">1</span>, cell.state_size])</span><br><span class="line"></span><br><span class="line">          cur_state_pos += cell.state_size</span><br><span class="line"></span><br><span class="line">        cur_inp, new_state = cell(cur_inp, cur_state)</span><br><span class="line"></span><br><span class="line">        new_states.append(new_state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_states = (<span class="built_in">tuple</span>(new_states) <span class="keyword">if</span> self._state_is_tuple <span class="keyword">else</span></span><br><span class="line"></span><br><span class="line">                  array_ops.concat(new_states, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cur_inp, new_states</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>参数 cell 是元素为 RNNCell对象 的 list 或 tuple. 其实还是只是计算一个时间步的 state</p>
<p><img src="https://panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn12.png"></p>
<p>这里先不考虑双向，只考虑 deep. 也就是使用 MultiRNNCell</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">num_units = [<span class="number">64</span>, <span class="number">128</span>]</span><br><span class="line"></span><br><span class="line">stack_rnns = [tf.nn.rnn_cell.BasicLSTMCell(num_units=i) <span class="keyword">for</span> i <span class="keyword">in</span> num_units]</span><br><span class="line"></span><br><span class="line">stack_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(stack_rnns)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = [cell.zero_state(batch_size=<span class="number">32</span>, dtype=tf.float32) <span class="keyword">for</span> cell <span class="keyword">in</span> stack_rnn]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">32</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = stack_rnn_cell(inputs=inputs, state=h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>TensorShape([Dimension(32), Dimension(128)])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">state[<span class="number">0</span>].c.shape, state[<span class="number">0</span>].h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(TensorShape([Dimension(32), Dimension(64)]),

 TensorShape([Dimension(32), Dimension(64)]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">state[<span class="number">1</span>].c.shape, state[<span class="number">1</span>].h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(TensorShape([Dimension(32), Dimension(128)]),

 TensorShape([Dimension(32), Dimension(128)]))
</code></pre>
<p>源码中的一部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cur_inp, new_state = cell(cur_inp, cur_state)</span><br><span class="line"></span><br><span class="line">new_states.append(new_state)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>其中从源码中也可以发现把每一层的 state 都储存起来了，而 output 要作为下一层的输入，最后得到的 output 是最后一层的输出。</p>
<h3 id="tf-contrib-rnn-DropoutWrapper"><a href="#tf-contrib-rnn-DropoutWrapper" class="headerlink" title="tf.contrib.rnn.DropoutWrapper"></a>tf.contrib.rnn.DropoutWrapper</h3><p>参考paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.DropoutWrapper&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DropoutWrapper</span>(<span class="params">RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Operator adding dropout to inputs and outputs of the given cell.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cell, input_keep_prob=<span class="number">1.0</span>, output_keep_prob=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               state_keep_prob=<span class="number">1.0</span>, variational_recurrent=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               input_size=<span class="literal">None</span>, dtype=<span class="literal">None</span>, seed=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_state_filter_visitor=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a cell with added input, state, and/or output dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If `variational_recurrent` is set to `True` (**NOT** the default behavior),</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    then the same dropout mask is applied at every step, as described in:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Y. Gal, Z Ghahramani.  &quot;A Theoretically Grounded Application of Dropout in</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Recurrent Neural Networks&quot;.  https://arxiv.org/abs/1512.05287</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    如果参数 variational_recurrent 设置为 True，那么 dropout 在每一个时间步都会执行 dropout，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Otherwise a different dropout mask is applied at every time step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note, by default (unless a custom `dropout_state_filter` is provided),</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    the memory state (`c` component of any `LSTMStateTuple`) passing through</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    a `DropoutWrapper` is never modified.  This behavior is described in the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    above article.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cell: an RNNCell, a projection to output_size is added to it.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      input_keep_prob: unit Tensor or float between 0 and 1, input keep</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probability; if it is constant and 1, no input dropout will be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      output_keep_prob: unit Tensor or float between 0 and 1, output keep</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probability; if it is constant and 1, no output dropout will be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state_keep_prob: unit Tensor or float between 0 and 1, output keep</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probability; if it is constant and 1, no output dropout will be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        State dropout is performed on the outgoing states of the cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        **Note** the state components to which dropout is applied when</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `state_keep_prob` is in `(0, 1)` are also determined by</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the argument `dropout_state_filter_visitor` (e.g. by default dropout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        is never applied to the `c` component of an `LSTMStateTuple`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      上面三个参数分别表示 input，output，state 是否 dropout，以及 dropout 率。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      variational_recurrent: Python bool.  If `True`, then the same</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        dropout pattern is applied across all time steps per run call.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If this parameter is set, `input_size` **must** be provided.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      这个参数如果为 True，那么每一个时间步都需要 dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      input_size: (optional) (possibly nested tuple of) `TensorShape` objects</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        containing the depth(s) of the input tensors expected to be passed in to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the `DropoutWrapper`.  Required and used **iff**</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         `variational_recurrent = True` and `input_keep_prob &lt; 1`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: (optional) The `dtype` of the input, state, and output tensors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Required and used **iff** `variational_recurrent = True`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      seed: (optional) integer, the randomness seed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_state_filter_visitor: (optional), default: (see below).  Function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        that takes any hierarchical level of the state and returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        a scalar or depth=1 structure of Python booleans describing</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        which terms in the state should be dropped out.  In addition, if the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        function returns `True`, dropout is applied across this sublevel.  If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the function returns `False`, dropout is not applied across this entire</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        sublevel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Default behavior: perform dropout on all terms except the memory (`c`)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        state of `LSTMCellState` objects, and don&#x27;t try to apply dropout to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `TensorArray` objects:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        but not `callable`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      ValueError: if any of the keep_probs are not between 0 and 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.LSTMCell(num_units=<span class="number">128</span>),</span><br><span class="line"></span><br><span class="line">                                     input_keep_prob=<span class="number">1.0</span>,</span><br><span class="line"></span><br><span class="line">                                     output_keep_prob=<span class="number">1.0</span>,</span><br><span class="line"></span><br><span class="line">                                     state_keep_prob=<span class="number">1.0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell.state_size, cell.output_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(LSTMStateTuple(c=128, h=128), 128)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 多层 rnn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.nn.rnn_cell <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">NUM_UNITS = [<span class="number">32</span>,<span class="number">64</span>, <span class="number">128</span>]</span><br><span class="line"></span><br><span class="line">rnn = MultiRNNCell([DropoutWrapper(LSTMCell(num_units=n), output_keep_prob=<span class="number">0.8</span>) <span class="keyword">for</span> n <span class="keyword">in</span> NUM_UNITS])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rnn.output_size, rnn.state_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(128,

 (LSTMStateTuple(c=32, h=32),

  LSTMStateTuple(c=64, h=64),

  LSTMStateTuple(c=128, h=128)))
</code></pre>
<h2 id="tf-nn-dynamic-rnn"><a href="#tf-nn-dynamic-rnn" class="headerlink" title="tf.nn.dynamic_rnn"></a>tf.nn.dynamic_rnn</h2><p>最后前面说了这么多 class，他们都只是一种计算当前时间步的 output 和 state 的方式，但是 rnn 处理的都是序列，所以怎么将这些 cell 对象封装到序列 rnn 中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_rnn</span>(<span class="params">cell, inputs, sequence_length=<span class="literal">None</span>, initial_state=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                dtype=<span class="literal">None</span>, parallel_iterations=<span class="literal">None</span>, swap_memory=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                time_major=<span class="literal">False</span>, scope=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates a recurrent neural network specified by RNNCell `cell`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Performs fully dynamic unrolling of `inputs`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rnn_layers = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=n)) <span class="keyword">for</span> n <span class="keyword">in</span> [<span class="number">32</span>, <span class="number">64</span>]]</span><br><span class="line"></span><br><span class="line">cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">30</span>, <span class="number">10</span>, <span class="number">100</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">initial_state = cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output, state = tf.nn.dynamic_rnn(cell,inputs=inputs, initial_state=initial_state, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>TensorShape([Dimension(30), Dimension(10), Dimension(64)])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell.state_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(LSTMStateTuple(c=32, h=32), LSTMStateTuple(c=64, h=64))
</code></pre>
<p>所以目前为止，暂时ojbk了～～ 接下来就是在 attention 封装 rnn 了</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-25T03:25:02.000Z" title="2018/8/25 上午11:25:02">2018-08-25</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">23 分钟读完 (大约3523个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/">论文笔记 Pointer Networks and copy mechanism</a></h1><div class="content"><p>paper:  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5866-pointer-networks">Pointer Networks, NIPS, 2015</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06393">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
</li>
</ul>
<h2 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence.   </p>
</blockquote>
<p>提出来了一种新的架构来学习得到这样的输出序列的条件概率，其中输出序列中的元素是输入序列中离散的 tokens.  </p>
<blockquote>
<p>Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable.  </p>
</blockquote>
<p>这样简单的从输入序列中 copy 输出相关的序列在 seq2seq 或是神经图灵机都很难实现，因为在 decoder 的每一步输出的次的类别依赖于输入序列的长度，这个长度是变化的。</p>
<blockquote>
<p>Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class.  </p>
</blockquote>
<p>和这类问题类似的还有给不定长序列的排序，组合优化等问题。  </p>
<blockquote>
<p> It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output.  </p>
</blockquote>
<p>同之前的 attention 不同的是，之前的 attention 是 decoder 时每一步计算通过 RNN 编码后的输入序列的隐藏变量与当前向量表示的 attention vector，然后生成当前词。而 Ptr-Net 则是使用 attention 作为指针，从输入序列中选择成员作为输出。  </p>
<blockquote>
<p>We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems – finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem – using training examples</p>
</blockquote>
<p>alone.  </p>
<p>Ptr-Net 可以用来学习类似的三个几何问题。</p>
<blockquote>
<p>Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on.  </p>
</blockquote>
<p>Ptr-Net 不仅可以提升 seq2seq with attention,而且能够泛化到变化的 dictionayies.  </p>
<p>从摘要以及 Introduction 来说， Ptr-Net 主要是解决两个方面的问题。  </p>
<ul>
<li><p>一是，简单的 copy 在传统的方法中很难实现，而 Ptr-Net 则是直接从输入序列中生成输出序列。  </p>
</li>
<li><p>而是，可以解决输出 dictionary 是变化的情况。普通的 Seq2Seq 的 output dictionary 大小是固定的，对输出中包含有输入单词(尤其是 OOV 和 rare word) 的情况很不友好。一方面，训练中不常见的单词的 word embedding 质量也不高，很难在 decoder 时预测出来，另一方面，即使 word embedding 很好，对一些命名实体，像人名等，word embedding 都很相似，也很难准确的 reproduce 出输入提到的单词。Point Network 以及在此基础上后续的研究 CopyNet 中的 copy mechanism 就可以很好的处理这种问题，decoder 在各 time step 下，会学习怎样直接 copy 出现在输入中的关键字。</p>
</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/01.png">  </p>
<p>在介绍 Ptr-Net 之前，作者先回顾了一下基本模型 seq2seq 和  input-attention.  </p>
<h4 id="sequence-to-sequence-Model"><a href="#sequence-to-sequence-Model" class="headerlink" title="sequence-to-sequence Model"></a>sequence-to-sequence Model</h4><p>实际上 seq2seq 解决的问题是在当前样本空间里面，给定输入下，使得输出序列的概率最大化。其实类似的 MT，QA，Summarization 都可以看作是这一类问题。只不过根据输入和输出之间的关系，调整相应的模型。  </p>
<p>$$p(C^P|P;\theta)=\sum_{i=1}^m(P)p_{\theta}(C_i|C_1,…,C_{i-1},P;\theta)$$</p>
<p>通过训练学习得到参数使得条件概率最大：  </p>
<p>$$\theta^* = {argmax}<em>{\theta}\sum</em>{P,C^P}logp(C^P|P;\theta)$$</p>
<p>其中类和是在训练样本上。</p>
<blockquote>
<p>In this sequence-to-sequence model, the output dictionary size for all symbols $C_i$ is fixed and equal to n, since the outputs are chosen from the input. Thus, we need to train a separate model for each n. This prevents us from learning solutions to problems that have an output dictionary with a size that depends on the input sequence length.   </p>
</blockquote>
<p>在 seq2seq 模型中，输出的 dictionary 是固定大小的。因为不能解决 dictionary 是变化的情况。</p>
<h4 id="Content-Based-Input-Attention"><a href="#Content-Based-Input-Attention" class="headerlink" title="Content Based Input Attention"></a>Content Based Input Attention</h4><p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"></p>
<p>在每一个 decoder step，先计算 $e_{ij}$ 得到对齐概率(或者说 how well input position j matches output position i)，然后做一个 softmax 得到 $a_{ij}$，再对 $a_{ij}$ 做一个加权和作为 context vector $c_i$，得到这个 context vector 之后在固定大小的 output dictionary 上做 softmax 预测输出的下一个单词。</p>
<blockquote>
<p>This model performs significantly better than the sequence-to-sequence model on the convex hull problem, but it is not applicable to problems where the output dictionary size depends on the input.  </p>
</blockquote>
<p>Nevertheless, a very simple extension (or rather reduction) of the model allows us to do this easily.</p>
<h4 id="Ptr-Net"><a href="#Ptr-Net" class="headerlink" title="Ptr-Net"></a>Ptr-Net</h4><p>seq2seq 模型的输出词是在固定的 dictionary 中进行 softmax，并选择概率最大的词，从而得到输出序列。但这里的输出 dictionary size 是取决于 input 序列的长度的。所以作者提出了新的模型，其实很简单。</p>
<p>$$u_j^i=v^Ttanh(W_1e_j+W_2d_i) ，j\in(1,…,n)$$</p>
<p>$$p(C_i|C_1,…,C_{i-1},P)=softmax(u^i)$$</p>
<p>i 表示decoder 的时间步，j 表示输入序列中的index. 所以$e_j$ 是 encoder 编码后的隐藏向量，$d_i$ 是 decoder 当前时间步 i 的隐藏向量。跟一般的 attention 基本上一致。只不过得到的 softmax 概率应用在输入序列 $C_1,…,C_{i-1}$ 上。</p>
<h4 id="Dataset-Structure"><a href="#Dataset-Structure" class="headerlink" title="Dataset Structure"></a>Dataset Structure</h4><ul>
<li>TensorFlow implementation of “Pointer Networks”：<a target="_blank" rel="noopener" href="https://github.com/devsisters/pointer-network-tensorflow">https://github.com/devsisters/pointer-network-tensorflow</a></li>
</ul>
<ul>
<li>Dataset：<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/0B2fg8yPGn2TCMzBtS0o4Q2RJaEU">https://drive.google.com/drive/folders/0B2fg8yPGn2TCMzBtS0o4Q2RJaEU</a></li>
</ul>
<h2 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation.  </p>
</blockquote>
<p>还是前面提到的问题，seq2seq 很难解决简单的 copy 问题。而在人类的对话中，出现 copy 的现象是很常见的。尤其是 命令实体 或者是长短语。</p>
<blockquote>
<p>The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation.  </p>
</blockquote>
<p>这也是 seq2seq 模型所需面对的挑战。</p>
<p>For example:  </p>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/03.png"></p>
<p>可以看到，对于 Chandralekha 这类实体词，可能是 OOV，也可能是其他实体或者是日期等很难被 decoder “还原” 出来的信息，CopyNet 可以更好的处理这类的信息。</p>
<p>那么问题来了：  </p>
<ul>
<li><p>What to copy: 输入中的哪些部分应该被 copy?  </p>
</li>
<li><p>Where to paste: 应该把这部分信息 paste 到输出的哪个位置？</p>
</li>
</ul>
<h3 id="Model-Architecture-1"><a href="#Model-Architecture-1" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>作者从两个角度来理解 CopyNet:  </p>
<ul>
<li><p>From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity.  从认知学角度，copy机制近似于死记硬背，不需要太多的理解，但是要保证文字的保真度。  </p>
</li>
<li><p>From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model.  从模型的角度，copy 操作更加死板和符号化，这也使得相比 soft attention 机制更难整合到一个完整的可微分的神经模型中去。  </p>
</li>
</ul>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/04.png"></p>
<p>整体还是基于 encoder-decoder 模型。</p>
<p><strong>Encoder:</strong>    </p>
<p>LSTM 将 source sequence 转换为隐藏状态 M(emory) $h_1,…,h_{T_S}$.</p>
<p><strong>Decoder:</strong>    </p>
<p>同 cannonical 的 decoder 一样，使用 RNN 读取 encoder 的隐藏状态 M. 但和传统的 decoder 不一样，他有如下区别：  </p>
<ul>
<li><strong>Prediction:</strong> COPYNET predicts words based on a mixed probabilistic model of two modes, namely the generate-mode and the copymode, where the latter picks words from the source sequence. 下一个词的预测由两种模式混合而成。生成 generate-mode 和 copy-mode. 后者就像前面 Ptr-Net 所说的，在 source sentence 获取词。</li>
</ul>
<ul>
<li><strong>State Update:</strong>  the predicted word at time t−1 is used in updating the state at t, but COPYNET uses not only its word-embedding but also its corresponding location-specific hidden state in M (if any). 更新 decoder 中的隐藏状态时，t 时间步的隐藏状态不仅与 t-1 步生成词的 embedding vector 有关，还与这个词对应于 source sentence 中的隐藏状态的位置有关。</li>
</ul>
<ul>
<li><strong>Reading M:</strong> in addition to the attentive read to M, COPYNET also has“selective read” to M, which leads to a powerful hybrid of</li>
</ul>
<p>content-based addressing and location-based addressing. 什么时候需要 copy，什么时候依赖理解来回答，怎么混合这两种模式很重要。</p>
<blockquote>
<p>个人思考： 感觉不管要不要 copy 都应该是在基于理解的基础上进行的。但是因为 OOV 或者当前词的 embedding vector 训练的不好，那就无法理解了对吧？ 是否可以添加 gate 机制呢？ 机器到底还是没理解语言对吧？ 貌似是个可以创新的点。</p>
</blockquote>
<p>接下来会详细讲解这三个不同之处怎么实现的。</p>
<h4 id="Prediction-with-Copying-and-Generation-s-t-rightarrow-y-t"><a href="#Prediction-with-Copying-and-Generation-s-t-rightarrow-y-t" class="headerlink" title="Prediction with Copying and Generation:$s_t\rightarrow y_t$"></a>Prediction with Copying and Generation:$s_t\rightarrow y_t$</h4><p>这部分是从 decoder 隐藏状态 $s_t$ 到输出词 $y_t$ 的过程。传统的encoder-decoder 是一个线性映射就可以了。</p>
<p>词表 $\mathcal{V}={v_1,…,v_N}$, 未登录词 OOV(out of vocabulary) 用 UNK 来表示（unk应该也会有对应的 embedding vector）. 以及用来表示输入序列中的 unique words $X={x_1,…,x_{T_S}}$. 其中 X 使得 copynet 输出 OOV.</p>
<p>对于三者有这样的集合关系（先不要看公式，后面会说到）：</p>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/05.png"></p>
<p>简而言之(In a nutshell), 对于当前 source sentence X 输出的词表范围 $\mathcal{V}\cup \text{UNK} \cup X$.</p>
<p>给定 decoder 中当前时间步的隐藏状态 $s_t$, 以及 encoder 的隐藏状态序列 M.</p>
<p>$$p(y_t|s_t,y_{t-1},c_t,M)=p(y_t,g|s_t,y_{t-1},c_t,M) + p(y_t,c|s_t,y_{t-1},c_t,M)$$</p>
<p>其中 g 代表 generate mode. c 代表 copy mode.</p>
<p>我们知道对于 encoder 部分的输出 $h_1,…,h_{T_S}$， 记做 M，M 其实同时包含了语义和位置信息。那么 decoder 对 M 的读取有两种形式：</p>
<ul>
<li>Content-base  </li>
</ul>
<p>Attentive read from word-embedding</p>
<ul>
<li>location-base  </li>
</ul>
<p>Selective read from location-specific hidden units</p>
<p>两种模式对应的概率计算，以及 score function:  </p>
<p>$$p(y_t,g|\cdot)=\begin{cases} \dfrac{1}{Z}e^{\psi_g(y_t)}&amp;y_t\in V\</p>
<p>0,&amp;y_t\in X \bigcap \overline V\</p>
<p>\dfrac{1}{Z}e^{\psi_g(UNK)},&amp;y_t\notin V\cup X</p>
<p>\end{cases}$$</p>
<p>$$p(y_t,c|\cdot)=\begin{cases}\dfrac{1}{Z}\sum_{j:x_j=y_t}{e^{\psi_c(x_j)}},&amp;y_t\in X\0&amp;\text {otherwise}\end{cases}$$</p>
<p>上面两个公式叠加(相加)可以表示为下图（可以将目标词看作类别为 4 的分类。）：  </p>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/05.png"></p>
<p>其中 $\psi_g(\cdot)$ 和 $\psi_c(\cdot)$ 是 generate mode 和 copy mode 的 score function.</p>
<p>Z 是两种模型共享的归一化项，$Z=\sum_{v\in V\cup{UNK}}e^{\psi_g(v)}+\sum_{x\in X}e^{\psi_c(x)}$.</p>
<p>然后对相应的类别计算对应的 score.</p>
<p><strong>Generate-Mode:</strong>  </p>
<p>$$\psi_g(y_t=v_i)=\nu_i^TW_os_t, v_i\in V\cup UNK$$</p>
<ul>
<li><p>$W_o\in R^{(N+1)\times d_s}$  </p>
</li>
<li><p>$\nu_i$ 是 $v_i$ 对应的 one-hot 向量. 得到的结果是当前词的概率。</p>
</li>
</ul>
<p>generate-mode 的 score $\psi(y_t=v_i)$ 和普通的 encoder-decoder 是一样的。全链接之后的 softmax.</p>
<p><strong>copy-mode:</strong>  </p>
<p>$$\psi(y_t=x_j)=\sigma(h_j^TW_c)s_t,x_j\in \mathcal{V}$$</p>
<ul>
<li><p>$h_j$ 是 encoder hidden state. j 表示输入序列中的位置。</p>
</li>
<li><p>$W_c\in R^{d_h\times d_s}$ 将 $h_j$ 映射到跟 $s_t$ 一样的语义空间。  </p>
</li>
<li><p>作者发现使用 tanh 非线性变换效果更好。同时考虑到 $y_t$ 这个词可能在输入中出现多次，所以需要考虑输入序列中所有的为 $y_t$ 的词的概率的类和。</p>
</li>
</ul>
<h4 id="state-update"><a href="#state-update" class="headerlink" title="state update"></a>state update</h4><p>上面一部分讲的是怎么从 decoder 中的隐藏状态计算对应的 vocabulary，也就是 $s_t\rightarrow y_t$. 那么怎么计算当前时间步的隐藏状态呢？ 我们知道传统的 encoder-decoder 中隐藏状态就是 content-based atention vector. 但是在 copynet 里面，作者对 $y_{t-1}\rightarrow s_t$ 这个计算方式做了一定的修改。  </p>
<p>先回顾下基本的 attention 模块，decoder 中隐藏状态的更新 $s_t=f(y_{t-1},s_{t-1},c_t)$, 其中 $c_t$ 也就是 attention 机制：</p>
<p>$$c_t=\sum_{\tau=1}^{T_S}\alpha_{t\tau}$$</p>
<p>$$\alpha_{t\tau}=\dfrac{e^{\eta(s_{t-1},h_{\tau})}}{\sum_{\tau’}e^{\eta(s_{t-1},h_{\tau’})}}$$</p>
<p>CopyNet 的 $y_{t-1}$ 在这里有所不同。不仅仅考虑了词向量，还使用了 M 矩阵中特定位置的 hidden state，或者说，$y_{t−1}$ 的表示中就包含了这两个部分的信息 $[e(y_{t−1});\zeta(y_{t−1})]$，$e(y_{t−1})$ 是词向量，后面多出来的一项 $\zeta(y_{t−1})$ 叫做 selective read, 是为了连续拷贝较长的短语。和attention 的形式差不多，是 M 矩阵中 hidden state 的加权和.</p>
<p>$$\zeta(y_{t-1})=\sum_{\tau=1}^{T_S}\rho_{t\tau}h_{\tau}$$</p>
<p>$$\rho_{t\tau}=\begin{cases}\dfrac{1}{K}p(x_{\tau},c|s_{t-1},M),&amp; x_{\tau}=y_{t-1}\</p>
<p>0,&amp; \text{otherwise}</p>
<p>\end{cases}$$</p>
<ul>
<li><p>当 $y_{t-1}$ 没有出现在 source sentence中时， $\zeta(y_{t-1})=0$.  </p>
</li>
<li><p>这里的 $K=\sum{\tau’:x_{\tau’}=y_{t-1}}p(x_{\tau’},c|s_{t-1},M)$ 是类和。还是因为输入序列中可能出现多个当前词，但是每个词在 encoder hidden state 的向量表示是不一样的，因为他们的权重也是不一样的。  </p>
</li>
<li><p>这里的 p 没有给出解释，我猜跟前面计算 copy 的 score 是一致的？  </p>
</li>
<li><p>直观上来看，当 $\zeta(y_{t-1})$ 可以看作是选择性读取 M (selective read). 先计算输入序列中对应所有 $y_{t-1}$ 的权重，然后加权求和，也就是 $\zeta(y_{t-1})$.</p>
</li>
</ul>
<h4 id="Hybrid-Adressing-of-M"><a href="#Hybrid-Adressing-of-M" class="headerlink" title="Hybrid Adressing of M"></a>Hybrid Adressing of M</h4><p>包括两种 Addressing 方式： content-based and location-based assressing.</p>
<p><strong>location-based Addressing:</strong>  </p>
<p>$$\zeta(y_{t-1}) \longrightarrow{update} \ s_t \longrightarrow predict \ y_t \longrightarrow sel. read \zeta(y_t)$$</p>
<h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>最小化概率的负对数：  </p>
<p>$$L=-\dfrac{1}{N}\sum_{k=1}^N\sum_{t=1}^Tlog[p(y_t^{(k)}|y_{&lt;t}^{(k)}, X^{(k)})]$$</p>
<p>N 是batch size，T 是 object sentence 长度。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-14T01:34:42.000Z" title="2018/8/14 上午9:34:42">2018-08-14</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.141Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">14 分钟读完 (大约2028个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/">论文笔记-Match LSTM</a></h1><div class="content"><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>SQuAD the answers do not come from a small set of candidate</p>
</blockquote>
<p>answers and they have variable lengths. We propose an end-to-end neural architecture for the task.  </p>
<p>针对 SQuAD 这样的阅读理解式任务提出的端到端的模型。 SQuAD 的答案不是从候选词中提取，而是类似于人类的回答，是不同长度的句子。  </p>
<blockquote>
<p>The architecture is based on match-LSTM, a model we proposed</p>
</blockquote>
<p>previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences.   </p>
<p>主要是基于 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.03134">Pointer Networks</a></p>
<p>关于阅读理解的数据集 benchmark dataset：  </p>
<ul>
<li><p>MCTest: A challenge dataset for the open-domain machine comprehension of text.  </p>
</li>
<li><p>Teaching machines to read and comprehend.  </p>
</li>
<li><p>The Goldilocks principle: Reading children’s books with explicit memory representations.  </p>
</li>
<li><p>Towards AI-complete question answering: A set of prerequisite toy tasks.  </p>
</li>
<li><p>SQuAD: 100,000+ questions for machine comprehension of text.  </p>
</li>
</ul>
<p><strong>SQuAD</strong>  </p>
<p><img src="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/01.png">  </p>
<blockquote>
<p>Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering, including syntactic parsing, named entity recognition, question classification, semantic parsing, etc. Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension.  </p>
</blockquote>
<p>传统的智能问答任务整个流程包括 句法分析、命名实体识别、问题分类、语义分析等。。随着深度学习的发展，端到端的模型开始出现。</p>
<p>End-to-end model architecture:  </p>
<ul>
<li><p>Teaching machines to read and comprehend.  </p>
</li>
<li><p>The Goldilocks principle: Reading children’s books with explicit memory representations.  </p>
</li>
<li><p>Attention-based convolutional neural network for machine comprehension  </p>
</li>
<li><p>Text understanding with the attention sum reader network.  </p>
</li>
<li><p>Consensus attention-based neural networks for chinese reading comprehension.  </p>
</li>
</ul>
<blockquote>
<p>However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.  </p>
</blockquote>
<p>之前的模型的 answer 要么是从候选答案中选择，要么是一个简单的符号。这都不适合 SQuDA.  </p>
<p>模型是基于作者早期提出的用于 textual entailment 的 match-LSTM<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.08849">Learning natural language inference with LSTM</a>，然后进一步应用了 Pointer Net(<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5866-pointer-networks">https://papers.nips.cc/paper/5866-pointer-networks</a>), 从而允许预测的结果能够从输入中获得，而不是从一个固定的词表中获取。</p>
<blockquote>
<p>We propose two ways to apply the Ptr-Net model for our task: a sequence model and a boundary model. We also further extend the boundary model with a search mechanism.  </p>
</blockquote>
<p>作者提出的两种模型。</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><h3 id="Match-LSTM"><a href="#Match-LSTM" class="headerlink" title="Match-LSTM"></a>Match-LSTM</h3><h3 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h3><p><strong>Pointer Network (Ptr-Net) model</strong> : to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence. Instead of picking an output token from a fixed vocabulary, Ptr-Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol.   </p>
<p>从输入 sentences 中生成 answer.</p>
<p>类似于 Pointer Network 的模型：   </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06393">Incorporating copying mechanism in sequence-to-sequence learning.</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.01547">Text understanding with the attention sum reader network.</a></p>
</li>
</ul>
<h3 id="MATCH-LSTM-AND-ANSWER-POINTER"><a href="#MATCH-LSTM-AND-ANSWER-POINTER" class="headerlink" title="MATCH-LSTM AND ANSWER POINTER"></a>MATCH-LSTM AND ANSWER POINTER</h3><p><img src="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/02.png">  </p>
<p>模型主要分为3部分：</p>
<ul>
<li><p>An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs. 使用 LSTM 处理 question 和 passage.  </p>
</li>
<li><p>A match-LSTM layer that tries to match the passage against the question. 使用 match-LSTM 对lstm编码后的 question 和 passage 进行匹配。  </p>
</li>
<li><p>An Answer Pointer (Ans-Ptr) layer that uses Ptr-Net to select a set of tokens from the passage as the answer. The difference between the two models only lies in the third layer.  使用 Pointer 来选择 tokens.  </p>
</li>
</ul>
<h4 id="LSTM-preprocessing-Layer"><a href="#LSTM-preprocessing-Layer" class="headerlink" title="LSTM preprocessing Layer"></a>LSTM preprocessing Layer</h4><p>$$H^p=\overrightarrow {LSTM}(P), H^q=\overrightarrow {LSTM}(Q)$$</p>
<p>直接使用单向LSTM，每一个时刻的隐含层向量输出 $H^p\in R^{l\times P}, H^q\in R^{l\times Q}$ 只包含左侧上下文信息.</p>
<h4 id="Match-LSTM-Layer"><a href="#Match-LSTM-Layer" class="headerlink" title="Match-LSTM Layer"></a>Match-LSTM Layer</h4><p>$$\overrightarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\overrightarrow {h^r}_{i-1}+b^p)\otimes e_Q)\in R^{l\times Q}$$</p>
<p>$$\overrightarrow \alpha_i=softmax(w^T\overrightarrow G_i + b\otimes e_Q)\in R^{1\times Q}$$</p>
<p>the resulting attention weight $\overrightarrow α_{i,j}$ above indicates the degree of matching between the</p>
<p>$i^{th}$ token in the passage with the $j^{th}$ token in the question.   </p>
<p>其中 $W^q,W^p,W^r \in R^{l\times l}, b^p,w\in R^l, b\in R$</p>
<p>所以 $\overrightarrow α_{i}$ 表示整个 question 与 passage 中的第 i 个词之间的 match 程度，也就是通常理解的 attention 程度。  </p>
<blockquote>
<p>传统的 attention 就是将 passage 和 question 矩阵相乘，比如 transformer 中 query 和 keys 相乘。复杂一点可能就是 dynamic memory networks 中的将 两个需要 match 的向量相减、element-wise相乘之后，使用两层的前馈神经网络来表示。  </p>
</blockquote>
<p>这里的 attention score 的计算方式又不一样了。 $\overrightarrow{h^r_{i-1}}$ 是通过 LSTM 耦合 weighted queston 和 passage 中上一个词得到的信息。</p>
<p>其中：</p>
<p>$$\overrightarrow z_i=\begin{bmatrix} h^p \ H^q\overrightarrow {\alpha_i^T} \ \end{bmatrix} $$</p>
<p>$$h^r=\overrightarrow{LSTM}(\overrightarrow{z_i},\overrightarrow{h^r_{i-1}})$$  </p>
<p>然后类似于LSTM将 $\overrightarrow{h_{i-1}^r}$ 和 当前 passage 的表示 $H^p_i$ 耦合得到的 $R^{l\times 1}$ 的向量重复Q 次，得到 $R^{l\times Q}$，所以 $\overrightarrow G_i\in R^{l\times Q}$, 在通过一个softmax-affine网络得到 attention weights.</p>
<blockquote>
<p>整个思路下来，就是 attention score 不是通过矩阵相乘，也不是向量 $h^p_i, H^q$ 相减之后通过神经网络得到。但是也相似，就是对当前要匹配的两个向量 $h^p_i, H^q$ 通过两层神经网络得到,其中的对当前向量 $H_i^p$ 和 $\overrightarrow {h_{i-1}^r}$ 要重复 Q 次。。。其实跟 DMN 还是相似的，只不过不是简单的 attention 当前的向量，还用了 LSTM 来耦合之前的信息。</p>
</blockquote>
<p>最终得到想要的结合了 attention 和 LSTM 的输出 $\overrightarrow h^r$.</p>
<p>作者做了一个反向的 LSTM. 方式是一样的：  </p>
<p>$$\overleftarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\overleftarrow {h^r}_{i-1}+b^p)\otimes e_Q)$$</p>
<p>$$\overleftarrow \alpha_i=softmax(w^T\overleftarrow G_i + b\otimes e_Q)$$</p>
<p>同样得到 $\overleftarrow {h_i^r}$.</p>
<ul>
<li><p>$\overrightarrow {H^r}\in R^{l\times P}$ 表示隐藏状态 $[\overrightarrow {h^r_1}, \overrightarrow {h^r_2},…,\overrightarrow {h^r_P}]$.</p>
</li>
<li><p>$\overleftarrow {H^r}\in R^{l\times P}$ 表示隐藏状态 $[\overleftarrow {h^r_1}, \overleftarrow {h^r_2},…,\overleftarrow {h^r_P}]$.</p>
</li>
</ul>
<p>然后把两者堆叠起来得到通过 question 匹配之后的 passage 向量表示： $H^r=\begin{bmatrix} \overrightarrow H^r \ \overleftarrow H^r \end{bmatrix} \in R^{2l\times P}$</p>
<h3 id="Answer-Pointer-Layer"><a href="#Answer-Pointer-Layer" class="headerlink" title="Answer Pointer Layer"></a>Answer Pointer Layer</h3><h4 id="The-Sequence-Model"><a href="#The-Sequence-Model" class="headerlink" title="The Sequence Model"></a>The Sequence Model</h4><p>The answer is represented by a sequence of integers $a=(a_1,a_2,…)$ indicating the positions of the selected tokens in the original passage.  </p>
<p>再一次利用 attention，$\beta_{k,j}$ 表示 answer 中第 k 个token选择 passage 中第 j 个次的概率。所以 $\beta_k\in R^{P+1}$.</p>
<p>$$F_k=tanh(V\tilde {H^r}+(W^ah^a_{k-1}+b^a)\otimes e_{P+1})\in R^{l\times P+1}$$</p>
<p>$$\beta_k=softmax(v^TF_k+c\otimes e_{P+1}) \in R^{1\times (P+1)}$$</p>
<p>其中 $\tilde H\in R^{2l\times (P+1)}$ 表示 $H^r$ 和 zero vector 的叠加, $\tilde H=[H^r, 0], V\in R^{l\times 2l}, W^a\in R^{l\times l}, b^a,v\in R, c\in R$.  </p>
<p>所以还是跟 match-LSTM 一样，先对 $H^r$ 中的每一个词通过全链接表示 $W^ah^a_{k+1}+b^a$, 然后重复 P+1 次，得到 $R^{l\times (P+1)}$. 在通过激活函数 tanh， 再通过一个全连接神经网络，然后使用 softmax 进行多分类。</p>
<p>$$h_k^a=\overrightarrow{LSTM}(\tilde {H^r}\beta_k^T, h^a_{k-1})$$</p>
<p>这里是把 $\tilde H^r$ 与权重 $\beta_k$ 矩阵相乘之后的结果作为 LSTM k 时刻的输入。很玄学， 感觉可以看作是 self-attention 结合了 LSTM.</p>
<p>对生成 answer sequence 的概率进行建模：  </p>
<p>$$p(a|H^r)=\prod_k p(a_k|a_1,a_2,…,a_{k-1}, H^r)$$</p>
<p>其中：</p>
<p>$$p(a_k=j|a_1,a_2,…,a_{k-1})=\beta_{k,j}$$</p>
<p>目标函数 loss function:</p>
<p>$$-\sum_{n=1}^N logp(a_n|P_n,Q_n)$$</p>
<h4 id="The-Boundary-Model"><a href="#The-Boundary-Model" class="headerlink" title="The Boundary Model"></a>The Boundary Model</h4><p>So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to Hr, and the probability of generating an answer is simply modeled as:</p>
<p>$$p(a|H^r)=p(a_s|H^r)p(a_e|a_s, H^r)$$</p>
<p><strong>Search mechanism, and bi-directional Ans-Ptr.</strong></p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>SQuAD: Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics. Each passage is a single paragraph from a Wikipedia article, and each passage has around 5 questions associated with it. In total, there are 23,215 passages and 107,785 questions. The data has been split into a training set (with 87,599 question-answer pairs), a development set (with 10,570 questionanswer pairs) and a hidden test set</p>
<h4 id="configuration"><a href="#configuration" class="headerlink" title="configuration"></a>configuration</h4><ul>
<li><p>dimension l of the hidden layers is set to 150 or 300.  </p>
</li>
<li><p>Adammax: $\beta_1=0.9, \beta_2=0.999$  </p>
</li>
<li><p>minibatch size = 30  </p>
</li>
<li><p>no L2 regularization.</p>
</li>
</ul>
<h4 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h4><p><img src="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/03.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-07T08:41:53.000Z" title="2018/8/7 下午4:41:53">2018-08-07</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.158Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">13 分钟读完 (大约2022个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QA%20BiDAF/">论文笔记-QA BiDAF</a></h1><div class="content"><p>paper:   </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01603">BiDAF:Bidirectional Attention Flow for Machine Comprehension</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.07905">Match-LSTM:Machine Comprehension Using Match-LSTM and Answer Pointer</a></p>
</li>
</ul>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query.  </p>
</blockquote>
<p>机器阅读的定义，query 和 context 之间的交互。</p>
<blockquote>
<p>Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention.  </p>
</blockquote>
<p>传统的使用 attention 机制的方法。</p>
<blockquote>
<p>In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization.  </p>
</blockquote>
<p>本文提出的方法 BiDAF. 使用多阶层次双向 attention flow 机制来表示内容的不同 levels 的粒度，从而获得 query-aware 的 context，而不使用 summarization.</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote>
<p>Attention mechanisms in previous works typically have one or more of the following characteristics. First, the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed-size vector. Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step. Third, they are usually uni-directional, wherein the query attends on the context paragraph or the image.  </p>
</blockquote>
<p>对 atention 在以前的研究中的特性做了一个总结。</p>
<ul>
<li><p>1.attention 的权重用来从 context 中提取最相关的信息，其中 context 压缩到一个固定 size 的向量。</p>
</li>
<li><p>2.在文本领域，context 中的表示在时间上是动态的。所以当前时间步的 attention 权重依赖于之前时间步的向量。  </p>
</li>
<li><p>3.它们通常是单向的，用 query 查询内容段落或图像。</p>
</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>BiDAF 相比传统的将 attention 应用于 MC 任务作出如下改进:  </p>
<ul>
<li><blockquote>
<p>First, our attention layer is not used to summarize the context paragraph into a fixed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer. This reduces the information loss caused by early summarization.  </p>
</blockquote>
</li>
</ul>
<p>1）并没有把 context 编码到固定大小的向量表示中，而是让每个时间步计算得到的 attended vactor 可以流动（在 modeling layer 通过 biLSTM 实现）这样可以减少早期加权和造成的信息丢失。</p>
<ul>
<li><blockquote>
<p>Second, we use a memory-less attention mechanism. That is, while we iteratively compute attention through time as in Bahdanau et al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step.  </p>
</blockquote>
</li>
</ul>
<p>2）memory-less，在每一个时刻，仅仅对 query 和当前时刻的 context paragraph 进行计算，并不直接依赖上一时刻的 attention.  </p>
<p>We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer. It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation (the output of the attention layer). It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps.  </p>
<p>也就是对 attention layer 和 modeling layer 进行分工，前者关注于 context 和 query 之间的交互。而后者则关注于 query-aware context 中词于词之间的交互，也就是加权了 attention weights 之后的 context 表示。这使得 attention 在每个时间步不受之前错误的影响。</p>
<ul>
<li><blockquote>
<p>Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other.  </p>
</blockquote>
</li>
</ul>
<p>计算了 query-to-context（Q2C） 和 context-to-query（C2Q）两个方向的 attention 信息，认为 C2Q 和 Q2C 实际上能够相互补充。实验发现模型在开发集上去掉 C2Q 与 去掉 Q2C 相比，分别下降了 12 和 10 个百分点，显然 C2Q 这个方向上的 attention 更为重要</p>
<p><img src="/2018/08/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QA%20BiDAF/01.png"></p>
<p>论文提出6层结构：</p>
<p>Character Embedding Layer and Word Embedding Layer -&gt; Contextual Embedding Layer -&gt; Attention Flow Layer -&gt; Modeling Layer -&gt; Output Layer</p>
<h4 id="Character-Embedding-Layer-and-word-embedding-alyer"><a href="#Character-Embedding-Layer-and-word-embedding-alyer" class="headerlink" title="Character Embedding Layer and word embedding alyer"></a>Character Embedding Layer and word embedding alyer</h4><ul>
<li><p>charatter embedding of each word using CNN.The outputs of the CNN are max-pooled over the entire width to obtain a fixed-size vector for each word.</p>
</li>
<li><p>pre-trained word vectors, GloVe  </p>
</li>
<li><p>concatenation of them above and is passed to a two-layer highway networks.</p>
</li>
</ul>
<p>context -&gt; $X\in R^{d\times T}$  </p>
<p>query -&gt; $Q\in R^{d\times J}$</p>
<h4 id="contextual-embedding-layer"><a href="#contextual-embedding-layer" class="headerlink" title="contextual embedding layer"></a>contextual embedding layer</h4><p>model the temporal interactions between words using biLSTM.  </p>
<p>context -&gt; $H\in R^{2d\times T}$  </p>
<p>query -&gt; $U\in R^{2d\times J}$</p>
<p>前三层网络是在不同的粒度层面来提取 context 和 query 的特征。</p>
<h4 id="attention-flow-layer"><a href="#attention-flow-layer" class="headerlink" title="attention flow layer"></a>attention flow layer</h4><blockquote>
<p>the attention flow layer is not used to summarize the query and context into single feature vectors. Instead, the attention vector at each time step, along with the embeddings from previous layers, are allowed to flow through to the subsequent modeling layer.  </p>
</blockquote>
<p>输入是 H 和 G，输出是 query-aware vector G, 以及上一层的 contextual layer.</p>
<p>这一层包含两个 attention，Context-to-query Attention 和 Query-to-context Attention. 它们共享相似矩阵 $S\in R^{T\times J}$(不是简单的矩阵相乘，而是类似于 <a target="_blank" rel="noopener" href="https://panxiebit.github.io/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/#more">Dynamic Memory Networks</a> 中的计算方式).  </p>
<p>$$S_{tj}=\alpha(H_{:t},U_{:j})\in R$$</p>
<p>其中 $\alpha(h,u)=w_{(S)}^T[h,u,h\circ u]$, $w_{(S)}\in R^{6d}$</p>
<p><strong>Context-to-query Attention:</strong>  </p>
<p>计算对每一个 context word 而言哪些 query words 和它最相关。所以 计算 t-th context word 对应的 query 每个词的权重:</p>
<p>$$a_t=softmax(S_{t:})\in R^J$$</p>
<p>然后将权重赋予到 query 上然后再加权求和(叠加赋予了权重的 query 中的每一个词)，得到 t-th 对应的 query-aware query:  </p>
<p>$$\tilde U_{:t}=\sum_j a_{tj}U_{:j}\in R^{2d}$$</p>
<p>然后 context 中的每一个词都这样计算，$\tilde U\in R^{2d\times T}$</p>
<p>就是通过 context 和 query 计算相似性后，通过 sortmax 转化为概率，然后作为权重赋予到 query 上，得到 context 每一个词对应的 attended-query.</p>
<p><strong>Query-to-context Attention:</strong>  </p>
<p>跟 C2Q 一样计算相似矩阵 S 后，计算对每一个 query word 而言哪些 context words 和它最相关，这些 context words 对回答问题很重要。  </p>
<p>先计算相关性矩阵每一列中的最大值，max function $max_{col}(S)\in R^T$, 然后softmax计算概率:  </p>
<p>$$b=softmax(max_{col}(S))\in R^T$$  </p>
<p>权重 b 表示与整个 query 比较之后，context 中每一个词的重要程度，然后与 context 加权和：</p>
<p>$$\tilde h = \sum_tb_tH_{:t}\in R^{2d}$$</p>
<p>在 tile T 次后得到 $\tilde H\in R^{2d\times T}$.</p>
<p>比较 C2Q 和 Q2C，显然 Q2C 更重要，因为最终我们要找的答案是 context 中的内容。而且两者的 attention 计算方式有区别是：对 query 进行加权和时，我们考虑的是 context 中的每一个词，而在对 context 进行加权和时，我们要考虑所有的 query 中相关性最大的词，是因为 context 中某个词只要与 query 中任何一个词有关，都需要被 attend.</p>
<p>将三个矩阵拼接起来，得到 G:</p>
<p>$$G_{:t}=\beta (H_{:t},\tilde U_{:t}, \tilde H_{:t})\in R^{d_G}$$</p>
<p>function $\beta$ 可以是 multi-layers perceptron. 在作者的实验中：</p>
<p>$$\beta(h,\tilde u,\tilde h)=[h;\tilde u;h\circ \tilde u;h\circ \tilde h]\in R^{8d\times T}$$</p>
<h4 id="Modeling-Layer"><a href="#Modeling-Layer" class="headerlink" title="Modeling Layer"></a>Modeling Layer</h4><p>captures the interaction among the context words conditioned on the query.</p>
<p>使用 biLSTM, 单向 LSTM 的输出维度是d，所以最终输出： $M\in R^{2d\times T}$.</p>
<h4 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h4><p>输出 layer 是基于应用确定的。如果是 QA，就从段落中找出 start p1 和 end p2.  </p>
<p>计算 start index:</p>
<p>$$p^1=softmax(W^T(p^1)[G;M])$$</p>
<p>其中 $w_{(p^1)}\in R^{10d}$</p>
<p>计算 end index，将 M 通过另一个 biLSTM 处理，得到 $M^2\in R^{2d\times T}$</p>
<p>$$p^2=softmax(W^T(p^2)[G;M^2])$$</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>目标损失函数：  </p>
<p>$$L(\theta)=-{1 \over N} \sum^N_i[log(p^1_{y_i^1})+log(p^2_{y_i^2})]$$</p>
<p>$\theta$ 包括参数：  </p>
<ul>
<li><p>the weights of CNN filters and LSTM cells  </p>
</li>
<li><p>$w_{S}$,$w_{p^1},w_{p^2}$  </p>
</li>
</ul>
<p>$y_i^1,y_i^2$ 表示i样本中开始可结束位置在 context 中的 index.</p>
<p>$p^1,p^2\in R^T$ 是经过 softmax 得到的概率，可以将 gold truth 看作是 one-hot 向量 [0,0,…,1,0,0,0]，所以对单个样本交叉熵是:</p>
<p>$$- log(p^1_{y_i^1})-log(p^2_{y_i^2})$$</p>
<h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>The answer span $(k; l)$ where $k \le l$ with the maximum value of $p^1_kp^2_l$ is chosen, which can be computed in linear time with dynamic programming.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-07T01:28:38.000Z" title="2018/8/7 上午9:28:38">2018-08-07</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.537Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">43 分钟读完 (大约6494个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/">机器学习-中文文本预处理</a></h1><div class="content"><h3 id="中文文本挖掘预处理特点"><a href="#中文文本挖掘预处理特点" class="headerlink" title="中文文本挖掘预处理特点"></a>中文文本挖掘预处理特点</h3><p>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6744056.html">https://www.cnblogs.com/pinard/p/6744056.html</a></p>
<p>首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。  </p>
<p>首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6677078.html">文本挖掘的分词原理</a>中，我们已经讲到了中文的分词原理，这里就不多说。</p>
<p>第二，中文的编码不是utf8，而是unicode。这样会导致在分词的时候，和英文相比，我们要处理编码的问题。</p>
<p>这两点构成了中文分词相比英文分词的一些不同点，后面我们也会重点讲述这部分的处理。当然，英文分词也有自己的烦恼，这个我们在以后再讲。了解了中文预处理的一些特点后，我们就言归正传，通过实践总结下中文文本挖掘预处理流程。</p>
<h3 id="数据集收集"><a href="#数据集收集" class="headerlink" title="数据集收集"></a>数据集收集</h3><p>在文本挖掘之前，我们需要得到文本数据，文本数据的获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。</p>
<p>对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“机器学习”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。</p>
<p>对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用<a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/">beautifulsoup</a>。但是我们我们需要某些特殊的语料数据，比如上面提到的“机器学习”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成。这个我一般使用<a target="_blank" rel="noopener" href="https://github.com/ViDA-NYU/ache">ache</a>。 ache允许我们用关键字或者一个分类算法来过滤出我们需要的主题语料，比较强大。</p>
<h3 id="除去数据中非文本部分"><a href="#除去数据中非文本部分" class="headerlink" title="除去数据中非文本部分"></a>除去数据中非文本部分</h3><p>这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用beautifulsoup来去除。去除掉这些非文本的内容后，我们就可以进行真正的文本预处理了。</p>
<h3 id="处理中文编码问题"><a href="#处理中文编码问题" class="headerlink" title="处理中文编码问题"></a>处理中文编码问题</h3><p>由于Python2不支持unicode的处理，因此我们使用Python2做中文文本预处理时需要遵循的原则是，存储数据都用utf8，读出来进行中文相关处理时，使用GBK之类的中文编码，在下面一节的分词时，我们再用例子说明这个问题。</p>
<h3 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h3><p>常用的中文分词软件有很多，个人比较推荐结巴分词。安装也很简单，比如基于Python的，用”pip install jieba”就可以完成。下面我们就用例子来看看如何中文分词。</p>
<p>首先我们准备了两段文本，这两段文本在两个文件中。两段文本的内容分别是nlp_test0.txt和nlp_test2.txt：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test1.txt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line"></span><br><span class="line">    document = f.read() <span class="comment"># 如果是python2，则需要用 decode(&quot;GBK&quot;)</span></span><br><span class="line"></span><br><span class="line">    document_cut = jieba.cut(document)</span><br><span class="line"></span><br><span class="line">document_cut</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>&lt;generator object Tokenizer.cut at 0x7f6a84cf09e8&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">result = <span class="string">&quot; &quot;</span>.join(document_cut)</span><br><span class="line"></span><br><span class="line">result</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>Building prefix dict from the default dictionary ...

Loading model from cache /tmp/jieba.cache

Loading model cost 0.438 seconds.

Prefix dict has been built succesfully.











&#39;        沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \n \n         沙 瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王 大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王 大路 就 在 京州帝 豪园 买 了 三套 别墅 ， 可是 李达 康和易 学习 都 不要 ， 这些 房子 都 在 王 大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f2:</span><br><span class="line"></span><br><span class="line">    f2.write(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以发现对于一些人名和地名，jieba处理的不好，不过我们可以帮jieba加入词汇如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;沙瑞金&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;易学习&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;王大路&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;京州&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>3
</code></pre>
<p>所以在很多 NLP 任务中先做命令实体识别的意义就在这里对吧?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test1.txt&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f1:</span><br><span class="line"></span><br><span class="line">    text = f1.read()</span><br><span class="line"></span><br><span class="line">    text_cut = jieba.cut(text)  <span class="comment"># list</span></span><br><span class="line"></span><br><span class="line">    result = <span class="string">&quot; &quot;</span>.join(text_cut)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f2:</span><br><span class="line"></span><br><span class="line">        f2.write(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>        沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。



         沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。
</code></pre>
<h3 id="引入停用词"><a href="#引入停用词" class="headerlink" title="引入停用词"></a>引入停用词</h3><p>在上面我们解析的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个，<a target="_blank" rel="noopener" href="http://files.cnblogs.com/files/pinard/stop_words.zip">下载地址在这</a>。当然也有其他版本的停用词表，不过这个1208词版是我常用的。</p>
<p>在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">stpword_path = <span class="string">&quot;stop_words.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(stpword_path, encoding=<span class="string">&quot;gbk&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line"></span><br><span class="line">    stpword_content = f.read()</span><br><span class="line"></span><br><span class="line">    stpword_list = stpword_content.splitlines()</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(stpword_list[:<span class="number">100</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;,&#39;, &#39;?&#39;, &#39;、&#39;, &#39;。&#39;, &#39;“&#39;, &#39;”&#39;, &#39;《&#39;, &#39;》&#39;, &#39;！&#39;, &#39;，&#39;, &#39;：&#39;, &#39;；&#39;, &#39;？&#39;, &#39;人民&#39;, &#39;末##末&#39;, &#39;啊&#39;, &#39;阿&#39;, &#39;哎&#39;, &#39;哎呀&#39;, &#39;哎哟&#39;, &#39;唉&#39;, &#39;俺&#39;, &#39;俺们&#39;, &#39;按&#39;, &#39;按照&#39;, &#39;吧&#39;, &#39;吧哒&#39;, &#39;把&#39;, &#39;罢了&#39;, &#39;被&#39;, &#39;本&#39;, &#39;本着&#39;, &#39;比&#39;, &#39;比方&#39;, &#39;比如&#39;, &#39;鄙人&#39;, &#39;彼&#39;, &#39;彼此&#39;, &#39;边&#39;, &#39;别&#39;, &#39;别的&#39;, &#39;别说&#39;, &#39;并&#39;, &#39;并且&#39;, &#39;不比&#39;, &#39;不成&#39;, &#39;不单&#39;, &#39;不但&#39;, &#39;不独&#39;, &#39;不管&#39;, &#39;不光&#39;, &#39;不过&#39;, &#39;不仅&#39;, &#39;不拘&#39;, &#39;不论&#39;, &#39;不怕&#39;, &#39;不然&#39;, &#39;不如&#39;, &#39;不特&#39;, &#39;不惟&#39;, &#39;不问&#39;, &#39;不只&#39;, &#39;朝&#39;, &#39;朝着&#39;, &#39;趁&#39;, &#39;趁着&#39;, &#39;乘&#39;, &#39;冲&#39;, &#39;除&#39;, &#39;除此之外&#39;, &#39;除非&#39;, &#39;除了&#39;, &#39;此&#39;, &#39;此间&#39;, &#39;此外&#39;, &#39;从&#39;, &#39;从而&#39;, &#39;打&#39;, &#39;待&#39;, &#39;但&#39;, &#39;但是&#39;, &#39;当&#39;, &#39;当着&#39;, &#39;到&#39;, &#39;得&#39;, &#39;的&#39;, &#39;的话&#39;, &#39;等&#39;, &#39;等等&#39;, &#39;地&#39;, &#39;第&#39;, &#39;叮咚&#39;, &#39;对&#39;, &#39;对于&#39;, &#39;多&#39;, &#39;多少&#39;, &#39;而&#39;, &#39;而况&#39;, &#39;而且&#39;, &#39;而是&#39;]
</code></pre>
<h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><p>现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick中</a>，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6693230.html">文本挖掘预处理之TF-IDF</a>中，我们也讲到了TF-IDF特征处理的方法。这里我们就用scikit-learn的TfidfVectorizer类来进行TF-IDF特征处理。</p>
<h4 id="向量化与-Hash-Trick"><a href="#向量化与-Hash-Trick" class="headerlink" title="向量化与 Hash Trick"></a>向量化与 Hash Trick</h4><h5 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h5><p>在讲向量化与Hash Trick之前，我们先说说词袋模型(Bag of Words,简称BoW)。词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。</p>
<p>词袋模型首先会进行分词，在分词之后，通过统计每个词在文本中出现的次数，我们就可以得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就是我们常说的向量化。向量化完毕后一般也会使用TF-IDF进行特征的权重修正，再将特征进行标准化。 再进行一些其他的特征工程后，就可以将数据带入机器学习算法进行分类聚类了。</p>
<p>总结下词袋模型的三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。</p>
<p>词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。</p>
<h5 id="词袋模型之向量化"><a href="#词袋模型之向量化" class="headerlink" title="词袋模型之向量化"></a>词袋模型之向量化</h5><p>在词袋模型的统计词频这一步，我们会得到该文本中所有词的词频，有了词频，我们就可以用词向量表示这个文本。这里我们举一个例子，例子直接用scikit-learn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">corpus=[<span class="string">&quot;I come to China to travel&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;This is a car polupar in China&quot;</span>,          </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;I love tea and Apple &quot;</span>,   </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;The work is to write some papers in science&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer.fit_transform(corpus))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 16)    1

  (0, 3)    1

  (0, 15)    2

  (0, 4)    1

  (1, 5)    1

  (1, 9)    1

  (1, 2)    1

  (1, 6)    1

  (1, 14)    1

  (1, 3)    1

  (2, 1)    1

  (2, 0)    1

  (2, 12)    1

  (2, 7)    1

  (3, 10)    1

  (3, 8)    1

  (3, 11)    1

  (3, 18)    1

  (3, 17)    1

  (3, 13)    1

  (3, 5)    1

  (3, 6)    1

  (3, 15)    1
</code></pre>
<p>可以看出4个文本的词频已经统计出，在输出中，左边的括号中的第一个数字是文本的序号，第2个数字是词的序号，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。</p>
<p>我们可以进一步看看每个文本的词向量特征和各个特征代表的词，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer.fit_transform(corpus).toarray())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]

 [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]

 [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]

 [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer.get_feature_names())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;and&#39;, &#39;apple&#39;, &#39;car&#39;, &#39;china&#39;, &#39;come&#39;, &#39;in&#39;, &#39;is&#39;, &#39;love&#39;, &#39;papers&#39;, &#39;polupar&#39;, &#39;science&#39;, &#39;some&#39;, &#39;tea&#39;, &#39;the&#39;, &#39;this&#39;, &#39;to&#39;, &#39;travel&#39;, &#39;work&#39;, &#39;write&#39;]
</code></pre>
<p>也就是先统计整个文本corpus, 去掉停用词，剩下的词就是向量的维度。然后统计每一行文字出现的词频，得到相应的向量。显然词表是按照字母顺序排序的。</p>
<p>可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词”I”在英文中是停用词，不参加词频的统计。</p>
<p>由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。</p>
<blockquote>
<p><strong>这里有个疑问？</strong> 向量化之后的维度是根据自己的数据集来定，为什么不就是词表大小呢。这里是根据自己的数据集来的，但我们对测试集分类时，会出现 UNK 词吧，但是这个词其实在词表中是有的。那么在训练集中如果加上这个维度，其实也没有太大意义，因为在训练集中这个维度上所有的值都为0.</p>
</blockquote>
<p>将文本做了词频统计后，我们一般会通过TF-IDF进行词特征值修订，这部分我们后面再讲。</p>
<p>向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，达到100万+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们怎么办呢？第一反应是我们要进行特征的降维，说的没错！而Hash Trick就是非常常用的文本特征降维方法。</p>
<h5 id="Hash-Trick"><a href="#Hash-Trick" class="headerlink" title="Hash Trick"></a>Hash Trick</h5><p>在大规模的文本处理中，由于特征的维度对应分词词汇表的大小，所以维度可能非常恐怖，此时需要进行降维，不能直接用我们上一节的向量化方法。而最常用的文本降维方法是Hash Trick。说到Hash，一点也不神秘，学过数据结构的同学都知道。这里的Hash意义也类似。</p>
<p>在Hash Trick里，我们会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于我们的词汇表的特征维度，因此可以看成是降维。具体的方法是，对应任意一个特征名，我们会用Hash函数找到对应哈希表的位置，然后将该特征名对应的词频统计值累加到该哈希表位置。如果用数学语言表示,假如哈希函数h使第i个特征哈希到位置j,即 $h(i)=j$,则第i个原始特征的词频数值 $\phi(i)$ 将累加到哈希后的第j个特征的词频数值 $\hat \phi(i)$上，即：</p>
<p>$$\hat \phi(i)=\sum_{i\in J;h(i)=j}\phi(i)$$</p>
<p>其中 J 是原始特征的维度。</p>
<p>但是上面的方法有一个问题，有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这个问题，出现了hash Trick的变种signed hash trick,此时除了哈希函数h,我们多了一个一个哈希函数：</p>
<p>$$\xi:N\rightarrow \pm1$$</p>
<p>此时我们有</p>
<p>$$\hat \phi(j)=\sum_{i\in J;h(i)=j}\phi(i)\xi(i)$$</p>
<p>这样做的好处是，哈希后的特征仍然是一个无偏的估计，不会导致某些哈希位置的值过大。</p>
<p>当然，大家会有疑惑，这种方法来处理特征，哈希后的特征是否能够很好的代表哈希前的特征呢？从实际应用中说，由于文本特征的高稀疏性，这么做是可行的。如果大家对理论上为何这种方法有效，建议参考论文：<a target="_blank" rel="noopener" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Feature hashing for large scale multitask learning</a>.这里就不多说了。</p>
<p>在scikit-learn的HashingVectorizer类中，实现了基于signed hash trick的算法，这里我们就用HashingVectorizer来实践一下Hash Trick，为了简单，我们使用上面的19维词汇表，并哈希降维到6维。当然在实际应用中，19维的数据根本不需要Hash Trick，这里只是做一个演示，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> HashingVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer2 = HashingVectorizer(n_features=<span class="number">6</span>, norm=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer2.fit_transform(corpus))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 1)    2.0

  (0, 2)    -1.0

  (0, 4)    1.0

  (0, 5)    -1.0

  (1, 0)    1.0

  (1, 1)    1.0

  (1, 2)    -1.0

  (1, 5)    -1.0

  (2, 0)    2.0

  (2, 5)    -2.0

  (3, 0)    0.0

  (3, 1)    4.0

  (3, 2)    -1.0

  (3, 3)    1.0

  (3, 5)    -1.0
</code></pre>
<p>大家可以看到结果里面有负数，这是因为我们的哈希函数ξ可以哈希到1或者-1导致的。</p>
<p>和PCA类似，Hash Trick降维后的特征我们已经不知道它代表的特征名字和意义。此时我们不能像上一节向量化时候可以知道每一列的意义，所以Hash Trick的解释性不强。</p>
<h5 id="向量化与-Hash-Track-小结"><a href="#向量化与-Hash-Track-小结" class="headerlink" title="向量化与 Hash Track 小结"></a>向量化与 Hash Track 小结</h5><p>这里我们对向量化与它的特例Hash Trick做一个总结。在特征预处理的时候，我们什么时候用一般意义的向量化，什么时候用Hash Trick呢？标准也很简单。</p>
<p>一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。</p>
<p>而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。因此，实际工作中我使用的都是特征向量化。</p>
<p>向量化与Hash Trick就介绍到这里，下一篇我们讨论TF-IDF。</p>
<h4 id="文本向量化特征的不足"><a href="#文本向量化特征的不足" class="headerlink" title="文本向量化特征的不足"></a>文本向量化特征的不足</h4><p>在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，比如在文本挖掘预处理之向量化与Hash Trick这篇文章中，我们将下面4个短文本做了词频统计：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">corpus=[&quot;I come to China to travel&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;This is a car polupar in China&quot;,          </span><br><span class="line"></span><br><span class="line">    &quot;I love tea and Apple &quot;,   </span><br><span class="line"></span><br><span class="line">    &quot;The work is to write some papers in science&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>不考虑停用词，处理后得到的词向量如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]</span><br><span class="line"></span><br><span class="line"> [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]</span><br><span class="line"></span><br><span class="line"> [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]</span><br><span class="line"></span><br><span class="line"> [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现”come”,”China”和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的”China”和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。</p>
<h4 id="TF-IDF概述"><a href="#TF-IDF概述" class="headerlink" title="TF-IDF概述"></a>TF-IDF概述</h4><p>TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。</p>
<p>前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们讲到几乎所有文本都会出现的”to”其词频虽然高，但是重要性却应该比词频低的”China”和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。</p>
<p>概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。</p>
<p>上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下：</p>
<p>$$IDF(x)=\dfrac{N}{N(x)}$$</p>
<p>其中，N代表语料库中文本的总数，而 $N(x)$ 代表语料库中包含词x的文本总数。为什么IDF的基本公式应该是是上面这样的而不是像 $N/N(x)$ 这样的形式呢？这就涉及到信息论相关的一些知识了。感兴趣的朋友建议阅读吴军博士的《数学之美》第11章。</p>
<p>上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为：</p>
<p>$$IDF(x)=log\dfrac{N+1}{N(x)+1}+1$$</p>
<p>有了IDF的定义，我们就可以计算某一个词的TF-IDF值了：</p>
<p>$$\text{TF-IDF(x)}=TF(x)*IDF(x)$$</p>
<p>其中TF(x)指词x在当前文本中的词频。</p>
<h4 id="用scikit-learn进行TF-IDF预处理"><a href="#用scikit-learn进行TF-IDF预处理" class="headerlink" title="用scikit-learn进行TF-IDF预处理"></a>用scikit-learn进行TF-IDF预处理</h4><p>在scikit-learn中，有两种方法进行TF-IDF的预处理。</p>
<p>第一种方法是在用CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理。第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理。</p>
<p>首先我们来看第一种方法，CountVectorizer+TfidfTransformer的组合，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">corpus = [<span class="string">&quot;I come to China to travel&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;This is a car polupar in China&quot;</span>,          </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;I love tea and Apple &quot;</span>,   </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;The work is to write some papers in science&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line"></span><br><span class="line">tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tfidf)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 4)    0.4424621378947393

  (0, 15)    0.697684463383976

  (0, 3)    0.348842231691988

  (0, 16)    0.4424621378947393

  (1, 3)    0.3574550433419527

  (1, 14)    0.45338639737285463

  (1, 6)    0.3574550433419527

  (1, 2)    0.45338639737285463

  (1, 9)    0.45338639737285463

  (1, 5)    0.3574550433419527

  (2, 7)    0.5

  (2, 12)    0.5

  (2, 0)    0.5

  (2, 1)    0.5

  (3, 15)    0.2811316284405006

  (3, 6)    0.2811316284405006

  (3, 5)    0.2811316284405006

  (3, 13)    0.3565798233381452

  (3, 17)    0.3565798233381452

  (3, 18)    0.3565798233381452

  (3, 11)    0.3565798233381452

  (3, 8)    0.3565798233381452

  (3, 10)    0.3565798233381452
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">tfidf2 = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line">re = tfidf2.fit_transform(corpus)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(re)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 4)    0.4424621378947393

  (0, 15)    0.697684463383976

  (0, 3)    0.348842231691988

  (0, 16)    0.4424621378947393

  (1, 3)    0.3574550433419527

  (1, 14)    0.45338639737285463

  (1, 6)    0.3574550433419527

  (1, 2)    0.45338639737285463

  (1, 9)    0.45338639737285463

  (1, 5)    0.3574550433419527

  (2, 7)    0.5

  (2, 12)    0.5

  (2, 0)    0.5

  (2, 1)    0.5

  (3, 15)    0.2811316284405006

  (3, 6)    0.2811316284405006

  (3, 5)    0.2811316284405006

  (3, 13)    0.3565798233381452

  (3, 17)    0.3565798233381452

  (3, 18)    0.3565798233381452

  (3, 11)    0.3565798233381452

  (3, 8)    0.3565798233381452

  (3, 10)    0.3565798233381452
</code></pre>
<p>输出的各个文本各个词的TF-IDF值和第一种的输出完全相同。大家可以自己去验证一下。</p>
<p>由于第二种方法比较的简洁，因此在实际应用中推荐使用，一步到位完成向量化，TF-IDF与标准化。</p>
<p>TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。</p>
<p>当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想</p>
<p><strong>还的好好理解下 TF-IDF 是怎么实现的！</strong></p>
<h3 id="建立分析模型"><a href="#建立分析模型" class="headerlink" title="建立分析模型"></a>建立分析模型</h3><p>有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。比如我们上面的两段文本，就可以是两个训练样本了。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而 <strong>主题模型</strong> 是自然语言处理比较特殊的一块，这个我们后面再单独讲。</p>
<h3 id="中文文本挖掘预处理总结"><a href="#中文文本挖掘预处理总结" class="headerlink" title="中文文本挖掘预处理总结"></a>中文文本挖掘预处理总结</h3><p>上面我们对中文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如我们涉及到词上下文关系的一些需求，此时不能使用词袋模型。而有时候我们对于特征的处理有自己的特殊需求，因此这个流程仅供自然语言处理入门者参考。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/14/">上一页</a></div><div class="pagination-next"><a href="/page/16/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/14/">14</a></li><li><a class="pagination-link is-current" href="/page/15/">15</a></li><li><a class="pagination-link" href="/page/16/">16</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>