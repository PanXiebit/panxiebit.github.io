<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="潘晓榭"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘晓榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="潘晓榭"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="潘晓榭"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="潘晓榭"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-07-06T02:32:32.000Z" title="2018/7/6 上午10:32:32">2018-07-06</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a></span><span class="level-item">16 分钟读完 (大约2440个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/07/06/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/">算法-排序算法</a></h1><div class="content"><p>剑指offer 第二章-面试需要的基础知识</p></div><a class="article-more button is-small is-size-7" href="/2018/07/06/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-27T00:55:48.000Z" title="2018/6/27 上午8:55:48">2018-06-27</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/NLP/">NLP</a></span><span class="level-item">13 分钟读完 (大约1878个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/27/chapter27-Question-Answering/">chapter27-Question Answering</a></h1><div class="content"><p>Speech and language Processing, chapter27:Question Answering</p></div><a class="article-more button is-small is-size-7" href="/2018/06/27/chapter27-Question-Answering/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-25T11:44:23.000Z" title="2018/6/25 下午7:44:23">2018-06-25</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:17.025Z" title="2021/6/29 下午4:12:17">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/">NLP</a></span><span class="level-item">20 分钟读完 (大约3021个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/">论文笔记 - Attention 综述</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces">Attention and Augmented Recurrent Neural Networks</a></p>
<p>循环神经网络是深度学习的根基之一，她允许神经网络对序列数据进行处理，比如文本，语音和视频。</p>
<p>这句话形容特别贴切，意思就是rnn能将序列转换成包含了理解，语义的表示。</p>
<blockquote>
<p>They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch!  </p>
</blockquote>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/01.png">  </p>
<p>基本的 rnn 在解决 long sequence 时非常挣扎，其变体 LSTM 有效的解决这一问题（但问题依然存在）。  </p>
<p>随着时间的发展，出现了各种 augument RNNs. 其中以下4种 stand out exciting.  </p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/02.png">  </p>
<p>这些变体都是 RNN 有力的拓展，更令人惊奇的是，他们还能有效的结合在一起，似乎只是广阔空间中的一点。除此之外，他们都依赖于同样的 underlying trick — attention.  </p>
<h3 id="Neural-Turing-Machines"><a href="#Neural-Turing-Machines" class="headerlink" title="Neural Turing Machines"></a>Neural Turing Machines</h3><p><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1410.5401.pdf">Neural Turing Machines</a> 神经图灵机将 RNNs 和外部 memory bank 结合在一起。向量用来表示神经网络中的自然语言，memory 是向量的数组。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/03.png"></p>
<p>如上图所示，能够看出其原理是将每一个词的向量表示存储在 memory 中。那么从 memory 中读、以及写入 memory 是怎么操作的呢？  </p>
<p>最大的难点在于让整个过程可微(differentiable).特别的是，我们希望在读出和写入的位置具有差异性，以便我们可以知道从哪儿读和写。这很棘手(tricky)，因为 memory 地址基本上是离散的。  </p>
<p>NMTs 采取了一个非常聪明的方法，每一步读和写都包括 memory 中的所有位置，只是对于不同的位置，读和写的程度不同。</p>
<p>举个例子，这里我们只关注 reading. RNN 输出一个 “attention distribution”，表示对于 memory 不同位置的读取程度。这样，读入的操作可以看作加权求和。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/04.png"></p>
<p>$$r \leftarrow \sum_ia_iM_i$$</p>
<p>类似的，在读入的过程中也是对 memory 中所有位置。再一次，输出一个 “attention distribution” 用来表示对每个位置的写入程度。我们在一个 memory 位置获得新的 value 是旧的 memory 和写入值的 凸结合(<strong>convex combination</strong>).</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/05.png"></p>
<p>$$M_i \leftarrow a_iw+(1-a_i)M_i$$</p>
<p>其中 $w$ 是写入值(write value)。</p>
<p>但是 NTMs 如何确定去注意 memory 中的那一个位置呢？ 他们使用了两种不同的方法的结合： content-based attention 和 location-based attention. 前者允许 NMTs 通过匹配 memory 中的每一个位置，并 focus on 最 match 的位置。后者允许 memory 中的相对移动，保证 NMT 能循环。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/06.png"></p>
<p>这种读和写的能力允许 NTM 执行许多简单的算法，而这些算法以前超越了神经网络。 例如，他们可以在 memory中存储一个长序列，然后遍历它，反向重复它。 当他们这样做时，我们可以看他们读和写的地方，以更好地理解他们在做什么：</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/07.png"></p>
<p>关于 repeat copy 的实验，可以参考这篇论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03044">Show, attend and tell: Neural image caption generation with visual attention</a></p>
<p>他们能够模仿一个 lookup table,甚至可以 sort numbers(althought they kind of cheat). 但另一方面，他们仍然不能做很多基本的事儿，比如 add or multiply numbers.</p>
<p>自从 NTM 这篇论文之后，又出现了很多相同方向的令人 exciting 的文章.  </p>
<blockquote>
<p>The Neural GPU <a href="(https://arxiv.org/pdf/1511.08228.pdf)">4</a> overcomes the NTM’s inability to add and multiply numbers.  </p>
</blockquote>
<p>Zaremba &amp; Sutskever <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.00521.pdf">5</a> train NTMs using reinforcement learning instead of the differentiable read/writes used by the original.  </p>
<p>Neural Random Access Machines <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.06392.pdf">6</a> work based on pointers.  </p>
<p>Some papers have explored differentiable data structures, like stacks and queues [7, 8].  </p>
<p>memory networks [9, 10] are another approach to attacking similar problems.</p>
<p><strong>Code</strong>  </p>
<ul>
<li><p>Neural Turing Machine<a target="_blank" rel="noopener" href="https://github.com/carpedm20/NTM-tensorflow"> Taehoon Kim’s</a></p>
</li>
<li><p>Neural GPU publication <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/neural_gpu"> TensorFlow Models repository</a></p>
</li>
<li><p>Memory Networks, <a target="_blank" rel="noopener" href="https://github.com/carpedm20/MemN2N-tensorflow">Taehoon Kim’s</a></p>
</li>
</ul>
<p>关于这篇 paper 真的很难看懂，</p>
<h3 id="Attention-Interfaces"><a href="#Attention-Interfaces" class="headerlink" title="Attention Interfaces"></a>Attention Interfaces</h3><p>当我翻译一个句子时，我特别注意我正在翻译的单词。 当我录制录音时，我会仔细聆听我正在积极写下的片段。 如果你要求我描述我正在坐的房间，那么我会浏览我正在描述的物体。</p>
<p>神经网络能够通过 attention 机制完成上述行为，也就是 focusing on 给出信息的部分内容。举个例子，一个 RNN 能够 attend over 另一个 RNN 的输出，并在每一个时间步， focus on 另一个 RNN 的不同的位置。</p>
<p>为了让 attention 可微，我们采用跟 NTM 同样的方法，focus 所有的位置，每个位置的程度不同。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/08.png"></p>
<p>上图中颜色的深浅表示注意的程度。</p>
<p>其中 attention distribution 是由 content-based attention 生成的。具体过程还是看英文描述吧：</p>
<blockquote>
<p>The attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution.</p>
</blockquote>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/09.png"></p>
<p>最经典的使用 RNNs 之间的 attention 就是机器翻译了，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a>. 传统的 seq2seq 模型通过 RNN 将整个输入序列转化为单个向量(也就是最后一层的隐藏状态)，然后将其展开得到输出(word by word). 注意力机制避免了这一点，它允许 RNN 在生成输出时，能看到所有输入中的每一个词，并且根据他们之间的相关性，来选择性注意部分词。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/10.png"></p>
<p>原图是可以操作的，大牛们的技术真是厉害。。。可视化也很6。。</p>
<p>这种类型的 RNN 还有很多其他的应用，比如在 语音识别上的使用<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1508.01211.pdf">Listen, Attend and Spell</a>, 使用一个 RNN 来处理音频，然后用另一个 RNN 来遍历它，并在生成一个抄本(transcript)时重点关注相关的部分。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/12.png"></p>
<p> 这个图看不出来，不能截出那种效果。 其实可以看出对语音的识别，在生成对应的词时更关注的是对应的音频，并不像文本那种需要长时间依赖。</p>
<p>还有很多其他的应用：  </p>
<ul>
<li><p>parsing tree:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.7449">Grammar as a foreign language</a></p>
</li>
<li><p>conversational modeling:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.05869.pdf">A Neural Conversational Model</a></p>
</li>
<li><p>image captioning: attention interface 也可以是 CNN 和 RNN 之间的，它允许 RNN 在每一个时间步生成文本时能关注图像中的不同的位置。</p>
</li>
</ul>
<blockquote>
<p>Then an RNN runs, generating a description of the image. As it generates each word in the description, the RNN focuses on the conv net’s interpretation of the relevant parts of the image. We can explicitly visualize this:  </p>
</blockquote>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/13.png"></p>
<p>图片来自于<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03044">Show, attend and tell: Neural image caption generation with visual attention</a></p>
<blockquote>
<p>More broadly, attentional interfaces can be used whenever one wants to interface with a neural network that has a repeating structure in its output.</p>
</blockquote>
<h3 id="Adaptive-Computation-time"><a href="#Adaptive-Computation-time" class="headerlink" title="Adaptive Computation time"></a>Adaptive Computation time</h3><blockquote>
<p>Standard RNNs do the same amount of computation for each time step. This seems unintuitive. Surely, one should think more when things are hard? It also limits RNNs to doing O(n) operations for a list of length n.</p>
</blockquote>
<p>标准的 RNN 在每一个时间步都做着相同的大量的计算。它貌似是很合理的，但当处理的信息很复杂时，是否可以在一个时间步考虑更多？这样同样的计算量的方式限制了 RNN 在处理长度为 n 的序列时，其复杂度也为 O(n).</p>
<blockquote>
<p>Adaptive Computation Time [15] is a way for RNNs to do different amounts of computation each step. The big picture idea is simple: allow the RNN to do multiple steps of computation for each time step.</p>
</blockquote>
<p>自适应计算时间步(ACT) 能让 RNN 在每一个时间步做不同的计算量. 它的大致原理很简单：允许 RNN 在每一个时间步做多步运算。</p>
<blockquote>
<p>In order for the network to learn how many steps to do, we want the number of steps to be differentiable. We achieve this with the same trick we used before: instead of deciding to run for a discrete number of steps, we have an attention distribution over the number of steps to run. The output is a weighted combination of the outputs of each step.  </p>
</blockquote>
<p>为了让网络学习得到每一个时间步的计算步数，我们希望这个计算步数是可微的（也就是把其步数当作学习得到的参数，然后通过反向传播来学习这个参数）。为了实现这个 trick，我们采用类似之前 NTM 的方式，相比每次计算一个离散的步数，我们使用注意力分布的机制来选择步数，输出的结果是所有步的加权求和。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/14.png"></p>
<blockquote>
<p>There are a few more details, which were left out in the previous diagram. Here’s a complete diagram of a time step with three computation steps.  </p>
</blockquote>
<p>上图中还有更多的细节。下图是 RNN 中的在一个时间步中有3个步数的计算量。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/15.png"></p>
<blockquote>
<p>That’s a bit complicated, so let’s work through it step by step. At a high-level, we’re still running the RNN and outputting a weighted combination of the states:  </p>
</blockquote>
<p>这个图看起来有点复杂，我们会一步一步的解释它。在更高的层次，我们依然运行 RNN 并输出一个状态的加权之和。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/16.png"></p>
<p>S 表示 RNN 中的隐藏状态。</p>
<blockquote>
<p>The weight for each step is determined by a “halting neuron.” It’s a sigmoid neuron that looks at the RNN state and gives a halting weight, which we can think of as the probability that we should stop at that step.</p>
</blockquote>
<p>每一步的权重由一个“停止神经元”确定。这是一个sigmoid神经元，用来关注当前的 RNN 的状态以及赋予它一个权重。我们可以看作是这一步是否应该停止的概率。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/17.png"></p>
<blockquote>
<p>We have a total budget for the halting weights of 1, so we track that budget along the top. When it gets to less than epsilon, we stop</p>
</blockquote>
<p>我们设总的停止权重为 1,依次减去每步输出的 halt 值，直到剩余的 halt 值小于 epsilon 后就停止。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/18.png"></p>
<blockquote>
<p>When we stop, might have some left over halting budget because we stop when it gets to less than epsilon. What should we do with it? Technically, it’s being given to future steps but we don’t want to compute those, so we attribute it to the last step.</p>
</blockquote>
<p>当我们结束后，应该还会遗留一些停止值，因为我们在停止值小于epsilon时停止的。我们该怎么处理这些剩余的停止值？从技术上讲，它们应该传递到后面的计算步骤中去，但我们不想计算这些值，所以我们就把这些剩余的停止值归总到最后一步。</p>
<p><img src="/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/19.png"></p>
<blockquote>
<p>When training Adaptive Computation Time models, one adds a “ponder cost” term to the cost function. This penalizes the model for the amount of computation it uses. The bigger you make this term, the more it will trade-off performance for lowering compute time.</p>
</blockquote>
<p>当训练 ACT 模型时，需要给损失函数加上一个惩罚项。用来惩罚整个模型中的总的计算量。这一项越大时，它会在模型性能和计算量的折衷中倾向与减小计算量。</p>
<p>Code：</p>
<blockquote>
<p>The only open source implementation of Adaptive Computation Time at the moment seems to be <a target="_blank" rel="noopener" href="https://github.com/DeNeutoy/act-tensorflow">Mark Neumann’s</a> (TensorFlow).</p>
</blockquote>
<p>reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces">Attention and Augmented Recurrent Neural Networks</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-10T03:02:48.000Z" title="2018/6/10 上午11:02:48">2018-06-10</time>发表</span><span class="level-item"><time dateTime="2021-07-27T09:51:14.347Z" title="2021/7/27 下午5:51:14">2021-07-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">39 分钟读完 (大约5846个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/">论文笔记 memory networks</a></h1><div class="content"><p>Memory Networks 相关论文笔记。</p>
<ul>
<li>Memory Network with strong supervision</li>
<li>End-to-End Memory Network</li>
<li>Dynamic Memory Network</li>
</ul>
<h2 id="Paper-reading-1-Memory-Networks-Jason-Weston"><a href="#Paper-reading-1-Memory-Networks-Jason-Weston" class="headerlink" title="Paper reading 1: Memory Networks, Jason Weston"></a>Paper reading 1: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1410.3916">Memory Networks, Jason Weston</a></h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>RNNs 将信息压缩到final state中的机制，使得其对信息的记忆能力很有限。而memory work的提出就是对这一问题进行改善。</p>
<p>However, their memory (encoded by hidden states and weights) is typically too small, and is not compartmentalized enough to accurately remember facts from the past (knowledge is compressed into dense vectors). RNNs are known to have difficulty in performing memorization.</p>
<p>Memory Networks 提出的基本动机是我们需要 长期记忆（long-term memory）来保存问答的知识或者聊天的语境信息，而现有的 RNN 在长期记忆中表现并没有那么好。</p>
<h3 id="Memory-Networks"><a href="#Memory-Networks" class="headerlink" title="Memory Networks"></a>Memory Networks</h3><p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/01.png"></p>
<h4 id="four-components"><a href="#four-components" class="headerlink" title="four components:"></a>four components:</h4><ul>
<li>I:(input feature map)</li>
</ul>
<p>  把输入映射为特征向量，可以包括各种特征工程，比如parsing, coreference, entity resolution,也可以是RNN/LSTM/GRU。通常以句子为单位，将sentence用向量表示，一个句子对应一个sparse or dense feature vector.</p>
<ul>
<li>G:(generalization)</li>
</ul>
<p>  使用新的输入数据更新 memories</p>
<ul>
<li>O:(output feature map)</li>
</ul>
<p>  给定新的输入和现有的 memory state，在特征空间里产生输出</p>
<ul>
<li>R:(response)</li>
</ul>
<p>  将输出转化为自然语言</p>
<h4 id="详细推导过程"><a href="#详细推导过程" class="headerlink" title="详细推导过程"></a>详细推导过程</h4><p><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/memory_networks1.png"></p>
<p>1.<strong>I component:</strong> :encode input text to internal feature representation.</p>
<p>可以选择多种特征，比如bag of words, RNN encoder states, etc.</p>
<p>2.<strong>G component:</strong> generalization 就是结合 old memories和输入来更新 memories. $m_i=G(m_i, I(x),m), ∀i$</p>
<p>最简单的更新memory的方法是 $m_{H(x)}=I(x)$, $H(x)$ 是一个寻址函数slot selecting function，G更新的是 m 的index，可以把新的memory m，也就是新的输入 I(x) 保存到下一个空闲的地址 $m_n$ 中，并不更新原有的memory. 更复杂的 G 函数可以去更新更早的memory，甚至是所有的memory.</p>
<p>这里的新的input，如果在QA中就是question 和 old  memmory的组合 $[I(x), m_i]$.</p>
<p>3.<strong>O component</strong>: reading from memories and performing inference, calculating what are the relevant memories to perform a good response.</p>
<p>给定新的输入和memory，在memories中寻找最相关的k个记忆</p>
<p>如果k=2：</p>
<p>$$o_1=O_1(q,m)=argmax_{i=1,2,..,N}s_O(q,m_i)$$</p>
<p>$$o_2=O_2(q,m)=argmax_{i=1,2,..,N}s_O([q,o_1],m_i)$$</p>
<p>output: $[q,o_1, o_2]$ 也是module R的输入.</p>
<p>$s_O$ is a function that scores the match between the pair of sentences x and mi. $s_O$ 用来表征 question x 和 记忆 $m_i$ 的相关程度。</p>
<p>$$s_O=qUU^Tm$$</p>
<p>$s_O$ 表示问题q和当前memory m的相关程度</p>
<p>U：bilinear regression参数，相关事实的 $qUU^Tm_{true}$ 的score高于不相关事实的分数 $qUU^Tm_{random}$</p>
<p>4.<strong>R component</strong> : 对 output feature o 进行解码，得到最后的response: r=R(o)</p>
<p>$$r=argmax_{w\in W}s_R([q,m_{o_1},m_{o_2}],w)$$</p>
<p>W 是词典，$s_R$ 表示与output feature o 最相关的单词。</p>
<p>$s_R$ 和 $s_O$ 的形式是相同的。</p>
<p>$$s(x,y)=xUU^Ty$$</p>
<h4 id="Huge-Memory-问题"><a href="#Huge-Memory-问题" class="headerlink" title="Huge Memory 问题"></a>Huge Memory 问题</h4><p>如果memory太大，比如 Freebase or Wikipedia，</p>
<ul>
<li><p>可以按 entity 或者 topic 来存储 memory，这样 G 就不用在整个 memories 上操作了</p>
</li>
<li><p>如果 memory 满了，可以引入 forgetting 机制，替换掉没那么有用的 memory，H 函数可以计算每个 memory 的分数，然后重写</p>
</li>
<li><p>还可以对单词进行 hashing，或者对 word embedding 进行聚类，总之是把输入 I(x) 放到一个或多个 bucket 里面，然后只对相同 bucket 里的 memory 计算分数</p>
</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数如下，选定 2 条 supporting fact (k=2)，response 是单词的情况：</p>
<p>多类支持向量机损失:</p>
<p>minimize:  $L_i = \sum_{j\ne y_i}max(0,s_j - s_{y_i}+\Delta)$</p>
<p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/02.png"></p>
<p>其中 $\overline f, \overline f’,\overline r$ 表示负采样。比如（8）式中r表示 true response, 而 $\overline r$ 表示随机抽样词典中的其他词。</p>
<p>QA实例：</p>
<p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/03.png"></p>
<p>(6) 有没有挑选出正确的第一句话</p>
<p>(7) 正确挑选出了第一句话后能不能正确挑出第二句话</p>
<p>(6)+(7) 合起来就是能不能挑选出正确的语境，用来训练 attention 参数</p>
<p>(8) 把正确的 supporting fact 作为输入，能不能挑选出正确的答案，来训练 response 参数</p>
<h2 id="Paper-reading-2-End-To-End-Memory-Networks"><a href="#Paper-reading-2-End-To-End-Memory-Networks" class="headerlink" title="Paper reading 2 End-To-End Memory Networks"></a>Paper reading 2 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.08895">End-To-End Memory Networks</a></h2><h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3><p>上一篇paper中的缺陷：</p>
<p>The model in that work was not easy to train via backpropagation, and required supervision at each layer of the network.</p>
<p>这篇论文可以看作是上一篇论文memory networks的改进版。</p>
<p>Our model can also be seen as a version of RNNsearch with multiple computational steps (which we term “hops”) per output symbol.</p>
<p>也可以看做是将multiple hops应用到RNNsearch这篇论文上 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>。</p>
<h3 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h3><h4 id="Single-layer"><a href="#Single-layer" class="headerlink" title="Single layer"></a>Single layer</h4><p><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_struc.png"></p>
<p>输入：</p>
<ul>
<li><p>input: $x_1,…,x_i$</p>
</li>
<li><p>query: q</p>
</li>
<li><p>answer: a</p>
</li>
</ul>
<p>对于单层网络，主要分为以下几个步骤：</p>
<p>1.将input和query映射到特征空间</p>
<ul>
<li><p>memory vector {$m_i$}: ${x_i}\stackrel A\longrightarrow {m_i}$</p>
</li>
<li><p>internal state u: $q\stackrel B \longrightarrow u$</p>
</li>
</ul>
<p>2.计算attention，也就是query的向量表示u，和input中各个sentence的向量表示 $m_i$ 的匹配度。compute the match between u and each memory mi by taking the inner product followed by a softmax.</p>
<p>$$p_i=softmax(u^Tm_i)$$</p>
<p>  p is a <strong>probability vector</strong> over the inputs.</p>
<p>3.得到context vector</p>
<ul>
<li>output vector: ${x_i}\stackrel C\longrightarrow {c_i}$</li>
</ul>
<p>The response vector from the memory o is then a sum over the transformed inputs ci, weighted by the probability vector from the input:</p>
<p>$$o = \sum_ip_ic_i$$</p>
<p>和 Memory Networks with Strong Supervision 版本不同，这里的 output 是加权平均而不是一个 argmax</p>
<p>4.预测最后答案，通常是一个单词</p>
<p>$$\hat a =softmax(Wu^{k+1})= softmax(W(o^k+u^k))$$</p>
<p>W可以看做反向embedding，W.shape=[embed_size, V]</p>
<p>5.对 $\hat a$ 进行解码，得到自然语言的response</p>
<p>$$\hat a \stackrel C \longrightarrow a$$</p>
<p>其中：</p>
<p>A: intput embedding matrix</p>
<p>C: output embedding matrix</p>
<p>W: answer prediction matrix</p>
<p>B: question embedding matrix</p>
<p>单层网络实例：</p>
<p><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_struc2.png"></p>
<p>这里的 memory {$m_i$} 直接用于输出向量 $c_i$. 其实我也疑惑，为啥要重新用一个output embedding C，直接用 $m_i$ 不好吗。其实这些小tricks也说不准好不好，都是试出来的吧，因为怎么说都合理。。。</p>
<h4 id="Multiple-Layers-Multiple-hops"><a href="#Multiple-Layers-Multiple-hops" class="headerlink" title="Multiple Layers/ Multiple hops"></a>Multiple Layers/ Multiple hops</h4><p>多层结构（K hops）也很简单，相当于做多次 addressing/多次 attention，每次 focus 在不同的 memory 上，不过在第 k+1 次 attention 时 query 的表示需要把之前的 context vector 和 query 拼起来，其他过程几乎不变。</p>
<p>$$u_{k+1}=u^k+o^k$$</p>
<p><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_multi_hop.png"></p>
<h3 id="对比上一篇paper来理解"><a href="#对比上一篇paper来理解" class="headerlink" title="对比上一篇paper来理解"></a>对比上一篇paper来理解</h3><p>多层网络也可以看做是四个组件构成的：</p>
<ul>
<li><p>input components: 就是将query和sentences映射到特征空间中</p>
</li>
<li><p>generalization components： 更新memory，这里的memory也是在变化的，${m_i}=AX$， 但是embedding matrix A 是逐层变化的</p>
</li>
<li><p>output components: attention就是根据inner product后softmax计算memory和query之间的匹配度，然后更新input，也就是[u_k,o_k]， 可以是相加/拼接，或者用RNN. 区别是，在上一篇论文中是argmax，$o_2=O_2(q,m)=argmax_{i=1,2,..,N}s_O([q,o_1],m_i)$, 也就是选出匹配程度最大的 memory $m_i$, 而这篇论文是对所有的memory进行加权求和</p>
</li>
<li><p>response components: 跟output components类似啊，上一篇论文是与词典中所有的词进行匹配，求出相似度最大的 $r=argmax_{w\in W}s_R([q,m_{o_1},m_{o_2}],w)$，而这篇论文是 $\hat a=softmax(Wu^{k+1})=softmax(W(u^k+o^k))$ 最小化交叉熵损失函数训练得到 answer prediction matrix W.</p>
</li>
</ul>
<p>Overall, it is similar to the Memory Network model in [23], except that the hard max operations within each layer have been replaced with a continuous weighting from the softmax.</p>
<h3 id="一些技术细节"><a href="#一些技术细节" class="headerlink" title="一些技术细节"></a>一些技术细节</h3><p>每一层都有 mebedding matrices $A^k, C^k$,用来embed inputs {$x_i$},为了减少训练参数.作者尝试了以下两种情况：</p>
<ol>
<li>Adjacent</li>
</ol>
<ul>
<li><p>上一层的output embedding matrix 是下一层的 input embedding matrix, 即 $A^{k+1}=C^k$</p>
</li>
<li><p>最后一层的output embedding 可用作 prediction embedding matrix， 即 $W^T=C^k$</p>
</li>
<li><p>question embedding matrix = input embedding matrix of the first layer, $B=A^1$</p>
</li>
</ul>
<ol start="2">
<li>Layer-wise (RNN-like)</li>
</ol>
<ul>
<li><p>$A^1=A^2=…=A^k, C^1=C^2=…C^k$</p>
</li>
<li><p>$u^{k+1} = Hu^k+o^k$</p>
</li>
</ul>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>数据集来源：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.05698">Towards AI-complete question answering: A set of prerequisite toy tasks</a></p>
<p>总共有 20 QA tasks，其中每个task有 $I(I\le 320)$ 个sentence {$x_i$}, 词典大小 V=170, 可以看做这是个玩具级的任务。每个task有1000个problems</p>
<h4 id="Modle-details"><a href="#Modle-details" class="headerlink" title="Modle details"></a>Modle details</h4><h5 id="Sentence-representations"><a href="#Sentence-representations" class="headerlink" title="Sentence representations"></a>Sentence representations</h5><p>也就是将input和query映射到特征空间，有两种方式：</p>
<p>1.Bag of words(BOW) representation</p>
<p>$$m_i=\sum_jAx_{ij}$$</p>
<p>$$c_i=\sum_jCx_{ij}$$</p>
<p>$$u=\sum_jBq_j$$</p>
<p>分别对每个词embed，然后sum，缺点是没有考虑词序</p>
<p>2.encodes the position of words within the sentence 考虑词序的编码</p>
<p>$$m_i=\sum_jl_j\cdot Ax_{ij}$$</p>
<p>i表示第i个sentence，j表示这个sentence中的第j个word</p>
<p>$$l_{kj}=(1-j/J)-(k/d)(1-2j/J)$$</p>
<p>查看源码时发现很多代码的position encoder与原paper不一样，比如<a target="_blank" rel="noopener" href="https://github.com/domluna/memn2n/blob/master/memn2n/memn2n.py#L12-L25">domluna/memn2n</a>中公式是：</p>
<p>$$l_{kj} = 1+4(k- (d+1)/2)(j-(J+1)/2)/d/J$$</p>
<p>原本词 $x_{ij}$ 的向量表示就是embeded后的 $Ax_{ij},(shape=[1, embed_size])$, 但现在要给这个向量加一个权重 $l_j$,而且这个权重不是一个值，而是一个向量，对 $Ax_{ij}$ 中每一个维度的权重也是不一样的。</p>
<p>令J=20, d=50. 具体两个公式的差别可以查看</p>
<p><a target="_blank" rel="noopener" href="https://www.wolframalpha.com/input/?i=(1.0+-+(y+/+20))+-+(x+/+50)+*+(1.0+-+(2.0+*+y+/+20))+for+0+%3C+x+%3C+50+and+0+%3C+y+%3C+20">wolframalpha1</a></p>
<p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/08.png"></p>
<p><a target="_blank" rel="noopener" href="https://www.wolframalpha.com/input/?i=1+++4+*+((y+-+(20+++1)+/+2)+*+(x+-+(50+++1)+/+2))+/+(20+*+50)+for+0+%3C+x+%3C+50+and+0+%3C+y+%3C+20">wolframalpha2</a></p>
<p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/09.png"></p>
<p>也就是说不仅跟word在sentence中的位置有关，还和embed_size中的维度有关。这就很难理解了。。。</p>
<p>好像跟句子的结构相关，北大有篇相关的论文<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/D16-1007">A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification</a></p>
<p>其中 J 表示sentence的长度，d表示 dimension of the embedding. 这种sentence representation称为 position encoding(PE).也就是词序会影响memory $m_i$.</p>
<p><strong>position encoding 代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_encoding</span>(<span class="params">sentence_size, embedding_size</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Position Encoding described in section 4.1 [1]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    encoding = np.ones((embedding_size, sentence_size), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    le = embedding_size+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    ls = sentence_size + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, le):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, ls):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># here is different from the paper.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># the formulation in paper is: l_&#123;kj&#125;=(1-j/J)-(k/d)(1-2j/J)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># here the formulation is: l_&#123;kj&#125; = 1+4(k- (d+1)/2)(j-(J+1)/2)/d/J,</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 具体表现可查看 https://www.wolframalpha.com/input/?i=1+%2B+4+*+((y+-+(20+%2B+1)+%2F+2)+*+(x+-+(50+%2B+1)+%2F+2))+%2F+(20+*+50)+for+0+%3C+x+%3C+50+and+0+%3C+y+%3C+20</span></span><br><span class="line"></span><br><span class="line">            encoding[k-<span class="number">1</span>, j-<span class="number">1</span>] = (k - (embedding_size+<span class="number">1</span>)/<span class="number">2</span>) * (j - (sentence_size+<span class="number">1</span>)/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    encoding = <span class="number">1</span> + <span class="number">4</span> * encoding / embedding_size / sentence_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make position encoding of time words identity to avoid modifying them</span></span><br><span class="line"></span><br><span class="line">    encoding[:, -<span class="number">1</span>] = <span class="number">1.0</span> <span class="comment"># 最后一个sentence的权重都为1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.transpose(encoding) <span class="comment"># [sentence_size, embedding_size]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="Temporal-Encoding"><a href="#Temporal-Encoding" class="headerlink" title="Temporal Encoding"></a>Temporal Encoding</h5><p>将memory改进为：</p>
<p>$$m_i=\sum_jAx_{ij}+T_A(i)$$</p>
<p>其中 $T_A(i)$ is the ith row of a special matrix $T_A$ that encodes temporal information. 用一个特殊的矩阵 $T_A$ 来编码时间信息。$T_A(i)$ i表示第i个sentence的包含时间信息？？</p>
<p>同样的output embedding:</p>
<p>$$c_i=\sum_jCx_{ij}+T_C(i)$$</p>
<h5 id="Learning-time-invariance-by-injecting-random-noise"><a href="#Learning-time-invariance-by-injecting-random-noise" class="headerlink" title="Learning time invariance by injecting random noise"></a>Learning time invariance by injecting random noise</h5><p>we have found it helpful to add “dummy” memories to regularize TA.</p>
<h4 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h4><p>1.learning rate decay  </p>
<p>2.gradient clip  </p>
<p>3.linear start training  </p>
<p>4.null padding, zero padding  </p>
<h3 id="完整代码实现"><a href="#完整代码实现" class="headerlink" title="完整代码实现"></a>完整代码实现</h3><p><a target="_blank" rel="noopener" href="https://github.com/PanXiebit/text-classification/blob/master/06-memory%20networks/memn2n_model.py">https://github.com/PanXiebit/text-classification/blob/master/06-memory%20networks/memn2n_model.py</a></p>
<h2 id="Paper-reading-3-Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing"><a href="#Paper-reading-3-Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing" class="headerlink" title="Paper reading 3 Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"></a>Paper reading 3 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.07285">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>Most tasks in natural language processing can be cast into question answering (QA) problems over language input.</p>
<p>大部分的自然语言处理的任务都可以看作是QA问题，比如QA, sentiment analysis, part-of-speech tagging.</p>
<p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/05.png"></p>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/06.png"></p>
<p>可以分为以下4个模块：</p>
<ul>
<li><p>Input Module: 将输入文本编码为distribution representations</p>
</li>
<li><p>Question Module: 将question编码为distribution representations</p>
</li>
<li><p>Episodic Memory Module: 通过attention机制选择focus on输入文本中的某些部分，然后生成memory vector representation.</p>
</li>
<li><p>Answer Module: 依据the final memory vector生成answer</p>
</li>
</ul>
<p>Detailed visualization:</p>
<p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/07.png"></p>
<h4 id="Input-Module"><a href="#Input-Module" class="headerlink" title="Input Module"></a>Input Module</h4><p>主要分为两种情况：</p>
<p>1.输入是single sentence，那么input module输出的就是通过RNN计算得到的隐藏状态 $T_C= T_I$, $T_I$ 表示一个sentence中的词的个数。</p>
<p>2.输入是a list of sentences，在每个句子后插入一个结束符号 end-of-sentence token, 然后每个sentence的final hidden作为这个sentence的representation. 那么input module输出 $T_C$, $T_C$等于sequence的sentence个数。</p>
<p>然后RNN使用的是GRU，作者也尝试过LSTM，发现效果差不多，但LSTM计算量更大。</p>
<h4 id="Question-Module"><a href="#Question-Module" class="headerlink" title="Question Module"></a>Question Module</h4><p>同样的使用GRU编码，在t时间步， 隐藏状态</p>
<p>$$q_t=GRU(L[w_t^Q],q_{t-1})$$</p>
<p>L代表embedding matrix.</p>
<p>最后输出 final hidden state.</p>
<p>$$q=q_{T_Q}$$</p>
<p>$T_Q$ 是question的词的个数。</p>
<h4 id="Episodic-Memory-Module"><a href="#Episodic-Memory-Module" class="headerlink" title="Episodic Memory Module"></a>Episodic Memory Module</h4><p>由 internal memory, attention mechansim, memory update mechanism 组成。 输入是 input module 和 question module 的输出。</p>
<p>把 input module 中每个句子的表达（fact representation c）放到 episodic memory module 里做推理，使用 attention 原理从 input module 中提取相关信息，同样有 multi-hop architecture。</p>
<p>1.<strong>Needs for multiple Episodes:</strong> 通过迭代使得模型具有了传递推理能力 transitive inference.</p>
<p>2.<strong>Attention Mechanism</strong>: 使用了一个gating function作为attention机制。相比在 end-to-end MemNN 中attention使用的是linear regression，即对inner production通过softmax求权重。 这里使用一个两层前向神经网络 G 函数.</p>
<p>$$g_t^i=G(c_t,m^{i-1},q)$$</p>
<p>$c_t$ 是candidate fact, $m_{i-1}$ 是previous memory， question q. t 表示sentence中的第t时间步，i表示episodic的迭代次数。</p>
<p>这里作者定义了 a large feture $z(c,m,q)$ 来表征input, memory, question之间的相似性。</p>
<p>$$z_t^i=[c_t, m^{i-1},q, c_t\circ q,c_t\circ m^{i-1},|c_t-q|,|c_t-m^{i-1}|, c_t^TW^{(b)}q, c_t^TW^{(b)}m^{i-1}]$$</p>
<p>总的来说，就是根据向量内积，向量相减来表示相似度。 跟<a href="http://www.panxiaoxie.cn/2018/05/21/cs224d-lecture16-dynamic-neural-network">cs224d-lecture16 dynamic Memory network</a>Richard Socher本人讲的有点区别，不过这个既然是人工定义的，好像怎么说都可以。</p>
<p>然后通过G函数，也就是两层前向神经网络得到一个scale score.</p>
<p>$$G = \sigma(W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)})$$</p>
<p>将 $c_t$, $m^{i-1}, q 带入到G函数，即可求得$$g_i^t$，也就是candidate fact $c_i$ 的score.</p>
<p>计算完每一次迭代后的分数后，来更新episode $e^i$, 相当于 context vector,</p>
<p><strong>soft attention</strong></p>
<p>在之前的attention机制中，比如<a href="http://www.panxiaoxie.cn/2018/05/08/cs224d-lecture10-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">cs224d-lecture10-机器翻译和注意力机制</a>介绍的attention得到的context vector，在end-to-end MemNN中attention也是fact representation的加权求和。</p>
<p><strong>attention based GRU</strong></p>
<p>但这篇论文中应用了GRU，对fact representation c 进行处理，然后加上gate</p>
<p>$$h_t^i=g_t^iGRU(c_t,h_{t-1}^i)+(1-g_t^i)h_{i-1}^t$$</p>
<p>所以这里的GRU应该是 $T_C$步吧？？</p>
<p>每次迭代的context vector是对 input module 的输出进行 attention-based GRU编码的最后的隐藏状态:</p>
<p>$$e^i=h_{T_C}^i$$</p>
<p><strong>总结一下：</strong></p>
<p><strong>这部分attention mechanism目的就是生成episode $e^i$,$e^i$ 是第i轮迭代的所有input相关信息的summary.也就是 context vector,将input text压缩到一个向量表示中，end-to-end MemNN用了soft attention，就是加权求和。而这里用了GRU，各个时间步的权重不是直接相乘，而是作为一个gate机制。</strong></p>
<p>3.<strong>Memory Update Mechanism</strong></p>
<p>上一步计算的episode $e^i$ 以及上一轮迭代的memory $m^{i-1}$ 作为输入来更新memory $m_i$</p>
<p>$$m_i=GRU(e^i,m^{i-1})$$</p>
<p>$m^0=q$, 所以这里的GRU是单步的吧</p>
<p>经过 $T_M$ 次迭代： $m=m^{T_M}$, 也就是episodic memory module的输出，即answer module的输入。</p>
<p>在end-to-end MemNN的memory update中，$u_{k+1}=u^k+o^k$, 而在这篇论文中,如果也采用这种形式的话就是 $m^{i}=e^i+m^{i-1}$，但作者采用了 RNN 做非线性映射，用 episode $e_i$ 和上一个 memory $m_{i−1}$ 来更新 episodic memory，其 GRU 的初始状态包含了 question 信息，$m_0=q$。</p>
<p>4.<strong>Criteria for stopping</strong></p>
<p>Episodic Memory Module 需要一个停止迭代的信号。一般可以在输入中加入一个特殊的 end-of-passes 的信号，如果 gate 选中了该特殊信号，就停止迭代。对于没有显性监督的数据集，可以设一个迭代的最大值。</p>
<h4 id="Answer-Module"><a href="#Answer-Module" class="headerlink" title="Answer Module"></a>Answer Module</h4><p>使用了GRU的decoder。输入是question module的输出q和上一个时刻的hidden state $a_{t-1}$,初始状态是episodic memory module的输出 $a_0=m^{T_M}$.</p>
<p>$$y_t=softmax(W^{(a)}a_t)$$</p>
<p>$$a_t=GRU([y_{t-1},q],a_{t-1})$$</p>
<p>这里应该就是单步GRU吧，毕竟question的向量表示q只有一个呀。</p>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>使用 cross-entroy 作为目标函数。如果 <strong>数据集有 gate 的监督数据</strong>，还可以将 gate 的 cross-entroy 加到总的 cost上去，一起训练。训练直接使用 backpropagation 和 gradient descent 就可以。</p>
<h3 id="总结-对比上一篇论文End-to-end-memory-networks"><a href="#总结-对比上一篇论文End-to-end-memory-networks" class="headerlink" title="总结:对比上一篇论文End-to-end memory networks"></a>总结:对比上一篇论文End-to-end memory networks</h3><ul>
<li><p>input components: end2end MemNN 采用embedding，而DMN使用GRU</p>
</li>
<li><p>generalization components: 也就是memory update，End2End MemNN采用线性相加 $u^{k+1}=u^k+o^k$,其中的 $o^k$ 就是经过attention之后得到的memory vector</p>
</li>
<li><p>output components: end2end MemNN采用的是对比memory和query,用内积求相似度，然后softmax求权重，最后使用加权求和得到context vector. 而DMN采用的是人工定义相似度的表示形式，然后用两层前向神经网络计算得到score，再对score用softmax求权重，再然后把权重当做gate机制，使用GRU求context vector</p>
</li>
</ul>
<ul>
<li>response components: end2end MemNN 直接使用最后的 top memory layer 预测，而DMN是把top memory 当做init hidden state</li>
</ul>
<p>总之，DMN实在是太太太复杂了。。每一个module都用到了RNN</p>
<h2 id="Paper-reading-4-DMN"><a href="#Paper-reading-4-DMN" class="headerlink" title="Paper reading 4  DMN+"></a>Paper reading 4  DMN+</h2><p>paper:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1603.01417.pdf">Dynamic Memory Networks for Visual and Textual Question Answering (2016)</a></p>
<h3 id="Motivate"><a href="#Motivate" class="headerlink" title="Motivate"></a>Motivate</h3><p>提出了DMN+，是DMN的改进版，同时将其应用到 Visual Question Answering 这一任务上。</p>
<p>However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images.</p>
<p>这段话是描述DMN的缺点的，在没有标注 supporting facts的情况下表现不好。但是DMN貌似也并不需要标注 supporting facts啊。。。</p>
<p>Like the original DMN, this memory network requires that supporting facts are labeled during QA training. End-toend memory networks (Sukhbaatar et al., 2015) do not have this limitation.</p>
<p>Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions.</p>
<p>这篇文章对DMN中的 input module进行了修改，并且提出了新的模型架构适用于图像的。</p>
<p>DMN 存在的两个问题：</p>
<p>输入模块只考虑了过去信息，没考虑到将来信息</p>
<p>只用 word level 的 GRU，很难记忆远距离 supporting sentences 之间的信息。</p>
<p>总的来说这篇文章贡献主要还是在应用到图像上了，至于作者所说的 input module的改进，只是为了减少计算量，而且改进版中的 bi-RNN 和 position encoding 都是在别人的论文中出现了的。</p>
<h3 id="Model-Architecture-1"><a href="#Model-Architecture-1" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>同DMN一样，也分为 input module, question module, episodic module 和 answer module.</p>
<h4 id="Input-Module-1"><a href="#Input-Module-1" class="headerlink" title="Input Module"></a>Input Module</h4><h5 id="input-module-for-text-QA"><a href="#input-module-for-text-QA" class="headerlink" title="input module for text QA"></a>input module for text QA</h5><p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/10.png"></p>
<p>主要分为两个组件： sentence reader 和 input fusion layer.</p>
<p>sentence reader: 用encoding position代替RNN对单个sentence进行编码。用 positional encoding 的原因是在这里用 GRU/LSTM 编码句子计算量大而且容易过拟合（毕竟 bAbI 的单词量很小就几十个单词。。），这种方法反而更好。</p>
<p>input fusion layer: 使用 bi-directional GRU 来得到context 信息，兼顾过去和未来的信息。</p>
<p>总的来说： DMN+ 把 single GRU 替换成了类似 hierarchical RNN 结构，一个 sentence reader 得到每个句子的 embedding，一个 input infusion layer 把每个句子的 embedding 放入另一个 GRU 中，得到 context 信息，来解决句子远距离依赖的问题。</p>
<h5 id="input-module-for-VQA"><a href="#input-module-for-VQA" class="headerlink" title="input module for VQA"></a>input module for VQA</h5><p><img src="/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/11.png"></p>
<p>1.Local region feature extraction:</p>
<p>获取局部特征信息，使用VGG预训练得到的特征。局部特征 feature vector 通过一个linear layer 和 tanh activation 得到 feature embedding.</p>
<p>2.Input fusion layer:</p>
<p>将 feature embedding 放入到 bi-GRU 中。</p>
<p>Without global information, their representational power is quite limited, with simple issues like object scaling or locational variance causing accuracy problems.  强调了为什么要使用 input fusion layer.</p>
<h3 id="Question-Module-1"><a href="#Question-Module-1" class="headerlink" title="Question Module"></a>Question Module</h3><p>这部分跟DMN是一样的, question 都是文本，用RNN编码。</p>
<h3 id="Episodic-Module"><a href="#Episodic-Module" class="headerlink" title="Episodic Module"></a>Episodic Module</h3><h4 id="score-mechanism"><a href="#score-mechanism" class="headerlink" title="score mechanism"></a>score mechanism</h4><p>input module 的输出是:</p>
<p>$$\overleftrightarrow F=[\overleftrightarrow f_1, …,\overleftrightarrow f_N]$$</p>
<p>同DMN一样，作者也是用了人工特征，相比DMN简化一点：</p>
<p>$$z_i^t=[\overleftrightarrow f_i\circ q,\overleftrightarrow f_i\circ m^{t-1},|\overleftrightarrow f_i-q|,|\overleftrightarrow f_i-m^{i-1}|]$$</p>
<p>这里与前面DMN的公式有点区别，就是这里的i表示input module中的时间步， t 表示episodic迭代次数。</p>
<p>同样使用一个两层前向神经网络：</p>
<p>$$G = W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)}$$</p>
<p>但是这里不是使用 sigmoid 函数来求的 score，而是使用softmax 来求score $g_i^t$.</p>
<p>$$g_i^t=\dfrac{Z_i^t}{\sum_{k=1}^{M_i}exp(Z_k^t)}$$</p>
<h4 id="attention-mechanism"><a href="#attention-mechanism" class="headerlink" title="attention mechanism"></a>attention mechanism</h4><p>比较了 soft attention 和 attention-based-GRU.相比DMN那篇论文，这里给出了详细的比较。</p>
<h5 id="soft-attention-就是简单的加权求和。"><a href="#soft-attention-就是简单的加权求和。" class="headerlink" title="soft attention, 就是简单的加权求和。"></a>soft attention, 就是简单的加权求和。</h5><p>$$c^t=\sum_{i=1}^Ng_i^t\overleftrightarrow f_i$$</p>
<p>其缺点在于丢失了位置信息和词序信息。</p>
<p>感觉简单的attention已经很好了吧。。前面 $\overleftrightarrow f_i$ 不就是考虑了词序信息的么，然后再用GRU对 $\overleftrightarrow f_i$ 处理不会过拟合吗？？？</p>
<h5 id="attention-based-GRU"><a href="#attention-based-GRU" class="headerlink" title="attention based GRU"></a>attention based GRU</h5><p>使用attention gate $g_i^t$ 代替 update gate $u_i$. 我们知道 $u_i$ 是通过 current input 和 previous hidden state得到的。 而使用 attention gate $g_i^t$ 能够考虑到 question 和 previous memory. 因为我们这里是要更新memory， 所以这样很合理呀。。厉害了</p>
<p>$$h_i=g_i^t\circ \tilde h_i+(1-g_i^t)\circ h_{i-1}$$</p>
<h4 id="memory-update-mechanism"><a href="#memory-update-mechanism" class="headerlink" title="memory update mechanism"></a>memory update mechanism</h4><p>在DMN中，更新memory在基于 previous memory 和 当前的 context vector 的GRU编码得到的。 DMN+采用的是将 previous memory $m^{t-1}$, 当前 context $c^t$，和question q 拼接起来，然后通过全连接层，以及relu激活函数得到的：</p>
<p>$$m_t = ReLU(W^t[m^{t-1},c^t,q]+b)$$</p>
<p>使用relu的全连接层能提升0.5%的准确率。</p>
<h3 id="Answer-Module-1"><a href="#Answer-Module-1" class="headerlink" title="Answer Module"></a>Answer Module</h3><p>同DMN.</p>
<p>reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.shuang0420.com/2017/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/">徐阿衡-论文笔记 - Memory Networks</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-09T02:40:22.000Z" title="2018/6/9 上午10:40:22">2018-06-09</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/CSAPP/">CSAPP</a></span><span class="level-item">37 分钟读完 (大约5537个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/09/CSAPP-02-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/">SCAPP-02-信息的表示和处理</a></h1><div class="content"><p>CSAPP 第二章</p></div><a class="article-more button is-small is-size-7" href="/2018/06/09/CSAPP-02-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/#more">阅读更多</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/15/">上一页</a></div><div class="pagination-next"><a href="/page/17/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/15/">15</a></li><li><a class="pagination-link is-current" href="/page/16/">16</a></li><li><a class="pagination-link" href="/page/17/">17</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="潘晓榭"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">潘晓榭</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">116</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-12T10:38:37.000Z">2021-09-12</time></p><p class="title"><a href="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/">论文笔记-Discrete Latent Variables Based Generation</a></p><p class="categories"><a href="/categories/generation/">generation</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-22T03:28:02.000Z">2021-08-22</time></p><p class="title"><a href="/2021/08/22/leetcode/">leetcode</a></p><p class="categories"><a href="/categories/leetcode/">leetcode</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-23T11:13:20.000Z">2021-07-23</time></p><p class="title"><a href="/2021/07/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Clip/">论文笔记-Clip!!!</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/">vision-language</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-11T07:58:30.000Z">2021-07-11</time></p><p class="title"><a href="/2021/07/11/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sign-language-recognition-and-translation/">论文笔记-sign language recognition and translation</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/">computer vision</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-02T04:37:58.000Z">2021-07-02</time></p><p class="title"><a href="/2021/07/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-constrast-learning-in-NLP/">论文笔记-constrast learning in NLP</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/">constrast learning</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">八月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">七月 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">四月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">九月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">六月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">六月 2018</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">四月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESA/"><span class="tag">ESA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN%EF%BC%8CRL/"><span class="tag">GAN，RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MRC-and-QA/"><span class="tag">MRC and QA</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Translation/"><span class="tag">Machine Translation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/capsules/"><span class="tag">capsules</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-augmentation/"><span class="tag">data augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/generation/"><span class="tag">generation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/leetcode/"><span class="tag">leetcode</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/open-set-recognition/"><span class="tag">open set recognition</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentence-embedding/"><span class="tag">sentence embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentiment-classification/"><span class="tag">sentiment classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language/"><span class="tag">sign language</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language-recognition/"><span class="tag">sign language recognition</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-matching/"><span class="tag">text matching</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/transfer-learning/"><span class="tag">transfer learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-language/"><span class="tag">vision-language</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="tag">数据结构与算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag">8</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>