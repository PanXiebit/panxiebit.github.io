<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="潘晓榭"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘晓榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="潘晓榭"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="潘晓榭"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="潘晓榭"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-09T02:40:22.000Z" title="2018/6/9 上午10:40:22">2018-06-09</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/CSAPP/">CSAPP</a></span><span class="level-item">37 分钟读完 (大约5537个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/09/CSAPP-02-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/">SCAPP-02-信息的表示和处理</a></h1><div class="content"><p>CSAPP 第二章</p></div><a class="article-more button is-small is-size-7" href="/2018/06/09/CSAPP-02-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-08T12:08:46.000Z" title="2018/6/8 下午8:08:46">2018-06-08</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.123Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/CSAPP/">CSAPP</a></span><span class="level-item">16 分钟读完 (大约2371个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/08/CSAPP-01-A-tour-of-computer-system/">CSAPP-01.A tour of computer system</a></h1><div class="content"><p>CSAPP 第一章</p></div><a class="article-more button is-small is-size-7" href="/2018/06/08/CSAPP-01-A-tour-of-computer-system/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-03T01:47:50.000Z" title="2018/6/3 上午9:47:50">2018-06-03</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">7 分钟读完 (大约1103个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks/">文本分类系列5-Hierarchical Attention Networks</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf">Hierarchical Attention Networks for Document Classification</a></p>
<h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>主要原理：</p>
<p>the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since **documents</p>
<p>have a hierarchical structure (words form sentences, sentences form a document)**, we likewise construct a document representation by first building representations of sentences and then aggregating those into</p>
<p>a document representation. Second, it is observed that different words and sentences in a documents are differentially informative.</p>
<p>对于一个document含有这样的层次结构，document由sentences组成，sentence由words组成。</p>
<p>the importance of words and sentences are highly context dependent, i.e. the same word or sentence may be differentially important in different context (x3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015) — one at the word level and one at the sentence level — that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document.</p>
<p>words和sentences都是高度上下文依赖的，同一个词或sentence在不同的上下文中，其表现的重要性会有差别。因此，这篇论文中使用了两个attention机制，来表示结合了上下文信息的词或句子的重要程度。（这里结合的上下文的词或句子，就是经过RNN处理后的隐藏状态）。</p>
<p>Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et</p>
<p>al., 2014)</p>
<p>attention不仅有好的效果，而且能够可视化的看见哪些词或句子对哪一类document的分类影响大。</p>
<p>本文的创新点在于，考虑了ducument中sentence这一层次结构，因为对于一个document的分类，可能前面几句话都是废话，而最后一句话来了一个转折，对document的分类起决定性作用。而之前的研究，只考虑了document中的词。</p>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="/2018/06/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks%5Chan.png"></p>
<h4 id="GRU-based-sequence-encoder"><a href="#GRU-based-sequence-encoder" class="headerlink" title="GRU-based sequence encoder"></a>GRU-based sequence encoder</h4><p><strong>reset gate:</strong> controls how much the past state contributes to the candidate state.</p>
<p>$$r_t=\sigma(W_rx_t+U_rh_{t-1}+b_r)$$</p>
<p><strong>candidate state:</strong></p>
<p>$$\tilde h_t=tanh(W_hx_t+r_t\circ (U_hh_{t-1})+b_h)$$</p>
<p><strong>update gate:</strong> decides how much past information is kept and how much new information is added.</p>
<p>$$z_t=\sigma(W_zx_t+U_zh_{t-1}+b_z)$$</p>
<p><strong>new state:</strong> a linear interpolation between the previous state $h_{t−1}$ and the current new state $\tilde h_t$ computed with new sequence information.</p>
<p>$$h_t=(1-z_t)\circ h_{t-1}+z_t\circ \tilde h_t$$</p>
<h4 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h4><h5 id="Word-Encoder"><a href="#Word-Encoder" class="headerlink" title="Word Encoder"></a>Word Encoder</h5><p>$$x_{it}=W_ew_{it}, t\in [1, T]$$</p>
<p>$$\overrightarrow h_{it}=\overrightarrow {GRU}(x_{it}),t\in[1,T]$$</p>
<p>$$\overleftarrow h_{it}=\overleftarrow {GRU}(x_{it}),t\in [T,1]$$</p>
<p>$$h_{it} = [\overrightarrow h_{it},\overleftarrow h_{it}]$$</p>
<p>i means the $i^{th}$ sentence in the document, and t means the $t^{th}$ word in the sentence.</p>
<h5 id="Word-Attention"><a href="#Word-Attention" class="headerlink" title="Word Attention"></a>Word Attention</h5><p>Not all words contribute equally to the representation of the sentence meaning.</p>
<p>Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector.</p>
<p>Attention机制说到底就是给予sentence中每个结合了上下文信息的词一个权重。关键在于这个权重怎么确定？</p>
<p>$$u_{it}=tanh(W_wh_{it}+b_w)$$</p>
<p>$$\alpha_{it}=\dfrac{exp(u_{it}^Tu_w)}{\sum_t^Texp(u_{it}^Tu_w)}$$</p>
<p>$$s_i=\sum_t^T\alpha_{it}h_{it}$$</p>
<p>这里首先是将 $h_{it}$ 通过一个全连接层得到 hidden representation $u_{it}$,然后计算 $u_{it}$ 与 $u_w$ 的相似性。并通过softmax归一化得到每个词与 $u_w$ 相似的概率。越相似的话，这个词所占比重越大，对整个sentence的向量表示影响越大。</p>
<p>那么关键是这个 $u_w$ 怎么表示？</p>
<p>The context vector $u_w$ can be seen as a high level representation of a fixed</p>
<p>query “what is the informative word” over the words like that used in memory networks (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.08895">Sukhbaatar et al., 2015, End-to-end memory networks.</a>; <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.07285">Kumar et al., 2015, Ask me anything: Dynamic memory networks for natural language processing.</a>). The word context vector $u_w$ is randomly initialized and jointly learned during the training process.</p>
<h5 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h5><p>$$\overrightarrow h_{i}=\overrightarrow {GRU}(s_{i}),t\in[1,L]$$</p>
<p>$$\overleftarrow h_{i}=\overleftarrow {GRU}(s_{i}),t\in [L,1]$$</p>
<p>$$H_i=[\overrightarrow h_{i}, \overleftarrow h_{i}]$$</p>
<p>hi summarizes the neighbor sentences around sentence i but still focus on sentence i.</p>
<h5 id="Sentence-Attention"><a href="#Sentence-Attention" class="headerlink" title="Sentence Attention"></a>Sentence Attention</h5><p>$$u_i=tanh(W_sH_i+b_s)$$</p>
<p>$$\alpha_i=\dfrac{exp(u_i^Tu_s)}{\sum_i^Lexp(u_i^Tu_s)}$$</p>
<p>$$v = \sum_i^L\alpha_ih_i$$</p>
<p>同样的 $u_s$ 表示： a sentence level context vector $u_s$</p>
<h4 id="Document-Classification"><a href="#Document-Classification" class="headerlink" title="Document Classification"></a>Document Classification</h4><p>The document vector v is a high level representation</p>
<p>of the document and can be used as features for document classification:</p>
<p>$$p=softmax(W_cv+b_c)$$</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="需要注意的问题"><a href="#需要注意的问题" class="headerlink" title="需要注意的问题"></a>需要注意的问题</h4><ul>
<li><p>如果使用tensorboard可视化</p>
</li>
<li><p>变量范围的问题</p>
</li>
</ul>
<h3 id="Context-dependent-attention-weights"><a href="#Context-dependent-attention-weights" class="headerlink" title="Context dependent attention weights"></a>Context dependent attention weights</h3><h3 id="Visualization-of-attention"><a href="#Visualization-of-attention" class="headerlink" title="Visualization of attention"></a>Visualization of attention</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-02T12:46:40.000Z" title="2018/6/2 下午8:46:40">2018-06-02</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.363Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/">Machine Translation</a></span><span class="level-item">33 分钟读完 (大约4906个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/">论文笔记, Attention Is All You Need</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
<h3 id="1-paper-reading"><a href="#1-paper-reading" class="headerlink" title="1. paper reading"></a>1. paper reading</h3><h4 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h4><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t.</p>
<p>This inherently sequential nature <strong>precludes parallelization within training examples</strong>, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.</p>
<p>RNN模型有两个很致命的缺点：</p>
<p>$$y_t=f(y_{t-1},x_t)$$</p>
<ul>
<li>一是无法解决长句子的长期依赖的问题（这是因为反向传播，loss的梯度很难传递到比较靠前的位置，造成前面的词对整体的影响偏小，[Gradient flow in</li>
</ul>
<p>recurrent nets: the difficulty of learning long-term dependencies](<a target="_blank" rel="noopener" href="http://www.bioinf.jku.at/publications/older/ch7.pdf)%EF%BC%89%EF%BC%9B">http://www.bioinf.jku.at/publications/older/ch7.pdf)）；</a></p>
<ul>
<li>二是计算无法并行化的问题（后一个时刻的计算依赖于前一个时刻的计算），导致训练速度很慢。</li>
</ul>
<p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.</p>
<p>Attention机制能够有效解决RNN无法长时间依赖的问题，但是对于无法并行化计算的问题依旧存在。</p>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h4><h5 id="2-1-Extended-Neural-GPU-16-ByteNet-18-and-ConvS2S-9"><a href="#2-1-Extended-Neural-GPU-16-ByteNet-18-and-ConvS2S-9" class="headerlink" title="2.1 Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]"></a>2.1 Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]</h5><h5 id="2-2-Self-attention"><a href="#2-2-Self-attention" class="headerlink" title="2.2 Self-attention"></a>2.2 Self-attention</h5><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03130">A structured self-attentive sentence embedding</a></p>
<h5 id="2-3-End-to-end-memory-networks"><a href="#2-3-End-to-end-memory-networks" class="headerlink" title="2.3 End-to-end memory networks"></a>2.3 End-to-end memory networks</h5><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.08895">End-to-end memory networks</a></p>
<h5 id="2-4-Transformer"><a href="#2-4-Transformer" class="headerlink" title="2.4 Transformer"></a>2.4 Transformer</h5><p>Transformer is the first transduction model relying</p>
<p>entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.</p>
<p>本文主要和以下三篇文章对比：</p>
<ul>
<li><p><a href>Neural GPUs learn algorithms</a></p>
</li>
<li><p><a href>Neural machine translation in linear time</a></p>
</li>
<li><p><a href>Convolutional sequence to sequence learning</a></p>
</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>在以往的encoder-decoder模型中：</p>
<blockquote>
<p>the encoder maps an input sequence of symbol representations $(x_1,…,x_n)$ to a sequence of continuous representations $z = (z_1,…,z_n)$. Given z, the decoder then generates an output sequence $(y_1,…,y_m)$ of symbols one element at a time. At each step the model is <strong>auto-regressive [10]</strong>, consuming the previously generated symbols as additional input when generating the next.</p>
</blockquote>
<p>以往的attention虽然也能解决long dependecy的问题，但是受制于RNN的原因，每一步的计算必须在上一时间步完成后进行。因此无法并行计算。</p>
<p>Transformer 也是由 encoder 和 decoder 组成。</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/01.png"></p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>其中 Encoder 由6个完全相同的layer堆叠（stack）而成。每一层layer由两个 sub-layer 组成，分别是 <strong>multi-head self-attention mechanism</strong> 和 <strong>point-wise fully connected feed-forward network.</strong> 每一个 sub-layer 应用一个残差连接（<strong>residual connection</strong>）,然后再连接一个 normalization 层。</p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>跟 encoder 非常类似，同样由6由6个完全相同的layer堆叠而成，但是每一层有3个 sub-layer， 增加了一个 multi-head attention. 同样的也有残差链接和normalization层。</p>
<p>对 self-attention 进行了修改，masking:</p>
<blockquote>
<p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p>
</blockquote>
<p>目的应该就是让下一个t时刻的生成词只依赖于t时刻之前的词。</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>Really love this short description of attention:</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
</blockquote>
<h5 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h5><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/02.png"></p>
<ul>
<li><p>queries: $Q\in R^{n\times d_k}$</p>
</li>
<li><p>keys: $K\in R^{n\times d_k}$</p>
</li>
<li><p>values: $V\in R^{n\times d_v}$</p>
</li>
</ul>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/03.png"></p>
<p>计算向量內积作为相似度，并使用softmax计算权重，然后加权求和。 这种 attention 其实也很常见了，这里 google 算是给这种结构一个官方的名字吧。</p>
<p>论文中作者还对比了比较常用的另一种attention机制， additive attention (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> 这篇非常经典的文章中提出的)，additive 是使用的前馈神经网络来计算 (具体公式可以看这里<a href="http://www.panxiaoxie.cn/2018/05/08/cs224d-lecture10-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">cs224d-lecture10 机器翻译和注意力机制</a>). 虽然从计算复杂度上来讲，两者是差不多的，但在实际应用中 dot-product 更快，而且空间复杂度更低，因为可以通过矩阵优化计算。关于attention机制的对比可参考<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03906">Massive Exploration of Neural Machine Translation Architectures</a></p>
<p>有一点需要注意的是，这里使用了归一化，也就是 Scaled. 当 $d_k$ 很大时， additive attention 的效果要优于 dot-product attention， 作者怀疑(suspect)是当 $d_k$ 太大时，通过 softmax 计算得到的权重都会很接近0或1,导致梯度很小。</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/04.png"></p>
<p>$q\cdot k=\sum_{i=1}^{d_k}q_ik_i$</p>
<p>当 $d_k$ 很大时，$q\cdot k$ 的方差也会很大。</p>
<h5 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/11.png"></p>
<p>接下来按照整个模型的数据流过程来介绍模型中的每一个模块。</p>
<h3 id="Components-and-Training"><a href="#Components-and-Training" class="headerlink" title="Components and Training"></a>Components and Training</h3><p>前面三部分 Encoder, Decoder, Attention 组成了 Transformer 模型的基本架构。其中具体细节，以及 Training 实现过程将通过代码实现。</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/10.png"></p>
<h3 id="Encoder-1"><a href="#Encoder-1" class="headerlink" title="Encoder"></a>Encoder</h3><h4 id="Stage1"><a href="#Stage1" class="headerlink" title="Stage1"></a>Stage1</h4><h5 id="Training-data-and-batching"><a href="#Training-data-and-batching" class="headerlink" title="Training data and batching"></a>Training data and batching</h5><p>WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.</p>
<p>作者使用了：</p>
<ul>
<li>byte-pair <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03906">这篇论文中有介绍：Massive exploration of neural machine translation architectures</a></li>
</ul>
<ul>
<li>word-piece <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.08144">Google’s neural machine translation system: Bridging the gap between human and machine translation</a></li>
</ul>
<p>这里我将使用创新工厂举办的 challenge.ai 的中英文比赛数据：<a target="_blank" rel="noopener" href="https://challenger.ai/datasets/translation">https://challenger.ai/datasets/translation</a></p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/06.png"></p>
<p>这里暂时先不管数据预处理，在模型中使用占位符 placeholder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># add placeholders</span></span><br><span class="line"></span><br><span class="line">self.input_x = tf.placeholder(dtype=tf.int32, shape=[<span class="literal">None</span>, self.sentence_len])</span><br><span class="line"></span><br><span class="line">self.input_y = tf.placeholder(dtype=tf.int32, shape=[<span class="literal">None</span>, self.sentence_len])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define decoder inputs</span></span><br><span class="line"></span><br><span class="line">self.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:<span class="number">1</span>])*<span class="number">2</span>, self.input_y[:,:-<span class="number">1</span>]],axis=-<span class="number">1</span>) <span class="comment"># 2:&lt;S&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>这里的sentence_len 指的是源语言句子的最大长度和目标语言句子的最大长度。长度不足的需要zero padding.</li>
</ul>
<ul>
<li>decoder 中self-attention的 query, keys, values 都是相同的，初始值是随机初始化的，shape 与 self.input_y 一致即可。</li>
</ul>
<h5 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h5><p>we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. In our model, we share the same weight matrix between the two embedding layers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              vocab_size,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              zero_pad=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              scale=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs:  A `Tensor` with type `int32` or `int64` containing the ids</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         to be looked up in `lookup table`. shape is [batch, sentence_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param vocab_size: vocabulary size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num_units: Number of embedding hidden units. in the paper, it is called d_model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param zero_pad: If True, all the values of the fist row (id 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        should be constant zeros.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param scale: If True. the outputs is multiplied by sqrt num_units.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param reuse: Boolean, whether to reuse the weights of a previous layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        by the same name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        A `Tensor` with one more rank than inputs&#x27;s. The last dimensionality</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        should be `num_units`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embedding-layer&quot;</span>, reuse=reuse):</span><br><span class="line"></span><br><span class="line">        embedding = tf.get_variable(<span class="string">&quot;embedding&quot;</span>, [vocab_size, num_units],</span><br><span class="line"></span><br><span class="line">                                    initializer=xavier_initializer())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> zero_pad:</span><br><span class="line"></span><br><span class="line">            embedding = tf.concat([tf.zeros([<span class="number">1</span>, num_units]),</span><br><span class="line"></span><br><span class="line">                                  embedding[<span class="number">1</span>:, :]], axis=<span class="number">0</span>)  <span class="comment"># index=0 for nil word</span></span><br><span class="line"></span><br><span class="line">        output = tf.nn.embedding_lookup(embedding, inputs)    <span class="comment"># [batch, sentence_len, num_units]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">            output = output * np.sqrt(num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>通常embedding我们在写的参数输入 vocab_size 和 num_units(也就是 embed_size)，但机器翻译中设计到两种语言，直接定义一个函数，并将input作为输入会让程序更简洁吧。。</li>
</ul>
<ul>
<li>这里将vocabulary 中index=0的设置为 constant 0, 也就是作为 input 中的 zero padding 的词向量。</li>
</ul>
<ul>
<li>归一化，除以 np.sqrt(num_units). 不懂为何要这么做？有论文研究过吗？</li>
</ul>
<h5 id="position-encoding"><a href="#position-encoding" class="headerlink" title="position encoding"></a>position encoding</h5><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/07.png"></p>
<p>pos　是word在句子中的位置， i 是对应 $d_{model}$ 词向量中的第 i 维。</p>
<p>That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π.</p>
<p>也就是说，位置编码的每个维度对应于正弦曲线。波长形成从2π到10000·2π的几何级数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_encoding_mine</span>(<span class="params">n_position, d_model</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Init the sinusoid position encoding table.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param n_position: the lenght of sentence</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param d_model: the same with embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep dim -1 for padding token position encoding zero vector</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># pos=-1 用于 padded zero vector</span></span><br><span class="line"></span><br><span class="line">    encoding = np.zeros([n_position, d_model], np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_position):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, d_model):</span><br><span class="line"></span><br><span class="line">            encoding[pos, i] = pos /np.power(<span class="number">10000</span>, <span class="number">2.</span>*i/d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>] = np.sin(encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># dim 2i</span></span><br><span class="line"></span><br><span class="line">    encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>] = np.cos(encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>]) <span class="comment"># dim 2i+1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        zero_pad=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        scale=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        scope=<span class="string">&quot;positional_encoding&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Sinusoidal Positional_Encoding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      inputs: A 2d Tensor with shape of (N, T).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: Output dimensionality</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      reuse: Boolean, whether to reuse the weights of a previous layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        by the same name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        A &#x27;Tensor&#x27; with one more rank than inputs&#x27;s, with the dimensionality should be &#x27;num_units&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    N, T = inputs.get_shape().as_list()  <span class="comment"># N means batch_size, T means the sentence length.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=reuse):</span><br><span class="line"></span><br><span class="line">        position_ind = tf.tile(tf.expand_dims(tf.<span class="built_in">range</span>(T), <span class="number">0</span>), [N, <span class="number">1</span>])  <span class="comment"># [N, T]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First part of the PE function: sin and cos argument</span></span><br><span class="line"></span><br><span class="line">        position_enc = np.array([</span><br><span class="line"></span><br><span class="line">            [pos / np.power(<span class="number">10000</span>, <span class="number">2.</span>*i/num_units) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_units)]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(T)])                                       <span class="comment"># [T, num_units]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Second part, apply the cosine to even columns and sin to odds.</span></span><br><span class="line"></span><br><span class="line">        position_enc[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position_enc[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line"></span><br><span class="line">        position_enc[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position_enc[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert to a tensor</span></span><br><span class="line"></span><br><span class="line">        lookup_table = tf.convert_to_tensor(position_enc, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> zero_pad:</span><br><span class="line"></span><br><span class="line">            lookup_table = tf.concat((tf.zeros(shape=[<span class="number">1</span>, num_units]),</span><br><span class="line"></span><br><span class="line">                                      lookup_table[<span class="number">1</span>:, :]), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)  <span class="comment"># [N, T, num_units]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">            outputs = outputs * num_units**<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>关于 position encoding 的解释，可以参考这篇blog</p>
<p><a target="_blank" rel="noopener" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#sf-Transformer-Attention-is-all-you-need-3">The Transformer – Attention is all you need.</a></p>
<p>In RNN (LSTM), the notion of time step is encoded in the sequence as inputs/outputs flow one at a time. In FNN, the positional encoding must be preserved to represent the time in some way to preserve the positional encoding. <strong>In case of the Transformer authors propose to encode time as sine wave, as an added extra input. Such signal is added to inputs and outputs to represent time passing.</strong></p>
<p>In general, adding positional encodings to the input embeddings is a quite interesting topic. One way is to embed the absolute position of input elements (as in ConvS2S). However, authors use “sine and cosine functions of different frequencies”. The “sinusoidal” version is quite complicated, while giving similar performance to the absolute position version. <strong>The crux is however, that it may allow the model to produce better translation on longer sentences at test time (at least longer than the sentences in the training data). This way sinusoidal method allows the model to extrapolate to longer sequence lengths.</strong></p>
<p>说真的，还是不太理解。。。</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/46452020/sinusoidal-embedding-attention-is-all-you-need">可视化 encoding 矩阵</a>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_units = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">sentence_len = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">i = np.tile(np.expand_dims(<span class="built_in">range</span>(num_units), <span class="number">0</span>), [sentence_len, <span class="number">1</span>]) <span class="comment"># (100,)-&gt; (1, 100) -&gt;(10, 100)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pos = np.tile(np.expand_dims(<span class="built_in">range</span>(sentence_len), <span class="number">1</span>), [<span class="number">1</span>, num_units]) <span class="comment">#(10,)-&gt; (10, 1) -&gt; (10, 100)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pos = np.multiply(pos, <span class="number">1</span>/<span class="number">10000.0</span>)</span><br><span class="line"></span><br><span class="line">i = np.multiply(i, <span class="number">2.0</span>/num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">matrix = np.power(pos, i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">matrix[:, <span class="number">1</span>::<span class="number">2</span>] = np.sin(matrix[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">matrix[:, ::<span class="number">2</span>] = np.cos(matrix[:, ::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">im = plt.imshow(matrix, aspect=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/09.png"></p>
<h4 id="Stage2"><a href="#Stage2" class="headerlink" title="Stage2"></a>Stage2</h4><h5 id="scaled-dot-product-attention"><a href="#scaled-dot-product-attention" class="headerlink" title="scaled dot-product attention"></a>scaled dot-product attention</h5><p>$$Attention(Q,K,V)=softmax\dfrac{QK^T}{\sqrt d_k}V$$</p>
<h5 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h5><p>Transformer reduces the number of operations required to relate (especially distant) positions in input and output sequence to a O(1). However, this comes at cost of reduced effective resolution because of averaging attention-weighted positions.</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/11.png"></p>
<ul>
<li>h = 8 attention layers (aka “heads”): that represent linear projection (for the purpose of dimension reduction) of key K and query Q into $d_k$-dimension and value V into $d_v$-dimension:</li>
</ul>
<p>$$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\dots,h$$</p>
<p>其中：</p>
<p>$$W^Q_i, W^K_i\in\mathbb{R}^{d_{model}\times d_k}, W^V_i\in\mathbb{R}^{d_{model}\times d_v}, for\ d_k=d_v=d_{model}/h = 64$$</p>
<ul>
<li>scaled-dot attention applied in parallel on each layer (different linear projections of k,q,v) results in $d_v$-dimensional output.</li>
</ul>
<ul>
<li>concatenate outputs of each layer (different linear projection; also referred as ”head”): Concat$(head_1,…,head_h)$</li>
</ul>
<ul>
<li>linearly project the concatenation result form the previous step:</li>
</ul>
<p>$$MultiHeadAttention(Q,K,V) = Concat(head_1,\dots,head_h) W^O$$</p>
<p>where $W^0\in\mathbb{R}^{d_{hd_v}\times d_{model}}$</p>
<h5 id="关于-attention-在模型中的应用，有三种情况"><a href="#关于-attention-在模型中的应用，有三种情况" class="headerlink" title="关于 attention 在模型中的应用，有三种情况"></a>关于 attention 在模型中的应用，有三种情况</h5><ul>
<li>1.In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder。</li>
</ul>
<ul>
<li>2.The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li>
</ul>
<ul>
<li>3.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this</li>
</ul>
<p>inside of scaled dot-product attention by masking out (setting to −1) all values in the input of the softmax which correspond to illegal connections.</p>
<p>总结下就是：</p>
<p>Transformer 中的attention机制总共有三种情况：</p>
<ul>
<li>1.encoder模块中的 self-attention，其中 queries, keys, values 都是来自 input_x, 也就是源语言的词表示。通过多层 multi-head attention, FFN, 得到最后的 input sentence 的向量表示，在没有使用RNN，CNN的情况下，其中的每个词都包含了其他所有词的信息，而且效果比 RNN，CNN 得到的向量表示要好。</li>
</ul>
<ul>
<li>2.encoder-encoder模块中的 attention. 其中 queries 来自上一个sub-layer, 也就是 decoder 中 masked multi-head attention 的输出，keys-values 来自 encoder 的输出。</li>
</ul>
<ul>
<li>3.decoder模块中的 self-attention，其中 queries, keys, values 都是来自于上一个 decoder 的输出。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiheadattention</span>(<span class="params">q,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       k,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       v,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       d_model,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       heads,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       keys_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       causality=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       dropout_keep_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       is_training=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; multi scaled dot product attention</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param q: A 3d tensor with shape of [batch, length_q, d_k].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param k: A 3d tensor with shape of [batch, lenght_kv, d_k].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param v:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param heads:An int. Number of heads.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param dropout_keep_prob:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param causality: If true, units that reference the future are masked.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Linear projections</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;linear-projection-multiheads&#x27;</span>):</span><br><span class="line"></span><br><span class="line">        q_proj = tf.layers.dense(q, d_model) <span class="comment"># [batch, lenght_q, d_model]</span></span><br><span class="line"></span><br><span class="line">        k_proj = tf.layers.dense(k, d_model) <span class="comment"># [batch, lenght_kv, d_model]</span></span><br><span class="line"></span><br><span class="line">        v_proj = tf.layers.dense(v, d_model) <span class="comment"># [batch, lenght_kv, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;multihead-attention&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># d_k = d_v = d_model/heads</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> d_model % heads != <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Key\values\query depth (%d) must be divisible by&quot;</span></span><br><span class="line"></span><br><span class="line">                             <span class="string">&quot;the number of attention heads (%d)&quot;</span> %(d_model, heads))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. split and concat</span></span><br><span class="line"></span><br><span class="line">        q_ = tf.concat(tf.split(q_proj, heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>)  <span class="comment"># [batch*heads, length_q, d_k]</span></span><br><span class="line"></span><br><span class="line">        k_ = tf.concat(tf.split(k_proj, heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>)  <span class="comment"># [batch*heads, length_kv, d_k]</span></span><br><span class="line"></span><br><span class="line">        v_ = tf.concat(tf.split(v_proj, heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>)  <span class="comment"># [batch*heads, length_kv, d_v]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. attention score</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># outputs.shape=[batch*heads, length_q, length_kv]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 要理解这个矩阵运算，对一个keys的句子长度为length_kv,需要计算的其中的每一个词与query中每一个词的內积。所以最后的score是[length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line">        scalar = tf.rsqrt(d_model/heads)  <span class="comment"># 1/sqrt(d_k)</span></span><br><span class="line"></span><br><span class="line">        outputs = tf.matmul(q_*scalar, k_, transpose_b=<span class="literal">True</span>)   <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. mask</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> keys_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># `y = sign(x) = -1` if `x &lt; 0`; 0 if `x == 0` or `tf.is_nan(x)`; 1 if `x &gt; 0`.</span></span><br><span class="line"></span><br><span class="line">            key_masks = tf.sign(tf.<span class="built_in">abs</span>(tf.reduce_sum(k, axis=-<span class="number">1</span>)))  <span class="comment"># (batch, length_kv)</span></span><br><span class="line"></span><br><span class="line">            key_masks = tf.tile(key_masks, [heads, <span class="number">1</span>])              <span class="comment"># (batch*heads, length_kv)</span></span><br><span class="line"></span><br><span class="line">            key_masks = tf.tile(tf.expand_dims(key_masks, <span class="number">1</span>), [<span class="number">1</span>, q.get_shape()[<span class="number">1</span>], <span class="number">1</span>])  <span class="comment"># (batch*heads, length_q, length_kv)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># def where(condition, x=None, y=None, name=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The `condition` tensor acts as a mask that chooses, based on the value at each</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># element, whether the corresponding element / row in the output should be taken</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># from `x` (if true) or `y` (if false).</span></span><br><span class="line"></span><br><span class="line">            paddings = tf.ones_like(outputs) * (-<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            outputs = tf.where(tf.equal(key_masks, <span class="number">0</span>), paddings, outputs)  <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Causality = Future blinding</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># causality参数告知我们是否屏蔽未来序列的信息（解码器self attention的时候不能看到自己之后的那些信息），</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 这里即causality为True时的屏蔽操作。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> causality:</span><br><span class="line"></span><br><span class="line">            diag_vals = tf.ones_like(outputs[<span class="number">0</span>, :, :])  <span class="comment"># [length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line">            tril = LinearOperatorLowerTriangular(diag_vals).to_dense()  <span class="comment"># [length_q, lenght_kv] 得到一个三角阵，下标index大于当前行的值都变为0</span></span><br><span class="line"></span><br><span class="line">            masks = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), [tf.shape(outputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            paddings = tf.ones_like(masks) * (-<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            outputs = tf.where(tf.equal(masks, <span class="number">0</span>), paddings, outputs)  <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将socre转换为概率</span></span><br><span class="line"></span><br><span class="line">        outpts = tf.nn.softmax(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Query Masking</span></span><br><span class="line"></span><br><span class="line">        query_mask = tf.sign(tf.<span class="built_in">abs</span>(tf.reduce_sum(q, axis=-<span class="number">1</span>, keepdims=<span class="literal">False</span>))) <span class="comment"># [batch, lenght_q]</span></span><br><span class="line"></span><br><span class="line">        query_mask = tf.tile(query_mask, [heads, <span class="number">1</span>])  <span class="comment"># [batch*heads, length_q] # 目的是为了让query和outputs保持形状一致</span></span><br><span class="line"></span><br><span class="line">        query_mask = tf.tile(tf.expand_dims(query_mask, axis=-<span class="number">1</span>), [<span class="number">1</span>, <span class="number">1</span>, tf.shape(k)[-<span class="number">1</span>]]) <span class="comment"># [batch*heads, length_q, length_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        paddings = tf.ones_like(outputs) * (-<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        outputs = tf.where(tf.equal(query_mask, <span class="number">0</span>), paddings, outputs) <span class="comment"># [batch*heads, length_q, length_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line"></span><br><span class="line">            outputs = tf.layers.dropout(outputs, dropout_keep_prob, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># weights sum</span></span><br><span class="line"></span><br><span class="line">        outputs = tf.matmul(outputs, v_)  <span class="comment"># [batch*heads, length_q, k_v]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># restore shape</span></span><br><span class="line"></span><br><span class="line">        outputs = tf.concat(tf.split(outputs, heads, axis=<span class="number">0</span>), axis=-<span class="number">1</span>) <span class="comment">#[batch,length_q, k_v*heads] = [batch, lenght_q, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line"></span><br><span class="line">        outputs += q    <span class="comment"># [batch, lenght_q, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line"></span><br><span class="line">        outputs = Normalize(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs   <span class="comment"># [batch, length_q, d_model]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>关于代码的详细解析，可以看这篇blog <a target="_blank" rel="noopener" href="http://lib.csdn.net/article/aiframework/68187">机器翻译模型Transformer代码详细解析</a>.</p>
<h4 id="Stage3-Position-wise-Feed-Forward-Networks"><a href="#Stage3-Position-wise-Feed-Forward-Networks" class="headerlink" title="Stage3: Position-wise Feed-Forward Networks"></a>Stage3: Position-wise Feed-Forward Networks</h4><p>$$FFN(x) = MAX(0, xW_1+b_1)W_2+b_2$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_wise_feed_forward</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units1=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units2=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Point-wise feed forward net.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs: A 3D tensor with shape of [batch, length_q, d_model]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num_units1: A integers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num_units2: A integers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param reuse: Boolean, whether to reuse the weights of a previous layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        by the same name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: A 3d tensor with the same shape and dtype as inputs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;feed-forward-networks&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># inner layers</span></span><br><span class="line"></span><br><span class="line">        params1 = &#123;<span class="string">&quot;inputs&quot;</span>:inputs, <span class="string">&quot;filters&quot;</span>:num_units1, <span class="string">&quot;kernel_size&quot;</span>:<span class="number">1</span>,</span><br><span class="line"></span><br><span class="line">                  <span class="string">&quot;activation&quot;</span>:tf.nn.relu, <span class="string">&quot;use_bias&quot;</span>:<span class="literal">True</span>, <span class="string">&quot;strides&quot;</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">        outputs = tf.layers.conv1d(**params1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># readout layer</span></span><br><span class="line"></span><br><span class="line">        params2 = &#123;<span class="string">&quot;inputs&quot;</span>:outputs, <span class="string">&quot;filters&quot;</span>:num_units2, <span class="string">&quot;kernel_size&quot;</span>:<span class="number">1</span>,</span><br><span class="line"></span><br><span class="line">                  <span class="string">&quot;activation&quot;</span>:<span class="literal">None</span>, <span class="string">&quot;use_bias&quot;</span>:<span class="literal">True</span>, <span class="string">&quot;strides&quot;</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">        outputs = tf.layers.conv1d(**params2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># residual connection</span></span><br><span class="line"></span><br><span class="line">        outputs += inputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line"></span><br><span class="line">        outputs = Normalize(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_wise_feed_forward_mine</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units1=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units2=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;feed-forward-networks&quot;</span>):</span><br><span class="line"></span><br><span class="line">        W1 = tf.get_variable(<span class="string">&quot;weight1&quot;</span>, [inputs.get_shape()[-<span class="number">1</span>], num_units1],initializer=xavier_initializer())</span><br><span class="line"></span><br><span class="line">        b1 = tf.get_variable(<span class="string">&#x27;bias1&#x27;</span>, [num_units1], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">        outputs = tf.einsum(<span class="string">&#x27;aij,jk-&gt;aik&#x27;</span>, inputs, W1) + b1  <span class="comment"># [batch, length_q, num_units1]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        W2 = tf.get_variable(<span class="string">&quot;weight1&quot;</span>, [outputs.get_shape()[-<span class="number">1</span>], num_units2], initializer=xavier_initializer())</span><br><span class="line"></span><br><span class="line">        b2 = tf.get_variable(<span class="string">&#x27;bias1&#x27;</span>, [num_units2], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">        outputs = tf.einsum(<span class="string">&#x27;aij,jk-&gt;aik&#x27;</span>, inputs, W2) + b2  <span class="comment"># [batch, length_q, num_units1]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># residual connection</span></span><br><span class="line"></span><br><span class="line">        outputs += inputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line"></span><br><span class="line">        outputs = Normalize(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="encoder-各模块组合在一起"><a href="#encoder-各模块组合在一起" class="headerlink" title="encoder 各模块组合在一起"></a>encoder 各模块组合在一起</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_encoder</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. embedding</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embedding-layer&quot;</span>):</span><br><span class="line"></span><br><span class="line">            self.enc = embedding(inputs=self.input_x,</span><br><span class="line"></span><br><span class="line">                                   vocab_size=self.vocab_size_cn,</span><br><span class="line"></span><br><span class="line">                                   num_units=self.d_model,</span><br><span class="line"></span><br><span class="line">                                   scale=<span class="literal">True</span>)   <span class="comment"># [batch, sentence_len, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. position encoding</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;position_encoding&quot;</span>):</span><br><span class="line"></span><br><span class="line">            encoding = position_encoding_mine(self.enc.get_shape()[<span class="number">1</span>], self.d_model)</span><br><span class="line"></span><br><span class="line">            self.enc *= encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3.dropout</span></span><br><span class="line"></span><br><span class="line">        self.enc = tf.layers.dropout(self.enc,</span><br><span class="line"></span><br><span class="line">                                     rate=self.dropout_keep_prob,</span><br><span class="line"></span><br><span class="line">                                     training=self.is_training)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. Blocks</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;num_layer_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i)):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># multihead attention</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># encoder: self-attention</span></span><br><span class="line"></span><br><span class="line">                self.enc = multiheadattention(q=self.enc,</span><br><span class="line"></span><br><span class="line">                                              k=self.enc,</span><br><span class="line"></span><br><span class="line">                                              v=self.enc,</span><br><span class="line"></span><br><span class="line">                                              d_model=self.d_model,</span><br><span class="line"></span><br><span class="line">                                              heads=self.heads,</span><br><span class="line"></span><br><span class="line">                                              causality=<span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">                                              dropout_keep_prob=self.dropout_keep_prob,</span><br><span class="line"></span><br><span class="line">                                              is_training=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Feed Froward</span></span><br><span class="line"></span><br><span class="line">                self.enc = position_wise_feed_forward(self.enc,</span><br><span class="line"></span><br><span class="line">                                                      num_units1= <span class="number">4</span>*self.d_model,</span><br><span class="line"></span><br><span class="line">                                                      num_units2= self.d_model,</span><br><span class="line"></span><br><span class="line">                                                      reuse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.enc</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder 模块中 self-attention 的初始输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># define decoder inputs</span></span><br><span class="line"></span><br><span class="line">self.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:<span class="number">1</span>])*<span class="number">2</span>, self.input_y[:,:-<span class="number">1</span>]],axis=-<span class="number">1</span>) <span class="comment"># 2:&lt;S&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>与encoder 不同的是，分为 encoder-decoder attention 和 self-attention.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_decoder</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;decoder&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding</span></span><br><span class="line"></span><br><span class="line">        self.dec = embedding(self.decoder_inputs,</span><br><span class="line"></span><br><span class="line">                             vocab_size=self.vocab_size_en,</span><br><span class="line"></span><br><span class="line">                             num_units=self.d_model)   <span class="comment"># [batch, sentence_len, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># position decoding</span></span><br><span class="line"></span><br><span class="line">        encoding = position_encoding_mine(self.dec.get_shape()[<span class="number">1</span>], self.d_model)</span><br><span class="line"></span><br><span class="line">        self.dec *= encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># blocks</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;num_layers_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i)):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># self-attention</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;self.attention&quot;</span>):</span><br><span class="line"></span><br><span class="line">                    self.dec = multiheadattention(q=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  k=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  v=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  d_model=self.d_model,</span><br><span class="line"></span><br><span class="line">                                                  heads=self.heads,</span><br><span class="line"></span><br><span class="line">                                                  keys_mask=<span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">                                                  causality=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment"># encoder-decoder-attention</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder-decoder-attention&quot;</span>):</span><br><span class="line"></span><br><span class="line">                    self.dec = multiheadattention(q=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  k=self.enc,</span><br><span class="line"></span><br><span class="line">                                                  v=self.enc,</span><br><span class="line"></span><br><span class="line">                                                  d_model=self.d_model,</span><br><span class="line"></span><br><span class="line">                                                  heads=self.heads,</span><br><span class="line"></span><br><span class="line">                                                  keys_mask=<span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">                                                  causality=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                self.dec = position_wise_feed_forward(self.dec,</span><br><span class="line"></span><br><span class="line">                                                      num_units1= <span class="number">4</span>*self.d_model,</span><br><span class="line"></span><br><span class="line">                                                      num_units2= self.d_model)   <span class="comment"># [batch, sentence_len, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.dec</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/12.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_train_op</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    self.optimizer = tf.train.AdamOptimizer(self.lr, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.98</span>, epsilon=<span class="number">1e-9</span>)</span><br><span class="line"></span><br><span class="line">    self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.train_op</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><h5 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h5><h5 id="label-smoothing"><a href="#label-smoothing" class="headerlink" title="label smoothing"></a>label smoothing</h5><p>During training, we employed label smoothing of value ls = 0:1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label_smoothing</span>(<span class="params">inputs, epsilon=<span class="number">0.1</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Applies label smoothing. See https://arxiv.org/abs/1512.00567</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param epsilon: Smoothing rate.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        For example,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    K = inputs.get_shape().as_list()[-<span class="number">1</span>]  <span class="comment"># number of channels</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ((<span class="number">1</span>-epsilon) * inputs) + (epsilon/K)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>Reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces">Attention and Augmented Recurrent Neural Networks</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#sf-Transformer-Attention-is-all-you-need-3">The Transformer – Attention is all you need.</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="http://lib.csdn.net/article/aiframework/68187">机器翻译模型Transformer代码详细解析</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-02T12:34:48.000Z" title="2018/6/2 下午8:34:48">2018-06-02</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">8 分钟读完 (大约1218个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/">深度学习-权重初始化</a></h1><div class="content"><ul>
<li><p>为什么要权重初始化</p>
</li>
<li><p>Xavier初始化的推导</p>
</li>
</ul>
<h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p>In order to avoid neurons becoming too correlated and ending up in poor local minimize, it is often helpful to randomly initialize parameters. 为了避免神经元高度相关和局部最优化，常常需要采用随机初始化权重参数，最常用的就是Xavier initiazation.</p>
<h4 id="为什么我们需要权重初始化？"><a href="#为什么我们需要权重初始化？" class="headerlink" title="为什么我们需要权重初始化？"></a>为什么我们需要权重初始化？</h4><p>如果权重参数很小的话，输入信号在前向传播过程中会不断减小（在0到1之间），那么每一层layer都会使得输入变小。同样的道理，如果权重参数过大的话，也会造成前向输入越来越大。这样会带来什么样的后果呢？以激活函数sogmoid为例：</p>
<p><img src="/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/2-sigmoid.png"></p>
<p>如果以sigmoid为激活函数，我们可以发现，在每一层layer输出 $W^Tx$ ，也就是激活函数的输入，其值越接近于0的时候，函数近似于线性的，因而就失去了非线性的性质。这种情况下，我们就失去了多层神经网络的优势了。</p>
<p><strong>如果初始权重过大，在前向传播的过程中，输入数据的方差variance会增长很快。怎么理解这句话？</strong></p>
<p>以one layer为例，假设输入是 $x\in R^{1000}$, 线性输出是 $y\in R^{100}$.</p>
<p>$$y_j=w_{j,1}x_1+w_{j,2}x_2+…+w_{(j,1000)}x_{1000}$$</p>
<p>x可以看作是1000维的正态分布，每一维 $x_i\sim N(0,1)$, 如果 $w_j$值很大，比如 $w_j=[100,100,…,100]$，那么输出神经元 $y_i$ 的方差就是10000，所以就会很大,均值还是0.</p>
<p><img src="/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/01.png"></p>
<p>那么激活函数的输入很有可能是一个远小于-1或远大于1的数，通过激活函数所得的值会非常接近于0或者1，也就是隐藏层神经元处于饱和状态(saturated)，其梯度也就接近于0了。</p>
<p>所以初始化权重狠狠狠重要。那么应该如何初始化呢，也就是需要保证经过每一层layer，要保证线性输出的方差保持不变。这样就可以避免数值溢出，或是梯度消失。</p>
<h4 id="Xavier-Initialization"><a href="#Xavier-Initialization" class="headerlink" title="Xavier Initialization"></a>Xavier Initialization</h4><p>我们的目的是保持线性输出的方差不变。</p>
<p>以线性输出的一个神经元为例，也就是y的一个维度：</p>
<p>$$y_j=w_{j,1}x_1+w_{j,2}x_2+…+w_{j,N} x_N+b$$</p>
<p>其方差：</p>
<p>$$var(y_j) = var(w_{j,1}x_1+w_{j,2}x_2+…+w_{j,N} x_N+b)$$</p>
<p>其中每一项根据方差公式可得：</p>
<p>$$var(w_{j,i}x_i) = E(x_i)^2var(w_{j,i}) + E(w_{j,i})^2var(xi) + var(w_{j,i})var(x_i)$$</p>
<p><img src="/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/02.png"></p>
<p>来自维基百科： <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variance">https://en.wikipedia.org/wiki/Variance</a></p>
<p>其中我们假设输入和权重都是来自于均值为0的正态分布。</p>
<p>$$var(w_{j,i}x_i)=var(w_{j,i})var(x_i)$$</p>
<p>其中b是常量，那么：</p>
<p>$$var(y_j) = var(w_{j,1})var(x_1) + … + var(w_{j,N})var(x_N)$$</p>
<p>因为 $x_1,x_2,..,x_N$ 都是相同的分布，$W_{j,i}$ 也是，那么就有：</p>
<p>$$var(y_j) = N * var(w{j,i}) * var(x_i)$$</p>
<p>可以看到，如果输入神经元数目N很大，参数权重W的值也很大的话，会造成线性输出的值的方差很大。</p>
<p>我们需要保证 $y_j$ 的方差和 $x_j$ 的方差一样，所以：</p>
<p>$$N*var(W_{j,i})=1$$</p>
<p>$$var(W_{j,i})=1/N$$</p>
<p>There we go! 这样我们就得到了Xavier initialization的初始化公式，也就是说参数权重初始化为均值为0，方差为 1/N 的高斯分布，其中N表示当前层输入神经元的个数。在caffe中就是这样实现的。</p>
<h4 id="更多初始化方式"><a href="#更多初始化方式" class="headerlink" title="更多初始化方式"></a>更多初始化方式</h4><p><a target="_blank" rel="noopener" href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a> 在这篇paper中提出</p>
<p>$$var(w)=2/(N_{in}+N_{out})$$</p>
<p><a target="_blank" rel="noopener" href="http://arxiv-web3.library.cornell.edu/abs/1502.01852"> Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a> 针对一种专门的初始化方式，使得 $var(w)=2.0/N$, 在实际工程中通常使用这种方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 正态分布</span></span><br><span class="line"></span><br><span class="line"> w = np.random.randn(N) * sqrt(<span class="number">2.0</span>/N)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 均匀分布</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_xavier_initializer</span>(<span class="params">shape, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    shape: Tuple or 1-d array that species dimensions of requested tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    out: tf.Tensor of specified shape sampled from Xavier distribution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  epsilon = np.sqrt(<span class="number">6</span>/np.<span class="built_in">sum</span>(shape))</span><br><span class="line"></span><br><span class="line">  out = tf.Variable(tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>均匀分布[a,b]的方差：$\dfrac{(b-a)^2}{12}$</p>
<p>参考资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://219.238.82.130/cache/10/03/proceedings.mlr.press/c896b216aca8427f10edb48249b207d1/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="http://219.238.82.130/cache/10/03/proceedings.mlr.press/c896b216aca8427f10edb48249b207d1/glorot10a.pdf">cs231n：Weight Initialization</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/">understanding-xavier-initialization-in-deep-neural-networks</a></li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/15/">上一页</a></div><div class="pagination-next"><a href="/page/17/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/15/">15</a></li><li><a class="pagination-link is-current" href="/page/16/">16</a></li><li><a class="pagination-link" href="/page/17/">17</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/23/">23</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="潘晓榭"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">潘晓榭</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">112</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-02T04:37:58.000Z">2021-07-02</time></p><p class="title"><a href="/2021/07/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-constrast-learning-in-NLP/">论文笔记-constrast learning in NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-05T02:03:16.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-image-based contrastive learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:07:08.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:05:10.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-16T07:48:48.000Z">2021-04-16</time></p><p class="title"><a href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">七月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">四月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">九月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">六月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">六月 2018</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">四月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESA/"><span class="tag">ESA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN%EF%BC%8CRL/"><span class="tag">GAN，RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MRC-and-QA/"><span class="tag">MRC and QA</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Translation/"><span class="tag">Machine Translation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/capsules/"><span class="tag">capsules</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-augmentation/"><span class="tag">data augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/open-set-recognition/"><span class="tag">open set recognition</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentence-embedding/"><span class="tag">sentence embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentiment-classification/"><span class="tag">sentiment classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language-recognition/"><span class="tag">sign language recognition</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-matching/"><span class="tag">text matching</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/transfer-learning/"><span class="tag">transfer learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="tag">数据结构与算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag">8</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>