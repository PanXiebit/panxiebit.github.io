<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-07T01:28:38.000Z" title="2018/8/7 上午9:28:38">2018-08-07</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">19 分钟读完 (大约2853个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/">机器学习-常用指标总结</a></h1><div class="content"><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/maybe2030/p/5375175.html#_label0">http://www.cnblogs.com/maybe2030/p/5375175.html#_label0</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/">http://alexkong.net/2013/06/introduction-to-auc-and-roc/</a></p>
</li>
</ul>
<h2 id="精确率-Precision-召回率-Recall-和-F1-值"><a href="#精确率-Precision-召回率-Recall-和-F1-值" class="headerlink" title="精确率 Precision, 召回率 Recall 和 F1 值"></a>精确率 Precision, 召回率 Recall 和 F1 值</h2><p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb10.png"></p>
<p>对于数据不均衡时，使用accuracy是不准确的。</p>
<p>举个栗子：</p>
<p>a million tweet: 999,900条不是关于pie的，只有100条是关于pie的</p>
<p>对于一个stupid分类器，他认为所有的tweet都是跟pie无关的，那么它的准确率是99.99%！但这个分类器显然不是我们想要的，因而accuracy不是一个好的metric，当它目标是rare，或是complete unbalanced.</p>
<p>引入另外两个指标：</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb11.png"></p>
<ul>
<li><strong>精度 precision:</strong> 是检索出来的条目（比如：文档、网页等）有多少是准确的。精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率。  </li>
</ul>
<ul>
<li><strong>召回 call：</strong> 召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率.</li>
</ul>
<p><strong>需要先确定一个正分类：</strong> 这里需要检索出来的是与 pie 无关的，也就是与 pie 无关的是正分类。那么精度就是分类器检测出来的正分类中 gold pos 所占的比例，那么 precision = 0/(0+0)</p>
<p>这里需要检索的是与 pie 无关的，检索出来的 true pos 占样本中 gold pos 的比例，那么 recall = 0/(100+0) = 0.</p>
<p>总结下来，precision 和 recall 都是以 true pos 作为分子，precision 是以分类器预测出来的 pos(true pos + false neg) 作为分母，所以是差准率. recall 则是以总的 gold pos(true pos + false neg) 作为分母，所以是查全率。</p>
<p>当然希望检索结果Precision越高越好，同时Recall也越高越好，但事实上这两者在某些情况下有矛盾的。比如极端情况下，我们只搜索出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就会很低。因此在不同的场合中需要自己判断希望Precision比较高或是Recall比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。</p>
<p><strong>Ｆ-measure</strong>  </p>
<p>$$F_{\beta}=\dfrac{(\beta^2+1)PR}{\beta^2P+R}$$</p>
<p>当 $\beta&gt;1$时，Recall的比重更大;当 $\beta&lt;1$时，precision的比重更大。使用最多的是 $\beta=1$,也就是 $F_{\beta=1}, F_1$. $\beta$ 的的取值取决于实际应用。</p>
<p>$$F_1 = \dfrac{2PR}{P+R}$$</p>
<p>Ｆ-measure 是 precision 和 recall 的 **加权调和平均值(weighted harmonic mean)**。</p>
<p>调和平均值是倒数的算术平均值的倒数。</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb12.png"></p>
<p>为什么要使用调和平均值呢？因为它是一个更 <strong>保守的度量(conservative metric)</strong>. 相比直接计算 P 和 R 的平均值， F-measure的值更看重两者中的较小值。</p>
<h2 id="ROC曲线和AUC"><a href="#ROC曲线和AUC" class="headerlink" title="ROC曲线和AUC"></a>ROC曲线和AUC</h2><p>考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。</p>
<ul>
<li><p><strong>TPR, 真正类率(true positive rate ,TPR),：</strong> 如果一个实例是正类并且也被 预测成正类，即为真正类（True positive），真正类率是指分类器所识别出来的 正实例占所有正实例的比例。就是 Recall 吧～  TPR = TP / (TP + FN)  </p>
</li>
<li><p><strong>FPR, 负正类率：</strong> 分类器错认为正类的负实例占所有负实例的比例，FPR = FP / (FP + TN)  </p>
</li>
<li><p><strong>TNR， 真负类率：</strong> 分类器认为负类的负实例占所有负实例的比例，也就是负类的 Recall 吧～ TNR = TN /(FP + TN) = 1 - FPR</p>
</li>
</ul>
<h3 id="为什么要引入-ROC-曲线"><a href="#为什么要引入-ROC-曲线" class="headerlink" title="为什么要引入 ROC 曲线"></a>为什么要引入 ROC 曲线</h3><ul>
<li>Motivation1：在一个二分类模型中，对于所得到的连续结果，假设已确定一个阀值，比如说 0.6，大于这个值的实例划归为正类，小于这个值则划到负类中。如果减小阀值，减到0.5，固然能识别出更多的正类，也就是提高了识别出的正例占所有正例 的比类，即TPR,但同时也将更多的负实例当作了正实例，即提高了FPR。为了形象化这一变化，引入ROC，ROC曲线可以用于评价一个分类器。</li>
</ul>
<ul>
<li>Motivation2：在类不平衡的情况下,如正样本90个,负样本10个,直接把所有样本分类为正样本,得到识别率为90%。但这显然是没有意义的。单纯根据Precision和Recall来衡量算法的优劣已经不能表征这种病态问题。</li>
</ul>
<h3 id="ROC-曲线"><a href="#ROC-曲线" class="headerlink" title="ROC 曲线"></a>ROC 曲线</h3><p>前面已经说道，对于数据不均衡的情况下，precision 和 recall 不足以表征这类问题，就比如上面的例子中，把找出不关于 pie 的 tweet看作是正类，那么它的 精度和召回率 都很高。所以引入 ROC</p>
<p>因为我们要关注的是正类，所以关注指标是 真正类率 TPR 和 负正类率 FPR，真正类率越高越好，负正类率越低越好。但显然这两者之间是矛盾的，与分类器的阈值有关，ROC 就是用来表征阈值与 TPR 和 FPR 之间的关系曲线。</p>
<p>ROC（Receiver Operating Characteristic）翻译为“接受者操作曲线”。曲线由两个变量1-specificity 和 Sensitivity绘制. 1-specificity=FPR，即负正类率。Sensitivity即是真正类率，TPR(True positive rate),反映了正类覆盖程度。</p>
<blockquote>
<p>为了更好地理解ROC曲线，我们使用具体的实例来说明：  </p>
</blockquote>
<p>如在医学诊断中,判断有病的样本。那么尽量把有病的揪出来是主要任务,也就是第一个指标TPR,要越高越好。而把没病的样本误诊为有病的,也就是第二个指标FPR,要越低越好。  </p>
<p>不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感,稍微的小症状都判断为有病,那么他的第一个指标应该会很高,但是第二个指标也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么第一个指标达到1,第二个指标也为1。</p>
<p>我们以FPR为横轴,TPR为纵轴,得到如下ROC空间。  </p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb8.png"></p>
<p>参考链接中 ROC 曲线的解释: <a target="_blank" rel="noopener" href="http://www.cnblogs.com/maybe2030/p/5375175.html#_label0">http://www.cnblogs.com/maybe2030/p/5375175.html#_label0</a></p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb9.png"></p>
<p>上面的图表示，分类器对于 pos 和 neg 的分别是有个阈值的，阈值很高，也就是 A 处，显然它的 TPR 不会很高，阈值越低，TPR 越高。 但是阈值很低的话，预测为正类的负实例也就越多， FPR 也会越高。</p>
<p>曲线距离左上角越近,证明分类器效果越好。我们用一个标量 AUC 来量化这个分类效果。</p>
<h3 id="怎么得到-ROC-曲线"><a href="#怎么得到-ROC-曲线" class="headerlink" title="怎么得到 ROC 曲线"></a>怎么得到 ROC 曲线</h3><p>我们知道对一个二值分类器，其预测出来的正类的 score 是一个概率。当一个样本为正类的概率大于 threshold时，我们判别它为正类。所以不同的 threshold，其对应的 TPR 和 FPR 的值也就不一样，这样就得到了 ROC 曲线。</p>
<p>详细可参考：<a target="_blank" rel="noopener" href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/">ROC和AUC介绍以及如何计算AUC</a>  非常清楚！！！</p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>AUC值为ROC曲线所覆盖的区域面积,显然,AUC越大,分类器分类效果越好。</p>
<ul>
<li><p>AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。  </p>
</li>
<li><p>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。  </p>
</li>
<li><p>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。  </p>
</li>
<li><p>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。  </p>
</li>
</ul>
<p>AUC的物理意义：假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。</p>
<h3 id="怎么计算-AUC"><a href="#怎么计算-AUC" class="headerlink" title="怎么计算 AUC"></a>怎么计算 AUC</h3><p>AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。</p>
<p>代码实现： <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html">sklearn.metrics.roc_auc_score</a></p>
<h3 id="置信度和置信区间"><a href="#置信度和置信区间" class="headerlink" title="置信度和置信区间"></a>置信度和置信区间</h3><p>这里好像跟置信度和置信区间没啥关系。。。但是了解下也没事</p>
<h4 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h4><p>再来理解置信度，首先要理解 95%置信区间 和 置信度。</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26419030">知乎：95%置信区间</a> 我的理解就是，首先总体均值是固定的，但只有上帝知道。我们就要用样本去估计总体均值。一次次抽样会得到很多样本均值，但是我们无法判断哪个均值最好，最接近总体均值。于是，我们构造区间来看这个区间是否含有总体均值。</p>
<p>怎么构造区间：  </p>
<p>通过一次次抽样得到的样本均值：  </p>
<p>$$M=\dfrac{X_1 + X_2+…+X_n}{n}$$</p>
<p>根据大数定律：  </p>
<p>$$M\sim N(\mu,\dfrac{\sigma^2}{n})$$</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb7.jpg"></p>
<p>通过查表 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/yanjunhelloworld/p/4844741.html">标准正太分布表</a>可知，这样可以计算出置信区间，（如果方差为止，则用样本方差代替）</p>
<p>我们以 $1.96\dfrac{\sigma }{\sqrt{n}}$ 为半径做区间，就构造出了 $95%$  置信区间。按这样去构造的100个区间，其中大约会有95个会包含 $\mu$ ：</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb6.gif"></p>
<p>那么，只有一个问题了，我们不知道、并且永远都不会知道真实的 $\mu$  是多少:</p>
<p>我们就只有用 $\hat{\mu }$ 来代替 $\mu$ ：</p>
<p>$$P(\hat \mu-1.96\dfrac{\sigma}{n}\le M\le\hat \mu+1.96\dfrac{\sigma}{n}) = 0.95$$</p>
<p>这样可以得到置信区间了。如果抽样100次，对应也就有100个置信区间，那么其中含有总体均值的概率约为 95%.</p>
<h4 id="置信度"><a href="#置信度" class="headerlink" title="置信度"></a>置信度</h4><p>样本数目不变的情况下，做一百次试验，有95个置信区间包含了总体真值。置信度为95%</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-07T01:28:38.000Z" title="2018/8/7 上午9:28:38">2018-08-07</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.634Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">34 分钟读完 (大约5038个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%8B%B1%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/">机器学习-英文文本预处理</a></h1><div class="content"><h3 id="转载自：http-www-cnblogs-com-pinard-p-6756534-html"><a href="#转载自：http-www-cnblogs-com-pinard-p-6756534-html" class="headerlink" title="转载自：http://www.cnblogs.com/pinard/p/6756534.html"></a>转载自：<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6756534.html">http://www.cnblogs.com/pinard/p/6756534.html</a></h3><h2 id="英文文本挖掘预处理特点"><a href="#英文文本挖掘预处理特点" class="headerlink" title="英文文本挖掘预处理特点"></a>英文文本挖掘预处理特点</h2><p>英文文本的预处理方法和中文的有部分区别。首先，英文文本挖掘预处理一般可以不做分词（特殊需求除外），而中文预处理分词是必不可少的一步。第二点，大部分英文文本都是uft-8的编码，这样在大多数时候处理的时候不用考虑编码转换的问题，而中文文本处理必须要处理unicode的编码问题。</p>
<p>而英文文本的预处理也有自己特殊的地方，第三点就是拼写问题，很多时候，我们的预处理要包括拼写检查，比如“Helo World”这样的错误，我们不能在分析的时候讲错纠错。所以需要在预处理前加以纠正。第四点就是词干提取(stemming)和词形还原(lemmatization)。这个东西主要是英文有单数，复数和各种时态，导致一个词会有不同的形式。比如“countries”和”country”，”wolf”和”wolves”，我们期望是有一个词。</p>
<h2 id="英文文本挖掘预处理一：数据收集"><a href="#英文文本挖掘预处理一：数据收集" class="headerlink" title="英文文本挖掘预处理一：数据收集"></a>英文文本挖掘预处理一：数据收集</h2><p>这部分英文和中文类似。获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。</p>
<p>对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“deep learning”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。</p>
<p>对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用beautifulsoup。但是我们我们需要某些特殊的语料数据，比如上面提到的“deep learning”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成。这个我一般使用ache。 ache允许我们用关键字或者一个分类算法模型来过滤出我们需要的主题语料，比较强大。</p>
<h2 id="英文文本挖掘预处理二：除去数据中非文本部分"><a href="#英文文本挖掘预处理二：除去数据中非文本部分" class="headerlink" title="英文文本挖掘预处理二：除去数据中非文本部分"></a>英文文本挖掘预处理二：除去数据中非文本部分</h2><p>这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用<a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/">beautifulsoup</a>来去除。另外还有一些特殊的非英文字符(non-alpha),也可以用Python的正则表达式(re)删除。</p>
<h3 id="re-模块"><a href="#re-模块" class="headerlink" title="re 模块"></a>re 模块</h3><p>参考 <a target="_blank" rel="noopener" href="https://songlee24.github.io/2014/09/01/python-library-02/">blog</a>  </p>
<p>正则表达式（Regular Expression）是字符串处理的常用工具，通常被用来检索、替换那些符合某个模式（Pattern）的文本。很多程序设计语言都支持正则表达式，像Perl、Java、C/C++。在 Python 中是通过标准库中的 re 模块 提供对正则的支持。</p>
<p>关于正则表达式的语法可以看</p>
<ul>
<li><p><a href="https://www.panxiaoxie.cn/2018/04/09/chapter2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E3%80%81%E6%96%87%E6%9C%AC%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/">speech and language processing chapter2</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wl_ss/article/details/78241782">正则表达式中的*，+，？以及\w和\W的区别等常见问题的总结</a></p>
</li>
</ul>
<h4 id="编译正则表达式"><a href="#编译正则表达式" class="headerlink" title="编译正则表达式"></a>编译正则表达式</h4><p>re 模块提供了 re.compile() 函数将一个字符串编译成 pattern object，用于匹配或搜索。函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">re.<span class="built_in">compile</span>(pattern, flags=<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>re.compile() 还接受一个可选的参数 flag，用于指定正则匹配的模式。关于匹配模式，后面将会讲到。</p>
<h4 id="反斜杠的困扰"><a href="#反斜杠的困扰" class="headerlink" title="反斜杠的困扰"></a>反斜杠的困扰</h4><p>在 python 的字符串中，\ 是被当做转义字符的。在正则表达式中，\ 也是被当做转义字符。这就导致了一个问题：如果你要匹配 \ 字符串，那么传递给 re.compile() 的字符串必须是 <code>”\\\\“</code>。</p>
<p>由于字符串的转义，所以实际传递给 re.compile() 的是 <code>”\\“</code>，然后再通过正则表达式的转义，<code>”\\“</code> 会匹配到字符”\“。这样虽然可以正确匹配到字符 \，但是很麻烦，而且容易漏写反斜杠而导致 Bug。那么有什么好的解决方案呢？</p>
<p>原始字符串很好的解决了这个问题，通过在字符串前面添加一个r，表示原始字符串，不让字符串的反斜杠发生转义。那么就可以使用<code>r&quot;\\\\&quot;</code>来匹配字符 <code>\</code>了。</p>
<h3 id="patern-object-执行匹配"><a href="#patern-object-执行匹配" class="headerlink" title="patern object 执行匹配"></a>patern object 执行匹配</h3><p>一旦你编译得到了一个 pattern object，你就可以使用 pattern object 的方法或属性进行匹配了，下面列举几个常用的方法，更多请看<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/re.html#regular-expression-objects">这里</a>。</p>
<p><code>Pattern.match(string[, pos[, endpos]])</code></p>
<ul>
<li><p>匹配从 pos 到 endpos 的字符子串的开头。匹配成功返回一个 match object，不匹配返回 None。  </p>
</li>
<li><p>pos 的默认值是0，endpos 的默认值是 len(string)，所以默认情况下是匹配整个字符串的开头。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&quot;d&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;dog&#x27;</span>))  <span class="comment"># 在字串开头，匹配成功</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;god&#x27;</span>))  <span class="comment"># 不再子串开头，匹配不成功</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;ddaa&#x27;</span>, <span class="number">1</span>,<span class="number">5</span>)) <span class="comment"># 在子串开头,匹配成功</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;monday&#x27;</span>, <span class="number">3</span>))  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(0, 1), match=&#x27;d&#x27;&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(0, 1), match=&#x27;g&#x27;&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(1, 2), match=&#x27;d&#x27;&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(3, 4), match=&#x27;d&#x27;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>regex.search(string[, pos[, endpos]])</code></p>
<ul>
<li><p>扫描整个字符串，并返回它找到的第一个匹配  </p>
</li>
<li><p>和 regex.match() 一样，可以通过 pos 和 endpos 指定范围</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&quot;ar&#123;1&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line">match = pattern.search(<span class="string">&quot;marray&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(match)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">1</span>, <span class="number">3</span>), match=<span class="string">&#x27;ar&#x27;</span>&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><code>regex.findall(string[, pos[, endpos]])</code></p>
<ul>
<li><p>找到所有匹配的子串，并返回一个 list    </p>
</li>
<li><p>可选参数 pos 和 endpos 和上面一样  </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d+&quot;</span>) <span class="comment"># 匹配字符串中的数字</span></span><br><span class="line"></span><br><span class="line">lst = pattern.findall(<span class="string">&quot;abc1def2rst3xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>regex.finditer(string[, pos[, endpos]])</code></p>
<ul>
<li><p>找到所有匹配的子串，并返回由这些匹配结果（match object）组成的迭代器  </p>
</li>
<li><p>可选参数 pos 和 endpos 和上面一样。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d+&quot;</span>)</span><br><span class="line"></span><br><span class="line">p = pattern.finditer(<span class="string">&quot;abc1def2rst3xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> p:</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">3</span>, <span class="number">4</span>), match=<span class="string">&#x27;1&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">7</span>, <span class="number">8</span>), match=<span class="string">&#x27;2&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">11</span>, <span class="number">12</span>), match=<span class="string">&#x27;3&#x27;</span>&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="match-object-获取结果"><a href="#match-object-获取结果" class="headerlink" title="match object 获取结果"></a>match object 获取结果</h4><p>在上面讲到，通过 pattern object 的方法（除 findall 外）进行匹配得到的返回结果都是 match object。每一个 match object 都包含了匹配到的相关信息，比如，起始位置、匹配到的子串。那么，我们如何从 match object 中提取这些信息呢？</p>
<p><code>match.group([group1, ...])：</code></p>
<ul>
<li><p>返回 match object 中的字符串。      </p>
</li>
<li><p>每一个 ( ) 都是一个分组，分组编号从1开始，从左往右，每遇到一个左括号，分组编号+1。   </p>
</li>
<li><p>组 0 总是存在的，它就是整个表达式  </p>
</li>
<li><p>没有参数时，group1默认为0，这时返回整个匹配到的字符串。  </p>
</li>
<li><p>指定一个参数（整数）时，返回该分组匹配到的字符串。  </p>
</li>
<li><p>指定多个参数时，返回由那几个分组匹配到的字符串组成的 tuple。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>) <span class="comment"># \w 匹配任意字母，数字，下划线</span></span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;He _ Kobe Bryant, Lakers player&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">4</span>), match=<span class="string">&#x27;He _&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">He _</span><br><span class="line"></span><br><span class="line">He</span><br><span class="line"></span><br><span class="line">_</span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;He&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p><code>match.groups()</code></p>
<ul>
<li>返回由所有分组匹配到的字符串组成的 tuple。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">m = re.match(<span class="string">r&quot;(\d+)\.(\d+)&quot;</span>, <span class="string">&#x27;24.163&#x27;</span>)</span><br><span class="line"></span><br><span class="line">m.groups()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;24&#x27;</span>, <span class="string">&#x27;163&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>match.start([group])</code></p>
<ul>
<li><p>没有参数时，返回匹配到的字符串的起始位置。  </p>
</li>
<li><p>指定参数（整数）时，返回该分组匹配到的字符串的起始位置。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.start())       <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.start(<span class="number">2</span>))      <span class="comment"># 5</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>match.end([group])：</code></p>
<ul>
<li><p>没有参数时，返回匹配到的字符串的结束位置。  </p>
</li>
<li><p>指定参数（整数）时，返回该分组匹配到的字符串的结束位置。  </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.end())       <span class="comment"># 11</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.end(<span class="number">1</span>))      <span class="comment"># 4</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>match.span([group])：</code></p>
<ul>
<li><p>返回一个二元 tuple 表示匹配到的字符串的范围，即 (start, end)。  </p>
</li>
<li><p>指定参数时，返回该分组匹配到的字符串的 (start, end)。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.span())     <span class="comment"># (0, 11)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.span(<span class="number">2</span>))    <span class="comment"># (5, 11)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="模块级别的函数"><a href="#模块级别的函数" class="headerlink" title="模块级别的函数"></a>模块级别的函数</h3><p>上面讲到的函数都是对象的方法，要使用它们必须先得到相应的对象。本节将介绍一些Module-Level Functions，比如 match()，search()，findall() 等等。你不需要创建一个 pattern object 就可以直接调用这些函数。</p>
<p><code>re.match(pattern, string, flags=0)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 相当于</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m = re.match(<span class="string">r&quot;(\w+) (\w+)&quot;</span>,<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.search(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相当于</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m = re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>,<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>re.findall(pattern, string, flags=0)</code>:与上面类似。</p>
<p><code>re.finditer(pattern, string, flags=0)</code>:与上面类似</p>
<h3 id="编译标志（匹配模式）"><a href="#编译标志（匹配模式）" class="headerlink" title="编译标志（匹配模式）"></a>编译标志（匹配模式）</h3><ul>
<li>re.IGNORECASE：忽略大小写，同 re.I。  </li>
</ul>
<ul>
<li>re.MULTILINE：多行模式，改变^和$的行为，同 re.M。  </li>
</ul>
<ul>
<li>re.DOTALL：点任意匹配模式，让’.’可以匹配包括’\n’在内的任意字符，同 re.S。  </li>
</ul>
<ul>
<li>re.LOCALE：使预定字符类 \w \W \b \B \s \S 取决于当前区域设定， 同 re.L。  </li>
</ul>
<ul>
<li>re.ASCII：使 \w \W \b \B \s \S 只匹配 ASCII 字符，而不是 Unicode 字符，同 re.A。  </li>
</ul>
<ul>
<li>re.VERBOSE：详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。主要是为了让正则表达式更易读，同 re.X。例如，以下两个正则表达式是等价的：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d + \. \d *#re.X&quot;</span>) the integral part</span><br><span class="line"></span><br><span class="line">b = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d+\.\d*&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.match(<span class="string">&quot;123.45&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">6</span>), match=<span class="string">&#x27;123.45&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="修改字符串"><a href="#修改字符串" class="headerlink" title="修改字符串"></a>修改字符串</h3><p>第二部分讲的是字符串的匹配和搜索，但是并没有改变字符串。下面就讲一下可以改变字符串的操作。</p>
<h4 id="分割字符串"><a href="#分割字符串" class="headerlink" title="分割字符串"></a>分割字符串</h4><p>split()函数在匹配的地方将字符串分割，并返回一个 list。同样的，re 模块提供了两种 split 函数，一个是 pattern object 的方法，一个是模块级的函数。</p>
<p><code>regex.split(string, maxsplit=0)：</code></p>
<ul>
<li>maxsplit用于指定最大分割次数，不指定将全部分割。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;[A-Z]+&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.split(<span class="string">&quot;abcDefgHijkLmnoPqrs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;abc&#x27;</span>, <span class="string">&#x27;efg&#x27;</span>, <span class="string">&#x27;ijk&#x27;</span>, <span class="string">&#x27;mno&#x27;</span>, <span class="string">&#x27;qrs&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><code>re.split(pattern, string, maxsplit=0, flags=0)：</code></p>
<ul>
<li><p>模块级函数，功能与 regex.split() 相同。  </p>
</li>
<li><p>flags用于指定匹配模式。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">m = re.split(<span class="string">r&quot;[A-Z]+&quot;</span>,<span class="string">&quot;abcDefgHijkLmnoPqrs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;abc&#x27;</span>, <span class="string">&#x27;efg&#x27;</span>, <span class="string">&#x27;ijk&#x27;</span>, <span class="string">&#x27;mno&#x27;</span>, <span class="string">&#x27;qrs&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="搜索与替换"><a href="#搜索与替换" class="headerlink" title="搜索与替换"></a>搜索与替换</h4><p>另一个常用的功能是找到所有的匹配，并把它们用不同的字符串替换。re 模块提供了sub()和subn()来实现替换的功能，而它们也分别有自己两个不同版本的函数。</p>
<p><code>regex.sub(repl, string, count=0)：</code></p>
<ul>
<li><p>使用 repl 替换 string 中每一个匹配的子串，返回替换后的字符串。若找不到匹配，则返回原字符串。</p>
</li>
<li><p>repl 可以是一个字符串，也可以是一个函数。</p>
</li>
<li><p>当repl是一个字符串时，任何在其中的反斜杠都会被处理。</p>
</li>
<li><p>当repl是一个函数时，这个函数应当只接受一个参数（pattern对象），对匹配到的对象进行处理，然后返回一个字符串用于替换。</p>
</li>
<li><p>count 用于指定最多替换次数，不指定时全部替换。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;like&quot;</span>, re.I)</span><br><span class="line"></span><br><span class="line">s1 = pattern.sub(<span class="string">r&quot;love&quot;</span>, <span class="string">&quot;I like you, do you like me?&quot;</span>)</span><br><span class="line"></span><br><span class="line">s2 = pattern.sub(<span class="keyword">lambda</span> m:m.group().upper(), <span class="string">&quot;I like you, do you like me?&quot;</span>)  <span class="comment"># repl 是函数，其参数是 pattern</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">I love you, do you love me?</span><br><span class="line"></span><br><span class="line">I LIKE you, do you LIKE me?</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p><code>re.sub(pattern, repl, string, count=0, flags=0)</code>：</p>
<ul>
<li><p>模块级函数，与 regex.sub() 函数功能相同。  </p>
</li>
<li><p>flags 用于指定匹配模式。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">s1 = re.sub(<span class="string">r&quot;(\w)&#x27;s\b&quot;</span>, <span class="string">r&quot;\1 is&quot;</span>, <span class="string">&quot;She&#x27;s Xie Pan&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">She <span class="keyword">is</span> Xie Pan</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>regex.subn(repl, string, count=0)</code></p>
<ul>
<li>同 sub()，只不过返回值是一个二元 tuple，即(sub函数返回值, 替换次数)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;like&quot;</span>, re.I)</span><br><span class="line"></span><br><span class="line">s1 = pattern.subn(<span class="string">r&quot;love&quot;</span>, <span class="string">&quot;I like you, do you like me?&quot;</span>)</span><br><span class="line"></span><br><span class="line">s2 = pattern.subn(<span class="keyword">lambda</span> m:m.group().upper(), <span class="string">&quot;I like you, do you like me?&quot;</span>)  <span class="comment"># repl 是函数，其参数是 pattern</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;I love you, do you love me?&#x27;</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;I LIKE you, do you LIKE me?&#x27;</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<p><code>re.subn(pattern, repl, string, count=0, flags=0)：</code></p>
<ul>
<li>同上</li>
</ul>
<h3 id="英文文本挖掘预处理三：拼写检查"><a href="#英文文本挖掘预处理三：拼写检查" class="headerlink" title="英文文本挖掘预处理三：拼写检查"></a>英文文本挖掘预处理三：拼写检查</h3><p>由于英文文本中可能有拼写错误，因此一般需要进行拼写检查。如果确信我们分析的文本没有拼写问题，可以略去此步。</p>
<p>拼写检查，我们一般用pyenchant类库完成。pyenchant的安装很简单：”pip install pyenchant”即可。</p>
<p>对于一段文本，我们可以用下面的方式去找出拼写错误：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 发现这样安装并不是在虚拟环境下，需要去终端对应的虚拟环境下安装</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># source avtivate NLP</span></span><br><span class="line"></span><br><span class="line">!pip install pyenchant</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>/bin/sh: 1: source: not found

Requirement already satisfied: pyenchant in /home/panxie/anaconda3/lib/python3.6/site-packages (2.0.0)

[31mdistributed 1.21.8 requires msgpack, which is not installed.[0m

[33mYou are using pip version 10.0.1, however version 18.0 is available.

You should consider upgrading via the &#39;pip install --upgrade pip&#39; command.[0m
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> enchant.checker <span class="keyword">import</span> SpellChecker</span><br><span class="line"></span><br><span class="line">chkr = SpellChecker(<span class="string">&#x27;en_US&#x27;</span>)</span><br><span class="line"></span><br><span class="line">chkr.set_text(<span class="string">&quot;Many peopel like too watch In the Name of people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> err <span class="keyword">in</span> chkr:</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ERROR:&quot;</span>, err.word)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ERROR: peopel</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>发现只能找单词拼写错误的，但 too 这样的是没办法找出的。找出错误后，我们可以自己来决定是否要改正。当然，我们也可以用pyenchant中的wxSpellCheckerDialog类来用对话框的形式来交互决定是忽略，改正还是全部改正文本中的错误拼写。  </p>
<p>更多操作可参考：  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/96c01666aeeb">https://www.jianshu.com/p/96c01666aeeb</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pythonhosted.org/pyenchant/tutorial.html">https://pythonhosted.org/pyenchant/tutorial.html</a></p>
</li>
</ul>
<h3 id="英文文本挖掘预处理四：词干提取-stemming-和词形还原-lemmatization"><a href="#英文文本挖掘预处理四：词干提取-stemming-和词形还原-lemmatization" class="headerlink" title="英文文本挖掘预处理四：词干提取(stemming)和词形还原(lemmatization)"></a>英文文本挖掘预处理四：词干提取(stemming)和词形还原(lemmatization)</h3><p>词干提取(stemming)和词型还原(lemmatization)是英文文本预处理的特色。两者其实有共同点，即都是要找到词的原始形式。只不过词干提取(stemming)会更加激进一点，它在寻找词干的时候可以会得到不是词的词干。比如”imaging”的词干可能得到的是”imag”, 并不是一个词。而词形还原则保守一些，它一般只对能够还原成一个正确的词的词进行处理。个人比较喜欢使用词型还原而不是词干提取。</p>
<p>在实际应用中，一般使用nltk来进行词干提取和词型还原。安装nltk也很简单，”pip install nltk”即可。只不过我们一般需要下载nltk的语料库，可以用下面的代码完成，nltk会弹出对话框选择要下载的内容。选择下载语料库就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">&#x27;wordnet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[nltk_data] Downloading package wordnet to /home/panxie/nltk_data...</span><br><span class="line"></span><br><span class="line">[nltk_data]   Unzipping corpora/wordnet.<span class="built_in">zip</span>.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>在nltk中，做词干提取的方法有PorterStemmer，LancasterStemmer和SnowballStemmer。个人推荐使用SnowballStemmer。这个类可以处理很多种语言，当然，除了中文。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> SnowballStemmer</span><br><span class="line"></span><br><span class="line">stemmer = SnowballStemmer(<span class="string">&quot;english&quot;</span>)</span><br><span class="line"></span><br><span class="line">stemmer.stem(<span class="string">&quot;countries&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;countri&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>输出是”countri”,这个词干并不是一个词。  </p>
<p>而如果是做词型还原，则一般可以使用WordNetLemmatizer类，即wordnet词形还原方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"></span><br><span class="line">wnl = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(wnl.lemmatize(<span class="string">&#x27;countries&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">country</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>输出是”country”,比较符合需求。  </p>
<p>在实际的英文文本挖掘预处理的时候，建议使用基于wordnet的词形还原就可以了。  </p>
<p>在<a target="_blank" rel="noopener" href="http://text-processing.com/demo/stem/">这里</a>有个词干提取和词型还原的demo，如果是这块的新手可以去看看，上手很合适。</p>
<h3 id="英文文本挖掘预处理五：转化为小写"><a href="#英文文本挖掘预处理五：转化为小写" class="headerlink" title="英文文本挖掘预处理五：转化为小写"></a>英文文本挖掘预处理五：转化为小写</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text = <span class="string">&#x27;XiePan&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(text.lower())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xiepan</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3 id="英文文本挖掘预处理六：引入停用词"><a href="#英文文本挖掘预处理六：引入停用词" class="headerlink" title="英文文本挖掘预处理六：引入停用词"></a>英文文本挖掘预处理六：引入停用词</h3><p>在英文文本中有很多无效的词，比如“a”，“to”，一些短词，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。个人常用的英文停用词表下载地址在这。当然也有其他版本的停用词表，不过这个版本是我常用的。</p>
<p>在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。这个方法和前文讲中文停用词的方法相同，这里就不写出代码，大家参考前文即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">stop = <span class="built_in">set</span>(stopwords.words(<span class="string">&#x27;english&#x27;</span>))  <span class="comment"># 停用词</span></span><br><span class="line"></span><br><span class="line">stop.add(<span class="string">&quot;foo&quot;</span>)    <span class="comment"># 增加一个词</span></span><br><span class="line"></span><br><span class="line">stop.remove(<span class="string">&quot;is&quot;</span>)  <span class="comment"># 去掉一个词</span></span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&quot;this is a foo bar sentence&quot;</span></span><br><span class="line"></span><br><span class="line">[i <span class="keyword">for</span> i <span class="keyword">in</span> word_tokenize(sentence.lower()) <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stop]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;is&#39;, &#39;bar&#39;, &#39;sentence&#39;]
</code></pre>
<h3 id="英文文本挖掘预处理七：特征处理"><a href="#英文文本挖掘预处理七：特征处理" class="headerlink" title="英文文本挖掘预处理七：特征处理"></a>英文文本挖掘预处理七：特征处理</h3><p>现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick</a>中，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在文本挖掘预处理之TF-IDF中，我们也讲到了<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6693230.html">TF-IDF特征处理的方法</a>。</p>
<p>TfidfVectorizer类可以帮助我们完成向量化，TF-IDF和标准化三步。当然，还可以帮我们处理停用词。这部分工作和中文的特征处理也是完全相同的，大家参考前文即可。</p>
<h3 id="英文文本挖掘预处理八：建立分析模型"><a href="#英文文本挖掘预处理八：建立分析模型" class="headerlink" title="英文文本挖掘预处理八：建立分析模型"></a>英文文本挖掘预处理八：建立分析模型</h3><p>有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而主题模型是自然语言处理比较特殊的一块，这个我们后面再单独讲。</p>
<h3 id="英文文本挖掘预处理总结"><a href="#英文文本挖掘预处理总结" class="headerlink" title="英文文本挖掘预处理总结"></a>英文文本挖掘预处理总结</h3><p>上面我们对英文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如有时候需要做词性标注，而有时候我们也需要英文分词，比如得到”New York”而不是“New”和“York”，因此这个流程仅供自然语言处理入门者参考，我们可以根据我们的数据分析目的选择合适的预处理方法。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-07-28T12:30:13.000Z" title="2018/7/28 下午8:30:13">2018-07-28</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">29 分钟读完 (大约4300个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/">深度学习-Batch Normalization</a></h1><div class="content"><h1 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h1><ul>
<li>paper: <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/ioffe15.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
</ul>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift.   </p>
</blockquote>
<p>神经网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，这使得不得不降低学习率、小心地初始化，并且使得那些具有易饱和非线性激活函数的网络训练臭名昭著。作者将分布发生变化称之为 internal covariate shift。</p>
<blockquote>
<p>stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate and the initial parameter values. The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper.  </p>
</blockquote>
<p>在深度学习中我们采用SGD取得了非常好的效果，SGD简单有效，但是它对超参数非常敏感，尤其是学习率和初始化参数。</p>
<blockquote>
<p>The change in the distributions of layers’ inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience covariate shift (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learning system</p>
</blockquote>
<p>as a whole, to apply to its parts, such as a sub-network or a layer.  </p>
<p>因为学习的过程中每一层需要去连续的适应每一层输入的分布，所以输入分布发生变化时，会产生一些问题。这里作者引用了 <em>covariate shift</em> 和 <strong>domain adaptation</strong> 这两个概念。</p>
<blockquote>
<p>Therefore, the input distribution properties that aid the network generalization – such as having the same distribution between the training and test data – apply to training the sub-network as well.As such it is advantageous for the distribution of x to remain fixed over time.</p>
</blockquote>
<p>有助于网络泛化的输入分布属性：例如在训练和测试数据之间具有相同的分布，也适用于训练子网络</p>
<blockquote>
<p>Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the subnetwork, as well.  </p>
</blockquote>
<p>固定输入分布对该子网络其他部分的网络的训练会产生积极的影响。</p>
<p><strong>总结下为什么要使用 BN</strong>：  </p>
<p>在训练的过程中，因为前一层的参数改变，将会导致后一层的输入的分布不断地发生改变，这就需要降低学习速率同时要注意参数的初始化，也使具有饱和非线性（saturating nonlinearity）结构的模型非常难训练（所谓的饱和就是指函数的值域是个有限值，即当函数自变量趋向无穷时，函数值不趋向无穷）。深度神经网络之所以复杂是因为它每一层的输出都会受到之前层的影响，因此一个小小的参数改变都会对网络产生巨大的改变。作者将这种现象称为internal covariate shift，提出了对每个输入层进行规范化来解决。在文中，作者提到使用BN可以在训练的过程中使用较高的学习速率，可以比较随意的对参数进行初始化，同时BN也起到了一种正则化的作用，在某种程度上可以取代dropout的作用。</p>
<p>考虑一个以sigmoid为激活函数的神经层：  </p>
<p>$z=g(Wu+b)$  </p>
<p>其中 u 是输入， g 是 sigmoid 激活函数 $g(x)=\dfrac{1}{1+exp(x)}$，当 |x| 增加时，$g’(x)$ 趋近于0, 这意味着 $x=Wu+b$ 的所有维度，除了绝对值较小的维度，其他的流向输入 u 的梯度都会消失,也就是进入非线性的饱和区域，这会降低模型训练速度。</p>
<p>在实际应用中，对于非线性饱和的情况，已经有很有对应策略：  </p>
<ul>
<li><p>ReLU  </p>
</li>
<li><p>初始化 Xavier initialization.  </p>
</li>
<li><p>用一个较小的学习速率进行学习  </p>
</li>
</ul>
<blockquote>
<p>If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate.  </p>
</blockquote>
<p>如果保证非线性输入的分布稳定，优化器也就不会陷于饱和区域了，训练也会加速。</p>
<blockquote>
<p>We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as Internal Covariate Shift. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs.  </p>
</blockquote>
<p>作者把这种输入分布的变化叫做内部协方差偏移。并提出了 <strong>Batch Normalization</strong>,通过固定输入的均值和方差。</p>
<blockquote>
<p>Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or</p>
</blockquote>
<p>of their initial values. This allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.  </p>
<p>BN 除了能解决 internal covariate shift 的问题，还能够降低梯度对学习率，初始化参数设置的依赖。这使得我们可以使用较大的学习率，正则化模型，降低对 dropout 的需求，最后还保证网络能够使用具有饱和性的非线性激活函数。</p>
<h2 id="Towards-Reducing-Internal-Covariate-Shift"><a href="#Towards-Reducing-Internal-Covariate-Shift" class="headerlink" title="Towards Reducing Internal Covariate Shift"></a>Towards Reducing Internal Covariate Shift</h2><h3 id="whitening-白化操作"><a href="#whitening-白化操作" class="headerlink" title="whitening 白化操作"></a>whitening 白化操作</h3><blockquote>
<p>It has been long known (LeCun et al., 1998b; Wiesler &amp; Ney, 2011) that the network training converges faster if its inputs are whitened – i.e., linearly transformed to have zero means and unit variances, and decorrelated.  </p>
</blockquote>
<p>使用白化 whitening 有助于模型收敛，白化是线性变化，转化为均值为0,方差为1,并且去相关性。</p>
<blockquote>
<p>However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step.  </p>
</blockquote>
<p>如果将白化与基于梯度下降的优化混合在一起，那么在执行梯度下降的过程中会受到标准化的参数更新的影响，这样会减弱甚至抵消梯度下降的产生的影响。</p>
<p>作者举了这样一个例子：  </p>
<p>考虑一个输入 u 和一个可学习的参数 b 相加作为一个 layer. 通过减去均值进行标准化 $\hat x=x-E[x]$, 其中 x=u+b. 则前向传播的过程：  </p>
<p>$x=u+b \rightarrow \hat x = x-E[x] \rightarrow loss$  </p>
<p>反向传播对参数 b 求导（不考虑 b 和 E[x] 的相关性）：  </p>
<p>$\dfrac{\partial l}{\partial b}=\dfrac{\partial l}{\partial \hat x}\dfrac{\partial \hat x}{\partial b} = \dfrac{\partial l}{\partial \hat x}$  </p>
<p>那么 $\Delta b = -\dfrac{\partial l}{\partial \hat x}$, 则对于参数 b 的更新： $b \leftarrow  \Delta b + b$.  </p>
<p>那么经过了标准化、梯度下降更新参数之后：  </p>
<p>$u+(b+\Delta b)-E[u+(b+\Delta b)]=u+b-E[u+b]$  </p>
<p>这意味着这个 layer 的输出没有变化，损失 $\dfrac{\partial l}{\partial \hat x}也没有变化$, 那么随着训练的进行，**b会无限的增长???**，而loss不变。</p>
<blockquote>
<p>This problem can get worse if the normalization not only centers but also scales the activations. We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step.  </p>
</blockquote>
<p>如果规范化不仅中心处理(即减去均值)，而且还对激活值进行缩放，问题会变得更严重。通过实验发现， 当归一化参数在梯度下降步骤之外进行，模型会爆炸。</p>
<h3 id="进行白化操作，并且在优化时考虑标准化的问题"><a href="#进行白化操作，并且在优化时考虑标准化的问题" class="headerlink" title="进行白化操作，并且在优化时考虑标准化的问题"></a>进行白化操作，并且在优化时考虑标准化的问题</h3><blockquote>
<p>The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. To address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution.Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters Θ.  </p>
</blockquote>
<p>之所以会产生以上的问题，主要是梯度优化的过程中没有考虑到标准化操作的进行(不好实现)。为了解决这一问题，作者提出我们需要保证网络产生的激活总是有相同的分布。这样做允许损失值关于模型参数的梯度考虑到标准化。</p>
<p>再一次考虑 x 是一个 layer 的输入，看作一个向量，$\chi$ 是整个训练集，则标准化：</p>
<p>$\hat x = Norm(x, \chi)$</p>
<p>这时标准化的参数不仅取决于当前的输入x，还和整个训练集 $\chi$ 有关，当x来自其它层的输出时，那么上式就会和前面层的网络参数 $\theta$ 有关，反向传播时需要计算:</p>
<p>$$\frac{\partial{Norm(x,\chi)}}{\partial{x}}\text{ and }\frac{\partial{Norm(x,\chi)}}{\partial{\chi}}$$</p>
<p>如果忽略上边第二项就会出现之前说到的问题。但是直接在这一架构下进行白话操作很非常的费时，代价很大。主要是需要计算协方差矩阵，进行归一化，以及反向传播时也需要进行相关的计算。因此这就需要寻找一种新的方法，既可以达到类似的效果，又不需要在每个参数更新后分析整个训练集。</p>
<h2 id="Normalization-via-Mini-Batch-Statistics"><a href="#Normalization-via-Mini-Batch-Statistics" class="headerlink" title="Normalization via Mini-Batch Statistics"></a>Normalization via Mini-Batch Statistics</h2><h3 id="对比于白化的两个简化"><a href="#对比于白化的两个简化" class="headerlink" title="对比于白化的两个简化"></a>对比于白化的两个简化</h3><blockquote>
<p>Since the full whitening of each layer’s inputs is costly, we make two necessary simplifications. The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have zero mean and unit variance.  </p>
</blockquote>
<p>既然白化操作这么费时费力，作者考虑两点必要的简化。<strong>第一点</strong>，对输入特征的每一维 $x=(x^{(1)},…,x^{(d)})$ 进行去均值和单位方差的处理。</p>
<p>$$\hat x^{(k)} = \dfrac{x^{(k)}-E[x^{(k)}]}{\sqrt {Var[x^{(k)}]}}$$</p>
<blockquote>
<p>where the expectation and variance are computed over the</p>
</blockquote>
<p>training data set.   </p>
<p>其中均值和方差是基于整个训练集计算得到的。</p>
<blockquote>
<p>Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity.  </p>
</blockquote>
<p>但是如果仅是简单的对每一层的输入进行标准化可能会对该层的表达造成能力改变。比如对一个sigmoid激活函数的输入标准化会将输入固定在线性区域。</p>
<p>为了解决这一问题，作者提出了这样的改变,引入一对参数 $\gamma^{(k)}$, $\beta^{(k)}$ 来对归一化之后的值进行缩放和平移。  </p>
<p>$$y^{(k)} = \gamma^{(k)}\hat x^{(k)} + \beta^{(k)}$$</p>
<p> $\gamma^{(k)}$, $\beta^{(k)}$ 是可学习的参数，用来回复经过标准化之后的网络的表达能力。如果  $\gamma^{(k)}=\sqrt {Var[x^{(k)}]}$, $\beta^{(k)}=E[x^{(k)}]$</p>
<blockquote>
<p>In the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations. However, this is impractical when using stochastic optimization. Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation.  </p>
</blockquote>
<p>在batch中使用整个训练集的均值和方差是不切实际的，因此，作者提出了 <strong>第二个简化</strong>，用 mini-batch 来估计均值和方差。</p>
<blockquote>
<p>Note that the use of mini-batches is enabled by computation of per-dimension variances rather than joint covariances; in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices.  </p>
</blockquote>
<p>注意到 mini-batches 是计算每一维的方差，而不是联合协方差。使用协方差就需要对模型进行正则化，mini-batches 的大小往往小于需要白化的激活值的数量,会得到 <strong>奇异协方差矩阵(singular vorariance matrices)???</strong>.</p>
<h3 id="BN-核心流程"><a href="#BN-核心流程" class="headerlink" title="BN 核心流程"></a>BN 核心流程</h3><p>batch size m, 我们关注其中某一个维度 $x^{k}$, k 表示第k维特征。那么对于 batch 中该维特征的 m 个值：</p>
<p>$$B={x_{1,…,m}}$$</p>
<p>经过线性转换：</p>
<p>$$BN_{\gamma, \beta}:x_{1,..,m}\rightarrow y_{1,..,m}$$</p>
<p><img src="/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/bn02.png"></p>
<ul>
<li><p>对于输入的 mini-batch 的一个维度，计算均值和方差</p>
</li>
<li><p>标准化（注意 epsilon 避免0错误）</p>
</li>
<li><p>使用两个参数进行平移和缩放</p>
</li>
</ul>
<p>这里有点疑惑：为什么在第三步已经完成标准化的情况下还要进行4操作，后来发现其实作者在前文已经说了。首先 $\hat x$ 是标准化后的输出，但是如果仅以此为输出，其输出就被限定为了标准正态分布，这样很可能会限制原始网络能表达的信息，前文已用sigmoid函数进行了举例说明。因为 $\gamma, \beta$ 这两个参数是可以学习的，所以的标准化后的”恢复”程度将在训练的过程中由网络自主决定。</p>
<p>利用链式法则，求损失函数对参数 $\gamma, \beta$ 求导：</p>
<p><img src="/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/bn03.png"></p>
<blockquote>
<p>Thus, BN transform is a differentiable transformation that introduces normalized activations into the network. This ensures that as the model is training, layers can continue learning on input distributions that exhibit less internal covariate shift, thus accelerating the training.  </p>
</blockquote>
<p>BN 是可微的，保证模型可训练，网络可以学习得到输入的分布，来减小 internal covarite shift, 从而加速训练。</p>
<h3 id="Training-and-Inference-with-Batch-Normalized-Networks"><a href="#Training-and-Inference-with-Batch-Normalized-Networks" class="headerlink" title="Training and Inference with Batch-Normalized Networks"></a>Training and Inference with Batch-Normalized Networks</h3><blockquote>
<p>The normalization of activations that depends on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want</p>
</blockquote>
<p>the output to depend only on the input, deterministically. For this, once the network has been trained, we use the normalization  </p>
<p>$\hat x = \dfrac{x-E[x]}{\sqrt{Var[x]+\epsilon}}$  </p>
<p>using the population, rather than mini-batch, statistics.  </p>
<p><strong>在训练阶段和推理(inference)阶段不一样</strong>，这里的推理阶段指的就是测试阶段，在测试阶段使用总体的均值，而不是 mini-batch 的均值。</p>
<blockquote>
<p>Using moving averages instead, we can track the accuracy of a model as it trains. Since the means and variances are fixed during inference, the normalization is simply a linear transform applied to each activation.</p>
</blockquote>
<h3 id="Batch-Normalized-Convolutional-Networks"><a href="#Batch-Normalized-Convolutional-Networks" class="headerlink" title="Batch-Normalized Convolutional Networks"></a>Batch-Normalized Convolutional Networks</h3><p><img src="/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/bn04.png"></p>
<ul>
<li><p>第1-5步是算法1的流程，对每一维标准化，得到 $N_{BN}^{tr}$  </p>
</li>
<li><p>6-7步优化训练参数 $\theta \bigcup {\gamma^{k}, \beta^{k}}$，在测试阶段参数是固定的  </p>
</li>
<li><p>8-12步骤是将训练阶段的统计信息转化为训练集整体的统计信息。因为完成训练后在预测阶段，我们使用的是模型存储的整体的统计信息。这里涉及到通过样本均值和方差估计总体的均值和方差的无偏估计，样本均值是等于总体均值的无偏估计的，而样本均值不等于总体均值的无偏估计。具体可看知乎上的解答 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20099757">https://www.zhihu.com/question/20099757</a>  </p>
</li>
</ul>
<h3 id="Batch-Normalization-enables-higher-learning-rates"><a href="#Batch-Normalization-enables-higher-learning-rates" class="headerlink" title="Batch Normalization enables higher learning rates"></a>Batch Normalization enables higher learning rates</h3><blockquote>
<p>In traditional deep networks, too high a learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima.  </p>
</blockquote>
<p>学习率过大容易发生梯度消失和梯度爆炸，从而陷入局部最小值。</p>
<blockquote>
<p>By normalizing activations throughout the network, it prevents small changes in layer parameters from amplifying as the data propagates through a deep network.  </p>
</blockquote>
<p>通过规范化整个网络中的激活，可以防止层参数的微小变化在数据通过深层网络传播时放大。</p>
<blockquote>
<p>Batch Normalization also makes training more resilient to the parameter scale. Normally, large learning rates may increase the scale of layer parameters, which then amplify the gradient during backpropagation and lead to the model explosion. However, with Batch Normalization, backpropagation through a layer is unaffected by the scale of its parameters.  </p>
</blockquote>
<p>BN 能让训练时的参数更有弹性。通常，学习率过大会增大网络参数，在反向传播中导致梯度过大而发生梯度爆炸。而 BN 使得网络不受参数的大小的影响。</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>除了可以更快地训练网络，BN层还有对模型起到正则化的作用。因为当训练一个BN网络的时候，对于一个给定的样本，它还可以”看到”一个batch中其他的情况，这样网络对于一个给定的样本输入每次就可以产生一个不确定的输出(因为标准化的过程和batch中其他的样本均有关联)，作者通过实验证明这对减少模型的过拟合具有作用。</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>tensorflow 已经封装好了 BN 层，可以直接通过 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/layers/batch_norm">tf.contrib.layers.batch_norm()</a> 调用，如果你想知道函数背后的具体实现方法，加深对BN层的理解，可以参考这篇文章<a target="_blank" rel="noopener" href="https://r2rt.com/implementing-batch-normalization-in-tensorflow.html">Implementing Batch Normalization in Tensorflow</a>。</p>
<h1 id="reference"><a href="#reference" class="headerlink" title="reference:"></a>reference:</h1><ul>
<li>paper: <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/ioffe15.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.11604">How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://byjiang.com/2017/05/17/batch_normalization/">关于Batch Normalization的一些阅读理解</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-07-10T06:17:15.000Z" title="2018/7/10 下午2:17:15">2018-07-10</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.522Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">14 分钟读完 (大约2116个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/">机器学习-过拟合</a></h1><div class="content"><p>过拟合的原理以及解决方法。</p>
<h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p>过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。</p>
<p>具体表现就是最终模型在训练集上效果好；在测试集上效果差。模型泛化能力弱。</p>
<p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C01.jpeg"></p>
<h2 id="为什么要解决过拟合"><a href="#为什么要解决过拟合" class="headerlink" title="为什么要解决过拟合"></a>为什么要解决过拟合</h2><p>为什么要解决过拟合现象？这是因为我们拟合的模型一般是用来预测未知的结果（不在训练集内），过拟合虽然在训练集上效果好，但是在实际使用时（测试集）效果差。同时，在很多问题上，我们无法穷尽所有状态，不可能将所有情况都包含在训练集上。所以，必须要解决过拟合问题。</p>
<p>为什么在机器学习中比较常见？这是因为机器学习算法为了满足尽可能复杂的任务，其模型的拟合能力一般远远高于问题复杂度，也就是说，机器学习算法有「拟合出正确规则的前提下，进一步拟合噪声」的能力。</p>
<p>而传统的函数拟合问题（如机器人系统辨识），一般都是通过经验、物理、数学等推导出一个含参模型，模型复杂度确定了，只需要调整个别参数即可。模型「无多余能力」拟合噪声。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="获取更多数据"><a href="#获取更多数据" class="headerlink" title="获取更多数据"></a>获取更多数据</h3><p>这是解决过拟合最有效的方法，只要给足够多的数据，让模型「看见」尽可能多的「例外情况」，它就会不断修正自己，从而得到更好的结果：</p>
<p>如何获取更多数据，可以有以下几个方法：</p>
<ul>
<li><p>从数据源头获取更多数据：这个是容易想到的，例如物体分类，我就再多拍几张照片好了；但是，在很多情况下，大幅增加数据本身就不容易；另外，我们不清楚获取多少数据才算够；</p>
</li>
<li><p>根据当前数据集估计数据分布参数，使用该分布产生更多数据：这个一般不用，因为估计分布参数的过程也会代入抽样误差。</p>
</li>
<li><p>数据增强（Data Augmentation）：通过一定规则扩充数据。如在物体分类问题里，物体在图像中的位置、姿态、尺度，整体图片明暗度等都不会影响分类结果。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充；</p>
</li>
</ul>
<p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C02.jpeg"></p>
<h3 id="使用合适的模型"><a href="#使用合适的模型" class="headerlink" title="使用合适的模型"></a>使用合适的模型</h3><p>前面说了，过拟合主要是有两个原因造成的：数据太少+模型太复杂。所以，我们可以通过使用合适复杂度的模型来防止过拟合问题，让其足够拟合真正的规则，同时又不至于拟合太多抽样误差。</p>
<p>（PS：如果能通过物理、数学建模，确定模型复杂度，这是最好的方法，这也就是为什么深度学习这么火的现在，我还坚持说初学者要学掌握传统的建模方法。）</p>
<p>对于神经网络而言，我们可以从以下四个方面来限制网络能力：</p>
<h4 id="网络结构-Architecture"><a href="#网络结构-Architecture" class="headerlink" title="网络结构 Architecture"></a>网络结构 Architecture</h4><p>这个很好理解，减少网络的层数、神经元个数等均可以限制网络的拟合能力；</p>
<h4 id="训练时间-Early-stopping"><a href="#训练时间-Early-stopping" class="headerlink" title="训练时间 Early stopping"></a>训练时间 Early stopping</h4><p>对于每个神经元而言，其激活函数在不同区间的性能是不同的：</p>
<p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C03.jpg"></p>
<p>当网络权值较小时，神经元的激活函数工作在线性区，此时神经元的拟合能力较弱（类似线性神经元）。</p>
<p>有了上述共识之后，我们就可以解释为什么限制训练时间（early stopping）有用：因为我们在初始化网络的时候一般都是初始为较小的权值。训练时间越长，部分网络权值可能越大。如果我们在合适时间停止训练，就可以将网络的能力限制在一定范围内。</p>
<h4 id="限制权值-Weight-decay，也叫正则化（regularization）"><a href="#限制权值-Weight-decay，也叫正则化（regularization）" class="headerlink" title="限制权值 Weight-decay，也叫正则化（regularization）"></a>限制权值 Weight-decay，也叫正则化（regularization）</h4><p>原理同上，但是这类方法直接将权值的大小加入到 Cost 里，在训练的时候限制权值变大。以 L2 regularization为例：</p>
<p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C04.svg"></p>
<p>训练过程需要降低整体的 Cost，这时候，一方面能降低实际输出与样本之间的误差 ，也能降低权值大小。</p>
<h4 id="增加噪声-Noise"><a href="#增加噪声-Noise" class="headerlink" title="增加噪声 Noise"></a>增加噪声 Noise</h4><p>给网络加噪声也有很多方法：</p>
<h5 id="在输入中加噪声："><a href="#在输入中加噪声：" class="headerlink" title="在输入中加噪声："></a>在输入中加噪声：</h5><p>噪声会随着网络传播，按照权值的平方放大，并传播到输出层，对误差 Cost 产生影响。推导直接看 Hinton 的 PPT 吧：</p>
<p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C05.jpeg"></p>
<p>在输入中加高斯噪声，会在输出中生成 的干扰项。训练时，减小误差，同时也会对噪声产生的干扰项进行惩罚，达到减小权值的平方的目的，达到与 L2 regularization 类似的效果（对比公式）。</p>
<h4 id="在权值上加噪声"><a href="#在权值上加噪声" class="headerlink" title="在权值上加噪声"></a>在权值上加噪声</h4><p>在初始化网络的时候，用0均值的高斯分布作为初始化。Alex Graves 的手写识别 RNN 就是用了这个方法</p>
<blockquote>
<p>Graves, Alex, et al. “A novel connectionist system for unconstrained handwriting recognition.” IEEE transactions on pattern analysis and machine intelligence 31.5 (2009): 855-868.</p>
</blockquote>
<ul>
<li>It may work better, especially in recurrent networks (Hinton)</li>
</ul>
<h4 id="对网络的响应加噪声"><a href="#对网络的响应加噪声" class="headerlink" title="对网络的响应加噪声"></a>对网络的响应加噪声</h4><p>如在前向传播过程中，让默写神经元的输出变为 binary 或 random。显然，这种有点乱来的做法会打乱网络的训练过程，让训练更慢，但据 Hinton 说，在测试集上效果会有显著提升 （But it does significantly better on the test set!）。</p>
<h3 id="结合多种模型"><a href="#结合多种模型" class="headerlink" title="结合多种模型"></a>结合多种模型</h3><p>简而言之，训练多个模型，以每个模型的平均输出作为结果。</p>
<p>从 N 个模型里随机选择一个作为输出的期望误差 ，会比所有模型的平均输出的误差 大（我不知道公式里的圆括号为什么显示不了）：</p>
<p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C07.jpeg"></p>
<p>大概基于这个原理，就可以有很多方法了：</p>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>简单理解，就是分段函数的概念：用不同的模型拟合不同部分的训练集。以随机森林（Rand Forests）为例，就是训练了一堆互不关联的决策树。但由于训练神经网络本身就需要耗费较多自由，所以一般不单独使用神经网络做Bagging。</p>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p>既然训练复杂神经网络比较慢，那我们就可以只使用简单的神经网络（层数、神经元数限制等）。通过训练一系列简单的神经网络，加权平均其输出。</p>
<p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C06.jpeg"></p>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C08.jpeg"></p>
<p>在训练时，每次随机（如50%概率）忽略隐层的某些节点；这样，我们相当于随机从2^H个模型中采样选择模型；同时，由于每个网络只见过一个训练数据（每次都是随机的新网络），所以类似 bagging 的做法，这就是我为什么将它分类到「结合多种模型」中；</p>
<p>此外，而不同模型之间权值共享（共同使用这 H 个神经元的连接权值），相当于一种权值正则方法，实际效果比 L2 regularization 更好。</p>
<h3 id="贝叶斯方法"><a href="#贝叶斯方法" class="headerlink" title="贝叶斯方法"></a>贝叶斯方法</h3><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88%5C09.jpeg"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-07-06T02:32:32.000Z" title="2018/7/6 上午10:32:32">2018-07-06</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a></span><span class="level-item">16 分钟读完 (大约2440个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/07/06/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/">算法-排序算法</a></h1><div class="content"><p>剑指offer 第二章-面试需要的基础知识</p></div><a class="article-more button is-small is-size-7" href="/2018/07/06/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/#more">阅读更多</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/15/">上一页</a></div><div class="pagination-next"><a href="/page/17/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/15/">15</a></li><li><a class="pagination-link is-current" href="/page/16/">16</a></li><li><a class="pagination-link" href="/page/17/">17</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>