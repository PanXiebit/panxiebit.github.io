<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-08-12T12:41:48.000Z" title="2019/8/12 下午8:41:48">2019-08-12</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.173Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/reinforcement-learning/">reinforcement learning</a></span><span class="level-item">几秒读完 (大约2个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/08/12/UCB-cs294-policy-gradient/">UCB-cs294-policy gradient</a></h1><div class="content"><h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-06-30T07:43:31.000Z" title="2019/6/30 下午3:43:31">2019-06-30</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/GAN/">GAN</a></span><span class="level-item">10 分钟读完 (大约1521个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/">从0开始GAN-6-pretraining for NLG</a></h1><div class="content"><h3 id="related-papers"><a href="#related-papers" class="headerlink" title="related papers"></a>related papers</h3><ul>
<li><p><a href>BERT</a>   </p>
</li>
<li><p><a href>BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</a>    </p>
</li>
<li><p><a href>GPT/GPT-2.0</a>  </p>
</li>
<li><p><a href>MASS: Masked Sequence to Sequence Pre-training for Language Generation</a>   </p>
</li>
<li><p><a href>Unified Language Model Pre-training for Natural Language Understanding and Generation</a>      </p>
</li>
<li><p><a href>Pretraining for Conditional Generation with Pseudo Self Attention</a>    </p>
</li>
<li><p><a href>Transformer-XL</a>       </p>
</li>
<li><p><a href>XLNet</a>     </p>
</li>
<li><p><a href>Defending Against Neural Fake News</a></p>
</li>
<li><p><a href>ERNIE</a>  </p>
</li>
<li><p><a href>WWM</a>  </p>
</li>
<li><p><a href>SpanBERT</a></p>
</li>
</ul>
<h3 id><a href="#" class="headerlink" title></a></h3><h3 id="cross-lingual-word-embedding"><a href="#cross-lingual-word-embedding" class="headerlink" title="cross-lingual word embedding"></a>cross-lingual word embedding</h3><p><a href>A survey of cross-lingual word embedding models, Ruder et al.2017</a>  </p>
<p><a href>Word translation without parallel data. Conneau et al.2017</a>  </p>
<p><a href>Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Artetxe et al.2018</a>  </p>
<h3 id="contextual-word-embedding"><a href="#contextual-word-embedding" class="headerlink" title="contextual word embedding"></a>contextual word embedding</h3><p><a href>ELMo</a>  </p>
<p><a href>Word2vec</a>  </p>
<p><a href>Glove</a>  </p>
<p><a href>GPT</a>  </p>
<p><a href>ULMFT: Universal language model fine-tuning for text classificatio</a>  </p>
<p><a href>Cross-lingual language model pretraining</a>  </p>
<p><a href>Polyglot contextual representations im- prove crosslingual transfer</a>  </p>
<h3 id="pre-trained-for-NMT"><a href="#pre-trained-for-NMT" class="headerlink" title="pre-trained for NMT"></a>pre-trained for NMT</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.05672">Towards Making the Most of BERT in Neural Machine Translation</a></p>
<p><a href>Unsupervised Pretraining for Sequence to Sequence Learning</a></p>
<p><a href>When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.07291">Cross-lingual Language Model Pretraining</a>  </p>
<h1 id="XLM"><a href="#XLM" class="headerlink" title="XLM"></a>XLM</h1><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.07291">Cross-lingual Language Model Pretraining</a>  </p>
<p>作者提出了两种方法来学习 cross-lingual 语言模型。其中一种是仅基于 monolingual data, 另一种是基于平行语料。在 cross-lingal 相关的任务上都有很大的提升。比如 XNLI，unsupervised machine translation, 以及 supervised machine tranlsation.</p>
<p>现有的在NLP领域的发展主要是围绕英文进行的，一些start-of-the-art或者NLP任务的benchmarks都是以英文为基础的。其他的一些语言受限于语料的问题，发展相对缓慢。近期随着cross-lingual sentence representation的发展，消除English-centric bias,并且构建一个通用的cross-lingual encoder来讲任何语言的sentence编码到共享的embedding空间成为可能。</p>
<p><strong>Shared sub-word vocabulary</strong></p>
<p>使用 bpe,并且不同的language共享词表.</p>
<blockquote>
<p>this greatly improves the alignment of embedding spaces across languages that share either the same alphabet or anchor tokens such as digits (Smith et al., 2017) or proper nouns.  </p>
</blockquote>
<p>共享词表能显著提升那些具有相同字母表或者anchor token(数字或专有名词)的语言之间的向量空间的对齐。</p>
<p>作者先从不同语言的monolingual data中筛选出部分data，然后学习bpe splits.</p>
<blockquote>
<p>Sentences are sampled according to a multinomial distribution with probabilities ${q_i}_{i=1…N}$, where:</p>
</blockquote>
<p><img src="/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/sample_data.png"></p>
<p>其中 $n_i$ 表示第 i 中语言中sentence的总数。 $\sum_{k=1}^nn_k$ 表示N种语言所有的sentence的总数。$p_i$ 则表示第 i 中语言sample的概率。设定 $\alpha=0.5$，这样能增加 low-resource 的比例，从而减轻 bias to high-resource language.</p>
<p>作者总共提出了三种 language model. 接下来一一介绍：</p>
<p><strong>Causal Language Modeling (CLM)</strong>  </p>
<p>$p(w_t|w_1,…,w_{t-1},\theta)$</p>
<p>也就是普通的 aotu-regressive 语言模型。</p>
<p><a target="_blank" rel="noopener" href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4182">Character-Level Language Modeling with Deeper Self-Attention</a> 这篇paper使用的self-attention, 我们知道self-attention 不像rnn那样具有hidden state的概率，这篇paper把上一个batch作为下一个batch的context，有点类似于 transformer-XL,但是这对于cross-lingual不太适合，所以这里的 CLM 与传统的language model完全一致。</p>
<p><strong>Masked Language Modeling (MLM)</strong></p>
<p><img src="/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/mlm.png"></p>
<p>与 BERT 中MLM的区别：  </p>
<blockquote>
<p>Differences between our approach and the MLM of Devlin et al. (2018) include the use of text streams of an arbitrary number of sentences (truncated at 256 tokens) instead of pairs of sentences.  </p>
</blockquote>
<p>文本 stream 是任意数量的sentences，而不是pairs.（这里的pairs in BERT应该指的是 next sentence prediction.）</p>
<p>在筛选用来mask的词时，为了处理 rare word 和 frequent word(punctuation or stop words) 的不均衡问题:    </p>
<blockquote>
<p>tokens in a text stream are sampled according to a multinomial distribution, whose weights are proportional to the square root of their invert frequencies.</p>
</blockquote>
<p><strong>Translation Language Modeling (TLM)</strong></p>
<p><img src="/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/tlm.png"></p>
<p>在预测一个 masked english word 的同时，不仅可以attend english context，也可以 attend franch translation.</p>
<p><strong>Cross-lingual Language Models</strong></p>
<p>如何使用这三种语言模型，CLM 和 MLM 在单语上进行训练。 TLM 在平行语料上训练。TLM 在使用时是联合 MLM 一起训练的，迭代优化两个目标函数。</p>
<blockquote>
<p>In this work, we consider cross-lingual language model pretraining with either CLM, MLM, or MLM used in combination with TLM. For the CLM and MLM objectives, we train the model with batches of 64 streams of continuous sentences composed of 256 tokens. At each iteration, a batch is composed of sentences coming from the same language, which is sampled from the distribution ${q_i}_{i=1…N}$ above, with α = 0.7. When TLM is used in combination with MLM, we alternate between these two objectives, and sample the language pairs with a similar approach.</p>
</blockquote>
<h1 id="CTNMT"><a href="#CTNMT" class="headerlink" title="CTNMT"></a>CTNMT</h1><p>paper: [Towards Making the Most of BERT in Neural Machine Translation</p>
<p>Jiacheng](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.05672">https://arxiv.org/abs/1908.05672</a>)</p>
<p>ByteDance 的一篇paper.</p>
<p>前人的研究中我们发现，BERT pretrian 对NMT几乎没有提升。作者认为其原因是，NMT 相对其他linguistic的任务，训练的steps会多很多，比如NMT一般是10万step，而 POS tagging只需要几百步。这使得在训练过程中，参数的更新太多导致 catastrophic forgetting problem. 也就是 BERT 训练得到的knowledge并不能给NMT代来提升。</p>
<p>于是乎，作者认为只是大家没有好好利用BERT而已，像我们这样搞, BERT还是能对NMT有提升的。然后提出了三种techniques:</p>
<p><strong>Asymptotic Distillation</strong>  </p>
<p><img src="/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/asymptotic_distill.png"></p>
<p>渐近蒸馏，主要是用来解决 catastrophic forgetting 这一问题的，和这篇paper “<a href>Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation</a>“ 相似，也是采用的 Elastic Weight Consolidation(EWC) 的方法，对weight采用MSE的约束。</p>
<p>$$L_{kd}=-||\hat h^{lm}-h_l||^2_2$$</p>
<p>$$L=\alpha\cdot L_{nmt}+(1-\alpha)\cdot L_{kd}$$</p>
<p>其中 $L_{kd}$ 是正则化项，$\hat h^{lm}$ 是经过BERT编码之后的 sentence embedding. $h_l$ 则是NMT的encoded之后的 sentence embedding. 作者都使用的最后一层的表示。在后续实验中，作者也测试了不同层的表示进行约束。</p>
<p><strong>Dynamic Switch</strong></p>
<p>动态开关。</p>
<p><img src="/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/switch.png"></p>
<p>就是GRU中的gate机制。</p>
<p>$$g = \sigma(Wh^{lm} + Uh^{nmt} + b)$$</p>
<p>$$h=g\odot h^{lm}+(1-g)\odot h^{nmt}$$</p>
<p><strong>Rate-scheduled learning</strong></p>
<p>slanted triangular learning, 斜三角学习率。最开始提出是在 <a href>ULMFT: Universal language model fine-tuning for text classificatio</a> 这篇论文中。</p>
<p>$$\theta_t=\theta_{t-1}-\eta\nabla_{\theta}L(\theta)$$</p>
<p>对 NMT 和 LM 对应的参数使用不同的学习率，但是都采用这种scheduled学习率.</p>
<p><strong>Result</strong></p>
<p><img src="/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/result.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-06-29T01:48:44.000Z" title="2019/6/29 上午9:48:44">2019-06-29</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/GAN/">GAN</a></span><span class="level-item">8 分钟读完 (大约1173个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/06/29/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-5-decoding/">从0开始GAN-5-NAT Decoding</a></h1><div class="content"><p>non-autoregressive decode 相关的paper：</p>
<ul>
<li><p><a href>Non-autoregressive neural machine translation. Gu et al. 2018 ICLR</a></p>
</li>
<li><p><a href>End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification</a>  </p>
</li>
<li><p><a href>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinemen</a>  </p>
</li>
</ul>
<h2 id="paper1"><a href="#paper1" class="headerlink" title="paper1"></a>paper1</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09324">Constant-Time Machine Translation with Conditional Masked Language Models</a></p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>把auto-regressive转换成non-autoregressive. 其做法是先确定一个target sentece的长度，然后可以看作是每一个time-step的分类任务了。这样decoder就是可并行的了。</p>
<h3 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h3><p>模型架构采用transformer的架构。  </p>
<p>原生的 Transformer:  </p>
<ul>
<li><p>source-language encoder: self-attention, 包括padding mask.  </p>
</li>
<li><p>translation model decoder  </p>
<ul>
<li><p>self-attention, 包括padding mask和 look ahead mask，用以mask掉future information.</p>
</li>
<li><p>interaction attention with enc_out, 包括 padding mask.</p>
</li>
</ul>
</li>
</ul>
<p>这篇paper中的 conditional mask language model(CMLM) 与transormer的区别在于 decoder 部分的self-attention去掉了 look ahead mask. 所以可以类似于 BERT 那样基于上下文来预测被 mask 的词，decoder 是 bi-directional.</p>
<h3 id="Decoding-with-Mask-Predict"><a href="#Decoding-with-Mask-Predict" class="headerlink" title="Decoding with Mask-Predict"></a>Decoding with Mask-Predict</h3><p>decoder 的具体操作是一个迭代的过程。</p>
<p>| src | Der Abzug der franzsischen Kampftruppen wurde am 20. November abgeschlossen .    |</p>
<p>| :—– | :—————————————————- |</p>
<p>|t=0|<strong>The withdrawal of French combat troops was completed on November 20th .</strong>|</p>
<p>|t=1|The <strong>departure of the French combat completed completed on</strong> 20 November .|</p>
<p>|t=2|The <strong>departure</strong> of the French combat completed <strong>completed</strong> on <strong>20 November</strong> .|</p>
<p>表中加粗的部分是被 mask 的。可以看到随着迭代进行，mask的词越来越少。</p>
<p>如何选择mask的词：  </p>
<ol>
<li><p>mask词的数量n: 基于一个递减的公式, $n=N\dfrac{T-t}{T}$. t 是迭代次数。  </p>
</li>
<li><p>mask哪些词呢：$Y^{(t)}_{mask}=argmin_i(p_i,n)$ $p_i$ 表示上一次prediction得到的每一个词的置信度，选择概率最低的 n 个词。</p>
</li>
</ol>
<p>基于 encoder_src, $Y_{obs}$ 对 mask token 进行预测:</p>
<p><img src="/2019/06/29/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-5-decoding/prediction.png"></p>
<h3 id="target-sequence-length"><a href="#target-sequence-length" class="headerlink" title="target sequence length"></a>target sequence length</h3><p>这中 non-Autoregressive 存在的一个大问题就是如何确定target sentence 的长度。在 auto-egressive 里面是根据 ${<eos>}$ 来确定句子长度的。</eos></p>
<p>针对这个问题，作者采用了类似于 BERT 中 CLS 的做法。使用了 $LENGTH$ 来预测sentence的长度，也是一个分类任务，这个 LENGTH 对应的词表应该就是长度~</p>
<p>作者选取 top b length，类似于 beam search. 然后选择 candidated b sentence 中概率最大的.</p>
<p>$$\dfrac{1}{N}\sum logp_i^{(T)}$$</p>
<h3 id="code-reading"><a href="#code-reading" class="headerlink" title="code reading"></a>code reading</h3><h2 id="paper2"><a href="#paper2" class="headerlink" title="paper2"></a>paper2</h2><p>Non-Autoregressive Neural Machine Translation</p>
<h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation:"></a>Motivation:</h3><p>现有的机器翻译模型在inference时，需要在生成前一个单词的基础上继续生成下一个单词，这种自回归的特性严重影响了推理的速度。</p>
<p>并且与训练阶段的不一致导致存在exposure bias。作者提出一个非自回归的方法，在infer阶段并行输出。</p>
<p>Exposure bias:</p>
<ul>
<li><p>training 阶段上一个token是ground truth</p>
</li>
<li><p>infer 阶段上一个token是生成得到的，这样自回归生成整个句子存在误差累积</p>
</li>
<li><p>两个阶段生成target的方式不一样，也就是 exposure bias.</p>
</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture:"></a>Model Architecture:</h3><ul>
<li><p>前一项表示基于监督学习来预测targets 句子的长度。在本文中作者使用了这个词 fertilities(生育能力) 来表示通过source句子通过encode之后所包含的知识.</p>
</li>
<li><p>后一项依旧是极大似然估计，也就是 independent cross-entropy losses on each output distribution。 但不同的是，在inference阶段也是可以并行的。</p>
</li>
</ul>
<p>这里有个疑问，在训练阶段会预测得到一个长度T，但是训练阶段时groud truth长度的，这个怎么解决？</p>
<p>这里在训练阶段显然需要长度与 ground truth 的target sentence长度一致，才能计算 word-wise corss entropy loss.</p>
<h3 id="Decoder-Stack"><a href="#Decoder-Stack" class="headerlink" title="Decoder Stack"></a>Decoder Stack</h3><p>1.decoder input</p>
<p>首先关于 decoder 的初始输入，在已有的模型中，训练阶段 decoder 的输入是 time-shifted target outputs，推理阶段是前面时间步预测的输出。</p>
<p>对于NAT模型，需要提前确定 target output 的长度，作者提出了两种方法：</p>
<ul>
<li><p>Copy source inputs uniformly</p>
</li>
<li><p>Copy source inputs using fertilities， 如上图中输入的每个时间步都有其对应的 fertility. 然后把source input按照其对应的次数copy到decoder的输入。</p>
</li>
</ul>
<p>2.Non-causal self-attention</p>
<p>因为不是自回归，也就是下一个词的生成并不依赖于previous的tokens，所以可以去掉transformer中decoder部分的cause-mask,也就是可以结合上下文的词，而不仅仅只是上文。</p>
<p>3.position attention</p>
<p>We also include an additional positional attention module in each decoder layer, which is a multi-head attention module with the same general attention mechanism used in other parts of the Transformer network. 为了强调位置信息。</p>
<p>Modeling fertility to tackle the multimodality problem</p>
<p>$P_F(f_{t’}|x_{1:T’}; \theta)$ 表示 fertility 在 t’ 时间步的概率分布，其是通过encoder顶层的 mlp + softmax 得到的。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-06-22T08:47:50.000Z" title="2019/6/22 下午4:47:50">2019-06-22</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/DRL/">DRL</a></span><span class="level-item">8 分钟读完 (大约1208个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/">UCL-DRL-04-policy gradient and Actor Critic Methods </a></h1><div class="content"><h2 id="policy-gradient"><a href="#policy-gradient" class="headerlink" title="policy gradient"></a>policy gradient</h2><h3 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h3><p><a target="_blank" rel="noopener" href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63">Deriving Policy Gradients and Implementing REINFORCE</a> 这篇博客详细推导了 policy gradients 的过程，虽然公式很多，但其实还算简单。</p>
<p>最终的结论就是，从公式(1)推导为公式(2):</p>
<p>$$J(\theta)=\mathbb{E}[\sum_{t=0}^{T-1}r_{t+1}|\pi_{\theta}]=\sum_{t=1}^{T-1}P(s_t, a_t|\tau)r_{t+1}\quad(1)$$</p>
<p>$$\nabla_{\theta}J(\theta)=\sum_{t=0}^{T-1}\nabla_{\theta}log\pi_{\theta}(a_t|s_t)(\sum_{t’=t+1}^T\gamma^{t’-t-1}\gamma_{t’})\quad(2)$$</p>
<p>其中 $Gt=\sum_{t’=t+1}^T\gamma^{t’-t-1}\gamma_{t’}$</p>
<p>公式(2)可简化为:  </p>
<p>$$\nabla_{\theta}J(\theta)=\sum_{t=0}^{T-1}\nabla_{\theta}log\pi_{\theta}(a_t|s_t)G_t$$</p>
<p>之后就是针对 policy gradient 的各种改进。接下来的公式参数是根据李宏毅老师课程来的。所以符号与上面的有差异。</p>
<h3 id="add-baseline"><a href="#add-baseline" class="headerlink" title="add baseline"></a>add baseline</h3><p>$$\nabla{\overline R}<em>{\theta}=\dfrac{1}{N}\sum</em>{n=1}^N\sum_{t=1}^{T_n}(R(\tau^n)-b)\nabla logp_{\theta}(a_t^n|s_t^n)$$</p>
<h3 id="assign-suitable-credit"><a href="#assign-suitable-credit" class="headerlink" title="assign suitable credit"></a>assign suitable credit</h3><p>$$\nabla{\overline R}<em>{\theta}=\dfrac{1}{N}\sum</em>{n=1}^N\sum_{t=1}^{T_n}(\sum_{t’=t}^{T_n}r_{t’}^n-b)\nabla logp_{\theta}(a_t^n|s_t^n)$$</p>
<h3 id="add-discount-factor"><a href="#add-discount-factor" class="headerlink" title="add discount factor"></a>add discount factor</h3><p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/policy_gradient.png"></p>
<h3 id="advantage-function"><a href="#advantage-function" class="headerlink" title="advantage function"></a>advantage function</h3><p>action $a_t$ 的好是相对的，而不是绝对的。它可以是 state-dependent.</p>
<p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/actor_critic.png"></p>
<h2 id="on-policy-and-off-policy"><a href="#on-policy-and-off-policy" class="headerlink" title="on-policy and off-policy"></a>on-policy and off-policy</h2><h3 id="importance-sampling"><a href="#importance-sampling" class="headerlink" title="importance sampling"></a>importance sampling</h3><p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/importance_sampling.png"></p>
<p>issue of importance sampling:</p>
<p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/issue_IS.png"></p>
<p>采用 importance sampling，会导致sample 得到的action的分布方差发生变化。因此要保证action的分布尽可能与</p>
<h3 id="on-policy-to-off-policy"><a href="#on-policy-to-off-policy" class="headerlink" title="on-policy to off-policy"></a>on-policy to off-policy</h3><p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/off_policy.png"></p>
<h3 id="PPO-proximal-policy-optimization"><a href="#PPO-proximal-policy-optimization" class="headerlink" title="PPO(proximal policy optimization)"></a>PPO(proximal policy optimization)</h3><p><strong>add constraint:</strong></p>
<p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/ppo.png"></p>
<p>$\theta, \theta’$ 并不是distribution，而是参数。那么这里的 $kl(\theta, \theta’)$ 到底是什么？</p>
<p>这里的kl divergence 实际上是行为上的差距，也就是 action 分布的距离。那这个意思就是我们还是得用 $\theta$ 去求出 action 的 distribution，但是我们不用去sample出样本了。可以继续需要用 $\theta’$ sample出来的样本，但是需要给reward乘以系数 $\dfrac{p_{\theta}(a_t|s_t)}{p_{\theta’}(a_t|s_t)}$.</p>
<p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/ppo_algorithm.png"></p>
<h3 id="PPO2"><a href="#PPO2" class="headerlink" title="PPO2"></a>PPO2</h3><p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/ppo2.png"></p>
<p>因为 $kl(\theta, \theta’)$ 的计算还是蛮复杂的，所以用ppo2来代替。方法也是很直接，用clip的方式代替原来的正则项。</p>
<p>总结一下ppo：  </p>
<ol>
<li><p>首先它是off-policy的，为什么将on-policy转换成off-policy呢，因为on-policy速度太慢了。在policy iteration的时候先sample尽可能多的(state, action)pairs，然后计算对应的reward，再基于policy gradient来更新policy的参数 $\theta$.这是一次迭代。再然后基于updated policy生成新的(state, action) pairs或者新的example吧，依次迭代。。。这个过程中，得到action的分布，然后sample得到尽可能多的actions，这一步是非常耗时的，而且每次迭代都需要重新生成样本。  </p>
</li>
<li><p>off-policy改进的就是搞一个近似于当前policy $\theta$ 的 $\theta’$. 用这个 $\theta’$ 去采样 $(state, action)$ 样本，这个 $\theta’$ 并不是随着 policy $\theta$ 的更新而更新的，所以它sample出来的样本可以用很久。  </p>
</li>
<li><p>但是policy $\theta$ 更新之后，其对应的 action 的分布也是变换的，这也是我们想要的。所以怎么减小 $\theta’$ 和 $\theta$ 对应的action分布的差异，就有了ppo公式的第一项，也就是 importance sampling.  </p>
</li>
</ol>
<p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/im_policy.png"></p>
<ol start="4">
<li>但是importance sampling得到的action的分布存在偏差（方差不一致)，所以需要尽可能保证 $\theta, \theta’$ 采样得到的action分布尽可能接近。于是有了ppo,ppo2的方法。</li>
</ol>
<h2 id="Critic"><a href="#Critic" class="headerlink" title="Critic"></a>Critic</h2><h3 id="state-value-function"><a href="#state-value-function" class="headerlink" title="state-value function"></a>state-value function</h3><p>$V_{\pi}(s)$ 表示的是当到达某个状态 s 之后，如果接下来一直按着策略 $\pi$ 来行动，能够获得的期望收益.</p>
<h3 id="action-value-function"><a href="#action-value-function" class="headerlink" title="action value function"></a>action value function</h3><p>$Q_{\pi}(s,a)$ 表示的是当达到某个状态 s 之后，如果强制采取行动 a ，接下来再按照策略 $\pi$ 来行动，能获得的期望收益。</p>
<p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/calue_funvtion.svg"></p>
<p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/action_value.svg"></p>
<p>显然地，状态价值函数和行动价值函数之间有关系:</p>
<p>$$V_{\pi}(s)=\sum_a\pi(a|s)Q_{\pi}(s,a)$$</p>
<h3 id="MC-v-s-TD"><a href="#MC-v-s-TD" class="headerlink" title="MC v.s. TD"></a>MC v.s. TD</h3><p><img src="/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/mvcstd.png"></p>
<p>sample同样的结果，采用 MC 和 TD最终计算的结果也是不一样的。</p>
<p>MC考虑的是，当前state $s_a$ 对未来的 state $s_b$ 可能也是有影响的. 所以MC实际上是要计算到整个游戏（episode）结束。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-06-22T07:59:37.000Z" title="2019/6/22 下午3:59:37">2019-06-22</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.140Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/GAN/">GAN</a></span><span class="level-item">11 分钟读完 (大约1608个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/">从0开始GAN-4-ScratchGAN</a></h1><div class="content"><h2 id="Training-Language-GANs-from-Scratch"><a href="#Training-Language-GANs-from-Scratch" class="headerlink" title="Training Language GANs from Scratch"></a>Training Language GANs from Scratch</h2><p>发现一个问题，目前看到language gans的相关paper大部分是Google，DeepMind的paper. 感觉是个深不见底的坑，弱渣哭了。。。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>我们知道language GAN非常难训练，主要是因为gradient estimation, optimization instability, and mode collapse等原因，这导致很多NLPer选择先基于maximum likelihood对模型进行预训练，然后在用language GAN进行fine-tune.作者认为这种 fine-tune 给模型带来的benefit并不clear，甚至会带来不好的效果。  </p>
<blockquote>
<p>关于mode collapse，李宏毅老师讲过，在对话生成时，模型总是倾向于生成“我不知道”，”我知道了”这样通用的没有太多sense的回复，其实就是属于mode collapse. 类似于图像领域，既要生成鼻子，又要生成嘴巴，但是模型会倾向于生成一个居中的distribution来模拟这两个distribution。  </p>
</blockquote>
<blockquote>
<p>关于gradient estimator，是因为对于离散的数据，其gradients的方差会很大。</p>
</blockquote>
<p>[13-16]就是先使用ML预训练模型，然后在此基础上adversarial fine-tune.[17-18]则说明了 “that the best-performing GANs tend to stay close to the solution given by maximum-likelihood training”.</p>
<p>所以作者为了证明language GAN真的能work，就from scratch训练了一个language GAN, 对，没有预训练。作者认为从头训练好language GAN的核心技术是 <strong>large batch sizes, dense rewards and discriminator regularization</strong>.</p>
<p>本文的贡献：  </p>
<ol>
<li><p>从头训练一个language GAN能达到基于ML方法的unconditional text generation.  </p>
</li>
<li><p>证明 <strong>large batch sizes, dense rewards and discriminator regularization</strong> 对于训练language GAN的重要性。  </p>
</li>
<li><p>作者对文本生成模型的evaluation提出了一些性的拓展，能充分挖掘生成的language更多的特性。比如：</p>
<ul>
<li><p>BLEU and Self-BLEU [19] capture basic local consistency.    </p>
</li>
<li><p>The Frechet Distance metric [17] captures global consistency and semantic information.    </p>
</li>
<li><p>Language and Reverse Language model scores [18] across various softmax temperatures to capture the diversity-quality trade-off.    </p>
</li>
<li><p>Nearest neighbor analysis in embedding and data space provide evidence that our model is not trivially overfitting.   </p>
</li>
</ul>
</li>
</ol>
<h3 id="Generative-Models-of-Text"><a href="#Generative-Models-of-Text" class="headerlink" title="Generative Models of Text"></a>Generative Models of Text</h3><p>生成模型的本质就是对unknown data distribution进行建模，也就是学习模型 p(x|y) 的参数。在传统的机器学习里面，我们认为模型 p(x|y) 的分布就是多维高斯正态分布，然后用EM算法去学习得到参数。在基于neural network的自然语言处理领域，对于 $x=[x_1,..,x_T]$， $p_{\theta}(x_t|x_1,…,x_{t-1})$ 也可以看作是学习这样一个distribution，只不过模型的参数不是高斯正态分布这么简单，而是基于network来模拟的。同样序列特性使得其非常适合使用自回归模型进行建模:</p>
<p>$$p_{\theta}=\prod_{t=1}^Tp_{\theta}(x_t|x_1,…,x_{t-1})$$</p>
<h3 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h3><p>一旦模型建立好了，接下来就是训练模型。最常用的方法就是使用极大似然估计 maximum likelihood estimation(MLE).</p>
<p>$$\argmax_{\theta}\mathbb{E}<em>{p^* (x)}logp</em>{\theta}(x)$$</p>
<p>关于 maximum likelihood 是否是最优解，这篇paper有讨论[9]。</p>
<h3 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h3><p><img src="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/gans.png"></p>
<p>前面seqgan也说过自回归模型中 $p_{\theta}=\prod_{t=1}^Tp_{\theta}(x_t|x_1,…,x_{t-1})$的过程有个sample的操作，这是不可导的。针对这个问题，有三种解决方法：  </p>
<ul>
<li><p>高方差，无偏估计的 reinforce[28]. 基于大数定律的条件下，去sample更多的example，来模拟 $p(y_t|s_t)$ 的分布，然后基于policy gradient去优化这个distribution，这使得速度很慢。  </p>
</li>
<li><p>低方差，有偏估计的 gumbel-softmax trick[29-30].  </p>
</li>
<li><p>other continuous relaxations[11].  </p>
</li>
</ul>
<h3 id="Learning-Signals"><a href="#Learning-Signals" class="headerlink" title="Learning Signals"></a>Learning Signals</h3><p>对于generator的训练，作者采用了基于 REINFORCE 的方法:</p>
<p><img src="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/reinforce.png"></p>
<p>其中同 MaliGAN[15] 一样，设置 $R(x)=\dfrac{p^* (x)}{p_{\theta}(x)}$, 这样等效于 MLE 估计。</p>
<p><img src="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/mailgan.png"></p>
<p>基于MLE eatimator的梯度更新可以看作是reinforce的一个spacial case.区别在于language gans的reward是可以学习的，也就是discriminator是不断更新的。可学习的discriminator的效果已经被证明过了[34].</p>
<p>如果learned reward能够提供相比MLE loss更光滑的信号，那么discriminator就能提供更多有意义的signal，甚至training data没有cover的distribution.</p>
<p>同时，discriminator是可以ensemble的，使用更多的domain knowledge.这样能学习到更多的信息。</p>
<h3 id="Training-Language-GANs-from-Scratch-1"><a href="#Training-Language-GANs-from-Scratch-1" class="headerlink" title="Training Language GANs from Scratch"></a>Training Language GANs from Scratch</h3><p>作者通过实验验证，要训好一个language gans，所需要的是：  </p>
<ul>
<li><p>a recurrent discriminator used to provide dense rewards at each time step  </p>
</li>
<li><p>large batches for variance reduction  </p>
</li>
<li><p>discriminator regularization</p>
</li>
</ul>
<p><img src="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/scratchgans.png"></p>
<h4 id="dense-rewards"><a href="#dense-rewards" class="headerlink" title="dense rewards"></a>dense rewards</h4><p>判别器能够判别generated sentence和real sentence，但是对于不完整的句子，就没办法去判断。这就造成，如果generated sentence很容易就被判断为fake，那么在fix discriminator训练generator时，生成器无法获得有意义的信号，也就是梯度为0吧。</p>
<p>为了避免这种情况，作者采用了 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.07736">MaskGAN</a>[32] 的方法:  </p>
<h4 id="maskGAN"><a href="#maskGAN" class="headerlink" title="maskGAN"></a>maskGAN</h4><p><img src="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/maskgan.png"></p>
<p>maskGAN是一种 actor-critic 方法，利用类似于完形填空的形式，只需要生成被挖去的词，就能对整个sentence进行判别，并计算reward，这样得到的reward相比sentence中的每一个词都是生成的，其variance会小很多。</p>
<p>具体做法是：</p>
<ol>
<li><p>生成器是 seq2seq 的形式，输入sequence $x=(x_1,…,x_T)$. 通过 binary mask $m=(m_1,…,m_T)$ 得到 $m(x)$.  </p>
</li>
<li><p>根据 m(x) 来生成得到完整的 generated examples $\hat x=(\hat x_1, \hat x_2,…,\hat x_T)$.</p>
</li>
</ol>
<p><img src="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/maskgan_gen.png"></p>
<ol start="3">
<li><p>这里生成的时候参考上图，如果当前time-step被mask了，则需要用到上一个time-step生成的词，如果没有被mask，就直接使用当前词，类似于teacher-forcing.  </p>
</li>
<li><p>判别器就是计算每一个词为真的概率，注意这里判别器的输入也有 m(x)，其原因是让模型更好的识别生成的sentence中，哪一个是之前被mask了的。  </p>
</li>
</ol>
<p>$$D_{\phi}(\tilde x_t|\tilde x_{0:T}, m(x)) = P(\tilde x_t=x_t^{real}|\tilde x_{0:T}, m(x))$$</p>
<ol start="5">
<li>reward 的计算：  </li>
</ol>
<p>$$r_t=logD_{\phi}(\tilde x_t|\tilde x_{0:T}, m(x))$$</p>
<h4 id="Large-Batch-Sizes-for-Variance-Reduction"><a href="#Large-Batch-Sizes-for-Variance-Reduction" class="headerlink" title="Large Batch Sizes for Variance Reduction"></a>Large Batch Sizes for Variance Reduction</h4><p>reference:</p>
<p>[9] How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv  </p>
<p>[12-16]    </p>
<p>[17-18]  </p>
<p>[32] Maskgan: Better text generation via filling in the ____  </p>
<p>[34]</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/4/">上一页</a></div><div class="pagination-next"><a href="/page/6/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/4/">4</a></li><li><a class="pagination-link is-current" href="/page/5/">5</a></li><li><a class="pagination-link" href="/page/6/">6</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/25/">25</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">121</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>