<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="潘晓榭"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘晓榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="潘晓榭"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="潘晓榭"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="潘晓榭"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-14T09:35:02.000Z" title="2019/1/14 下午5:35:02">2019-01-14</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/DRL/">DRL</a></span><span class="level-item">10 分钟读完 (大约1556个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/01/14/UCL-DRL-02-MDP/">UCL-DRL-02-MDP</a></h1><div class="content"><h2 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h2><h3 id="Introduction-to-MDPs"><a href="#Introduction-to-MDPs" class="headerlink" title="Introduction to MDPs"></a>Introduction to MDPs</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/02.png"></p>
<p>马尔可夫决策过程：   </p>
<ul>
<li><p>环境完全可观测  </p>
</li>
<li><p>当前状态能完整的描述这个过程  </p>
</li>
<li><p>绝大多数 RL 问题都可以形式化为 MDPs:  </p>
<ul>
<li><p>优化控制问题  </p>
</li>
<li><p>部分可观测问题可转化为 MDPs  </p>
</li>
<li><p>Bandits（老虎机问题）</p>
</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21388070">multi-armed bandits problems</a></p>
<h3 id="Markov-property"><a href="#Markov-property" class="headerlink" title="Markov property"></a>Markov property</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/03.png"></p>
<p>一旦当前状态确定后，所有的历史信息都可以扔掉了。这个状态足够去预测 future.</p>
<h3 id="state-transition-matrix"><a href="#state-transition-matrix" class="headerlink" title="state transition matrix"></a>state transition matrix</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/04.png"></p>
<p>状态转移矩阵定义了所有的从某一个状态到另一个状态的概率。所以每一行相加为 1.</p>
<h3 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/05.png"></p>
<p>马尔可夫链可以看做是 一个 tuple(S, P)</p>
<ul>
<li><p>S 是有穷的状态的集合  </p>
</li>
<li><p>P 是状态转移矩阵  </p>
</li>
</ul>
<p>$$P_{ss’}=P[S_{t+1}=s’|S_t=s]$$</p>
<p>example:</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/06.png"></p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/09.png"></p>
<p>简单的马尔可夫链，没有 reward, action 等。</p>
<h2 id="Markov-reward-process"><a href="#Markov-reward-process" class="headerlink" title="Markov reward process"></a>Markov reward process</h2><p><img src="/2019/01/14/UCL-DRL-02-MDP/10.png"></p>
<p>在 Markov chain 基础上增加了 reward function.</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/11.png"></p>
<h3 id="return"><a href="#return" class="headerlink" title="return"></a>return</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/07.png"></p>
<p>折扣因子 $\gamma \in [0,1]$   </p>
<p>越接近0， 意味着越短视  </p>
<p>越接近1, 意味着有远见  </p>
<p><strong>Why discount</strong></p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/08.png"></p>
<ul>
<li><p>数值计算方便  </p>
</li>
<li><p>避免无限循环  </p>
</li>
<li><p>对未来的不确定性  </p>
</li>
<li><p>如果reward是经济的，即刻的reward能获得更多的利益相比之后的reward  </p>
</li>
<li><p>动物/人更倾向于即刻的奖励  </p>
</li>
<li><p>有时候也会使用 undiscounted 马尔可夫奖励过程</p>
</li>
</ul>
<h3 id="value-function"><a href="#value-function" class="headerlink" title="value function"></a>value function</h3><p>因为 future 有很多中不确定性，所以需要采样. sample 得到的 $G_t$ 的期望，也就是 value</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/12.png"></p>
<p>$$v(s)=E[G_t|S_t=s]$$</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/13.png"></p>
<p>上图中 $v_1$ 应该是 $g_1$.</p>
<p>$$G_1=R_2+\gamma R_3+…+\gamma^{T-2}R_T$$</p>
<p>也就是计算从 t 时刻开始到结束，整个过程可能的奖励值。因为未来可能有很多中情况，所以需要 sample. 印象中，好像 alpha go 就是用的蒙特卡洛模拟。</p>
<h3 id="Bellman-Equation-for-MRPs"><a href="#Bellman-Equation-for-MRPs" class="headerlink" title="Bellman Equation for MRPs"></a>Bellman Equation for MRPs</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/14.png"></p>
<p>动态规划。。</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/15.png"></p>
<p>这里容易理解错的一点就是： $R_s$ 实际上就是 $R_{t+1}$, s 表示 v(s) 中的当前状态。</p>
<p>$P_{ss’}$ 是状态转移的概率， 从 s 到 后继状态 s’ 的概率。</p>
<p>所以：</p>
<p>$$\gamma v(S_{t+1}|S_t=s) = \gamma\sum_{s’\in S} P_{ss’}v(s’)$$</p>
<p>转换成矩阵形式：</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/16.png"></p>
<p>事实上: 系数矩阵 P 就是 状态转移矩阵，有 n 中状态，那就是 $n\times n$ matrix.</p>
<h3 id="solving-bellman-problems"><a href="#solving-bellman-problems" class="headerlink" title="solving bellman problems"></a>solving bellman problems</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/17.png"></p>
<p>直接计算是 $O(n^3)$, 因为 n 个状态，P 的计算就是 $O(n^2)$</p>
<p>对于很复杂的 MRPs，可以采用：  </p>
<ul>
<li><p>dynamic programming  </p>
</li>
<li><p>monte-carlo evalution  </p>
</li>
<li><p>temporal-difference learning  </p>
</li>
</ul>
<h2 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h2><p><img src="/2019/01/14/UCL-DRL-02-MDP/19.png"></p>
<p>想对于 Markov reward process 增加了 decision.</p>
<p>$P_{ss’}^a$ 是状态转移矩阵，但是这里的状态之间的转移概率不再是确定好了的。而是取决于 当前状态，以及当前状态下的 action.</p>
<p>$$P_{ss’}^a=P[S_{t+1=s’}|S_t=s, A_t=a]$$</p>
<p>因此 R reward function：</p>
<p>$$R_s^a=E[R_{t+1}|S_t=s,A_t=a]$$</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/20.png"></p>
<p>可以发现，与之前的区别是，state 到 state 之间是通过 action 决定的。比如 第一个 state 下，可以是 study or facebook.</p>
<h3 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/21.png"></p>
<p>这个决策就是想对于 Markov reward process 多出来的一部分，你需要自己去做决策。只不过在每一个状态下，其决策也是一个 distribution</p>
<p>$$\pi(a|s)=P[A_t=a|S_t=s]$$</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/22.png"></p>
<p>事实上 Markov decision process 也可以转换成 Markov reward process, 从状态 s 到 s’ 的概率：</p>
<p>$$P_{s,s’}^{\pi}=\sum_{a\in A}\pi(a|s)P_{ss’}^a$$</p>
<h3 id="value-function-1"><a href="#value-function-1" class="headerlink" title="value function"></a>value function</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/23.png"></p>
<p>value 是当前状态 s 下的 reward $G_t$ 的期望。</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/24.png"></p>
<h3 id="Bellman-expectation-Equation"><a href="#Bellman-expectation-Equation" class="headerlink" title="Bellman expectation Equation"></a>Bellman expectation Equation</h3><h4 id="v-pi-基于-q-pi-的表示"><a href="#v-pi-基于-q-pi-的表示" class="headerlink" title="$v_{\pi}$ 基于 $q_{\pi}$ 的表示"></a>$v_{\pi}$ 基于 $q_{\pi}$ 的表示</h4><p><img src="/2019/01/14/UCL-DRL-02-MDP/25.png"></p>
<p>state-value $v_{\pi}$ 是 action-value $q_{\pi}$ 基于 action 的期望。</p>
<p>$$v_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s, a)\quad \text{(1)}$$</p>
<h4 id="q-pi-基于-v-pi-的表示"><a href="#q-pi-基于-v-pi-的表示" class="headerlink" title="$q_{\pi}$ 基于 $v_{\pi}$ 的表示"></a>$q_{\pi}$ 基于 $v_{\pi}$ 的表示</h4><p><img src="/2019/01/14/UCL-DRL-02-MDP/26.png"></p>
<p>action-value：</p>
<p>$$q_{\pi}(s,a)=R_s^a+\gamma\sum_{s’\in S}P_{ss’}^av_{\pi}(s’)\quad \text{(2)}$$</p>
<p>从状态 s 到 s’， 基于 statetransition probability matrix $P_{ss’}^a$.</p>
<h4 id="v-pi-基于-v-pi-的表示"><a href="#v-pi-基于-v-pi-的表示" class="headerlink" title="$v_{\pi}$ 基于 $v_{\pi}$  的表示"></a>$v_{\pi}$ 基于 $v_{\pi}$  的表示</h4><p><img src="/2019/01/14/UCL-DRL-02-MDP/27.png"></p>
<p>将 （2）带入（1）可得：</p>
<p>$$v_{\pi}(s)=\sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s’\in S}P_{ss’}^av_{\pi}(s’))\quad \text{(3)}$$</p>
<p>这里得到的是 state-value 的表达式。第二个类和符号是 从状态 s 到 s’ 所有可能的后继状态 s’。第一个类和符号是 状态 s 下所有可能的 action.  </p>
<p>这里的类和都是表示期望。</p>
<h4 id="q-pi-基于-q-pi-的表示"><a href="#q-pi-基于-q-pi-的表示" class="headerlink" title="$q_{\pi}$ 基于 $q_{\pi}$  的表示"></a>$q_{\pi}$ 基于 $q_{\pi}$  的表示</h4><p><img src="/2019/01/14/UCL-DRL-02-MDP/28.png"></p>
<p>将 （1） 带入 （2）可得：</p>
<p>$$q_{\pi}(s,a) = R_s^a+\gamma\sum_{s’\in S}P_{ss’}^a\sum_{a\in A}\pi(a’|s’)q_{\pi}(s’,a’)$$</p>
<p>这里得到的是 action-value 的表达式。第一个类和符号是在 action a 下，所有可能的后继状态 s’ 的类和。i第二个类和符号是 后继状态 s’ 下所有的 action 的类和。</p>
<p>example：</p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/29.png"></p>
<h4 id="Matrix-form"><a href="#Matrix-form" class="headerlink" title="Matrix form"></a>Matrix form</h4><p><img src="/2019/01/14/UCL-DRL-02-MDP/30.png"></p>
<p>类似于前面 Markov reward process 的表示，但是这里的状态转移矩阵变成了 $P^{\pi}$, reward 变成了 $R^{\pi}$.</p>
<p>个人理解： Markov decision process 与 reward process 的区别在于原本在一个 state s 到下一个 state s’ 的概率是给定了的。但是在 decision process，并不存在这个概率，而是取决于 action，而在当前 state s 下，每一个 action 的概率取决于 policy $\pi$.</p>
<h3 id="optimal-value-function"><a href="#optimal-value-function" class="headerlink" title="optimal value function"></a>optimal value function</h3><p><img src="/2019/01/14/UCL-DRL-02-MDP/31.png"></p>
<h4 id="optimal-policy"><a href="#optimal-policy" class="headerlink" title="optimal policy"></a>optimal policy</h4><p><img src="/2019/01/14/UCL-DRL-02-MDP/32.png"></p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/33.png"></p>
<h4 id="Solving-the-Bellman-Optimality-Equation"><a href="#Solving-the-Bellman-Optimality-Equation" class="headerlink" title="Solving the Bellman Optimality Equation"></a>Solving the Bellman Optimality Equation</h4><p><img src="/2019/01/14/UCL-DRL-02-MDP/34.png"></p>
<p>最优 policy 就是找到 $v_{* }(s)$ 和 $q_{* }(s,a)$</p>
<p>推导过程与前面类似，通过 bellman optimally equation 得到：</p>
<p>$$v_*(s)=max_{a}q_*(s,a)$$</p>
<p>$$q_*(s,a)=R_s^a+\gamma\sum_{s’\in S}P_{ss’}^av_*(s’)$$</p>
<p>$$v_*(s)={max}<em>{a}(R_s^a+\gamma\sum</em>{s’\in S}P_{ss’}^av_*(s’))$$</p>
<p>$$q_*(s,a)=R_s^a+\gamma\sum_{s’\in S}P_{ss’}^aq_*(s’,a’)$$</p>
<p><strong>state-value function optimal</strong></p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/38.png"></p>
<p><strong>action-value function optimal</strong></p>
<p><img src="/2019/01/14/UCL-DRL-02-MDP/37.png"></p>
<h2 id="Extension-to-MDP"><a href="#Extension-to-MDP" class="headerlink" title="Extension to MDP"></a>Extension to MDP</h2><p><img src="/2019/01/14/UCL-DRL-02-MDP/35.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-14T05:49:21.000Z" title="2019/1/14 下午1:49:21">2019-01-14</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.159Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/DRL/">DRL</a></span><span class="level-item">4 分钟读完 (大约635个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/01/14/UCL-DRL-01-introduction/">UCL-DRL-01-introduction</a></h1><div class="content"><h2 id="About-reinforcement-learning"><a href="#About-reinforcement-learning" class="headerlink" title="About reinforcement learning"></a>About reinforcement learning</h2><p>强化学习的特性：</p>
<p><img src="/2019/01/14/UCL-DRL-01-introduction/01.png"></p>
<p>不同于有监督学习，没有 supervisor, 只有一个 reward signal.</p>
<h2 id="The-reinforcement-learning-problem"><a href="#The-reinforcement-learning-problem" class="headerlink" title="The reinforcement learning problem"></a>The reinforcement learning problem</h2><h3 id="reward"><a href="#reward" class="headerlink" title="reward"></a>reward</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/02.png"></p>
<p>强化学习就是基于 奖励假设(reward hypothesis).</p>
<p><img src="/2019/01/14/UCL-DRL-01-introduction/03.png"></p>
<p>决策的选择是为了获得更大的 future reward.</p>
<h3 id="environments"><a href="#environments" class="headerlink" title="environments"></a>environments</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/04.png"></p>
<h3 id="state"><a href="#state" class="headerlink" title="state"></a>state</h3><h4 id="history-and-state"><a href="#history-and-state" class="headerlink" title="history and state"></a>history and state</h4><p><img src="/2019/01/14/UCL-DRL-01-introduction/05.png"></p>
<p>state 是 history 的函数：</p>
<p>$$S_t = f(H_t)$$</p>
<h4 id="environment-state"><a href="#environment-state" class="headerlink" title="environment state"></a>environment state</h4><p><img src="/2019/01/14/UCL-DRL-01-introduction/06.png"></p>
<p>环境状态对于 agent 可以是可见的，也可以是不可见的。即使是可见的，也不一定是有用的信息。</p>
<h4 id="Agent-state"><a href="#Agent-state" class="headerlink" title="Agent state"></a>Agent state</h4><p><img src="/2019/01/14/UCL-DRL-01-introduction/07.png"></p>
<p>代理状态就是 agent 的中间表示。</p>
<p><img src="/2019/01/14/UCL-DRL-01-introduction/08.png"></p>
<p>information state 又称 Markov state.</p>
<p>看做是马尔可夫链，当前状态包含了过去所有的信息。那么之前的 history 就可以扔掉了。</p>
<h4 id="Markov-decision-process"><a href="#Markov-decision-process" class="headerlink" title="Markov decision process"></a>Markov decision process</h4><p><img src="/2019/01/14/UCL-DRL-01-introduction/09.png"></p>
<p>马尔可夫决策过程条件：环境 state 与 agent state 一样    </p>
<p>Agent state = environment state = information state</p>
<h4 id="partially-observable-environments-POMDP"><a href="#partially-observable-environments-POMDP" class="headerlink" title="partially observable environments(POMDP)"></a>partially observable environments(POMDP)</h4><p><img src="/2019/01/14/UCL-DRL-01-introduction/10.png"></p>
<p>agent 并不能直接观察环境：  </p>
<ul>
<li><p>机器人具有摄像头的视觉信息，但不知道自己的绝对位置  </p>
</li>
<li><p>trading agent 只能观察到当前价格  </p>
</li>
<li><p>扑克牌选手只能看到公开的卡牌  </p>
</li>
</ul>
<p>agent state $\ne$ environment state</p>
<p>agent 必须重新构建自己的状态表示 $S_t^a$:</p>
<p>比如循环神经网络就是 POMDP</p>
<p>$$S_t^a=tanh(s_{t-1}^aW_s+O_tW_o)$$</p>
<h2 id="Inside-An-RL-Agent"><a href="#Inside-An-RL-Agent" class="headerlink" title="Inside An RL Agent"></a>Inside An RL Agent</h2><p>在一个 agent 内部具体有什么呢?我们怎么去定义一个 agent:</p>
<p><img src="/2019/01/14/UCL-DRL-01-introduction/11.png"></p>
<ul>
<li><p>policy: agent’s behaviour function  </p>
</li>
<li><p>value function: how good is each state and/or action  </p>
</li>
<li><p>model: agent’s representation of the environment</p>
</li>
</ul>
<h3 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/12.png"></p>
<p>策略 policy： 就是将 state 映射到 action.</p>
<h3 id="value-function"><a href="#value-function" class="headerlink" title="value function"></a>value function</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/13.png"></p>
<p>如何设计 value function 来计算 future reward 感觉是个难度呀～</p>
<h3 id="model"><a href="#model" class="headerlink" title="model"></a>model</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/15.png"></p>
<p>模型：用来预测环境如何变化，也就是模拟环境吧。比如 RNN 模型，就是用神经网络模拟序列变化。</p>
<h2 id="Problems-with-Reinforcement-Learning"><a href="#Problems-with-Reinforcement-Learning" class="headerlink" title="Problems with Reinforcement Learning"></a>Problems with Reinforcement Learning</h2><h3 id="Learning-and-planning"><a href="#Learning-and-planning" class="headerlink" title="Learning and planning"></a>Learning and planning</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/14.png"></p>
<p>学习和规划的区别  </p>
<ul>
<li><p>environment 是否 known  </p>
</li>
<li><p>agent 与 environment 是否有 interaction  </p>
</li>
</ul>
<h3 id="Exploration-and-Exploitation"><a href="#Exploration-and-Exploitation" class="headerlink" title="Exploration and Exploitation"></a>Exploration and Exploitation</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/16.png"></p>
<p>探索与开发：之前在三星听在线学习的讲座时，通过 多臂老虎机 和 在线广告 讨论过这个问题</p>
<ul>
<li><p>Exploration（探索） finds more information about the environment  </p>
</li>
<li><p>Exploitation（开发） exploits known information to maximise reward  </p>
</li>
<li><p>It is usually important to explore as well as exploit</p>
</li>
</ul>
<p>这是一个需要权衡或博弈的问题。</p>
<p><img src="/2019/01/14/UCL-DRL-01-introduction/17.png"></p>
<h3 id="prediction-and-control"><a href="#prediction-and-control" class="headerlink" title="prediction and control"></a>prediction and control</h3><p><img src="/2019/01/14/UCL-DRL-01-introduction/18.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-09T12:28:36.000Z" title="2019/1/9 下午8:28:36">2019-01-09</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a></span><span class="level-item">5 分钟读完 (大约701个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/01/09/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/">邓公数据结构与算法4-栈和队列</a></h1><div class="content"><p>栈和队列 都是线性结构的特例，因此其实现都可以用 向量或列表 来实现。</p>
<h2 id="STL-中-stack-的接口使用"><a href="#STL-中-stack-的接口使用" class="headerlink" title="STL 中 stack 的接口使用"></a>STL 中 stack 的接口使用</h2><h2 id="栈的使用"><a href="#栈的使用" class="headerlink" title="栈的使用"></a>栈的使用</h2><h3 id="栈与递归"><a href="#栈与递归" class="headerlink" title="栈与递归"></a>栈与递归</h3><p>递归算法所需的空间量，主要决定于最大递归深度。在达到这一深度的时刻，同时活跃的递归实例最多。</p>
<p>那么操作系统是如何实现函数（递归）调用的呢？如何同时记录调用与被调用函数（递归）实例之间的关系？如何实现函数（递归）调用的返回？又是如何维护同时活跃的所有函数（递归）实例的？</p>
<p>这些问题，都归结于栈。[课本 88 页]</p>
<h3 id="场景一：逆序输出"><a href="#场景一：逆序输出" class="headerlink" title="场景一：逆序输出"></a>场景一：逆序输出</h3><p>逆序输出：conversion, 输出次序与处理过程颠倒，递归深度和输出长度不易预知。</p>
<h4 id="进制转换"><a href="#进制转换" class="headerlink" title="进制转换"></a>进制转换</h4><h3 id="场景二：递归嵌套"><a href="#场景二：递归嵌套" class="headerlink" title="场景二：递归嵌套"></a>场景二：递归嵌套</h3><p>具有自相似性的问题多可嵌套的递归描述，但因分支位置和嵌套深度并不固定，其递归算法的复杂度不易控制。栈结构及其操作天然的具有递归嵌套性，故可以高效的解决这类问题。</p>
<h4 id="括号匹配"><a href="#括号匹配" class="headerlink" title="括号匹配"></a>括号匹配</h4><p>课本[92页]</p>
<p>算法其实很简单，主要是思考的过程。使用减治 和 分治 都不能解决这个问题。  </p>
<p><img src="/2019/01/09/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/01.png"></p>
<p>从大规模问题到小规模问题并不完全等效。也就是分成子问题之后，不满足条件的情况下，合成大问题却可以满足情况。</p>
<p><img src="/2019/01/09/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/02.png"></p>
<p>用 栈 真的太合适了。而且可以推广到多种括号的情况。</p>
<h4 id="栈混洗"><a href="#栈混洗" class="headerlink" title="栈混洗"></a>栈混洗</h4><p>借助一个中间 栈， 将 栈A 的数转移到 栈B 中去。称作 栈混洗(stack permutation).</p>
<blockquote>
<p>在 pytorch 中根据 index 进行重新排列 permutation，实际上也是用的 栈 对吧？ <code>index_select</code></p>
</blockquote>
<p><img src="/2019/01/09/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/04.png"></p>
<p>n 个数的栈混洗的总数， 恰好是著名的 catalan 数</p>
<p>$$\dfrac{2n!}{(n+1)!n!}$$</p>
<p>怎么甄别出哪些不是栈混洗的序列呢？</p>
<p><img src="/2019/01/09/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/03.png"></p>
<p>这里有一个禁止的情况出现。对于任何三个互异的数</p>
<p>$1\le i &lt; i &lt; j &lt; k \le n$ 出现 $k…,i,…,j$ 则必非为栈混洗。</p>
<p>怎么判断一个序列 B 是否是另一个序列 A 的 栈混洗？ 通过 栈 能实现，线性复杂度的算法。</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/became_a_wolf/article/details/49002593">https://blog.csdn.net/became_a_wolf/article/details/49002593</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Inkblots/p/4950331.html">https://www.cnblogs.com/Inkblots/p/4950331.html</a></p>
</li>
</ul>
<h4 id="延时"><a href="#延时" class="headerlink" title="延时"></a>延时</h4><h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-09T00:44:13.000Z" title="2019/1/9 上午8:44:13">2019-01-09</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/">GAN</a></span><span class="level-item">20 分钟读完 (大约2930个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/">论文笔记-GAN tutorial NIPS 2016</a></h1><div class="content"><h1 id="为什么要学习-GAN？"><a href="#为什么要学习-GAN？" class="headerlink" title="为什么要学习 GAN？"></a>为什么要学习 GAN？</h1><ul>
<li>High-dimensional probability distributions, 从高维概率分布中训练和采样的生成模型具有很强的能力来表示高维概率分布。</li>
</ul>
<ul>
<li>Reinforcement learning. 和强化学习结合。</li>
</ul>
<ul>
<li>Missing data. 生成模型能有效的利用无标签数据，也就是半监督学习 semi-supervised learning。</li>
</ul>
<h1 id="生成模型如何工作的"><a href="#生成模型如何工作的" class="headerlink" title="生成模型如何工作的"></a>生成模型如何工作的</h1><h2 id="Maximum-likehood-estimation"><a href="#Maximum-likehood-estimation" class="headerlink" title="Maximum likehood estimation"></a>Maximum likehood estimation</h2><p>极大似然估计就是在给定数据集的情况下，通过合理的模型（比如语言模型）计算相应的似然（概率），使得这个概率最大的情况下，估计模型的参数。</p>
<p>$$\sum_i^mp_{\text{model}}(x^{(i)}; \theta)$$</p>
<p>m 是样本数量。</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/05.png"></p>
<p>将上诉概率转换到 log 空间（log 函数是单调递增的），可以将乘积转换为相加，简化了计算。</p>
<p>同样的，也可以把上诉似然估计的极大化看做是最小化 KL 散度（或者交叉熵）。这样一来，相当于把无监督问题转换成了有监督问题。（不知理解的是否正确？）</p>
<h3 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h3><p>熵是信息量 $log{\dfrac{1}{p(i)}}$ (概率越大信息量越少)。</p>
<p>p 是真实分布， q 是非真实分布。  </p>
<p>交叉熵是用 q 分布来估计 p 分布所需要消除的代价 cost:</p>
<p>$$H(p,q) = \sum_ip(i)log{\dfrac{1}{q(i)}}$$</p>
<p>用真实分布 p 估计真实分布所需要的 cost:</p>
<p>$$H(p) = \sum_ip(i)log{\dfrac{1}{p(i)}}$$</p>
<p>从这里也能看出，当概率 p 为 1 时，所需要的 cost 就为 0 了。</p>
<p>相对熵，又称 KL 散度(Kullback–Leibler divergence)，就是指上诉两者所需要消耗的 cost 的差值：</p>
<p>$$D(p||q) = H(p,q)-H(p) =  \sum_ip(i)log{\dfrac{p(i)}{q(i)}}$$</p>
<h1 id="GAN-是如何工作的？"><a href="#GAN-是如何工作的？" class="headerlink" title="GAN 是如何工作的？"></a>GAN 是如何工作的？</h1><h2 id="GAN-框架"><a href="#GAN-框架" class="headerlink" title="GAN 框架"></a>GAN 框架</h2><p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/01.png"></p>
<ul>
<li><p>判别器 discriminator  </p>
</li>
<li><p>生成器 gererator  </p>
</li>
</ul>
<p>在左边场景，判别器学习如何分别样本是 real or fake. 所以左边的输入是 half real and half fake.</p>
<p>在右边的v场景，判别器和生成器都参与了。G 用来生成 G(z), D 用来判别 G(z) 是 real or fake.</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/02.png"></p>
<p>结构化概率模型 <a target="_blank" rel="noopener" href="http://www.deeplearningbook.org/contents/graphical_models.html">Structured Probabilistic Modelsfor Deep Learning</a></p>
<p>containing latent variables z and observed variables x.  </p>
<p>z 是需要学习的隐藏变量，x 是可观察到的变量。</p>
<p>判别器 D 的输入是 x，其需要学习的参数是 $\theta^{(D)}$. 生成器 G 的输入是 z, 其需要学习的参数是 $\theta^{(G)}$.  </p>
<p>两个玩家都有各自的损失函数。 $J^{(D)}(\theta^{(D)}, \theta^{(G)})$, 但是 $J^{(D)}$ 并不影响参数 $\theta^{(G)}$. 同样的道理，反过来也是 $J^{(G)}(\theta^{(G)}, \theta^{(D)})$.</p>
<p>每个玩家的损失函数依赖于另一个玩家的参数，但是确不能控制它的参数。所以这是一种 Nash equilibrium 问题。找到这样一个局部最优解 $tuple(\theta^{(G)}, \theta^{(D)})$ 使得 $J^{(D)}$ 关于 $\theta^{(D)}$ 局部最小，$J^{(G)}$ 关于 $\theta^{(G)}$ 局部最小。</p>
<h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p>differentiable function G, 可微分函数 G. 实际上就是 神经网络。z 来自简单的先验分布，G(z) 通过模型 $p_{model}$ 生成样本 x. 实践中，对于 G 的输入不一定只在第一层 layer, 也可以在 第二层 等等。总之，生成器的设计很灵活。</p>
<h3 id="Training-process"><a href="#Training-process" class="headerlink" title="Training process"></a>Training process</h3><p>两个 minibatch 的输入： x 来自 training dataset. z 来自先验隐藏变量构成的模型 $p_\text{model}$。通过优化算法 SGD/Adam 对两个损失 $J^{(D)} J^{(G)}$ 通过进行优化。梯度下降一般是同步的，也有人认为两者的迭代步数可以不一样。在这篇 tutorial 的时候，得到的共识是同步的。</p>
<h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><p>目前大多数 GAN 的损失函数，$J(D)$ 都是一样的。区别在于 $J(G)$.</p>
<h3 id="The-discriminator’s-cost-J-D"><a href="#The-discriminator’s-cost-J-D" class="headerlink" title="The discriminator’s cost, J (D)"></a>The discriminator’s cost, J (D)</h3><p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/03.png"></p>
<p>这是个标准的用于二分类的交叉熵损失函数。只不过这里，正分类都来自于训练集，正分类的概率是 $\dfrac{1}{2}E_{x～d_{data}}$, 负分类来自于生成器，则其概率是 $\dfrac{1}{2}E_z$</p>
<p>通过训练判别器，我们可以得到这样一个比例:  </p>
<p>$$\dfrac{p_{data}(x)}{p_{\text{model}}(x)}$$</p>
<p>GANs 通过监督学习来获得这个 ratio 的估计，这也是 GANs 不同于 变分自编码 和 波尔兹曼机 (variational autoencoders and Boltzmann machines) 的区别。</p>
<p>但是 GANs 通过监督学习来估计这个 ratio,会向监督学习一样遇到同样的问题：过拟合和欠拟合。但是通过足够的数据和完美的优化可以解决这个问题。</p>
<h3 id="Minimax-zero-sum-game"><a href="#Minimax-zero-sum-game" class="headerlink" title="Minimax, zero-sum game"></a>Minimax, zero-sum game</h3><p>设计 cost function for generator.  </p>
<p>前面我们知道，两个玩家 player 或者说 神经网络 G，D 实际上是一种博弈，D 希望能找出 G(z) 是 fake，而 G 希望能让 G(z) 尽可能像 real. 所以最简单的一种方式就是 all player’s cost is always zero.  </p>
<p>$$J^{(G)} = -J^{(D)}$$</p>
<p>这样 $J^{(G)}, J^{(D)}$ 都可以用 value function 表示：</p>
<p>$$V(\theta^{(D)}, \theta^{(G)})=-J^{(D)}(\theta^{(D)}, \theta^{(G)})$$</p>
<p>那么整个 game 也就是一个 minmax 游戏：</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/04.png"></p>
<p>outer loop 是关于 $\theta^{(G)}$ 的最小化，inner loop 是关于 $\theta^{(D)}$ 的最大化。</p>
<p>但是这种 function 在实际中并不能使用，因为其非凸性。  </p>
<blockquote>
<p>In practice, the players are represented with deep neural nets and updates are made in parameter space, so these results, which depend on convexity, do not apply</p>
</blockquote>
<h3 id="Heuristic-non-saturating-game"><a href="#Heuristic-non-saturating-game" class="headerlink" title="Heuristic, non-saturating game"></a>Heuristic, non-saturating game</h3><blockquote>
<p>Minimizing the cross-entropy between a target class and a classifier’s predicted distribution is highly effective because the cost never saturates when the classifier has the wrong output.  </p>
</blockquote>
<p>对一个分类器，最小化 目标类和预测概率的交叉熵 是一个非常有效的方法，因为当 分类器 存在误分类时，损失函数就永远不可能饱和。</p>
<p>所以，对于生成器的 cost 依旧使用交叉熵，但如果使用和 判别器一模一样的 cost(这里应该就是把正负分类反过来？)：</p>
<p>$$J^{(G)}(\theta^{(D)}, \theta^{(G)})=-\dfrac{1}{2}E_zlogD(G(z))-\dfrac{1}{2}E_{x～p_{data}}log(1-D(x))$$</p>
<p>猜想应该是这样，文中没有给出。</p>
<p>不幸的是这样的在是实际中，也并不可行。当 判别器 拒绝一个高置信度(high confidence) 的样本时，生成器会出现梯度消失。</p>
<p>所以，改进之后就是:  </p>
<p>$$J^{(G)}(\theta^{(D)}, \theta^{(G)})=-\dfrac{1}{2}E_zlogD(G(z))$$</p>
<blockquote>
<p>In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the logprobability of the discriminator being mistaken.  </p>
</blockquote>
<p>在 Minmax，zero-game 中，生成器的目的是最小化 判别器 自认为自己判别对了的 log-probability, 而在 non-saturating game 中，生成器是最大化 判别器判别错误 的 log-probability.  </p>
<h3 id="Maximum-likelihood-game"><a href="#Maximum-likelihood-game" class="headerlink" title="Maximum likelihood game"></a>Maximum likelihood game</h3><p>前面提到极大似然估计是最小化模型与数据之间的 KL 散度。 GANs 使用极大似然估计则是对比不同的模型。</p>
<p>$$J^{(G)}=-\dfrac{1}{2}E_zexp(\sigma^{-1}(D(G(z))))$$</p>
<blockquote>
<p>in practice, both stochastic gradient descent on the KL divergence and the GAN training procedure will have some variance around the true expected gradient due to the use of sampling (of x for maximum likelihood and z for GANs) to construct the estimated gradient.  </p>
</blockquote>
<p>在实践中，由于使用采样（x表示最大似然，z表示GAN）来构建估计的梯度，因此KL散度和GAN训练过程的随机梯度下降都会在真实预期梯度附近产生一些变化。</p>
<h3 id="Is-the-choice-of-divergence-a-distinguishing-feature-of-GANs"><a href="#Is-the-choice-of-divergence-a-distinguishing-feature-of-GANs" class="headerlink" title="Is the choice of divergence a distinguishing feature of GANs?"></a>Is the choice of divergence a distinguishing feature of GANs?</h3><h4 id="Jensen-Shannon-divergence，-reverse-KL"><a href="#Jensen-Shannon-divergence，-reverse-KL" class="headerlink" title="Jensen-Shannon divergence， reverse KL"></a>Jensen-Shannon divergence， reverse KL</h4><p>许多人认为 GANs 能够生成更加清晰、逼真的样本是因为他们最小化的是 Jensen-Shannon divergence. 而 VAEs 生成模糊的样本是因为他们最小化的是 KL divergence between the data and the model.  </p>
<p>KL 散度并不是对称的。$D_{KL}(p_{data}||q_{model})$ 与 $D_{KL}(p_{model}||q_{data})$ 是不一样的。极大似然估计是前者，最小化 Jensen-Shannon divergence 则更像后者。</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/06.png"></p>
<p>比较 $D_{KL}(p_{data}||q_{model})$ 与 $D_{KL}(p_{model}||q_{data})$ 区别。在模型能力不足以拟合数据的分布时，表现的尤为明显，如上图。给定的数据是两个高斯分布混合的分布。而模型是一个高斯模型。然后分别用极大似然 Maximum likehood 、reverse KL 散度作为 criterion，也就是 cost.</p>
<p>可以看到前者选择去平均两个模态，并希望在两个模态上都能得到较高的概率。而后者只选择其中一个模态，也有可能是另外一个模态，两个模态对于 reverse KL 都含有局部最优解。</p>
<p>所以，从这个视角来看， Maximum likehood 倾向于给 data 出现的位置更高的概率，而 reverse KL 则倾向于给没有出现 data 的位置较低的概率。所以 $D_{KL}(p_{model}||q_{data})$ 可以生成更棒的样本，因为模型不会生成不常见的样本，因为数据之间具有欺骗性的模态。</p>
<p>然而，也有一些新的研究表明，Jensen-Shannon divergence 并不是 GANs 能生成更清晰样本的原因。</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/07.png"></p>
<p>f-GAN 证明，KL 散度也能生成清晰的sample，并且也只选择少量的modes, 说明 Jensen-Shannon divergence 并不是 GANs 不同于其他模型的特征。</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/08.png"></p>
<p>GANs 通常选择少量的 mode 来生成样本，这个少量指的是小于模型的能力。 而 reverse KL 则是选择更可能多的 mode of the data distribution 在模型能力范围内。它通常不会选择更少的 mode. 这也解释了 mode collapse 并不是散度选择的原因。</p>
<blockquote>
<p>Altogether, this suggests that GANs choose to generate a small number of modes due to a defect in the training procedure, rather than due to the divergence they aim to minimize.  </p>
</blockquote>
<p>所以，GANs 选择少量的 mode 是因为训练过程中的其他缺陷，而不是 散度 选择的问题。</p>
<h3 id="Comparison-of-cost-functions"><a href="#Comparison-of-cost-functions" class="headerlink" title="Comparison of cost functions"></a>Comparison of cost functions</h3><p>生成对抗网络可以看做一种 reinforcement learning. 但是$j^{(G)}$ 并没有直接参考 training data，所有关于 training data 的信息都来自于 判别器 的学习。</p>
<p>所以和传统的强化学习是有区别的：</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/09.png"></p>
<p>比较不同的 cost function：</p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/10.png"></p>
<p>$D(G(z))$ 表示 判别器 给 generate sample 为真的概率。</p>
<p>在左侧，Minimax 和 Maximum likehood 都趋向于饱和。也就是说，当一个样本很明显为 fake 时，cost 接近于 0.</p>
<p>似然估计还有个问题，cost 主要来源于 特别像真 的少部分样本，这也是不好的。需要用到 variance reduction techniques.</p>
<blockquote>
<p>Maximum likelihood also suffers from the problem that nearly all of the gradient comes from the right end of the curve, meaning that a very small number of samples dominate the gradient computation for each minibatch. This suggests that variance reduction techniques could be an important research area for improving the performance of GANs, especially GANs based on maximum likelihood.</p>
</blockquote>
<h2 id="The-DCGAN-architecture"><a href="#The-DCGAN-architecture" class="headerlink" title="The DCGAN architecture"></a>The DCGAN architecture</h2><p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/11.png"></p>
<p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/12.png"></p>
<p>DCGAN 的结构。</p>
<h2 id="GAN，NCE，-MLE-的对比"><a href="#GAN，NCE，-MLE-的对比" class="headerlink" title="GAN，NCE， MLE 的对比"></a>GAN，NCE， MLE 的对比</h2><p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/13.png"></p>
<p>相同点：  </p>
<ul>
<li>MiniMax GAN 和 NCE 的 cost function 相同</li>
</ul>
<p>不同点：  </p>
<ul>
<li>更新策略不一样，GAN 和 MLE 都是梯度下降，而 MLE copies the density model learned inside the discriminator and converts it into a sampler to be used as the generator. NCE never updates the generator; it is just a fixed source of noise.</li>
</ul>
<h1 id="Tips-and-Tricks"><a href="#Tips-and-Tricks" class="headerlink" title="Tips and Tricks"></a>Tips and Tricks</h1><p>How to train a GAN: <a target="_blank" rel="noopener" href="https://github.com/soumith/ganhacks">https://github.com/soumith/ganhacks</a></p>
<h1 id="Research-Frontiers"><a href="#Research-Frontiers" class="headerlink" title="Research Frontiers"></a>Research Frontiers</h1><h2 id="Non-convergence"><a href="#Non-convergence" class="headerlink" title="Non-convergence"></a>Non-convergence</h2><h2 id="mode-collapse"><a href="#mode-collapse" class="headerlink" title="mode collapse"></a>mode collapse</h2><p><img src="/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/14.png"></p>
<h2 id><a href="#" class="headerlink" title></a></h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-12-27T02:09:13.000Z" title="2018/12/27 上午10:09:13">2018-12-27</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.159Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/">data augmentation</a></span><span class="level-item">5 分钟读完 (大约775个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/">论文笔记-Contextual Augmentation</a></h1><div class="content"><h3 id="paper-1"><a href="#paper-1" class="headerlink" title="paper 1"></a>paper 1</h3><p><a target="_blank" rel="noopener" href="http://aclweb.org/anthology/N18-2072">Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations</a></p>
<p>在 NLP 领域，数据增强比图像领域要复杂的多。它很难有一个统一的规则的去适用于各种 domain. 目前常用的方法是 基于 WordNet 的同义词替换，以及根据距离计算的词的相似度。但是，同义词是很少的，并且，一个词本身也是多义的，它在 WordNet 中的同义词也许并不适合当前的语境，而这一限制也是很难通过一个规则去限定。所以，传统的方法都很难发挥。</p>
<p>传统的数据增强方法：  </p>
<ul>
<li><p>基于规则，聚类，人工干预。 <a target="_blank" rel="noopener" href="http://aclweb.org/anthology/D15-1306">Wang and Yang(2015)</a>  </p>
</li>
<li><p>基于近义词 <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf">Character-level convolutional networks for text classification</a></p>
</li>
</ul>
<p>作者提出了一种新的方法，基于语言模型的 contextual augmentation. 根据上下文预测得到的不同的词具有范式关系 <strong>paradigmatic relations.</strong> 更进一步的，为了让生成的词与当前句子的 label 是兼容的 (compatible)，作者加入了 label-conditional.</p>
<p><img src="/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/01.png"></p>
<p>根据上文预测 target position i $w_i$, $P(\cdot|S/{w_i})$. 加上 label 限制条件之后就是  $P(\cdot|y, S/{w_i})$</p>
<p>作者并不是直接使用 top-k. 而是利用的退火的方法，temperature parameter $\tau$, 这样预测分布就是</p>
<p>$$P(\cdot|y, S/{w_i})^{1/\tau}$$</p>
<p>当 $\tau \rightarrow \infty$ 时，那么对应的预测词分布是 uniform distribution. 当 $\tau \rightarrow 0$ 时，预测得到的就是概率最大的词。我猜作者这样做的目的是让生成的词更丰富，避免单一化。</p>
<p>在不同任务上的对比实验，与 synonym 同义词替换进行的对比，分类模型都用的 CNN.</p>
<p><img src="/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/02.png"></p>
<p>效果还可以，但是并不明显。而 w/synonym 反而会有反作用。。</p>
<p>其中某个样本的效果展示：</p>
<p><img src="/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/03.png"></p>
<h3 id="paper-2"><a href="#paper-2" class="headerlink" title="paper 2"></a>paper 2</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.06705.pdf">Conditional BERT Contextual Augmentation</a>  </p>
<p>看完paper真的想吐槽下中国人写的论文真的给人一种粗制滥造的感觉。。尽管paper出来的很及时，与 Bert 结合也很赞，但是 。。。其实还可以好好写，还可以多分析分析，看 paper 都能感觉到一种为了发论文而发论文的感觉，我自己又何尝不是呢。。。</p>
<p><img src="/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/04.png"></p>
<p>Bert 想对于 LSTM 更获得更深层的含义。</p>
<p><img src="/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/05.png"></p>
<p>将 segmentation embedding 换成 label embedding. 然后使用预训练的 BERT 模型进行 fine-tune. 迭代直到收敛，生成新的句子后，在进行下游任务。</p>
<p><img src="/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/06.png"></p>
<blockquote>
<p>When the task-specific dataset is with more than two different labels,we should re-train a label size compatible label embeddings layer instead of directly fine-tuning the pre-trained one.  </p>
</blockquote>
<p>对于多标签任务，作者是这么说的，不太明白。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/7/">上一页</a></div><div class="pagination-next"><a href="/page/9/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/7/">7</a></li><li><a class="pagination-link is-current" href="/page/8/">8</a></li><li><a class="pagination-link" href="/page/9/">9</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/23/">23</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="潘晓榭"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">潘晓榭</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">112</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-02T04:37:58.000Z">2021-07-02</time></p><p class="title"><a href="/2021/07/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-constrast-learning-in-NLP/">论文笔记-constrast learning in NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-05T02:03:16.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-image-based contrastive learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:07:08.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:05:10.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-16T07:48:48.000Z">2021-04-16</time></p><p class="title"><a href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">七月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">四月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">九月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">六月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">六月 2018</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">四月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESA/"><span class="tag">ESA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN%EF%BC%8CRL/"><span class="tag">GAN，RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MRC-and-QA/"><span class="tag">MRC and QA</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Translation/"><span class="tag">Machine Translation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/capsules/"><span class="tag">capsules</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-augmentation/"><span class="tag">data augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/open-set-recognition/"><span class="tag">open set recognition</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentence-embedding/"><span class="tag">sentence embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentiment-classification/"><span class="tag">sentiment classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language-recognition/"><span class="tag">sign language recognition</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-matching/"><span class="tag">text matching</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/transfer-learning/"><span class="tag">transfer learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="tag">数据结构与算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag">8</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>