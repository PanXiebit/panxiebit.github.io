<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-04-11T01:18:02.000Z" title="2018/4/11 上午9:18:02">2018-04-11</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.522Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/NLP/">NLP</a></span><span class="level-item">13 分钟读完 (大约1967个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/04/11/chapter3-%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E8%87%AA%E5%8A%A8%E6%9C%BA/">chapter3-有限状态自动机</a></h1><div class="content"><ul>
<li><p>有限状态机 FSA</p>
</li>
<li><p>确定性的识别器 DFSA</p>
</li>
<li><p>确定性的识别器 NFSA：深度优先搜索 or 广度优先搜索</p>
</li>
<li><p>形式语言</p>
</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2018/04/11/chapter3-%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E8%87%AA%E5%8A%A8%E6%9C%BA/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-04-09T02:53:33.000Z" title="2018/4/9 上午10:53:33">2018-04-09</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.105Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/NLP/">NLP</a></span><span class="level-item">29 分钟读完 (大约4290个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/04/09/chapter2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E3%80%81%E6%96%87%E6%9C%AC%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/">chapter2:正则表达式、文本标准化和编辑距离</a></h1><div class="content"><ul>
<li><p>各种正则表达式，析取，组合和优先关系</p>
</li>
<li><p>文本标准化：各种预处理方法</p>
</li>
<li><p>编辑距离：动态规划</p>
</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2018/04/09/chapter2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E3%80%81%E6%96%87%E6%9C%AC%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-04-06T04:27:53.000Z" title="2018/4/6 下午12:27:53">2018-04-06</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.540Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/NLP/">NLP</a></span><span class="level-item">39 分钟读完 (大约5907个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/04/06/chapter9-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-standford/">chapter9 隐马尔可夫模型</a></h1><div class="content"><p>chapter6: 隐马尔科夫模型 standford tutorial, 可以说是看过的关于隐马尔科夫最好的教程了。要是看英文原版能和看中文一样easy该多好，可惜文化差异的情况下，即使是单词都认识，理解起来也会有点困难。对此，只能自己重新总结一下吧～</p>
<p>可以说，斯坦福从未让人失望过，太赞了！</p>
<p>也是无意中在google上看到这篇文章，才发现了这么好的一本书, Speech and language processing.</p></div><a class="article-more button is-small is-size-7" href="/2018/04/06/chapter9-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-standford/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-03-29T08:15:26.000Z" title="2018/3/29 下午4:15:26">2018-03-29</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">29 分钟读完 (大约4421个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/">代码实现高斯混合模型</a></h1><div class="content"><p>sklearn源码阅读，用em算法计算高斯混合模型GMM</p>
<h3 id="代码实现高斯混合模型"><a href="#代码实现高斯混合模型" class="headerlink" title="代码实现高斯混合模型"></a>代码实现高斯混合模型</h3><p>参考这篇博客<a target="_blank" rel="noopener" href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/">Regularized Gaussian Covariance Estimation</a>非常值得一读，同事这篇博客很深入的讲了协方差怎么求的问题，在前文中我也有提到～但我解释的很low。。</p>
<p>代码直接就看sklearn里面的源码吧～网上很多不靠谱。。。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/mixture/gaussian_mixture.py">github源码</a></p>
<h3 id="类初始化"><a href="#类初始化" class="headerlink" title="类初始化"></a>类初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GaussianMixture</span>(<span class="params">BaseMixture</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Representation of a Gaussian mixture model probability distribution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This class allows to estimate the parameters of a Gaussian mixture</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  distribution. 对混合的高斯分布进行参数估计～</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_components=<span class="number">1</span>, covariance_type=<span class="string">&#x27;full&#x27;</span>, tol=<span class="number">1e-3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             reg_covar=<span class="number">1e-6</span>, max_iter=<span class="number">100</span>, n_init=<span class="number">1</span>, init_params=<span class="string">&#x27;kmeans&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             weights_init=<span class="literal">None</span>, means_init=<span class="literal">None</span>, precisions_init=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             random_state=<span class="literal">None</span>, warm_start=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             verbose=<span class="number">0</span>, verbose_interval=<span class="number">10</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(GaussianMixture, self).__init__(</span><br><span class="line"></span><br><span class="line">        n_components=n_components, tol=tol, reg_covar=reg_covar,</span><br><span class="line"></span><br><span class="line">        max_iter=max_iter, n_init=n_init, init_params=init_params,</span><br><span class="line"></span><br><span class="line">        random_state=random_state, warm_start=warm_start,</span><br><span class="line"></span><br><span class="line">        verbose=verbose, verbose_interval=verbose_interval)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主要是针对3个要学习的参数的初始化</span></span><br><span class="line"></span><br><span class="line">    self.covariance_type = covariance_type <span class="comment"># 协方差矩阵形式</span></span><br><span class="line"></span><br><span class="line">    self.weights_init = weights_init <span class="comment"># 多项式分布，每一类的概率</span></span><br><span class="line"></span><br><span class="line">    self.means_init = means_init  <span class="comment"># 均值 (n_components, n_features)</span></span><br><span class="line"></span><br><span class="line">    self.precisions_init = precisions_init <span class="comment"># 协方差</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>先看初始化构造函数，参数是真的多。。。</p>
<ul>
<li><p><strong>n_components=1:</strong> The number of mixture components.表示混合类别的个数，也就是混合高斯分布的个数</p>
</li>
<li><p><strong>covariance_type=’full’:</strong> 协方差矩阵的类型。{‘full’, ‘tied’, ‘diag’, ‘spherical’} 分别对应完全协方差矩阵（元素都不为零），相同的完全协方差矩阵（HMM会用到），对角协方差矩阵（非对角为零，对角不为零），球面协方差矩阵（非对角为零，对角完全相同，球面特性），默认‘full’ 完全协方差矩阵</p>
</li>
<li><p><strong>tol=1e-3</strong> 收敛阈值，EM iterations will stop when the lower bound average gain is</p>
</li>
</ul>
<p>below this threshold.也就是当下界的平均增益小于阈值时，em迭代就停止。这里的下界指的是公式</p>
<p>（3）中的下界凸函数。我们知道em算法分两步，e step是期望，也就是不等式相等，m setp是最大化，</p>
<p>也就是下界凸函数最大化。这里的阈值平均增益就是指凸函数的最大化过程中的增益。</p>
<ul>
<li><strong>reg_covar=1e-6：</strong> Non-negative regularization added to the diagonal of</li>
</ul>
<p>covariance.Allows to assure that the covariance matrices are all positive.</p>
<p>非负正则化添加到协方差矩阵对角线上，保证协方差矩阵都是正定的。</p>
<ul>
<li><p><strong>max_iter=100:</strong> em算法的最大迭代次数</p>
</li>
<li><p><strong>n_init:</strong> int, defaults to 1.初始化的次数</p>
</li>
<li><p><strong>init_params:</strong> {‘kmeans’, ‘random’}, defaults to ‘kmeans’.</p>
</li>
</ul>
<p>The method used to initialize the weights, the means and the precisionsself.</p>
<p> Must be one of::</p>
<pre><code>-  &#39;kmeans&#39; : responsibilities are initialized using kmeans.

- &#39;random&#39; : responsibilities are initialized randomly.

- 这里对应的初始化，是指的隐藏变量z的分类所占比例，也就是weight_init，kmeans表示“hard”guess， &#123;0, 1&#125; or &#123;1, . . . , k&#125;)
</code></pre>
<p>  random应该就是”soft”guess吧。</p>
<ul>
<li>weights_init : shape (n_components, ), optional The user-provided initial weights, defaults to None. If it None, weights are initialized using the <code>init_params</code> method.</li>
</ul>
<p>  先验权重初始化，对应的就是隐藏变量有n_components类，而每一类所占的比例，也就是多项式分布的初始化～对应$\phi_i$</p>
<ul>
<li><strong>means_init :</strong> array-like, shape (n_components, n_features), optional. The user-provided initial means, defaults to None, If it None, means are initialized using the <code>init_params</code> method.混合高斯分布的均值初始化，注意shape=(n_components, n_features),有n_components这样的多维高斯分布，每个高斯分布有n_features维度</li>
</ul>
<ul>
<li><p><strong>precisions_init :</strong> The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the ‘init_params’ method.The shape depends on ‘covariance_type’::</p>
<ul>
<li><p>(n_components,)                        if ‘spherical’,</p>
</li>
<li><p>(n_features, n_features)               if ‘tied’,</p>
</li>
<li><p>(n_components, n_features)             if ‘diag’,</p>
</li>
<li><p>(n_components, n_features, n_features) if ‘full’</p>
</li>
<li><p>用来初始化高斯分布中的协方差矩阵，协方差矩阵代表的是n_features维向量中每一维特征与其他维度特征的关系，对于一个高斯分布来说是n_features<em>n_features，n_components个混合也就是’full’。其中要学习的参数个数是(n_features+1)</em> n_features/2.具体关于协方差矩阵参考前面那篇博客</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>random_state :</strong> int, RandomState instance or None, optional (default=None) 随机数生成器</li>
</ul>
<ul>
<li><strong>warm_start :</strong> bool, default to False.If ‘warm_start’ is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. 若为True，则fit（）调用会以上一次fit（）的结果作为初始化参数，适合相同问题多次fit的情况，能加速收敛，默认为False。</li>
</ul>
<ul>
<li><strong>verbose :</strong> int, default to 0. Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step. 使能迭代信息显示，默认为0，可以为1或者大于1（显示的信息不同）</li>
</ul>
<ul>
<li><strong>verbose_interval:</strong> 与13挂钩，若使能迭代信息显示，设置多少次迭代后显示信息，默认10次。</li>
</ul>
<h3 id="E-step"><a href="#E-step" class="headerlink" title="E step"></a>E step</h3><p>就是求$w_j^i$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_e_step</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;E step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_prob_norm :</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_responsibility : 后验概率，样本i是j类的概率w_j^&#123;i&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    log_prob_norm, log_resp = self._estimate_log_prob_resp(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.mean(log_prob_norm), log_resp</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>那么如何求$w_j^{i}$呢？</p>
<p>$$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\dfrac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$</p>
<p><font size="4" color="#D2691E">要注意的是，为了计算方便，有以下几点：</font></p>
<ul>
<li><p>因为分子分母中设计到正态分布，即指数形式，故先计算其log形式。然后带入到M step中取回指数形式即可。</p>
</li>
<li><p>对于协方差矩阵，如果n_features很大的话，计算其逆矩阵和行列式就很复杂，因此可以先计算其precision矩阵，然后进行cholesky分解，以便优化计算。</p>
</li>
</ul>
<h4 id="先计算分子对数形式，两个对数相加："><a href="#先计算分子对数形式，两个对数相加：" class="headerlink" title="先计算分子对数形式，两个对数相加："></a>先计算分子对数形式，两个对数相加：</h4><p>$$logp(x^{(i)}|z^{(i)}=j;\mu,\Sigma)+logp(z^{(i)}=j;\phi)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 计算P(x|z)p(z)的对数形式</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_weighted_log_prob</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Estimate the weighted log-probabilities, log P(X | Z) + log weights.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        weighted_log_prob : array, shape (n_samples, n_component)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._estimate_log_prob(X) + self._estimate_log_weights()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>其中前者是高斯分布概率的对数,根据均值，协方差矩阵的cholesky分解可求得。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_prob</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> _estimate_log_gaussian_prob(</span><br><span class="line"></span><br><span class="line">            X, self.means_, self.precisions_cholesky_, self.covariance_type)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个函数，<code>_estimate_log_gaussian_prob</code>根据高斯分布的参数计算概率，涉及到协方差矩阵，要优化计算，很复杂，放在最后说。先把整个流程走完。</p>
<ol start="2">
<li>后者是每一类高斯分布所占的权重，也就是$\phi_j$</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_weights</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 刚开始是初始值，后面随着m step而更新</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.log(self.weights_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="再计算-w-j-i"><a href="#再计算-w-j-i" class="headerlink" title="再计算$w_j^i$"></a>再计算$w_j^i$</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_prob_resp</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate log probabilities and responsibilities for each sample.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Compute the log probabilities, weighted log probabilities per</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    component and responsibilities for each sample in X with respect to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    the current state of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_prob_norm : array, shape (n_samples,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        log p(X)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_responsibilities : array, shape (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        logarithm of the responsibilities</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算分子log P(X | Z) + log weights.</span></span><br><span class="line"></span><br><span class="line">    weighted_log_prob = self._estimate_weighted_log_prob(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算分母log P(x)</span></span><br><span class="line"></span><br><span class="line">    log_prob_norm = logsumexp(weighted_log_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> np.errstate(under=<span class="string">&#x27;ignore&#x27;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 忽略下溢，计算log(w_J^j)，也就是两个对数相减</span></span><br><span class="line"></span><br><span class="line">        log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> log_prob_norm, log_resp</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="M-setp"><a href="#M-setp" class="headerlink" title="M setp"></a>M setp</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_m_step</span>(<span class="params">self, X, log_resp</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;M step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_resp : array-like, shape (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Logarithm of the posterior probabilities (or responsibilities) of</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the point of each sample in X.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_samples, _ = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据E step中求得的log_resp,更新权重，均值和协方差</span></span><br><span class="line"></span><br><span class="line">    self.weights_, self.means_, self.covariances_ = (</span><br><span class="line"></span><br><span class="line">        _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,</span><br><span class="line"></span><br><span class="line">                                      self.covariance_type))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新类别权重phi_j</span></span><br><span class="line"></span><br><span class="line">    self.weights_ /= n_samples</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新协方差矩阵的精度矩阵</span></span><br><span class="line"></span><br><span class="line">    self.precisions_cholesky_ = _compute_precision_cholesky(</span><br><span class="line"></span><br><span class="line">        self.covariances_, self.covariance_type)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>具体怎么求，就是根据前面推导的公式了。根据前面的公式分别求对应的估计参数：</p>
<p>$$\Sigma_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<h4 id="协方差矩阵：以‘full’为例"><a href="#协方差矩阵：以‘full’为例" class="headerlink" title="协方差矩阵：以‘full’为例"></a>协方差矩阵：以‘full’为例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_gaussian_covariances_full</span>(<span class="params">resp, X, nk, means, reg_covar</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate the full covariance matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    resp:表示E step中猜测的w_j^&#123;i&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_components, n_features = means.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 协方差矩阵</span></span><br><span class="line"></span><br><span class="line">    covariances = np.empty((n_components, n_features, n_features))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_components):</span><br><span class="line"></span><br><span class="line">        diff = X - means[k]</span><br><span class="line"></span><br><span class="line">        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正则化，flat表示展开成一维，然后每隔n_features取一个元素，单个协方差矩阵shape是</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># [n_features, n_features],所以就是对角线元素加上reg_covar</span></span><br><span class="line"></span><br><span class="line">        covariances[k].flat[::n_features + <span class="number">1</span>] += reg_covar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> covariances</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="然后是正态分布的参数估计-u-j-phi-j"><a href="#然后是正态分布的参数估计-u-j-phi-j" class="headerlink" title="然后是正态分布的参数估计$u_j, \phi_j$"></a>然后是正态分布的参数估计$u_j, \phi_j$</h4><p>$$\phi_j:=\frac{1}{m}\sum_{i=1}^mw_j^{(i)}$$</p>
<p>$$\mu_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_gaussian_parameters</span>(<span class="params">X, resp, reg_covar, covariance_type</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate the Gaussian distribution parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : 样本数据 (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    resp : Estep猜测的样本i是j类的概率w_i^&#123;j&#125;, shape (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    reg_covar : 对角线正则化项</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    nk : 当前类别下的样本和 (n_components,) 也就是\sum_i^&#123;m&#125;(w_j^&#123;i&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    means : k个n维正态分布的均值, shape (n_components, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariances : 协方差矩阵</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为要做分母，避免为0</span></span><br><span class="line"></span><br><span class="line">    nk = resp.<span class="built_in">sum</span>(axis=<span class="number">0</span>) + <span class="number">10</span> * np.finfo(resp.dtype).eps</span><br><span class="line"></span><br><span class="line">    means = np.dot(resp.T, X) / nk[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">    covariances = &#123;<span class="string">&quot;full&quot;</span>: _estimate_gaussian_covariances_full,</span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;tied&quot;</span>: _estimate_gaussian_covariances_tied,</span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;diag&quot;</span>: _estimate_gaussian_covariances_diag,</span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;spherical&quot;</span>: _estimate_gaussian_covariances_spherical</span><br><span class="line"></span><br><span class="line">                   &#125;[covariance_type](resp, X, nk, means, reg_covar)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nk, means, covariances</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="迭代收敛，重复以上过程"><a href="#迭代收敛，重复以上过程" class="headerlink" title="迭代收敛，重复以上过程"></a>迭代收敛，重复以上过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate model parameters with the EM algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The method fit the model `n_init` times and set the parameters with</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    which the model has the largest likelihood or lower bound. Within each</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    trial, the method iterates between E-step and M-step for `max_iter`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    times until the change of likelihood or lower bound is less than</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    `tol`, otherwise, a `ConvergenceWarning` is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    迭代终止条件： 迭代次数ｎ_init，极大似然函数或下界函数的增益小于`tol`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    self</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    X = _check_X(X, self.n_components)</span><br><span class="line"></span><br><span class="line">    self._check_initial_parameters(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># if we enable warm_start, we will have a unique initialisation</span></span><br><span class="line"></span><br><span class="line">    do_init = <span class="keyword">not</span>(self.warm_start <span class="keyword">and</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;converged_&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    n_init = self.n_init <span class="keyword">if</span> do_init <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    max_lower_bound = -np.infty</span><br><span class="line"></span><br><span class="line">    self.converged_ = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    random_state = check_random_state(self.random_state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    n_samples, _ = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化次数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> init <span class="keyword">in</span> <span class="built_in">range</span>(n_init):</span><br><span class="line"></span><br><span class="line">        self._print_verbose_msg_init_beg(init)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 先初始化参数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> do_init:</span><br><span class="line"></span><br><span class="line">            self._initialize_parameters(X, random_state)</span><br><span class="line"></span><br><span class="line">            self.lower_bound_ = -np.infty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n_iter <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line"></span><br><span class="line">            prev_lower_bound = self.lower_bound_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># E step求出后验概率w_j^i或是Q分布</span></span><br><span class="line"></span><br><span class="line">            log_prob_norm, log_resp = self._e_step(X)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ｍ step更新参数</span></span><br><span class="line"></span><br><span class="line">            self._m_step(X, log_resp)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 求出下界函数的最大值</span></span><br><span class="line"></span><br><span class="line">            self.lower_bound_ = self._compute_lower_bound(</span><br><span class="line"></span><br><span class="line">                log_resp, log_prob_norm)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 下界函数的增益</span></span><br><span class="line"></span><br><span class="line">            change = self.lower_bound_ - prev_lower_bound</span><br><span class="line"></span><br><span class="line">            self._print_verbose_msg_iter_end(n_iter, change)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 比较下界函数增益与ｔｏｌ</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(change) &lt; self.tol:</span><br><span class="line"></span><br><span class="line">                self.converged_ = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self._print_verbose_msg_init_end(self.lower_bound_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.lower_bound_ &gt; max_lower_bound:</span><br><span class="line"></span><br><span class="line">            max_lower_bound = self.lower_bound_</span><br><span class="line"></span><br><span class="line">            best_params = self._get_parameters()</span><br><span class="line"></span><br><span class="line">            best_n_iter = n_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.converged_:</span><br><span class="line"></span><br><span class="line">        warnings.warn(<span class="string">&#x27;Initialization %d did not converge. &#x27;</span></span><br><span class="line"></span><br><span class="line">                      <span class="string">&#x27;Try different init parameters, &#x27;</span></span><br><span class="line"></span><br><span class="line">                      <span class="string">&#x27;or increase max_iter, tol &#x27;</span></span><br><span class="line"></span><br><span class="line">                      <span class="string">&#x27;or check for degenerate data.&#x27;</span></span><br><span class="line"></span><br><span class="line">                      % (init + <span class="number">1</span>), ConvergenceWarning)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._set_parameters(best_params)</span><br><span class="line"></span><br><span class="line">    self.n_iter_ = best_n_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="E-step中p-x-z-j"><a href="#E-step中p-x-z-j" class="headerlink" title="E step中p(x|z=j)"></a>E step中p(x|z=j)</h3><p>根据高斯分布的参数计算概率,优化的计算方法。</p>
<p>先计算协方差矩阵的precision矩阵，并进行cholesky分解</p>
<p>Precision matrix 协方差矩阵的逆矩阵：<a target="_blank" rel="noopener" href="https://www.statlect.com/glossary/precision-matrix">https://www.statlect.com/glossary/precision-matrix</a></p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/precision.png"></p>
<p>然后根据精度矩阵的cholesky分解形式,这样可以优化矩阵运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_precision_cholesky</span>(<span class="params">covariances, covariance_type</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the Cholesky decomposition of the precisions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariances : array-like</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The covariance matrix of the current components.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The shape depends of the covariance_type.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The type of precision matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    precisions_cholesky : array-like</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The cholesky decomposition of sample precisions of the current</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        components. The shape depends of the covariance_type.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> covariance_type <span class="keyword">in</span> <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        n_components, n_features, _ = covariances.shape</span><br><span class="line"></span><br><span class="line">        precisions_chol = np.empty((n_components, n_features, n_features))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, covariance <span class="keyword">in</span> <span class="built_in">enumerate</span>(covariances):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">                cov_chol = linalg.cholesky(covariance, lower=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">except</span> linalg.LinAlgError:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">raise</span> ValueError(estimate_precision_error_message)</span><br><span class="line"></span><br><span class="line">            precisions_chol[k] = linalg.solve_triangular(cov_chol,</span><br><span class="line"></span><br><span class="line">                                                         np.eye(n_features),</span><br><span class="line"></span><br><span class="line">                                                         lower=<span class="literal">True</span>).T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        _, n_features = covariances.shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">            cov_chol = linalg.cholesky(covariances, lower=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> linalg.LinAlgError:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(estimate_precision_error_message)</span><br><span class="line"></span><br><span class="line">        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),</span><br><span class="line"></span><br><span class="line">                                                  lower=<span class="literal">True</span>).T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">any</span>(np.less_equal(covariances, <span class="number">0.0</span>)):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(estimate_precision_error_message)</span><br><span class="line"></span><br><span class="line">        precisions_chol = <span class="number">1.</span> / np.sqrt(covariances)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> precisions_chol</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="计算cholesky分解的行列式"><a href="#计算cholesky分解的行列式" class="headerlink" title="计算cholesky分解的行列式"></a>计算cholesky分解的行列式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Gaussian mixture probability estimators</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据cholesky分解计算行列式的log</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_log_det_cholesky</span>(<span class="params">matrix_chol, covariance_type, n_features</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the log-det of the cholesky decomposition of matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    matrix_chol : 协方差矩阵的cholesky分解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    n_features : int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Number of features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_det_precision_chol : array-like, shape (n_components,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The determinant of the precision matrix for each component.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> covariance_type == <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        n_components, _, _ = matrix_chol.shape</span><br><span class="line"></span><br><span class="line">        log_det_chol = (np.<span class="built_in">sum</span>(np.log(</span><br><span class="line"></span><br><span class="line">            matrix_chol.reshape(</span><br><span class="line"></span><br><span class="line">                n_components, -<span class="number">1</span>)[:, ::n_features + <span class="number">1</span>]), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        log_det_chol = (np.<span class="built_in">sum</span>(np.log(np.diag(matrix_chol))))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;diag&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        log_det_chol = (np.<span class="built_in">sum</span>(np.log(matrix_chol), axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">        log_det_chol = n_features * (np.log(matrix_chol))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> log_det_chol</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="计算分子-logp-x-i-z-i-j-mu-Sigma"><a href="#计算分子-logp-x-i-z-i-j-mu-Sigma" class="headerlink" title="计算分子: $logp(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$"></a>计算分子: $logp(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_gaussian_prob</span>(<span class="params">X, means, precisions_chol, covariance_type</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate the log Gaussian probability.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : 样本数据(n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    means : k个n维正态分布的均值(n_components, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    precisions_chol : 精度矩阵的Cholesky分解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_prob : (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">    n_components, _ = means.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># det(precision_chol) is half of det(precision)</span></span><br><span class="line"></span><br><span class="line">    log_det = _compute_log_det_cholesky(</span><br><span class="line"></span><br><span class="line">        precisions_chol, covariance_type, n_features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> covariance_type == <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        log_prob = np.empty((n_samples, n_components))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, (mu, prec_chol) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(means, precisions_chol)):</span><br><span class="line"></span><br><span class="line">            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)</span><br><span class="line"></span><br><span class="line">            log_prob[:, k] = np.<span class="built_in">sum</span>(np.square(y), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;diag&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;spherical&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">.5</span> * (n_features * np.log(<span class="number">2</span> * np.pi) + log_prob) + log_det</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="sklearn中实例"><a href="#sklearn中实例" class="headerlink" title="sklearn中实例"></a>sklearn中实例</h3><p>Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid.</p>
<p>We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data.</p>
<p>On the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only the first two dimensions are shown here, and thus some points are separated in other dimensions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">colors = [<span class="string">&#x27;navy&#x27;</span>, <span class="string">&#x27;turquoise&#x27;</span>, <span class="string">&#x27;darkorange&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()  <span class="comment"># data, target</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Break up the dataset into non-overlapping training (75%) and testing</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (25%) sets.</span></span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only take the first fold.</span></span><br><span class="line"></span><br><span class="line">train_index, test_index = <span class="built_in">next</span>(<span class="built_in">iter</span>(skf.split(iris.data, iris.target)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_train = iris.data[train_index]     <span class="comment"># (111, 4)</span></span><br><span class="line"></span><br><span class="line">y_train = iris.target[train_index]   <span class="comment"># (111,)</span></span><br><span class="line"></span><br><span class="line">X_test = iris.data[test_index]       <span class="comment"># (39, 4)</span></span><br><span class="line"></span><br><span class="line">y_test = iris.target[test_index]     <span class="comment"># (39,)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n_classes = <span class="built_in">len</span>(np.unique(y_train))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try GMMs using different types of covariances. 根据协方差矩阵，有4中不同的GMM模型</span></span><br><span class="line"></span><br><span class="line">estimators = <span class="built_in">dict</span>((cov_type, GaussianMixture(n_components=n_classes,</span><br><span class="line"></span><br><span class="line">                   covariance_type=cov_type, max_iter=<span class="number">20</span>, random_state=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">                  <span class="keyword">for</span> cov_type <span class="keyword">in</span> [<span class="string">&#x27;spherical&#x27;</span>, <span class="string">&#x27;diag&#x27;</span>, <span class="string">&#x27;tied&#x27;</span>, <span class="string">&#x27;full&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n_estimators = <span class="built_in">len</span>(estimators)  <span class="comment"># 4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># figsize表示图像的尺寸（width, height in inches）</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">3</span> * <span class="number">5</span> // <span class="number">2</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像之间的间距</span></span><br><span class="line"></span><br><span class="line">plt.subplots_adjust(bottom=<span class="number">.01</span>, top=<span class="number">0.95</span>, hspace=<span class="number">.15</span>, wspace=<span class="number">.05</span>,</span><br><span class="line"></span><br><span class="line">                    left=<span class="number">.01</span>, right=<span class="number">.99</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 椭圆</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_ellipses</span>(<span class="params">gmm, ax</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, color <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> gmm.covariance_type == <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = gmm.covariances_[n][:<span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> gmm.covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = gmm.covariances_[:<span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> gmm.covariance_type == <span class="string">&#x27;diag&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = np.diag(gmm.covariances_[n][:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> gmm.covariance_type == <span class="string">&#x27;spherical&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = np.eye(gmm.means_.shape[<span class="number">1</span>]) * gmm.covariances_[n]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 参数估计得到的混合二维高斯分布，将其用椭圆表示出来～</span></span><br><span class="line"></span><br><span class="line">        v, w = np.linalg.eigh(covariances) <span class="comment"># 返回协方差矩阵的特征值和列向量由特征矩阵构成的矩阵</span></span><br><span class="line"></span><br><span class="line">        u = w[<span class="number">0</span>] / np.linalg.norm(w[<span class="number">0</span>]) <span class="comment"># order=None 表示 Frobenius norm，2-norm</span></span><br><span class="line"></span><br><span class="line">        angle = np.arctan2(u[<span class="number">1</span>], u[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        angle = <span class="number">180</span> * angle / np.pi  <span class="comment"># 转换为角度</span></span><br><span class="line"></span><br><span class="line">        v = <span class="number">2.</span> * np.sqrt(<span class="number">2.</span>) * np.sqrt(v)</span><br><span class="line"></span><br><span class="line">        ell = mpl.patches.Ellipse(gmm.means_[n, :<span class="number">2</span>], v[<span class="number">0</span>], v[<span class="number">1</span>],</span><br><span class="line"></span><br><span class="line">                                  <span class="number">180</span> + angle, color=color)</span><br><span class="line"></span><br><span class="line">        ell.set_clip_box(ax.bbox)</span><br><span class="line"></span><br><span class="line">        ell.set_alpha(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        ax.add_artist(ell)  <span class="comment"># 增加文字</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, (name, estimator) <span class="keyword">in</span> <span class="built_in">enumerate</span>(estimators.items()):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we have class labels for the training data, we can</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the GMM parameters in a supervised manner.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里因为有类标签，所以直接用真实均值来初始化GMM的均值。在无标签或者标签较少的情况下，则需要随机初始化</span></span><br><span class="line"></span><br><span class="line">    estimator.means_init = np.array([X_train[y_train == i].mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the other parameters using the EM algorithm.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用em算法来估计其他参数</span></span><br><span class="line"></span><br><span class="line">    estimator.fit(X_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画椭圆</span></span><br><span class="line"></span><br><span class="line">    h = plt.subplot(<span class="number">2</span>, n_estimators // <span class="number">2</span>, index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    make_ellipses(estimator, h)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, color <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors):</span><br><span class="line"></span><br><span class="line">        data = iris.data[iris.target == n]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不同的种类数据用不同的点表示</span></span><br><span class="line"></span><br><span class="line">        plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], s=<span class="number">0.8</span>, color=color,</span><br><span class="line"></span><br><span class="line">                    label=iris.target_names[n])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用xx表示测试集</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, color <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors):</span><br><span class="line"></span><br><span class="line">        data = X_test[y_test == n]</span><br><span class="line"></span><br><span class="line">        plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], marker=<span class="string">&#x27;x&#x27;</span>, color=color)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集的准确率</span></span><br><span class="line"></span><br><span class="line">    y_train_pred = estimator.predict(X_train) <span class="comment"># 预测是选取概率最大的一类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当无标签时是没有办法计算准确率的。但是这里有标签，</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># y_train_pred返回的是概率最大的索引，　y_train的元素是[0,1,2,3]中的一个</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因此可以求得准确率</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(y_train_pred[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(y_train[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    plt.text(<span class="number">0.05</span>, <span class="number">0.9</span>, <span class="string">&#x27;Train accuracy: %.1f&#x27;</span> % train_accuracy,</span><br><span class="line"></span><br><span class="line">             transform=h.transAxes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    y_test_pred = estimator.predict(X_test)</span><br><span class="line"></span><br><span class="line">    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    plt.text(<span class="number">0.05</span>, <span class="number">0.8</span>, <span class="string">&#x27;Test accuracy: %.1f&#x27;</span> % test_accuracy,</span><br><span class="line"></span><br><span class="line">             transform=h.transAxes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.xticks(())</span><br><span class="line"></span><br><span class="line">    plt.yticks(())</span><br><span class="line"></span><br><span class="line">    plt.title(name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.legend(scatterpoints=<span class="number">1</span>, loc=<span class="string">&#x27;lower right&#x27;</span>, prop=<span class="built_in">dict</span>(size=<span class="number">12</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/Figure_1.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-03-29T08:15:26.000Z" title="2018/3/29 下午4:15:26">2018-03-29</time>发表</span><span class="level-item"><time dateTime="2022-04-09T15:09:13.073Z" title="2022/4/9 下午11:09:13">2022-04-09</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">40 分钟读完 (大约5972个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/">机器学习-生成模型到高斯判别分析再到GMM和EM算法</a></h1><div class="content"><p>生成模型-高斯判别分析GDA- 高斯混合模型GMM- EM算法</p>
<p>在学习生成模型之前，先学习了解下密度估计和高斯混合模型。</p>
<h3 id="生成学习算法-cs229-Ng"><a href="#生成学习算法-cs229-Ng" class="headerlink" title="生成学习算法(cs229,Ng)"></a>生成学习算法(cs229,Ng)</h3><h4 id="生成算法和判别算法的区别"><a href="#生成算法和判别算法的区别" class="headerlink" title="生成算法和判别算法的区别"></a>生成算法和判别算法的区别</h4><p>举个栗子：</p>
<p>我们要区分elephants(y=1)和dogs(y=0)</p>
<ol>
<li>对判别模型（discriminative），以logistic回归为例：</li>
</ol>
<ul>
<li><p>logistic回归模型：$p(y|x;\theta)，h_{\theta}=g(\theta^Tx)$,对应的模型其中g是sigmoid函数。通过logistic回归，我们找到一条决策边界decision boundary，能够区分elephants和dogs.</p>
</li>
<li><p>这个学习的过程就是找到表征这个决策过程的参数 $\theta$.</p>
</li>
</ul>
<ol start="2">
<li>生成模型（generative）：</li>
</ol>
<p>同样的我们也是要通过给定的特征x来判别其对应的类别y。但我们换个思路，就是先求p(x|y),也就是通过y来分析对应x满足的一个概率模型p(x|y)。然后在反过来看特征x，以二分类为例，p(x|y=0)和p(x|y=1)哪个概率大，那么x就属于哪一类。</p>
<ul>
<li><p>模型：p(x|y)，在给定了样本所属的类的条件下，对样本特征建立概率模型。</p>
</li>
<li><p>p(x|y=1)是elephants的分类特征模型</p>
</li>
<li><p>p(x|y=0)是dogs的分类特征模型</p>
</li>
</ul>
<p>然后通过p(x|y)来判断特征x所属的类别，根据贝叶斯公式：</p>
<p>$$p(y=1|x) = \dfrac{p(x|y=1)p(x)}{p(x)}$$</p>
<p>在给定了x的情况下p(x)是个定值，p(y)是先验分布，那么计算方法如下：</p>
<p>$$arg\max_yp(y|x) = arg\max_{y}\dfrac{p(x|y)p(y)}{p(x)}= arg\max_{y}p(x|y)p(y)$$</p>
<p>总结下就是：</p>
<ul>
<li>生成模型：一般是学习一个代表目标的模型，然后通过它去搜索图像区域，然后最小化重构误差。类似于生成模型描述一个目标，然后就是模式匹配了，在图像中找到和这个模型最匹配的区域，就是目标了。</li>
</ul>
<ul>
<li>判别模型：以分类问题为例，然后找到目标和背景的决策边界。它不管目标是怎么描述的，那只要知道目标和背景的差别在哪，然后你给一个图像，它看它处于边界的那一边，就归为哪一类。</li>
</ul>
<ul>
<li> 由生成模型可以得到判别模型，但由判别模型得不到生成模型。</li>
</ul>
<p>然鹅，生成模型p(x|y)怎么得到呢？不慌，我们先了解下多维正态分布～</p>
<h4 id="多维正态分布-the-multivariate-nirmal-distribution"><a href="#多维正态分布-the-multivariate-nirmal-distribution" class="headerlink" title="多维正态分布(the multivariate nirmal distribution)"></a>多维正态分布(the multivariate nirmal distribution)</h4><p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/multi_normal.png"></p>
<p>关于一维正态分布怎么推导出多维正态分布的概率密度函数，可参考知乎:<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/36339816">多维高斯分布是如何由一维发展而来的？</a></p>
<p>首先一维正态分布:</p>
<p>$p(x) = \dfrac{1}{\sqrt{2\pi}}exp(\dfrac{-x^2}{2})$</p>
<p>二维标准正态分布，就是两个独立的一维标准正态分布随机变量的联合分布：</p>
<p>$p(x,y) = p(x)p(y)=\dfrac{1}{2\pi}exp(-\dfrac{x^2+y^2}{2})$</p>
<p>把两个随机变量组合成一个随机向量：$v=[x\quad y]^T$</p>
<p>$p(v)=\dfrac{1}{2\pi}exp(-\dfrac{1}{2}v^Tv)\quad$ 显然x,y相互独立的话，就是上面的二维标准正态分布公式～</p>
<p>然后从标准正态分布推广到一般正态分布，通过一个线性变化：$v=A(x-\mu)$</p>
<p>$p(x)=\dfrac{|A|}{2\pi}exp[-\dfrac{1}{2}(x-\mu)^TA^TA(x-\mu)]$</p>
<p>注意前面的系数多了一个|A|（A的行列式）。</p>
<p>可以证明这个分布的均值为$\mu$，协方差为$(A^TA)^{-1}$。记$\Sigma = (A^TA)^{-1}$，那就有</p>
<p>$$p(\mathbf{x}) = \frac{1}{2\pi|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \mu) ^T \Sigma^{-1} (\mathbf{x} - \mu) \right]$$</p>
<p>推广到n维：</p>
<p>$$p(\mathbf{x}) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \mu) ^T \Sigma^{-1} (\mathbf{x} - \mu) \right]$$</p>
<blockquote>
<p>需要注意的是：这里的二维、n维到底指的是什么？</p>
</blockquote>
<ul>
<li><p>以飞机检测的数据点为例，假设它由heat和time决定，那么这就是个二维正态分布，数据点的生成所处的位置由其概率决定，也就是$p(\mathbf{x})$</p>
</li>
<li><p>如果这个数据有n个特征，那么其分布就是n维正态分布。</p>
</li>
<li><p>之前一直理解的是，n维正态分布是两个向量巴拉巴拉。。好像一直没搞懂。。</p>
</li>
</ul>
<p>再顺便了解下协方差矩阵吧～</p>
<h4 id="关于协方差矩阵，参考blog"><a href="#关于协方差矩阵，参考blog" class="headerlink" title="关于协方差矩阵，参考blog"></a>关于协方差矩阵，<a target="_blank" rel="noopener" href="http://blog.csdn.net/zhengjihao/article/details/78030918">参考blog</a></h4><p>对多维随机变量$X=[X_1,X_2,…,X_n]^T$，我们往往需要计算各维度之间的协方差，这样协方差就组成了一个n×n的矩阵，称为协方差矩阵。协方差矩阵是一个对角矩阵，对角线上的元素是各维度上随机变量的方差,非对角线元素是维度之间的协方差。 我们定义协方差为$\Sigma$, 矩阵内的元素$\Sigma_{ij}$为:</p>
<p>$$\Sigma_{ij} = cov(X_i,X_j) = E[(X_i - E(X_i)) (X_j - E(X_j))]$$</p>
<p>则协方差矩阵为:</p>
<p>$$<br>\Sigma = E[(X-E(X)) (X-E(X))^T] = \left[</p>
<p>\begin{array}{cccc}</p>
<p>cov(X_1,X_1) &amp; cov(X_1,X_2) &amp; \cdots &amp; cov(X_1,X_n) \</p>
<p>cov(X_2,X_1) &amp; cov(X_2,X_2) &amp; \cdots &amp;cov(X_2,X_n) \</p>
<p>\vdots &amp; \vdots&amp; \vdots &amp; \vdots \</p>
<p>cov(X_n,X_1) &amp; cov(X_n,X_2,)&amp;\cdots&amp; cov(X_n,X_n)</p>
<p>\end{array}</p>
<p>\right]<br>$$<br>如果X~$N(\mu,\Sigma)$,则$Cov(X)=\Sigma$</p>
<p>可以这么理解协方差，对于n维随机变量X，第一维是体重$X_1$，第二维是颜值$X_2$，显然这两个维度是有一定联系的，就用$cov(X_1,X_2)$来表征，这个值越小，代表他们越相似。协方差怎么求，假设有m个样本，那么所有的样本的第一维就构成$X_1$…不要把$X_1$和样本搞混淆了。</p>
<p>了解了多维正态分布和协方差，我们再回到生成模型p(x|y)。。其实我们就是假设对于n维特征，p(x|y)是n维正态分布～怎么理解呢，下面就说！</p>
<h4 id="高斯判别分析模型The-Gaussian-Discriminant-Analysis-model"><a href="#高斯判别分析模型The-Gaussian-Discriminant-Analysis-model" class="headerlink" title="高斯判别分析模型The Gaussian Discriminant Analysis model"></a>高斯判别分析模型The Gaussian Discriminant Analysis model</h4><p>高斯判别模型就是：假设p(x|y)是一个多维正态分布，为什么可以这么假设呢？因为对于给定y的条件下对应的特征x都是用来描述这一类y的，比如特征是n维的，第一维描述身高，一般都是满足正态分布的吧，第二维描述体重，也可认为是正态分布吧～</p>
<p>则生成模型：</p>
<p>y ~ Bernoulli($\phi)$ 伯努利分布，又称两点分布，0-1分布</p>
<p>x|y=0 ~ $N(u_0,\Sigma)$</p>
<p>x|y=1 ~ $N(u_1,\Sigma)$</p>
<ul>
<li>这里可以看作是一个二分类，y=0和y=1,可以看作是伯努利分布，则$p(y)=\phi^y(1-\phi)^{1-y}$，要学的参数之一: $\phi=p(y=1)$，试想如果是多分类呢，那么要学习的参数就有$\phi_1,\phi_2,….\phi_k$</li>
</ul>
<ul>
<li>其中类别对应的特征x|y=0,x|y=1服从正态分布。怎么理解呢？就是既然你们都是一类人，那么你们的身高啊，体重啊等等应该满足正态分布。。有几维特征就满足几维正态分布</li>
</ul>
<ul>
<li>这里x是n维特征，身高，体重，颜值…balabala，所以x|y=0满足n维正态分布～x|y=1也是啦，只不过对于不同的类，对应n维特征的均值不一样，奇怪为什么协方差矩阵是一样的？？这里是将它特殊化了，后面会讲的一般性的em算法就不是这样的了</li>
</ul>
<ul>
<li>每个分类对应的n维特征的分布显然不是独立的，比如体重和颜值还是有关系的吧～他们的协方差，方差就统统都在$\Sigma$协方差矩阵里面了</li>
</ul>
<p>$$p(x|y=0) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_0) ^T \Sigma^{-1} (x - \mu_0) \right]$$</p>
<p>$$p(x|y=1) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_1) ^T \Sigma^{-1} (x - \mu_1) \right]$$</p>
<p>这样，模型中我们要学习的参数有$\phi,\Sigma, \mu_0,\mu_1$，对于训练数据，就是观测到的数据x,y，既然他们出现了，那么他们的联合概率，也就是似然函数$\prod_{i=1}^mp(x,y)$就要最大～其对数似然log-likelihood：</p>
<p>$$\begin{equation}\begin{aligned}</p>
<p>L(\phi,\Sigma, \mu_0,\mu_1) &amp;= log\prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,\Sigma, \mu_0,\mu_1$) \</p>
<p>  &amp;= log\prod_{i=1}^mp(x^{(i)}|y^{(i)};\phi,\Sigma, \mu_0,\mu_1$) p(y^{(i)};\phi)\</p>
<p>\end{aligned}\end{equation}\label{eq2}$$</p>
<p>其中$p(y^{(i)};\phi)$是已知的，也就是先验概率(class priors)，$p(x^{(i)}|y^{(i)})$就是上面推导的～代入后，分别对参数求导即可：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/GDA.png"></p>
<p>在回过头来看这些公式，</p>
<ul>
<li><p>$\phi$很好理解，就是样本中正分类的概率。</p>
</li>
<li><p>$\mu_0$就是负分类中x对应的均值</p>
</li>
<li><p>$\mu_1$就是正分类中x对应的均值</p>
</li>
<li><p>$\Sigma$就是$(x-\mu_1)$和$x-\mu_2$的协方差矩阵</p>
</li>
</ul>
<p>然后通过p(x|y=0),p(x|y=1)即可对需要预测的x求出对应的概率，然后做出判别了。这样看来，如果直接对x|y=1,和x|y=0做出了正态分布的猜测，就可以直接写出来了。只不过，我们用极大似然估计重新推导了一遍。</p>
<h3 id="高斯混合模型GMM"><a href="#高斯混合模型GMM" class="headerlink" title="高斯混合模型GMM"></a>高斯混合模型GMM</h3><h4 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h4><p>前面GDA是有标签的，也算是有监督学习。而在没有标签的情况下呢，就是无监督学习了，虽然我们无法给出x所属的类叫啥，但是我们可以判断出哪些x是同一类，以及样本中总共有多少类（虽然这个类数嘛。。类似于k-means的类数，可根据交叉验证选择）。</p>
<p>其实和GDA非常相似，不过这里没有了类标签，只有一堆样本特征，${x^{(1)},x^{(2)},…,x^{(m)}}$,</p>
<p>我们不知道这些样本属于几个类别，也不知道有哪些类了。但虽然不知道，我们确定他们是存在的，只是看不见而已。我们可以假设存在k类，${z^{(1)},z^{(2)},…,z^{(k)}}$,看不见的，我们就叫它们隐藏随机变量(latent random variable)，</p>
<p>这样一来，就训练样本就可以用这样的联合分概率模型表示了，$p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})$</p>
<ul>
<li>同GDA不一样的是，这里是多分类，可假定$z^{(i)}\sim Multinomial(\phi)$，多项式分布（二项分布的拓展～），那么$p(z^{(i)})=\phi_j$</li>
</ul>
<ul>
<li>同GDA相同的是，对于每一个类别，其对应的样本满足n维正态分布，也就是：$x^{(i)}|z^{(i)}=j\sim N(\mu_j,\Sigma_j)$,但注意哦，这里每个高斯分布使用了不同的协方差矩阵$\Sigma_j$</li>
</ul>
<p>$$p(x|z^{(1)}) = \frac{1}{(2\pi)^{n/2}|\Sigma_0|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_0) ^T \Sigma_0^{-1} (x - \mu_0) \right]$$</p>
<p>$$p(x|z^{(2)}) = \frac{1}{(2\pi)^{n/2}|\Sigma_1|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_1) ^T \Sigma_1^{-1} (x - \mu_1) \right]$$</p>
<p>$$….$$</p>
<p>$$p(x|z^{(k)}) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_k) ^T \Sigma_k^{-1} (x - \mu_k) \right]$$</p>
<p>然后带入到训练样本的对数似然（log-likelihood）:</p>
<p>$$L(\phi,\mu,\Sigma)=\sum_{i=1}^{m}logp(x^{(i)};\phi,\mu,\Sigma)$$</p>
<p>$$L(\phi,\mu,\Sigma)=\sum_{i=1}^{m}log\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)};\mu,\Sigma) p(z^{(i)};\phi)\$$</p>
<p>这里需要注意下标：对于类别有k类，第一个求和符号是对第i个样本在k个类别上的联合概率，第二个求和符号是m个样本的联合概率。</p>
<p>我们可以注意到，如果我们知道$z^{(i)}$,那么这个似然函数求极大值就很容易了，类似于高斯判别分析，这里的$z^{(i)}$相当于标签，分别对参数求导可得：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/GMM2.png"></p>
<p>其中的参数:</p>
<ul>
<li>$1{z^{(i)}=j}$表示第i个样本为j类时，这个值就为１，那么$\phi_j=\frac{1}{m}\sum_{i=1}^m1{z^{(i)}=j}$表示样本中类别为j的概率</li>
</ul>
<ul>
<li>其中$p(z^{(i)};\phi)$是根据伯努利分布得到的，在GDA中$p(y|\phi)$是已知的频率概率。</li>
</ul>
<p>So $z^{(i)}$ 到底有多少个分类？每个类别的概率是多少？譬如上式中 $\sum_{i=1}^{m}1{z^{(i)}=j}$ 这个没法求对吧～它是隐藏变量！所以还是按照这个方法是求不出来的～</p>
<p>这个时候EM算法就登场了～～～</p>
<h4 id="用EM算法求解GMM模型"><a href="#用EM算法求解GMM模型" class="headerlink" title="用EM算法求解GMM模型"></a>用EM算法求解GMM模型</h4><p>上面也提到了，如果$z^({i})$是已知的话，那么$\phi_j=\frac{1}{m}\sum_{i=1}^m1{z^{(i)}=j}$表示类别j的概率$p(z^{(i)}=j)$也就已知了，但是呢？我们不知道。。所以我们要猜测$p(z^{(i)}=j)$这个值，也就是EM算法的第一步：</p>
<p><strong>Repeat until convergence 迭代直到收敛:{</strong></p>
<p>(E-step):for each i,j,set:</p>
<p>$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)$</p>
<p>$w_j^{(i)}$什么意思呢?就是对于i样本,它是j类的后验概率。在GDA里面，x_i的类别是确定的，在GMM里面呢？不知道它的类别，所以只能假设k类都有可能，它是j类别的概率就是$w_j^{(i)}$，它仅仅取决于$\phi_j$,而在GMM里面，它取决于$\phi_j,\mu_j,\Sigma_j$，实际上$w_j^{(i)}$的值，就包含了两个我们在GMM所做的假设，多项式分布和正态分布。</p>
<blockquote>
<p>The values $w_j$ calculated in the E-step represent our “soft” guesses for</p>
</blockquote>
<p>the values of $z^{(i)}$ .</p>
<p>The term “soft” refers to our guesses being probabilities and taking values in [0, 1]; in</p>
<p>contrast, a “hard” guess is one that represents a single best guess (such as taking values</p>
<p>in {0, 1} or {1, . . . , k}).</p>
<p>硬猜测是k均值聚类，GMM是软猜测。</p>
<p>这样一来，参数更新就可以这样写了，也就是EM算法的第二步：</p>
<p>(M-step) Updata the parameters:</p>
<p>然后对似然函数求导，后面会详细介绍</p>
<p>$$\phi_j:=\frac{1}{m}\sum_{i=1}^mw_j^{(i)}$$</p>
<p>$$\mu_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<p>$$\Sigma_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<p><strong>｝</strong></p>
<p>训练过程的理解可参考<a target="_blank" rel="noopener" href="http://blog.pluskid.org/?p=39">blog</a></p>
<p><strong>$w_j^{(i)}表示第i个样本为ｊ类别的概率，而\phi_j$表示m个样本中j类别的概率，$\mu_j,\Sigma_j$分别表示j类别对应的n维高斯分布的期望和协方差矩阵</strong></p>
<p>所以，求出$w_j^{(i)}$，一切就都解决了吧？对于后验概率$p(z^{(i)}=j|x^{(i)})$可以根据Bayes公式：</p>
<p>$$p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\dfrac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$</p>
<ul>
<li>其中先验概率$p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$可以根据高斯分布的密度函数来求，$z^{(i)}=j\sim N(\mu_j,\Sigma_j)$</li>
</ul>
<ul>
<li>类先验(class priors)$p(z^{(i)}=j;\phi)$可以取决于多项式分布中j类的概率$\phi_j$</li>
</ul>
<blockquote>
<p>The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments $c^(i)$, we instead have the “soft” assignments $w_j$ . Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea.  </p>
</blockquote>
<p>EM算法使我们联想起了k-means,区别在于k-means的聚类是通过欧氏距离c(i)来定义的，而EM是通过$w_j$probabilities来分类的。同k-means一样，这里的EM算法也是局部优化，因此最好采用不同的方式初始化～</p>
<h4 id="convergence"><a href="#convergence" class="headerlink" title="convergence?"></a>convergence?</h4><p>我们知道k-means一定是收敛的，虽然结果不一定是全局最优解，但它总能达到一个最优解。但是EM算法呢，也是收敛的。</p>
<h3 id="The-EM-algorithm"><a href="#The-EM-algorithm" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h3><p>前面我们讲的是基于高斯混合模型的EM算法，但一定所有的类别都是高斯分布吗？还有卡方分布，泊松分布等等呢，接下来我们就将讨论EM算法的一般性。</p>
<p>在学习一般性的EM算法前，先了解下Jensen’s inequality</p>
<h4 id="Jensen’s-inequality"><a href="#Jensen’s-inequality" class="headerlink" title="Jensen’s inequality"></a>Jensen’s inequality</h4><p>如果函数$f$，其二阶导恒大与等于0 $(f^{‘’}\ge 0)$，则它是凸函数f(convec function)。</p>
<p>如果凸函数的输入是向量vector-valued inputs，那么它的海森矩阵(hessian)H是半正定的。Jensen’s 不等式：</p>
<blockquote>
<p>Let f be a convex function, and let X be a random variable.</p>
</blockquote>
<p>Then:</p>
<p>$$E[f (X)] ≥ f (EX).$$</p>
<p>Moreover, if f is strictly convex, then $E[f (X)] = f (EX)$ holds true if and</p>
<p>only if $X = E[X]$ with probability 1 (i.e., if X is a constant).</p>
<p>举个栗子来解释jensen不等式：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/jensen.png"></p>
<p>假设输入随机变量X是一维的哈，然后Ｘ取a,b的概率都是0.5,那么</p>
<p>$$EX=(a+b)/2,f(EX)=f(\dfrac{a+b}{2})$$,$$E[f(X)]=\dfrac{f(a)+f(b)}{2}$$</p>
<p>因为是凸函数，所以 $f(EX)\le E[f(X)]$</p>
<p>同理，如果是凹函数(concave function),那么不等式方向相反$f(EX)\ge E[f(X)]$。后面EM算法里面就要用到log(X)，log(x)就是个典型的凹函数～</p>
<h4 id="The-EM-algorithm-1"><a href="#The-EM-algorithm-1" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h4><p>首先，问题是：我们要基于给定的m个训练样本${x^{(1)},x^{(2)},…,x^{(m)}}$来进行密度估计～</p>
<p>像前面一样，创建一个参数模型p(x,z)来最大化训练样本的对数似然：</p>
<p>$$L(\theta)=\sum_{i=1}^mlogp(x;\theta)$$</p>
<p>$$L(\theta)=\sum_{i=1}^mlog\sum_zp(x,z;\theta)$$</p>
<p>一般性就是把前面特殊化的假设去掉，没有了正态分布和多项式分布。</p>
<p>可以看到，$z^{(i)}$是隐藏的随机变量(latent random variable),关于参数$\theta$的最大似然估计就很难计算了。</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em2.png"></p>
<p>解释下公式中的推导：</p>
<ul>
<li>这里是针对样本i来说，对于样本i，它可能是$z^1,z^2,…,z^k$都有可能，但他们的probability之和为１，也就是</li>
</ul>
<p>$\sum_zQ_i(z)=1$</p>
<ul>
<li>(2)到(3)的推导：可以将</li>
</ul>
<p>$\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$</p>
<p>看做随机变量Ｘ,那么（２）式中的后半部分 $log\sum_{z^{(i)})}[\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]$就是log(EX)了，$logx$是一个凹函数，则其大于$E[log(x)]$</p>
<p><font size="4" color="#D2691E">EM迭代过程(重点):</font></p>
<ul>
<li><p>（1）根据上式可以看做$L(\theta)\ge J(Q,\theta)$.两边都是关于$\theta$的函数，那么将$\theta$固定，调整Q在一定条件下能使等式成立。</p>
</li>
<li><p>（2）然后固定Q,调整$\theta^t$到$\theta^{t+1}$找到下界函数的最大值$J(Q,\theta^{t+1})$.显然在当前Q的条件下，$L(\theta^{t+1})\ne J(Q,\theta^{t+1})$,那么根据Jensen不等式，$L(\theta_{t+1})&gt;J(Q,\theta^{t+1})=L(\theta^{t})$,也就是说找到了使得对数似然L更大的$\theta$.这不就是我们的目的吗？！</p>
</li>
<li><p>然后迭代循环(1)(2)步骤，直到在调整$\theta$时，下界函数$J(Q,\theta)$不在增加，即小于某个阈值。</p>
</li>
</ul>
<p>看下Ng画的图：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em1.png" alt="em1.png"></p>
<p>任意初始化$\theta$和Q,然后找下界函数和$l(\theta)$交接的点，这就是EM算法的第一步：</p>
<p>我们要让不等式相等,即Jensen’s inequality中的随机变量取值是一个常量，看(2)式：</p>
<p>$$\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c$$</p>
<p>对左边分子分母同时对z求类和：</p>
<p>$$\dfrac{\sum_zp(x^{(i)},z^{(i)};\theta)}{\sum_zQ_i(z^{(i)})}=c$$</p>
<p>根据$\sum_zQ_i(z)=1$：</p>
<p>$$\sum_zp(x^{(i)},z^{(i)};\theta)=c$$</p>
<p>带回去可得：</p>
<p>$$Q_i(z^{(i)})=\dfrac{p(x^{(i)},z^{(i)};\theta)}{\sum_zp(x^{(i)},z^{(i)};\theta)}$$</p>
<p>$$Q_i(z^{(i)})=\dfrac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}$$</p>
<p>$$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta)$$</p>
<p>EM总结下来：</p>
<p>Repeat until convergence {</p>
<p>(E-step)</p>
<p>For each i,找到下界函数, set:</p>
<p>$$Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)$$</p>
<p>(M-step)找到下界凹函数的最大值,也就是(3)式 Set:</p>
<p>$$\theta:=arg\max_{\theta}\sum_i^m\sum_{z^{(i)}}^kQ_i(z^{(i)})log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$</p>
<p>}</p>
<p><font size="4" color="#D2691E">要理解的是：</font></p>
<p>EM算法只是一种计算方式，对于上式中的$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta)$我们还是要根据假设来求得，比如GMM中的多类高斯分布。然后带回到对数似然中，通过求导得到参数估计。我们费尽心机证明EM算法收敛，只是为了去证明这样去求似然函数的极大值是可行的，然后应用到类似于GMM，HMM中。</p>
<h4 id="training-and-will-converge"><a href="#training-and-will-converge" class="headerlink" title="training and will converge?"></a>training and will converge?</h4><p>首先说是否收敛，答案是肯定收敛的。。懒得输公式了。。直接贴图吧，这个比较好理解：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em3.png" alt="em3.png"></p>
<p>上面写这么多，其实就是证明$L(\theta_{t+1})&gt;L(\theta_t)$.</p>
<h4 id="Mixture-of-Gaussians-revisited"><a href="#Mixture-of-Gaussians-revisited" class="headerlink" title="Mixture of Gaussians revisited"></a>Mixture of Gaussians revisited</h4><p>我们知道了em算法是一种计算方式，用来解决含有隐变量似然对数很难求的问题，那么我们把它运用到GMM中。</p>
<p><font size="4" color="#D2691E">E step:</font></p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em11.png"></p>
<p>$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)$</p>
<p>$$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\dfrac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em13.png"></p>
<ul>
<li><p>其中先验概率$p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$可以根据高斯分布的密度函数来求，$z^{(i)}=j\sim N(\mu_j,\Sigma_j)$</p>
</li>
<li><p>类先验(class priors)$p(z^{(i)}=j;\phi)$可以取决于多项式分布中j类的概率$\phi_j$</p>
</li>
</ul>
<p>这样我们就完成了对$w_j^{(i)}$的soft ‘guess’，也就是E step.</p>
<p><font size="4" color="#D2691E">M step:</font></p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em12.png"></p>
<p>然后对参数求导：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em14.png"></p>
<p>详细推导过程，参考cs229-notes8</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em15.jpeg"></p>
<p>我们在整体回顾一下整个过程，所谓的E step就是找到$Q_i(z^{j}),w_i^j$（在一定假设下是可以通过bayes公式求得的），使得下界函数与log函数相等，也就是Jensen取等号时。然后是M step就是在Q的条件下找到下界函数最大值，也就是对参数求导，导数为0的地方。</p>
<p>然后在根据求得的参数，再求Q，再带入求导。。。迭代直到收敛。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/23/">上一页</a></div><div class="pagination-next"><a href="/page/25/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/23/">23</a></li><li><a class="pagination-link is-current" href="/page/24/">24</a></li><li><a class="pagination-link" href="/page/25/">25</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">121</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>