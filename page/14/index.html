<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-22T10:56:03.000Z" title="2018/9/22 下午6:56:03">2018-09-22</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">12 分钟读完 (大约1777个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/">论文笔记-QANet</a></h1><div class="content"><p>paper:</p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.09541">combining local convolution with local self-attention for reading comprehension</a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.   </p>
</blockquote>
<p> encoder 编码方式仅仅由 卷积 和 自注意力 机制构成，没了 rnn 速度就是快。</p>
<blockquote>
<p>The key motivation behind the design of our model is the following: convolution captures the local structure of the text, while the self-attention learns the global interaction between each pair of words.  </p>
</blockquote>
<p> 这篇论文最主要的创新点：使用 CNN 来捕捉文本结构的局部信息，使用 self-attention 来学习全局中每两个词之间的交互信息，使得其能耦合上下文信息。相比 RNN，attention 能够有效的解决长期依赖问题。只是相比少了词序信息。说到底，也是一种 contextualize 的 encoder 方式。</p>
<blockquote>
<p>we propose a complementary data augmentation technique to enhance the training data. This technique paraphrases the examples by translating the original sentences from English to another language and then back to English, which not only enhances the number of training instances but also diversifies the phrasing.  </p>
</blockquote>
<p>使用了一种数据增强的方式，先将源语言转换成另一种语言，然后再翻译回英语。这样能有效增加训练样本，同时也丰富了短语的多样性。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa01.png"></p>
<p>模型分为5部分:  </p>
<ul>
<li><p>an embedding layer  </p>
</li>
<li><p>an embedding encoder layer  </p>
</li>
<li><p>a context-query attention layer  </p>
</li>
<li><p>a model encoder layer  </p>
</li>
<li><p>an output layer.</p>
</li>
</ul>
<blockquote>
<p>the combination of convolutions and self-attention is novel, and is significantly better than self-attention alone and gives 2.7 F1 gain in our experiments. The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth (layer dropout) (Huang et al., 2016), which gives an additional gain of 0.2 F1 in our experiments.  </p>
</blockquote>
<p>CNN 和 self-attention 的结合比单独的 self-attention 效果要好。同时使用了 CNN 之后能够使用常用的正则化方式 dropout, 这也能带来一点增益。</p>
<h3 id="Input-embedding-layer"><a href="#Input-embedding-layer" class="headerlink" title="Input embedding layer"></a>Input embedding layer</h3><blockquote>
<p>obtain the embedding of each word w by concatenating its word embedding and character embedding.</p>
</blockquote>
<p>由词向量和字符向量拼接而成。其中词向量采用预训练的词向量 Glove，并且不可训练，fixed. 只有 OOV (out of vocabulary) 是可训练的，用来映射所有不在词表内的词。</p>
<blockquote>
<p>Each character is represented as a trainable vector of dimension p2 = 200, meaning each word can be viewed as the concatenation of the embedding vectors for each of its characters. The length of each word is either truncated or padded to 16. We take maximum value of each row of this matrix to get a fixed-size vector representation of each word.  </p>
</blockquote>
<p>字符向量的处理。每个字母是可训练的，对应的维度是 200 维。然后每个词都 truncated 或者 padded 成16个字母，保证每个词的向量维度是一样大小。  </p>
<p>所以一个词的向量维度是 $300+200=500$.</p>
<h3 id="Embedding-encoding-layer"><a href="#Embedding-encoding-layer" class="headerlink" title="Embedding encoding layer"></a>Embedding encoding layer</h3><blockquote>
<p>The encoder layer is a stack of the following basic building block: [convolution-layer × # + self-attention-layer + feed-forward-layer]</p>
</blockquote>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa02.png"></p>
<p>其中：  </p>
<ul>
<li>convolution: 使用 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02357">depthwise separable convolutions</a> 而不是用传统的 convolution，因为作者发现 <strong>it is memory efficient and has better generalization.</strong> 怎么理解这个，还得看原 paper. The kernel size is 7, the number of filters is d = 128.</li>
</ul>
<ul>
<li>self-attention: the multi-head attention mechanism <a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/">论文笔记, Attention Is All You Need</a></li>
</ul>
<blockquote>
<p>Each of these basic operations (conv/self-attention/ffn) is placed inside a residual block, shown lower-right in Figure 1. For an input x and a given operation f, the output is f(layernorm(x))+x.  </p>
</blockquote>
<p>在 cnn/self-attention/ffn 层都有 layer normalization.</p>
<h4 id="为什么要用-CNN："><a href="#为什么要用-CNN：" class="headerlink" title="为什么要用 CNN："></a>为什么要用 CNN：</h4><p>用来获取局部信息 k-gram features</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa03.png"></p>
<p>相信看了这个图能对 QANet 中的 cnn 怎么实现的更清楚了。上图中每个卷积核的尺寸分别是 [2, embed_size], [3, embed_size], [3, embed_size]. padding参数 使用的是 “SAME”. 得到 3 个 [1, sequence_len]，然后拼接起来, 得到最终结果 [filters_num, sequence_len].</p>
<p>在 QANet 的实现中，kernel_size 都设置为7, num_filters=128.</p>
<h4 id="为什么要用-self-attention"><a href="#为什么要用-self-attention" class="headerlink" title="为什么要用 self-attention"></a>为什么要用 self-attention</h4><p>用来获取全局信息。</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa04.png"></p>
<p>上图中的这种方式显然不太好，复杂度高且效果不好。于是有了 self-attention.</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa05.png"></p>
<p>矩阵内部向量之间作內积，并通过 softmax 得到其他词对于 “The” 这个词的权重大小（权重比例与相似度成正比，这里看似不太合理 similarity == match??，但实际上效果很不错，可能跟词向量的训练有关）。</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa06.png"></p>
<p>然后将对应的权重大小 $[w_1,w_2,w_3,w_4,w_5]$ 与对应的词相乘，累和得到蕴含了上下文信息的 contextualized “The”.</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa07.png">  </p>
<p>并且，这是可以并行化的。大大加速了训练速度。</p>
<h3 id="Context-Query-Attention-Layer"><a href="#Context-Query-Attention-Layer" class="headerlink" title="Context-Query Attention Layer"></a>Context-Query Attention Layer</h3><p>跟 BIDAF 是一样的。来，不看笔记把公式过一遍。  </p>
<p>content: $C={c_1, c_2,…,c_n}$   </p>
<p>query: $Q={q_1,q_2,…q_m}$.</p>
<p>所以 embeded 之后，   </p>
<ul>
<li><p>content: [batch, content_n, embed_size]    </p>
</li>
<li><p>query: [batch, query_m, embed_size]  </p>
</li>
</ul>
<p>做矩阵相乘得到相似矩阵 similarity matrix $S\in R^{n\times m}$:    </p>
<p>sim_matrix: [batch, content_n, query_m]</p>
<blockquote>
<p>The similarity function used here is the trilinear function (Seo et al., 2016). $f(q,c)=W_0[q,c,q\circ c]$.  </p>
</blockquote>
<p>相似矩阵的计算可以不是直接矩阵相乘，而是加个前馈神经网络。毕竟 similarity 不一定等于 match.</p>
<h4 id="content-to-query"><a href="#content-to-query" class="headerlink" title="content-to-query"></a>content-to-query</h4><p>对 S 每一行 row 做 softmax 得到对应的概率，得到权重矩阵 $\tilde S\in R^{n\times m}$, shape = [batch, content_n, query_m].</p>
<p>然后与 query $Q^T$ [batch, query_m, embed_size] 矩阵相乘得到编码了 query 信息的 content:  </p>
<p>$A = \tilde SQ^T$, shape = [batch, content_n, embed_size]</p>
<h4 id="query-to-content"><a href="#query-to-content" class="headerlink" title="query_to_content"></a>query_to_content</h4><blockquote>
<p>Empirically, we find that, the DCN attention can provide a little benefit over simply applying context-to-query attention, so we adopt this strategy.  </p>
</blockquote>
<p>这里没有采用 BiDAF 里面的方法，而是采用 DCN 中的方式，利用了 $\tilde S$.</p>
<p>对 S 每一列 column 做 softmax 得到矩阵 $\overline S$, shape = [batch, content_n, query_n].</p>
<p>然后矩阵相乘得到 $B=\tilde S \overline S^T C^T$.  </p>
<p>$\tilde S$.shape=[batch, content_n, query_m]  </p>
<p>$\overline S^T$.shape=[batch, query_m, content_n]  </p>
<p>$C^T$.shape=[batch, query_m, embed_size]  </p>
<p>所以最后 B.shape=[batch, content_n, embed_size]</p>
<h3 id="Model-Encoder-Layer"><a href="#Model-Encoder-Layer" class="headerlink" title="Model Encoder Layer"></a>Model Encoder Layer</h3><p>同 BiDAF 一样输入是 $[c,a,c\circ a,c\circ b]$， 其中 a, b 分别是 attention matrix A，B 的行向量。不过不同的是，这里不同 bi-LSTM，而是类似于 encoder 模块的 [conv + self-attention + ffn]. 其中 conv 层数是 2, 总的 blocks 是7.</p>
<h3 id="Ouput-layer"><a href="#Ouput-layer" class="headerlink" title="Ouput layer"></a>Ouput layer</h3><p>$$p^1=softmax(W_1[M_0;M_1]), p^2=softmax(W_2[M_0;M_2])$$</p>
<p>其中 $W_1, w_2$ 是可训练的参数矩阵，$M_0, M_1, M_2$ 如图所示。</p>
<p>然后计算交叉熵损失函数：  </p>
<p>$$L(\theta)=-\dfrac{1}{N}\sum_i^N[log(p^1_{y^1})+log(p^2_{y^2})]$$</p>
<h3 id="QANet-哪里好，好在哪儿？"><a href="#QANet-哪里好，好在哪儿？" class="headerlink" title="QANet 哪里好，好在哪儿？"></a>QANet 哪里好，好在哪儿？</h3><p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa08.png"></p>
<ul>
<li><p>separable conv 不仅参数量少，速度快，还效果好。将 sep 变成传统 cnn, F1 值减小 0.7.  </p>
</li>
<li><p>去掉 CNN， F1值减小 2.7.  </p>
</li>
<li><p>去掉 self-attention, F1值减小 1.3.</p>
</li>
</ul>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa09.png">  </p>
<ul>
<li><p>layer normalization  </p>
</li>
<li><p>residual connections  </p>
</li>
<li><p>L2 regularization  </p>
</li>
</ul>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa10.png"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01604">Dynamic Coattention Networks For Question Answering</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02357">Xception: Deep Learning with Depthwise Separable Convolutions</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://thinklab.com/content/2940784">qanet_talk_v1.pdf</a></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-01T13:17:28.000Z" title="2018/9/1 下午9:17:28">2018-09-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/TensorFlow/">TensorFlow</a></span><span class="level-item">33 分钟读完 (大约4894个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/01/tensorflow-Attention-API/">Tensorflow Attention API 源码阅读1</a></h1><div class="content"><p>这节内容是详细了解下 tensorflow 关于 attention 的api，由于封装的太好，之前使用过发现报错了很难去找出哪儿 bug，所以用 eager execution 来看具体细节。</p>
<p>按照官方教程 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_guides/python/contrib.seq2seq#Attention">Seq2seq Library (contrib)</a> 这里的流程逐步深入。</p>
<p>This library is composed of two primary components:  </p>
<ul>
<li>New attention wrappers for tf.contrib.rnn.RNNCell objects.</li>
</ul>
<ul>
<li>A new object-oriented dynamic decoding framework.  </li>
</ul>
<p>主要包括两个部分，一个是新的基于 attention 的 RNNCell 对象，一个面向对象的动态解码框架。</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention wrappers are RNNCell objects that wrap other RNNCell objects and implement attention. The form of attention is determined by a subclass of tf.contrib.seq2seq.AttentionMechanism. These subclasses describe the form of attention (e.g. additive vs. multiplicative) to use when creating the wrapper. An instance of an AttentionMechanism is constructed with a memory tensor, from which lookup keys and values tensors are created.</p>
<p>attenion wrapper 也是 RNNCell 对象，父类是 tf.contrib.seq2seq.AttentionMechanism,然后其子类是针对不同 attention 形式（additive vs. multiplicative）的实现。AttentionMechanism 的构造是在 memory 的基础上，memory 也就是 attention 过程中的 keys values.</p>
<h2 id="Attention-Mechanisms"><a href="#Attention-Mechanisms" class="headerlink" title="Attention Mechanisms"></a>Attention Mechanisms</h2><p>attention 的提出来自于：    </p>
<ul>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>  </p>
</li>
<li><p>paper:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></p>
</li>
</ul>
<p><img src="https://panxiaoxie.cn/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"></p>
<ol>
<li>encoder 采用单层或多层的单向或双向的 rnn 得到source sentence 的隐藏状态表示 $H=[h_1,…,h_T]$。  </li>
</ol>
<ol start="2">
<li>decoder 的 t 时间步的隐藏状态为 $s_t$, 在 decoder 阶段也是 rnn，其中隐藏状态的更新为： $s_i=f(s_{i-1},y_{i-1},c_i)$ 其中 $s_{i-1}$ 是上一个隐藏状态，$y_{i-1}$ 是上一时间步的输出，$c_i$ 是当前时间步的 attention vector. 那么现在就是怎么计算当前时间步的 $c_i$.  </li>
</ol>
<ol start="3">
<li>当前时间步的 $e_t=a(s_{i-1}, h_j)$, 这是对齐模型，也就是计算上一个隐藏状态 $s_{i-1}$ 与 encoder 中每一个 hidden 的 match 程度，计算这个 score 有很多中方式，其中最常见的，也是 tf api 中使用的两种 BahdanauAttention 和 LuongAttention.</li>
</ol>
<p>$$\text{BahdanauAttention:}\quad e_{ij}=v_a^Ttanh(W^as_{i-1}+U_ah_j)$$</p>
<p>$$\text{LuongAttention:}\quad e_{ij}=h_j^TW^as_i$$</p>
<ol start="4">
<li>然后对得到的对齐 score 使用 softmax 得到相应的概率:</li>
</ol>
<p>$$a_{ij}=\dfrac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}$$</p>
<p>softmax 实际上相比上面的公式有点区别，就是 $exp(e^{ij}-max(e^{ik}))$ 防止数值溢出。</p>
<ol start="5">
<li>将得到的 $s_{i-1}$ 与 encoder 中的 $h_j$ 计算得到的概率与 $h_j$ 做加权和得到当前时间步的 attention vector $c_i$</li>
</ol>
<ol start="6">
<li>在然后使用 $c_{i-1},s_{i-1},y_{i-1}$ 更新decoder 中的隐藏状态，循环下去。。。</li>
</ol>
<ol start="7">
<li>根据当前的隐藏状态 $s_i$ 计算得到当前时间步的输出 $y_t$</li>
</ol>
<p>$$y_t=Ws_{i}+b$$</p>
<h3 id="先看父类-tf-contrib-seq2seq-AttentionMechanism"><a href="#先看父类-tf-contrib-seq2seq-AttentionMechanism" class="headerlink" title="先看父类 tf.contrib.seq2seq.AttentionMechanism"></a>先看父类 tf.contrib.seq2seq.AttentionMechanism</h3><p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionMechanism</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alignments_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>两个属性： alignments_size 和 state_size 分别对应 sequence 的长度，所以这个 alignment_size 是表示 mask 之后的长度吧？接下来看源码。 state_size 表示隐藏层的状态。显然这里的 attention 也是一个时间步内的计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseAttentionMechanism</span>(<span class="params">AttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A base AttentionMechanism class providing common functionality.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Common functionality includes:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    1. Storing the query and memory layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    2. Preprocessing and storing the memory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               query_layer,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_layer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               check_inner_dims_defined=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct base AttentionMechanism class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      query_layer: Callable.  Instance of `tf.layers.Layer`.  The layer&#x27;s depth</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        must match the depth of `memory_layer`.  If `query_layer` is not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        provided, the shape of `query` must match that of `memory_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory: The memory to query; usually the output of an RNN encoder.  This</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        tensor should be shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      probability_fn: A `callable`.  Converts the score and previous alignments</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        to probabilities. Its signature should be:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probabilities = probability_fn(score, state)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_sequence_length (optional): Sequence lengths for the batch entries</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in memory.  If provided, the memory tensor rows are masked with zeros</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for values past the respective sequence lengths.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_layer: Instance of `tf.layers.Layer` (may be None).  The layer&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        depth must match the depth of `query_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If `memory_layer` is not provided, the shape of `memory` must match</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        that of `query_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      check_inner_dims_defined: Python boolean.  If `True`, the `memory`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        argument&#x27;s shape is checked to ensure all but the two outermost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        dimensions are fully defined.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      score_mask_value: (optional): The mask value for score before passing into</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probability_fn`. The default is -inf. Only used if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `memory_sequence_length` is not None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: Name to use when creating ops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (query_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(query_layer, layers_base.Layer)):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;query_layer is not a Layer: %s&quot;</span> % <span class="built_in">type</span>(query_layer).__name__)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (memory_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(memory_layer, layers_base.Layer)):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;memory_layer is not a Layer: %s&quot;</span> % <span class="built_in">type</span>(memory_layer).__name__)</span><br><span class="line"></span><br><span class="line">    self._query_layer = query_layer</span><br><span class="line"></span><br><span class="line">    self._memory_layer = memory_layer</span><br><span class="line"></span><br><span class="line">    self.dtype = memory_layer.dtype</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">callable</span>(probability_fn):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(<span class="string">&quot;probability_fn must be callable, saw type: %s&quot;</span> %</span><br><span class="line"></span><br><span class="line">                      <span class="built_in">type</span>(probability_fn).__name__)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> score_mask_value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      score_mask_value = dtypes.as_dtype(</span><br><span class="line"></span><br><span class="line">          self._memory_layer.dtype).as_numpy_dtype(-np.inf)</span><br><span class="line"></span><br><span class="line">    self._probability_fn = <span class="keyword">lambda</span> score, prev: (  <span class="comment"># pylint:disable=g-long-lambda</span></span><br><span class="line"></span><br><span class="line">        probability_fn(</span><br><span class="line"></span><br><span class="line">            _maybe_mask_score(score, memory_sequence_length, score_mask_value),</span><br><span class="line"></span><br><span class="line">            prev))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ops.name_scope(</span><br><span class="line"></span><br><span class="line">        name, <span class="string">&quot;BaseAttentionMechanismInit&quot;</span>, nest.flatten(memory)):</span><br><span class="line"></span><br><span class="line">      self._values = _prepare_memory(</span><br><span class="line"></span><br><span class="line">          memory, memory_sequence_length,</span><br><span class="line"></span><br><span class="line">          check_inner_dims_defined=check_inner_dims_defined)</span><br><span class="line"></span><br><span class="line">      self._keys = (</span><br><span class="line"></span><br><span class="line">          self.memory_layer(self._values) <span class="keyword">if</span> self.memory_layer  <span class="comment"># pylint: disable=not-callable</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">else</span> self._values)</span><br><span class="line"></span><br><span class="line">      self._batch_size = (</span><br><span class="line"></span><br><span class="line">          self._keys.shape[<span class="number">0</span>].value <span class="keyword">or</span> array_ops.shape(self._keys)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">      self._alignments_size = (self._keys.shape[<span class="number">1</span>].value <span class="keyword">or</span></span><br><span class="line"></span><br><span class="line">                               array_ops.shape(self._keys)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">memory_layer</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._memory_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">query_layer</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._query_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">values</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">keys</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._keys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">batch_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alignments_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._alignments_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._alignments_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initial_alignments</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates the initial alignment values for the `AttentionWrapper` class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is important for AttentionMechanisms that use the previous alignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    to calculate the alignment at the next time step (e.g. monotonic attention).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The default behavior is to return a tensor of all zeros.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: `int32` scalar, the batch_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The `dtype`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A `dtype` tensor shaped `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is the values&#x27; `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    max_time = self._alignments_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> _zero_state_tensors(max_time, batch_size, dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initial_state</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates the initial state values for the `AttentionWrapper` class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is important for AttentionMechanisms that use the previous alignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    to calculate the alignment at the next time step (e.g. monotonic attention).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The default behavior is to return the same output as initial_alignments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: `int32` scalar, the batch_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The `dtype`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A structure of all-zero tensors with shapes as described by `state_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.initial_alignments(batch_size, dtype)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>这个类 _BaseAttentionMechanism 是最基本的 attention 类了。可以看到 self._keys 和 self._values 的计算方式都是需要考虑 memory_sequence_length 这个参数的。</p>
<p>有这几个属性：    </p>
<ul>
<li><p>values: 其计算使用了 _prepare_memory 函数对应的是把输入序列 memory 的超过对应实际长度的部分的值变为 0    </p>
</li>
<li><p>keys： self._keys = self.memory_layer(self._values)  是在得到了 values 之后进行全链接的值，其shape=[batch, max_times, num_units]  </p>
</li>
<li><p>state_size 和 alignment_size 是一样的，都是 max_times  </p>
</li>
<li><p>self._probability_fn(score, prev) 使用了 _maybe_mask_score 这个函数计算得到 score 之后并 mask 的概率，然后还要利用 prev state?</p>
</li>
</ul>
<h4 id="maybe-mask-score"><a href="#maybe-mask-score" class="headerlink" title="_maybe_mask_score"></a>_maybe_mask_score</h4><p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_maybe_mask_score</span>(<span class="params">score, memory_sequence_length, score_mask_value</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">  message = (<span class="string">&quot;All values in memory_sequence_length must greater than zero.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> ops.control_dependencies(</span><br><span class="line"></span><br><span class="line">      [check_ops.assert_positive(memory_sequence_length, message=message)]):</span><br><span class="line"></span><br><span class="line">    score_mask = array_ops.sequence_mask(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length, maxlen=array_ops.shape(score)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    score_mask_values = score_mask_value * array_ops.ones_like(score)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> array_ops.where(score_mask, score, score_mask_values)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">score = tf.random_uniform(shape=[<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">tf.shape(score).numpy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>array([ 2, 10], dtype=int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">score = tf.random_uniform(shape=[<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">memeory_sequence_len = [<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">score_mask_value = -<span class="number">100000000</span></span><br><span class="line"></span><br><span class="line">score_mask = tf.sequence_mask(lengths=memeory_sequence_len, maxlen=tf.shape(score)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;true or false: %s\n&quot;</span> %score_mask)</span><br><span class="line"></span><br><span class="line">score_mask_values = score_mask_value * tf.ones_like(score)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-inf: %s\n&quot;</span>%score_mask_values)</span><br><span class="line"></span><br><span class="line">ans = tf.where(score_mask, score, score_mask_values)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ans)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>true or false: tf.Tensor(

[[ True  True  True  True  True False False False False False]

 [ True  True  True  True  True  True  True  True False False]], shape=(2, 10), dtype=bool)



-inf: tf.Tensor(

[[-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08

  -1.e+08]

 [-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08

  -1.e+08]], shape=(2, 10), dtype=float32)



tf.Tensor(

[[ 2.3987615e-01  4.9896538e-01  7.2822869e-01  4.7516704e-02

   1.6099060e-01 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08

  -1.0000000e+08 -1.0000000e+08]

 [ 3.5503960e-01  2.5502288e-01  8.1264114e-01  4.3110681e-01

   1.1858845e-01  2.5748730e-02  4.8437893e-01  2.8339624e-02

  -1.0000000e+08 -1.0000000e+08]], shape=(2, 10), dtype=float32)
</code></pre>
<h4 id="prepare-memory"><a href="#prepare-memory" class="headerlink" title="_prepare_memory\"></a>_prepare_memory\</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">self._keys = _prepare_memory(memory, memory_sequence_length,</span><br><span class="line"></span><br><span class="line">check_inner_dims_defined=check_inner_dims_defined)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>其中 _prepare_memory 这个函数,也就是怎么计算 mask 的，其计算如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prepare_memory</span>(<span class="params">memory, memory_sequence_length, check_inner_dims_defined</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Convert to tensor and possibly mask `memory`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    memory: `Tensor`, shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    check_inner_dims_defined: Python boolean.  If `True`, the `memory`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      argument&#x27;s shape is checked to ensure all but the two outermost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dimensions are fully defined.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    A (possibly masked), checked, new `memory`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ValueError: If `check_inner_dims_defined` is `True` and not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `memory.shape[2:].is_fully_defined()`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  memory = nest.map_structure(</span><br><span class="line"></span><br><span class="line">      <span class="keyword">lambda</span> m: ops.convert_to_tensor(m, name=<span class="string">&quot;memory&quot;</span>), memory)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    memory_sequence_length = ops.convert_to_tensor(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length, name=<span class="string">&quot;memory_sequence_length&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> check_inner_dims_defined:</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_dims</span>(<span class="params">m</span>):</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> m.get_shape()[<span class="number">2</span>:].is_fully_defined():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected memory %s to have fully defined inner dims, &quot;</span></span><br><span class="line"></span><br><span class="line">                         <span class="string">&quot;but saw shape: %s&quot;</span> % (m.name, m.get_shape()))</span><br><span class="line"></span><br><span class="line">    nest.map_structure(_check_dims, memory)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    seq_len_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">    seq_len_mask = array_ops.sequence_mask(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        maxlen=array_ops.shape(nest.flatten(memory)[<span class="number">0</span>])[<span class="number">1</span>],</span><br><span class="line"></span><br><span class="line">        dtype=nest.flatten(memory)[<span class="number">0</span>].dtype)</span><br><span class="line"></span><br><span class="line">    seq_len_batch_size = (</span><br><span class="line"></span><br><span class="line">        memory_sequence_length.shape[<span class="number">0</span>].value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">or</span> array_ops.shape(memory_sequence_length)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_maybe_mask</span>(<span class="params">m, seq_len_mask</span>):</span></span><br><span class="line"></span><br><span class="line">    rank = m.get_shape().ndims</span><br><span class="line"></span><br><span class="line">    rank = rank <span class="keyword">if</span> rank <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> array_ops.rank(m)</span><br><span class="line"></span><br><span class="line">    extra_ones = array_ops.ones(rank - <span class="number">2</span>, dtype=dtypes.int32)</span><br><span class="line"></span><br><span class="line">    m_batch_size = m.shape[<span class="number">0</span>].value <span class="keyword">or</span> array_ops.shape(m)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      message = (<span class="string">&quot;memory_sequence_length and memory tensor batch sizes do not &quot;</span></span><br><span class="line"></span><br><span class="line">                 <span class="string">&quot;match.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> ops.control_dependencies([</span><br><span class="line"></span><br><span class="line">          check_ops.assert_equal(</span><br><span class="line"></span><br><span class="line">              seq_len_batch_size, m_batch_size, message=message)]):</span><br><span class="line"></span><br><span class="line">        seq_len_mask = array_ops.reshape(</span><br><span class="line"></span><br><span class="line">            seq_len_mask,</span><br><span class="line"></span><br><span class="line">            array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> m * seq_len_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> nest.map_structure(<span class="keyword">lambda</span> m: _maybe_mask(m, seq_len_mask), memory)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>_prepare_memory 其实很简单，就是根据 batch 中每个样本的实际长度，将超出部分设置为 0</p>
<h3 id="tf-contrib-seq2seq-BahdanauAttention"><a href="#tf-contrib-seq2seq-BahdanauAttention" class="headerlink" title="tf.contrib.seq2seq.BahdanauAttention"></a>tf.contrib.seq2seq.BahdanauAttention</h3><p>这里涉及到了两篇 paper:  </p>
<ul>
<li><p>[Neural Machine Translation by Jointly Learning to Align and Translate.”</p>
<p>ICLR 2015. ](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>)  </p>
</li>
<li><p>[Weight Normalization: A Simple Reparameterization to Accelerate</p>
<p> Training of Deep Neural Networks.”](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a>)</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span>(<span class="params">_BaseAttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Bahdanau-style (additive) attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This attention has two forms.  The first is Bahdanau attention,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  as described in:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  ICLR 2015. https://arxiv.org/abs/1409.0473</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The second is the normalized form.  This form is inspired by the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  weight normalization article:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Tim Salimans, Diederik P. Kingma.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Weight Normalization: A Simple Reparameterization to Accelerate</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Training of Deep Neural Networks.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1602.07868</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  To enable the second form, construct the object with parameter</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  `normalize=True`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               normalize=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="string">&quot;BahdanauAttention&quot;</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the Attention mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: The depth of the query mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory: The memory to query; usually the output of an RNN encoder.  This</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        tensor should be shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_sequence_length (optional): Sequence lengths for the batch entries</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in memory.  If provided, the memory tensor rows are masked with zeros</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for values past the respective sequence lengths.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      normalize: Python boolean.  Whether to normalize the energy term.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      probability_fn: (optional) A `callable`.  Converts the score to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probabilities.  The default is @&#123;tf.nn.softmax&#125;. Other options include</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @&#123;tf.contrib.seq2seq.hardmax&#125; and @&#123;tf.contrib.sparsemax.sparsemax&#125;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Its signature should be: `probabilities = probability_fn(score)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      score_mask_value: (optional): The mask value for score before passing into</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probability_fn`. The default is -inf. Only used if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `memory_sequence_length` is not None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The data type for the query and memory layers of the attention</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: Name to use when creating ops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>num_units 是query mechanism 的维度. 它可以既不是 query 的维度,也可以不是 memory 的维度对吧?</p>
</li>
<li><p>query 的维度要和 memory(也就是 keys/values) 的维度一致吗?是不需要的.在 BahdanauAttention 的实现中比较好理解,两个全链接最后的维度一致即可相加.但是在 LuongAttention 中矩阵矩阵相乘时需要注意维度变化.  </p>
</li>
<li><p>memory_sequence_length: 这个参数很重要, mask 消除 padding 的影响.  </p>
</li>
<li><p>score_mask_value: 上一个参数存在时,这个参数才会使用,默认为 -inf.</p>
</li>
</ul>
<p>继续看源码的实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> probability_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">  probability_fn = nn_ops.softmax</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">  dtype = dtypes.float32</span><br><span class="line"></span><br><span class="line">wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line"><span class="built_in">super</span>(BahdanauAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">    query_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">        num_units, name=<span class="string">&quot;query_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">    memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">        num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">    memory=memory,</span><br><span class="line"></span><br><span class="line">    probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">    memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">    score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">    name=name)</span><br><span class="line"></span><br><span class="line">self._num_units = num_units</span><br><span class="line"></span><br><span class="line">self._normalize = normalize</span><br><span class="line"></span><br><span class="line">self._name = name</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>现在理解了 _BaseAttentionMechanism 这个类中 query_layer 和 memory_layer 的意义了.  </p>
</li>
<li><p>score_mask_value 沿用父类中的计算方式.  </p>
</li>
</ul>
<p>继续看 call 函数,也就是 attention 的计算方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, query, state</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Score the query based on the keys and values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, query_depth]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    state: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is memory&#x27;s `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    alignments: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]` (`alignments_size` is memory&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> variable_scope.variable_scope(<span class="literal">None</span>, <span class="string">&quot;bahdanau_attention&quot;</span>, [query]):</span><br><span class="line"></span><br><span class="line">    processed_query = self.query_layer(query) <span class="keyword">if</span> self.query_layer <span class="keyword">else</span> query</span><br><span class="line"></span><br><span class="line">    score = _bahdanau_score(processed_query, self._keys, self._normalize)</span><br><span class="line"></span><br><span class="line">  alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line">  next_state = alignments</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> alignments, next_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>然后看怎么计算的 score.  </p>
<p>score = _bahdanau_score(processed_query, self._keys, self._normalize) 其中</p>
<p>processed_query 和 self._keys 都是通过全链接层后得到的, [batch, alignments_size, num_units]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bahdanau_score</span>(<span class="params">processed_query, keys, normalize</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Bahdanau-style (additive) scoring function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  dtype = processed_query.dtype</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Get the number of hidden units from the trailing dimension of keys</span></span><br><span class="line"></span><br><span class="line">  num_units = keys.shape[<span class="number">2</span>].value <span class="keyword">or</span> array_ops.shape(keys)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.</span></span><br><span class="line"></span><br><span class="line">  processed_query = array_ops.expand_dims(processed_query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  v = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">      <span class="string">&quot;attention_v&quot;</span>, [num_units], dtype=dtype)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> normalize:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar used in weight normalization</span></span><br><span class="line"></span><br><span class="line">    g = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_g&quot;</span>, dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.constant_initializer(math.sqrt((<span class="number">1.</span> / num_units))),</span><br><span class="line"></span><br><span class="line">        shape=())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Bias added prior to the nonlinearity</span></span><br><span class="line"></span><br><span class="line">    b = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_b&quot;</span>, [num_units], dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># normed_v = g * v / ||v||</span></span><br><span class="line"></span><br><span class="line">    normed_v = g * v * math_ops.rsqrt(</span><br><span class="line"></span><br><span class="line">        math_ops.reduce_sum(math_ops.square(v)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> math_ops.reduce_sum(</span><br><span class="line"></span><br><span class="line">        normed_v * math_ops.tanh(keys + processed_query + b), [<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [<span class="number">2</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>源码中计算 score 的最后一步不是全链接，而是这样的：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">v = tf.get_variable(<span class="string">&quot;attention_v&quot;</span>, [num_units])</span><br><span class="line"></span><br><span class="line">score = tf.reduce_sum(v * tanh(keys + processed_query), [<span class="number">2</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tfe.executing_eagerly())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">memory = tf.ones(shape=[<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>]) <span class="comment"># batch=1, max_sequence_len=10, embed_size=5</span></span><br><span class="line"></span><br><span class="line">memory_sequence_len = [<span class="number">5</span>]                   <span class="comment"># 有效长度为 5</span></span><br><span class="line"></span><br><span class="line">attention_mechnism = tf.contrib.seq2seq.BahdanauAttention(num_units=<span class="number">32</span>, memory=memory,</span><br><span class="line"></span><br><span class="line">                                                          memory_sequence_length=memory_sequence_len)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.state_size, attention_mechnism.alignments_size)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>10 10
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">memory</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&lt;tf.Tensor: id=3, shape=(1, 10, 5), dtype=float32, numpy=

array([[[1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.]]], dtype=float32)&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">attention_mechnism.values   <span class="comment"># 可以发现 values 就是把 memory 中超过memory_sequence_length 的部分变为 0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&lt;tf.Tensor: id=30, shape=(1, 10, 5), dtype=float32, numpy=

array([[[1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.]]], dtype=float32)&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.keys.shape)  <span class="comment"># 经过了全链接之后的</span></span><br><span class="line"></span><br><span class="line">attention_mechnism.keys.numpy()[<span class="number">0</span>,<span class="number">1</span>,:]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(1, 10, 32)



array([ 0.09100786,  0.18448338, -0.7751561 ,  0.00775184,  0.467805  ,

        0.9172474 ,  0.57645243, -0.3915946 , -0.22213435,  0.76866853,

        0.3591721 ,  0.8922573 ,  0.15866229,  0.6033571 ,  0.51816225,

        0.3820553 , -0.39130217,  0.04532939, -0.02089322,  0.6878175 ,

       -0.28697258,  0.59283376, -0.37825382, -0.5865691 ,  0.17466056,

       -0.5915747 ,  0.6070496 , -0.18531135, -0.821724  ,  1.2838829 ,

        0.15700272, -0.2608306 ], dtype=float32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.query_layer, attention_mechnism.memory_layer)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&lt;tensorflow.python.layers.core.Dense object at 0x7fa0464da908&gt; &lt;tensorflow.python.layers.core.Dense object at 0x7fa0464dab38&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 利用 call 函数来计算下一个 state 和 attention vector</span></span><br><span class="line"></span><br><span class="line">query = tf.ones(shape=[<span class="number">1</span>, <span class="number">8</span>])  <span class="comment"># query_depth = 10</span></span><br><span class="line"></span><br><span class="line">state_h0 = attention_mechnism.initial_alignments(batch_size=<span class="number">1</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">attention_vector = attention_mechnism(query=query, state=state_h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_vector)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(&lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;, &lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;)
</code></pre>
<h3 id="tf-contrib-seq2seq-LuongAttention"><a href="#tf-contrib-seq2seq-LuongAttention" class="headerlink" title="tf.contrib.seq2seq.LuongAttention"></a>tf.contrib.seq2seq.LuongAttention</h3><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015.</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LuongAttention</span>(<span class="params">_BaseAttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Luong-style (multiplicative) attention scoring.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               scale=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="string">&quot;LuongAttention&quot;</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> probability_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      probability_fn = nn_ops.softmax</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      dtype = dtypes.float32</span><br><span class="line"></span><br><span class="line">    wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LuongAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._scale = scale</span><br><span class="line"></span><br><span class="line">    self._name = name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以发现 query 没有经过 query_layer 的处理，也就是没有全链接。但是 memory 还是要用全链接处理的，得到 <code>[batch, max_times, num_units]</code></p>
<p>再看使用 call 函数计算对其概率 alignment 和 next_state.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, query, state</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Score the query based on the keys and values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, query_depth]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    state: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is memory&#x27;s `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    alignments: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]` (`alignments_size` is memory&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> variable_scope.variable_scope(<span class="literal">None</span>, <span class="string">&quot;luong_attention&quot;</span>, [query]):</span><br><span class="line"></span><br><span class="line">    score = _luong_score(query, self._keys, self._scale)</span><br><span class="line"></span><br><span class="line">  alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line">  next_state = alignments</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> alignments, next_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>接下来看怎么计算的 score</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_luong_score</span>(<span class="params">query, keys, scale</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Luong-style (multiplicative) scoring function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor, shape `[batch_size, num_units]` to compare to keys.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    keys: Processed memory, shape `[batch_size, max_time, num_units]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    scale: Whether to apply a scale to the score function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    A `[batch_size, max_time]` tensor of unnormalized score values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ValueError: If `key` and `query` depths do not match.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  depth = query.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  key_units = keys.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> depth != key_units:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Incompatible or unknown inner dimensions between query and keys.  &quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Query (%s) has units: %s.  Keys (%s) have units: %s.  &quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Perhaps you need to set num_units to the keys&#x27; dimension (%s)?&quot;</span></span><br><span class="line"></span><br><span class="line">        % (query, depth, keys, key_units, key_units))</span><br><span class="line"></span><br><span class="line">  dtype = query.dtype</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Reshape from [batch_size, depth] to [batch_size, 1, depth]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># for matmul.</span></span><br><span class="line"></span><br><span class="line">  query = array_ops.expand_dims(query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Inner product along the query units dimension.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># matmul shapes: query is [batch_size, 1, depth] and</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#                keys is [batch_size, max_time, depth].</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># the inner product is asked to **transpose keys&#x27; inner shape** to get a</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># batched matmul on:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#   [batch_size, 1, depth] . [batch_size, depth, max_time]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># resulting in an output shape of:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#   [batch_size, 1, max_time].</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># we then squeeze out the center singleton dimension.</span></span><br><span class="line"></span><br><span class="line">  score = math_ops.matmul(query, keys, transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  score = array_ops.squeeze(score, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar used in weight scaling</span></span><br><span class="line"></span><br><span class="line">    g = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_g&quot;</span>, dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.ones_initializer, shape=())</span><br><span class="line"></span><br><span class="line">    score = g * score</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>通过源码可以发现 <code>LuongAttention</code> 调用 call 函数时，其 query 的维度必须是 num_units. 而 BahdanauAttention 并不需要。</p>
<p>其是计算 score 的方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">query_depth = num_units = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">memory_depth = <span class="number">15</span></span><br><span class="line"></span><br><span class="line">max_times = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">embed_size = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">scale = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.random_normal(shape=[batch_size, num_units])</span><br><span class="line"></span><br><span class="line"><span class="comment"># memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># values = self._prepaer_memory(memory)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># keys = memory_layer(values)</span></span><br><span class="line"></span><br><span class="line">values = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span><br><span class="line"></span><br><span class="line">keys = tf.layers.dense(inputs=values, units=num_units)    <span class="comment"># [batch, max_times, num_units]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.expand_dims(query, axis=<span class="number">1</span>)                     <span class="comment"># [batch, 1, num_units]</span></span><br><span class="line"></span><br><span class="line">score = tf.matmul(query, keys, transpose_b=<span class="literal">True</span>)          <span class="comment"># [batch, 1, max_times]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">score = tf.squeeze(score, axis=<span class="number">1</span>)   <span class="comment"># [batch, max_times]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(score.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(2, 10)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 完整的过一遍</span></span><br><span class="line"></span><br><span class="line">memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span><br><span class="line"></span><br><span class="line">memory_sequence_len = [<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">query_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.random_normal(shape=[batch_size, num_units])</span><br><span class="line"></span><br><span class="line">state = tf.zeros(shape=[batch_size, max_times])</span><br><span class="line"></span><br><span class="line">attention_mechnism = tf.contrib.seq2seq.LuongAttention(num_units=num_units,</span><br><span class="line"></span><br><span class="line">                                                       memory=memory,</span><br><span class="line"></span><br><span class="line">                                                       memory_sequence_length=memory_sequence_len)</span><br><span class="line"></span><br><span class="line">attention_vector = attention_mechnism(query, state)</span><br><span class="line"></span><br><span class="line">attention_vector[<span class="number">0</span>], attention_vector[<span class="number">1</span>]   <span class="comment"># attention_vector 和 state</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(&lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy=

 array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02,

         6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,

         0.0000000e+00, 0.0000000e+00],

        [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06,

         3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04,

         0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;,

 &lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy=

 array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02,

         6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,

         0.0000000e+00, 0.0000000e+00],

        [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06,

         3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04,

         0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.reduce_sum(attention_vector[<span class="number">0</span>][<span class="number">1</span>]).numpy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>1.0
</code></pre>
<p>这只是针对单个 query 的情况，但实际上 query 一般是这样的 [batch, query_len, num_units]，那怎么办呢？</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="最后总结一下"><a href="#最后总结一下" class="headerlink" title="最后总结一下"></a>最后总结一下</h3><p>再看一遍两个 attention 初始化的差异</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">super</span>(BahdanauAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;query_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">super</span>(LuongAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>作为一个类对象时，<code>AttentionMechanism</code>，<code>BahdanauAttention</code>，<code>LuongAttention</code>它们具有如下属性：</p>
<ul>
<li><p>query_layer: 在 BahdanauAttention 中一般是 tf.layer.dense 的实例对象，其维度是 num_units. 所以 BahdanauAttention 中 query 的维度可以是任意值。而 LuongAttention 中 query_layer 为 None，所以 query 的维度只能是 num_units.  </p>
</li>
<li><p>memory_layer: 在两个 attention 中都是一样的，tf.layer.dense,且维度为 num_units.  </p>
</li>
<li><p>alignments_size: 对齐size，是 memory 的 max_times.  </p>
</li>
<li><p>batch_size: 批量大小  </p>
</li>
<li><p>values: 是经过 mask 处理后的 memory. [batch, max_times, embed_size]  </p>
</li>
<li><p>keys: 是经过 memory_layer 全链接处理后的。 [batch, max_times, num_units].</p>
</li>
<li><p>state_size: 等于 alignment_size.</p>
</li>
</ul>
<p>然后是对应的方法：  </p>
<p><strong><strong>init</strong>:</strong> 初始化类实例，里面的参数：  </p>
<ul>
<li><p>num_units: 在　Bahdanau 中这个参数其实是个中间值，将 query 和 keys 转化为这个维度，叠加，但最后还是要在这个维度上　reduce_sum． 但是在　LuongAttention 中它必须和 query 的维度一致，然后和 memory_layer 处理后的 memory 做矩阵相乘。  </p>
</li>
<li><p>memory: [batch, max_times, embed_size]  </p>
</li>
<li><p>normalize: 是佛有归一化  </p>
</li>
<li><p>probability_fn: <code>tf.nn.softmax</code>，<code>tf.contrib.seq2seq.hardmax</code>，<code> tf.contrib.sparsemax.sparsemax</code>  </p>
</li>
<li><p>memory_sequence_length： 没有经过 padding 时 memory 的长度。其维度应该是 [1, batch_size]  </p>
</li>
</ul>
<p><strong>call(query, state)</strong> 调用该实例  </p>
<ul>
<li><p>query: [batch_size, query_length]. 在 LuongAttention 中 query_length 必须等于 num_units.  </p>
</li>
<li><p>state: [batch_size, alignments_size].  </p>
</li>
</ul>
<p>一直不太理解 state 有啥用？在源码中是用来计算 alignments 的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self._probability_fn = <span class="keyword">lambda</span> score, prev: (  <span class="comment"># pylint:disable=g-long-lambda</span></span><br><span class="line"></span><br><span class="line">        probability_fn(</span><br><span class="line"></span><br><span class="line">            _maybe_mask_score(score, memory_sequence_length, score_mask_value),</span><br><span class="line"></span><br><span class="line">            prev))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中 score 是可能需要 mask 的. probability_fn 是 tf.nn.softmax. 所以呢？？？？不需要 prev 啊？</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后发现确实不需要啊。。。一步步往上找</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">probability_fn=wrapped_probability_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><strong>initial_alignments(batch_size, dtype)</strong> 初始化对齐  </p>
<p>Args:  </p>
<ul>
<li><p>batch_size: int32 scalar, the batch_size.  </p>
</li>
<li><p>dtype: The dtype.  </p>
</li>
</ul>
<p>Returns:  </p>
<ul>
<li>A dtype tensor shaped [batch_size, alignments_size]  </li>
</ul>
<p><strong>initial_state(batch_size, dtype)：</strong>  </p>
<p>Creates the initial state values for the AttentionWrapper class.</p>
<ul>
<li>batch_size: int32.   </li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-01T09:49:53.000Z" title="2018/9/1 下午5:49:53">2018-09-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.141Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/TensorFlow/">TensorFlow</a></span><span class="level-item">44 分钟读完 (大约6574个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/01/tensorflow-rnn-api-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/">Tensorflow RNN API 源码阅读</a></h1><div class="content"><p>在三星研究院实习一段时间发现在公司写代码和在学校还是有差别的。一是在公司要追求效率，会使用很多官方封装好的api，而在学校的时候因为要去理解内部原理，更多的是在造轮子，导致对很多 api 不是很熟悉。但实际上官方api不仅在速度，以及全面性上都比自己写的还是好很多的。二是，在公司对代码的复用率要求比较高，模型跑到哪一个版本了，对应的参数都要留下来，随时可以跑起来，而不是重新训练，这对模型、参数的保存要求很重要。以及在测试集上的性能指标都要在代码上很完整，而不是仅仅看看 loss 和 accuracy 就可以的。</p>
<p>这节内容主要是详细过一遍 tensorflow 里面的 rnn api，根据<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods">RNN and Cells (contrib)</a>这里的顺序逐步深入研究</p>
<h2 id="先回顾一下-RNN-LSTM-GRU"><a href="#先回顾一下-RNN-LSTM-GRU" class="headerlink" title="先回顾一下 RNN/LSTM/GRU:"></a>先回顾一下 RNN/LSTM/GRU:</h2><p>参考之前 cs224d 的笔记  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/05/07/cs224d-lecture9-machine-translation/">cs224d-lecture9 机器翻译</a>   </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/">cs224d-lecture8-RNN</a> 发现有些小错误，但不影响自己复习。。</p>
</li>
</ul>
<p>basic rnn：  </p>
<p><img src="https://panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn16.png"></p>
<p>$$h_t = \sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$</p>
<h2 id="先看-tf-contrib-rnn-RNNCell"><a href="#先看-tf-contrib-rnn-RNNCell" class="headerlink" title="先看 tf.contrib.rnn.RNNCell"></a>先看 tf.contrib.rnn.RNNCell</h2><p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/ops/rnn_cell_impl.py#L170">https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/ops/rnn_cell_impl.py#L170</a>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.RNNCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNCell</span>(<span class="params">base_layer.Layer</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Abstract object representing an RNN cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Every `RNNCell` must have the properties below and implement `call` with</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  the signature `(output, next_state) = call(input, state)`.  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  RNNCell 是一个抽象的父类，之后更复杂的 RNN/LSTM/GRU 都是重新实现 call 函数，也就是更新隐藏状态</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  的方式改变了。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The optional third input argument, `scope`, is allowed for backwards compatibility</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  purposes; but should be left off for new subclasses.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  scope 这个参数管理变量，在反向传播中变量是否可训练。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This definition of cell differs from the definition used in the literature.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  In the literature, &#x27;cell&#x27; refers to an object with a single scalar output.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This definition refers to a horizontal array of such units.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  这里的 cell 的概念和一些论文中是不一样的。在论文中，cell 表示一个神经元，也就是单个值。而这里表示的是</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  一组神经元，比如隐藏状态[batch, num_units].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  An RNN cell, in the most abstract setting, is anything that has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  a state and performs some operation that takes a matrix of inputs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This operation results in an output matrix with `self.output_size` columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  If `self.state_size` is an integer, this operation also results in a new</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  state matrix with `self.state_size` columns.  If `self.state_size` is a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  (possibly nested tuple of) TensorShape object(s), then it should return a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  matching structure of Tensors having shape `[batch_size].concatenate(s)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  for each `s` in `self.batch_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  rnn cell 的输入是一个状态 state 和 input 矩阵，参数有 self.output_size 和 self.state_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  分别表示输出层和隐藏层的维度。其中 state_size 可能是 tuple，这个之后在看。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, inputs, state, scope=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run this RNN cell on inputs, starting from the given state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      inputs: `2-D` tensor with shape `[batch_size, input_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state: if `self.state_size` is an integer, this should be a `2-D Tensor`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        with shape `[batch_size, self.state_size]`.  Otherwise, if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `self.state_size` is a tuple of integers, this should be a tuple</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        with shapes `[batch_size, s] for s in self.state_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      scope: VariableScope for the created subgraph; defaults to class name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A pair containing:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      - New state: Either a single `2-D` tensor, or a tuple of tensors matching</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the arity and shapes of `state`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> vs.variable_scope(scope,</span><br><span class="line"></span><br><span class="line">                             custom_getter=self._rnn_get_variable) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(RNNCell, self).__call__(inputs, state, scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      scope_attrname = <span class="string">&quot;rnncell_scope&quot;</span></span><br><span class="line"></span><br><span class="line">      scope = <span class="built_in">getattr</span>(self, scope_attrname, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">        scope = vs.variable_scope(vs.get_variable_scope(),</span><br><span class="line"></span><br><span class="line">                                  custom_getter=self._rnn_get_variable)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">setattr</span>(self, scope_attrname, scope)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> scope:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(RNNCell, self).__call__(inputs, state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_rnn_get_variable</span>(<span class="params">self, getter, *args, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">    variable = getter(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> context.executing_eagerly():</span><br><span class="line"></span><br><span class="line">      trainable = variable._trainable  <span class="comment"># pylint: disable=protected-access</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      trainable = (</span><br><span class="line"></span><br><span class="line">          variable <span class="keyword">in</span> tf_variables.trainable_variables() <span class="keyword">or</span></span><br><span class="line"></span><br><span class="line">          (<span class="built_in">isinstance</span>(variable, tf_variables.PartitionedVariable) <span class="keyword">and</span></span><br><span class="line"></span><br><span class="line">           <span class="built_in">list</span>(variable)[<span class="number">0</span>] <span class="keyword">in</span> tf_variables.trainable_variables()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> trainable <span class="keyword">and</span> variable <span class="keyword">not</span> <span class="keyword">in</span> self._trainable_weights:</span><br><span class="line"></span><br><span class="line">      self._trainable_weights.append(variable)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> <span class="keyword">not</span> trainable <span class="keyword">and</span> variable <span class="keyword">not</span> <span class="keyword">in</span> self._non_trainable_weights:</span><br><span class="line"></span><br><span class="line">      self._non_trainable_weights.append(variable)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;size(s) of state(s) used by this cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It can be represented by an Integer, a TensorShape or a tuple of Integers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    or TensorShapes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Abstract method&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Integer or TensorShape: size of outputs produced by this cell.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Abstract method&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, _</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># This tells the parent Layer object that it&#x27;s OK to call</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># self.add_variable() inside the call() method.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">zero_state</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return zero-filled state tensor(s).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: int, float, or unit Tensor representing the batch size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: the data type to use for the state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      If `state_size` is an int or TensorShape, then the return value is a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `N-D` tensor of shape `[batch_size, state_size]` filled with zeros.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      If `state_size` is a nested list or tuple, then the return value is</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      a nested list or tuple (of the same structure) of `2-D` tensors with</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      the shapes `[batch_size, s]` for each s in `state_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Try to use the last cached zero_state. This is done to avoid recreating</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># zeros, especially when eager execution is enabled.</span></span><br><span class="line"></span><br><span class="line">    state_size = self.state_size</span><br><span class="line"></span><br><span class="line">    is_eager = context.executing_eagerly()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_eager <span class="keyword">and</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;_last_zero_state&quot;</span>):</span><br><span class="line"></span><br><span class="line">      (last_state_size, last_batch_size, last_dtype,</span><br><span class="line"></span><br><span class="line">       last_output) = <span class="built_in">getattr</span>(self, <span class="string">&quot;_last_zero_state&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (last_batch_size == batch_size <span class="keyword">and</span></span><br><span class="line"></span><br><span class="line">          last_dtype == dtype <span class="keyword">and</span></span><br><span class="line"></span><br><span class="line">          last_state_size == state_size):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> last_output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ops.name_scope(<span class="built_in">type</span>(self).__name__ + <span class="string">&quot;ZeroState&quot;</span>, values=[batch_size]):</span><br><span class="line"></span><br><span class="line">      output = _zero_state_tensors(state_size, batch_size, dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_eager:</span><br><span class="line"></span><br><span class="line">      self._last_zero_state = (state_size, batch_size, dtype, output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p>两个属性 output_size, state_size 分别表示输出层的维度和隐藏层的维度。call 函数用来表示计算下一个时间步的隐藏状态和输出，zero_state 函数用来初始化初始状态全为 0, 这里 state_size 有两种情况，一种是 int 或 tensorshape,那么 [batch, state_size]. 如果是多层嵌套 rnn, 那么初始状态 <code>[batch, s] for s in state_size</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerRNNCell</span>(<span class="params">RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Subclass of RNNCells that act like proper `tf.Layer` objects.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __call__(self, inputs, state, scope=None, *args, **kwargs):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>Run this RNN cell on inputs, starting <span class="keyword">from</span> the given state.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line"></span><br><span class="line">      inputs: `<span class="number">2</span>-D` tensor <span class="keyword">with</span> shape `[batch_size, input_size]`.</span><br><span class="line"></span><br><span class="line">      state: <span class="keyword">if</span> `self.state_size` <span class="keyword">is</span> an integer, this should be a `<span class="number">2</span>-D Tensor`</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> shape `[batch_size, self.state_size]`.  Otherwise, <span class="keyword">if</span></span><br><span class="line"></span><br><span class="line">        `self.state_size` <span class="keyword">is</span> a <span class="built_in">tuple</span> of integers, this should be a <span class="built_in">tuple</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> shapes `[batch_size, s] <span class="keyword">for</span> s <span class="keyword">in</span> self.state_size`.</span><br><span class="line"></span><br><span class="line">      scope: optional cell scope.</span><br><span class="line"></span><br><span class="line">      *args: Additional positional arguments.</span><br><span class="line"></span><br><span class="line">      **kwargs: Additional keyword arguments.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line"></span><br><span class="line">      A pair containing:</span><br><span class="line"></span><br><span class="line">      - Output: A `<span class="number">2</span>-D` tensor <span class="keyword">with</span> shape `[batch_size, self.output_size]`.</span><br><span class="line"></span><br><span class="line">      - New state: Either a single `<span class="number">2</span>-D` tensor, <span class="keyword">or</span> a <span class="built_in">tuple</span> of tensors matching</span><br><span class="line"></span><br><span class="line">        the arity <span class="keyword">and</span> shapes of `state`.</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Bypass RNNCell&#x27;s variable capturing semantics for LayerRNNCell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Instead, it is up to subclasses to provide a proper build</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # method.  See the class docstring for more details.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return base_layer.Layer.__call__(self, inputs, state, scope=scope,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                                     *args, **kwargs)</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>



<h2 id="再看-Core-RNN-Cells"><a href="#再看-Core-RNN-Cells" class="headerlink" title="再看 Core RNN Cells"></a>再看 Core RNN Cells</h2><ul>
<li><p>tf.contrib.rnn.BasicRNNCell</p>
</li>
<li><p>tf.contrib.rnn.BasicLSTMCell</p>
</li>
<li><p>tf.contrib.rnn.GRUCell</p>
</li>
<li><p>tf.contrib.rnn.LSTMCell</p>
</li>
<li><p>tf.contrib.rnn.LayerNormBasicLSTMCell</p>
</li>
</ul>
<h3 id="tf-contrib-rnn-BasicRNNCell"><a href="#tf-contrib-rnn-BasicRNNCell" class="headerlink" title="tf.contrib.rnn.BasicRNNCell"></a>tf.contrib.rnn.BasicRNNCell</h3><p>直接扒源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.BasicRNNCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicRNNCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;The most basic RNN cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the RNN cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    name: String, the name of the layer. Layers with the same name will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      share weights, but to avoid mistakes we require reuse=True in such</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cases.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dtype: Default dtype of the layer (default of `None` means use the type</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      of the first input). Required when `build` is called before `call`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(BasicRNNCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Inputs must be 2-dimensional.</span></span><br><span class="line"></span><br><span class="line">    self.input_spec = base_layer.InputSpec(ndim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, inputs_shape</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> inputs_shape[<span class="number">1</span>].value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                       % inputs_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    input_depth = inputs_shape[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    self._kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + self._num_units, self._num_units])</span><br><span class="line"></span><br><span class="line">    self._bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.zeros_initializer(dtype=self.dtype))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Most basic RNN: output = new_state = act(W * input + U * state + B).&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    gate_inputs = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, state], <span class="number">1</span>), self._kernel)</span><br><span class="line"></span><br><span class="line">    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line"></span><br><span class="line">    output = self._activation(gate_inputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以发现 state_size = output_size = num_units, 以及输出就是下一个隐藏状态 output = new_state = act(W * input + U * state + B) = act(W[input, state] + b) 其中 self._kernel 表示 W, 其维度是 [input_depth + num_units, num_units]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tfe.executing_eagerly())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell = tf.contrib.rnn.BasicRNNCell(num_units=<span class="number">128</span>, activation=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cell.state_size, cell.output_size)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>128 128
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">32</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">h0 = cell.zero_state(batch_size=<span class="number">32</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = cell(inputs=inputs, state=h0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output.shape, state.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(32, 128) (32, 128)
</code></pre>
<h3 id="tf-contrib-rnn-BasicLSTMCell"><a href="#tf-contrib-rnn-BasicLSTMCell" class="headerlink" title="tf.contrib.rnn.BasicLSTMCell"></a>tf.contrib.rnn.BasicLSTMCell</h3><p>先回顾下 LSTM:</p>
<p><img src="https://panxiaoxie.cn/2018/05/07/cs224d-lecture9-machine-translation/lstm.png"></p>
<p>自己试着手敲公式～ 看着图还是简单，不看图是否也可以呢？</p>
<p>三个gate：遗忘门，输入/更新门，输出门</p>
<p>$$f_t=\sigma(W^{f}x_t + U^{f}h_{t-1})$$</p>
<p>$$i_t=\sigma(W^{i}x_t + U^{i}h_{t-1})$$</p>
<p>$$o_t=\sigma(W^{o}x_t + U^{o}h_{t-1})$$</p>
<p>new memory cell:</p>
<p>$$\hat c=tanh(W^cx_t + U^ch_{t-1})$$</p>
<p>输入门作用于新的记忆细胞,遗忘门作用于上一个记忆细胞，并得到最终的记忆细胞:</p>
<p>$$c_t=f_t\circ c_{t-1} + i_t\circ\hat c$$</p>
<p>用新的memory cell 和输出门得到新的隐藏状态：</p>
<p>$$h_t = tanh(o_t\circ c_t)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicLSTMCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Basic LSTM recurrent network cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The implementation is based on: http://arxiv.org/abs/1409.2329.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  We add forget_bias (default: 1) to the biases of the forget gate in order to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  reduce the scale of forgetting in the beginning of the training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  It does not allow cell clipping, a projection layer, and does not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  use peep-hole connections: it is the basic baseline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  For advanced models, please use the full @&#123;tf.nn.rnn_cell.LSTMCell&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  that follows.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               forget_bias=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               state_is_tuple=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize the basic LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: int, The number of units in the LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      forget_bias: float, The bias added to forget gates (see above).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Must set to `0.0` manually when restoring from CudnnLSTM-trained</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        checkpoints.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state_is_tuple: If True, accepted and returned states are 2-tuples of</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the `c_state` and `m_state`.  If False, they are concatenated</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        along the column axis.  The latter behavior will soon be deprecated.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      activation: Activation function of the inner states.  Default: `tanh`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: String, the name of the layer. Layers with the same name will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        share weights, but to avoid mistakes we require reuse=True in such</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        cases.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: Default dtype of the layer (default of `None` means use the type</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        of the first input). Required when `build` is called before `call`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      When restoring from CudnnLSTM-trained checkpoints, must use</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `CudnnCompatibleLSTMCell` instead.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(BasicLSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> state_is_tuple:</span><br><span class="line"></span><br><span class="line">      logging.warn(<span class="string">&quot;%s: Using a concatenated state is slower and will soon be &quot;</span></span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;deprecated.  Use state_is_tuple=True.&quot;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Inputs must be 2-dimensional.</span></span><br><span class="line"></span><br><span class="line">    self.input_spec = base_layer.InputSpec(ndim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._forget_bias = forget_bias</span><br><span class="line"></span><br><span class="line">    self._state_is_tuple = state_is_tuple</span><br><span class="line"></span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (LSTMStateTuple(self._num_units, self._num_units)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self._state_is_tuple <span class="keyword">else</span> <span class="number">2</span> * self._num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, inputs_shape</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> inputs_shape[<span class="number">1</span>].value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                       % inputs_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    input_depth = inputs_shape[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    h_depth = self._num_units</span><br><span class="line"></span><br><span class="line">    self._kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + h_depth, <span class="number">4</span> * self._num_units])</span><br><span class="line"></span><br><span class="line">    self._bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[<span class="number">4</span> * self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.zeros_initializer(dtype=self.dtype))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Long short-term memory cell (LSTM).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      inputs: `2-D` tensor with shape `[batch_size, input_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state: An `LSTMStateTuple` of state tensors, each shaped</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `[batch_size, num_units]`, if `state_is_tuple` has been set to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `True`.  Otherwise, a `Tensor` shaped</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `[batch_size, 2 * num_units]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A pair containing the new hidden state, and the new state (either a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `LSTMStateTuple` or a concatenated state, depending on</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `state_is_tuple`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    sigmoid = math_ops.sigmoid</span><br><span class="line"></span><br><span class="line">    one = constant_op.constant(<span class="number">1</span>, dtype=dtypes.int32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parameters of gates are concatenated into one multiply for efficiency.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">      c, h = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      c, h = array_ops.split(value=state, num_or_size_splits=<span class="number">2</span>, axis=one)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    gate_inputs = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, h], <span class="number">1</span>), self._kernel)</span><br><span class="line"></span><br><span class="line">    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span></span><br><span class="line"></span><br><span class="line">    i, j, f, o = array_ops.split(</span><br><span class="line"></span><br><span class="line">        value=gate_inputs, num_or_size_splits=<span class="number">4</span>, axis=one)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note that using `add` and `multiply` instead of `+` and `*` gives a</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># performance improvement. So using those at the cost of readability.</span></span><br><span class="line"></span><br><span class="line">    add = math_ops.add</span><br><span class="line"></span><br><span class="line">    multiply = math_ops.multiply</span><br><span class="line"></span><br><span class="line">    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),</span><br><span class="line"></span><br><span class="line">                multiply(sigmoid(i), self._activation(j)))</span><br><span class="line"></span><br><span class="line">    new_h = multiply(self._activation(new_c), sigmoid(o))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">      new_state = LSTMStateTuple(new_c, new_h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      new_state = array_ops.concat([new_c, new_h], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_h, new_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>阅读源码可以发现具体实现与上面的公式还是有点差别的。  </p>
<ul>
<li><p>先 concat[input, h], 然后 gate_input = matmul(concat[input, h], self._kernel)+self._bias,多了偏置项,这里的矩阵维度 [input_depth + h_depth,4*num_units]. 然后 i,j,f,o = split(gate_input, 4, axis=1). 其中 j 表示 new memory cell. 然后计算 new_c，其中 i,f,o 对应的激活函数确定是 sigmoid,因为其范围只能在(0,1)之间。但是 j 的激活函数self._activation 可以选择，默认是 tanh.  </p>
</li>
<li><p>与公式的差别之二在于 self._forget_bias.遗忘门在激活函数 $\sigma$ 之前加了偏置，目的是避免在训练初期丢失太多信息。    </p>
</li>
<li><p>要注意 state 的形式，取决于参数 self._state_is_tuple. 其中 c,h=state，表示 $c_{t-1},h_{t-1}$</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=<span class="number">128</span>, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>WARNING:tensorflow:From &lt;ipython-input-9-3f4ca183c5d7&gt;:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.

Instructions for updating:

This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name=&#39;basic_lstm_cell&#39;).
</code></pre>
<p>提示要更新了，那就换成最新的吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=<span class="number">128</span>, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lstm_cell.output_size, lstm_cell.state_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(128, LSTMStateTuple(c=128, h=128))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = lstm_cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>我们发现 lstm 的状态 state 是一个tuple,分别对应 c_t 和 h_t.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMStateTuple</span>(<span class="params">_LSTMStateTuple</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    and `h` is the output.</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<p>这里的解释感觉是有点问题的，<code>c</code> is the hidden state and <code>h</code> is the output. 看源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),</span><br><span class="line"></span><br><span class="line">            multiply(sigmoid(i), self._activation(j)))</span><br><span class="line"></span><br><span class="line">new_h = multiply(self._activation(new_c), sigmoid(o))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">  new_state = LSTMStateTuple(new_c, new_h)</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">  new_state = array_ops.concat([new_c, new_h], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> new_h, new_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>发现 c 表示的就是 new memory cell, 而 h 表示的是最后的隐藏状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 计算下一步的 output 和 state</span></span><br><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">30</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = lstm_cell(inputs, h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape, state[<span class="number">0</span>].shape, state[<span class="number">1</span>].shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">state.c, state.h  <span class="comment"># c 和 h 的值是不一样的</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(&lt;tf.Tensor: id=108, shape=(30, 128), dtype=float32, numpy=

 array([[ 0.08166471,  0.14020835,  0.07970127, ..., -0.1540019 ,

          0.38848224, -0.0842322 ],

        [-0.03643086, -0.20558938,  0.1503458 , ...,  0.01846285,

          0.15610473,  0.04408235],

        [-0.0933667 ,  0.03454542, -0.09073547, ..., -0.12701994,

         -0.34669587,  0.09373946],

        ...,

        [-0.00752909,  0.22412673, -0.270195  , ...,  0.09341058,

         -0.20986181, -0.18622127],

        [ 0.18778914,  0.37687936, -0.24727295, ..., -0.06409463,

          0.00218048,  0.5940756 ],

        [ 0.04073388, -0.08431841,  0.35944715, ...,  0.14135318,

          0.08472287, -0.11058106]], dtype=float32)&gt;,

 &lt;tf.Tensor: id=111, shape=(30, 128), dtype=float32, numpy=

 array([[ 0.04490132,  0.07412361,  0.03662094, ..., -0.07611651,

          0.17290959, -0.0277745 ],

        [-0.02212535, -0.13554382,  0.08272093, ...,  0.00918258,

          0.0861209 ,  0.02614526],

        [-0.05723168,  0.01372226, -0.02919216, ..., -0.06374882,

         -0.1918035 ,  0.03912015],

        ...,

        [-0.00377504,  0.15181372, -0.14555399, ...,  0.06073361,

         -0.09804281, -0.07492835],

        [ 0.10244624,  0.17440473, -0.09896267, ..., -0.03794969,

          0.00123257,  0.21985768],

        [ 0.01832823, -0.03795732,  0.1654894 , ...,  0.05827027,

          0.02769112, -0.05957894]], dtype=float32)&gt;)
</code></pre>
<h3 id="tf-nn-rnn-cell-GRUCell"><a href="#tf-nn-rnn-cell-GRUCell" class="headerlink" title="tf.nn.rnn_cell.GRUCell"></a>tf.nn.rnn_cell.GRUCell</h3><p>先回顾下 GRU.</p>
<p><img src="https://panxiaoxie.cn/2018/05/07/cs224d-lecture9-machine-translation/mt15.png"></p>
<p>手敲 GRU 公式：</p>
<p>$$r_t=\sigma(W^rx_t + U^rh_{t-1})$$</p>
<p>$$z_t=\sigma(W^zx_t + U^zh_{t-1})$$</p>
<p>$$\tilde h_t = tanh(Wx_t + r_t\circ h_{t-1})$$</p>
<p>$$h_t=(1-z_t)\circ\tilde h_t + z_t\circ h_{t-1}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.GRUCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GRUCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the GRU cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    kernel_initializer: (optional) The initializer to use for the weight and</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    projection matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    bias_initializer: (optional) The initializer to use for the bias.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    name: String, the name of the layer. Layers with the same name will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      share weights, but to avoid mistakes we require reuse=True in such</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cases.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dtype: Default dtype of the layer (default of `None` means use the type</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      of the first input). Required when `build` is called before `call`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               kernel_initializer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               bias_initializer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(GRUCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Inputs must be 2-dimensional.</span></span><br><span class="line"></span><br><span class="line">    self.input_spec = base_layer.InputSpec(ndim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line"></span><br><span class="line">    self._kernel_initializer = kernel_initializer</span><br><span class="line"></span><br><span class="line">    self._bias_initializer = bias_initializer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, inputs_shape</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> inputs_shape[<span class="number">1</span>].value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                       % inputs_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    input_depth = inputs_shape[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    self._gate_kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;gates/%s&quot;</span> % _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + self._num_units, <span class="number">2</span> * self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=self._kernel_initializer)</span><br><span class="line"></span><br><span class="line">    self._gate_bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;gates/%s&quot;</span> % _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[<span class="number">2</span> * self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=(</span><br><span class="line"></span><br><span class="line">            self._bias_initializer</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self._bias_initializer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span> init_ops.constant_initializer(<span class="number">1.0</span>, dtype=self.dtype)))</span><br><span class="line"></span><br><span class="line">    self._candidate_kernel = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;candidate/%s&quot;</span> % _WEIGHTS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[input_depth + self._num_units, self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=self._kernel_initializer)</span><br><span class="line"></span><br><span class="line">    self._candidate_bias = self.add_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;candidate/%s&quot;</span> % _BIAS_VARIABLE_NAME,</span><br><span class="line"></span><br><span class="line">        shape=[self._num_units],</span><br><span class="line"></span><br><span class="line">        initializer=(</span><br><span class="line"></span><br><span class="line">            self._bias_initializer</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self._bias_initializer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span> init_ops.zeros_initializer(dtype=self.dtype)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Gated recurrent unit (GRU) with nunits cells.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    gate_inputs = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, state], <span class="number">1</span>), self._gate_kernel)</span><br><span class="line"></span><br><span class="line">    gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    value = math_ops.sigmoid(gate_inputs)</span><br><span class="line"></span><br><span class="line">    r, u = array_ops.split(value=value, num_or_size_splits=<span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    r_state = r * state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    candidate = math_ops.matmul(</span><br><span class="line"></span><br><span class="line">        array_ops.concat([inputs, r_state], <span class="number">1</span>), self._candidate_kernel)</span><br><span class="line"></span><br><span class="line">    candidate = nn_ops.bias_add(candidate, self._candidate_bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    c = self._activation(candidate)</span><br><span class="line"></span><br><span class="line">    new_h = u * state + (<span class="number">1</span> - u) * c</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_h, new_h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_LSTMStateTuple = collections.namedtuple(<span class="string">&quot;LSTMStateTuple&quot;</span>, (<span class="string">&quot;c&quot;</span>, <span class="string">&quot;h&quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>仔细阅读源码发现在 $sigma$ 计算 gate，以及 tanh 计算 candidate 之前都有偏置项，不过公式中都没写出来。而且在不设置 bias 的初始值时，默认的 GRU 中 gate_bias 的初始值是 1, 而 LSTM 中 gate_bias 的初始值是 0.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">gru_cell = tf.nn.rnn_cell.GRUCell(num_units=<span class="number">128</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">gru_cell.state_size, gru_cell.output_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<pre><code>(128, 128)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = gru_cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">30</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = gru_cell(inputs, h0)</span><br><span class="line"></span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<p>出现个很神奇的现象，如果我写成这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output, state = gru_cell.call(inputs, h0) <span class="comment"># 会报错的，gru_cell 没有 self._gate_kernel 这个属性，很神奇，</span></span><br><span class="line"></span><br><span class="line">                                          <span class="comment"># 不过这里先运行上面那行代码，所以没有出现报错</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="tf-nn-rnn-cell-LSTMCell-tf-contrib-rnn-LSTMCell"><a href="#tf-nn-rnn-cell-LSTMCell-tf-contrib-rnn-LSTMCell" class="headerlink" title="tf.nn.rnn_cell.LSTMCell, tf.contrib.rnn.LSTMCell"></a>tf.nn.rnn_cell.LSTMCell, tf.contrib.rnn.LSTMCell</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.LSTMCell&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMCell</span>(<span class="params">LayerRNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Long short-term memory unit (LSTM) recurrent network cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The default non-peephole implementation is based on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    http://www.bioinf.jku.at/publications/older/2604.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  S. Hochreiter and J. Schmidhuber.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Long Short-Term Memory&quot;. Neural Computation, 9(8):1735-1780, 1997.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The peephole implementation is based on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://research.google.com/pubs/archive/43905.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Hasim Sak, Andrew Senior, and Francoise Beaufays.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Long short-term memory recurrent neural network architectures for</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   large scale acoustic modeling.&quot; INTERSPEECH, 2014.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The class uses optional peep-hole connections, optional cell clipping, and</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  an optional projection layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               use_peepholes=<span class="literal">False</span>, cell_clip=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               initializer=<span class="literal">None</span>, num_proj=<span class="literal">None</span>, proj_clip=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_unit_shards=<span class="literal">None</span>, num_proj_shards=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=<span class="literal">None</span>, reuse=<span class="literal">None</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize the parameters for an LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>相比 BasicLSTMCell 多了这四个参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Args:</span><br><span class="line"></span><br><span class="line">    use_peepholes: <span class="built_in">bool</span>, <span class="built_in">set</span> <span class="literal">True</span> to enable diagonal/peephole connections.</span><br><span class="line"></span><br><span class="line">    cell_clip: (optional) A <span class="built_in">float</span> value, <span class="keyword">if</span> provided the cell state <span class="keyword">is</span> clipped</span><br><span class="line"></span><br><span class="line">        by this value prior to the cell output activation.</span><br><span class="line"></span><br><span class="line">    num_proj: (optional) <span class="built_in">int</span>, The output dimensionality <span class="keyword">for</span> the projection</span><br><span class="line"></span><br><span class="line">        matrices.  If <span class="literal">None</span>, no projection <span class="keyword">is</span> performed.</span><br><span class="line"></span><br><span class="line">    proj_clip: (optional) A <span class="built_in">float</span> value.  If `num_proj &gt; <span class="number">0</span>` <span class="keyword">and</span> `proj_clip` <span class="keyword">is</span></span><br><span class="line"></span><br><span class="line">        provided, then the projected values are clipped elementwise to within</span><br><span class="line"></span><br><span class="line">        `[-proj_clip, proj_clip]`.</span><br><span class="line"></span><br><span class="line">````</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">其中 cell_clip 很好理解，就是限制隐藏状态的大小，也就是 output 和 state 的大小。 而 num_proj 呢？</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> num_proj:</span><br><span class="line"></span><br><span class="line">      self._state_size = (</span><br><span class="line"></span><br><span class="line">          LSTMStateTuple(num_units, num_proj)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> state_is_tuple <span class="keyword">else</span> num_units + num_proj)</span><br><span class="line"></span><br><span class="line">      self._output_size = num_proj</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      self._state_size = (</span><br><span class="line"></span><br><span class="line">          LSTMStateTuple(num_units, num_units)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> state_is_tuple <span class="keyword">else</span> <span class="number">2</span> * num_units)</span><br><span class="line"></span><br><span class="line">      self._output_size = num_units</span><br><span class="line"></span><br><span class="line"> ````</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">通过源码可以发现，如果有 num_proj 那么 state 还要加一个全链接， state_size = num_units + num_proj. 而 proj_clip 是限制这个全链接的输出的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BacisLSTMCell 和 LSTMCell 区别还在于后者增加了 peephole 和 cell_clip</span><br><span class="line"></span><br><span class="line">![](https://img-blog.csdn.net/<span class="number">20171201095120010</span>?watermark/<span class="number">2</span>/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWJjbGhxMjAwNQ==/font/5a6L5L2T/fontsize/<span class="number">400</span>/fill/I0JBQkFCMA==/dissolve/<span class="number">70</span>/gravity/SouthEast)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">输入输出的shape，以及状态都是一样的，只不过内部计算方式更加复杂了。对于 peephole 是值得计算 gate 时，考虑到了 $c_&#123;t-<span class="number">1</span>&#125;$ 和 $c_t$.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"></span><br><span class="line">cell = tf.nn.rnn_cell.LSTMCell(num_units=<span class="number">64</span>,cell_clip=<span class="number">0.000000001</span>, num_proj=<span class="number">128</span>, proj_clip=<span class="number">0.001</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell.state_size, cell.output_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(LSTMStateTuple(c=64, h=128), 128)
</code></pre>
<p>发现 state_size 中的 h 维度发生了变化，相当于在每一个时间步得到的 state.h 之后再添加一个全链接。 在 decoder 中可以将 num_proj 设置为词表的大小，那么输出就是对应时间步的词表的分布。在此基础上在 softmax 就可以吧？ 但是下一个时间步的输入的隐藏状态 $h_{t-1}$ 岂不是维度为词表大小。。。感觉最好还是不用这个参数吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">h0.c.shape, h0.h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(TensorShape([Dimension(30), Dimension(64)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.ones(shape=[<span class="number">30</span>,<span class="number">50</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output, state = cell(inputs=inputs, state=h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape, state.c.shape, state.h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>(TensorShape([Dimension(30), Dimension(128)]),

 TensorShape([Dimension(30), Dimension(64)]),

 TensorShape([Dimension(30), Dimension(128)]))
</code></pre>
<h2 id="封装了-RNN-的其他组件"><a href="#封装了-RNN-的其他组件" class="headerlink" title="封装了 RNN 的其他组件"></a>封装了 RNN 的其他组件</h2><p>Core RNN Cell wrappers (RNNCells that wrap other RNNCells)</p>
<ul>
<li><p>tf.contrib.rnn.MultiRNNCell  </p>
</li>
<li><p>tf.contrib.rnn.LSTMBlockWrapper  </p>
</li>
<li><p>tf.contrib.rnn.DropoutWrapper  </p>
</li>
<li><p>tf.contrib.rnn.EmbeddingWrapper  </p>
</li>
<li><p>tf.contrib.rnn.InputProjectionWrapper  </p>
</li>
<li><p>tf.contrib.rnn.OutputProjectionWrapper  </p>
</li>
<li><p>tf.contrib.rnn.DeviceWrapper  </p>
</li>
<li><p>tf.contrib.rnn.ResidualWrapper  </p>
</li>
</ul>
<p>主要看 <code>tf.contrib.rnn.MultiRNNCell</code> 和 <code>tf.contrib.rnn.DropoutWrapper</code>吧，其他的封装的太好了也不好，用的其实也少。</p>
<h3 id="tf-contrib-rnn-MultiRNNCell"><a href="#tf-contrib-rnn-MultiRNNCell" class="headerlink" title="tf.contrib.rnn.MultiRNNCell"></a>tf.contrib.rnn.MultiRNNCell</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiRNNCell</span>(<span class="params">RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  RNN cell composed sequentially of multiple simple cells.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cells, state_is_tuple=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a RNN cell composed sequentially of a number of RNNCells.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cells: list of RNNCells that will be composed in this order.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state_is_tuple: If True, accepted and returned states are n-tuples, where</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `n = len(cells)`.  If False, the states are all</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        concatenated along the column axis.  This latter behavior will soon be</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        deprecated.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      ValueError: if cells is empty (not allowed), or at least one of the cells</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        returns a state tuple but the flag `state_is_tuple` is `False`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(MultiRNNCell, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> cells:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Must specify at least one cell for MultiRNNCell.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nest.is_sequence(cells):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;cells must be a list or tuple, but saw: %s.&quot;</span> % cells)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._cells = cells</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> cell_number, cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(self._cells):</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Add Checkpointable dependencies on these cells so their variables get</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># saved with this object when using object-based saving.</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(cell, checkpointable.CheckpointableBase):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># TODO(allenl): Track down non-Checkpointable callers.</span></span><br><span class="line"></span><br><span class="line">        self._track_checkpointable(cell, name=<span class="string">&quot;cell-%d&quot;</span> % (cell_number,))</span><br><span class="line"></span><br><span class="line">    self._state_is_tuple = state_is_tuple</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> state_is_tuple:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">any</span>(nest.is_sequence(c.state_size) <span class="keyword">for</span> c <span class="keyword">in</span> self._cells):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Some cells return tuples of states, but the flag &quot;</span></span><br><span class="line"></span><br><span class="line">                         <span class="string">&quot;state_is_tuple is not set.  State sizes are: %s&quot;</span></span><br><span class="line"></span><br><span class="line">                         % <span class="built_in">str</span>([c.state_size <span class="keyword">for</span> c <span class="keyword">in</span> self._cells]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">tuple</span>(cell.state_size <span class="keyword">for</span> cell <span class="keyword">in</span> self._cells)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">sum</span>([cell.state_size <span class="keyword">for</span> cell <span class="keyword">in</span> self._cells])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._cells[-<span class="number">1</span>].output_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">zero_state</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ops.name_scope(<span class="built_in">type</span>(self).__name__ + <span class="string">&quot;ZeroState&quot;</span>, values=[batch_size]):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">tuple</span>(cell.zero_state(batch_size, dtype) <span class="keyword">for</span> cell <span class="keyword">in</span> self._cells)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We know here that state_size of each cell is not a tuple and</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># presumably does not contain TensorArrays or anything else fancy</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(MultiRNNCell, self).zero_state(batch_size, dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run this multi-layer cell on inputs, starting from state.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    cur_state_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    cur_inp = inputs</span><br><span class="line"></span><br><span class="line">    new_states = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(self._cells):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">&quot;cell_%d&quot;</span> % i):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> <span class="keyword">not</span> nest.is_sequence(state):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line"></span><br><span class="line">                <span class="string">&quot;Expected state to be a tuple of length %d, but received: %s&quot;</span> %</span><br><span class="line"></span><br><span class="line">                (<span class="built_in">len</span>(self.state_size), state))</span><br><span class="line"></span><br><span class="line">          cur_state = state[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">          cur_state = array_ops.<span class="built_in">slice</span>(state, [<span class="number">0</span>, cur_state_pos],</span><br><span class="line"></span><br><span class="line">                                      [-<span class="number">1</span>, cell.state_size])</span><br><span class="line"></span><br><span class="line">          cur_state_pos += cell.state_size</span><br><span class="line"></span><br><span class="line">        cur_inp, new_state = cell(cur_inp, cur_state)</span><br><span class="line"></span><br><span class="line">        new_states.append(new_state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_states = (<span class="built_in">tuple</span>(new_states) <span class="keyword">if</span> self._state_is_tuple <span class="keyword">else</span></span><br><span class="line"></span><br><span class="line">                  array_ops.concat(new_states, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cur_inp, new_states</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>参数 cell 是元素为 RNNCell对象 的 list 或 tuple. 其实还是只是计算一个时间步的 state</p>
<p><img src="https://panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn12.png"></p>
<p>这里先不考虑双向，只考虑 deep. 也就是使用 MultiRNNCell</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">num_units = [<span class="number">64</span>, <span class="number">128</span>]</span><br><span class="line"></span><br><span class="line">stack_rnns = [tf.nn.rnn_cell.BasicLSTMCell(num_units=i) <span class="keyword">for</span> i <span class="keyword">in</span> num_units]</span><br><span class="line"></span><br><span class="line">stack_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(stack_rnns)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h0 = [cell.zero_state(batch_size=<span class="number">32</span>, dtype=tf.float32) <span class="keyword">for</span> cell <span class="keyword">in</span> stack_rnn]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">32</span>, <span class="number">100</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">output, state = stack_rnn_cell(inputs=inputs, state=h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>TensorShape([Dimension(32), Dimension(128)])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">state[<span class="number">0</span>].c.shape, state[<span class="number">0</span>].h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(TensorShape([Dimension(32), Dimension(64)]),

 TensorShape([Dimension(32), Dimension(64)]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">state[<span class="number">1</span>].c.shape, state[<span class="number">1</span>].h.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(TensorShape([Dimension(32), Dimension(128)]),

 TensorShape([Dimension(32), Dimension(128)]))
</code></pre>
<p>源码中的一部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cur_inp, new_state = cell(cur_inp, cur_state)</span><br><span class="line"></span><br><span class="line">new_states.append(new_state)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>其中从源码中也可以发现把每一层的 state 都储存起来了，而 output 要作为下一层的输入，最后得到的 output 是最后一层的输出。</p>
<h3 id="tf-contrib-rnn-DropoutWrapper"><a href="#tf-contrib-rnn-DropoutWrapper" class="headerlink" title="tf.contrib.rnn.DropoutWrapper"></a>tf.contrib.rnn.DropoutWrapper</h3><p>参考paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@tf_export(<span class="params"><span class="string">&quot;nn.rnn_cell.DropoutWrapper&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DropoutWrapper</span>(<span class="params">RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Operator adding dropout to inputs and outputs of the given cell.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cell, input_keep_prob=<span class="number">1.0</span>, output_keep_prob=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               state_keep_prob=<span class="number">1.0</span>, variational_recurrent=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               input_size=<span class="literal">None</span>, dtype=<span class="literal">None</span>, seed=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_state_filter_visitor=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a cell with added input, state, and/or output dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If `variational_recurrent` is set to `True` (**NOT** the default behavior),</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    then the same dropout mask is applied at every step, as described in:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Y. Gal, Z Ghahramani.  &quot;A Theoretically Grounded Application of Dropout in</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Recurrent Neural Networks&quot;.  https://arxiv.org/abs/1512.05287</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    如果参数 variational_recurrent 设置为 True，那么 dropout 在每一个时间步都会执行 dropout，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Otherwise a different dropout mask is applied at every time step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note, by default (unless a custom `dropout_state_filter` is provided),</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    the memory state (`c` component of any `LSTMStateTuple`) passing through</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    a `DropoutWrapper` is never modified.  This behavior is described in the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    above article.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      cell: an RNNCell, a projection to output_size is added to it.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      input_keep_prob: unit Tensor or float between 0 and 1, input keep</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probability; if it is constant and 1, no input dropout will be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      output_keep_prob: unit Tensor or float between 0 and 1, output keep</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probability; if it is constant and 1, no output dropout will be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      state_keep_prob: unit Tensor or float between 0 and 1, output keep</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probability; if it is constant and 1, no output dropout will be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        State dropout is performed on the outgoing states of the cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        **Note** the state components to which dropout is applied when</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `state_keep_prob` is in `(0, 1)` are also determined by</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the argument `dropout_state_filter_visitor` (e.g. by default dropout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        is never applied to the `c` component of an `LSTMStateTuple`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      上面三个参数分别表示 input，output，state 是否 dropout，以及 dropout 率。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      variational_recurrent: Python bool.  If `True`, then the same</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        dropout pattern is applied across all time steps per run call.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If this parameter is set, `input_size` **must** be provided.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      这个参数如果为 True，那么每一个时间步都需要 dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      input_size: (optional) (possibly nested tuple of) `TensorShape` objects</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        containing the depth(s) of the input tensors expected to be passed in to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the `DropoutWrapper`.  Required and used **iff**</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         `variational_recurrent = True` and `input_keep_prob &lt; 1`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: (optional) The `dtype` of the input, state, and output tensors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Required and used **iff** `variational_recurrent = True`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      seed: (optional) integer, the randomness seed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_state_filter_visitor: (optional), default: (see below).  Function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        that takes any hierarchical level of the state and returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        a scalar or depth=1 structure of Python booleans describing</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        which terms in the state should be dropped out.  In addition, if the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        function returns `True`, dropout is applied across this sublevel.  If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the function returns `False`, dropout is not applied across this entire</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        sublevel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Default behavior: perform dropout on all terms except the memory (`c`)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        state of `LSTMCellState` objects, and don&#x27;t try to apply dropout to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `TensorArray` objects:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        but not `callable`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      ValueError: if any of the keep_probs are not between 0 and 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.LSTMCell(num_units=<span class="number">128</span>),</span><br><span class="line"></span><br><span class="line">                                     input_keep_prob=<span class="number">1.0</span>,</span><br><span class="line"></span><br><span class="line">                                     output_keep_prob=<span class="number">1.0</span>,</span><br><span class="line"></span><br><span class="line">                                     state_keep_prob=<span class="number">1.0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell.state_size, cell.output_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(LSTMStateTuple(c=128, h=128), 128)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 多层 rnn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.nn.rnn_cell <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">NUM_UNITS = [<span class="number">32</span>,<span class="number">64</span>, <span class="number">128</span>]</span><br><span class="line"></span><br><span class="line">rnn = MultiRNNCell([DropoutWrapper(LSTMCell(num_units=n), output_keep_prob=<span class="number">0.8</span>) <span class="keyword">for</span> n <span class="keyword">in</span> NUM_UNITS])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rnn.output_size, rnn.state_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(128,

 (LSTMStateTuple(c=32, h=32),

  LSTMStateTuple(c=64, h=64),

  LSTMStateTuple(c=128, h=128)))
</code></pre>
<h2 id="tf-nn-dynamic-rnn"><a href="#tf-nn-dynamic-rnn" class="headerlink" title="tf.nn.dynamic_rnn"></a>tf.nn.dynamic_rnn</h2><p>最后前面说了这么多 class，他们都只是一种计算当前时间步的 output 和 state 的方式，但是 rnn 处理的都是序列，所以怎么将这些 cell 对象封装到序列 rnn 中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_rnn</span>(<span class="params">cell, inputs, sequence_length=<span class="literal">None</span>, initial_state=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                dtype=<span class="literal">None</span>, parallel_iterations=<span class="literal">None</span>, swap_memory=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                time_major=<span class="literal">False</span>, scope=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates a recurrent neural network specified by RNNCell `cell`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Performs fully dynamic unrolling of `inputs`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rnn_layers = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=n)) <span class="keyword">for</span> n <span class="keyword">in</span> [<span class="number">32</span>, <span class="number">64</span>]]</span><br><span class="line"></span><br><span class="line">cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs = tf.random_normal(shape=[<span class="number">30</span>, <span class="number">10</span>, <span class="number">100</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">initial_state = cell.zero_state(batch_size=<span class="number">30</span>, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output, state = tf.nn.dynamic_rnn(cell,inputs=inputs, initial_state=initial_state, dtype=tf.float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">output.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>TensorShape([Dimension(30), Dimension(10), Dimension(64)])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cell.state_size</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(LSTMStateTuple(c=32, h=32), LSTMStateTuple(c=64, h=64))
</code></pre>
<p>所以目前为止，暂时ojbk了～～ 接下来就是在 attention 封装 rnn 了</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-25T03:25:02.000Z" title="2018/8/25 上午11:25:02">2018-08-25</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">23 分钟读完 (大约3523个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/">论文笔记 Pointer Networks and copy mechanism</a></h1><div class="content"><p>paper:  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5866-pointer-networks">Pointer Networks, NIPS, 2015</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06393">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
</li>
</ul>
<h2 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence.   </p>
</blockquote>
<p>提出来了一种新的架构来学习得到这样的输出序列的条件概率，其中输出序列中的元素是输入序列中离散的 tokens.  </p>
<blockquote>
<p>Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable.  </p>
</blockquote>
<p>这样简单的从输入序列中 copy 输出相关的序列在 seq2seq 或是神经图灵机都很难实现，因为在 decoder 的每一步输出的次的类别依赖于输入序列的长度，这个长度是变化的。</p>
<blockquote>
<p>Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class.  </p>
</blockquote>
<p>和这类问题类似的还有给不定长序列的排序，组合优化等问题。  </p>
<blockquote>
<p> It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output.  </p>
</blockquote>
<p>同之前的 attention 不同的是，之前的 attention 是 decoder 时每一步计算通过 RNN 编码后的输入序列的隐藏变量与当前向量表示的 attention vector，然后生成当前词。而 Ptr-Net 则是使用 attention 作为指针，从输入序列中选择成员作为输出。  </p>
<blockquote>
<p>We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems – finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem – using training examples</p>
</blockquote>
<p>alone.  </p>
<p>Ptr-Net 可以用来学习类似的三个几何问题。</p>
<blockquote>
<p>Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on.  </p>
</blockquote>
<p>Ptr-Net 不仅可以提升 seq2seq with attention,而且能够泛化到变化的 dictionayies.  </p>
<p>从摘要以及 Introduction 来说， Ptr-Net 主要是解决两个方面的问题。  </p>
<ul>
<li><p>一是，简单的 copy 在传统的方法中很难实现，而 Ptr-Net 则是直接从输入序列中生成输出序列。  </p>
</li>
<li><p>而是，可以解决输出 dictionary 是变化的情况。普通的 Seq2Seq 的 output dictionary 大小是固定的，对输出中包含有输入单词(尤其是 OOV 和 rare word) 的情况很不友好。一方面，训练中不常见的单词的 word embedding 质量也不高，很难在 decoder 时预测出来，另一方面，即使 word embedding 很好，对一些命名实体，像人名等，word embedding 都很相似，也很难准确的 reproduce 出输入提到的单词。Point Network 以及在此基础上后续的研究 CopyNet 中的 copy mechanism 就可以很好的处理这种问题，decoder 在各 time step 下，会学习怎样直接 copy 出现在输入中的关键字。</p>
</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/01.png">  </p>
<p>在介绍 Ptr-Net 之前，作者先回顾了一下基本模型 seq2seq 和  input-attention.  </p>
<h4 id="sequence-to-sequence-Model"><a href="#sequence-to-sequence-Model" class="headerlink" title="sequence-to-sequence Model"></a>sequence-to-sequence Model</h4><p>实际上 seq2seq 解决的问题是在当前样本空间里面，给定输入下，使得输出序列的概率最大化。其实类似的 MT，QA，Summarization 都可以看作是这一类问题。只不过根据输入和输出之间的关系，调整相应的模型。  </p>
<p>$$p(C^P|P;\theta)=\sum_{i=1}^m(P)p_{\theta}(C_i|C_1,…,C_{i-1},P;\theta)$$</p>
<p>通过训练学习得到参数使得条件概率最大：  </p>
<p>$$\theta^* = {argmax}<em>{\theta}\sum</em>{P,C^P}logp(C^P|P;\theta)$$</p>
<p>其中类和是在训练样本上。</p>
<blockquote>
<p>In this sequence-to-sequence model, the output dictionary size for all symbols $C_i$ is fixed and equal to n, since the outputs are chosen from the input. Thus, we need to train a separate model for each n. This prevents us from learning solutions to problems that have an output dictionary with a size that depends on the input sequence length.   </p>
</blockquote>
<p>在 seq2seq 模型中，输出的 dictionary 是固定大小的。因为不能解决 dictionary 是变化的情况。</p>
<h4 id="Content-Based-Input-Attention"><a href="#Content-Based-Input-Attention" class="headerlink" title="Content Based Input Attention"></a>Content Based Input Attention</h4><p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"></p>
<p>在每一个 decoder step，先计算 $e_{ij}$ 得到对齐概率(或者说 how well input position j matches output position i)，然后做一个 softmax 得到 $a_{ij}$，再对 $a_{ij}$ 做一个加权和作为 context vector $c_i$，得到这个 context vector 之后在固定大小的 output dictionary 上做 softmax 预测输出的下一个单词。</p>
<blockquote>
<p>This model performs significantly better than the sequence-to-sequence model on the convex hull problem, but it is not applicable to problems where the output dictionary size depends on the input.  </p>
</blockquote>
<p>Nevertheless, a very simple extension (or rather reduction) of the model allows us to do this easily.</p>
<h4 id="Ptr-Net"><a href="#Ptr-Net" class="headerlink" title="Ptr-Net"></a>Ptr-Net</h4><p>seq2seq 模型的输出词是在固定的 dictionary 中进行 softmax，并选择概率最大的词，从而得到输出序列。但这里的输出 dictionary size 是取决于 input 序列的长度的。所以作者提出了新的模型，其实很简单。</p>
<p>$$u_j^i=v^Ttanh(W_1e_j+W_2d_i) ，j\in(1,…,n)$$</p>
<p>$$p(C_i|C_1,…,C_{i-1},P)=softmax(u^i)$$</p>
<p>i 表示decoder 的时间步，j 表示输入序列中的index. 所以$e_j$ 是 encoder 编码后的隐藏向量，$d_i$ 是 decoder 当前时间步 i 的隐藏向量。跟一般的 attention 基本上一致。只不过得到的 softmax 概率应用在输入序列 $C_1,…,C_{i-1}$ 上。</p>
<h4 id="Dataset-Structure"><a href="#Dataset-Structure" class="headerlink" title="Dataset Structure"></a>Dataset Structure</h4><ul>
<li>TensorFlow implementation of “Pointer Networks”：<a target="_blank" rel="noopener" href="https://github.com/devsisters/pointer-network-tensorflow">https://github.com/devsisters/pointer-network-tensorflow</a></li>
</ul>
<ul>
<li>Dataset：<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/0B2fg8yPGn2TCMzBtS0o4Q2RJaEU">https://drive.google.com/drive/folders/0B2fg8yPGn2TCMzBtS0o4Q2RJaEU</a></li>
</ul>
<h2 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation.  </p>
</blockquote>
<p>还是前面提到的问题，seq2seq 很难解决简单的 copy 问题。而在人类的对话中，出现 copy 的现象是很常见的。尤其是 命令实体 或者是长短语。</p>
<blockquote>
<p>The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation.  </p>
</blockquote>
<p>这也是 seq2seq 模型所需面对的挑战。</p>
<p>For example:  </p>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/03.png"></p>
<p>可以看到，对于 Chandralekha 这类实体词，可能是 OOV，也可能是其他实体或者是日期等很难被 decoder “还原” 出来的信息，CopyNet 可以更好的处理这类的信息。</p>
<p>那么问题来了：  </p>
<ul>
<li><p>What to copy: 输入中的哪些部分应该被 copy?  </p>
</li>
<li><p>Where to paste: 应该把这部分信息 paste 到输出的哪个位置？</p>
</li>
</ul>
<h3 id="Model-Architecture-1"><a href="#Model-Architecture-1" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>作者从两个角度来理解 CopyNet:  </p>
<ul>
<li><p>From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity.  从认知学角度，copy机制近似于死记硬背，不需要太多的理解，但是要保证文字的保真度。  </p>
</li>
<li><p>From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model.  从模型的角度，copy 操作更加死板和符号化，这也使得相比 soft attention 机制更难整合到一个完整的可微分的神经模型中去。  </p>
</li>
</ul>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/04.png"></p>
<p>整体还是基于 encoder-decoder 模型。</p>
<p><strong>Encoder:</strong>    </p>
<p>LSTM 将 source sequence 转换为隐藏状态 M(emory) $h_1,…,h_{T_S}$.</p>
<p><strong>Decoder:</strong>    </p>
<p>同 cannonical 的 decoder 一样，使用 RNN 读取 encoder 的隐藏状态 M. 但和传统的 decoder 不一样，他有如下区别：  </p>
<ul>
<li><strong>Prediction:</strong> COPYNET predicts words based on a mixed probabilistic model of two modes, namely the generate-mode and the copymode, where the latter picks words from the source sequence. 下一个词的预测由两种模式混合而成。生成 generate-mode 和 copy-mode. 后者就像前面 Ptr-Net 所说的，在 source sentence 获取词。</li>
</ul>
<ul>
<li><strong>State Update:</strong>  the predicted word at time t−1 is used in updating the state at t, but COPYNET uses not only its word-embedding but also its corresponding location-specific hidden state in M (if any). 更新 decoder 中的隐藏状态时，t 时间步的隐藏状态不仅与 t-1 步生成词的 embedding vector 有关，还与这个词对应于 source sentence 中的隐藏状态的位置有关。</li>
</ul>
<ul>
<li><strong>Reading M:</strong> in addition to the attentive read to M, COPYNET also has“selective read” to M, which leads to a powerful hybrid of</li>
</ul>
<p>content-based addressing and location-based addressing. 什么时候需要 copy，什么时候依赖理解来回答，怎么混合这两种模式很重要。</p>
<blockquote>
<p>个人思考： 感觉不管要不要 copy 都应该是在基于理解的基础上进行的。但是因为 OOV 或者当前词的 embedding vector 训练的不好，那就无法理解了对吧？ 是否可以添加 gate 机制呢？ 机器到底还是没理解语言对吧？ 貌似是个可以创新的点。</p>
</blockquote>
<p>接下来会详细讲解这三个不同之处怎么实现的。</p>
<h4 id="Prediction-with-Copying-and-Generation-s-t-rightarrow-y-t"><a href="#Prediction-with-Copying-and-Generation-s-t-rightarrow-y-t" class="headerlink" title="Prediction with Copying and Generation:$s_t\rightarrow y_t$"></a>Prediction with Copying and Generation:$s_t\rightarrow y_t$</h4><p>这部分是从 decoder 隐藏状态 $s_t$ 到输出词 $y_t$ 的过程。传统的encoder-decoder 是一个线性映射就可以了。</p>
<p>词表 $\mathcal{V}={v_1,…,v_N}$, 未登录词 OOV(out of vocabulary) 用 UNK 来表示（unk应该也会有对应的 embedding vector）. 以及用来表示输入序列中的 unique words $X={x_1,…,x_{T_S}}$. 其中 X 使得 copynet 输出 OOV.</p>
<p>对于三者有这样的集合关系（先不要看公式，后面会说到）：</p>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/05.png"></p>
<p>简而言之(In a nutshell), 对于当前 source sentence X 输出的词表范围 $\mathcal{V}\cup \text{UNK} \cup X$.</p>
<p>给定 decoder 中当前时间步的隐藏状态 $s_t$, 以及 encoder 的隐藏状态序列 M.</p>
<p>$$p(y_t|s_t,y_{t-1},c_t,M)=p(y_t,g|s_t,y_{t-1},c_t,M) + p(y_t,c|s_t,y_{t-1},c_t,M)$$</p>
<p>其中 g 代表 generate mode. c 代表 copy mode.</p>
<p>我们知道对于 encoder 部分的输出 $h_1,…,h_{T_S}$， 记做 M，M 其实同时包含了语义和位置信息。那么 decoder 对 M 的读取有两种形式：</p>
<ul>
<li>Content-base  </li>
</ul>
<p>Attentive read from word-embedding</p>
<ul>
<li>location-base  </li>
</ul>
<p>Selective read from location-specific hidden units</p>
<p>两种模式对应的概率计算，以及 score function:  </p>
<p>$$p(y_t,g|\cdot)=\begin{cases} \dfrac{1}{Z}e^{\psi_g(y_t)}&amp;y_t\in V\</p>
<p>0,&amp;y_t\in X \bigcap \overline V\</p>
<p>\dfrac{1}{Z}e^{\psi_g(UNK)},&amp;y_t\notin V\cup X</p>
<p>\end{cases}$$</p>
<p>$$p(y_t,c|\cdot)=\begin{cases}\dfrac{1}{Z}\sum_{j:x_j=y_t}{e^{\psi_c(x_j)}},&amp;y_t\in X\0&amp;\text {otherwise}\end{cases}$$</p>
<p>上面两个公式叠加(相加)可以表示为下图（可以将目标词看作类别为 4 的分类。）：  </p>
<p><img src="/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/05.png"></p>
<p>其中 $\psi_g(\cdot)$ 和 $\psi_c(\cdot)$ 是 generate mode 和 copy mode 的 score function.</p>
<p>Z 是两种模型共享的归一化项，$Z=\sum_{v\in V\cup{UNK}}e^{\psi_g(v)}+\sum_{x\in X}e^{\psi_c(x)}$.</p>
<p>然后对相应的类别计算对应的 score.</p>
<p><strong>Generate-Mode:</strong>  </p>
<p>$$\psi_g(y_t=v_i)=\nu_i^TW_os_t, v_i\in V\cup UNK$$</p>
<ul>
<li><p>$W_o\in R^{(N+1)\times d_s}$  </p>
</li>
<li><p>$\nu_i$ 是 $v_i$ 对应的 one-hot 向量. 得到的结果是当前词的概率。</p>
</li>
</ul>
<p>generate-mode 的 score $\psi(y_t=v_i)$ 和普通的 encoder-decoder 是一样的。全链接之后的 softmax.</p>
<p><strong>copy-mode:</strong>  </p>
<p>$$\psi(y_t=x_j)=\sigma(h_j^TW_c)s_t,x_j\in \mathcal{V}$$</p>
<ul>
<li><p>$h_j$ 是 encoder hidden state. j 表示输入序列中的位置。</p>
</li>
<li><p>$W_c\in R^{d_h\times d_s}$ 将 $h_j$ 映射到跟 $s_t$ 一样的语义空间。  </p>
</li>
<li><p>作者发现使用 tanh 非线性变换效果更好。同时考虑到 $y_t$ 这个词可能在输入中出现多次，所以需要考虑输入序列中所有的为 $y_t$ 的词的概率的类和。</p>
</li>
</ul>
<h4 id="state-update"><a href="#state-update" class="headerlink" title="state update"></a>state update</h4><p>上面一部分讲的是怎么从 decoder 中的隐藏状态计算对应的 vocabulary，也就是 $s_t\rightarrow y_t$. 那么怎么计算当前时间步的隐藏状态呢？ 我们知道传统的 encoder-decoder 中隐藏状态就是 content-based atention vector. 但是在 copynet 里面，作者对 $y_{t-1}\rightarrow s_t$ 这个计算方式做了一定的修改。  </p>
<p>先回顾下基本的 attention 模块，decoder 中隐藏状态的更新 $s_t=f(y_{t-1},s_{t-1},c_t)$, 其中 $c_t$ 也就是 attention 机制：</p>
<p>$$c_t=\sum_{\tau=1}^{T_S}\alpha_{t\tau}$$</p>
<p>$$\alpha_{t\tau}=\dfrac{e^{\eta(s_{t-1},h_{\tau})}}{\sum_{\tau’}e^{\eta(s_{t-1},h_{\tau’})}}$$</p>
<p>CopyNet 的 $y_{t-1}$ 在这里有所不同。不仅仅考虑了词向量，还使用了 M 矩阵中特定位置的 hidden state，或者说，$y_{t−1}$ 的表示中就包含了这两个部分的信息 $[e(y_{t−1});\zeta(y_{t−1})]$，$e(y_{t−1})$ 是词向量，后面多出来的一项 $\zeta(y_{t−1})$ 叫做 selective read, 是为了连续拷贝较长的短语。和attention 的形式差不多，是 M 矩阵中 hidden state 的加权和.</p>
<p>$$\zeta(y_{t-1})=\sum_{\tau=1}^{T_S}\rho_{t\tau}h_{\tau}$$</p>
<p>$$\rho_{t\tau}=\begin{cases}\dfrac{1}{K}p(x_{\tau},c|s_{t-1},M),&amp; x_{\tau}=y_{t-1}\</p>
<p>0,&amp; \text{otherwise}</p>
<p>\end{cases}$$</p>
<ul>
<li><p>当 $y_{t-1}$ 没有出现在 source sentence中时， $\zeta(y_{t-1})=0$.  </p>
</li>
<li><p>这里的 $K=\sum{\tau’:x_{\tau’}=y_{t-1}}p(x_{\tau’},c|s_{t-1},M)$ 是类和。还是因为输入序列中可能出现多个当前词，但是每个词在 encoder hidden state 的向量表示是不一样的，因为他们的权重也是不一样的。  </p>
</li>
<li><p>这里的 p 没有给出解释，我猜跟前面计算 copy 的 score 是一致的？  </p>
</li>
<li><p>直观上来看，当 $\zeta(y_{t-1})$ 可以看作是选择性读取 M (selective read). 先计算输入序列中对应所有 $y_{t-1}$ 的权重，然后加权求和，也就是 $\zeta(y_{t-1})$.</p>
</li>
</ul>
<h4 id="Hybrid-Adressing-of-M"><a href="#Hybrid-Adressing-of-M" class="headerlink" title="Hybrid Adressing of M"></a>Hybrid Adressing of M</h4><p>包括两种 Addressing 方式： content-based and location-based assressing.</p>
<p><strong>location-based Addressing:</strong>  </p>
<p>$$\zeta(y_{t-1}) \longrightarrow{update} \ s_t \longrightarrow predict \ y_t \longrightarrow sel. read \zeta(y_t)$$</p>
<h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>最小化概率的负对数：  </p>
<p>$$L=-\dfrac{1}{N}\sum_{k=1}^N\sum_{t=1}^Tlog[p(y_t^{(k)}|y_{&lt;t}^{(k)}, X^{(k)})]$$</p>
<p>N 是batch size，T 是 object sentence 长度。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-14T01:34:42.000Z" title="2018/8/14 上午9:34:42">2018-08-14</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.141Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">14 分钟读完 (大约2028个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/">论文笔记-Match LSTM</a></h1><div class="content"><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>SQuAD the answers do not come from a small set of candidate</p>
</blockquote>
<p>answers and they have variable lengths. We propose an end-to-end neural architecture for the task.  </p>
<p>针对 SQuAD 这样的阅读理解式任务提出的端到端的模型。 SQuAD 的答案不是从候选词中提取，而是类似于人类的回答，是不同长度的句子。  </p>
<blockquote>
<p>The architecture is based on match-LSTM, a model we proposed</p>
</blockquote>
<p>previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences.   </p>
<p>主要是基于 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.03134">Pointer Networks</a></p>
<p>关于阅读理解的数据集 benchmark dataset：  </p>
<ul>
<li><p>MCTest: A challenge dataset for the open-domain machine comprehension of text.  </p>
</li>
<li><p>Teaching machines to read and comprehend.  </p>
</li>
<li><p>The Goldilocks principle: Reading children’s books with explicit memory representations.  </p>
</li>
<li><p>Towards AI-complete question answering: A set of prerequisite toy tasks.  </p>
</li>
<li><p>SQuAD: 100,000+ questions for machine comprehension of text.  </p>
</li>
</ul>
<p><strong>SQuAD</strong>  </p>
<p><img src="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/01.png">  </p>
<blockquote>
<p>Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering, including syntactic parsing, named entity recognition, question classification, semantic parsing, etc. Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension.  </p>
</blockquote>
<p>传统的智能问答任务整个流程包括 句法分析、命名实体识别、问题分类、语义分析等。。随着深度学习的发展，端到端的模型开始出现。</p>
<p>End-to-end model architecture:  </p>
<ul>
<li><p>Teaching machines to read and comprehend.  </p>
</li>
<li><p>The Goldilocks principle: Reading children’s books with explicit memory representations.  </p>
</li>
<li><p>Attention-based convolutional neural network for machine comprehension  </p>
</li>
<li><p>Text understanding with the attention sum reader network.  </p>
</li>
<li><p>Consensus attention-based neural networks for chinese reading comprehension.  </p>
</li>
</ul>
<blockquote>
<p>However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.  </p>
</blockquote>
<p>之前的模型的 answer 要么是从候选答案中选择，要么是一个简单的符号。这都不适合 SQuDA.  </p>
<p>模型是基于作者早期提出的用于 textual entailment 的 match-LSTM<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.08849">Learning natural language inference with LSTM</a>，然后进一步应用了 Pointer Net(<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5866-pointer-networks">https://papers.nips.cc/paper/5866-pointer-networks</a>), 从而允许预测的结果能够从输入中获得，而不是从一个固定的词表中获取。</p>
<blockquote>
<p>We propose two ways to apply the Ptr-Net model for our task: a sequence model and a boundary model. We also further extend the boundary model with a search mechanism.  </p>
</blockquote>
<p>作者提出的两种模型。</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><h3 id="Match-LSTM"><a href="#Match-LSTM" class="headerlink" title="Match-LSTM"></a>Match-LSTM</h3><h3 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h3><p><strong>Pointer Network (Ptr-Net) model</strong> : to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence. Instead of picking an output token from a fixed vocabulary, Ptr-Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol.   </p>
<p>从输入 sentences 中生成 answer.</p>
<p>类似于 Pointer Network 的模型：   </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06393">Incorporating copying mechanism in sequence-to-sequence learning.</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.01547">Text understanding with the attention sum reader network.</a></p>
</li>
</ul>
<h3 id="MATCH-LSTM-AND-ANSWER-POINTER"><a href="#MATCH-LSTM-AND-ANSWER-POINTER" class="headerlink" title="MATCH-LSTM AND ANSWER POINTER"></a>MATCH-LSTM AND ANSWER POINTER</h3><p><img src="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/02.png">  </p>
<p>模型主要分为3部分：</p>
<ul>
<li><p>An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs. 使用 LSTM 处理 question 和 passage.  </p>
</li>
<li><p>A match-LSTM layer that tries to match the passage against the question. 使用 match-LSTM 对lstm编码后的 question 和 passage 进行匹配。  </p>
</li>
<li><p>An Answer Pointer (Ans-Ptr) layer that uses Ptr-Net to select a set of tokens from the passage as the answer. The difference between the two models only lies in the third layer.  使用 Pointer 来选择 tokens.  </p>
</li>
</ul>
<h4 id="LSTM-preprocessing-Layer"><a href="#LSTM-preprocessing-Layer" class="headerlink" title="LSTM preprocessing Layer"></a>LSTM preprocessing Layer</h4><p>$$H^p=\overrightarrow {LSTM}(P), H^q=\overrightarrow {LSTM}(Q)$$</p>
<p>直接使用单向LSTM，每一个时刻的隐含层向量输出 $H^p\in R^{l\times P}, H^q\in R^{l\times Q}$ 只包含左侧上下文信息.</p>
<h4 id="Match-LSTM-Layer"><a href="#Match-LSTM-Layer" class="headerlink" title="Match-LSTM Layer"></a>Match-LSTM Layer</h4><p>$$\overrightarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\overrightarrow {h^r}_{i-1}+b^p)\otimes e_Q)\in R^{l\times Q}$$</p>
<p>$$\overrightarrow \alpha_i=softmax(w^T\overrightarrow G_i + b\otimes e_Q)\in R^{1\times Q}$$</p>
<p>the resulting attention weight $\overrightarrow α_{i,j}$ above indicates the degree of matching between the</p>
<p>$i^{th}$ token in the passage with the $j^{th}$ token in the question.   </p>
<p>其中 $W^q,W^p,W^r \in R^{l\times l}, b^p,w\in R^l, b\in R$</p>
<p>所以 $\overrightarrow α_{i}$ 表示整个 question 与 passage 中的第 i 个词之间的 match 程度，也就是通常理解的 attention 程度。  </p>
<blockquote>
<p>传统的 attention 就是将 passage 和 question 矩阵相乘，比如 transformer 中 query 和 keys 相乘。复杂一点可能就是 dynamic memory networks 中的将 两个需要 match 的向量相减、element-wise相乘之后，使用两层的前馈神经网络来表示。  </p>
</blockquote>
<p>这里的 attention score 的计算方式又不一样了。 $\overrightarrow{h^r_{i-1}}$ 是通过 LSTM 耦合 weighted queston 和 passage 中上一个词得到的信息。</p>
<p>其中：</p>
<p>$$\overrightarrow z_i=\begin{bmatrix} h^p \ H^q\overrightarrow {\alpha_i^T} \ \end{bmatrix} $$</p>
<p>$$h^r=\overrightarrow{LSTM}(\overrightarrow{z_i},\overrightarrow{h^r_{i-1}})$$  </p>
<p>然后类似于LSTM将 $\overrightarrow{h_{i-1}^r}$ 和 当前 passage 的表示 $H^p_i$ 耦合得到的 $R^{l\times 1}$ 的向量重复Q 次，得到 $R^{l\times Q}$，所以 $\overrightarrow G_i\in R^{l\times Q}$, 在通过一个softmax-affine网络得到 attention weights.</p>
<blockquote>
<p>整个思路下来，就是 attention score 不是通过矩阵相乘，也不是向量 $h^p_i, H^q$ 相减之后通过神经网络得到。但是也相似，就是对当前要匹配的两个向量 $h^p_i, H^q$ 通过两层神经网络得到,其中的对当前向量 $H_i^p$ 和 $\overrightarrow {h_{i-1}^r}$ 要重复 Q 次。。。其实跟 DMN 还是相似的，只不过不是简单的 attention 当前的向量，还用了 LSTM 来耦合之前的信息。</p>
</blockquote>
<p>最终得到想要的结合了 attention 和 LSTM 的输出 $\overrightarrow h^r$.</p>
<p>作者做了一个反向的 LSTM. 方式是一样的：  </p>
<p>$$\overleftarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\overleftarrow {h^r}_{i-1}+b^p)\otimes e_Q)$$</p>
<p>$$\overleftarrow \alpha_i=softmax(w^T\overleftarrow G_i + b\otimes e_Q)$$</p>
<p>同样得到 $\overleftarrow {h_i^r}$.</p>
<ul>
<li><p>$\overrightarrow {H^r}\in R^{l\times P}$ 表示隐藏状态 $[\overrightarrow {h^r_1}, \overrightarrow {h^r_2},…,\overrightarrow {h^r_P}]$.</p>
</li>
<li><p>$\overleftarrow {H^r}\in R^{l\times P}$ 表示隐藏状态 $[\overleftarrow {h^r_1}, \overleftarrow {h^r_2},…,\overleftarrow {h^r_P}]$.</p>
</li>
</ul>
<p>然后把两者堆叠起来得到通过 question 匹配之后的 passage 向量表示： $H^r=\begin{bmatrix} \overrightarrow H^r \ \overleftarrow H^r \end{bmatrix} \in R^{2l\times P}$</p>
<h3 id="Answer-Pointer-Layer"><a href="#Answer-Pointer-Layer" class="headerlink" title="Answer Pointer Layer"></a>Answer Pointer Layer</h3><h4 id="The-Sequence-Model"><a href="#The-Sequence-Model" class="headerlink" title="The Sequence Model"></a>The Sequence Model</h4><p>The answer is represented by a sequence of integers $a=(a_1,a_2,…)$ indicating the positions of the selected tokens in the original passage.  </p>
<p>再一次利用 attention，$\beta_{k,j}$ 表示 answer 中第 k 个token选择 passage 中第 j 个次的概率。所以 $\beta_k\in R^{P+1}$.</p>
<p>$$F_k=tanh(V\tilde {H^r}+(W^ah^a_{k-1}+b^a)\otimes e_{P+1})\in R^{l\times P+1}$$</p>
<p>$$\beta_k=softmax(v^TF_k+c\otimes e_{P+1}) \in R^{1\times (P+1)}$$</p>
<p>其中 $\tilde H\in R^{2l\times (P+1)}$ 表示 $H^r$ 和 zero vector 的叠加, $\tilde H=[H^r, 0], V\in R^{l\times 2l}, W^a\in R^{l\times l}, b^a,v\in R, c\in R$.  </p>
<p>所以还是跟 match-LSTM 一样，先对 $H^r$ 中的每一个词通过全链接表示 $W^ah^a_{k+1}+b^a$, 然后重复 P+1 次，得到 $R^{l\times (P+1)}$. 在通过激活函数 tanh， 再通过一个全连接神经网络，然后使用 softmax 进行多分类。</p>
<p>$$h_k^a=\overrightarrow{LSTM}(\tilde {H^r}\beta_k^T, h^a_{k-1})$$</p>
<p>这里是把 $\tilde H^r$ 与权重 $\beta_k$ 矩阵相乘之后的结果作为 LSTM k 时刻的输入。很玄学， 感觉可以看作是 self-attention 结合了 LSTM.</p>
<p>对生成 answer sequence 的概率进行建模：  </p>
<p>$$p(a|H^r)=\prod_k p(a_k|a_1,a_2,…,a_{k-1}, H^r)$$</p>
<p>其中：</p>
<p>$$p(a_k=j|a_1,a_2,…,a_{k-1})=\beta_{k,j}$$</p>
<p>目标函数 loss function:</p>
<p>$$-\sum_{n=1}^N logp(a_n|P_n,Q_n)$$</p>
<h4 id="The-Boundary-Model"><a href="#The-Boundary-Model" class="headerlink" title="The Boundary Model"></a>The Boundary Model</h4><p>So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to Hr, and the probability of generating an answer is simply modeled as:</p>
<p>$$p(a|H^r)=p(a_s|H^r)p(a_e|a_s, H^r)$$</p>
<p><strong>Search mechanism, and bi-directional Ans-Ptr.</strong></p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>SQuAD: Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics. Each passage is a single paragraph from a Wikipedia article, and each passage has around 5 questions associated with it. In total, there are 23,215 passages and 107,785 questions. The data has been split into a training set (with 87,599 question-answer pairs), a development set (with 10,570 questionanswer pairs) and a hidden test set</p>
<h4 id="configuration"><a href="#configuration" class="headerlink" title="configuration"></a>configuration</h4><ul>
<li><p>dimension l of the hidden layers is set to 150 or 300.  </p>
</li>
<li><p>Adammax: $\beta_1=0.9, \beta_2=0.999$  </p>
</li>
<li><p>minibatch size = 30  </p>
</li>
<li><p>no L2 regularization.</p>
</li>
</ul>
<h4 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h4><p><img src="/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/03.png"></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/13/">上一页</a></div><div class="pagination-next"><a href="/page/15/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/13/">13</a></li><li><a class="pagination-link is-current" href="/page/14/">14</a></li><li><a class="pagination-link" href="/page/15/">15</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">117</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>