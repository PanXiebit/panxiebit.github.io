<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-10-15T11:34:48.000Z" title="2018/10/15 下午7:34:48">2018-10-15</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.158Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">21 分钟读完 (大约3214个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">论文笔记-CNN与自然语言处理</a></h1><div class="content"><p>最近在参加 AI challenge 观点型阅读理解的比赛。数据集形式如下：</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/01.png"></p>
<p>最开始尝试的模型主要包括几个部分：</p>
<ul>
<li><p><strong>Embedding:</strong> 使用预训练的中文词向量。  </p>
</li>
<li><p><strong>Encoder:</strong> 基于 Bi-GRU 对 passage,query 和 alternatives 进行编码处理。  </p>
</li>
<li><p><strong>Attention:</strong> 用 trilinear 的方式，并 mask 之后得到相似矩阵，然后采用类似于 <a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/08/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QA%20BiDAF/">BiDAF</a> 中的形式 bi-attention flow 得到 attened passage.  </p>
</li>
<li><p><strong>contextual:</strong> 用 Bi-GRU 对 attened passage 进行编码，得到 fusion.  </p>
</li>
<li><p><strong>match</strong> 使用 attention pooling 的方式将 fusion 和 enc_answer 转换为单个 vector. 然后使用 cosin 进行匹配计算得到最相似的答案。  </p>
</li>
</ul>
<p>目前能得到的准确率是 0.687. 距离第一名差了 0.1…其实除了换模型，能提升和改进的地方是挺多的。</p>
<ul>
<li><p>可以用 ELMO 或 wordvec 先对训练集进行预训练得到自己的词向量。  </p>
</li>
<li><p>attention 层可以使用更丰富的方式，很多paper 中也有提到。甚至可以加上人工提取的特征。比如苏剑林 <a target="_blank" rel="noopener" href="https://kexue.fm/archives/5409">blog</a> 中提到的。</p>
</li>
<li><p>还有个很重要的就是 match 部分， attention pooling 是否可以换成其他更好的方式？</p>
</li>
</ul>
<p>但是，不断尝试各种模型的前提也要考虑速度吧。。rnn 实在是太慢了，所以决定试试 CNN 的方式来处理 NLP 的任务。</p>
<p>关于使用 CNN 来处理阅读理解的任务的大作还是挺多的，这里主要介绍这两篇：  </p>
<ul>
<li><p>Facebook: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.03122">Convolutional Sequence to Sequence Learning</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.04352">Fast Reading Comprehension with ConvNets</a></p>
</li>
</ul>
<h1 id="ConvS2S"><a href="#ConvS2S" class="headerlink" title="ConvS2S"></a>ConvS2S</h1><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.03122">Convolutional Sequence to Sequence Learning</a></p>
<p>这篇 paper 对应的 NLP 任务是机器翻译，除了用 CNN 对 sentence 进行编码之外，其核心是在 decoder 的时候也使用 CNN. 对于阅读理解来说，能够借用的是其编码 sentence 的方式。但这里作为学习，也多了解一下 decoder 吧～</p>
<p>对文本来说，看到 CNN 我们首先想到的是 cnn 能有效利用局部信息，提取出局部特征，所以适合做文本分类。但是对于 机器翻译、阅读理解这样的需要考虑全局信息的任务，CNN 似乎看起来并不那么有效。而且在 decoder 的时候，词的生成是 one by one 的，下一个词的生成是依赖于上一个词的。所以在 decoder 中使用 RNN 也是很自然而然的。</p>
<p>Facebook 的这篇 paper 就改变了这些传统的思维，不仅用 CNN 编码全局信息，而且还能 decoder.</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher layers.  </p>
</blockquote>
<p>多层 CNN 具有层级表示结构，相邻的词之间在较低层的 layer 交互，距离较远的词在较高层的 layer 交互（交互的目的就是语义消歧）。</p>
<blockquote>
<p>Hierarchical structure provides a shorter path to capture long-range dependencies compared to the chain structure modeled by recurrent networks, e.g. we can obtain a feature representation capturing relationships within a window of n words by applying only O(n/k) convolutional operations for kernels of width k, compared to a linear number O(n) for recurrent neural networks.  </p>
</blockquote>
<p>层级结构提供了一个更短的路径来获取长期依赖。比如相距为 n 的两个词，在 rnn 中交互需要的步数是 O(n),在层级 CNN 中需要 O(n/k).这样减少了非线性的操作，降低了梯度消失的情况。所以这两个词的交互效果会更好～</p>
<blockquote>
<p>Inputs to a convolutional network are fed through a constant number of kernels and non-linearities, whereas recurrent networks apply up to n operations and non-linearities to the first word and only a single set of operations to the last word. Fixing the number of nonlinearities applied to the inputs also eases learning.  </p>
</blockquote>
<p>输入到 CNN 中每个词都会经历固定的 kernel 和 非线性操作。而输入到 RNN 的，第一个词需要经过 n 个 operations，最后一个词只经历了一个 operations. 作者认为固定的操作更容易学习。  </p>
<p>这一点我个人认为并不一定就是合理的，本来一个句子中不同词的重要性就是不一样的。  </p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Ｍodel Architecture"></a>Ｍodel Architecture</h2><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/02.png"></p>
<p>模型分为以下几个部分：  </p>
<ul>
<li><p>position embedding  </p>
</li>
<li><p>convolution block structure  </p>
</li>
<li><p>Multi-step attention  </p>
</li>
</ul>
<h3 id="position-encoding"><a href="#position-encoding" class="headerlink" title="position encoding"></a>position encoding</h3><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/05.png"></p>
<p>这部分在很多地方都出现过了，在没有 rnn 的情况下，都会用 PE 来编码位置信息。但是在这篇 paper 中，作者通过实验发现，PE 作用似乎并不是很重要。  </p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/03.png"></p>
<h3 id="convolution-blocks"><a href="#convolution-blocks" class="headerlink" title="convolution blocks"></a>convolution blocks</h3><p>作者使用的是门激活机制， GLU, gate linear units.  </p>
<blockquote>
<p>来自于 paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.08083">Language modeling with gated convolutional networks</a>  </p>
</blockquote>
<p>在这篇 paper 中，作者用无监督的方式，来训练语言模型，将 CNN 得到的语言模型与 LSTM 进行对比。</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/06.png"></p>
<p>也就是:</p>
<p>$$h_l=(X<em>W+b)\otimes \sigma(X</em>V+c)$$</p>
<blockquote>
<p>The output of each layer is a linear projection X ∗ W + b modulated by the gates σ(X ∗ V + c). Similar to LSTMs, these gates multiply each element of the matrix X ∗W+b</p>
</blockquote>
<p>and control the information passed on in the hierarchy.</p>
<p>如果是 LSTM-style，应该是 GTU：</p>
<p>$$h_i^l=tanh(X<em>W+b)\otimes \sigma(X</em>V+c)$$</p>
<p>作者将两者进行了对比，发现 GLU 效果更好。</p>
<p><strong>residual connection:</strong> 为了得到更 deep 的卷积神经网络，作者增加了残差链接。</p>
<p>$$h_i^l=v(W^l[h_{i-k/2}^{l-1},…,h_{i+k/2}^{l-1}]+b_w^l)+h_i^{l-1}$$</p>
<p>卷积的整个过程：  </p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/07.png"></p>
<p>论文中举了这样一个例子：  </p>
<blockquote>
<p>For instance, stacking 6 blocks with k = 5 results in an input field of 25 elements, i.e. each output depends on 25 inputs. Non-linearities allow the networks to exploit the full input field, or to focus on fewer elements if needed.  </p>
</blockquote>
<p>这个怎么算的呢？看下图：</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/09.jpg"></p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/10.jpg"></p>
<p>从上图中可以看到，当 k=3 时，3 个 blocks，第三层中的每一个输入都与输入中的 7 列有关。所以计算方式是 k + (k-1)* (blocks-1).</p>
<p>一维卷积和二维卷积的区别：</p>
<ul>
<li><p>ConvS2S 是 1D 卷积，kernel 只是在时间维度上平移，且 stride 的固定 size 为1,这是因为语言不具备图像的可伸缩性，图像在均匀的进行降采样后不改变图像的特征，而一个句子间隔着取词，意思就会改变很多了。  </p>
</li>
<li><p>在图像中一个卷积层往往有多个 filter，以获取图像不同的 pattern，但是在 ConvS2S 中，每一层只有一个 filter。一个句子进入 filter 的数据形式是 [1, n, d]. 其中 n 为句子长度， filter 对数据进行 n 方向上卷积，而 d 是词的向量维度，可以理解为 channel，与彩色图片中的 rgb 三个 channel 类似。</p>
</li>
</ul>
<blockquote>
<p>Facebook 在设计时，并没有像图像中常做的那样，每一层只设置一个 filter。这样做的原因，一是为了简化模型，加速模型收敛，二是他们认为一个句子的 pattern 要较图像简单很多，通过每层设置一个 filter，逐层堆叠后便能抓到所有的 pattern. 更有可能的原因是前者。因为在 transorfmer 中，multi-head attention 多头聚焦取得了很好的效果，说明一个句子的 pattern 是有多个的.  </p>
</blockquote>
<p>这段话是有问题的吧？ filter 的个数难道不是 2d吗？ 只不过这里说的 transorfmer 的多头聚焦是值得聚焦到一个词向量中的部分维度。记得在 cs224d 中 manning 曾经讲过一个例子，经过训练或词与词之间的交互后，词向量中的部分维度发生了变化。</p>
<p>在 paper 中，卷积核的尺寸大小是 $W\in R^{2d\times kd}$.</p>
<blockquote>
<p>For encoder networks we ensure that the output of the convolutional layers matches the input length by padding the input at each layer. However, for decoder networks we have to take care that no future information is available to the decoder (Oord et al., 2016a). Specifically, we pad the input by k − 1 elements on both the left and right side by zero vectors, and then remove k elements from the end of the convolution output.</p>
</blockquote>
<p>在 encoder 和 decoder 网络中，padding 的方式是不一样的。因为在 decoder 的时候不能考虑未来信息.  </p>
<p>在 encoder 时，将 (k-1) pad 到左右两边，保证卷积层的长度不变。  </p>
<p>在 decoder 中，将 (k-1) pad 到句子的左边。因此生成的词依旧是 one by one.</p>
<h3 id="Multi-step-Attention"><a href="#Multi-step-Attention" class="headerlink" title="Multi-step Attention"></a>Multi-step Attention</h3><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/08.gif"></p>
<p>$$d_i^l=W_d^lh_i^l+b_d^l+g_i$$</p>
<p>$$a_{ij}^l=\dfrac{exp(d_i^l\cdot z_j^u)}{\sum_{t=1}^mexp(d_i^l\cdot z_j^u)}$$</p>
<p>$$c_i^l=\sum_{j=1}^ma_{ij}^l(z_j^u+e_j)$$</p>
<p>上式中，l 表示 decoder 中卷积层的层数，i 表示时间步。  </p>
<p>实际上跟 rnn 的 decoder 还是比较接近的。  </p>
<ul>
<li><p>在训练阶段是 teacher forcing, 卷积核 $W_d^l$ 在 target sentence $h^l$ 上移动做卷积得到 $(W_d^lh_i^l + b_d^l)$，类似与 rnn-decoder 中的隐藏状态。然后加上上一个词的 embedding $g_i$,得到 $d_i^l$.  </p>
</li>
<li><p>与 encdoer 得到的 source sentence 做交互，通过 softmax 得到 attention weights $a_{ij}^l$.  </p>
</li>
<li><p>得到 attention vector 跟 rnn-decoder 有所不同，这里加上了 input element embedding $e_j$.</p>
</li>
</ul>
<p><strong>至于这里为什么要加 $e_j$?</strong>  </p>
<blockquote>
<p>We found adding e_j to be beneficial and it resembles key-value memory networks where the keys are the z_j^u and the values are the z^u_j + e_j (Miller et al., 2016). Encoder outputs z_j^u represent potentially large input contexts and e_j provides point information about a specific input element that is useful when making a prediction. Once c^l_i has been computed, it is simply added to the output of the corresponding decoder layer h^l_i.</p>
</blockquote>
<p>$z_j^u$ 表示更丰富的信息，而 $e_j$ 能够能具体的指出输入中对预测有用的信息。还是谁用谁知道吧。。</p>
<p><strong>关于 multi-hop attention:</strong>  </p>
<blockquote>
<p>This can be seen as attention with multiple ’hops’ (Sukhbaatar et al., 2015) compared to single step attention (Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016). In particular, the attention of the first layer determines a useful source context which is then fed to the second layer that takes this information into account when computing attention etc. The decoder also has immediate access to the attention history of the k − 1 previous time steps because the conditional inputs $c^{l-1}_{i−k}, . . . , c^{l-1}<em>i$ are part of $h^{l-1}</em>{i-k}, . . . , h^{l-1}_i$ which are input to $h^l_i$. This makes it easier for the model to take into account which previous inputs have been attended to already compared to recurrent nets where this information is in the recurrent state and needs to survive several non-linearities. Overall, our attention mechanism considers which words we previously attended to (Yang et al., 2016) and performs multiple attention ’hops’ per time step. In Appendix §C, we plot attention scores for a deep decoder and show that at different layers, different portions of the source are attended to.</p>
</blockquote>
<p>这个跟 memory networks 中的 multi-hop 是有点类似。</p>
<h1 id="FAST-READING-COMPREHENSION-WITH-CONVNETS"><a href="#FAST-READING-COMPREHENSION-WITH-CONVNETS" class="headerlink" title="FAST READING COMPREHENSION WITH CONVNETS"></a>FAST READING COMPREHENSION WITH CONVNETS</h1><p>Gated Linear Dilated Residual Network (GLDR):   </p>
<p>a combination of <strong>residual networks</strong> (He et al., 2016), <strong>dilated convolutions</strong> (Yu &amp; Koltun, 2016) and <strong>gated linear units (Dauphin et al., 2017)</strong>.</p>
<h2 id="text-understanding-with-dilated-convolution"><a href="#text-understanding-with-dilated-convolution" class="headerlink" title="text understanding with dilated convolution"></a>text understanding with dilated convolution</h2><p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/11.png"></p>
<p>kernel:$k=[k_{-l},k_{-l+1},…,k_l]$, size=$2l+1$    </p>
<p>input: $x=[x_1,x_2,…,x_n]$  </p>
<p>dilation: d</p>
<p>卷积可以表示为：</p>
<p>$$(k*x)_ t=\sum_{i=-l}^lk_i\cdot x_{t + d\cdot i}$$</p>
<p><strong>为什么要使用膨胀卷积呢？ Why Dilated convolution?</strong>  </p>
<blockquote>
<p>Repeated dilated convolution (Yu &amp; Koltun, 2016) increases the receptive region of ConvNet outputs exponentially with respect to the network depth, which results in drastically shortened computation paths.   </p>
</blockquote>
<p>能够显著缩短两个词之间的计算路径。</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/12.png"></p>
<p>作者将 GLDR 和 self-attention,以及 RNN 进行了对比，<strong>input sequence length n, network width w, kernel size k, and network depth D</strong>.</p>
<h2 id="model-Architecture"><a href="#model-Architecture" class="headerlink" title="model Architecture"></a>model Architecture</h2><p>作者与 BiDAF 和 DrQA 进行了对比，将 BiDAF 和 DrQA 中的 BiLSTM 部分替换成 GLDR Convolution.</p>
<p><img src="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/13.png"></p>
<blockquote>
<p>The receptive field of this convolutional network grows</p>
</blockquote>
<p>exponentially with depth and soon encompasses a long sequence, essentially enabling it to capture</p>
<p>similar long-term dependencies as an actual sequential model.  </p>
<p>感受野的尺寸大小指数增加，能够迅速压缩 long sentence,并 capture 长期依赖。</p>
<blockquote>
<p>Convolutional BiDAF. In our convolutional version of BiDAF, we replaced all bidirectional LSTMs with GLDRs . We have two 5-layer GLDRs in the contextual layer whose weights are un-tied. In the modeling layer, a 17-layer GLDR with dilation 1, 2, 4, 8, 16 in the first 5 residual blocks is used, which results in a reception region of 65 words. A 3-layer GLDR replaces the bidirectional LSTM in the output layer. For simplicity, we use same-padding and kernel size 3 for all convolutions unless specified. The hidden size of all GLDRs is 100 which is the same as the LSTMs in BiDAF.  </p>
</blockquote>
<p>具体网络结构，实际参数可以看 paper 实验部分。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-10-01T01:50:57.000Z" title="2018/10/1 上午9:50:57">2018-10-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.140Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">16 分钟读完 (大约2451个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/">论文笔记-batch,layer,weights normalization</a></h1><div class="content"><p>paper:  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">Batch Normalization</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1602.07868.pdf">weights Normalization</a>  </p>
</li>
</ul>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>在之前的笔记已经详细看过了:<a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/">深度学习-Batch Normalization</a></p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.  </p>
</blockquote>
<p>关于 batch normalisztion.</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/01.png"></p>
<p>从 Ng 的课上截来的一张图，全链接层相比卷积层更容易理解点，但形式上是一样的.  </p>
<p>样本数量是 m，第 l 层经过激活函数输出是第 l+1 层的输入，其中第 i 个神经元的值:  </p>
<p>线性输出： $z_i^l={w_i^l}^Th^l$.  </p>
<p>非线性输出： $h_i^{l+1} = a_i^l=f(z_i^l+b_i^l)$</p>
<p>其中 f 是非线性激活函数，$a_i^l$ 是下一层的 summed inputs. 如果 $a_i^l$ 的分布变化较大（change in a highly correlated way）,下一层的权重 $w^{l+1}$ 的梯度也会相应变化很大（反向传播中 $w^{l+1}$ 的梯度就是 $a_i^l$）。</p>
<p>Batch Normalization 就是将线性输出归一化。  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/02.png"></p>
<p>其中 $u_i^l$ 是均值，$\sigma_i^l$ 是方差。 $\overline a_i^l$ 是归一化之后的输出。 $g_i^l$ 是需要学习的参数，也就是 scale.</p>
<blockquote>
<p>有个疑问？为什么 BN 要在激活函数之前进行，而不是之后进行呢？</p>
</blockquote>
<p>上图中是单个样本，而所有的样本其实是共享层与层之间的参数的。样本与样本之间也存在差异，所以在某一个特征维度上进行归一化，（每一层其中的一个神经元可以看作一个特征维度）。</p>
<blockquote>
<p>batch normalization requires running averages of the summed input statistics. In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps.  </p>
</blockquote>
<p>BN 不是用于 RNN 是因为 batch 中的 sentence 长度不一致。我们可以把每一个时间步看作一个维度的特征提取，如果像 BN 一样在这个维度上进行归一化，显然在 RNN 上是行不通的。比如这个 batch 中最长的序列的最后一个时间步，他的均值就是它本身了，岂不是出现了 BN 在单个样本上训练的情况。</p>
<blockquote>
<p>In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case.  </p>
</blockquote>
<p>所以作者在这篇 paper 中提出了 Layer Normalization. 在单个样本上计算均值和方差进行归一化。然而是怎么进行的呢？</p>
<h3 id="Layer-Normalization-1"><a href="#Layer-Normalization-1" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>layer normalization 并不是在样本上求平均值和方差，而是在 hidden units 上求平均值和方差。</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/03.png"></p>
<p>其中 H 是 hidden units 的个数。</p>
<p>BN 和 LN 的差异：  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/04.jpg"></p>
<p>Layer normalisztion 在单个样本上取均值和方差，所以在训练和测试阶段都是一致的。</p>
<p>并且，尽管求均值和方差的方式不一样，但是在转换成 beta 和 gamma 的方式是一样的，都是在 channels 或者说 hidden_size 上进行的。</p>
<h3 id="Layer-normalized-recurrent-neural-networks"><a href="#Layer-normalized-recurrent-neural-networks" class="headerlink" title="Layer normalized recurrent neural networks"></a>Layer normalized recurrent neural networks</h3><blockquote>
<p>RNN is common among the NLP tasks to have different sentence lengths for different training cases. This is easy to deal with in an RNN because the same weights are used at every time-step. But when we apply batch normalization to an RNN in the obvious way, we need to to compute and store separate statistics for each time step in a sequence. This is problematic if a test sequence is longer than any of the training sequences. Layer normalization does not have such problem because its normalization terms depend only on the summed inputs to a layer at the current time-step. It also has only one set of gain and bias parameters shared over all time-steps.  </p>
</blockquote>
<p>这一部分也解释了 BN 不适用于 RNN 的原因，从 test sequence longer 的角度。RNN 的每个时间步计算共享参数权重.</p>
<p>$a^t=W_{hh}h^{t-1}+W_{xh}x^t$</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/05.png"></p>
<p>其中 b 和 g 是可学习的参数。</p>
<p><strong>layer normalize 在 LSTM 上的使用：</strong>  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/06.png"></p>
<h2 id="tensorflow-实现"><a href="#tensorflow-实现" class="headerlink" title="tensorflow 实现"></a>tensorflow 实现</h2><h3 id="batch-Normalization"><a href="#batch-Normalization" class="headerlink" title="batch Normalization"></a>batch Normalization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.training.moving_averages <span class="keyword">import</span> assign_moving_average</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.layers <span class="keyword">import</span> batch_norm</span><br><span class="line"></span><br><span class="line"><span class="comment">### batch normalization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span>(<span class="params">inputs, decay=<span class="number">0.9</span>, is_training=<span class="literal">True</span>, epsilon=<span class="number">1e-6</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs:  [batch, length, width, channels]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param is_training:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param eplison:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    pop_mean = tf.Variable(tf.zeros(inputs.shape[-<span class="number">1</span>]), trainable=<span class="literal">False</span>, name=<span class="string">&quot;pop_mean&quot;</span>)</span><br><span class="line"></span><br><span class="line">    pop_var = tf.Variable(tf.ones(inputs.shape[-<span class="number">1</span>]), trainable=<span class="literal">False</span>, name=<span class="string">&quot;pop_variance&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mean_and_var</span>():</span></span><br><span class="line"></span><br><span class="line">        axes = <span class="built_in">list</span>(<span class="built_in">range</span>(inputs.shape.ndims))</span><br><span class="line"></span><br><span class="line">        batch_mean, batch_var = tf.nn.moments(inputs, axes=axes)</span><br><span class="line"></span><br><span class="line">        moving_average_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (<span class="number">1</span>-decay))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 也可用 assign_moving_average(pop_mean, batch_mean, decay)</span></span><br><span class="line"></span><br><span class="line">        moving_average_var = tf.assign(pop_var, pop_var * decay + batch_var * (<span class="number">1</span>-decay))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 也可用 assign_moving_average(pop_var, batch_var, decay)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.control_dependencies([moving_average_mean, moving_average_var]):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> tf.identity(batch_mean), tf.identity(batch_var)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    mean, variance = tf.cond(tf.equal(is_training, <span class="literal">True</span>), update_mean_and_var,</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">lambda</span>: (pop_mean, pop_var))</span><br><span class="line"></span><br><span class="line">    beta = tf.Variable(initial_value=tf.zeros(inputs.get_shape()[-<span class="number">1</span>]), name=<span class="string">&quot;shift&quot;</span>)</span><br><span class="line"></span><br><span class="line">    gamma = tf.Variable(initial_value=tf.ones(inputs.get_shape()[-<span class="number">1</span>]), name=<span class="string">&quot;scale&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="layer-normalization"><a href="#layer-normalization" class="headerlink" title="layer normalization"></a>layer normalization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">hidden_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">whh = tf.random_normal(shape=[batch, hidden_size], mean=<span class="number">5.0</span>, stddev=<span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">whh_norm = tf.contrib.layers.layer_norm(inputs=whh, center=<span class="literal">True</span>, scale=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(whh)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(whh_norm)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh[<span class="number">0</span>]), tf.reduce_mean(whh[<span class="number">1</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh_norm[<span class="number">0</span>]), tf.reduce_mean(whh_norm[<span class="number">5</span>]), tf.reduce_mean(whh_norm[<span class="number">59</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh_norm[:,<span class="number">0</span>]), tf.reduce_mean(whh_norm[:,<span class="number">1</span>]), tf.reduce_mean(whh_norm[:,<span class="number">63</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(var)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(sess.run(var))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Tensor(&quot;random_normal:0&quot;, shape=(60, 64), dtype=float32)</span><br><span class="line"></span><br><span class="line">Tensor(&quot;LayerNorm/batchnorm/add_1:0&quot;, shape=(60, 64), dtype=float32)</span><br><span class="line"></span><br><span class="line">[5.3812757, 4.607581]</span><br><span class="line"></span><br><span class="line">[-1.4901161e-08, -2.9802322e-08, -3.7252903e-09]</span><br><span class="line"></span><br><span class="line">[-0.22264712, 0.14112064, -0.07268284]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;tf.Variable &#x27;LayerNorm/beta:0&#x27; shape=(64,) dtype=float32_ref&gt;</span><br><span class="line"></span><br><span class="line">[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"></span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"></span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"></span><br><span class="line">&lt;tf.Variable &#x27;LayerNorm/gamma:0&#x27; shape=(64,) dtype=float32_ref&gt;</span><br><span class="line"></span><br><span class="line">[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.</span><br><span class="line"></span><br><span class="line"> 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.</span><br><span class="line"></span><br><span class="line"> 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p> 发现一个很奇怪的问题， layer norm 是在每一个训练样本上求均值和方差，为啥 beta 和 gamma 的shape却是 [hidden_size]. 按理说不应该是 [batch,] 吗？ 带着疑问去看了源码，原来是这样的。。</p>
<p> 将源码用简介的方式写出来了：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm_mine</span>(<span class="params">inputs, epsilon=<span class="number">1e-12</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    inputs: [batch, sequence_len, hidden_size] or [batch, hidden_size]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    inputs_shape = inputs.shape</span><br><span class="line"></span><br><span class="line">    inputs_rank = inputs_shape.ndims</span><br><span class="line"></span><br><span class="line">    params_shape = inputs_shape[-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    beta, gamma = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> center:</span><br><span class="line"></span><br><span class="line">        beta = tf.get_variable(</span><br><span class="line"></span><br><span class="line">            name=<span class="string">&quot;beta&quot;</span>,</span><br><span class="line"></span><br><span class="line">            shape=params_shape,</span><br><span class="line"></span><br><span class="line">            initializer=tf.zeros_initializer(),</span><br><span class="line"></span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">        gamma = tf.get_variable(</span><br><span class="line"></span><br><span class="line">            name=<span class="string">&quot;gamma&quot;</span>,</span><br><span class="line"></span><br><span class="line">            shape=params_shape,</span><br><span class="line"></span><br><span class="line">            initializer=tf.ones_initializer(),</span><br><span class="line"></span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    norm_axes = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, inputs_rank))</span><br><span class="line"></span><br><span class="line">    mean, variance = tf.nn.moments(inputs, norm_axes, keep_dims=<span class="literal">True</span>)      <span class="comment"># [batch]</span></span><br><span class="line"></span><br><span class="line">    inv = tf.rsqrt(variance + epsilon)</span><br><span class="line"></span><br><span class="line">    inv *= gamma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inputs*inv + ((beta-mean)*inv <span class="keyword">if</span> beta <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> - mean * inv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">hidden_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">whh = tf.random_normal(shape=[batch, hidden_size], mean=<span class="number">5.0</span>, stddev=<span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">whh_norm = layer_norm_mine(whh)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>layer_norm_mine 得到的结果与源码一致。可以发现 计算均值和方差时， <code>tf.nn.moments</code> 中 <code>axes=[1:-1]</code>. （tf.nn.moments 中 axes 的含义是在这些维度上求均值和方差）. 也就是说得到的均值和方差确实是 [batch,]. 只是在转换成 beta 和 gamma 的分布时，依旧是在最后一个维度上进行的。有意思，所以最终的效果应该和 batch normalization 效果是一致的。只不过是否符合图像或文本的特性就另说了。</p>
<h3 id="LayerNormBasicLSTMCell"><a href="#LayerNormBasicLSTMCell" class="headerlink" title="LayerNormBasicLSTMCell"></a>LayerNormBasicLSTMCell</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNormBasicLSTMCell</span>(<span class="params">rnn_cell_impl.RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;LSTM unit with layer normalization and recurrent dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This class adds layer normalization and recurrent dropout to a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  basic LSTM unit. Layer normalization implementation is based on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1607.06450.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Layer Normalization&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  and is applied before the internal nonlinearities.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Recurrent dropout is base on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1603.05118</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Recurrent Dropout without Memory Loss&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               forget_bias=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               input_size=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=math_ops.tanh,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               layer_norm=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               norm_gain=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               norm_shift=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_keep_prob=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_prob_seed=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initializes the basic LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: int, The number of units in the LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      forget_bias: float, The bias added to forget gates (see above).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      input_size: Deprecated and unused.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      activation: Activation function of the inner states.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      layer_norm: If `True`, layer normalization will be applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      norm_gain: float, The layer normalization gain initial value. If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `layer_norm` has been set to `False`, this argument will be ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      norm_shift: float, The layer normalization shift initial value. If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `layer_norm` has been set to `False`, this argument will be ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_keep_prob: unit Tensor or float between 0 and 1 representing the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        recurrent dropout probability value. If float and 1.0, no dropout will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        be applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_prob_seed: (optional) integer, the randomness seed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LayerNormBasicLSTMCell, self).__init__(_reuse=reuse)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      logging.warn(<span class="string">&quot;%s: The input_size parameter is deprecated.&quot;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._activation = activation</span><br><span class="line"></span><br><span class="line">    self._forget_bias = forget_bias</span><br><span class="line"></span><br><span class="line">    self._keep_prob = dropout_keep_prob</span><br><span class="line"></span><br><span class="line">    self._seed = dropout_prob_seed</span><br><span class="line"></span><br><span class="line">    self._layer_norm = layer_norm</span><br><span class="line"></span><br><span class="line">    self._norm_gain = norm_gain</span><br><span class="line"></span><br><span class="line">    self._norm_shift = norm_shift</span><br><span class="line"></span><br><span class="line">    self._reuse = reuse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rnn_cell_impl.LSTMStateTuple(self._num_units, self._num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_norm</span>(<span class="params">self, inp, scope, dtype=dtypes.float32</span>):</span></span><br><span class="line"></span><br><span class="line">    shape = inp.get_shape()[-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    gamma_init = init_ops.constant_initializer(self._norm_gain)</span><br><span class="line"></span><br><span class="line">    beta_init = init_ops.constant_initializer(self._norm_shift)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(scope):</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Initialize beta and gamma for use by layer_norm.</span></span><br><span class="line"></span><br><span class="line">      vs.get_variable(<span class="string">&quot;gamma&quot;</span>, shape=shape, initializer=gamma_init, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      vs.get_variable(<span class="string">&quot;beta&quot;</span>, shape=shape, initializer=beta_init, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    normalized = layers.layer_norm(inp, reuse=<span class="literal">True</span>, scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> normalized</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_linear</span>(<span class="params">self, args</span>):</span></span><br><span class="line"></span><br><span class="line">    out_size = <span class="number">4</span> * self._num_units</span><br><span class="line"></span><br><span class="line">    proj_size = args.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dtype = args.dtype</span><br><span class="line"></span><br><span class="line">    weights = vs.get_variable(<span class="string">&quot;kernel&quot;</span>, [proj_size, out_size], dtype=dtype)</span><br><span class="line"></span><br><span class="line">    out = math_ops.matmul(args, weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      bias = vs.get_variable(<span class="string">&quot;bias&quot;</span>, [out_size], dtype=dtype)</span><br><span class="line"></span><br><span class="line">      out = nn_ops.bias_add(out, bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;LSTM cell with layer normalization and recurrent dropout.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    c, h = state</span><br><span class="line"></span><br><span class="line">    args = array_ops.concat([inputs, h], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    concat = self._linear(args)</span><br><span class="line"></span><br><span class="line">    dtype = args.dtype</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=<span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      i = self._norm(i, <span class="string">&quot;input&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      j = self._norm(j, <span class="string">&quot;transform&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      f = self._norm(f, <span class="string">&quot;forget&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      o = self._norm(o, <span class="string">&quot;output&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    g = self._activation(j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">not</span> <span class="built_in">isinstance</span>(self._keep_prob, <span class="built_in">float</span>)) <span class="keyword">or</span> self._keep_prob &lt; <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">      g = nn_ops.dropout(g, self._keep_prob, seed=self._seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_c = (</span><br><span class="line"></span><br><span class="line">        c * math_ops.sigmoid(f + self._forget_bias) + math_ops.sigmoid(i) * g)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      new_c = self._norm(new_c, <span class="string">&quot;state&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    new_h = self._activation(new_c) * math_ops.sigmoid(o)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_h, new_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-24T03:06:40.000Z" title="2018/9/24 上午11:06:40">2018-09-24</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.159Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/">language model</a></span><span class="level-item">13 分钟读完 (大约1992个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/">论文笔记-character embedding and ELMO</a></h1><div class="content"><ul>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1508.06615.pdf">Character-Aware Neural Language Models</a>  </p>
</li>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a></p>
</li>
</ul>
<h1 id="character-embedding"><a href="#character-embedding" class="headerlink" title="character embedding"></a>character embedding</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>A language model is formalized as a probability distribution over a sequence of strings (words), and traditional methods usually involve making an n-th order Markov assumption and estimating n-gram probabilities via counting and subsequent smoothing (Chen and Goodman 1998). The count-based models are simple to train, but probabilities of rare n-grams can be poorly estimated due to data sparsity (despite smoothing techniques).  </p>
</blockquote>
<p>对语言模型的描述：语言模型是 一个单词序列的概率分布 的形式化描述（什么意思？就是比如这个句子长度为 10, 那么每个位置可能是词表中的任意一个词，而出现当前词是有一个概率的, 这个概率是依赖于之前的词的）。  </p>
<p>在传统的方法主要是运用 n阶马尔可夫假设来估计 n-gram 的概率，通过统计计数，以及子序列平滑的方式。这种基于计数的模型虽然简单，但是在数据稀疏的情况下，对不常见的 n-gram 的概率估计会很差。  </p>
<blockquote>
<p>While NLMs have been shown to outperform count-based n-gram language models (Mikolov et al. 2011), they are blind to subword information (e.g. morphemes). For example, they do not know, a priori, that eventful, eventfully, uneventful, and uneventfully should have structurally related embeddings in the vector space. Embeddings of rare words can thus be poorly estimated, leading to high perplexities for rare words (and words surrounding them). This is especially problematic in morphologically rich languages with long-tailed frequency distributions or domains with dynamic vocabularies (e.g. social media).  </p>
</blockquote>
<p>neural language models 将词嵌入到低维的向量中，使得语义相似的词在向量空间的位置也是相近的。然后 Mikolov word2vec 这种方式不能有效的解决子单词的信息问题，比如一个单词的各种形态，也不能认识前缀。这种情况下，不可避免的会造成不常见词的向量表示估计很差，对于不常见词会有较高的困惑度。这对于词语形态很丰富的语言是一个难题，同样这种问题也是动态词表的问题所在（比如社交媒体）。</p>
<h2 id="Recurrent-Neural-Network-Language-Model"><a href="#Recurrent-Neural-Network-Language-Model" class="headerlink" title="Recurrent Neural Network Language Model"></a>Recurrent Neural Network Language Model</h2><p>给定词表为 V，之前的序列是 $w_{1:t}=[w_1,..,w_t]$,在 RNN-LM 中通过全链接 affine transformation 计算 $w_{t+1}$ 个词的概率分布：  </p>
<p>$$Pr(w_{t+1}=j|w_{1:t})=\dfrac{exp(h_t\cdot p^j+q^j)}{\sum_{j’\in V}exp(h_t\cdot p^{j’}+q^{j’})}$$</p>
<p>其中 $h_t$ 是当前 t 时刻的隐藏状态。也就是先通过全链接映射到词表的 V 的维度，然后通过 softmax 计算其是词表中第 j 个词的概率。</p>
<p>然后假设训练预料库的 sentence 是 $w_{1:T}=[w_1,…,w_T]$,那么训练也就是最小化这个序列的 似然概率的负对数：</p>
<p>$$NLL=-\sum_{T}^{t=1}logPr(w_t|w_{1:t-1})$$</p>
<h2 id="Chracter-level-Convolution-Neural-Network"><a href="#Chracter-level-Convolution-Neural-Network" class="headerlink" title="Chracter-level Convolution Neural Network"></a>Chracter-level Convolution Neural Network</h2><p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/01.png"></p>
<p>以单词 absurdity 为例，有 l 个字符（通常会 padded 到一个固定size），通过 character embedding 映射成矩阵 $C\in R^{d\times l}$. d 是 embedding size. 图中 embedding size 为 4.</p>
<p>然后使用卷积核 kernel H 做卷积运算, $H\in R^{d\times w}$，所以得到的 feature map $f^k\in R^{l-w+1}$. 跟之前 CNN 做文本分类其实挺像的, kernel 的长是 embedding size d, 宽度 w 分别是 2,3,4. 上图中蓝色区域为例，filter 宽度为 2 的个数是3, 那么卷积得到的 featur map 是 $3 \times (9-2+1) = 3\times 8$.</p>
<p>$$f^k[i]=tanh(&lt;C^k[* ,i:i-w+1], H&gt; +b)$$</p>
<p>&lt;&gt;表示做卷积运算(Frobenius inner product). 然后加上 bias 和 非线性激活函数 tanh.</p>
<p>接着基于 times 维度做 max pooling. 上图中 filter 宽度为 3,2,4 的个数分别为 4,3,5.所以得到长度为 4+3+5=12 的向量。</p>
<p>这里每一个 filter matrix 得到一个相应的特征 feature. 在通常的 NLP 任务中这些 filter 的总数 $h\in[100, 1000]$</p>
<h2 id="Highway-Network"><a href="#Highway-Network" class="headerlink" title="Highway Network"></a>Highway Network</h2><p>通过卷积层得到单词 k 的向量表示为 $y^k$.</p>
<p>Highway Network 分为两层 layer.</p>
<ul>
<li>one layer of an MLP applies an affine transformation:</li>
</ul>
<p>$$z=g(W_y+b)$$</p>
<ul>
<li>one layer 有点类似 LSTM 中的 gate 机制：</li>
</ul>
<p>$$z=t\circ g(W_Hy+b_H)+(1-t)\circ y$$</p>
<p>其中  g 是非线性函数。$t=\sigma(W_Ty+b_T)$. t 成为 transform gate, (1-t) 是 carry gate. 同 LSTM 类似， highway network 允许输出能自适应的从 $y^k$ 中直接获取信息。</p>
<h1 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h1><p>传统的提取 word embedding 的方法，比如 word2vec 和 language model， 前者是通过词与词之间的共现，后者是 contextual，但他们都是获得固定的 embedding，也就是每一个词对应一个单一的 embedding.  而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练 language model, 而 word embedding 是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 且 ELMo 输出多个层的 embedding 表示, 试验中已经发现每层 LM 输出的信息对于不同的任务效果不同, 因此对每个 token 用不同层 embedding 表示会提升效果.</p>
<p>个人觉得，可以从这个角度去理解。RNN 可以看做一个高阶马尔可夫链，而不同于 马尔可夫模型，RNN 中的状态转移矩阵是用神经网络来模拟的，也就是我们计算隐藏层所用的 $h_t=tanh(w_{hh}h_{t-1}+w_{hx}x_t)$. 这个状态转移是动态的，也是不断更新的。而使用 语言模型 来训练 RNN/LSTM 目的就是得到这样的一套参数，使得它能学习到任何 合理的，自然的 sentence. 所以，这个语料库越大越好。事实上，有监督的训练也可以达到这个目的，但是有监督的数据有限，并且整个模型是有偏置的，比如文本分类的任务去训练，那么它更倾向于 局部信息。相比之下，机器翻译作为有监督的效果会更好，最好的还是语言模型呢，不仅可用的数据量很大，而且因为要预测每一个词的信息，它会努力结合每一个词的上下文去学习这个词的表示。这也正是我们需要的。ELMo 和 BERT 都是这样的道理，而 BERT 的优势前一篇 blog 说过了。</p>
<h2 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h2><p>给定 sentence $t_1, t_2,…,t_N$, 通过前面的词 $t_1,..,t_{k-1}$ 计算 token $t_k$ 的概率分布:</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/06.png"></p>
<p>反向：</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/02.png"></p>
<p>语言模型的训练就是采用极大似然估计，最大化这个概率：</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/03.png"></p>
<p>传统的方法就是 提取出对应位置的向量表示作为对应位置的词向量 context-independent token representation $x_k^{LM}$.</p>
<h2 id="ELMo-1"><a href="#ELMo-1" class="headerlink" title="ELMo"></a>ELMo</h2><blockquote>
<p>ELMo is a task specific combination of the intermediate layer representations in the biLM.</p>
</blockquote>
<p>ELMo 实际上只是下游任务的中间层，跟 BERT 一样。但也有不同的是， ELMo 每一层的向量表示会获得不同的 信息。底层更能捕捉 syntax and semantics 信息，更适用于 part-of-speech tagging 任务，高层更能获得 contextual 信息，更适用于 word sense disambiguation 任务。所以对不同的任务，会对不同层的向量表示的利用不同。</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/05.png"></p>
<p>在使用 ELMo 进行下游有监督训练时，通常是这样 $[x_k; ELMo_k^{task}]$. 对于 SQuAD 这样的任务，$[h_k, ELMo_k^{task}]$.</p>
<h2 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h2><blockquote>
<p>The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second</p>
</blockquote>
<p>layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation.</p>
<p>具体模型还是得看代码。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-22T10:56:03.000Z" title="2018/9/22 下午6:56:03">2018-09-22</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/">MRC and QA</a></span><span class="level-item">12 分钟读完 (大约1777个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/">论文笔记-QANet</a></h1><div class="content"><p>paper:</p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.09541">combining local convolution with local self-attention for reading comprehension</a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.   </p>
</blockquote>
<p> encoder 编码方式仅仅由 卷积 和 自注意力 机制构成，没了 rnn 速度就是快。</p>
<blockquote>
<p>The key motivation behind the design of our model is the following: convolution captures the local structure of the text, while the self-attention learns the global interaction between each pair of words.  </p>
</blockquote>
<p> 这篇论文最主要的创新点：使用 CNN 来捕捉文本结构的局部信息，使用 self-attention 来学习全局中每两个词之间的交互信息，使得其能耦合上下文信息。相比 RNN，attention 能够有效的解决长期依赖问题。只是相比少了词序信息。说到底，也是一种 contextualize 的 encoder 方式。</p>
<blockquote>
<p>we propose a complementary data augmentation technique to enhance the training data. This technique paraphrases the examples by translating the original sentences from English to another language and then back to English, which not only enhances the number of training instances but also diversifies the phrasing.  </p>
</blockquote>
<p>使用了一种数据增强的方式，先将源语言转换成另一种语言，然后再翻译回英语。这样能有效增加训练样本，同时也丰富了短语的多样性。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa01.png"></p>
<p>模型分为5部分:  </p>
<ul>
<li><p>an embedding layer  </p>
</li>
<li><p>an embedding encoder layer  </p>
</li>
<li><p>a context-query attention layer  </p>
</li>
<li><p>a model encoder layer  </p>
</li>
<li><p>an output layer.</p>
</li>
</ul>
<blockquote>
<p>the combination of convolutions and self-attention is novel, and is significantly better than self-attention alone and gives 2.7 F1 gain in our experiments. The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth (layer dropout) (Huang et al., 2016), which gives an additional gain of 0.2 F1 in our experiments.  </p>
</blockquote>
<p>CNN 和 self-attention 的结合比单独的 self-attention 效果要好。同时使用了 CNN 之后能够使用常用的正则化方式 dropout, 这也能带来一点增益。</p>
<h3 id="Input-embedding-layer"><a href="#Input-embedding-layer" class="headerlink" title="Input embedding layer"></a>Input embedding layer</h3><blockquote>
<p>obtain the embedding of each word w by concatenating its word embedding and character embedding.</p>
</blockquote>
<p>由词向量和字符向量拼接而成。其中词向量采用预训练的词向量 Glove，并且不可训练，fixed. 只有 OOV (out of vocabulary) 是可训练的，用来映射所有不在词表内的词。</p>
<blockquote>
<p>Each character is represented as a trainable vector of dimension p2 = 200, meaning each word can be viewed as the concatenation of the embedding vectors for each of its characters. The length of each word is either truncated or padded to 16. We take maximum value of each row of this matrix to get a fixed-size vector representation of each word.  </p>
</blockquote>
<p>字符向量的处理。每个字母是可训练的，对应的维度是 200 维。然后每个词都 truncated 或者 padded 成16个字母，保证每个词的向量维度是一样大小。  </p>
<p>所以一个词的向量维度是 $300+200=500$.</p>
<h3 id="Embedding-encoding-layer"><a href="#Embedding-encoding-layer" class="headerlink" title="Embedding encoding layer"></a>Embedding encoding layer</h3><blockquote>
<p>The encoder layer is a stack of the following basic building block: [convolution-layer × # + self-attention-layer + feed-forward-layer]</p>
</blockquote>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa02.png"></p>
<p>其中：  </p>
<ul>
<li>convolution: 使用 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02357">depthwise separable convolutions</a> 而不是用传统的 convolution，因为作者发现 <strong>it is memory efficient and has better generalization.</strong> 怎么理解这个，还得看原 paper. The kernel size is 7, the number of filters is d = 128.</li>
</ul>
<ul>
<li>self-attention: the multi-head attention mechanism <a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/">论文笔记, Attention Is All You Need</a></li>
</ul>
<blockquote>
<p>Each of these basic operations (conv/self-attention/ffn) is placed inside a residual block, shown lower-right in Figure 1. For an input x and a given operation f, the output is f(layernorm(x))+x.  </p>
</blockquote>
<p>在 cnn/self-attention/ffn 层都有 layer normalization.</p>
<h4 id="为什么要用-CNN："><a href="#为什么要用-CNN：" class="headerlink" title="为什么要用 CNN："></a>为什么要用 CNN：</h4><p>用来获取局部信息 k-gram features</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa03.png"></p>
<p>相信看了这个图能对 QANet 中的 cnn 怎么实现的更清楚了。上图中每个卷积核的尺寸分别是 [2, embed_size], [3, embed_size], [3, embed_size]. padding参数 使用的是 “SAME”. 得到 3 个 [1, sequence_len]，然后拼接起来, 得到最终结果 [filters_num, sequence_len].</p>
<p>在 QANet 的实现中，kernel_size 都设置为7, num_filters=128.</p>
<h4 id="为什么要用-self-attention"><a href="#为什么要用-self-attention" class="headerlink" title="为什么要用 self-attention"></a>为什么要用 self-attention</h4><p>用来获取全局信息。</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa04.png"></p>
<p>上图中的这种方式显然不太好，复杂度高且效果不好。于是有了 self-attention.</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa05.png"></p>
<p>矩阵内部向量之间作內积，并通过 softmax 得到其他词对于 “The” 这个词的权重大小（权重比例与相似度成正比，这里看似不太合理 similarity == match??，但实际上效果很不错，可能跟词向量的训练有关）。</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa06.png"></p>
<p>然后将对应的权重大小 $[w_1,w_2,w_3,w_4,w_5]$ 与对应的词相乘，累和得到蕴含了上下文信息的 contextualized “The”.</p>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa07.png">  </p>
<p>并且，这是可以并行化的。大大加速了训练速度。</p>
<h3 id="Context-Query-Attention-Layer"><a href="#Context-Query-Attention-Layer" class="headerlink" title="Context-Query Attention Layer"></a>Context-Query Attention Layer</h3><p>跟 BIDAF 是一样的。来，不看笔记把公式过一遍。  </p>
<p>content: $C={c_1, c_2,…,c_n}$   </p>
<p>query: $Q={q_1,q_2,…q_m}$.</p>
<p>所以 embeded 之后，   </p>
<ul>
<li><p>content: [batch, content_n, embed_size]    </p>
</li>
<li><p>query: [batch, query_m, embed_size]  </p>
</li>
</ul>
<p>做矩阵相乘得到相似矩阵 similarity matrix $S\in R^{n\times m}$:    </p>
<p>sim_matrix: [batch, content_n, query_m]</p>
<blockquote>
<p>The similarity function used here is the trilinear function (Seo et al., 2016). $f(q,c)=W_0[q,c,q\circ c]$.  </p>
</blockquote>
<p>相似矩阵的计算可以不是直接矩阵相乘，而是加个前馈神经网络。毕竟 similarity 不一定等于 match.</p>
<h4 id="content-to-query"><a href="#content-to-query" class="headerlink" title="content-to-query"></a>content-to-query</h4><p>对 S 每一行 row 做 softmax 得到对应的概率，得到权重矩阵 $\tilde S\in R^{n\times m}$, shape = [batch, content_n, query_m].</p>
<p>然后与 query $Q^T$ [batch, query_m, embed_size] 矩阵相乘得到编码了 query 信息的 content:  </p>
<p>$A = \tilde SQ^T$, shape = [batch, content_n, embed_size]</p>
<h4 id="query-to-content"><a href="#query-to-content" class="headerlink" title="query_to_content"></a>query_to_content</h4><blockquote>
<p>Empirically, we find that, the DCN attention can provide a little benefit over simply applying context-to-query attention, so we adopt this strategy.  </p>
</blockquote>
<p>这里没有采用 BiDAF 里面的方法，而是采用 DCN 中的方式，利用了 $\tilde S$.</p>
<p>对 S 每一列 column 做 softmax 得到矩阵 $\overline S$, shape = [batch, content_n, query_n].</p>
<p>然后矩阵相乘得到 $B=\tilde S \overline S^T C^T$.  </p>
<p>$\tilde S$.shape=[batch, content_n, query_m]  </p>
<p>$\overline S^T$.shape=[batch, query_m, content_n]  </p>
<p>$C^T$.shape=[batch, query_m, embed_size]  </p>
<p>所以最后 B.shape=[batch, content_n, embed_size]</p>
<h3 id="Model-Encoder-Layer"><a href="#Model-Encoder-Layer" class="headerlink" title="Model Encoder Layer"></a>Model Encoder Layer</h3><p>同 BiDAF 一样输入是 $[c,a,c\circ a,c\circ b]$， 其中 a, b 分别是 attention matrix A，B 的行向量。不过不同的是，这里不同 bi-LSTM，而是类似于 encoder 模块的 [conv + self-attention + ffn]. 其中 conv 层数是 2, 总的 blocks 是7.</p>
<h3 id="Ouput-layer"><a href="#Ouput-layer" class="headerlink" title="Ouput layer"></a>Ouput layer</h3><p>$$p^1=softmax(W_1[M_0;M_1]), p^2=softmax(W_2[M_0;M_2])$$</p>
<p>其中 $W_1, w_2$ 是可训练的参数矩阵，$M_0, M_1, M_2$ 如图所示。</p>
<p>然后计算交叉熵损失函数：  </p>
<p>$$L(\theta)=-\dfrac{1}{N}\sum_i^N[log(p^1_{y^1})+log(p^2_{y^2})]$$</p>
<h3 id="QANet-哪里好，好在哪儿？"><a href="#QANet-哪里好，好在哪儿？" class="headerlink" title="QANet 哪里好，好在哪儿？"></a>QANet 哪里好，好在哪儿？</h3><p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa08.png"></p>
<ul>
<li><p>separable conv 不仅参数量少，速度快，还效果好。将 sep 变成传统 cnn, F1 值减小 0.7.  </p>
</li>
<li><p>去掉 CNN， F1值减小 2.7.  </p>
</li>
<li><p>去掉 self-attention, F1值减小 1.3.</p>
</li>
</ul>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa09.png">  </p>
<ul>
<li><p>layer normalization  </p>
</li>
<li><p>residual connections  </p>
</li>
<li><p>L2 regularization  </p>
</li>
</ul>
<p><img src="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/qa10.png"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01604">Dynamic Coattention Networks For Question Answering</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02357">Xception: Deep Learning with Depthwise Separable Convolutions</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://thinklab.com/content/2940784">qanet_talk_v1.pdf</a></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-09-01T13:17:28.000Z" title="2018/9/1 下午9:17:28">2018-09-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/TensorFlow/">TensorFlow</a></span><span class="level-item">33 分钟读完 (大约4894个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/01/tensorflow-Attention-API/">Tensorflow Attention API 源码阅读1</a></h1><div class="content"><p>这节内容是详细了解下 tensorflow 关于 attention 的api，由于封装的太好，之前使用过发现报错了很难去找出哪儿 bug，所以用 eager execution 来看具体细节。</p>
<p>按照官方教程 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_guides/python/contrib.seq2seq#Attention">Seq2seq Library (contrib)</a> 这里的流程逐步深入。</p>
<p>This library is composed of two primary components:  </p>
<ul>
<li>New attention wrappers for tf.contrib.rnn.RNNCell objects.</li>
</ul>
<ul>
<li>A new object-oriented dynamic decoding framework.  </li>
</ul>
<p>主要包括两个部分，一个是新的基于 attention 的 RNNCell 对象，一个面向对象的动态解码框架。</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention wrappers are RNNCell objects that wrap other RNNCell objects and implement attention. The form of attention is determined by a subclass of tf.contrib.seq2seq.AttentionMechanism. These subclasses describe the form of attention (e.g. additive vs. multiplicative) to use when creating the wrapper. An instance of an AttentionMechanism is constructed with a memory tensor, from which lookup keys and values tensors are created.</p>
<p>attenion wrapper 也是 RNNCell 对象，父类是 tf.contrib.seq2seq.AttentionMechanism,然后其子类是针对不同 attention 形式（additive vs. multiplicative）的实现。AttentionMechanism 的构造是在 memory 的基础上，memory 也就是 attention 过程中的 keys values.</p>
<h2 id="Attention-Mechanisms"><a href="#Attention-Mechanisms" class="headerlink" title="Attention Mechanisms"></a>Attention Mechanisms</h2><p>attention 的提出来自于：    </p>
<ul>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>  </p>
</li>
<li><p>paper:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></p>
</li>
</ul>
<p><img src="https://panxiaoxie.cn/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"></p>
<ol>
<li>encoder 采用单层或多层的单向或双向的 rnn 得到source sentence 的隐藏状态表示 $H=[h_1,…,h_T]$。  </li>
</ol>
<ol start="2">
<li>decoder 的 t 时间步的隐藏状态为 $s_t$, 在 decoder 阶段也是 rnn，其中隐藏状态的更新为： $s_i=f(s_{i-1},y_{i-1},c_i)$ 其中 $s_{i-1}$ 是上一个隐藏状态，$y_{i-1}$ 是上一时间步的输出，$c_i$ 是当前时间步的 attention vector. 那么现在就是怎么计算当前时间步的 $c_i$.  </li>
</ol>
<ol start="3">
<li>当前时间步的 $e_t=a(s_{i-1}, h_j)$, 这是对齐模型，也就是计算上一个隐藏状态 $s_{i-1}$ 与 encoder 中每一个 hidden 的 match 程度，计算这个 score 有很多中方式，其中最常见的，也是 tf api 中使用的两种 BahdanauAttention 和 LuongAttention.</li>
</ol>
<p>$$\text{BahdanauAttention:}\quad e_{ij}=v_a^Ttanh(W^as_{i-1}+U_ah_j)$$</p>
<p>$$\text{LuongAttention:}\quad e_{ij}=h_j^TW^as_i$$</p>
<ol start="4">
<li>然后对得到的对齐 score 使用 softmax 得到相应的概率:</li>
</ol>
<p>$$a_{ij}=\dfrac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}$$</p>
<p>softmax 实际上相比上面的公式有点区别，就是 $exp(e^{ij}-max(e^{ik}))$ 防止数值溢出。</p>
<ol start="5">
<li>将得到的 $s_{i-1}$ 与 encoder 中的 $h_j$ 计算得到的概率与 $h_j$ 做加权和得到当前时间步的 attention vector $c_i$</li>
</ol>
<ol start="6">
<li>在然后使用 $c_{i-1},s_{i-1},y_{i-1}$ 更新decoder 中的隐藏状态，循环下去。。。</li>
</ol>
<ol start="7">
<li>根据当前的隐藏状态 $s_i$ 计算得到当前时间步的输出 $y_t$</li>
</ol>
<p>$$y_t=Ws_{i}+b$$</p>
<h3 id="先看父类-tf-contrib-seq2seq-AttentionMechanism"><a href="#先看父类-tf-contrib-seq2seq-AttentionMechanism" class="headerlink" title="先看父类 tf.contrib.seq2seq.AttentionMechanism"></a>先看父类 tf.contrib.seq2seq.AttentionMechanism</h3><p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionMechanism</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alignments_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>两个属性： alignments_size 和 state_size 分别对应 sequence 的长度，所以这个 alignment_size 是表示 mask 之后的长度吧？接下来看源码。 state_size 表示隐藏层的状态。显然这里的 attention 也是一个时间步内的计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseAttentionMechanism</span>(<span class="params">AttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A base AttentionMechanism class providing common functionality.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Common functionality includes:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    1. Storing the query and memory layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    2. Preprocessing and storing the memory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               query_layer,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_layer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               check_inner_dims_defined=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct base AttentionMechanism class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      query_layer: Callable.  Instance of `tf.layers.Layer`.  The layer&#x27;s depth</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        must match the depth of `memory_layer`.  If `query_layer` is not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        provided, the shape of `query` must match that of `memory_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory: The memory to query; usually the output of an RNN encoder.  This</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        tensor should be shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      probability_fn: A `callable`.  Converts the score and previous alignments</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        to probabilities. Its signature should be:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probabilities = probability_fn(score, state)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_sequence_length (optional): Sequence lengths for the batch entries</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in memory.  If provided, the memory tensor rows are masked with zeros</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for values past the respective sequence lengths.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_layer: Instance of `tf.layers.Layer` (may be None).  The layer&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        depth must match the depth of `query_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If `memory_layer` is not provided, the shape of `memory` must match</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        that of `query_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      check_inner_dims_defined: Python boolean.  If `True`, the `memory`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        argument&#x27;s shape is checked to ensure all but the two outermost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        dimensions are fully defined.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      score_mask_value: (optional): The mask value for score before passing into</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probability_fn`. The default is -inf. Only used if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `memory_sequence_length` is not None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: Name to use when creating ops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (query_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(query_layer, layers_base.Layer)):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;query_layer is not a Layer: %s&quot;</span> % <span class="built_in">type</span>(query_layer).__name__)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (memory_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(memory_layer, layers_base.Layer)):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;memory_layer is not a Layer: %s&quot;</span> % <span class="built_in">type</span>(memory_layer).__name__)</span><br><span class="line"></span><br><span class="line">    self._query_layer = query_layer</span><br><span class="line"></span><br><span class="line">    self._memory_layer = memory_layer</span><br><span class="line"></span><br><span class="line">    self.dtype = memory_layer.dtype</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">callable</span>(probability_fn):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(<span class="string">&quot;probability_fn must be callable, saw type: %s&quot;</span> %</span><br><span class="line"></span><br><span class="line">                      <span class="built_in">type</span>(probability_fn).__name__)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> score_mask_value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      score_mask_value = dtypes.as_dtype(</span><br><span class="line"></span><br><span class="line">          self._memory_layer.dtype).as_numpy_dtype(-np.inf)</span><br><span class="line"></span><br><span class="line">    self._probability_fn = <span class="keyword">lambda</span> score, prev: (  <span class="comment"># pylint:disable=g-long-lambda</span></span><br><span class="line"></span><br><span class="line">        probability_fn(</span><br><span class="line"></span><br><span class="line">            _maybe_mask_score(score, memory_sequence_length, score_mask_value),</span><br><span class="line"></span><br><span class="line">            prev))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ops.name_scope(</span><br><span class="line"></span><br><span class="line">        name, <span class="string">&quot;BaseAttentionMechanismInit&quot;</span>, nest.flatten(memory)):</span><br><span class="line"></span><br><span class="line">      self._values = _prepare_memory(</span><br><span class="line"></span><br><span class="line">          memory, memory_sequence_length,</span><br><span class="line"></span><br><span class="line">          check_inner_dims_defined=check_inner_dims_defined)</span><br><span class="line"></span><br><span class="line">      self._keys = (</span><br><span class="line"></span><br><span class="line">          self.memory_layer(self._values) <span class="keyword">if</span> self.memory_layer  <span class="comment"># pylint: disable=not-callable</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">else</span> self._values)</span><br><span class="line"></span><br><span class="line">      self._batch_size = (</span><br><span class="line"></span><br><span class="line">          self._keys.shape[<span class="number">0</span>].value <span class="keyword">or</span> array_ops.shape(self._keys)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">      self._alignments_size = (self._keys.shape[<span class="number">1</span>].value <span class="keyword">or</span></span><br><span class="line"></span><br><span class="line">                               array_ops.shape(self._keys)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">memory_layer</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._memory_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">query_layer</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._query_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">values</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">keys</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._keys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">batch_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alignments_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._alignments_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._alignments_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initial_alignments</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates the initial alignment values for the `AttentionWrapper` class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is important for AttentionMechanisms that use the previous alignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    to calculate the alignment at the next time step (e.g. monotonic attention).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The default behavior is to return a tensor of all zeros.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: `int32` scalar, the batch_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The `dtype`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A `dtype` tensor shaped `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is the values&#x27; `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    max_time = self._alignments_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> _zero_state_tensors(max_time, batch_size, dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initial_state</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates the initial state values for the `AttentionWrapper` class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is important for AttentionMechanisms that use the previous alignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    to calculate the alignment at the next time step (e.g. monotonic attention).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The default behavior is to return the same output as initial_alignments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: `int32` scalar, the batch_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The `dtype`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A structure of all-zero tensors with shapes as described by `state_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.initial_alignments(batch_size, dtype)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>这个类 _BaseAttentionMechanism 是最基本的 attention 类了。可以看到 self._keys 和 self._values 的计算方式都是需要考虑 memory_sequence_length 这个参数的。</p>
<p>有这几个属性：    </p>
<ul>
<li><p>values: 其计算使用了 _prepare_memory 函数对应的是把输入序列 memory 的超过对应实际长度的部分的值变为 0    </p>
</li>
<li><p>keys： self._keys = self.memory_layer(self._values)  是在得到了 values 之后进行全链接的值，其shape=[batch, max_times, num_units]  </p>
</li>
<li><p>state_size 和 alignment_size 是一样的，都是 max_times  </p>
</li>
<li><p>self._probability_fn(score, prev) 使用了 _maybe_mask_score 这个函数计算得到 score 之后并 mask 的概率，然后还要利用 prev state?</p>
</li>
</ul>
<h4 id="maybe-mask-score"><a href="#maybe-mask-score" class="headerlink" title="_maybe_mask_score"></a>_maybe_mask_score</h4><p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_maybe_mask_score</span>(<span class="params">score, memory_sequence_length, score_mask_value</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">  message = (<span class="string">&quot;All values in memory_sequence_length must greater than zero.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> ops.control_dependencies(</span><br><span class="line"></span><br><span class="line">      [check_ops.assert_positive(memory_sequence_length, message=message)]):</span><br><span class="line"></span><br><span class="line">    score_mask = array_ops.sequence_mask(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length, maxlen=array_ops.shape(score)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    score_mask_values = score_mask_value * array_ops.ones_like(score)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> array_ops.where(score_mask, score, score_mask_values)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">score = tf.random_uniform(shape=[<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">tf.shape(score).numpy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>array([ 2, 10], dtype=int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">score = tf.random_uniform(shape=[<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">memeory_sequence_len = [<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">score_mask_value = -<span class="number">100000000</span></span><br><span class="line"></span><br><span class="line">score_mask = tf.sequence_mask(lengths=memeory_sequence_len, maxlen=tf.shape(score)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;true or false: %s\n&quot;</span> %score_mask)</span><br><span class="line"></span><br><span class="line">score_mask_values = score_mask_value * tf.ones_like(score)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-inf: %s\n&quot;</span>%score_mask_values)</span><br><span class="line"></span><br><span class="line">ans = tf.where(score_mask, score, score_mask_values)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ans)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>true or false: tf.Tensor(

[[ True  True  True  True  True False False False False False]

 [ True  True  True  True  True  True  True  True False False]], shape=(2, 10), dtype=bool)



-inf: tf.Tensor(

[[-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08

  -1.e+08]

 [-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08

  -1.e+08]], shape=(2, 10), dtype=float32)



tf.Tensor(

[[ 2.3987615e-01  4.9896538e-01  7.2822869e-01  4.7516704e-02

   1.6099060e-01 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08

  -1.0000000e+08 -1.0000000e+08]

 [ 3.5503960e-01  2.5502288e-01  8.1264114e-01  4.3110681e-01

   1.1858845e-01  2.5748730e-02  4.8437893e-01  2.8339624e-02

  -1.0000000e+08 -1.0000000e+08]], shape=(2, 10), dtype=float32)
</code></pre>
<h4 id="prepare-memory"><a href="#prepare-memory" class="headerlink" title="_prepare_memory\"></a>_prepare_memory\</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">self._keys = _prepare_memory(memory, memory_sequence_length,</span><br><span class="line"></span><br><span class="line">check_inner_dims_defined=check_inner_dims_defined)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>其中 _prepare_memory 这个函数,也就是怎么计算 mask 的，其计算如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prepare_memory</span>(<span class="params">memory, memory_sequence_length, check_inner_dims_defined</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Convert to tensor and possibly mask `memory`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    memory: `Tensor`, shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    check_inner_dims_defined: Python boolean.  If `True`, the `memory`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      argument&#x27;s shape is checked to ensure all but the two outermost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dimensions are fully defined.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    A (possibly masked), checked, new `memory`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ValueError: If `check_inner_dims_defined` is `True` and not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `memory.shape[2:].is_fully_defined()`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  memory = nest.map_structure(</span><br><span class="line"></span><br><span class="line">      <span class="keyword">lambda</span> m: ops.convert_to_tensor(m, name=<span class="string">&quot;memory&quot;</span>), memory)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    memory_sequence_length = ops.convert_to_tensor(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length, name=<span class="string">&quot;memory_sequence_length&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> check_inner_dims_defined:</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_dims</span>(<span class="params">m</span>):</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> m.get_shape()[<span class="number">2</span>:].is_fully_defined():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected memory %s to have fully defined inner dims, &quot;</span></span><br><span class="line"></span><br><span class="line">                         <span class="string">&quot;but saw shape: %s&quot;</span> % (m.name, m.get_shape()))</span><br><span class="line"></span><br><span class="line">    nest.map_structure(_check_dims, memory)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    seq_len_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">    seq_len_mask = array_ops.sequence_mask(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        maxlen=array_ops.shape(nest.flatten(memory)[<span class="number">0</span>])[<span class="number">1</span>],</span><br><span class="line"></span><br><span class="line">        dtype=nest.flatten(memory)[<span class="number">0</span>].dtype)</span><br><span class="line"></span><br><span class="line">    seq_len_batch_size = (</span><br><span class="line"></span><br><span class="line">        memory_sequence_length.shape[<span class="number">0</span>].value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">or</span> array_ops.shape(memory_sequence_length)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_maybe_mask</span>(<span class="params">m, seq_len_mask</span>):</span></span><br><span class="line"></span><br><span class="line">    rank = m.get_shape().ndims</span><br><span class="line"></span><br><span class="line">    rank = rank <span class="keyword">if</span> rank <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> array_ops.rank(m)</span><br><span class="line"></span><br><span class="line">    extra_ones = array_ops.ones(rank - <span class="number">2</span>, dtype=dtypes.int32)</span><br><span class="line"></span><br><span class="line">    m_batch_size = m.shape[<span class="number">0</span>].value <span class="keyword">or</span> array_ops.shape(m)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      message = (<span class="string">&quot;memory_sequence_length and memory tensor batch sizes do not &quot;</span></span><br><span class="line"></span><br><span class="line">                 <span class="string">&quot;match.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> ops.control_dependencies([</span><br><span class="line"></span><br><span class="line">          check_ops.assert_equal(</span><br><span class="line"></span><br><span class="line">              seq_len_batch_size, m_batch_size, message=message)]):</span><br><span class="line"></span><br><span class="line">        seq_len_mask = array_ops.reshape(</span><br><span class="line"></span><br><span class="line">            seq_len_mask,</span><br><span class="line"></span><br><span class="line">            array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> m * seq_len_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> nest.map_structure(<span class="keyword">lambda</span> m: _maybe_mask(m, seq_len_mask), memory)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>_prepare_memory 其实很简单，就是根据 batch 中每个样本的实际长度，将超出部分设置为 0</p>
<h3 id="tf-contrib-seq2seq-BahdanauAttention"><a href="#tf-contrib-seq2seq-BahdanauAttention" class="headerlink" title="tf.contrib.seq2seq.BahdanauAttention"></a>tf.contrib.seq2seq.BahdanauAttention</h3><p>这里涉及到了两篇 paper:  </p>
<ul>
<li><p>[Neural Machine Translation by Jointly Learning to Align and Translate.”</p>
<p>ICLR 2015. ](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>)  </p>
</li>
<li><p>[Weight Normalization: A Simple Reparameterization to Accelerate</p>
<p> Training of Deep Neural Networks.”](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a>)</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span>(<span class="params">_BaseAttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Bahdanau-style (additive) attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This attention has two forms.  The first is Bahdanau attention,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  as described in:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  ICLR 2015. https://arxiv.org/abs/1409.0473</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The second is the normalized form.  This form is inspired by the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  weight normalization article:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Tim Salimans, Diederik P. Kingma.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Weight Normalization: A Simple Reparameterization to Accelerate</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Training of Deep Neural Networks.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1602.07868</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  To enable the second form, construct the object with parameter</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  `normalize=True`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               normalize=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="string">&quot;BahdanauAttention&quot;</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the Attention mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: The depth of the query mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory: The memory to query; usually the output of an RNN encoder.  This</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        tensor should be shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_sequence_length (optional): Sequence lengths for the batch entries</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in memory.  If provided, the memory tensor rows are masked with zeros</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for values past the respective sequence lengths.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      normalize: Python boolean.  Whether to normalize the energy term.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      probability_fn: (optional) A `callable`.  Converts the score to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probabilities.  The default is @&#123;tf.nn.softmax&#125;. Other options include</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @&#123;tf.contrib.seq2seq.hardmax&#125; and @&#123;tf.contrib.sparsemax.sparsemax&#125;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Its signature should be: `probabilities = probability_fn(score)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      score_mask_value: (optional): The mask value for score before passing into</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probability_fn`. The default is -inf. Only used if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `memory_sequence_length` is not None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The data type for the query and memory layers of the attention</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: Name to use when creating ops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>num_units 是query mechanism 的维度. 它可以既不是 query 的维度,也可以不是 memory 的维度对吧?</p>
</li>
<li><p>query 的维度要和 memory(也就是 keys/values) 的维度一致吗?是不需要的.在 BahdanauAttention 的实现中比较好理解,两个全链接最后的维度一致即可相加.但是在 LuongAttention 中矩阵矩阵相乘时需要注意维度变化.  </p>
</li>
<li><p>memory_sequence_length: 这个参数很重要, mask 消除 padding 的影响.  </p>
</li>
<li><p>score_mask_value: 上一个参数存在时,这个参数才会使用,默认为 -inf.</p>
</li>
</ul>
<p>继续看源码的实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> probability_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">  probability_fn = nn_ops.softmax</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">  dtype = dtypes.float32</span><br><span class="line"></span><br><span class="line">wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line"><span class="built_in">super</span>(BahdanauAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">    query_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">        num_units, name=<span class="string">&quot;query_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">    memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">        num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">    memory=memory,</span><br><span class="line"></span><br><span class="line">    probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">    memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">    score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">    name=name)</span><br><span class="line"></span><br><span class="line">self._num_units = num_units</span><br><span class="line"></span><br><span class="line">self._normalize = normalize</span><br><span class="line"></span><br><span class="line">self._name = name</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>现在理解了 _BaseAttentionMechanism 这个类中 query_layer 和 memory_layer 的意义了.  </p>
</li>
<li><p>score_mask_value 沿用父类中的计算方式.  </p>
</li>
</ul>
<p>继续看 call 函数,也就是 attention 的计算方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, query, state</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Score the query based on the keys and values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, query_depth]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    state: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is memory&#x27;s `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    alignments: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]` (`alignments_size` is memory&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> variable_scope.variable_scope(<span class="literal">None</span>, <span class="string">&quot;bahdanau_attention&quot;</span>, [query]):</span><br><span class="line"></span><br><span class="line">    processed_query = self.query_layer(query) <span class="keyword">if</span> self.query_layer <span class="keyword">else</span> query</span><br><span class="line"></span><br><span class="line">    score = _bahdanau_score(processed_query, self._keys, self._normalize)</span><br><span class="line"></span><br><span class="line">  alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line">  next_state = alignments</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> alignments, next_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>然后看怎么计算的 score.  </p>
<p>score = _bahdanau_score(processed_query, self._keys, self._normalize) 其中</p>
<p>processed_query 和 self._keys 都是通过全链接层后得到的, [batch, alignments_size, num_units]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bahdanau_score</span>(<span class="params">processed_query, keys, normalize</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Bahdanau-style (additive) scoring function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  dtype = processed_query.dtype</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Get the number of hidden units from the trailing dimension of keys</span></span><br><span class="line"></span><br><span class="line">  num_units = keys.shape[<span class="number">2</span>].value <span class="keyword">or</span> array_ops.shape(keys)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.</span></span><br><span class="line"></span><br><span class="line">  processed_query = array_ops.expand_dims(processed_query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  v = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">      <span class="string">&quot;attention_v&quot;</span>, [num_units], dtype=dtype)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> normalize:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar used in weight normalization</span></span><br><span class="line"></span><br><span class="line">    g = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_g&quot;</span>, dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.constant_initializer(math.sqrt((<span class="number">1.</span> / num_units))),</span><br><span class="line"></span><br><span class="line">        shape=())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Bias added prior to the nonlinearity</span></span><br><span class="line"></span><br><span class="line">    b = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_b&quot;</span>, [num_units], dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># normed_v = g * v / ||v||</span></span><br><span class="line"></span><br><span class="line">    normed_v = g * v * math_ops.rsqrt(</span><br><span class="line"></span><br><span class="line">        math_ops.reduce_sum(math_ops.square(v)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> math_ops.reduce_sum(</span><br><span class="line"></span><br><span class="line">        normed_v * math_ops.tanh(keys + processed_query + b), [<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [<span class="number">2</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>源码中计算 score 的最后一步不是全链接，而是这样的：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">v = tf.get_variable(<span class="string">&quot;attention_v&quot;</span>, [num_units])</span><br><span class="line"></span><br><span class="line">score = tf.reduce_sum(v * tanh(keys + processed_query), [<span class="number">2</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tfe.executing_eagerly())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">memory = tf.ones(shape=[<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>]) <span class="comment"># batch=1, max_sequence_len=10, embed_size=5</span></span><br><span class="line"></span><br><span class="line">memory_sequence_len = [<span class="number">5</span>]                   <span class="comment"># 有效长度为 5</span></span><br><span class="line"></span><br><span class="line">attention_mechnism = tf.contrib.seq2seq.BahdanauAttention(num_units=<span class="number">32</span>, memory=memory,</span><br><span class="line"></span><br><span class="line">                                                          memory_sequence_length=memory_sequence_len)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.state_size, attention_mechnism.alignments_size)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>10 10
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">memory</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&lt;tf.Tensor: id=3, shape=(1, 10, 5), dtype=float32, numpy=

array([[[1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.]]], dtype=float32)&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">attention_mechnism.values   <span class="comment"># 可以发现 values 就是把 memory 中超过memory_sequence_length 的部分变为 0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&lt;tf.Tensor: id=30, shape=(1, 10, 5), dtype=float32, numpy=

array([[[1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.]]], dtype=float32)&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.keys.shape)  <span class="comment"># 经过了全链接之后的</span></span><br><span class="line"></span><br><span class="line">attention_mechnism.keys.numpy()[<span class="number">0</span>,<span class="number">1</span>,:]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(1, 10, 32)



array([ 0.09100786,  0.18448338, -0.7751561 ,  0.00775184,  0.467805  ,

        0.9172474 ,  0.57645243, -0.3915946 , -0.22213435,  0.76866853,

        0.3591721 ,  0.8922573 ,  0.15866229,  0.6033571 ,  0.51816225,

        0.3820553 , -0.39130217,  0.04532939, -0.02089322,  0.6878175 ,

       -0.28697258,  0.59283376, -0.37825382, -0.5865691 ,  0.17466056,

       -0.5915747 ,  0.6070496 , -0.18531135, -0.821724  ,  1.2838829 ,

        0.15700272, -0.2608306 ], dtype=float32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.query_layer, attention_mechnism.memory_layer)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&lt;tensorflow.python.layers.core.Dense object at 0x7fa0464da908&gt; &lt;tensorflow.python.layers.core.Dense object at 0x7fa0464dab38&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 利用 call 函数来计算下一个 state 和 attention vector</span></span><br><span class="line"></span><br><span class="line">query = tf.ones(shape=[<span class="number">1</span>, <span class="number">8</span>])  <span class="comment"># query_depth = 10</span></span><br><span class="line"></span><br><span class="line">state_h0 = attention_mechnism.initial_alignments(batch_size=<span class="number">1</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">attention_vector = attention_mechnism(query=query, state=state_h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_vector)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(&lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;, &lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;)
</code></pre>
<h3 id="tf-contrib-seq2seq-LuongAttention"><a href="#tf-contrib-seq2seq-LuongAttention" class="headerlink" title="tf.contrib.seq2seq.LuongAttention"></a>tf.contrib.seq2seq.LuongAttention</h3><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015.</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LuongAttention</span>(<span class="params">_BaseAttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Luong-style (multiplicative) attention scoring.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               scale=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="string">&quot;LuongAttention&quot;</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> probability_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      probability_fn = nn_ops.softmax</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      dtype = dtypes.float32</span><br><span class="line"></span><br><span class="line">    wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LuongAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._scale = scale</span><br><span class="line"></span><br><span class="line">    self._name = name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以发现 query 没有经过 query_layer 的处理，也就是没有全链接。但是 memory 还是要用全链接处理的，得到 <code>[batch, max_times, num_units]</code></p>
<p>再看使用 call 函数计算对其概率 alignment 和 next_state.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, query, state</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Score the query based on the keys and values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, query_depth]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    state: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is memory&#x27;s `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    alignments: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]` (`alignments_size` is memory&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> variable_scope.variable_scope(<span class="literal">None</span>, <span class="string">&quot;luong_attention&quot;</span>, [query]):</span><br><span class="line"></span><br><span class="line">    score = _luong_score(query, self._keys, self._scale)</span><br><span class="line"></span><br><span class="line">  alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line">  next_state = alignments</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> alignments, next_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>接下来看怎么计算的 score</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_luong_score</span>(<span class="params">query, keys, scale</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Luong-style (multiplicative) scoring function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor, shape `[batch_size, num_units]` to compare to keys.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    keys: Processed memory, shape `[batch_size, max_time, num_units]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    scale: Whether to apply a scale to the score function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    A `[batch_size, max_time]` tensor of unnormalized score values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ValueError: If `key` and `query` depths do not match.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  depth = query.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  key_units = keys.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> depth != key_units:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Incompatible or unknown inner dimensions between query and keys.  &quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Query (%s) has units: %s.  Keys (%s) have units: %s.  &quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Perhaps you need to set num_units to the keys&#x27; dimension (%s)?&quot;</span></span><br><span class="line"></span><br><span class="line">        % (query, depth, keys, key_units, key_units))</span><br><span class="line"></span><br><span class="line">  dtype = query.dtype</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Reshape from [batch_size, depth] to [batch_size, 1, depth]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># for matmul.</span></span><br><span class="line"></span><br><span class="line">  query = array_ops.expand_dims(query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Inner product along the query units dimension.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># matmul shapes: query is [batch_size, 1, depth] and</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#                keys is [batch_size, max_time, depth].</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># the inner product is asked to **transpose keys&#x27; inner shape** to get a</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># batched matmul on:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#   [batch_size, 1, depth] . [batch_size, depth, max_time]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># resulting in an output shape of:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#   [batch_size, 1, max_time].</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># we then squeeze out the center singleton dimension.</span></span><br><span class="line"></span><br><span class="line">  score = math_ops.matmul(query, keys, transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  score = array_ops.squeeze(score, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar used in weight scaling</span></span><br><span class="line"></span><br><span class="line">    g = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_g&quot;</span>, dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.ones_initializer, shape=())</span><br><span class="line"></span><br><span class="line">    score = g * score</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>通过源码可以发现 <code>LuongAttention</code> 调用 call 函数时，其 query 的维度必须是 num_units. 而 BahdanauAttention 并不需要。</p>
<p>其是计算 score 的方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">query_depth = num_units = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">memory_depth = <span class="number">15</span></span><br><span class="line"></span><br><span class="line">max_times = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">embed_size = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">scale = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.random_normal(shape=[batch_size, num_units])</span><br><span class="line"></span><br><span class="line"><span class="comment"># memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># values = self._prepaer_memory(memory)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># keys = memory_layer(values)</span></span><br><span class="line"></span><br><span class="line">values = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span><br><span class="line"></span><br><span class="line">keys = tf.layers.dense(inputs=values, units=num_units)    <span class="comment"># [batch, max_times, num_units]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.expand_dims(query, axis=<span class="number">1</span>)                     <span class="comment"># [batch, 1, num_units]</span></span><br><span class="line"></span><br><span class="line">score = tf.matmul(query, keys, transpose_b=<span class="literal">True</span>)          <span class="comment"># [batch, 1, max_times]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">score = tf.squeeze(score, axis=<span class="number">1</span>)   <span class="comment"># [batch, max_times]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(score.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(2, 10)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 完整的过一遍</span></span><br><span class="line"></span><br><span class="line">memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span><br><span class="line"></span><br><span class="line">memory_sequence_len = [<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">query_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.random_normal(shape=[batch_size, num_units])</span><br><span class="line"></span><br><span class="line">state = tf.zeros(shape=[batch_size, max_times])</span><br><span class="line"></span><br><span class="line">attention_mechnism = tf.contrib.seq2seq.LuongAttention(num_units=num_units,</span><br><span class="line"></span><br><span class="line">                                                       memory=memory,</span><br><span class="line"></span><br><span class="line">                                                       memory_sequence_length=memory_sequence_len)</span><br><span class="line"></span><br><span class="line">attention_vector = attention_mechnism(query, state)</span><br><span class="line"></span><br><span class="line">attention_vector[<span class="number">0</span>], attention_vector[<span class="number">1</span>]   <span class="comment"># attention_vector 和 state</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(&lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy=

 array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02,

         6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,

         0.0000000e+00, 0.0000000e+00],

        [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06,

         3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04,

         0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;,

 &lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy=

 array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02,

         6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,

         0.0000000e+00, 0.0000000e+00],

        [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06,

         3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04,

         0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.reduce_sum(attention_vector[<span class="number">0</span>][<span class="number">1</span>]).numpy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>1.0
</code></pre>
<p>这只是针对单个 query 的情况，但实际上 query 一般是这样的 [batch, query_len, num_units]，那怎么办呢？</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="最后总结一下"><a href="#最后总结一下" class="headerlink" title="最后总结一下"></a>最后总结一下</h3><p>再看一遍两个 attention 初始化的差异</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">super</span>(BahdanauAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;query_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">super</span>(LuongAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>作为一个类对象时，<code>AttentionMechanism</code>，<code>BahdanauAttention</code>，<code>LuongAttention</code>它们具有如下属性：</p>
<ul>
<li><p>query_layer: 在 BahdanauAttention 中一般是 tf.layer.dense 的实例对象，其维度是 num_units. 所以 BahdanauAttention 中 query 的维度可以是任意值。而 LuongAttention 中 query_layer 为 None，所以 query 的维度只能是 num_units.  </p>
</li>
<li><p>memory_layer: 在两个 attention 中都是一样的，tf.layer.dense,且维度为 num_units.  </p>
</li>
<li><p>alignments_size: 对齐size，是 memory 的 max_times.  </p>
</li>
<li><p>batch_size: 批量大小  </p>
</li>
<li><p>values: 是经过 mask 处理后的 memory. [batch, max_times, embed_size]  </p>
</li>
<li><p>keys: 是经过 memory_layer 全链接处理后的。 [batch, max_times, num_units].</p>
</li>
<li><p>state_size: 等于 alignment_size.</p>
</li>
</ul>
<p>然后是对应的方法：  </p>
<p><strong><strong>init</strong>:</strong> 初始化类实例，里面的参数：  </p>
<ul>
<li><p>num_units: 在　Bahdanau 中这个参数其实是个中间值，将 query 和 keys 转化为这个维度，叠加，但最后还是要在这个维度上　reduce_sum． 但是在　LuongAttention 中它必须和 query 的维度一致，然后和 memory_layer 处理后的 memory 做矩阵相乘。  </p>
</li>
<li><p>memory: [batch, max_times, embed_size]  </p>
</li>
<li><p>normalize: 是佛有归一化  </p>
</li>
<li><p>probability_fn: <code>tf.nn.softmax</code>，<code>tf.contrib.seq2seq.hardmax</code>，<code> tf.contrib.sparsemax.sparsemax</code>  </p>
</li>
<li><p>memory_sequence_length： 没有经过 padding 时 memory 的长度。其维度应该是 [1, batch_size]  </p>
</li>
</ul>
<p><strong>call(query, state)</strong> 调用该实例  </p>
<ul>
<li><p>query: [batch_size, query_length]. 在 LuongAttention 中 query_length 必须等于 num_units.  </p>
</li>
<li><p>state: [batch_size, alignments_size].  </p>
</li>
</ul>
<p>一直不太理解 state 有啥用？在源码中是用来计算 alignments 的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self._probability_fn = <span class="keyword">lambda</span> score, prev: (  <span class="comment"># pylint:disable=g-long-lambda</span></span><br><span class="line"></span><br><span class="line">        probability_fn(</span><br><span class="line"></span><br><span class="line">            _maybe_mask_score(score, memory_sequence_length, score_mask_value),</span><br><span class="line"></span><br><span class="line">            prev))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中 score 是可能需要 mask 的. probability_fn 是 tf.nn.softmax. 所以呢？？？？不需要 prev 啊？</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后发现确实不需要啊。。。一步步往上找</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">probability_fn=wrapped_probability_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><strong>initial_alignments(batch_size, dtype)</strong> 初始化对齐  </p>
<p>Args:  </p>
<ul>
<li><p>batch_size: int32 scalar, the batch_size.  </p>
</li>
<li><p>dtype: The dtype.  </p>
</li>
</ul>
<p>Returns:  </p>
<ul>
<li>A dtype tensor shaped [batch_size, alignments_size]  </li>
</ul>
<p><strong>initial_state(batch_size, dtype)：</strong>  </p>
<p>Creates the initial state values for the AttentionWrapper class.</p>
<ul>
<li>batch_size: int32.   </li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/13/">上一页</a></div><div class="pagination-next"><a href="/page/15/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/13/">13</a></li><li><a class="pagination-link is-current" href="/page/14/">14</a></li><li><a class="pagination-link" href="/page/15/">15</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/24/">24</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>