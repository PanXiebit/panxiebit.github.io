<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>标签: 文本分类 - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">文本分类</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-07T01:28:38.000Z" title="2018/8/7 上午9:28:38">2018-08-07</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.537Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">43 分钟读完 (大约6494个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/">机器学习-中文文本预处理</a></h1><div class="content"><h3 id="中文文本挖掘预处理特点"><a href="#中文文本挖掘预处理特点" class="headerlink" title="中文文本挖掘预处理特点"></a>中文文本挖掘预处理特点</h3><p>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6744056.html">https://www.cnblogs.com/pinard/p/6744056.html</a></p>
<p>首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。  </p>
<p>首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6677078.html">文本挖掘的分词原理</a>中，我们已经讲到了中文的分词原理，这里就不多说。</p>
<p>第二，中文的编码不是utf8，而是unicode。这样会导致在分词的时候，和英文相比，我们要处理编码的问题。</p>
<p>这两点构成了中文分词相比英文分词的一些不同点，后面我们也会重点讲述这部分的处理。当然，英文分词也有自己的烦恼，这个我们在以后再讲。了解了中文预处理的一些特点后，我们就言归正传，通过实践总结下中文文本挖掘预处理流程。</p>
<h3 id="数据集收集"><a href="#数据集收集" class="headerlink" title="数据集收集"></a>数据集收集</h3><p>在文本挖掘之前，我们需要得到文本数据，文本数据的获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。</p>
<p>对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“机器学习”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。</p>
<p>对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用<a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/">beautifulsoup</a>。但是我们我们需要某些特殊的语料数据，比如上面提到的“机器学习”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成。这个我一般使用<a target="_blank" rel="noopener" href="https://github.com/ViDA-NYU/ache">ache</a>。 ache允许我们用关键字或者一个分类算法来过滤出我们需要的主题语料，比较强大。</p>
<h3 id="除去数据中非文本部分"><a href="#除去数据中非文本部分" class="headerlink" title="除去数据中非文本部分"></a>除去数据中非文本部分</h3><p>这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用beautifulsoup来去除。去除掉这些非文本的内容后，我们就可以进行真正的文本预处理了。</p>
<h3 id="处理中文编码问题"><a href="#处理中文编码问题" class="headerlink" title="处理中文编码问题"></a>处理中文编码问题</h3><p>由于Python2不支持unicode的处理，因此我们使用Python2做中文文本预处理时需要遵循的原则是，存储数据都用utf8，读出来进行中文相关处理时，使用GBK之类的中文编码，在下面一节的分词时，我们再用例子说明这个问题。</p>
<h3 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h3><p>常用的中文分词软件有很多，个人比较推荐结巴分词。安装也很简单，比如基于Python的，用”pip install jieba”就可以完成。下面我们就用例子来看看如何中文分词。</p>
<p>首先我们准备了两段文本，这两段文本在两个文件中。两段文本的内容分别是nlp_test0.txt和nlp_test2.txt：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test1.txt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line"></span><br><span class="line">    document = f.read() <span class="comment"># 如果是python2，则需要用 decode(&quot;GBK&quot;)</span></span><br><span class="line"></span><br><span class="line">    document_cut = jieba.cut(document)</span><br><span class="line"></span><br><span class="line">document_cut</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>&lt;generator object Tokenizer.cut at 0x7f6a84cf09e8&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">result = <span class="string">&quot; &quot;</span>.join(document_cut)</span><br><span class="line"></span><br><span class="line">result</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>Building prefix dict from the default dictionary ...

Loading model from cache /tmp/jieba.cache

Loading model cost 0.438 seconds.

Prefix dict has been built succesfully.











&#39;        沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \n \n         沙 瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王 大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王 大路 就 在 京州帝 豪园 买 了 三套 别墅 ， 可是 李达 康和易 学习 都 不要 ， 这些 房子 都 在 王 大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f2:</span><br><span class="line"></span><br><span class="line">    f2.write(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以发现对于一些人名和地名，jieba处理的不好，不过我们可以帮jieba加入词汇如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;沙瑞金&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;易学习&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;王大路&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">jieba.suggest_freq(<span class="string">&#x27;京州&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>3
</code></pre>
<p>所以在很多 NLP 任务中先做命令实体识别的意义就在这里对吧?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test1.txt&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f1:</span><br><span class="line"></span><br><span class="line">    text = f1.read()</span><br><span class="line"></span><br><span class="line">    text_cut = jieba.cut(text)  <span class="comment"># list</span></span><br><span class="line"></span><br><span class="line">    result = <span class="string">&quot; &quot;</span>.join(text_cut)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./nlp_test2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f2:</span><br><span class="line"></span><br><span class="line">        f2.write(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>        沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。



         沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。
</code></pre>
<h3 id="引入停用词"><a href="#引入停用词" class="headerlink" title="引入停用词"></a>引入停用词</h3><p>在上面我们解析的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个，<a target="_blank" rel="noopener" href="http://files.cnblogs.com/files/pinard/stop_words.zip">下载地址在这</a>。当然也有其他版本的停用词表，不过这个1208词版是我常用的。</p>
<p>在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">stpword_path = <span class="string">&quot;stop_words.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(stpword_path, encoding=<span class="string">&quot;gbk&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line"></span><br><span class="line">    stpword_content = f.read()</span><br><span class="line"></span><br><span class="line">    stpword_list = stpword_content.splitlines()</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(stpword_list[:<span class="number">100</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;,&#39;, &#39;?&#39;, &#39;、&#39;, &#39;。&#39;, &#39;“&#39;, &#39;”&#39;, &#39;《&#39;, &#39;》&#39;, &#39;！&#39;, &#39;，&#39;, &#39;：&#39;, &#39;；&#39;, &#39;？&#39;, &#39;人民&#39;, &#39;末##末&#39;, &#39;啊&#39;, &#39;阿&#39;, &#39;哎&#39;, &#39;哎呀&#39;, &#39;哎哟&#39;, &#39;唉&#39;, &#39;俺&#39;, &#39;俺们&#39;, &#39;按&#39;, &#39;按照&#39;, &#39;吧&#39;, &#39;吧哒&#39;, &#39;把&#39;, &#39;罢了&#39;, &#39;被&#39;, &#39;本&#39;, &#39;本着&#39;, &#39;比&#39;, &#39;比方&#39;, &#39;比如&#39;, &#39;鄙人&#39;, &#39;彼&#39;, &#39;彼此&#39;, &#39;边&#39;, &#39;别&#39;, &#39;别的&#39;, &#39;别说&#39;, &#39;并&#39;, &#39;并且&#39;, &#39;不比&#39;, &#39;不成&#39;, &#39;不单&#39;, &#39;不但&#39;, &#39;不独&#39;, &#39;不管&#39;, &#39;不光&#39;, &#39;不过&#39;, &#39;不仅&#39;, &#39;不拘&#39;, &#39;不论&#39;, &#39;不怕&#39;, &#39;不然&#39;, &#39;不如&#39;, &#39;不特&#39;, &#39;不惟&#39;, &#39;不问&#39;, &#39;不只&#39;, &#39;朝&#39;, &#39;朝着&#39;, &#39;趁&#39;, &#39;趁着&#39;, &#39;乘&#39;, &#39;冲&#39;, &#39;除&#39;, &#39;除此之外&#39;, &#39;除非&#39;, &#39;除了&#39;, &#39;此&#39;, &#39;此间&#39;, &#39;此外&#39;, &#39;从&#39;, &#39;从而&#39;, &#39;打&#39;, &#39;待&#39;, &#39;但&#39;, &#39;但是&#39;, &#39;当&#39;, &#39;当着&#39;, &#39;到&#39;, &#39;得&#39;, &#39;的&#39;, &#39;的话&#39;, &#39;等&#39;, &#39;等等&#39;, &#39;地&#39;, &#39;第&#39;, &#39;叮咚&#39;, &#39;对&#39;, &#39;对于&#39;, &#39;多&#39;, &#39;多少&#39;, &#39;而&#39;, &#39;而况&#39;, &#39;而且&#39;, &#39;而是&#39;]
</code></pre>
<h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><p>现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick中</a>，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6693230.html">文本挖掘预处理之TF-IDF</a>中，我们也讲到了TF-IDF特征处理的方法。这里我们就用scikit-learn的TfidfVectorizer类来进行TF-IDF特征处理。</p>
<h4 id="向量化与-Hash-Trick"><a href="#向量化与-Hash-Trick" class="headerlink" title="向量化与 Hash Trick"></a>向量化与 Hash Trick</h4><h5 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h5><p>在讲向量化与Hash Trick之前，我们先说说词袋模型(Bag of Words,简称BoW)。词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。</p>
<p>词袋模型首先会进行分词，在分词之后，通过统计每个词在文本中出现的次数，我们就可以得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就是我们常说的向量化。向量化完毕后一般也会使用TF-IDF进行特征的权重修正，再将特征进行标准化。 再进行一些其他的特征工程后，就可以将数据带入机器学习算法进行分类聚类了。</p>
<p>总结下词袋模型的三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。</p>
<p>词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。</p>
<h5 id="词袋模型之向量化"><a href="#词袋模型之向量化" class="headerlink" title="词袋模型之向量化"></a>词袋模型之向量化</h5><p>在词袋模型的统计词频这一步，我们会得到该文本中所有词的词频，有了词频，我们就可以用词向量表示这个文本。这里我们举一个例子，例子直接用scikit-learn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">corpus=[<span class="string">&quot;I come to China to travel&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;This is a car polupar in China&quot;</span>,          </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;I love tea and Apple &quot;</span>,   </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;The work is to write some papers in science&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer.fit_transform(corpus))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 16)    1

  (0, 3)    1

  (0, 15)    2

  (0, 4)    1

  (1, 5)    1

  (1, 9)    1

  (1, 2)    1

  (1, 6)    1

  (1, 14)    1

  (1, 3)    1

  (2, 1)    1

  (2, 0)    1

  (2, 12)    1

  (2, 7)    1

  (3, 10)    1

  (3, 8)    1

  (3, 11)    1

  (3, 18)    1

  (3, 17)    1

  (3, 13)    1

  (3, 5)    1

  (3, 6)    1

  (3, 15)    1
</code></pre>
<p>可以看出4个文本的词频已经统计出，在输出中，左边的括号中的第一个数字是文本的序号，第2个数字是词的序号，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。</p>
<p>我们可以进一步看看每个文本的词向量特征和各个特征代表的词，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer.fit_transform(corpus).toarray())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]

 [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]

 [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]

 [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer.get_feature_names())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;and&#39;, &#39;apple&#39;, &#39;car&#39;, &#39;china&#39;, &#39;come&#39;, &#39;in&#39;, &#39;is&#39;, &#39;love&#39;, &#39;papers&#39;, &#39;polupar&#39;, &#39;science&#39;, &#39;some&#39;, &#39;tea&#39;, &#39;the&#39;, &#39;this&#39;, &#39;to&#39;, &#39;travel&#39;, &#39;work&#39;, &#39;write&#39;]
</code></pre>
<p>也就是先统计整个文本corpus, 去掉停用词，剩下的词就是向量的维度。然后统计每一行文字出现的词频，得到相应的向量。显然词表是按照字母顺序排序的。</p>
<p>可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词”I”在英文中是停用词，不参加词频的统计。</p>
<p>由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。</p>
<blockquote>
<p><strong>这里有个疑问？</strong> 向量化之后的维度是根据自己的数据集来定，为什么不就是词表大小呢。这里是根据自己的数据集来的，但我们对测试集分类时，会出现 UNK 词吧，但是这个词其实在词表中是有的。那么在训练集中如果加上这个维度，其实也没有太大意义，因为在训练集中这个维度上所有的值都为0.</p>
</blockquote>
<p>将文本做了词频统计后，我们一般会通过TF-IDF进行词特征值修订，这部分我们后面再讲。</p>
<p>向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，达到100万+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们怎么办呢？第一反应是我们要进行特征的降维，说的没错！而Hash Trick就是非常常用的文本特征降维方法。</p>
<h5 id="Hash-Trick"><a href="#Hash-Trick" class="headerlink" title="Hash Trick"></a>Hash Trick</h5><p>在大规模的文本处理中，由于特征的维度对应分词词汇表的大小，所以维度可能非常恐怖，此时需要进行降维，不能直接用我们上一节的向量化方法。而最常用的文本降维方法是Hash Trick。说到Hash，一点也不神秘，学过数据结构的同学都知道。这里的Hash意义也类似。</p>
<p>在Hash Trick里，我们会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于我们的词汇表的特征维度，因此可以看成是降维。具体的方法是，对应任意一个特征名，我们会用Hash函数找到对应哈希表的位置，然后将该特征名对应的词频统计值累加到该哈希表位置。如果用数学语言表示,假如哈希函数h使第i个特征哈希到位置j,即 $h(i)=j$,则第i个原始特征的词频数值 $\phi(i)$ 将累加到哈希后的第j个特征的词频数值 $\hat \phi(i)$上，即：</p>
<p>$$\hat \phi(i)=\sum_{i\in J;h(i)=j}\phi(i)$$</p>
<p>其中 J 是原始特征的维度。</p>
<p>但是上面的方法有一个问题，有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这个问题，出现了hash Trick的变种signed hash trick,此时除了哈希函数h,我们多了一个一个哈希函数：</p>
<p>$$\xi:N\rightarrow \pm1$$</p>
<p>此时我们有</p>
<p>$$\hat \phi(j)=\sum_{i\in J;h(i)=j}\phi(i)\xi(i)$$</p>
<p>这样做的好处是，哈希后的特征仍然是一个无偏的估计，不会导致某些哈希位置的值过大。</p>
<p>当然，大家会有疑惑，这种方法来处理特征，哈希后的特征是否能够很好的代表哈希前的特征呢？从实际应用中说，由于文本特征的高稀疏性，这么做是可行的。如果大家对理论上为何这种方法有效，建议参考论文：<a target="_blank" rel="noopener" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Feature hashing for large scale multitask learning</a>.这里就不多说了。</p>
<p>在scikit-learn的HashingVectorizer类中，实现了基于signed hash trick的算法，这里我们就用HashingVectorizer来实践一下Hash Trick，为了简单，我们使用上面的19维词汇表，并哈希降维到6维。当然在实际应用中，19维的数据根本不需要Hash Trick，这里只是做一个演示，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> HashingVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer2 = HashingVectorizer(n_features=<span class="number">6</span>, norm=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vectorizer2.fit_transform(corpus))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 1)    2.0

  (0, 2)    -1.0

  (0, 4)    1.0

  (0, 5)    -1.0

  (1, 0)    1.0

  (1, 1)    1.0

  (1, 2)    -1.0

  (1, 5)    -1.0

  (2, 0)    2.0

  (2, 5)    -2.0

  (3, 0)    0.0

  (3, 1)    4.0

  (3, 2)    -1.0

  (3, 3)    1.0

  (3, 5)    -1.0
</code></pre>
<p>大家可以看到结果里面有负数，这是因为我们的哈希函数ξ可以哈希到1或者-1导致的。</p>
<p>和PCA类似，Hash Trick降维后的特征我们已经不知道它代表的特征名字和意义。此时我们不能像上一节向量化时候可以知道每一列的意义，所以Hash Trick的解释性不强。</p>
<h5 id="向量化与-Hash-Track-小结"><a href="#向量化与-Hash-Track-小结" class="headerlink" title="向量化与 Hash Track 小结"></a>向量化与 Hash Track 小结</h5><p>这里我们对向量化与它的特例Hash Trick做一个总结。在特征预处理的时候，我们什么时候用一般意义的向量化，什么时候用Hash Trick呢？标准也很简单。</p>
<p>一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。</p>
<p>而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。因此，实际工作中我使用的都是特征向量化。</p>
<p>向量化与Hash Trick就介绍到这里，下一篇我们讨论TF-IDF。</p>
<h4 id="文本向量化特征的不足"><a href="#文本向量化特征的不足" class="headerlink" title="文本向量化特征的不足"></a>文本向量化特征的不足</h4><p>在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，比如在文本挖掘预处理之向量化与Hash Trick这篇文章中，我们将下面4个短文本做了词频统计：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">corpus=[&quot;I come to China to travel&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;This is a car polupar in China&quot;,          </span><br><span class="line"></span><br><span class="line">    &quot;I love tea and Apple &quot;,   </span><br><span class="line"></span><br><span class="line">    &quot;The work is to write some papers in science&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>不考虑停用词，处理后得到的词向量如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]</span><br><span class="line"></span><br><span class="line"> [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]</span><br><span class="line"></span><br><span class="line"> [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]</span><br><span class="line"></span><br><span class="line"> [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现”come”,”China”和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的”China”和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。</p>
<h4 id="TF-IDF概述"><a href="#TF-IDF概述" class="headerlink" title="TF-IDF概述"></a>TF-IDF概述</h4><p>TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。</p>
<p>前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们讲到几乎所有文本都会出现的”to”其词频虽然高，但是重要性却应该比词频低的”China”和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。</p>
<p>概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。</p>
<p>上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下：</p>
<p>$$IDF(x)=\dfrac{N}{N(x)}$$</p>
<p>其中，N代表语料库中文本的总数，而 $N(x)$ 代表语料库中包含词x的文本总数。为什么IDF的基本公式应该是是上面这样的而不是像 $N/N(x)$ 这样的形式呢？这就涉及到信息论相关的一些知识了。感兴趣的朋友建议阅读吴军博士的《数学之美》第11章。</p>
<p>上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为：</p>
<p>$$IDF(x)=log\dfrac{N+1}{N(x)+1}+1$$</p>
<p>有了IDF的定义，我们就可以计算某一个词的TF-IDF值了：</p>
<p>$$\text{TF-IDF(x)}=TF(x)*IDF(x)$$</p>
<p>其中TF(x)指词x在当前文本中的词频。</p>
<h4 id="用scikit-learn进行TF-IDF预处理"><a href="#用scikit-learn进行TF-IDF预处理" class="headerlink" title="用scikit-learn进行TF-IDF预处理"></a>用scikit-learn进行TF-IDF预处理</h4><p>在scikit-learn中，有两种方法进行TF-IDF的预处理。</p>
<p>第一种方法是在用CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理。第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理。</p>
<p>首先我们来看第一种方法，CountVectorizer+TfidfTransformer的组合，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">corpus = [<span class="string">&quot;I come to China to travel&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;This is a car polupar in China&quot;</span>,          </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;I love tea and Apple &quot;</span>,   </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;The work is to write some papers in science&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line"></span><br><span class="line">tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tfidf)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 4)    0.4424621378947393

  (0, 15)    0.697684463383976

  (0, 3)    0.348842231691988

  (0, 16)    0.4424621378947393

  (1, 3)    0.3574550433419527

  (1, 14)    0.45338639737285463

  (1, 6)    0.3574550433419527

  (1, 2)    0.45338639737285463

  (1, 9)    0.45338639737285463

  (1, 5)    0.3574550433419527

  (2, 7)    0.5

  (2, 12)    0.5

  (2, 0)    0.5

  (2, 1)    0.5

  (3, 15)    0.2811316284405006

  (3, 6)    0.2811316284405006

  (3, 5)    0.2811316284405006

  (3, 13)    0.3565798233381452

  (3, 17)    0.3565798233381452

  (3, 18)    0.3565798233381452

  (3, 11)    0.3565798233381452

  (3, 8)    0.3565798233381452

  (3, 10)    0.3565798233381452
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">tfidf2 = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line">re = tfidf2.fit_transform(corpus)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(re)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>  (0, 4)    0.4424621378947393

  (0, 15)    0.697684463383976

  (0, 3)    0.348842231691988

  (0, 16)    0.4424621378947393

  (1, 3)    0.3574550433419527

  (1, 14)    0.45338639737285463

  (1, 6)    0.3574550433419527

  (1, 2)    0.45338639737285463

  (1, 9)    0.45338639737285463

  (1, 5)    0.3574550433419527

  (2, 7)    0.5

  (2, 12)    0.5

  (2, 0)    0.5

  (2, 1)    0.5

  (3, 15)    0.2811316284405006

  (3, 6)    0.2811316284405006

  (3, 5)    0.2811316284405006

  (3, 13)    0.3565798233381452

  (3, 17)    0.3565798233381452

  (3, 18)    0.3565798233381452

  (3, 11)    0.3565798233381452

  (3, 8)    0.3565798233381452

  (3, 10)    0.3565798233381452
</code></pre>
<p>输出的各个文本各个词的TF-IDF值和第一种的输出完全相同。大家可以自己去验证一下。</p>
<p>由于第二种方法比较的简洁，因此在实际应用中推荐使用，一步到位完成向量化，TF-IDF与标准化。</p>
<p>TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。</p>
<p>当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想</p>
<p><strong>还的好好理解下 TF-IDF 是怎么实现的！</strong></p>
<h3 id="建立分析模型"><a href="#建立分析模型" class="headerlink" title="建立分析模型"></a>建立分析模型</h3><p>有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。比如我们上面的两段文本，就可以是两个训练样本了。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而 <strong>主题模型</strong> 是自然语言处理比较特殊的一块，这个我们后面再单独讲。</p>
<h3 id="中文文本挖掘预处理总结"><a href="#中文文本挖掘预处理总结" class="headerlink" title="中文文本挖掘预处理总结"></a>中文文本挖掘预处理总结</h3><p>上面我们对中文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如我们涉及到词上下文关系的一些需求，此时不能使用词袋模型。而有时候我们对于特征的处理有自己的特殊需求，因此这个流程仅供自然语言处理入门者参考。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-07T01:28:38.000Z" title="2018/8/7 上午9:28:38">2018-08-07</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.634Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">34 分钟读完 (大约5038个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%8B%B1%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/">机器学习-英文文本预处理</a></h1><div class="content"><h3 id="转载自：http-www-cnblogs-com-pinard-p-6756534-html"><a href="#转载自：http-www-cnblogs-com-pinard-p-6756534-html" class="headerlink" title="转载自：http://www.cnblogs.com/pinard/p/6756534.html"></a>转载自：<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6756534.html">http://www.cnblogs.com/pinard/p/6756534.html</a></h3><h2 id="英文文本挖掘预处理特点"><a href="#英文文本挖掘预处理特点" class="headerlink" title="英文文本挖掘预处理特点"></a>英文文本挖掘预处理特点</h2><p>英文文本的预处理方法和中文的有部分区别。首先，英文文本挖掘预处理一般可以不做分词（特殊需求除外），而中文预处理分词是必不可少的一步。第二点，大部分英文文本都是uft-8的编码，这样在大多数时候处理的时候不用考虑编码转换的问题，而中文文本处理必须要处理unicode的编码问题。</p>
<p>而英文文本的预处理也有自己特殊的地方，第三点就是拼写问题，很多时候，我们的预处理要包括拼写检查，比如“Helo World”这样的错误，我们不能在分析的时候讲错纠错。所以需要在预处理前加以纠正。第四点就是词干提取(stemming)和词形还原(lemmatization)。这个东西主要是英文有单数，复数和各种时态，导致一个词会有不同的形式。比如“countries”和”country”，”wolf”和”wolves”，我们期望是有一个词。</p>
<h2 id="英文文本挖掘预处理一：数据收集"><a href="#英文文本挖掘预处理一：数据收集" class="headerlink" title="英文文本挖掘预处理一：数据收集"></a>英文文本挖掘预处理一：数据收集</h2><p>这部分英文和中文类似。获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。</p>
<p>对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“deep learning”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。</p>
<p>对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用beautifulsoup。但是我们我们需要某些特殊的语料数据，比如上面提到的“deep learning”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成。这个我一般使用ache。 ache允许我们用关键字或者一个分类算法模型来过滤出我们需要的主题语料，比较强大。</p>
<h2 id="英文文本挖掘预处理二：除去数据中非文本部分"><a href="#英文文本挖掘预处理二：除去数据中非文本部分" class="headerlink" title="英文文本挖掘预处理二：除去数据中非文本部分"></a>英文文本挖掘预处理二：除去数据中非文本部分</h2><p>这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用<a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/">beautifulsoup</a>来去除。另外还有一些特殊的非英文字符(non-alpha),也可以用Python的正则表达式(re)删除。</p>
<h3 id="re-模块"><a href="#re-模块" class="headerlink" title="re 模块"></a>re 模块</h3><p>参考 <a target="_blank" rel="noopener" href="https://songlee24.github.io/2014/09/01/python-library-02/">blog</a>  </p>
<p>正则表达式（Regular Expression）是字符串处理的常用工具，通常被用来检索、替换那些符合某个模式（Pattern）的文本。很多程序设计语言都支持正则表达式，像Perl、Java、C/C++。在 Python 中是通过标准库中的 re 模块 提供对正则的支持。</p>
<p>关于正则表达式的语法可以看</p>
<ul>
<li><p><a href="https://www.panxiaoxie.cn/2018/04/09/chapter2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E3%80%81%E6%96%87%E6%9C%AC%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/">speech and language processing chapter2</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wl_ss/article/details/78241782">正则表达式中的*，+，？以及\w和\W的区别等常见问题的总结</a></p>
</li>
</ul>
<h4 id="编译正则表达式"><a href="#编译正则表达式" class="headerlink" title="编译正则表达式"></a>编译正则表达式</h4><p>re 模块提供了 re.compile() 函数将一个字符串编译成 pattern object，用于匹配或搜索。函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">re.<span class="built_in">compile</span>(pattern, flags=<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>re.compile() 还接受一个可选的参数 flag，用于指定正则匹配的模式。关于匹配模式，后面将会讲到。</p>
<h4 id="反斜杠的困扰"><a href="#反斜杠的困扰" class="headerlink" title="反斜杠的困扰"></a>反斜杠的困扰</h4><p>在 python 的字符串中，\ 是被当做转义字符的。在正则表达式中，\ 也是被当做转义字符。这就导致了一个问题：如果你要匹配 \ 字符串，那么传递给 re.compile() 的字符串必须是 <code>”\\\\“</code>。</p>
<p>由于字符串的转义，所以实际传递给 re.compile() 的是 <code>”\\“</code>，然后再通过正则表达式的转义，<code>”\\“</code> 会匹配到字符”\“。这样虽然可以正确匹配到字符 \，但是很麻烦，而且容易漏写反斜杠而导致 Bug。那么有什么好的解决方案呢？</p>
<p>原始字符串很好的解决了这个问题，通过在字符串前面添加一个r，表示原始字符串，不让字符串的反斜杠发生转义。那么就可以使用<code>r&quot;\\\\&quot;</code>来匹配字符 <code>\</code>了。</p>
<h3 id="patern-object-执行匹配"><a href="#patern-object-执行匹配" class="headerlink" title="patern object 执行匹配"></a>patern object 执行匹配</h3><p>一旦你编译得到了一个 pattern object，你就可以使用 pattern object 的方法或属性进行匹配了，下面列举几个常用的方法，更多请看<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/re.html#regular-expression-objects">这里</a>。</p>
<p><code>Pattern.match(string[, pos[, endpos]])</code></p>
<ul>
<li><p>匹配从 pos 到 endpos 的字符子串的开头。匹配成功返回一个 match object，不匹配返回 None。  </p>
</li>
<li><p>pos 的默认值是0，endpos 的默认值是 len(string)，所以默认情况下是匹配整个字符串的开头。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&quot;d&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;dog&#x27;</span>))  <span class="comment"># 在字串开头，匹配成功</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;god&#x27;</span>))  <span class="comment"># 不再子串开头，匹配不成功</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;ddaa&#x27;</span>, <span class="number">1</span>,<span class="number">5</span>)) <span class="comment"># 在子串开头,匹配成功</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pattern.match(<span class="string">&#x27;monday&#x27;</span>, <span class="number">3</span>))  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(0, 1), match=&#x27;d&#x27;&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(0, 1), match=&#x27;g&#x27;&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(1, 2), match=&#x27;d&#x27;&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match object; span=(3, 4), match=&#x27;d&#x27;&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>regex.search(string[, pos[, endpos]])</code></p>
<ul>
<li><p>扫描整个字符串，并返回它找到的第一个匹配  </p>
</li>
<li><p>和 regex.match() 一样，可以通过 pos 和 endpos 指定范围</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&quot;ar&#123;1&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line">match = pattern.search(<span class="string">&quot;marray&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(match)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">1</span>, <span class="number">3</span>), match=<span class="string">&#x27;ar&#x27;</span>&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><code>regex.findall(string[, pos[, endpos]])</code></p>
<ul>
<li><p>找到所有匹配的子串，并返回一个 list    </p>
</li>
<li><p>可选参数 pos 和 endpos 和上面一样  </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d+&quot;</span>) <span class="comment"># 匹配字符串中的数字</span></span><br><span class="line"></span><br><span class="line">lst = pattern.findall(<span class="string">&quot;abc1def2rst3xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>regex.finditer(string[, pos[, endpos]])</code></p>
<ul>
<li><p>找到所有匹配的子串，并返回由这些匹配结果（match object）组成的迭代器  </p>
</li>
<li><p>可选参数 pos 和 endpos 和上面一样。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d+&quot;</span>)</span><br><span class="line"></span><br><span class="line">p = pattern.finditer(<span class="string">&quot;abc1def2rst3xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> p:</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">3</span>, <span class="number">4</span>), match=<span class="string">&#x27;1&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">7</span>, <span class="number">8</span>), match=<span class="string">&#x27;2&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">11</span>, <span class="number">12</span>), match=<span class="string">&#x27;3&#x27;</span>&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="match-object-获取结果"><a href="#match-object-获取结果" class="headerlink" title="match object 获取结果"></a>match object 获取结果</h4><p>在上面讲到，通过 pattern object 的方法（除 findall 外）进行匹配得到的返回结果都是 match object。每一个 match object 都包含了匹配到的相关信息，比如，起始位置、匹配到的子串。那么，我们如何从 match object 中提取这些信息呢？</p>
<p><code>match.group([group1, ...])：</code></p>
<ul>
<li><p>返回 match object 中的字符串。      </p>
</li>
<li><p>每一个 ( ) 都是一个分组，分组编号从1开始，从左往右，每遇到一个左括号，分组编号+1。   </p>
</li>
<li><p>组 0 总是存在的，它就是整个表达式  </p>
</li>
<li><p>没有参数时，group1默认为0，这时返回整个匹配到的字符串。  </p>
</li>
<li><p>指定一个参数（整数）时，返回该分组匹配到的字符串。  </p>
</li>
<li><p>指定多个参数时，返回由那几个分组匹配到的字符串组成的 tuple。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>) <span class="comment"># \w 匹配任意字母，数字，下划线</span></span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;He _ Kobe Bryant, Lakers player&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.group(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">4</span>), match=<span class="string">&#x27;He _&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">He _</span><br><span class="line"></span><br><span class="line">He</span><br><span class="line"></span><br><span class="line">_</span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;He&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p><code>match.groups()</code></p>
<ul>
<li>返回由所有分组匹配到的字符串组成的 tuple。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">m = re.match(<span class="string">r&quot;(\d+)\.(\d+)&quot;</span>, <span class="string">&#x27;24.163&#x27;</span>)</span><br><span class="line"></span><br><span class="line">m.groups()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;24&#x27;</span>, <span class="string">&#x27;163&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>match.start([group])</code></p>
<ul>
<li><p>没有参数时，返回匹配到的字符串的起始位置。  </p>
</li>
<li><p>指定参数（整数）时，返回该分组匹配到的字符串的起始位置。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.start())       <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.start(<span class="number">2</span>))      <span class="comment"># 5</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>match.end([group])：</code></p>
<ul>
<li><p>没有参数时，返回匹配到的字符串的结束位置。  </p>
</li>
<li><p>指定参数（整数）时，返回该分组匹配到的字符串的结束位置。  </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.end())       <span class="comment"># 11</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.end(<span class="number">1</span>))      <span class="comment"># 4</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>match.span([group])：</code></p>
<ul>
<li><p>返回一个二元 tuple 表示匹配到的字符串的范围，即 (start, end)。  </p>
</li>
<li><p>指定参数时，返回该分组匹配到的字符串的 (start, end)。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.span())     <span class="comment"># (0, 11)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.span(<span class="number">2</span>))    <span class="comment"># (5, 11)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="模块级别的函数"><a href="#模块级别的函数" class="headerlink" title="模块级别的函数"></a>模块级别的函数</h3><p>上面讲到的函数都是对象的方法，要使用它们必须先得到相应的对象。本节将介绍一些Module-Level Functions，比如 match()，search()，findall() 等等。你不需要创建一个 pattern object 就可以直接调用这些函数。</p>
<p><code>re.match(pattern, string, flags=0)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.match(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 相当于</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m = re.match(<span class="string">r&quot;(\w+) (\w+)&quot;</span>,<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(\w+) (\w+)&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.search(<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相当于</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m = re.search(<span class="string">r&quot;(\w+) (\w+)&quot;</span>,<span class="string">&quot;Kobe Bryant, Lakers&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">11</span>), match=<span class="string">&#x27;Kobe Bryant&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>re.findall(pattern, string, flags=0)</code>:与上面类似。</p>
<p><code>re.finditer(pattern, string, flags=0)</code>:与上面类似</p>
<h3 id="编译标志（匹配模式）"><a href="#编译标志（匹配模式）" class="headerlink" title="编译标志（匹配模式）"></a>编译标志（匹配模式）</h3><ul>
<li>re.IGNORECASE：忽略大小写，同 re.I。  </li>
</ul>
<ul>
<li>re.MULTILINE：多行模式，改变^和$的行为，同 re.M。  </li>
</ul>
<ul>
<li>re.DOTALL：点任意匹配模式，让’.’可以匹配包括’\n’在内的任意字符，同 re.S。  </li>
</ul>
<ul>
<li>re.LOCALE：使预定字符类 \w \W \b \B \s \S 取决于当前区域设定， 同 re.L。  </li>
</ul>
<ul>
<li>re.ASCII：使 \w \W \b \B \s \S 只匹配 ASCII 字符，而不是 Unicode 字符，同 re.A。  </li>
</ul>
<ul>
<li>re.VERBOSE：详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。主要是为了让正则表达式更易读，同 re.X。例如，以下两个正则表达式是等价的：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d + \. \d *#re.X&quot;</span>) the integral part</span><br><span class="line"></span><br><span class="line">b = re.<span class="built_in">compile</span>(<span class="string">r&quot;\d+\.\d*&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.match(<span class="string">&quot;123.45&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">6</span>), match=<span class="string">&#x27;123.45&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="修改字符串"><a href="#修改字符串" class="headerlink" title="修改字符串"></a>修改字符串</h3><p>第二部分讲的是字符串的匹配和搜索，但是并没有改变字符串。下面就讲一下可以改变字符串的操作。</p>
<h4 id="分割字符串"><a href="#分割字符串" class="headerlink" title="分割字符串"></a>分割字符串</h4><p>split()函数在匹配的地方将字符串分割，并返回一个 list。同样的，re 模块提供了两种 split 函数，一个是 pattern object 的方法，一个是模块级的函数。</p>
<p><code>regex.split(string, maxsplit=0)：</code></p>
<ul>
<li>maxsplit用于指定最大分割次数，不指定将全部分割。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;[A-Z]+&quot;</span>)</span><br><span class="line"></span><br><span class="line">m = pattern.split(<span class="string">&quot;abcDefgHijkLmnoPqrs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;abc&#x27;</span>, <span class="string">&#x27;efg&#x27;</span>, <span class="string">&#x27;ijk&#x27;</span>, <span class="string">&#x27;mno&#x27;</span>, <span class="string">&#x27;qrs&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><code>re.split(pattern, string, maxsplit=0, flags=0)：</code></p>
<ul>
<li><p>模块级函数，功能与 regex.split() 相同。  </p>
</li>
<li><p>flags用于指定匹配模式。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">m = re.split(<span class="string">r&quot;[A-Z]+&quot;</span>,<span class="string">&quot;abcDefgHijkLmnoPqrs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;abc&#x27;</span>, <span class="string">&#x27;efg&#x27;</span>, <span class="string">&#x27;ijk&#x27;</span>, <span class="string">&#x27;mno&#x27;</span>, <span class="string">&#x27;qrs&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="搜索与替换"><a href="#搜索与替换" class="headerlink" title="搜索与替换"></a>搜索与替换</h4><p>另一个常用的功能是找到所有的匹配，并把它们用不同的字符串替换。re 模块提供了sub()和subn()来实现替换的功能，而它们也分别有自己两个不同版本的函数。</p>
<p><code>regex.sub(repl, string, count=0)：</code></p>
<ul>
<li><p>使用 repl 替换 string 中每一个匹配的子串，返回替换后的字符串。若找不到匹配，则返回原字符串。</p>
</li>
<li><p>repl 可以是一个字符串，也可以是一个函数。</p>
</li>
<li><p>当repl是一个字符串时，任何在其中的反斜杠都会被处理。</p>
</li>
<li><p>当repl是一个函数时，这个函数应当只接受一个参数（pattern对象），对匹配到的对象进行处理，然后返回一个字符串用于替换。</p>
</li>
<li><p>count 用于指定最多替换次数，不指定时全部替换。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;like&quot;</span>, re.I)</span><br><span class="line"></span><br><span class="line">s1 = pattern.sub(<span class="string">r&quot;love&quot;</span>, <span class="string">&quot;I like you, do you like me?&quot;</span>)</span><br><span class="line"></span><br><span class="line">s2 = pattern.sub(<span class="keyword">lambda</span> m:m.group().upper(), <span class="string">&quot;I like you, do you like me?&quot;</span>)  <span class="comment"># repl 是函数，其参数是 pattern</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">I love you, do you love me?</span><br><span class="line"></span><br><span class="line">I LIKE you, do you LIKE me?</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p><code>re.sub(pattern, repl, string, count=0, flags=0)</code>：</p>
<ul>
<li><p>模块级函数，与 regex.sub() 函数功能相同。  </p>
</li>
<li><p>flags 用于指定匹配模式。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">s1 = re.sub(<span class="string">r&quot;(\w)&#x27;s\b&quot;</span>, <span class="string">r&quot;\1 is&quot;</span>, <span class="string">&quot;She&#x27;s Xie Pan&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">She <span class="keyword">is</span> Xie Pan</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>regex.subn(repl, string, count=0)</code></p>
<ul>
<li>同 sub()，只不过返回值是一个二元 tuple，即(sub函数返回值, 替换次数)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;like&quot;</span>, re.I)</span><br><span class="line"></span><br><span class="line">s1 = pattern.subn(<span class="string">r&quot;love&quot;</span>, <span class="string">&quot;I like you, do you like me?&quot;</span>)</span><br><span class="line"></span><br><span class="line">s2 = pattern.subn(<span class="keyword">lambda</span> m:m.group().upper(), <span class="string">&quot;I like you, do you like me?&quot;</span>)  <span class="comment"># repl 是函数，其参数是 pattern</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;I love you, do you love me?&#x27;</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;I LIKE you, do you LIKE me?&#x27;</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<p><code>re.subn(pattern, repl, string, count=0, flags=0)：</code></p>
<ul>
<li>同上</li>
</ul>
<h3 id="英文文本挖掘预处理三：拼写检查"><a href="#英文文本挖掘预处理三：拼写检查" class="headerlink" title="英文文本挖掘预处理三：拼写检查"></a>英文文本挖掘预处理三：拼写检查</h3><p>由于英文文本中可能有拼写错误，因此一般需要进行拼写检查。如果确信我们分析的文本没有拼写问题，可以略去此步。</p>
<p>拼写检查，我们一般用pyenchant类库完成。pyenchant的安装很简单：”pip install pyenchant”即可。</p>
<p>对于一段文本，我们可以用下面的方式去找出拼写错误：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 发现这样安装并不是在虚拟环境下，需要去终端对应的虚拟环境下安装</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># source avtivate NLP</span></span><br><span class="line"></span><br><span class="line">!pip install pyenchant</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>/bin/sh: 1: source: not found

Requirement already satisfied: pyenchant in /home/panxie/anaconda3/lib/python3.6/site-packages (2.0.0)

[31mdistributed 1.21.8 requires msgpack, which is not installed.[0m

[33mYou are using pip version 10.0.1, however version 18.0 is available.

You should consider upgrading via the &#39;pip install --upgrade pip&#39; command.[0m
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> enchant.checker <span class="keyword">import</span> SpellChecker</span><br><span class="line"></span><br><span class="line">chkr = SpellChecker(<span class="string">&#x27;en_US&#x27;</span>)</span><br><span class="line"></span><br><span class="line">chkr.set_text(<span class="string">&quot;Many peopel like too watch In the Name of people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> err <span class="keyword">in</span> chkr:</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ERROR:&quot;</span>, err.word)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ERROR: peopel</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>发现只能找单词拼写错误的，但 too 这样的是没办法找出的。找出错误后，我们可以自己来决定是否要改正。当然，我们也可以用pyenchant中的wxSpellCheckerDialog类来用对话框的形式来交互决定是忽略，改正还是全部改正文本中的错误拼写。  </p>
<p>更多操作可参考：  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/96c01666aeeb">https://www.jianshu.com/p/96c01666aeeb</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pythonhosted.org/pyenchant/tutorial.html">https://pythonhosted.org/pyenchant/tutorial.html</a></p>
</li>
</ul>
<h3 id="英文文本挖掘预处理四：词干提取-stemming-和词形还原-lemmatization"><a href="#英文文本挖掘预处理四：词干提取-stemming-和词形还原-lemmatization" class="headerlink" title="英文文本挖掘预处理四：词干提取(stemming)和词形还原(lemmatization)"></a>英文文本挖掘预处理四：词干提取(stemming)和词形还原(lemmatization)</h3><p>词干提取(stemming)和词型还原(lemmatization)是英文文本预处理的特色。两者其实有共同点，即都是要找到词的原始形式。只不过词干提取(stemming)会更加激进一点，它在寻找词干的时候可以会得到不是词的词干。比如”imaging”的词干可能得到的是”imag”, 并不是一个词。而词形还原则保守一些，它一般只对能够还原成一个正确的词的词进行处理。个人比较喜欢使用词型还原而不是词干提取。</p>
<p>在实际应用中，一般使用nltk来进行词干提取和词型还原。安装nltk也很简单，”pip install nltk”即可。只不过我们一般需要下载nltk的语料库，可以用下面的代码完成，nltk会弹出对话框选择要下载的内容。选择下载语料库就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">&#x27;wordnet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[nltk_data] Downloading package wordnet to /home/panxie/nltk_data...</span><br><span class="line"></span><br><span class="line">[nltk_data]   Unzipping corpora/wordnet.<span class="built_in">zip</span>.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>在nltk中，做词干提取的方法有PorterStemmer，LancasterStemmer和SnowballStemmer。个人推荐使用SnowballStemmer。这个类可以处理很多种语言，当然，除了中文。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> SnowballStemmer</span><br><span class="line"></span><br><span class="line">stemmer = SnowballStemmer(<span class="string">&quot;english&quot;</span>)</span><br><span class="line"></span><br><span class="line">stemmer.stem(<span class="string">&quot;countries&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;countri&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>输出是”countri”,这个词干并不是一个词。  </p>
<p>而如果是做词型还原，则一般可以使用WordNetLemmatizer类，即wordnet词形还原方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"></span><br><span class="line">wnl = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(wnl.lemmatize(<span class="string">&#x27;countries&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">country</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>输出是”country”,比较符合需求。  </p>
<p>在实际的英文文本挖掘预处理的时候，建议使用基于wordnet的词形还原就可以了。  </p>
<p>在<a target="_blank" rel="noopener" href="http://text-processing.com/demo/stem/">这里</a>有个词干提取和词型还原的demo，如果是这块的新手可以去看看，上手很合适。</p>
<h3 id="英文文本挖掘预处理五：转化为小写"><a href="#英文文本挖掘预处理五：转化为小写" class="headerlink" title="英文文本挖掘预处理五：转化为小写"></a>英文文本挖掘预处理五：转化为小写</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text = <span class="string">&#x27;XiePan&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(text.lower())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xiepan</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3 id="英文文本挖掘预处理六：引入停用词"><a href="#英文文本挖掘预处理六：引入停用词" class="headerlink" title="英文文本挖掘预处理六：引入停用词"></a>英文文本挖掘预处理六：引入停用词</h3><p>在英文文本中有很多无效的词，比如“a”，“to”，一些短词，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。个人常用的英文停用词表下载地址在这。当然也有其他版本的停用词表，不过这个版本是我常用的。</p>
<p>在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。这个方法和前文讲中文停用词的方法相同，这里就不写出代码，大家参考前文即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">stop = <span class="built_in">set</span>(stopwords.words(<span class="string">&#x27;english&#x27;</span>))  <span class="comment"># 停用词</span></span><br><span class="line"></span><br><span class="line">stop.add(<span class="string">&quot;foo&quot;</span>)    <span class="comment"># 增加一个词</span></span><br><span class="line"></span><br><span class="line">stop.remove(<span class="string">&quot;is&quot;</span>)  <span class="comment"># 去掉一个词</span></span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&quot;this is a foo bar sentence&quot;</span></span><br><span class="line"></span><br><span class="line">[i <span class="keyword">for</span> i <span class="keyword">in</span> word_tokenize(sentence.lower()) <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stop]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;is&#39;, &#39;bar&#39;, &#39;sentence&#39;]
</code></pre>
<h3 id="英文文本挖掘预处理七：特征处理"><a href="#英文文本挖掘预处理七：特征处理" class="headerlink" title="英文文本挖掘预处理七：特征处理"></a>英文文本挖掘预处理七：特征处理</h3><p>现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick</a>中，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在文本挖掘预处理之TF-IDF中，我们也讲到了<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6693230.html">TF-IDF特征处理的方法</a>。</p>
<p>TfidfVectorizer类可以帮助我们完成向量化，TF-IDF和标准化三步。当然，还可以帮我们处理停用词。这部分工作和中文的特征处理也是完全相同的，大家参考前文即可。</p>
<h3 id="英文文本挖掘预处理八：建立分析模型"><a href="#英文文本挖掘预处理八：建立分析模型" class="headerlink" title="英文文本挖掘预处理八：建立分析模型"></a>英文文本挖掘预处理八：建立分析模型</h3><p>有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而主题模型是自然语言处理比较特殊的一块，这个我们后面再单独讲。</p>
<h3 id="英文文本挖掘预处理总结"><a href="#英文文本挖掘预处理总结" class="headerlink" title="英文文本挖掘预处理总结"></a>英文文本挖掘预处理总结</h3><p>上面我们对英文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如有时候需要做词性标注，而有时候我们也需要英文分词，比如得到”New York”而不是“New”和“York”，因此这个流程仅供自然语言处理入门者参考，我们可以根据我们的数据分析目的选择合适的预处理方法。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-03T01:47:50.000Z" title="2018/6/3 上午9:47:50">2018-06-03</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">7 分钟读完 (大约1103个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks/">文本分类系列5-Hierarchical Attention Networks</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf">Hierarchical Attention Networks for Document Classification</a></p>
<h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>主要原理：</p>
<p>the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since **documents</p>
<p>have a hierarchical structure (words form sentences, sentences form a document)**, we likewise construct a document representation by first building representations of sentences and then aggregating those into</p>
<p>a document representation. Second, it is observed that different words and sentences in a documents are differentially informative.</p>
<p>对于一个document含有这样的层次结构，document由sentences组成，sentence由words组成。</p>
<p>the importance of words and sentences are highly context dependent, i.e. the same word or sentence may be differentially important in different context (x3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015) — one at the word level and one at the sentence level — that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document.</p>
<p>words和sentences都是高度上下文依赖的，同一个词或sentence在不同的上下文中，其表现的重要性会有差别。因此，这篇论文中使用了两个attention机制，来表示结合了上下文信息的词或句子的重要程度。（这里结合的上下文的词或句子，就是经过RNN处理后的隐藏状态）。</p>
<p>Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et</p>
<p>al., 2014)</p>
<p>attention不仅有好的效果，而且能够可视化的看见哪些词或句子对哪一类document的分类影响大。</p>
<p>本文的创新点在于，考虑了ducument中sentence这一层次结构，因为对于一个document的分类，可能前面几句话都是废话，而最后一句话来了一个转折，对document的分类起决定性作用。而之前的研究，只考虑了document中的词。</p>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="/2018/06/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks%5Chan.png"></p>
<h4 id="GRU-based-sequence-encoder"><a href="#GRU-based-sequence-encoder" class="headerlink" title="GRU-based sequence encoder"></a>GRU-based sequence encoder</h4><p><strong>reset gate:</strong> controls how much the past state contributes to the candidate state.</p>
<p>$$r_t=\sigma(W_rx_t+U_rh_{t-1}+b_r)$$</p>
<p><strong>candidate state:</strong></p>
<p>$$\tilde h_t=tanh(W_hx_t+r_t\circ (U_hh_{t-1})+b_h)$$</p>
<p><strong>update gate:</strong> decides how much past information is kept and how much new information is added.</p>
<p>$$z_t=\sigma(W_zx_t+U_zh_{t-1}+b_z)$$</p>
<p><strong>new state:</strong> a linear interpolation between the previous state $h_{t−1}$ and the current new state $\tilde h_t$ computed with new sequence information.</p>
<p>$$h_t=(1-z_t)\circ h_{t-1}+z_t\circ \tilde h_t$$</p>
<h4 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h4><h5 id="Word-Encoder"><a href="#Word-Encoder" class="headerlink" title="Word Encoder"></a>Word Encoder</h5><p>$$x_{it}=W_ew_{it}, t\in [1, T]$$</p>
<p>$$\overrightarrow h_{it}=\overrightarrow {GRU}(x_{it}),t\in[1,T]$$</p>
<p>$$\overleftarrow h_{it}=\overleftarrow {GRU}(x_{it}),t\in [T,1]$$</p>
<p>$$h_{it} = [\overrightarrow h_{it},\overleftarrow h_{it}]$$</p>
<p>i means the $i^{th}$ sentence in the document, and t means the $t^{th}$ word in the sentence.</p>
<h5 id="Word-Attention"><a href="#Word-Attention" class="headerlink" title="Word Attention"></a>Word Attention</h5><p>Not all words contribute equally to the representation of the sentence meaning.</p>
<p>Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector.</p>
<p>Attention机制说到底就是给予sentence中每个结合了上下文信息的词一个权重。关键在于这个权重怎么确定？</p>
<p>$$u_{it}=tanh(W_wh_{it}+b_w)$$</p>
<p>$$\alpha_{it}=\dfrac{exp(u_{it}^Tu_w)}{\sum_t^Texp(u_{it}^Tu_w)}$$</p>
<p>$$s_i=\sum_t^T\alpha_{it}h_{it}$$</p>
<p>这里首先是将 $h_{it}$ 通过一个全连接层得到 hidden representation $u_{it}$,然后计算 $u_{it}$ 与 $u_w$ 的相似性。并通过softmax归一化得到每个词与 $u_w$ 相似的概率。越相似的话，这个词所占比重越大，对整个sentence的向量表示影响越大。</p>
<p>那么关键是这个 $u_w$ 怎么表示？</p>
<p>The context vector $u_w$ can be seen as a high level representation of a fixed</p>
<p>query “what is the informative word” over the words like that used in memory networks (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.08895">Sukhbaatar et al., 2015, End-to-end memory networks.</a>; <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.07285">Kumar et al., 2015, Ask me anything: Dynamic memory networks for natural language processing.</a>). The word context vector $u_w$ is randomly initialized and jointly learned during the training process.</p>
<h5 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h5><p>$$\overrightarrow h_{i}=\overrightarrow {GRU}(s_{i}),t\in[1,L]$$</p>
<p>$$\overleftarrow h_{i}=\overleftarrow {GRU}(s_{i}),t\in [L,1]$$</p>
<p>$$H_i=[\overrightarrow h_{i}, \overleftarrow h_{i}]$$</p>
<p>hi summarizes the neighbor sentences around sentence i but still focus on sentence i.</p>
<h5 id="Sentence-Attention"><a href="#Sentence-Attention" class="headerlink" title="Sentence Attention"></a>Sentence Attention</h5><p>$$u_i=tanh(W_sH_i+b_s)$$</p>
<p>$$\alpha_i=\dfrac{exp(u_i^Tu_s)}{\sum_i^Lexp(u_i^Tu_s)}$$</p>
<p>$$v = \sum_i^L\alpha_ih_i$$</p>
<p>同样的 $u_s$ 表示： a sentence level context vector $u_s$</p>
<h4 id="Document-Classification"><a href="#Document-Classification" class="headerlink" title="Document Classification"></a>Document Classification</h4><p>The document vector v is a high level representation</p>
<p>of the document and can be used as features for document classification:</p>
<p>$$p=softmax(W_cv+b_c)$$</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="需要注意的问题"><a href="#需要注意的问题" class="headerlink" title="需要注意的问题"></a>需要注意的问题</h4><ul>
<li><p>如果使用tensorboard可视化</p>
</li>
<li><p>变量范围的问题</p>
</li>
</ul>
<h3 id="Context-dependent-attention-weights"><a href="#Context-dependent-attention-weights" class="headerlink" title="Context dependent attention weights"></a>Context dependent attention weights</h3><h3 id="Visualization-of-attention"><a href="#Visualization-of-attention" class="headerlink" title="Visualization of attention"></a>Visualization of attention</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-01T06:09:10.000Z" title="2018/6/1 下午2:09:10">2018-06-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.522Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">7 分钟读完 (大约1107个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/06/01/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%974-textRCNN/">文本分类系列4-textRCNN</a></h1><div class="content"><h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>paper: <a target="_blank" rel="noopener" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552">Recurrent Convolutional Neural Networks for Text Classification</a></p>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>先对之前的研究进行一番批判(0.0).</p>
<ol>
<li>传统的文本分类方法都是基于特征工程 feature representation，主要包括：</li>
</ol>
<ul>
<li><p>词袋模型 bag-of-words(BOW)model，用于提取unigram, bigram, n-grams的特征。</p>
</li>
<li><p>常见的特征选择的方法：frequency, MI (Cover and Thomas 2012), pLSA (Cai and Hofmann 2003), LDA (Hingmire et al. 2013)，用于选择具有更好的判别效果的特征。</p>
</li>
</ul>
<p>其原理就是去噪声来提高分类效果。比如去掉停用词，使用信息增益，互信息，或者L1正则化来获取有用的特征。但传统的特征表示的方法通常忽视了上下文信息和词序信息。</p>
<ol start="2">
<li>Richard Socher 提出的 <strong>Recursive Neural Network</strong></li>
</ol>
<p>RecusiveNN 通过语言的tree结构来获取句子的语义信息。但是分类的准确率太依赖文本的树结构。在文本分类之前建立一个树结构需要的计算复杂度就是 $O(n^2)$ （n是句子的长度）。所以对于很长的句子并不适用。</p>
<ol start="3">
<li>循环神经网络 Recurrent Neural Network</li>
</ol>
<p>计算复杂度是 $O(n)$，优点是能够很好的捕获长文本的语义信息，但是在rnn模型中，later words are more dominatant than earlier words. 但是如果对与某一个文本的分类，出现在之前的word影响更大的话，RNN的表现就不会很好。</p>
<p>为解决RNN这个问题，可以将CNN这个没有偏见的模型引入到NLP的工作中来，CNN能公平的对待句子中的每一个短语。 To tackle the bias problem, the Convolutional Neural Network (CNN), an unbiased model is introduced to NLP tasks, which can fairly determine discriminative phrases in a text with a max-pooling layer.</p>
<p>但是呢，通过前面的学习我们知道CNN的filter是固定尺寸的（fixed window），如果尺寸太短，会丢失很多信息，如果尺寸过长，计算复杂度又太大。所以作者提出个问题：能不能通过基于窗口的神经网络（CNN）学到更多的上下文信息，更好的表示文本的语义信息呢？ Therefore, it raises a question: can we learn more contextual information than conventional window-based neural networks and represent the semantic of texts more precisely for text classification.</p>
<p>于是，这篇论文提出了 Recurrent Concolution Neural Network(RCNN).</p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p><img src="/2018/06/01/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%974-textRCNN/model.png"></p>
<p>$$c_l{(w_i)} = f(W^{(l)}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))$$</p>
<p>$$c_r{(w_i)} = f(W^{(r)}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))$$</p>
<p>这两个公式类似于双向RNN，将 $c_l(w_i)$ 看作前一个时刻的隐藏状态 $h_{t-1}$, $c_l(w_{i-1})$ 就是 t-2 时刻的隐藏状态 $h_{t-2}$… 所以这就是个双向RNN…. 然后比较有创新的是，作者将隐藏状态 $h_{t-1}$ 和 $\tilde h_{t+1}$ ($\tilde h$ 表示反向), 以及当前word的词向量堆在一起，作为当前词以及获取了上下文信息的向量表示。</p>
<p>$$x_i = [c_l(w_i);e(w_i);c_r(w_i)]$$</p>
<p>然后是一个全连接层，这个可以看做textCNN中的卷积层,只是filter_size=1：</p>
<p>$$y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})$$</p>
<p>接着是最大池化层：</p>
<p>$$y^{(3)} = max_{i=1}^ny_i^{(2)}$$</p>
<p>然后是全连接层+softmax：</p>
<p>$$y^{(4)} = W^{(4)}y^{(3)}+b^{(4)}$$</p>
<p>$$p_i=\dfrac{exp(y_i^{(4)})}{\sum_{k=1}^nexp(y_k^{(4)})}$$</p>
<p>感觉就是双向rnn呀，只不过之前的方法是用最后一个隐藏层的输出作为整个sentence的向量表示，但这篇论文是用每一个时刻的向量表示(叠加了上下时刻的隐藏状态)，通过卷积层、maxpool后得到的向量来表示整个sentence.</p>
<p>确实是解决了RNN过于重视句子中靠后的词的问题，但是RNN训练慢的问题还是没有解决呀。但是在这里 <a target="_blank" rel="noopener" href="https://github.com/brightmart/text_classification#3textrnn">brightmart/text_classification</a> 中textCNN 和 RCNN的训练时间居然是一样的。why？</p>
<h4 id="Results-and-Discussion"><a href="#Results-and-Discussion" class="headerlink" title="Results and Discussion"></a>Results and Discussion</h4><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h3 id="需要注意的问题："><a href="#需要注意的问题：" class="headerlink" title="需要注意的问题："></a>需要注意的问题：</h3><ul>
<li>tf.nn.rnn_cell.DropoutWrapper</li>
</ul>
<ul>
<li>tf.nn.bidirectional_dynamic_rnn</li>
</ul>
<ul>
<li>tf.einsum</li>
</ul>
<ul>
<li>损失函数的对比 tf.nn.softmax_cross_entropy_with_logits</li>
</ul>
<ul>
<li>词向量是否需要正则化</li>
</ul>
<ul>
<li>tensorflow.contrib.layers.python.layers import optimize_loss 和 tf.train.AdamOptimizer(learning_rate).minimize(self.loss, self.global_steps) 的区别</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-31T13:25:54.000Z" title="2018/5/31 下午9:25:54">2018-05-31</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.479Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">4 分钟读完 (大约625个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/05/31/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%973-TextRNN/">文本分类系列3-TextRNN</a></h1><div class="content"><h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。</p>
<p>RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，<a target="_blank" rel="noopener" href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf">Recurrent Neural Network for Text Classification with Multi-Task Learning</a>文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。</p>
<h3 id="关于解决RNN无法并行化，计算效率低的问题"><a href="#关于解决RNN无法并行化，计算效率低的问题" class="headerlink" title="关于解决RNN无法并行化，计算效率低的问题"></a>关于解决RNN无法并行化，计算效率低的问题</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.10722">Factorization tricks for LSTM networks</a></p>
<ul>
<li>We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is “matrix factorization by design” of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></p>
<ul>
<li>The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-30T07:37:47.000Z" title="2018/5/30 下午3:37:47">2018-05-30</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.540Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">8 分钟读完 (大约1222个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/05/30/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%972-textCNN/">文本分类系列2-textCNN</a></h1><div class="content"><h3 id="paper-reading"><a href="#paper-reading" class="headerlink" title="paper reading"></a>paper reading</h3><p>主要框架和使用CNN进行文本分类的意图参考paper: <a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/D14-1181">Convolutional Neural Networks for Sentence Classification</a> 可参考cs224d中的课堂笔记，这堂课就是讲的这篇paper：</p>
<p><a href="http://www.panxiaoxie.cn/2018/05/14/cs224d-lecture13-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/#more">cs224d-lecture13 卷积神经网络</a></p>
<p><img src="/2018/05/30/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%972-textCNN/TextCNN.JPG"></p>
<p><strong>TextCNN详细过程</strong>：</p>
<ul>
<li>第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了,然后在图像中图像的表示是[length, width, channel],这里将文本的表示[sequence_len, embed_size, 1]。可以看到下面代码中：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">embeded_words = tf.nn.embedding_lookup(self.embedding, self.input_x) <span class="comment"># [None, sentence_len, embed_size]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># three channels similar to the image. using the tf.nn.conv2d</span></span><br><span class="line"></span><br><span class="line">self.sentence_embedding_expanded = tf.expand_dims(embeded_words, axis=-<span class="number">1</span>) <span class="comment"># [None, sentence_len, embed_size, 1]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。上图中的filter有3个，分别为：</p>
<ul>
<li><p>filter:[2, 5, 2] ==&gt; feature map:[6,1,2]</p>
</li>
<li><p>filter:[3, 5, 2] ==&gt; feature map:[5,1,2]</p>
</li>
<li><p>filter:[4, 5, 2] ==&gt; feature map:[4,1,2]</p>
</li>
<li><p>第三维表示channels，卷积后得到两个feature maps.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成scale，这里每个fiter_size的channel为2，所以输入的pooling.shape=[batch_size,1,1,2], 然后concat为一个flatten向量。</li>
</ul>
<ul>
<li>最后接一层全连接的 softmax 层，输出每个类别的概率。</li>
</ul>
<p><strong>特征</strong>：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。</p>
<p><strong>通道（Channels）</strong>：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。下面代码中的通道为1.</p>
<p><strong>一维卷积（conv-1d）</strong>：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。</p>
<p><strong>Pooling层</strong>：利用CNN解决文本分类问题的文章还是很多的，比如这篇 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a> 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：</p>
<pre><code>        “ 我觉得这个地方景色还不错，但是人也实在太多了 ”
</code></pre>
<p>虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>参数设置和模型具体实现参考paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1510.03820">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a></p>
<p>设计一个模型需要考虑的：  </p>
<ul>
<li><p>input word vector representations 输入的词向量表示  </p>
</li>
<li><p>filter region size(s); 卷积核的大小  </p>
</li>
<li><p>the number of feature maps; 特征图的通道数  </p>
</li>
<li><p>the activation function 激活函数  </p>
</li>
<li><p>the pooling strategy 池化的方式  </p>
</li>
<li><p>regularization terms (dropout/l2) 正则化项（dropout/l2）  </p>
</li>
</ul>
<h3 id="需要注意的问题"><a href="#需要注意的问题" class="headerlink" title="需要注意的问题"></a>需要注意的问题</h3><h4 id="一个单本对应单个标签和多个标签的区别？"><a href="#一个单本对应单个标签和多个标签的区别？" class="headerlink" title="一个单本对应单个标签和多个标签的区别？"></a>一个单本对应单个标签和多个标签的区别？</h4><p>关于多标签分类，应该看看周志华老师的这篇文章<a target="_blank" rel="noopener" href="http://cse.seu.edu.cn/people/zhangml/files/TKDE%2713.pdf">A Review on Multi-Label Learning Algorithms</a>, 知乎上还有其他资料<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/35486862">多标签（multi-label）数据的学习问题，常用的分类器或者分类策略有哪些？</a></p>
<p>本文代码中的方法：</p>
<ul>
<li>真实值labels的输入：单个标签的真实值是 <code>input_y.shape=[batch_size]</code>, 多个标签的真实值是 <code>input_y_multilabels.shape=[batch_size, label_size]</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">self.input_y = tf.placeholder(dtype=tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;input_y&#x27;</span>)</span><br><span class="line"></span><br><span class="line">self.input_y_multilabels = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>, num_classes], name=<span class="string">&quot;input_y_multilabels&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>损失函数的选择：</li>
</ul>
<ul>
<li>评价指标的区别：</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-25T07:12:40.000Z" title="2018/5/25 下午3:12:40">2018-05-25</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.522Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">19 分钟读完 (大约2870个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">文本分类系列0：NLTK学习和特征工程</a></h1><div class="content"><h3 id="计算语言：简单的统计"><a href="#计算语言：简单的统计" class="headerlink" title="计算语言：简单的统计"></a>计算语言：简单的统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.book <span class="keyword">import</span> *</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>*** Introductory Examples for the NLTK Book ***

Loading text1, ..., text9 and sent1, ..., sent9

Type the name of the text or sentence to view it.

Type: &#39;texts()&#39; or &#39;sents()&#39; to list the materials.

text1: Moby Dick by Herman Melville 1851

text2: Sense and Sensibility by Jane Austen 1811

text3: The Book of Genesis

text4: Inaugural Address Corpus

text5: Chat Corpus

text6: Monty Python and the Holy Grail

text7: Wall Street Journal

text8: Personals Corpus

text9: The Man Who Was Thursday by G . K . Chesterton 1908
</code></pre>
<p>找出text1,《白鲸记》中的词monstrous，以及其上下文</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text1.concordance(<span class="string">&quot;monstrous&quot;</span>, width=<span class="number">40</span>, lines=<span class="number">10</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>Displaying 10 of 11 matches:

 was of a most monstrous size . ... Thi

 Touching that monstrous bulk of the wh

enish array of monstrous clubs and spea

 wondered what monstrous cannibal and s

e flood ; most monstrous and most mount

Moby Dick as a monstrous fable , or sti

PTER 55 Of the Monstrous Pictures of Wh

exion with the monstrous pictures of wh

ose still more monstrous stories of the

ed out of this monstrous cabinet there
</code></pre>
<p>找出text1中与monstrous具有相同语境的词。比如monstrous的上下文 the __ pictures, the __ size. 同样在text1中与monstrous类似的上下文的词。很好奇这个是怎么实现的？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similar</span>(<span class="params">self, word, num=<span class="number">20</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Distributional similarity: find other words which appear in the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    same contexts as the specified word; list most similar words first.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text1.similar(<span class="string">&quot;monstrous&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>true contemptible christian abundant few part mean careful puzzled

mystifying passing curious loving wise doleful gamesome singular

delightfully perilous fearless
</code></pre>
<p>共用两个或两个以上词汇的上下文，如monstrous和very</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text2.common_contexts([<span class="string">&quot;monstrous&quot;</span>, <span class="string">&quot;very&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>a_pretty am_glad a_lucky is_pretty be_glad
</code></pre>
<p>自动检测出现在文本中的特定词，并显示同一上下文中出现的其他词。text4是《就职演说语料》，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    text4.dispersion_plot([<span class="string">&quot;citizens&quot;</span>, <span class="string">&quot;liberty&quot;</span>, <span class="string">&quot;freedom&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<pre><code>&lt;matplotlib.figure.Figure at 0x7f3794818588&gt;
</code></pre>
<p>如果不使用 if <strong>name</strong>==”<strong>main</strong>“ 的话会报错``` ‘NoneType’ object has no attribute ‘show’</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"></span><br><span class="line">fdist1 = FreqDist(text1)</span><br><span class="line"></span><br><span class="line">vocabulary1 = list(fdist1.keys())  # keys() 返回key值组成的list</span><br><span class="line"></span><br><span class="line">print(vocabulary1[:10])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;[&#39;, &#39;Moby&#39;, &#39;Dick&#39;, &#39;by&#39;, &#39;Herman&#39;, &#39;Melville&#39;, &#39;1851&#39;, &#39;]&#39;, &#39;ETYMOLOGY&#39;, &#39;.&#39;]
</code></pre>
<p>需要加list，不然回报错，<code>“TypeError: &#39;dict_keys&#39; object is not subscriptable”</code></p>
<p>dict.keys() returns an iteratable but not indexable object. The most simple (but not so efficient) solution would be:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 同样的道理这里也需要加list，因为生成的&lt;class &#x27;dict_items&#x27;&gt;z在python3中是迭代器</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(fdist1.items()))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(fdist1.items())[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&lt;class &#39;dict_items&#39;&gt;

[(&#39;[&#39;, 3), (&#39;Moby&#39;, 84), (&#39;Dick&#39;, 84), (&#39;by&#39;, 1137), (&#39;Herman&#39;, 1), (&#39;Melville&#39;, 1), (&#39;1851&#39;, 3), (&#39;]&#39;, 1), (&#39;ETYMOLOGY&#39;, 1), (&#39;.&#39;, 6862)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># dict.items() 实际上是将dict转换为可迭代对象list，list的对象是 (&#x27;[&#x27;, 3), (&#x27;Moby&#x27;, 84), (&#x27;Dick&#x27;, 84), (&#x27;by&#x27;, 1137)这样的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这下总能记住dict按照value排序了吧。。。尴尬，以前居然没弄懂？？</span></span><br><span class="line"></span><br><span class="line">fdist_sorted = <span class="built_in">sorted</span>(fdist1.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(fdist_sorted[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;,&#39;, 18713), (&#39;the&#39;, 13721), (&#39;.&#39;, 6862), (&#39;of&#39;, 6536), (&#39;and&#39;, 6024), (&#39;a&#39;, 4569), (&#39;to&#39;, 4542), (&#39;;&#39;, 4072), (&#39;in&#39;, 3916), (&#39;that&#39;, 2982)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 这个就是按照key排序。</span></span><br><span class="line"></span><br><span class="line">fdist_sorted2 = <span class="built_in">sorted</span>(fdist1.keys(), reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(fdist_sorted2[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;zoology&#39;, &#39;zones&#39;, &#39;zoned&#39;, &#39;zone&#39;, &#39;zodiac&#39;, &#39;zig&#39;, &#39;zephyr&#39;, &#39;zeal&#39;, &#39;zay&#39;, &#39;zag&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">fdist1.plot(<span class="number">20</span>, cumulative=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><img src="/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/output_16_0.png" alt="png"></p>
<p>可以看到高频词大都是无用的停用词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 低频词 fdist.hapaxes() 出现次数为1的词</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(fdist1.hapaxes()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> fdist1.hapaxes():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> fdist1[i] <span class="keyword">is</span> <span class="keyword">not</span> <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;hh&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>9002
</code></pre>
<p>可以看到低频词也很多，而且大都也是很无用的词。</p>
<h4 id="词语搭配"><a href="#词语搭配" class="headerlink" title="词语搭配"></a>词语搭配</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">list</span>(bigrams([<span class="string">&#x27;more&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;sad&#x27;</span>, <span class="string">&#x27;than&#x27;</span>, <span class="string">&#x27;done&#x27;</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>[(&#39;more&#39;, &#39;is&#39;), (&#39;is&#39;, &#39;sad&#39;), (&#39;sad&#39;, &#39;than&#39;), (&#39;than&#39;, &#39;done&#39;)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text4.collocations(window_size=<span class="number">4</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>United States; fellow citizens; four years; years ago; men women;

Federal Government; General Government; self government; Vice

President; American people; every citizen; within limits; Old World;

Almighty God; Fellow citizens; Chief Magistrate; Chief Justice; one

another; Declaration Independence; protect defend
</code></pre>
<p>文本4是就职演说语料，可以看到n-grams能够很好的展现出文本的特性，说明n-grams是不错的特征。</p>
<p>collections()源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collocations</span>(<span class="params">self, num=<span class="number">20</span>, window_size=<span class="number">2</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Print collocations derived from the text, ignoring stopwords.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :seealso: find_collocations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num: The maximum number of collocations to print.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :type num: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param window_size: The number of tokens spanned by a collocation (default=2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :type window_size: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (<span class="string">&#x27;_collocations&#x27;</span> <span class="keyword">in</span> self.__dict__ <span class="keyword">and</span> self._num == num <span class="keyword">and</span> self._window_size == window_size):</span><br><span class="line"></span><br><span class="line">        self._num = num</span><br><span class="line"></span><br><span class="line">        self._window_size = window_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#print(&quot;Building collocations list&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line">        ignored_words = stopwords.words(<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        finder = BigramCollocationFinder.from_words(self.tokens, window_size)</span><br><span class="line"></span><br><span class="line">        finder.apply_freq_filter(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        finder.apply_word_filter(<span class="keyword">lambda</span> w: <span class="built_in">len</span>(w) &lt; <span class="number">3</span> <span class="keyword">or</span> w.lower() <span class="keyword">in</span> ignored_words)</span><br><span class="line"></span><br><span class="line">        bigram_measures = BigramAssocMeasures()</span><br><span class="line"></span><br><span class="line">        self._collocations = finder.nbest(bigram_measures.likelihood_ratio, num)</span><br><span class="line"></span><br><span class="line">    colloc_strings = [w1+<span class="string">&#x27; &#x27;</span>+w2 <span class="keyword">for</span> w1, w2 <span class="keyword">in</span> self._collocations]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(tokenwrap(colloc_strings, separator=<span class="string">&quot;; &quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="自动理解自然语言"><a href="#自动理解自然语言" class="headerlink" title="自动理解自然语言"></a>自动理解自然语言</h3><ul>
<li><p>词义消歧 Ambiguity 关于词义消歧的理解可以看之前的笔记<a href="http://www.panxiaoxie.cn/2018/04/20/chapter12-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/">chapter12-句法分析</a></p>
</li>
<li><p>指代消解 anaphora resolution</p>
</li>
<li><p>自动问答</p>
</li>
<li><p>机器翻译</p>
</li>
<li><p>人机对话系统</p>
</li>
</ul>
<h3 id="获得文本语料和词汇资源"><a href="#获得文本语料和词汇资源" class="headerlink" title="获得文本语料和词汇资源"></a>获得文本语料和词汇资源</h3><h4 id="布朗语料库"><a href="#布朗语料库" class="headerlink" title="布朗语料库"></a>布朗语料库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>有以下这些类别的文本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(brown.categories())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;adventure&#39;, &#39;belles_lettres&#39;, &#39;editorial&#39;, &#39;fiction&#39;, &#39;government&#39;, &#39;hobbies&#39;, &#39;humor&#39;, &#39;learned&#39;, &#39;lore&#39;, &#39;mystery&#39;, &#39;news&#39;, &#39;religion&#39;, &#39;reviews&#39;, &#39;romance&#39;, &#39;science_fiction&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">news_text = brown.words(categories=<span class="string">&quot;news&quot;</span>)</span><br><span class="line"></span><br><span class="line">fdist_news = nltk.FreqDist([w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> news_text])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(fdist_news))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>13112
</code></pre>
<h3 id="标注文本语料库"><a href="#标注文本语料库" class="headerlink" title="标注文本语料库"></a>标注文本语料库</h3><p>经过了标注的语料库，有词性标注、命名实体、句法结构、语义角色等。</p>
<h4 id="分类和标注词汇"><a href="#分类和标注词汇" class="headerlink" title="分类和标注词汇"></a>分类和标注词汇</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text = nltk.word_tokenize(<span class="string">&quot;and now for something completely differences!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(text)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.pos_tag(text))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;and&#39;, &#39;now&#39;, &#39;for&#39;, &#39;something&#39;, &#39;completely&#39;, &#39;differences&#39;, &#39;!&#39;]

[(&#39;and&#39;, &#39;CC&#39;), (&#39;now&#39;, &#39;RB&#39;), (&#39;for&#39;, &#39;IN&#39;), (&#39;something&#39;, &#39;NN&#39;), (&#39;completely&#39;, &#39;RB&#39;), (&#39;differences&#39;, &#39;VBZ&#39;), (&#39;!&#39;, &#39;.&#39;)]
</code></pre>
<h5 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h5><p>NLTK中采用的方法可参考：<a target="_blank" rel="noopener" href="https://explosion.ai/blog/part-of-speech-pos-tagger-in-python">A Good Part-of-Speech Tagger in about 200 Lines of Python</a></p>
<p>对于一些同形同音异义词，通过词性标注能消除歧义.很多文本转语音系统通常需要进行词性标注，因为不同意思发音会不太一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">text1 = nltk.word_tokenize(<span class="string">&quot;They refuse to permit us tpo obtain the refuse permit&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.pos_tag(text1))</span><br><span class="line"></span><br><span class="line">text2 = nltk.word_tokenize(<span class="string">&quot;They refuse to permit us to obtain the refuse permit&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.pos_tag(text2))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;They&#39;, &#39;PRP&#39;), (&#39;refuse&#39;, &#39;VBP&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;permit&#39;, &#39;VB&#39;), (&#39;us&#39;, &#39;PRP&#39;), (&#39;tpo&#39;, &#39;VB&#39;), (&#39;obtain&#39;, &#39;VB&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;refuse&#39;, &#39;NN&#39;), (&#39;permit&#39;, &#39;NN&#39;)]

[(&#39;They&#39;, &#39;PRP&#39;), (&#39;refuse&#39;, &#39;VBP&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;permit&#39;, &#39;VB&#39;), (&#39;us&#39;, &#39;PRP&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;obtain&#39;, &#39;VB&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;refuse&#39;, &#39;NN&#39;), (&#39;permit&#39;, &#39;NN&#39;)]
</code></pre>
<h5 id="获取已经标注好的语料库"><a href="#获取已经标注好的语料库" class="headerlink" title="获取已经标注好的语料库"></a>获取已经标注好的语料库</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.brown.tagged_words())</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;The&#39;, &#39;AT&#39;), (&#39;Fulton&#39;, &#39;NP-TL&#39;), ...]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.treebank.tagged_words())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.treebank.tagged_sents()[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#39;Pierre&#39;, &#39;NNP&#39;), (&#39;Vinken&#39;, &#39;NNP&#39;), (&#39;,&#39;, &#39;,&#39;), ...]

[(&#39;Pierre&#39;, &#39;NNP&#39;), (&#39;Vinken&#39;, &#39;NNP&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;61&#39;, &#39;CD&#39;), (&#39;years&#39;, &#39;NNS&#39;), (&#39;old&#39;, &#39;JJ&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;will&#39;, &#39;MD&#39;), (&#39;join&#39;, &#39;VB&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;board&#39;, &#39;NN&#39;), (&#39;as&#39;, &#39;IN&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;nonexecutive&#39;, &#39;JJ&#39;), (&#39;director&#39;, &#39;NN&#39;), (&#39;Nov.&#39;, &#39;NNP&#39;), (&#39;29&#39;, &#39;CD&#39;), (&#39;.&#39;, &#39;.&#39;)]
</code></pre>
<p>查看brown语料库中新闻类最常见的词性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">brown_news_tagged = brown.tagged_words(categories=<span class="string">&#x27;news&#x27;</span>, tagset=<span class="string">&#x27;universal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tag_fd = nltk.FreqDist(tag <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> brown_news_tagged)</span><br><span class="line"></span><br><span class="line">tag_fd.keys()</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>dict_keys([&#39;DET&#39;, &#39;NOUN&#39;, &#39;ADJ&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;.&#39;, &#39;ADV&#39;, &#39;CONJ&#39;, &#39;PRT&#39;, &#39;PRON&#39;, &#39;NUM&#39;, &#39;X&#39;])
</code></pre>
<h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><h4 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h4><p>选取特征，将名字的最后一个字母作为特征. 返回的字典称为特征集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features</span>(<span class="params">word</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;last_letter&#x27;</span>:word[-<span class="number">1</span>]&#125;</span><br><span class="line"></span><br><span class="line">gender_features(<span class="string">&#x27;Shrek&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&#123;&#39;last_letter&#39;: &#39;k&#39;&#125;
</code></pre>
<p>定义一个特征提取器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> names</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">names = ([(name, <span class="string">&#x27;male&#x27;</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">&#x27;male.txt&#x27;</span>)] +</span><br><span class="line"></span><br><span class="line">        [(name, <span class="string">&#x27;female&#x27;</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">&#x27;female.txt&#x27;</span>)])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.corpus.names.words(<span class="string">&#x27;male.txt&#x27;</span>)[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(names[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[&#39;Aamir&#39;, &#39;Aaron&#39;, &#39;Abbey&#39;, &#39;Abbie&#39;, &#39;Abbot&#39;, &#39;Abbott&#39;, &#39;Abby&#39;, &#39;Abdel&#39;, &#39;Abdul&#39;, &#39;Abdulkarim&#39;]

[(&#39;Aamir&#39;, &#39;male&#39;), (&#39;Aaron&#39;, &#39;male&#39;), (&#39;Abbey&#39;, &#39;male&#39;), (&#39;Abbie&#39;, &#39;male&#39;), (&#39;Abbot&#39;, &#39;male&#39;), (&#39;Abbott&#39;, &#39;male&#39;), (&#39;Abby&#39;, &#39;male&#39;), (&#39;Abdel&#39;, &#39;male&#39;), (&#39;Abdul&#39;, &#39;male&#39;), (&#39;Abdulkarim&#39;, &#39;male&#39;)]
</code></pre>
<p>使用特征提取器处理names数据，并把数据集分为训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 二分类</span></span><br><span class="line"></span><br><span class="line">features = [(gender_features(n), g) <span class="keyword">for</span> (n, g) <span class="keyword">in</span> names]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">train_set, test_set = features[<span class="number">500</span>:], features[:<span class="number">500</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_set[:<span class="number">10</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>[(&#123;&#39;last_letter&#39;: &#39;n&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;b&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;b&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;y&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;y&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;t&#39;&#125;, &#39;male&#39;), (&#123;&#39;last_letter&#39;: &#39;e&#39;&#125;, &#39;male&#39;)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 预测一个未出现的名字</span></span><br><span class="line"></span><br><span class="line">classifier.classify(gender_features(<span class="string">&#x27;Pan&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>&#39;male&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 测试集上的准确率</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, test_set))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>0.602
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">classifier.show_most_informative_features(<span class="number">5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>Most Informative Features

             last_letter = &#39;a&#39;            female : male   =     35.5 : 1.0

             last_letter = &#39;k&#39;              male : female =     34.1 : 1.0

             last_letter = &#39;f&#39;              male : female =     15.9 : 1.0

             last_letter = &#39;p&#39;              male : female =     13.5 : 1.0

             last_letter = &#39;v&#39;              male : female =     12.7 : 1.0
</code></pre>
<p>构建包含所有实例特征的单独list会占用大量内存，所有应该把这些特征集成起来。</p>
<h4 id="定义一个特征提取器包含多个特征"><a href="#定义一个特征提取器包含多个特征" class="headerlink" title="定义一个特征提取器包含多个特征"></a>定义一个特征提取器包含多个特征</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 添加多个特征</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> apply_features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features2</span>(<span class="params">word</span>):</span></span><br><span class="line"></span><br><span class="line">    features = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    features[<span class="string">&#x27;firstletter&#x27;</span>] = word[<span class="number">0</span>].lower()</span><br><span class="line"></span><br><span class="line">    features[<span class="string">&#x27;lastletter&#x27;</span>] = word[-<span class="number">1</span>].lower()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        features[<span class="string">&quot;count(%s)&quot;</span>%letter] = word.lower().count(letter)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gender_features2(<span class="string">&#x27;xiepan&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(gender_features2(<span class="string">&#x27;xiepan&#x27;</span>))) <span class="comment"># 有28个特征， 2+26=28</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&#123;&#39;firstletter&#39;: &#39;x&#39;, &#39;lastletter&#39;: &#39;n&#39;, &#39;count(a)&#39;: 1, &#39;count(b)&#39;: 0, &#39;count(c)&#39;: 0, &#39;count(d)&#39;: 0, &#39;count(e)&#39;: 1, &#39;count(f)&#39;: 0, &#39;count(g)&#39;: 0, &#39;count(h)&#39;: 0, &#39;count(i)&#39;: 1, &#39;count(j)&#39;: 0, &#39;count(k)&#39;: 0, &#39;count(l)&#39;: 0, &#39;count(m)&#39;: 0, &#39;count(n)&#39;: 1, &#39;count(o)&#39;: 0, &#39;count(p)&#39;: 1, &#39;count(q)&#39;: 0, &#39;count(r)&#39;: 0, &#39;count(s)&#39;: 0, &#39;count(t)&#39;: 0, &#39;count(u)&#39;: 0, &#39;count(v)&#39;: 0, &#39;count(w)&#39;: 0, &#39;count(x)&#39;: 1, &#39;count(y)&#39;: 0, &#39;count(z)&#39;: 0&#125;

28
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 对每个样本进行特征处理</span></span><br><span class="line"></span><br><span class="line">features = [(gender_features(n), g) <span class="keyword">for</span> (n,g) <span class="keyword">in</span> names]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(features))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>7944
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 训练集，开发集和测试集</span></span><br><span class="line"></span><br><span class="line">train_set = features[<span class="number">1500</span>:]</span><br><span class="line"></span><br><span class="line">dev_set = apply_features(gender_features2, names[<span class="number">500</span>:<span class="number">1500</span>])</span><br><span class="line"></span><br><span class="line">test_set = apply_features(gender_features2, names[:<span class="number">500</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, dev_set))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, test_set))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nltk.classify.accuracy(classifier, train_set))  <span class="comment">## 明显过拟合了～</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>0.007

0.008

0.883302296710118
</code></pre>
<h3 id="文档分类"><a href="#文档分类" class="headerlink" title="文档分类"></a>文档分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> movie_reviews</span><br><span class="line"></span><br><span class="line">documents = [(<span class="built_in">list</span>(movie_reviews.words(fileid)), category) <span class="keyword">for</span> category <span class="keyword">in</span> movie_reviews.categories()</span><br><span class="line"></span><br><span class="line">             <span class="keyword">for</span> fileid <span class="keyword">in</span> movie_reviews.fileids(category) ]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">movie_reviews.categories()</span><br><span class="line"></span><br></pre></td></tr></table></figure>









<pre><code>[&#39;neg&#39;, &#39;pos&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">neg_docu = movie_reviews.fileids(<span class="string">&#x27;neg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(neg_docu))   <span class="comment"># neg类别的文档数　1000</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))  <span class="comment">#　总的文档数    1000</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(movie_reviews.words(neg_docu[<span class="number">0</span>])) <span class="comment"># 第一个文件中单词数  879</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>1000

2000

879
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">random.shuffle(documents)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="文档分类的特征提取器"><a href="#文档分类的特征提取器" class="headerlink" title="文档分类的特征提取器"></a>文档分类的特征提取器</h4><p>所谓特征提取器实际上就是将文档原本的内容用认为选定的特征来表示。然后用分类器找出这些特征和对应类标签的映射关系。</p>
<p>那么什么样的特征才是好的特征，这就是特征工程了吧。</p>
<h4 id="文本分类概述"><a href="#文本分类概述" class="headerlink" title="文本分类概述"></a>文本分类概述</h4><p>文本分类，顾名思义，就是根据文本内容本身将文本归为不同的类别，通常是有监督学习的任务。根据文本内容的长短，有做句子、段落或者文章的分类；文本的长短不同可能会导致文本可抽取的特征上的略微差异，<strong>但是总体上来说，文本分类的核心都是如何从文本中抽取出能够体现文本特点的关键特征，抓取特征到类别之间的映射。</strong> 所以，特征工程就显得非常重要，特征找的好，分类效果也会大幅提高（当然前提是标注数据质量和数量也要合适，数据的好坏决定效果的下限，特征工程决定效果的上限）。</p>
<p>也许会有人问最近的深度学习技术能够避免我们构造特征这件事，为什么还需要特征工程？深度学习并不是万能的，在NLP领域深度学习技术取得的效果有限（毕竟语言是高阶抽象的信息，深度学习在图像、语音这些低阶具体的信息处理上更适合，因为在低阶具体的信息上构造特征是一件费力的事情），并不是否认深度学习在NLP领域上取得的成绩，工业界现在通用的做法都是会把深度学习模型作为系统的一个子模块（也是一维特征），和一些传统的基于统计的自然语言技术的特征，还有一些针对具体任务本身专门设计的特征，一起作为一个或多个模型（也称Ensemble，即模型集成）的输入，最终构成一个文本处理系统。</p>
<h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><p>那么，对于文本分类任务而言，工业界常用到的特征有哪些呢？下面用一张图以概括：</p>
<p><img src="/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/01.png"></p>
<p>我主要将这些特征分为四个层次，由下往上，特征由抽象到具体，粒度从细到粗。我们希望能够从不同的角度和纬度来设计特征，以捕捉这些特征和类别之间的关系。下面详细介绍这四个层次上常用到的特征表示。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-23T02:15:48.000Z" title="2018/5/23 上午10:15:48">2018-05-23</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><span class="level-item">13 分钟读完 (大约1950个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/05/23/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%971-fasttext/">文本分类系列1-fasttext</a></h1><div class="content"><p>在Facebook fasttext github主页中，关于fasttext的使用包括两个方面，词向量表示学习以及文本分类。</p>
<ul>
<li><p>词向量表示学习：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1607.04606.pdf">Enriching Word Vectors with Subword Information</a></p>
</li>
<li><p>文本分类：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1607.01759.pdf">Bag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov</a></p>
</li>
</ul>
<h3 id="Paper-Reading1"><a href="#Paper-Reading1" class="headerlink" title="Paper Reading1"></a>Paper Reading1</h3><p>这篇文章是用来进行文本分类的: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1607.01759.pdf">Bag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov</a></p>
<p>这个模型跟word2vec 中的CBOw模型极其相似，区别在于将中心词换成文本标签。那么输入层是文本中单词经过嵌入曾之后的词向量构成的的n-gram，然后求平均操作得到一个文本sentence的向量，也就是隐藏层h，然后再经过一个输出层映射到所有类别中，论文里面还详细论述了如何使用n-gram feature考虑单词的顺序关系，以及如何使用Hierarchical softmax机制加速softmax函数的计算速度。模型的原理图如下所示：</p>
<p><img src="/2018/05/23/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%971-fasttext/01.png"></p>
<p>目标函数：</p>
<p>$$\dfrac{1}{N}\sum_{n=1}^Ny_nlog(f(BAx_n))$$</p>
<ul>
<li><p>N表示文本数量，训练时就是Batch size吧？</p>
</li>
<li><p>$x_n$ 表示第n个文本的 normalized bag of features</p>
</li>
<li><p>$y_n$ 表示第n个文本的类标签</p>
</li>
<li><p>A is the look up table over n-gram. 类似于attention中的权重吧</p>
</li>
<li><p>B is the weight matrix</p>
</li>
</ul>
<p>隐藏层到输出层的计算复杂度是 $O(hk)$. h是隐藏层的维度，k是总的类别数。经过hierarchical softmax处理后，复杂度为 $O(hlog_2k)$</p>
<ul>
<li><p>这种模型的优点在于简单，无论训练还是预测的速度都很快，比其他深度学习模型高了几个量级</p>
</li>
<li><p>缺点是模型过于简单，准确度较低。</p>
</li>
</ul>
<h3 id="paper-reading2"><a href="#paper-reading2" class="headerlink" title="paper reading2"></a>paper reading2</h3><p>这篇文章是在word2vec的基础上拓展了，用来学习词向量表示 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1607.04606.pdf">Enriching Word Vectors with Subword Information</a></p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word.</p>
<p>之前的模型在用离散的向量表示单词时都忽略了单词的形态。</p>
<p>In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations.</p>
<p>这篇文章提出了一个skipgram模型,其中每一个单词表示为组成这个单词的字袋模型 a bag of character n-grams. 一个单词的词向量表示为这些 n-grams表示的总和。</p>
<p>Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information. We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach. 这篇文章可以看作是word2vec的拓展，主要是针对一些形态特别复杂的语言。</p>
<p>word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。</p>
<h4 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h4><h5 id="Morphological-word-representations"><a href="#Morphological-word-representations" class="headerlink" title="Morphological word representations"></a>Morphological word representations</h5><p>针对形态词表示已有的工作：</p>
<p>传统的用单词的形态特征来表示单词：</p>
<ul>
<li><p>[Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored neural language models. In Proc. NAACL] introduced factored neural language models. 因式分解模型</p>
<ul>
<li><p>words are represented as sets of features.</p>
</li>
<li><p>These features might include morphological information</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>Schütze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing</li>
</ul>
<p>the four-grams representations. 这篇文正的工作跟本文的方法是比较接近的。</p>
<h5 id="Character-level-features-for-NLP-NLP的字符特征"><a href="#Character-level-features-for-NLP-NLP的字符特征" class="headerlink" title="Character level features for NLP NLP的字符特征"></a>Character level features for NLP NLP的字符特征</h5><p>字符级别的研究工作最近很多了。一类是基于RNN的，另一类是基于CNN的。</p>
<h4 id="General-Model"><a href="#General-Model" class="headerlink" title="General Model"></a>General Model</h4><p>这一部分是对word2vec中跳字模型的回顾。<a href="http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/">Skip-gram predicts the distribution (probability) of context words from a center word.</a></p>
<p>giving a sequence of words $w_1, w_2,…,w_T$</p>
<p>那么skipgram 模型就是最大化对数似然函数：</p>
<p>$$\sum_{t=1}^T\sum_{c\in C_t}logp(w_c|w_t)$$</p>
<p>we are given a scoring function s which maps pairs of (word, context) to scores in R.</p>
<p>$$p(w_c|w_t)=\dfrac{e^{s(w_t,w_c)}}{\sum_{j=1}^We^{s(w_t,j)}}$$</p>
<p>The problem of predicting context words can instead be framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. For the word at position t we consider all context words as positive examples and sample negatives at</p>
<p>random from the dictionary.</p>
<p>这篇文章采用负采样的方法。与原本的softmax或者是hierarchical softmax不一样的是，负采样中预测一个上下文的单词context $w_c$ 是把它看做一个独立的二分类，存在或者是不存在。</p>
<p>因此选择一个上下文 position c, using binary logistic loss we obtain the following negative log-likelihood::</p>
<p>$$log(1+e^{-s(w_t,w_c)})+\sum_{n\in N_{t,c}}log(1+e^{s(w_t,n)})$$</p>
<p>其实跟word2vec中是一样的就是 $-log\sigma(s(w_t,w_c))=-log\dfrac{1}{1+e^{-s(w_t,w_c)}}=log(1+e^{-s(w_t,w_c)})$</p>
<p>那么对于整个Sequence，设定 $l: x\rightarrow log(1+e^{-x})$ 那么：</p>
<p>$$\sum_{t=1}^T[\sum_{c\in C_t}l(s(w_t,w_c))+\sum_{n\in N_{t,c}}l(-s(w_t,n))]$$</p>
<p>$N_{t,c}$ is a set of negative examples sampled from the vocabulary. 怎么选负采样呢？　每个单词都被给予一个等于它频率的权重（单词出现的数目）的3/4次方。选择某个单词的概率就是它的权重除以所有单词权重之和。</p>
<p>$$p(w_i)=\dfrac{f(w_i)^{3/4}}{\sum_{j=0}^W(f(w_j)^{3/4})}$$</p>
<p>Then the score can be computed as the scalar product between word and context vectors as:</p>
<p>$$s(w_t,w_c) = u_{w_t}^Tv_{w_v}$$</p>
<h4 id="Subword-model"><a href="#Subword-model" class="headerlink" title="Subword model"></a>Subword model</h4><p>By using a distinct vector representation for each word, the skipgram model ignores the internal structure of words. In this section, we propose a different scoring function s, in order to take into account this information. 单词的离散词向量表示是忽略了单词内部的结构信息的，也就是其字母组成。</p>
<p>给每个单词左右加上 &lt; 和 &gt;，用来区分前缀和后缀。对于单词 where 来说，用 character trigram 表示：</p>
<p><img src="/2018/05/23/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%971-fasttext/03.png"></p>
<p>用 $z_g$ 表示n-gram g 的向量表示。那么 scoring function:</p>
<p>$$s(w,c)=\sum_{g\in G_w}z_g^Tv_c$$</p>
<p>如果词表很大的话，其对应的 n-gram 也会非常多吧，为了限制占用的内存，we use a hashing function that maps n-grams to integers in 1 to K. We hash character sequences using the Fowler-Noll-Vo hashing function (specifically the FNV-1a variant).1 We set $K = 2.10^6$ below.</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h3 id="需要注意的问题"><a href="#需要注意的问题" class="headerlink" title="需要注意的问题"></a>需要注意的问题</h3><ul>
<li>代码实现中对于sentence的向量表示，是unigram的平均值，如果要让效果更好，可以添加bigram, trigram等。</li>
</ul>
<ul>
<li>tf.train.exponential_decay</li>
</ul>
<ul>
<li>tf.nn.nce_loss</li>
</ul>
</div></article></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">117</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>