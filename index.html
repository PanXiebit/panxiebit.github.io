<!doctype html>
<html lang="de"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="潘小榭"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘小榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="潘小榭"><meta property="og:url" content="http://www.panxiaoxie.cn/"><meta property="og:site_name" content="潘小榭"><meta property="og:image" content="http://www.panxiaoxie.cn/img/og_image.png"><meta property="article:author" content="Xie Pan"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/logo.svg"}},"description":null}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Suche" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Gepostet vor&nbsp;<time dateTime="2021-05-05T02:03:16.000Z" title="2021/5/5 上午10:03:16">2021-05-05</time></span><span class="level-item">Aktualisiert vor&nbsp;<time dateTime="2021-06-29T07:09:27.141Z" title="2021/6/29 下午3:09:27">2021-06-29</time></span><span class="level-item">15 minutes lesen (Über 2175 Wörter)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-contrastive learning</a></h1><div class="content"><h1 id="image-based"><a href="#image-based" class="headerlink" title="image-based"></a>image-based</h1><h2 id="simCLR"><a href="#simCLR" class="headerlink" title="simCLR"></a>simCLR</h2><p>A Simple Framework for Contrastive Learning of Visual Representations</p>
<p>作者提出了一个简单的对比学习框架，不需要特殊的网络结构和memory bank.</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>现有的无监督视觉表示学习的方法主要分为两类：生成式和判别式。</p>
<p>生成式主要包括以下三类：    </p>
<ul>
<li>deep belief nets[^1]   </li>
<li>Auto-encoding[^2]    </li>
<li>Generative adversarial nets[^3]    </li>
</ul>
<p>pixel-level 生成式算法非常消耗计算资源，因而对于有效的表示学习并不是必须的。</p>
<p>判别式方法的目标函数更接近监督学习，不过其对应的监督任务是从没有标签的数据集中自行构造的，因而学到的视觉表示能力受限于预定义的任务，而泛化能力有限。现有的达到sota的几篇paper[^4][^5][^6]    </p>
<p>作者提出了一个简单的对比学习方法，不仅达到了sota，而且不需要复杂的网络结构[^6][^7]，也不需要memory bank[^8][^9][^10][^11].    </p>
<p>为了系统的理解怎样才能获得有效的的对比学习，作者研究了以下几个重要组成部分：   </p>
<ul>
<li>data augmentation：相比有监督学习，对比学习更需要数据增强 $t\sim T$      </li>
<li>nonlinear projection：如图所示，在视觉表示和contrast loss之间增加一个非线性projection $g(\cdot)$ 很有必要  </li>
<li>normalized embeddings and an appropriately adjusted temperature parameter: 归一化的cross entropy和可调整的temperature parameter.      </li>
<li>larger batch size and more training steps   </li>
</ul>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/simslr.png"></p>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>如上图所示，simCLR 主要包括四部分：  </p>
<ul>
<li>A stochastic data augmentation module: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur    </li>
<li>A neural network base encoder $f(\cdot)$，作者采用的是 ResNet. $h_i = f(\tilde x_i) = ResNet(\tilde x_i)$   </li>
<li>A small neural network projection head $g(\cdot)$, $z_i = g(h_i) = W^{(2)}σ(W^{(1)}h_i)$. 作者发现在 $z_i$ 上计算 contrast loss，比 $h_i$ 效果更好。   </li>
<li>A contrastive loss function：NT-Xent (the normalized temperature-scaled cross entropy loss.  </li>
</ul>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss_func.png"></p>


<p>其中 $sim(u,v)=\dfrac{u^Tv}{\lVert u \rVert \lVert v\rVert}$.</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/algorithm.png"></p>
<h4 id="Training-with-Large-Batch-Size"><a href="#Training-with-Large-Batch-Size" class="headerlink" title="Training with Large Batch Size"></a>Training with Large Batch Size</h4><p>作者采用了更大的batch size(256 $\rightarrow$ 8192)，因而不需要memory bank. 这样一个batch有 ($8192\times 2=16382$) 个负样本。</p>
<p>在超大的batch size情况下，使用SGD/Momentum学习率不稳定，因此作者使用LARS optimizer.</p>
<p>作者使用 32-128 cores TPU进行训练。（这真的劝退。。。</p>
<h5 id="Global-BN"><a href="#Global-BN" class="headerlink" title="Global BN"></a>Global BN</h5><p>在分布式训练的场景下，BN的均值和方差是在单个device上计算的。而两个正样本是在同一个device上计算的，因此在拉进两个正样本之间的agreement时，BN会造成信息泄露。为了解决这个问题，作者采用的方法是在所有的device上计算BN的均值和方差。类似地解决这一问题的方法还有：shuffling data examples across devices[^10], replacing BN with layer norm[^7].</p>
<blockquote>
<p>这点其实不太理解，为啥BN会造成信息泄露？</p>
</blockquote>
<h4 id="Evaluation-Protocol"><a href="#Evaluation-Protocol" class="headerlink" title="Evaluation Protocol"></a>Evaluation Protocol</h4><h5 id="Dataset-and-Metrics"><a href="#Dataset-and-Metrics" class="headerlink" title="Dataset and Metrics."></a>Dataset and Metrics.</h5><p>作者先在CIFAR-10上进行试验，得到了94.0%的准确率（有监督的准确率是95.1%）.</p>
<p>为了验证学习得到的视觉表示，作者采用广泛使用的linear evaluation protocol[^5][^6]: a linear classifier is trained on top of the frozen base network, and test accuracy is used as a proxy for representation quality.</p>
<p>除了linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning.</p>
<h5 id="Default-setting"><a href="#Default-setting" class="headerlink" title="Default setting"></a>Default setting</h5><p>We use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space.</p>
<p>As the loss, we use NT-Xent, optimized using LARS with learning rate of <strong>4.8</strong> (= 0.3 × BatchSize/256) and weight decay of 10−6. We train at batch size 4096 for 100 epochs.</p>
<h3 id="Data-Augmentation-for-Contrastive-Representation-Learning"><a href="#Data-Augmentation-for-Contrastive-Representation-Learning" class="headerlink" title="Data Augmentation for Contrastive Representation Learning"></a>Data Augmentation for Contrastive Representation Learning</h3><h4 id="Data-augmentation-defines-predictive-tasks"><a href="#Data-augmentation-defines-predictive-tasks" class="headerlink" title="Data augmentation defines predictive tasks"></a>Data augmentation defines predictive tasks</h4><p>数据增强定义预预测任务。</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/random_crop.png"></p>
<p>随机裁剪既包括了 global and local views, 也包括了 adjacent views.</p>
<h4 id="Composition-of-data-augmentation-operations-is-crucial-for-learning-good-representations"><a href="#Composition-of-data-augmentation-operations-is-crucial-for-learning-good-representations" class="headerlink" title="Composition of data augmentation operations is crucial for learning good representations"></a>Composition of data augmentation operations is crucial for learning good representations</h4><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/data_aug_method.png"></p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/data_augmentation.png"></p>
<p>为了验证不同的数据增强对于表示学习的影响，作者进行了ablation实验，只对图2中的某一分支进行transformation. 实验结果如图5所示，对角线只有一种augmentation方法，非对角线是两种组合。</p>
<p>结果表明，单一的增强方法都不能学到好的表示。两种组合时，预测任务越难，学习到的表示能力越好。最好的组合是 random crop 和 color distortion. 但是只是单独用其中某一种效果都不好。</p>
<p>作者对只用单独一种数据增强方法不好的原因进行了解释：</p>
<blockquote>
<ol>
<li>We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution.</li>
<li>Figure 6 shows that color histograms alone suffice to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features.</li>
</ol>
</blockquote>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/distortion.png"></p>
<h4 id="Contrastive-learning-needs-stronger-data-augmentation-than-supervised-learning"><a href="#Contrastive-learning-needs-stronger-data-augmentation-than-supervised-learning" class="headerlink" title="Contrastive learning needs stronger data augmentation than supervised learning"></a>Contrastive learning needs stronger data augmentation than supervised learning</h4><p>相比监督学习，stronger数据增强对contrastive learning更为重要。</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/vs_supervised.png"></p>
<blockquote>
<p>When training supervised models with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning.</p>
</blockquote>
<h3 id="Architectures-for-Encoder-and-Head"><a href="#Architectures-for-Encoder-and-Head" class="headerlink" title="Architectures for Encoder and Head"></a>Architectures for Encoder and Head</h3><h4 id="Unsupervised-contrastive-learning-benefits-more-from-bigger-models"><a href="#Unsupervised-contrastive-learning-benefits-more-from-bigger-models" class="headerlink" title="Unsupervised contrastive learning benefits (more) from bigger models"></a>Unsupervised contrastive learning benefits (more) from bigger models</h4><p>对比学习在大模型下获益更多。</p>
<h4 id="A-nonlinear-projection-head-improves-the-representation-quality-of-the-layer-before-it"><a href="#A-nonlinear-projection-head-improves-the-representation-quality-of-the-layer-before-it" class="headerlink" title="A nonlinear projection head improves the representation quality of the layer before it"></a>A nonlinear projection head improves the representation quality of the layer before it</h4><p>作者探究了projection的三种方式：  </p>
<ul>
<li>identity mapping  </li>
<li>linear projection  </li>
<li>non-linear projection  </li>
</ul>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/projection.png"></p>
<p>结果表明，非线性projection更好，没有的话效果很差。</p>
<p>除此之外，使用project head之前的hidden layer $h(i)$ 比project layer之后的表示 $z(i)$ 效果更好, $\ge 10%$。</p>
<ol>
<li>为什么使用 non-linear projection head 之前的hidden layer效果更好？<br> 作者认为对比loss会损失信息。z = g(h) 被训练成transformation invariant 变换不变性（因为contrast loss要拉近两个不同变换的正样本）。因此，$g(\cdot)$ 会丢失信息。<br> 作者通过实验验证这一猜想，在保证最终的dimension不变的情况下，<br><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/before_head.png"></li>
</ol>
<h3 id="Loss-Functions-and-Batch-Size"><a href="#Loss-Functions-and-Batch-Size" class="headerlink" title="Loss Functions and Batch Size"></a>Loss Functions and Batch Size</h3><h4 id="Normalized-cross-entropy-loss-with-adjustable-temperature-works-better-than-alternatives"><a href="#Normalized-cross-entropy-loss-with-adjustable-temperature-works-better-than-alternatives" class="headerlink" title="Normalized cross entropy loss with adjustable temperature works better than alternatives"></a>Normalized cross entropy loss with adjustable temperature works better than alternatives</h4><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss.png"></p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/loss_res.png"></p>
<p>实验表明 NT-Xent 效果最好。这是因为其他的目标函数并没有衡量负样本的难度：unlike cross-entropy, other objective functions do not weight the negatives by their relative hardness.</p>
<p>$l_2$ normalization 和 temperture 很重要：$l_2$ normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives;</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/normalization.png"></p>
<ul>
<li>没有 $l_2$ normalization,尽管对比准确率很高，但是学习到的表示能力并不好。  </li>
<li>合适的temperture也很重要</li>
</ul>
<h4 id="Contrastive-learning-benefits-more-from-larger-batch-sizes-and-longer-training"><a href="#Contrastive-learning-benefits-more-from-larger-batch-sizes-and-longer-training" class="headerlink" title="Contrastive learning benefits (more) from larger batch sizes and longer training"></a>Contrastive learning benefits (more) from larger batch sizes and longer training</h4><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/batch_size.png"></p>
<p>实验表明，batch size很重要，越大收敛的越快，但最终效果也不是越大越好。随着训练的增加，batch size造成的表现差异也随着逐渐消失。</p>
<h3 id="Comparison-with-State-of-the-art"><a href="#Comparison-with-State-of-the-art" class="headerlink" title="Comparison with State-of-the-art"></a>Comparison with State-of-the-art</h3><p>作者采用了三种方法来验证performance。</p>
<h4 id="Linear-evaluation"><a href="#Linear-evaluation" class="headerlink" title="Linear evaluation"></a>Linear evaluation</h4><p>相比fine-tune，linear evaluation 的区别在于学习率的设置。</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/linear_evaluation.png"></p>
<blockquote>
<p>没搞懂为啥叫 linear evaluation？ 和fine-tune的区别就在于学习率的设置？</p>
</blockquote>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/linear_eval.png"></p>
<h4 id="Semi-supervised-learning"><a href="#Semi-supervised-learning" class="headerlink" title="Semi-supervised learning"></a>Semi-supervised learning</h4><p>sample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (∼12.8 and ∼128 images per class respectively).</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/few_label.png"></p>
<h4 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h4><p>在imageNet上训练，在其他数据集上测试。同上，采用了两种方式， Linear evaluation 和 fine-tune.</p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/transfer_learn.png"></p>
<h1 id="speech-based"><a href="#speech-based" class="headerlink" title="speech-based"></a>speech-based</h1><h2 id="Representation-Learning-with-Contrastive-Predictive-Coding"><a href="#Representation-Learning-with-Contrastive-Predictive-Coding" class="headerlink" title="Representation Learning with Contrastive Predictive Coding"></a>Representation Learning with Contrastive Predictive Coding</h2><h2 id="wav2vec-Unsupervised-pre-training-for-speech-recognition"><a href="#wav2vec-Unsupervised-pre-training-for-speech-recognition" class="headerlink" title="wav2vec: Unsupervised pre-training for speech recognition"></a>wav2vec: Unsupervised pre-training for speech recognition</h2><p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/wav2vec.png"></p>
<ul>
<li>encoder: $X \rightarrow Z$  </li>
<li>content-network: $C\rightarrow C$</li>
</ul>
<p><strong>noise contrastive binary classification task.</strong></p>
<p><img src="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/wav2vec_loss.png"></p>
<ul>
<li>$\sigma$ 是 sigmoid 函数</li>
<li>$c_i$ 是当前step的content feature  </li>
<li>$h_k(x_i)$ 是step-specific affine transformation $h_k(c_i) = W_kc_i+b_k$  </li>
<li>$z_{i+k}$ 是距离当前step为k的encoded feature, 为正样本  </li>
<li>$\hat z\sim p_n$是 (T-k) 帧中均匀选择10个负样本，得到对数概率的期望后，再乘以 $\lambda=10$.</li>
</ul>
<p>[^1]: A fast learning algorithm for deep belief nets.<br>[^2]: Auto-encoding variational bayes.<br>[^3]: Generative adversarial nets. NIPS2014<br>[^4]: Discriminative unsupervised feature learning with convolutional neural networks. NIPS2014<br>[^5]: Representation learning with contrastive predictive coding. arXiv2018<br>[^6]: Learning representations by maximizing mutual information across views. NIPS2019<br>[^7]: CPC: Data-efficient image recognition with contrastive predictive coding, arXiv2019<br>[^8]: Unsupervised feature learning via non-parametric instance discrimination. CVPR2018<br>[^9]: Contrastive multiview coding. arXiv2019<br>[^10]: MoCo: Momentum contrast for unsupervised visual representation learning, arXiv2019<br>[^11]: Self-supervised learning of pretext-invariant representations. arXiv2019   </p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Gepostet vor&nbsp;<time dateTime="2021-04-29T01:07:08.000Z" title="2021/4/29 上午9:07:08">2021-04-29</time></span><span class="level-item">Aktualisiert vor&nbsp;<time dateTime="2021-06-29T05:19:29.482Z" title="2021/6/29 下午1:19:29">2021-06-29</time></span><span class="level-item">a minute lesen (Über 153 Wörter)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></h1><div class="content"><p>paper list:  </p>
<ul>
<li>Training data-efficient image transformers &amp; distillation through attention.</li>
<li>An image is worth 16x16 words: Transformers for image recognition at scale.  </li>
<li>ViViT: A Video Vision Transformer.    </li>
<li>Is space-time attention all you need for video understanding  </li>
<li>Video transformer network.  </li>
</ul>
<h2 id="DeiT"><a href="#DeiT" class="headerlink" title="DeiT"></a>DeiT</h2><h2 id="ViViT-A-Video-Vision-Transformer"><a href="#ViViT-A-Video-Vision-Transformer" class="headerlink" title="ViViT: A Video Vision Transformer"></a>ViViT: A Video Vision Transformer</h2><p>作者通过大量实验来研究 vision transformer，并探索最合适的且efficient的结构使其能适用于small datasets. 其中，主要包括以下三个方面的探索：    </p>
<ul>
<li>tokenization strategies  </li>
<li>model architecture  </li>
<li>regularisation methods.</li>
</ul>
<p>transformer-based architecture的缺点：相比卷积网络，transformer缺乏了一定的归纳偏置能力，比如平移不变性。因此，需要大量的数据来学习。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Gepostet vor&nbsp;<time dateTime="2021-04-29T01:05:10.000Z" title="2021/4/29 上午9:05:10">2021-04-29</time></span><span class="level-item">Aktualisiert vor&nbsp;<time dateTime="2021-06-29T05:19:29.563Z" title="2021/6/29 下午1:19:29">2021-06-29</time></span><span class="level-item">3 minutes lesen (Über 470 Wörter)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></h1><div class="content"><p>paper list:   </p>
<ul>
<li>CARAFE: Content-Aware ReAssembly of FEatures  </li>
<li>Involution: Inverting the Inherence of Convolution for Visual Recognition  </li>
<li>Pay less attention with lightweight and dynamic convolutions  </li>
<li>ConvBERT: Improving BERT with Span-based Dynamic Convolution   </li>
<li>Dynamic Region-Aware Convolution  </li>
</ul>
<h1 id="Involution"><a href="#Involution" class="headerlink" title="Involution"></a>Involution</h1><h2 id="为什么要叫反卷积呢？"><a href="#为什么要叫反卷积呢？" class="headerlink" title="为什么要叫反卷积呢？"></a>为什么要叫反卷积呢？</h2><p>卷积的特性是：  </p>
<ul>
<li>space-agnostic：空间不变性，也就是用一个kernel在feature map上滑动。这样学的到feature是单一的为了增加feature的丰富性，采用很大的channel  </li>
<li>channel-specific: 通道特异性。尽管channel增加能学到更多特征，但是通道太大其实是有冗余的，有人做低秩实验，发现很多channel对应的参数是线性相关的  </li>
</ul>
<p>于是，作者设计了一个与卷积完全相反的算子，反卷积：</p>
<ul>
<li>space-specific: 根据content生成相应的weights    </li>
<li>channel-agnostic: 共享参数。类似attention的projection和feedforward.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 1. 生成pixel-wise对应的权重，每个pixel对应的权重是 [kernel_size^2*group].</span></span><br><span class="line">        <span class="comment"># print(&quot;after avgpool: &quot;, self.avgpool(x).shape)</span></span><br><span class="line">        <span class="comment"># print(&quot;after conv1: &quot;, self.conv1(x if self.stride == 1 else self.avgpool(x)).shape)</span></span><br><span class="line">        weight = self.conv2(self.conv1(x <span class="keyword">if</span> self.stride == <span class="number">1</span> <span class="keyword">else</span> self.avgpool(x)))</span><br><span class="line">        <span class="comment"># print(&quot;weight: &quot;, weight.shape)</span></span><br><span class="line">        b, c, h, w = weight.shape</span><br><span class="line">        weight = weight.view(b, self.groups, self.kernel_size**<span class="number">2</span>, h, w).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;weight: &quot;</span>, weight.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 将x通过unfold以kernel_size为大小，stride为步长i女性展开</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;after unfold: &quot;</span>, self.unfold(x).shape) <span class="comment"># [bs, channel*kernel*2, ((h-kernel+1+2*pad)/stride))^2]</span></span><br><span class="line">        out = self.unfold(x).view(b, self.groups, self.group_channels, self.kernel_size**<span class="number">2</span>, h, w)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;out: &quot;</span>, out.shape)</span><br><span class="line">        out = (weight * out).<span class="built_in">sum</span>(dim=<span class="number">3</span>).view(b, self.channels, h, w)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>


<ul>
<li><p>思路很像local attention, 区别在于这个weight是通过content+linear得到的，而不是通过pixel之间的relation得到的。而且，看源代码这个weights并没有做normalization.</p>
</li>
<li><p>然后把生成的weights与对应的pixel周围的[kernel,kernel]的pixels进行加权求和。</p>
</li>
</ul>
<p><img src="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/involution_fig.png"></p>
<p><img src="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/involution.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Gepostet vor&nbsp;<time dateTime="2021-04-16T07:48:48.000Z" title="2021/4/16 下午3:48:48">2021-04-16</time></span><span class="level-item">Aktualisiert vor&nbsp;<time dateTime="2021-06-29T05:19:29.575Z" title="2021/6/29 下午1:19:29">2021-06-29</time></span><span class="level-item">8 minutes lesen (Über 1205 Wörter)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></h1><div class="content"><p>基于标准的似然训练的语言模型在解码时会出现很dull的重复问题。如下图所示：</p>
<p><img src="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/repetition_gpt.png"></p>
<p>诚然将likelihood作为训练目标，能得到强大的具有语言理解能力的模型。但是将似然作为目标会导致语言的平淡和奇怪的重复。而且基于强大的GPT2-117M，使用beam search，理论上beam size越大，生成的句子概率越大，效果越好才对。但事实却是反直觉的，beam size越大，语言的退化效果更明显。（我猜这也是很多时候beam size设为10左右，而不会更大。）</p>
<p>Holtzman[^1] 揭示了语言模型的这种退化现象，他们将机器生成的语言和人类语言进行对比，如下图所示，人类文本在生成每个token的困惑度中表现出相当大的波动，而由最大似然解码产生的机器文本的分布则出现不自然的平坦和较高的token概率.</p>
<p><img src="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/human_vs_machine.png"></p>
<p>事实上，语言模型在生成词的时候，大部分的概率集中在几百个tokens上。这限制了机器文本的多样性，也是导致模型在长文本生成退化的原因。基于此，如下图所示，top-k, nucleus sampling是不错的方法。</p>
<p><img src="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/repetition.png"></p>
<p>但是sampling的方法并没有改变模型生成的概率其本身。生成文本退化的真正原因还未知，已有的论文认为有以下几种可能：</p>
<ul>
<li>模型架构选择的by-product，例如 Transformer 更喜欢重复;  </li>
<li>人类语言的内在属性，而不是模型缺陷；  </li>
<li>语料有限;</li>
</ul>
<p>相比之下，Welleck[^2] 认为生成模型以最大似然最为训练目标会导致文本生成退化。其原因有：  </p>
<ul>
<li>将注意力集中在argmax或top-k,而不是优化整个distribution  </li>
<li>只是集中于下一个token的生成，而不是整个句子的优化  </li>
</ul>
<p>为此，Welleck[^2] 提出了 unlikelihood training:  </p>
<ul>
<li>依据likelihood来优化target tokens，并给予较大的概率  </li>
<li>依据unlikelihood来更新，避免给予target tokens太大的概率。（不知理解的对错，原文如下）</li>
</ul>
<blockquote>
<p>Unlikelihood training works by combining two types of updates: a likelihood update on the true target tokens so they are assigned high probability, and an unlikelihood update on tokens that are otherwise assigned too high a probability.</p>
</blockquote>
<h3 id="unlikelihood-loss"><a href="#unlikelihood-loss" class="headerlink" title="unlikelihood loss"></a>unlikelihood loss</h3><p>unlikelihood loss的核心就是降低negative condidates $C^t$ 的似然概率。</p>
<p><img src="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/unlikelihood_loss.png"></p>
<h4 id="token-level-unlikelihood-loss"><a href="#token-level-unlikelihood-loss" class="headerlink" title="token-level unlikelihood loss"></a>token-level unlikelihood loss</h4><p>以自回归的语言模型为例，下一个时间步的loss计算包括target的似然最大化，以及negative candidates的似然最小化。</p>
<p><img src="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/token_level.png"></p>
<p>negative candidates 是当前试了之前的词汇。</p>
<h4 id="sequence-level-unlikelihood-loss"><a href="#sequence-level-unlikelihood-loss" class="headerlink" title="sequence_level unlikelihood loss"></a>sequence_level unlikelihood loss</h4><p>我们知道基于自回归模型的训练和解码是存在exposure bias的，也就是解码的时候有误差累积。其实这种distribution mismatch是maxmimum-likelihood追求当前时刻的概率最大话。而没有从整个句子的层面去考虑。比如重复问题，你上一个词出现过了，下一个词还出现它；你这句话说过一遍了，你还要再说一遍。。这不离谱吗。但是模型就是这么傻，或者说模型没有大局观的原因是之前的优化都是在token-level层面。</p>
<p>因此，Welleck[^2] 提出了sequence-level unlikelihood loss.</p>
<p><img src="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/sequence-level.png"></p>
<p>这个公式看起来跟前面一样，但是区别在于 negative condidates的选择。这里的negative是从n-gram层面来考量的。</p>
<p><img src="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/n_gram.png"></p>
<p>也就是对于当前时间步，如果它是重复的n-gram的一部分，那么它就是negative candidate.  </p>
<p>这里有点不太好理解，token-level就是把之前出现的词作为negative candidate. 如果有些词在前面并没有出现，但它的出现会导致重复的n-gram？这不合理啊，它都没出现，怎么可能出现包含它的n-gram呢？？ 这样想 sequence-level 不就是 token-level的一种特殊形式。</p>
<p>带着疑问去看代码吧。看完代码，sequence-level是在训练完token-level之后，再进行finetune，对导致出现重复的ngram的某个time-step进行惩罚。。在我的实验上不太靠谱，重复反而变多了</p>
<p>[^1]: The Curious Case of Neural Text Degeneration<br>[^2]: Neural text degeneration with unlikelihood training  </p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Gepostet vor&nbsp;<time dateTime="2021-04-12T08:16:35.000Z" title="2021/4/12 下午4:16:35">2021-04-12</time></span><span class="level-item">Aktualisiert vor&nbsp;<time dateTime="2021-06-29T05:19:29.449Z" title="2021/6/29 下午1:19:29">2021-06-29</time></span><span class="level-item">15 minutes lesen (Über 2186 Wörter)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/">论文笔记-DETR and Deformable DETR</a></h1><div class="content"><ul>
<li>DETR: End-to-End Object Detection with Transformers  </li>
<li>Deformable DETR： Deformable Transformer for End-to-End Object Detection</li>
</ul>
<h2 id="DETR-End-to-End-Object-Detection-with-Transformers"><a href="#DETR-End-to-End-Object-Detection-with-Transformers" class="headerlink" title="DETR: End-to-End Object Detection with Transformers"></a>DETR: End-to-End Object Detection with Transformers</h2><p>paper link: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.12872">https://arxiv.org/abs/2005.12872</a></p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>传统的目标检测范式：</p>
<ul>
<li>使用CNN Backbone提取feature map;  </li>
<li>使用 Region Proposal Network (RPN) 在feature map上枚举所有的windows,输出N个候选boxes  </li>
<li>使用分类器对每个box进行分类  </li>
</ul>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/faster_rcnn.png"></p>
<p>这样存在很多问题：</p>
<p><strong>问题1：Enumerate candidate boxes in RPN</strong></p>
<ul>
<li>枚举所有的pixel  </li>
<li>对每个pixel，枚举出所有预定义大小的boxes  </li>
<li>大部分的候选框都是无效的，因此 Inefficient, slow<br><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/rpn.png"></li>
</ul>
<p><strong>问题2：Redundant boxes and NMS</strong></p>
<ul>
<li>RPN 输出大量的boxes  </li>
<li>Non-maximum suppression (NMS) merges/removes redundant boxes  </li>
<li>These hand-designed components have a few hyperparameters    </li>
<li>Model tuning is complex<br><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/nms.png"></li>
</ul>
<h3 id="Architecture-of-DETR"><a href="#Architecture-of-DETR" class="headerlink" title="Architecture of DETR"></a>Architecture of DETR</h3><p>因此，作者提出了更为简单的end-to-end的目标检测模型：<br><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/detr_vs_fast_rcnn.png"></p>
<h4 id="Self-attention-vs-Convolution-in-Vision"><a href="#Self-attention-vs-Convolution-in-Vision" class="headerlink" title="Self-attention vs. Convolution in Vision"></a>Self-attention vs. Convolution in Vision</h4><p>Transformer 已经非常熟悉了，就不详细介绍了。这里对比下 self-attention 和 CNN 在视觉领域的应用</p>
<ul>
<li>每个pixel可以看作是自然语言中的 token  </li>
<li>convolution is used to integrate pixels  <ul>
<li>Recognize patterns within a small window of pixels  </li>
<li>Difficult to integrate non-local pixels  </li>
<li>Have to make network very deep to “see the big picture”  </li>
</ul>
</li>
<li>Self-attention (transformer) also integrates multiple pixels  <ul>
<li>Works when the correlated pixels are non-local  </li>
<li>Trunk, tail, legs of an elephant to a whole elephant  </li>
</ul>
</li>
</ul>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/elephant.png"></p>
<h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/detr_arch.png"></p>
<p>整个结构可以看作四部分：</p>
<ul>
<li>CNN backbone  </li>
<li>Transformer Encoder  </li>
<li>Transformer Decoder  </li>
<li>bipartite matching loss</li>
</ul>
<h5 id="CNN-backbone-Transformer-Encoder"><a href="#CNN-backbone-Transformer-Encoder" class="headerlink" title="CNN backbone + Transformer Encoder"></a>CNN backbone + Transformer Encoder</h5><p>前两个好理解，需要注意的是 position encoding 与NLP中不同。考虑到输入是二维图像，对应的位置 (x,y) 也是二维的。</p>
<p>总结下和原始transformer编码器不同的地方：  </p>
<ul>
<li>输入编码器的位置编码需要考虑2-D空间位置。  </li>
<li>位置编码向量需要加入到每个Encoder Layer中。  </li>
<li>在编码器内部位置编码Positional Encoding仅仅作用于Query和Key，即只与Query和Key相加，Value不做任何处理。</li>
</ul>
<h4 id="Transformer-Decoder"><a href="#Transformer-Decoder" class="headerlink" title="Transformer Decoder"></a>Transformer Decoder</h4><p>decoder与原始的transformer decoder的区别在于两点：  </p>
<ul>
<li>其输入是 Object queries. 是可学习的 nn.Embedding, 维度为 [100, bsz, 256]. 其实可以理解成可学习的位置编码。  </li>
<li>非自回归的可并行解码。</li>
</ul>
<h4 id="bipartite-matching-loss"><a href="#bipartite-matching-loss" class="headerlink" title="bipartite matching loss"></a>bipartite matching loss</h4><p>decoder 的输出张量的维度是 分类分支：[bsz, 100, class + 1] 和回归分支：[bsz, 100, 4]. 其中 class + 1 表示类别总数+背景。有物体的boxes计算回归任务；没有物体的背景框，则不用回归。</p>
<p><strong>问题来了：</strong><br>这100个候选框如何去与 不确定数量的 targets 进行匹配呢，预测框和真值是怎么一一对应的？换句话说：你怎么知道第47个预测框对应图片里的狗，第88个预测框对应图片里的车？等等。</p>
<blockquote>
<p>相比Faster R-CNN等做法，DETR最大特点是将目标检测问题转化为无序集合预测问题(set prediction)。论文中特意指出Faster R-CNN这种设置一大堆anchor，然后基于anchor进行分类和回归其实属于代理做法即不是最直接做法，目标检测任务就是输出无序集合，而Faster R-CNN等算法通过各种操作，并结合复杂后处理最终才得到无序集合属于绕路了，而DETR就比较纯粹了。现在核心问题来了：输出的 [bsz, 100] 个检测结果是无序的，如何和 ground-truth bounding box 计算loss？这就需要用到经典的双边匹配算法了，也就是常说的 <strong>匈牙利算法</strong>，该算法广泛应用于最优分配问题。</p>
</blockquote>
<p>整个loss function的计算分为两个步骤：  </p>
<ul>
<li><ol>
<li>依据匈牙利算法先找到最优的匹配  </li>
</ol>
</li>
<li><ol start="2">
<li>根据找到的匹配，计算最终的loss  </li>
</ol>
</li>
</ul>
<p><strong>如何找到最优的匹配呢？</strong><br>假设一张图片有3个目标：dog, cat, car. 那么我们可以得到一个 [100, 3] 的矩阵, 矩阵的值分别表示这100个候选token为其中某一目标的“损失”. 让这个损失最小的匹配就是我们要找的最优匹配。这个过程就是匈牙利算法。</p>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/loss.png"></p>
<p>再介绍匈牙利算法之前，我们要知道这个矩阵对应的损失值是啥呢？也就是怎么衡量这个候选token是某个目标的损失值？</p>
<p>假设 $y_i$ 对应的预测是 $\hat y_{\sigma_i}$,那么这个构成这个搭配的损失就是 $\hat y_{\sigma_i}$ 对应的类别为 $c_i$ 以及 box 的 mse 值。</p>
<p>$$L_{match}(y_i, \hat y_{\sigma(i)})=-\mathbb{1}<em>{c_i\ne \varnothing}\hat p</em>{\sigma(i)}(c_i) + 1_{c_i\ne \varnothing}L_{box}(b_i, \hat b_{\sigma_i})$$</p>
<p>$L_{match}$ 就是矩阵中的元素。然后通过匈牙利算法找到对应的匹配。</p>
<p><strong>找到匹配之后计算最终的loss</strong></p>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/loss_last.png"></p>
<h2 id="Deformable-DETR"><a href="#Deformable-DETR" class="headerlink" title="Deformable DETR"></a>Deformable DETR</h2><p>DETR 的优缺点：</p>
<ul>
<li>优点<ol>
<li>纯端到端，不需要手工设计的NMS之类</li>
</ol>
</li>
<li>DETR 缺点：<ol>
<li>小目标检测的低精度，高精度的feature map的复杂度是DETR所不能忍受的。<ol>
<li>这是因为 attention 机制的原因，复杂度是 O(L^2)</li>
</ol>
</li>
<li>收敛速度慢，slow convergence。原因是 pattern 是稀疏的，很难快速学到吧<ol>
<li>when $N_k$ is large, it will lead $N_k$<br>to ambiguous gradients for input features. Thus, long training schedules are required so that the attention weights can focus on specific keys.</li>
</ol>
</li>
</ol>
</li>
</ul>
<p>Deformable DETR 的设计初衷：</p>
<ol>
<li>motivation: mitigates the slow convergence and high complexity issues of DETR</li>
</ol>
<ul>
<li>具体设计方法<ol>
<li>融合Deformable conv的空间稀疏采样和transformer的关系建模的优势 combines the best of the sparse spatial sampling of deformable convo- lution, and the relation modeling capability of Transformers</li>
</ol>
</li>
<li>deformable attention modules： 代替Transformer attention module</li>
</ul>
<h3 id="Multi-Head-Attention-in-Transformers"><a href="#Multi-Head-Attention-in-Transformers" class="headerlink" title="Multi-Head Attention in Transformers"></a>Multi-Head Attention in Transformers</h3><p>常规的 attention:</p>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/attention.png"></p>
<ul>
<li>$A_{mqk}$ 是attention matrix.   </li>
<li>$z_q\in R^C$ 是query feature.  </li>
<li>$x_k\in R^C$ 是key/value feature. $\Omega_k$ 是key index的集合.   </li>
<li>$m$ 是attention head index.  </li>
</ul>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/computation.png"></p>
<p>总的计算量是: $O(N_qC^2) + 2O(N_kC^2) + 2O(N_qN_kC^2)$</p>
<p>复杂度：$O(N_qC^2 + N_kC^2 + N_qN_kC)$</p>
<p>在长文本，或者image patched之后，$N_q=N_k &gt;&gt; C$. 所以复杂度取决于 $O(N_qN_kC)$</p>
<h3 id="Deformable-attention-module"><a href="#Deformable-attention-module" class="headerlink" title="Deformable attention module"></a>Deformable attention module</h3><p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/deformable_attention.png"></p>
<p>相比常规的attention, 需要考虑所有的key features，也就是 HW 个。 deformable attention只考虑K个keys. 如下面两个公式所示 $\sum_{k\in \Omega_k} \rightarrow \sum_{k=1}^K$. 因此复杂度会大大降低，收敛速度也会加快。</p>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/attention.png"></p>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/deformable_attention_formulation.png"></p>
<p>其中：  </p>
<ul>
<li>m 表示attention head index  </li>
<li>$z_q$ 表示query feature  </li>
<li>$K &lt;&lt; HW$ 表示sampling number  </li>
<li>$p_q$ 表示 $z_q$ 对应的参考2D点  </li>
<li>$\Delta p_{mqk}$ 表示相对参考点的sampling offset  </li>
<li>$A_{mqk}$ 则是对应采样点的weights, $\sum_{k=1}^{K}A_{mqk}=1$  </li>
</ul>
<p>现在的问题在于，这K个keys是怎么选择的呢？也就是 offset $\Delta p_{mqk}$ 是怎么来的？文中是这么解释的：</p>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/k_selector.png"></p>
<p>这个过程如图2所示：  </p>
<ul>
<li>通过 linear projection 得到 2D 实数值 $\Delta p_{mqk}$， 然后通过线性插值得到 $p_q + \Delta p_{mqk}$.    </li>
<li>对应的权重 $A_{mqk}$ 也是通过linear projection得到的。</li>
</ul>
<p>这样 deformable attention 的复杂度：$N_qC^2 + O(N_qKC)$.</p>
<h3 id="Multi-scale-Deformable-Attention-Module"><a href="#Multi-scale-Deformable-Attention-Module" class="headerlink" title="Multi-scale Deformable Attention Module"></a>Multi-scale Deformable Attention Module</h3><p>Deformable attention 可以很自然的拓展到multi-scale情况下。</p>
<p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/multi_scale_formulation.png">  </p>
<ul>
<li>m 表示 attention head index  </li>
<li>l 表示 input feature level  </li>
<li>k 表示 sampling index   </li>
<li>$A_{mlqk}$ 表示 $k^{th}$ sampling point在 $m^{th}$ head和 $l^{th}$ level的权重, $\sum_{l=1}^{L}\sum_{k=1}^{K}A_{mlqk}=1$.  </li>
<li>$\Delta p_{mlqk}$ 表示sampling offset  </li>
</ul>
<p>为了确保multi-scale中 point的对应关系，作者用 normalized coordinates $\hat p_q\in [0,1]^2$ 来表示参考点。 $(0,0), (1,1)$ 表示top-left和bottom-right. $\phi_l(\hat p_q)$ 表示将归一化之后的 $\hat p_q$ rescale 到 $l^{th}$ level feature中。</p>
<h3 id="Deformable-Transformer-Encoder"><a href="#Deformable-Transformer-Encoder" class="headerlink" title="Deformable Transformer Encoder"></a>Deformable Transformer Encoder</h3><p>从 resnet $C_3$ 到 $C_5$ 抽取multi-scale特征图. 其中 $C_l$ 表示分辨率是输入图像的 $\dfrac{1}{2^l}$. 这样就有3层feature map了，然后最后一层feature map是通过 kernel=$3\times 3$, stride=$3$ 的卷积在 $C_5$ 上得到的。 总共4 levels feature map.</p>
<p>key和query来自feature map中的pixels. 对于每一个query pixel, 其reference point是其本身。除了位置编码之外，作者还加入了level编码 $e_l$，用来表示在哪一层level. 位置编码是固定的，level编码是可训练的。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Zurück</a></div><div class="pagination-next"><a href="/page/2/">Weiter</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/7/">7</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Seiten</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Kategorien</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">8</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Folgen</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Kategorien</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Letzte Einträge</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-05T02:03:16.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-contrastive learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:07:08.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:05:10.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-16T07:48:48.000Z">2021-04-16</time></p><p class="title"><a href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-12T08:16:35.000Z">2021-04-12</time></p><p class="title"><a href="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/">论文笔记-DETR and Deformable DETR</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archive</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Abonnieren Sie Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Abonnieren"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("default");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Zurück nach oben" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "Diese Website verwendet Cookies, um Ihre Erfahrung zu verbessern.",
          dismiss: "Verstanden!",
          allow: "Cookies zulassen",
          deny: "Ablehnen",
          link: "Mehr erfahren",
          policy: "Cookie-Richtlinie",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Tippen Sie etwas..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Tippen Sie etwas...","untitled":"(Ohne Titel)","posts":"Seiten","pages":"Pages","categories":"Kategorien","tags":"Tags"});
        });</script></body></html>