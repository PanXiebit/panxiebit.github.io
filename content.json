{"pages":[{"title":"leetcode","text":"Link: æ•°æ®ç»“æ„å’Œç®—æ³•å­¦ä¹ æŒ‡å— (qq.com) æ‰‹æŠŠæ‰‹åˆ·é“¾è¡¨é¢˜ç›® (å­¦å®Œä¸€æ®µæ—¶é—´ä¹‹åï¼Œéœ€è¦å†å›è¿‡å¤´æ¥å†çœ‹ä¸€é) æ•°æ®ç»“æ„çš„å­˜å‚¨æ–¹å¼ æ•°ç»„ï¼ˆé¡ºåºå­˜å‚¨ï¼‰ï¼šç´§å‡‘è¿ç»­å­˜å‚¨,å¯ä»¥éšæœºè®¿é—®ï¼Œé€šè¿‡ç´¢å¼•å¿«é€Ÿæ‰¾åˆ°å¯¹åº”å…ƒç´ ï¼Œè€Œä¸”ç›¸å¯¹èŠ‚çº¦å­˜å‚¨ç©ºé—´ã€‚ä½†æ­£å› ä¸ºè¿ç»­å­˜å‚¨ï¼Œå†…å­˜ç©ºé—´å¿…é¡»ä¸€æ¬¡æ€§åˆ†é…å¤Ÿï¼Œæ‰€ä»¥è¯´æ•°ç»„å¦‚æœè¦æ‰©å®¹ï¼Œéœ€è¦é‡æ–°åˆ†é…ä¸€å—æ›´å¤§çš„ç©ºé—´ï¼Œå†æŠŠæ•°æ®å…¨éƒ¨å¤åˆ¶è¿‡å»ï¼Œæ—¶é—´å¤æ‚åº¦ O(N)ï¼›è€Œä¸”ä½ å¦‚æœæƒ³åœ¨æ•°ç»„ä¸­é—´è¿›è¡Œæ’å…¥å’Œåˆ é™¤ï¼Œæ¯æ¬¡å¿…é¡»æ¬ç§»åé¢çš„æ‰€æœ‰æ•°æ®ä»¥ä¿æŒè¿ç»­ï¼Œæ—¶é—´å¤æ‚åº¦ O(N)ã€‚ é“¾è¡¨ï¼ˆé“¾å¼å­˜å‚¨ï¼‰ï¼šå…ƒç´ ä¸è¿ç»­ï¼Œè€Œæ˜¯é æŒ‡é’ˆæŒ‡å‘ä¸‹ä¸€ä¸ªå…ƒç´ çš„ä½ç½®ï¼Œæ‰€ä»¥ä¸å­˜åœ¨æ•°ç»„çš„æ‰©å®¹é—®é¢˜ï¼›å¦‚æœçŸ¥é“æŸä¸€å…ƒç´ çš„å‰é©±å’Œåé©±ï¼Œæ“ä½œæŒ‡é’ˆå³å¯åˆ é™¤è¯¥å…ƒç´ æˆ–è€…æ’å…¥æ–°å…ƒç´ ï¼Œæ—¶é—´å¤æ‚åº¦ O(1)ã€‚ä½†æ˜¯æ­£å› ä¸ºå­˜å‚¨ç©ºé—´ä¸è¿ç»­ï¼Œä½ æ— æ³•æ ¹æ®ä¸€ä¸ªç´¢å¼•ç®—å‡ºå¯¹åº”å…ƒç´ çš„åœ°å€ï¼Œæ‰€ä»¥ä¸èƒ½éšæœºè®¿é—®ï¼›è€Œä¸”ç”±äºæ¯ä¸ªå…ƒç´ å¿…é¡»å­˜å‚¨æŒ‡å‘å‰åå…ƒç´ ä½ç½®çš„æŒ‡é’ˆï¼Œä¼šæ¶ˆè€—ç›¸å¯¹æ›´å¤šçš„å‚¨å­˜ç©ºé—´ã€‚ åˆ·é¢˜æ–¹æ³•æ•°æ®ç»“æ„çš„åŸºæœ¬å­˜å‚¨æ–¹å¼å°±æ˜¯é“¾å¼å’Œé¡ºåºä¸¤ç§ï¼ŒåŸºæœ¬æ“ä½œå°±æ˜¯å¢åˆ æŸ¥æ”¹ï¼Œéå†æ–¹å¼æ— éè¿­ä»£å’Œé€’å½’ã€‚ åˆ·ç®—æ³•é¢˜å»ºè®®ä»ã€Œæ ‘ã€åˆ†ç±»å¼€å§‹åˆ·ï¼Œç»“åˆæ¡†æ¶æ€ç»´ï¼ŒæŠŠè¿™å‡ åé“é¢˜åˆ·å®Œï¼Œå¯¹äºæ ‘ç»“æ„çš„ç†è§£åº”è¯¥å°±åˆ°ä½äº†ã€‚è¿™æ—¶å€™å»çœ‹å›æº¯ã€åŠ¨è§„ã€åˆ†æ²»ç­‰ç®—æ³•ä¸“é¢˜ï¼Œå¯¹æ€è·¯çš„ç†è§£å¯èƒ½ä¼šæ›´åŠ æ·±åˆ»ä¸€äº›ã€‚ æ‰‹æŠŠæ‰‹åˆ·é“¾è¡¨é¢˜ç›® é€’å½’åè½¬é“¾è¡¨ï¼šå¦‚ä½•æ‹†è§£å¤æ‚é—®é¢˜ å•é“¾è¡¨çš„å…­å¤§è§£é¢˜å¥—è·¯ Leetcode21 åˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨ è™šæ‹Ÿå¤´èŠ‚ç‚¹ Leetcode23 åˆå¹¶kä¸ªå‡åºé“¾è¡¨ å…³é”®åœ¨äºå¦‚ä½•åœ¨è¿­ä»£è¿‡ç¨‹ä¸­æ‰¾åˆ°kä¸ªé“¾è¡¨ä¸­æœ€å°çš„èŠ‚ç‚¹ï¼šå¯ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆäºŒå‰å †ï¼‰ï¼Œæ¯æ¬¡æå–å‡ºæœ€å°çš„èŠ‚ç‚¹ï¼Œä¹Ÿå°±æ˜¯æœ€å°å †ï¼Œå¹¶ç»´æŠ¤è¿™ä¸ªæœ€å°å †( O(NlogN) ) Leetcode19 åˆ é™¤é“¾è¡¨çš„å€’æ•°ç¬¬Nä¸ªèŠ‚ç‚¹ åŒæŒ‡é’ˆ p and p+N Leetcode876 å•é“¾è¡¨çš„ä¸­ç‚¹ åŒæŒ‡é’ˆ Leetcode141 åˆ¤æ–­é“¾è¡¨æ˜¯å¦åŒ…å«ç¯ åŒæŒ‡é’ˆ slow and fast å¦‚ä½•æ‰¾åˆ°ç¯çš„èµ·ç‚¹ Leetcode160 åˆ¤æ–­ä¸¤ä¸ªé“¾è¡¨æ˜¯å¦ç›¸äº¤","link":"/leetcode.html"},{"title":"","text":"**Pan Xie** *New main building E813, Beihang University, Beijing, 100191, China* Email: [ftdpanxie@gmail.com](ftdpanxie@gmail.com) Homepage: [panxiaoxie.cn](panxiaoxie.cn) Github: [https://github.com/PanXiebit](https://github.com/PanXiebit) Research Interest: machine translation, natural language generation","link":"/resume/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Deep Generative Models","text":"Catalog: Autoregressive Models (ARMs) Flow-based models (flows): RealNVP and IDFs (Integer Discrete Flows) Variational Auto-Encoders (VAEs): a plain VAE and various priors, a hierarchical VAE Hybrid modeling Energy-based Models Generative Adversarial Networks (GANs) Diffusion-based Deep Generative Models (DDGMs): a Gaussian forward diffusion Neural Compression with Deep Generative Modeling VAEs ä»¥å›¾åƒç”Ÿæˆä¸ºä¾‹ï¼Œæƒ³è±¡ä¸€ä¸‹è¿™ä¸ªåœºæ™¯ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€å †é©¬ğŸçš„å›¾ç‰‡ï¼Œæˆ‘ä»¬ç°åœ¨å¸Œæœ›å­¦ä¹  $p(x)$ï¼Œä»è€Œç”Ÿæˆæ–°çš„å›¾ç‰‡ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é—®ä¸‹è‡ªå·±ï¼Œæˆ‘ä»¬å¦‚ä½•å»ç”»å‡ºé©¬çš„å›¾ç‰‡å‘¢ï¼Ÿæ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æŠŠè‡ªå·±å½“ä½œä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¦‚ä½•åšè¿™ä»¶äº‹å‘¢ï¼Ÿä¹Ÿè®¸æˆ‘ä»¬ä¼šå…ˆå‹¾å‹’å‡ºä¸€åŒ¹é©¬çš„å¤§è‡´è½®å»“ï¼Œå®ƒçš„å¤§å°å’Œå½¢çŠ¶ï¼Œç„¶åæ·»åŠ é©¬è¹„ï¼Œå¡«å……å¤´éƒ¨çš„ç»†èŠ‚ï¼Œç»™å®ƒä¸Šè‰²ç­‰ç­‰ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘èƒŒæ™¯ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è¯´æ•°æ®ä¸­æœ‰ä¸€äº›å› ç´ ï¼ˆä¾‹å¦‚è½®å»“ã€é¢œè‰²ã€èƒŒæ™¯ï¼‰å¯¹äºç”Ÿæˆå¯¹è±¡ï¼ˆè¿™é‡Œæ˜¯é©¬ï¼‰è‡³å…³é‡è¦ã€‚ ä¸€æ—¦æˆ‘ä»¬å†³å®šäº†è¿™äº›å› ç´ ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡æ·»åŠ ç»†èŠ‚æ¥ç”Ÿæˆå®ƒä»¬ã€‚ å½“æˆ‘ä»¬ç”»æŸç‰©æ—¶ï¼Œè¿™æˆ–å¤šæˆ–å°‘æ˜¯æˆ‘ä»¬ç”Ÿæˆä¸€å¹…ç”»çš„è¿‡ç¨‹ã€‚ æˆ‘ä»¬ç°åœ¨ç”¨æ•°å­¦æ¥è¡¨è¾¾è¿™ä¸ªç”Ÿæˆè¿‡ç¨‹ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æœ‰æˆ‘ä»¬æ„Ÿå…´è¶£çš„é«˜ç»´å¯¹è±¡ $x\\in \\mathcal{X}^D$ï¼ˆä¾‹å¦‚ï¼Œå¯¹äºå›¾åƒï¼Œ$\\mathcal{X}\\in {0,1,2,â€¦,255}$ï¼‰å’Œä¸€ä¸ªä½ç»´æ½œåœ¨å˜é‡ $z\\in\\mathcal{Z}^M$ï¼ˆä¾‹å¦‚ï¼Œ$\\mathcal{Z}=\\mathbb{R}$)ï¼Œæˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸ºæ•°æ®ä¸­çš„éšè—å› ç´ ã€‚ åœ¨æ•°å­¦ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†$\\mathcal{Z}^M$ç§°ä¸ºä½ç»´æµå½¢ã€‚ é‚£ä¹ˆï¼Œç”Ÿæˆè¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºï¼š $z\\sim p(z)$ (Figure1, In red) $x\\sim p(x|z)$(Figure1, In Blue) Figure 1. æ½œåœ¨å˜é‡æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹çš„ç®€å›¾. æ³¨æ„åµŒå…¥åœ¨é«˜ç»´ç©ºé—´(æ­¤å¤„ä¸º3D)ä¸­çš„ä½ç»´æµå½¢(æ­¤å¤„ä¸º2D) ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡æ ·ğ³ï¼ˆä¾‹å¦‚ï¼Œæˆ‘ä»¬æƒ³è±¡æˆ‘çš„é©¬çš„å¤§å°ã€å½¢çŠ¶å’Œé¢œè‰²ï¼‰ï¼Œç„¶ååˆ›å»ºå…·æœ‰æ‰€æœ‰å¿…è¦ç»†èŠ‚çš„å›¾åƒï¼Œå³ï¼Œæˆ‘ä»¬ä»æ¡ä»¶åˆ†å¸ƒ $p(x|z)$ä¸­é‡‡æ ·å¾—åˆ°xã€‚ ç”±äºè®¸å¤šå„ç§å¤–éƒ¨å› ç´ ï¼Œåˆ›å»ºä¸¤æ¬¡å®Œå…¨ç›¸åŒçš„å›¾åƒå‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚ æ½œåœ¨å˜é‡æ¨¡å‹èƒŒåçš„ideaæ˜¯æˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨å˜é‡ $z$ ï¼Œå¹¶ä¸”è”åˆåˆ†å¸ƒè¢«åˆ†è§£å¦‚ä¸‹ï¼š$ ğ‘(ğ±,ğ³)=ğ‘(ğ±|ğ³)ğ‘(ğ³)$. è¿™ä¸ªå…¬å¼è¡¨è¾¾çš„å°±æ˜¯ä¸Šè¿°çš„ç”Ÿæˆè¿‡ç¨‹ã€‚ ä½†æ˜¯åœ¨è®­ç»ƒæ—¶æˆ‘ä»¬åªèƒ½è®¿é—®æ ·æœ¬ $x$ã€‚ å› æ­¤ï¼Œæ ¹æ®æ¦‚ç‡æ¨ç†ï¼Œæˆ‘ä»¬åº”è¯¥å¯¹æœªçŸ¥çš„æ½œåœ¨å˜é‡ï¼Œä¹Ÿå°±æ˜¯ğ³ ï¼Œè¿›è¡Œæ±‚å’Œï¼ˆsum out, ä¹Ÿå«è¾¹ç¼˜åŒ– marginalize outï¼‰ã€‚æœ€åï¼Œå…¶è¾¹ç¼˜ä¼¼ç„¶å‡½æ•°å¦‚ä¸‹ï¼š $$p(\\mathbf{x}) = \\int p(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})\\ \\mathrm{d} \\mathbf{z} $$ é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œå¦‚ä½•è®¡ç®—è¿™ä¸ªç§¯åˆ†å‘¢ï¼Ÿæ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯ä¸ªéå¸¸å¤æ‚çš„ä»»åŠ¡ã€‚æœ‰ä¸¤ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ³•ï¼š1. ç›´æ¥å¤„ç†è¿™ä¸ªç§¯åˆ† (Probabilistic PCA )ï¼›2. åˆ©ç”¨è¿‘ä¼¼çš„æ–¹æ³•ä¹Ÿæ¥è§£å†³ï¼Œä¹Ÿå°±æ˜¯å˜åˆ†æ¨æ–­ (variational inference). Probabilistic PCAå¯¹äº $p(x|z)$æ˜¯çº¿æ€§æ¨¡å‹æ—¶ï¼Œå¯ä»¥ç›´æ¥æ±‚è§£ã€‚ä½†æ¨å¯¼è¿‡ç¨‹æ²¡çœ‹æ‡‚ï¼Œå…ˆä¸å†™äº†ã€‚ Variational Inference for Non-linear Latent Variable Modelsæ¨¡å‹å’Œç›®æ ‡ è®©æˆ‘ä»¬å†çœ‹ä¸€æ¬¡ç§¯åˆ†ï¼Œå¹¶è€ƒè™‘æˆ‘ä»¬æ— æ³•å‡†ç¡®è®¡ç®—ç§¯åˆ†çš„ä¸€èˆ¬æƒ…å†µã€‚ æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨è’™ç‰¹å¡ç½—è¿‘ä¼¼ï¼š$$\\begin{align}p(\\mathbf{x}) &amp;= \\int p(\\mathbf{x} | \\mathbf{z})\\ p(\\mathbf{z})\\ \\mathrm{d} \\mathbf{z} \\\\ &amp;= E_{\\mathbf{z}\\sim p(\\mathbf{z})} \\left[ p(\\mathbf{x} | \\mathbf{z}) \\right] \\\\ &amp;\\approx \\frac{1}{K} \\sum_{k} p(\\mathbf{x} | \\mathbf{z}_{k}) \\end{align}$$å…¶ä¸­ï¼Œåœ¨æœ€åä¸€è¡Œæˆ‘ä»¬é‡‡ç”¨è¿‘ä¼¼çš„æ–¹æ³•æ¥æ¨¡æ‹Ÿè¿™ä¸ªæœŸæœ›/ç§¯åˆ†å‡½æ•°ã€‚æˆ‘ä»¬åŸºäºæ½œåœ¨å˜é‡çš„çš„å…ˆéªŒæ¦‚ç‡æ¥å¾—åˆ°samplesï¼Œ$z_k\\sim p(z)$. åœ¨å½“å‰è®¡ç®—æœºè¶Šæ¥è¶Šå¿«çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªæ–¹æ³•æ˜¯ç›¸å¯¹ç®€å•çš„ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨çŸ­æ—¶é—´å†…é‡‡æ ·å‡ºæ— æ•°çš„ç‚¹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å­¦è¿‡çš„ç»Ÿè®¡å­¦çŸ¥è¯†å‘Šè¯‰æˆ‘ä»¬ï¼Œå½“æ½œåœ¨ç©ºé—´æ˜¯$z\\in\\mathcal{Z}^M$å¤šç»´ï¼Œä¸”$M$å¾ˆå¤§çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šé™·å…¥ç»´åº¦ç¾éš¾ã€‚ä¸ºäº†coverä½æ ·æœ¬ç©ºé—´ï¼Œæˆ‘ä»¬æ‰€éœ€é‡‡æ ·çš„æ ·æœ¬æ˜¯Mçš„æŒ‡æ•°å½¢å¼ã€‚å¦‚æœæˆ‘ä»¬é‡‡æ ·çš„æ ·æœ¬æ•°ä¸å¤Ÿï¼Œé‚£ä¹ˆè¿™ä¸ªè¿‘ä¼¼æ•ˆæœå°±ä¸å¥½ã€‚ æˆ‘ä»¬å½“ç„¶å¯ä»¥é‡‡ç”¨ä¸€äº›æ›´å…ˆè¿›çš„è’™ç‰¹å¡æ´›æ–¹æ³•ï¼Œç„¶è€Œï¼Œå®ƒä»¬å§‹ç»ˆä¼šå—åˆ°ç»´åº¦ç¾éš¾çš„å½±å“ã€‚å¦ä¸€ä¸ªå¯é€‰æ‹©çš„è¿‘ä¼¼æ–¹æ³•æ˜¯å˜åˆ†æ¨æ–­variational inference (Jordan et al., 1999). æˆ‘ä»¬è€ƒè™‘ä¸€ç»„ç”± $\\phi$ å‚æ•°åŒ–çš„å˜åˆ†åˆ†å¸ƒï¼Œ${q_{\\phi}(z)}$. æ¯”å¦‚ï¼Œæˆ‘ä»¬å‡è®¾$\\phi$ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œ $\\phi={\\mu, \\sigma^2}$. æˆ‘ä»¬çŸ¥é“è¿™äº›åˆ†å¸ƒçš„å½¢å¼ï¼Œå¹¶å‡è®¾å®ƒä»¬å°†éé›¶çš„æ¦‚ç‡åˆ†é…ç»™æ‰€æœ‰çš„æ½œåœ¨å˜é‡ $z\\in \\mathcal{Z}^{M}$. ç„¶åï¼Œè¿™ä¸ªè¾¹ç¼˜æ¦‚ç‡åˆ†å¸ƒå¯ä»¥è¿‘ä¼¼æ¨å¯¼å¦‚ä¸‹ï¼š$$\\begin{align}\\ln p(x) &amp;= ln\\int p(x|z)p(z)dz \\\\&amp;= ln\\int \\dfrac{q_{\\phi}(z)}{q_{\\phi}(z)}p(x|z)p(z)dz \\\\&amp;= lnE_{z\\sim q_{\\phi}(z)}[\\dfrac{p(x|z)p(z)}{q_{\\phi}(z)}] \\\\&amp;\\ge E_{z\\sim q_{\\phi}(z)}ln[\\dfrac{p(x|z)p(z)}{q_{\\phi}(z)}] \\\\&amp;= E_{z\\sim q_{\\phi}(z)} [lnp(x|z)+lnp(z)-lnq_{\\phi}(z)] \\\\&amp;= E_{z\\sim q_{\\phi}(z)} [lnp(x|z)] - E_{z\\sim q_{\\phi}(z)}[lnq_{\\phi}(z) - lnp(z)]\\end{align}$$ç¬¬4è¡Œä½¿ç”¨äº†Jensenâ€™s inequality. ä¸Šè¿°æ¨å¯¼è¿‡ç¨‹ï¼Œæˆ‘ä»¬æŠŠ $q_{\\phi}(z)$ æ¢æˆ amortized variational posteriorï¼Œä¹Ÿå°±æ˜¯ $q_{\\phi}(z|x)$ æ˜¯ä¸å½±å“æ¨å¯¼è¿‡ç¨‹çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š $$lnp(z)\\ge E_{z\\sim q_{\\phi}(z|x)} [lnp(x|z)] - E_{z\\sim q_{\\phi}(z|x)}[lnq_{\\phi}(z|x) - lnp(z)]$$ amortized variational posterior éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ç¥ç»ç½‘ç»œå¾—åˆ°è¿™æ ·ä¸€ä¸ªæ¨¡å‹ï¼Œç»™å®šè¾“å…¥ $x$ï¼Œç„¶åè¾“å‡ºå¯¹åº”åˆ†å¸ƒçš„å‚æ•°ã€‚åœ¨ (Kim et al., 2018) ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†ä¸€ç§ semi-amortized variational inference. æœ€åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªauto encoder-likeæ¨¡å‹ï¼Œå…¶åŒ…æ‹¬ä¸€ä¸ªencoder, $q_{\\phi}(z|x)$ å’Œ ä¸€ä¸ª decoder, $p(x|z)$. æˆ‘ä»¬ç”¨éšæœºæ€§æ¥å¼ºè°ƒencoderå’Œdecoderå…¶å®å°±æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¸deterministic auto-encoderæ˜¯æœ‰åŒºåˆ«çš„ï¼ˆè¿™é‡Œæ²¡å¤ªæ‡‚ï¼‰ã€‚è¿™ä¸ªå¸¦æœ‰ amortized variational posteriorçš„è‡ªç¼–ç æ¨¡å‹å°±æ˜¯å˜åˆ†è‡ªç¼–ç  Variational Auto-Encoder (Kingma &amp; Welling, 2013; Rezende et al., 2014). å…¶ä¼¼ç„¶å‡½æ•°çš„ä¸‹ç•Œå°±æ˜¯ Evidence LOwer Bound (ELBO). ELBOçš„ç¬¬ä¸€é¡¹æ˜¯ reconstruction error,$E_{z\\sim q_{\\phi}(z|x)} [lnp(x|z)]$ ; ç¬¬äºŒé¡¹æ˜¯regularizerï¼Œ$E_{z\\sim q_{\\phi}(z|x)}[lnq_{\\phi}(z|x) - lnp(z)]$,æ°å¥½å°±æ˜¯KLæ•£åº¦ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥æˆä¸ºKL item. ä½†åœ¨æ›´å¤æ‚çš„æ¨¡å‹ä¸­ï¼Œè¿™ä¸€é¡¹å¹¶ä¸ä¸€å®šæ˜¯ KL termï¼Œå› æ­¤ç§°ä½œregularizeræ›´é€šç”¨ä¸€ç‚¹ã€‚ å›é¡¾ä¸‹ï¼šç†µ $\\rightarrow$ äº¤å‰ç†µ $\\rightarrow$ ç›¸å¯¹ç†µ/KLæ•£åº¦ ä¿¡æ¯é‡ä¸æ¦‚ç‡æˆåæ¯”ï¼Œå½“æ¦‚ç‡è¶Šå¤§æ—¶ï¼Œä¿¡æ¯é‡è¶Šå°ï¼›å¹¶ä¸”ä¿¡æ¯é‡ä¸ºéè´Ÿæ•°ã€‚å› æ­¤ï¼Œå®šä¹‰ä¿¡æ¯é‡ä¸º $log\\dfrac{1}{p}$. ç†µæ˜¯ä¿¡æ¯é‡çš„æœŸæœ›ï¼š$E_{x\\sim p(x)}ln\\dfrac{1}{p(x)}$ äº¤å‰ç†µæŒ‡çš„æ˜¯ï¼šæ ¹æ®çœŸå®åˆ†å¸ƒpæ¥è¡¡é‡é¢„æµ‹åˆ†å¸ƒqçš„åº¦é‡ã€‚åŒæ ·çš„æˆ‘ä»¬ä¹Ÿç”¨ä¿¡æ¯é‡çš„æœŸæœ›æ¥è¡¡é‡è¿™ä¸ªäº¤å‰ç†µåº¦é‡ï¼Œæˆ‘ä»¬å¸Œæœ›äº¤å‰ç†µè¶Šå°æ—¶ï¼Œé¢„æµ‹çš„qè¶Šå‡†ç¡®ï¼Œä¹Ÿå°±æ˜¯è¶Šæ¥è¿‘äº1ã€‚ç±»ä¼¼åœ°ï¼Œäº¤å‰ç†µå¯ä»¥å†™æˆ $H(p, q) = E_{x\\sim p(x)}ln\\dfrac{1}{q(x)}$. å®é™…ä¸Šï¼Œæˆ‘ä»¬æ˜¯æ— æ³•çŸ¥é“çœŸå®åˆ†å¸ƒpçš„ï¼Œåªèƒ½ä¾æ®ç°æœ‰çš„æ ·æœ¬ç»Ÿè®¡å¾—åˆ°ã€‚ 1234567891011121314151617181920212223import torchimport torch.nn as nnimport torch.nn.functional as Fsize = 3input = torch.randn(2, size)target = torch.Tensor([0, 2]).long()# use loss functionloss_fn = nn.CrossEntropyLoss(reduction=&quot;mean&quot;)loss = loss_fn(input, target)# computer nll loss step by stepscore = torch.log_softmax(input, dim=1)my_nll = torch.sum(-score * F.one_hot(target, size)) / target.size(0)# use nll lossnll_loss_fn = nn.NLLLoss()nll_loss = nll_loss_fn(score, target)print(nll_loss == loss == my_nll) # True ç›¸å¯¹ç†µï¼š æ ¹æ®Gibbsâ€™ inequalityä¸Šè¿°ä¾‹å­ä¸­çš„ $H(p,q) &gt;= H(p)$ æ’æˆç«‹ã€‚å½“ä¸”ä»…å½“q=pæ—¶ï¼Œè¿™ä¸ªç­‰å·æ‰æˆç«‹ã€‚é‚£ä¹ˆç†µH(p,q)ç›¸æ¯”ç†µH(q)å¤šå‡ºæ¥çš„éƒ¨åˆ†å°±æ˜¯ç›¸å¯¹ç†µ ï¼Œä¹Ÿç§°ä¸ºKLæ•£åº¦(Kullbackâ€“Leibler divergenceï¼ŒKLD). $$D(p||q)=H(p,q)-H(p)=\\sum_{x\\sim p(x)}[ln\\dfrac{1}{q(x)} - ln\\dfrac{1}{p(x)}]=E_{x\\sim p(x)}[ln\\dfrac{p(x)}{q(x)}]=\\int_{x}p(x)[ln\\dfrac{p(x)}{q(x)}]$$ A different perspective on the ELBOä¸‹é¢æä¾›ä¸€ç§æ–°çš„æ¨å¯¼æ–¹æ³•ï¼Œä¸ªäººè§‰å¾—æ›´èƒ½ç†è§£ELBO:$$\\begin{align}\\ln p(\\mathbf{x}) &amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}) \\right] \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln \\frac{p(\\mathbf{z}|\\mathbf{x}) p(\\mathbf{x})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln \\frac{p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{æ ¹æ®è´å¶æ–¯å…¬å¼p(z|x)p(x)=p(x|z)p(z)}\\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln \\frac{p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{p(\\mathbf{z}|\\mathbf{x})} \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\right] \\quad \\text{è¿™ä¸€æ­¥å¾ˆå…³é”®, } 1 = \\frac{q_{\\phi(z|x)}}{q_{\\phi(z|x)}}, q_{\\phi(z|x)} æ˜¯ç”¨æ¥è¿‘ä¼¼çœŸå®åéªŒæ¦‚ç‡çš„å˜åˆ†åéªŒæ¦‚ç‡ \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\frac{p(\\mathbf{z})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) - \\ln \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{p(\\mathbf{z})} + \\ln \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{åˆ°è¿™é‡Œèƒ½çœ‹å‡ºKLæ•£åº¦äº†}\\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\right] - KL\\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}) \\right] + KL \\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) |p(\\mathbf{z}|\\mathbf{x}) \\right] .\\end{align}$$å‰ä¸¤é¡¹å°±æ˜¯ ELBOï¼Œæœ€åä¸€é¡¹ä¸­ $p(z|x)$ è¿™ä¸ªè¡¨ç¤ºçœŸå®çš„åéªŒæ¦‚ç‡ real posterior, $q_{\\phi}(z|x)$ è¡¨ç¤ºå˜åˆ†åéªŒæ¦‚ç‡ variational posterior. æˆ‘ä»¬å¹¶ä¸çŸ¥é“çœŸå®çš„åéªŒæ¦‚ç‡ï¼Œä½†æ˜¯æˆ‘ä»¬å‘¢å¯ä»¥é¢è·³è¿‡è¿™ä¸€é¡¹ï¼Œå› ä¸ºKLæ•£åº¦ä¸€å®šå¤§äºç­‰äº0. å»æ‰æœ€åä¸€é¡¹ï¼Œæˆ‘ä»¬å¾—åˆ°ELBOï¼ŒåŒæ—¶æˆ‘ä»¬çŸ¥é“ ELBOå’ŒçœŸå®çš„å¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„é—´éš”æ˜¯ $KL \\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) |p(\\mathbf{z}|\\mathbf{x}) \\right] $.$$\\begin{align}\\ln p(\\mathbf{x}) &amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\right] - KL\\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}) \\right] + KL \\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) |p(\\mathbf{z}|\\mathbf{x}) \\right] \\\\&amp;\\ge E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\right] - KL\\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}) \\right]\\end{align}$$Beautiful! ä½†åŒæ ·æˆ‘ä»¬èƒ½çœ‹å‡ºä¸€äº›é—®é¢˜ï¼Œå¦‚æœ $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ å’Œ $p(z|x)$ è·ç¦»å¾ˆå¤§ï¼Œé‚£ä¹ˆELBOä¼˜åŒ–çš„å†å¥½ï¼ŒELBOå’ŒçœŸå®çš„å¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„å·®è·ä¾ç„¶å¾ˆå¤§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬é‡‡ç”¨å¾ˆç®€å•çš„åéªŒæ¦‚ç‡ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªä¸å¤ªå¥½çš„VAEæ¨¡å‹ã€‚ Figure 2. ELBOæ˜¯å¯¹æ•°ä¼¼ç„¶çš„ä¸‹ç•Œã€‚ELBOæœ€å¤§æ—¶å¯¹åº”çš„ $\\hat {\\theta}$ å¹¶ä¸ä¸€å®šä¹Ÿèƒ½è®©å¯¹æ•°ä¼¼ç„¶æœ€å¤§ã€‚ELBO è¶Šlosserï¼Œè¿™å¯¹æ¨¡å‹å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„åå·®è¶Šå¤§ã€‚ Components of VAEs æˆ‘ä»¬ä½¿ç”¨amortized variational posteriors ${q_{\\phi}(z|x)}_{\\phi}$ æ¥è¿‘ä¼¼çœŸå®çš„åéªŒåˆ†å¸ƒ $p(z|x)$. è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒå¯ä»¥çœ‹ä½œæ˜¯ encoder. conditional likelihood p(x|z) å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥çœ‹ä½œæ˜¯ decoder $p(z)$ æ˜¯å¯¹äºæ½œåœ¨å˜é‡çš„è¾¹ç¼˜åˆ†å¸ƒï¼Œä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ prior æ¨¡å‹ è‡³æ­¤ï¼Œè¿˜æœ‰ä¸¤ä¸ªé—®é¢˜éœ€è¦è§£å†³ï¼š å¦‚ä½•å‚æ•°åŒ–è¿™äº›distributions? å¦‚ä½•è®¡ç®—è¿™äº›æœŸæœ›å‘¢ï¼Ÿä¹Ÿå°±æ˜¯ç§¯åˆ†ã€‚ Parameterization of distributionsæ˜¾ç„¶ï¼Œæˆ‘ä»¬ç”¨Neural Networksæ¥è¡¨è¾¾ä¸Šè¿°ä¸¤ä¸ªdistributions: encoder å’Œ decoderã€‚åœ¨VAEæ¡†æ¶ä¸‹ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•distributions. ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¹Ÿå¿…é¡»æ»¡è¶³å¯¹åº”çš„ä»»åŠ¡ã€‚ å¯¹äºdecoderåˆ†å¸ƒ $p_{\\theta}(x|z)$ æ˜¾ç„¶ä¸å¯èƒ½æ˜¯æ­£æ€åˆ†å¸ƒï¼Œå› ä¸ºimageçš„pixel valuesæ˜¯ç¦»æ•£çš„ã€‚ä¸€ä¸ªå¯èƒ½çš„distributionå¯ä»¥æ˜¯ categorical distribution: â€‹ $$p_{\\theta}(x|ğ³)=\\text{Categorical}(ğ±|\\theta(ğ³))$$ â€‹ å…¶ä¸­ $\\theta(z) = \\text{softmax}(NN(z))$. è¿™é‡ŒæŠŠé‡æ„å½“ä½œåˆ†ç±»ä»»åŠ¡æ¥åšï¼Œå½“ç„¶ä¹Ÿå¯ä»¥æ˜¯å›å½’ä»»åŠ¡ã€‚ å¯¹äºæ½œåœ¨å˜é‡çš„åˆ†å¸ƒï¼Œä¸ºäº†æ–¹ä¾¿ï¼Œé€šå¸¸ $z$ å¯è§†ä¸ºè¿ç»­éšæœºå˜é‡çš„å‘é‡ï¼Œ$z\\in \\mathbb{R}^M$. å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Gaussians æ¥è¡¨ç¤º variational posterior å’Œ prior.$$\\begin{align}q_{\\phi}(\\mathbf{z}|\\mathbf{x}) &amp;= \\mathcal{N}\\left(\\mathbf{z} | \\mu_{\\phi}(\\mathbf{x}), \\mathrm{diag}\\left[ \\sigma_{\\phi}^2(\\mathbf{x}) \\right] \\right) \\\\p(\\mathbf{z}) &amp;= \\mathcal{N}\\left(\\mathbf{z} | 0, \\mathbf{I} \\right)\\end{align}$$å…¶ä¸­ $\\mu_{phi}(x), \\sigma_{\\phi}(x)$ æ˜¯ç¥ç»ç½‘ç»œçš„è¾“å‡ºã€‚åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨NNå¾—åˆ° $2M$ çš„ values $\\in R^{1\\times 2M}$ï¼Œå…¶ä¸­ $R^{1\\times M}$ è¡¨ç¤º meansï¼Œ$R^{1\\times M}$ è¡¨ç¤º variances. Reparameterization trickåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å­¦ä¹ äº†log-likelihoodå’ŒELBOã€‚ä½†æ˜¯ä»ç„¶æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç”¨encoder $q_{\\phi}(z|x)$ å¾—åˆ°å…³äºæ½œåœ¨å˜é‡çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•è®¡ç®—$E_{z\\sim q_{\\phi}(z|x)}(x|z)$è¿™ä¸ªç§¯åˆ†å‘¢ï¼Ÿæ˜¾ç„¶ï¼Œ$z\\sim q_{\\phi}(z|x)$è¿™ä¸ªé‡‡æ ·è¿‡ç¨‹æ˜¯ä¸å¯å¯¼çš„ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨MC-approximation,ä½†æ˜¯è¿™æ ·ä»ç„¶æœ‰ä¸ªé—®é¢˜ï¼Œä»è¿™ä¸ªå˜åˆ†åéªŒsampleå¾—åˆ°çš„zï¼Œåœ¨ELBOçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåœ¨è®¡ç®—å…³äº$\\phi$çš„æ¢¯åº¦æ—¶ï¼Œæ¢¯åº¦çš„æ–¹å·®ç‰¹åˆ«å¤§ã€‚ å› æ­¤ï¼Œå¦ä¸€ä¸ªå¯èƒ½çš„æ–¹æ³•æ˜¯reparameterizingè¿™ä¸ªåˆ†å¸ƒ(Devroye, 1996). å…·ä½“åœ°ï¼Œæˆ‘ä»¬å¯ä»¥å°†éšæœºå˜é‡ $z$ è¡¨ç¤ºä¸ºå…·æœ‰ç®€å•åˆ†å¸ƒçš„ç‹¬ç«‹éšæœºå˜é‡çš„åŸå§‹å˜æ¢ï¼ˆä¾‹å¦‚ç®—æœ¯è¿ç®—ã€å¯¹æ•°ç­‰ï¼‰çš„ç»„åˆã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨é‡å‚æ•°æŠ€å·§è¡¨è¾¾ä¸ºç¡®å®šæ€§çš„å˜é‡:$$z = \\mu + \\sigma \\cdot \\epsilon$$å…¶ä¸­ $\\epsilon \\sim \\mathcal{N}(\\epsilon|0,1)$ . ä½¿ç”¨Reparameterizationæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°æ¢¯åº¦çš„æ–¹æ³•ã€‚Whyï¼Ÿå› ä¸ºéšæœºæ€§æ¥è‡ªç‹¬ç«‹çš„åˆ†å¸ƒ$p(\\epsilon)$ï¼Œæˆ‘ä»¬è®¡ç®—æ¢¯åº¦æ˜¯å…³äºç¡®å®šæ€§å‡½æ•°ï¼ˆå³ç¥ç»ç½‘ç»œï¼‰ï¼Œè€Œä¸æ˜¯éšæœºçš„å¯¹è±¡$z\\sim q_{\\phi}(z|x)$ã€‚ æ›´æ£’çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ¥å­¦ä¹  VAEï¼Œå› æ­¤åœ¨è®­ç»ƒæœŸé—´ä»…é‡‡æ ·ä¸€æ¬¡ $z$ å°±è¶³å¤Ÿäº†ï¼ ç»¼ä¸Šï¼ŒVAEæ¡†æ¶ä¸»è¦åŒ…æ‹¬ï¼š variational posterior $q_{\\phi}(z|x)$ using encoder sample z from $q_{\\phi}(z|x)$ and feed it to decoder, using reparameterization trick conditional likelihood $p_{\\theta}(x|z)$ using decoder reconstruction loss kl loss between variational posterior and prior $p_{\\theta}$. å…¶ä¸­ $q_{\\phi}(z|x)\\sim \\mathcal{N}(z|\\mu, \\sigma^2I)$ å¤šç»´é«˜æ–¯åˆ†å¸ƒï¼Œ$p_{\\theta}\\sim \\mathcal{N}(0,1)$ æ˜¯æ­£æ€åˆ†å¸ƒã€‚ KL æ•£åº¦çš„æ¨å¯¼å¦‚ä¸‹ï¼š ä»£ç å¦‚ä¸‹ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class VariationalEncoder(nn.Module): def __init__(self, latent_dims): super(VariationalEncoder, self).__init__() self.linear1 = nn.Linear(784, 512) self.linear2 = nn.Linear(512, latent_dims) self.linear3 = nn.Linear(512, latent_dims) self.kl = 0 def forward(self, x): x = torch.flatten(x, start_dim=1) x = F.relu(self.linear1(x)) mu = self.linear2(x) sigma = torch.exp(self.linear3(x)) # reparameterization trick z = mu + sigma*torch.randn_like(sigma) self.kl = 0.5*(sigma**2 + mu**2 - torch.log(sigma) - 1).sum() return zclass Decoder(nn.Module): def __init__(self, latent_dims): super(Decoder, self).__init__() self.linear1 = nn.Linear(latent_dims, 512) self.linear2 = nn.Linear(512, 784) def forward(self, z): x_hat = F.relu(self.linear1(z)) x_hat = torch.sigmoid(self.linear2(x_hat)) return x_hat.reshape((-1, 1, 28, 28)) class VariationalAutoencoder(nn.Module): def __init__(self, latent_dims): super(VariationalAutoencoder, self).__init__() self.encoder = VariationalEncoder(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z = self.encoder(x) return self.decoder(z) def train(model, data, epochs=20): opt = torch.optim.Adam(model.parameters()) for epoch in range(epochs): print(f&quot;epoch: {epoch+1}/{epochs}&quot;) for x, y in data: x = x.to(device) # GPU opt.zero_grad() x_hat = model(x) loss = ((x - x_hat)**2).sum() + model.encoder.kl loss.backward() opt.step() return model latent_dims = 2vae = VariationalAutoencoder(latent_dims).to(device) # GPUvae = train(vae, data) More about VAEs! Estimation of the log-likelihood using importance weighting æˆ‘ä»¬å‰é¢æåˆ°è¿‡ ELBO åªæ˜¯å¯¹æ•°ä¼¼ç„¶çš„ä¸‹ç•Œï¼Œå®ƒä¸åº”è¯¥è¢«ç”¨ä½œå¯¹æ•°ä¼¼ç„¶çš„è‰¯å¥½ä¼°è®¡ã€‚(Burda et al., 2015; Rezende et al., 2014) é‡‡ç”¨äº†ä¸€ç§ importance weighting procedure æ–¹æ³•ã€‚ Enhancing VAEs: Better encoders æ„å‘³ç€æ›´å¥½çš„åéªŒæ¦‚ç‡, ä¸€ä¸ªå¾ˆé‡è¦çš„æ–¹å‘æ˜¯ conditional flow-based models (van den Berg et al., 2018; Hoogeboom et al., 2020; Kingma et al., 2016; Rezende &amp; Mohamed, 2015; Tomczak &amp; Welling, 2016; Tomczak &amp; Welling, 2017). Enhancing VAEs: Better decoders å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ¨¡å‹æˆ–è€…loss functionæ¥æ‹ŸåˆåŸå§‹çš„æ•°æ®ï¼Œæ¯”å¦‚pixel-CNN, transformerç­‰ã€‚ Enhancing VAEs: Better priors è®¾å®šä¸€ä¸ªå¥½çš„å…ˆéªŒä¹Ÿæ˜¯å¾ˆé‡è¦çš„ï¼Œèƒ½å¤Ÿå‡å°ä¸å˜åˆ†åéªŒçš„gapã€‚å¾ˆå¤šç ”ç©¶å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚ï¼šusing a multimodal prior mimicking the aggregated posterior (known as the VampPrior) (Tomczak &amp; Welling, 2018), or a flow-based prior (e.g., (Gatopoulos &amp; Tomczak, 2020)), an ARM-based prior (Chen et al., 2016) or using an idea of resampling (Bauer &amp; Mnih, 2019). VAEs for non-image data ä¸ä»…ä»…æ˜¯å›¾åƒæ•°æ®ï¼Œæ–‡æœ¬ã€åºåˆ—æ•°æ®ç­‰ä¹Ÿå¯ä»¥ã€‚ Extending VAEs Here, we present the unsupervised version of VAEs. However, there is no restriction to that and we can introduce labels or other variables. In (Kingma et al., 2014) a semi-supervised VAE was proposed. This idea was further extended to the concept of fair representations (Louizos et al., 2015). In (Ilse et al., 2020), the authors proposed a specific latent representation that allows domain generalization in VAEs. In (Blundell et al., 2015) variational inference and the reparameterization trick were used for Bayesian Neural Nets. This paper is not necessarily introducing a VAE, but a VAE-like way of dealing with Bayesian neural nets. **Different latent spacesï¼š ** in (Davidson et al., 2018; Davidson et al., 2019) a hyperspherical latent-space was used, and in (Mathieu et al., 2019) the hyperbolic latent space was utilized. **The posterior collapseï¼š ** There were many ideas proposed to deal with the posterior collapse. For instance, (He et al., 2019) propose to update variational posteriors more often than the decoder. In (Dieng et al., 2019) a new architecture of the decoder is proposed by introducing skip connection to avoiding the posterior collapse. Various perspectives on the objective The core of the VAE is the ELBO. However, we can consider different objectives. For instance, (Dieng et al., 2017) propose an upper-bound to the log-likelihood that is based on the chi-square divergence (CUBO). In (Alemi et al., 2018) an information-theoretic perspective on the ELBO is presented. (Higgins et al., 2016) introduced the ğ›½Î²-VAE where the regularization term is weighted by a fudge factor ğ›½Î². The objective does not correspond to the lowe-bound of the log-likelihood though. Deterministic Regularized Auto-Encoders: We can take look at the VAE and the objective, as mentioned before, and think of it as a regularized version of an auto-encoder with a stochastic encoder and a stochastic decoder. (Ghosh et al., 2020) â€œpeeled offâ€ VAEs from all stochasticity and indicated similarities between deterministic regularized auto-encoders and VAEs, and highlited potential issues with VAEs. Moreover, they brilliantly pointed out that even with a deterministic encoders, due to stochasticity of the empirical distribution, we can fit a model to the aggregated posterior. As a result, the deterministic (regularized) auto-encoder could be turned into a generative model by sampling from our model, ğ‘ğœ†(ğ³)pÎ»(z), and then, deterministically, mapping ğ³z to the space of observable ğ±x. In my opinion, this direction should be further explored and an important question is whether we indeed need any stochasticity at all. Hierarchical VAEs Very recently, there are many VAEs with a deep, hierarchical structure of latent variables that achieved remarkable results! The most important ones are definitely BIVA (MaalÃ¸e et al., 2019), NVA (Vahdat &amp; Kautz, 2020), and very deep VAEs (Child, 2020). Another interesting perspective on a deep, hierarchical VAE was presented in (Gatopoulos &amp; Tomczak, 2020) where, additionally, a series of deterministic functions was used. Adversarial Auto-Encoders Another interesting perspective on VAEs is presented in (Makhzani et al., 2015). Since learning the aggregated posterior as the prior is an important component mentioned in some papers (e.g., (Tomczak &amp; Welling, 2018)), a different approach would be to train the prior with an adversarial loss. Further, (Makhzani et al., 2015) present various ideas how auto-encoders could benefit from adverarial learning. Prior in VAEs","link":"/2021/12/07/Deep-Generative-Models/"},{"title":"CSAPP-01.A tour of computer system","text":"CSAPP ç¬¬ä¸€ç«  1.1 ä¿¡æ¯å°±æ˜¯ä½ï¼‹ä¸Šä¸‹æ–‡ ä½ï¼ˆbitï¼‰ç”±å€¼ï¼å’Œï¼‘ç»„æˆï¼Œä¸€ä¸ªå­—èŠ‚æœ‰ï¼˜ä½ã€‚ ä¸€ä¸ªå­—é•¿ï¼Œå¯¹32ä½æœºå™¨æ˜¯4ä¸ªå­—èŠ‚ï¼Œå¯¹64ä½æœºå™¨æ˜¯8ä¸ªå­—èŠ‚ã€‚ 1.2 ç¨‹åºè¢«ç¿»è¯‘æˆå…¶ä»–ä¸åŒçš„æ ¼å¼ä»æºç¨‹åºåˆ°å¯æ‰§è¡Œç›®æ ‡æ–‡ä»¶çš„è¿‡ç¨‹åˆ†ä¸º4ä¸ªé˜¶æ®µï¼š æ‰§è¡Œè¿™4ä¸ªé˜¶æ®µçš„ç¨‹åºï¼ˆé¢„å¤„ç†å™¨ã€ç¼–è¯‘å™¨ã€æ±‡ç¼–å™¨å’Œé“¾æ¥å™¨ï¼‰ä¸€èµ·æ„æˆäº†ç¼–è¯‘ç³»ç»Ÿã€‚ hello.cï¼ˆæºç¨‹åºï¼Œæ–‡æœ¬æ–‡ä»¶ï¼‰ â€“&gt; hello.iï¼ˆæ·»åŠ äº†å¤´æ–‡ä»¶çš„æºç¨‹åºï¼Œæ–‡æœ¬ï¼‰ â€“&gt; hello.sï¼ˆæ±‡ç¼–ç¨‹åºï¼Œæ–‡æœ¬ï¼‰ â€“&gt; hello.0ï¼ˆç›®æ ‡ç¨‹åºï¼ŒäºŒè¿›åˆ¶ï¼‰â€“&gt; helloï¼ˆå¯æ‰§è¡Œçš„ç›®æ ‡ç¨‹åºï¼ŒäºŒè¿›åˆ¶ï¼‰ 1.3 ä¸ºä»€ä¹ˆè¦äº†è§£ç¼–è¯‘ç³»ç»Ÿæ˜¯å¦‚ä½•å·¥ä½œçš„ ä¼˜åŒ–ç¨‹åºæ€§èƒ½ ä¸ºäº†åœ¨Cç¨‹åºä¸­åšå‡ºå¥½çš„ç¼–ç é€‰æ‹©ï¼Œéœ€è¦å»äº†è§£ä¸€èµ·æœºå™¨ä»£ç ä»¥åŠç¼–è¯‘å™¨å°†ä¸åŒçš„Cè¯­å¥è½¬åŒ–ä¸ºæœºå™¨ä»£ç çš„æ–¹å¼ã€‚ ä¾‹å¦‚ï¼š ä¸€ä¸ªå‡½æ•°è°ƒç”¨çš„å¼€é”€æœ‰å¤šå¤§ï¼Ÿ whileå¾ªç¯æ¯”forå¾ªç¯æ›´æœ‰æ•ˆå—ï¼Ÿ æŒ‡é’ˆå¼•ç”¨æ¯”æ•°ç»„å¼•ç”¨æ›´æœ‰æ•ˆå—ï¼Ÿ ç¬¬3ç« ä¼šä»‹ç»ç¼–è¯‘å™¨å¦‚ä½•æŠŠä¸åŒçš„Cè¯­è¨€ç»“æ„è½¬æ¢æˆä»–ä»¬çš„æœºå™¨è¯­è¨€çš„ã€‚ç¬¬5ç« ä¼šå­¦ä¹ ç®€å•çš„è½¬æ¢Cè¯­è¨€ä»£ç ã€‚ ç†è§£é“¾æ¥æ—¶å‡ºç°çš„é”™è¯¯ é“¾æ¥å™¨æŠ¥å‘Šæ— æ³•è§£æä¸€ä¸ªå¼•ç”¨ï¼Œè¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ é™æ€å˜é‡å’Œå…¨å±€å˜é‡çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ ä¸åŒçš„Cæ–‡ä»¶ä¸­å®šä¹‰äº†ä¸¤ä¸ªç›¸åŒçš„å…¨å±€å˜é‡ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ ä¸ºä»€ä¹ˆæœ‰äº›é“¾æ¥é”™è¯¯ç›´åˆ°è¿è¡Œæ—¶æ‰ä¼šå‡ºç°ï¼Ÿ ç¬¬7ç« ä¼šè§£é‡Šè¿™äº›ã€‚ é¿å…å®‰å…¨æ¼æ´ ç¼“å†²åŒºæº¢å‡ºé”™è¯¯ã€‚ç¬¬3ç« ä¼šæè¿°å †æ ˆåŸç†å’Œç¼“å†²åŒºæº¢å‡ºé”™è¯¯ã€‚ 1.4 å¤„ç†å™¨è¯»å¹¶è§£é‡Šå­˜å‚¨åœ¨å­˜å‚¨å™¨ä¸­çš„æŒ‡ä»¤1.4.1 ç³»ç»Ÿç¡¬ä»¶çš„ç»„æˆ æ€»çº¿ï¼š æ¯æ¬¡ä¼ é€1ä¸ªå­—ã€‚ I/O è®¾å¤‡ï¼š I/Oæ€»çº¿å’ŒI/Oè®¾å¤‡ä¹‹é—´ä¼ é€’ä¿¡æ¯ã€‚ ä¸»å­˜ï¼š ä¸€ç»„åŠ¨æ€éšæœºå­˜å–å­˜å‚¨å™¨(DRAM)èŠ¯ç‰‡ç»„æˆã€‚å­˜å‚¨å™¨æ˜¯ä¸€ä¸ªçº¿æ€§å­—èŠ‚æ•°ç»„ï¼Œæ¯ä¸ªå­—èŠ‚éƒ½æœ‰å…¶å”¯ä¸€çš„åœ°å€ï¼ˆå³æ•°ç»„ç´¢å¼•ï¼‰ã€‚ å¤„ç†å™¨ï¼š ä¸­å¤®å¤„ç†å•å…ƒï¼ˆCPUï¼‰ï¼Œè§£é‡Šï¼ˆæˆ–æ‰§è¡Œï¼‰åœ¨ä¸»å­˜ä¸­æŒ‡ä»¤çš„å¼•æ“ã€‚å¤„ç†å™¨æ ¸å¿ƒæ˜¯ä¸€ä¸ªå­—é•¿çš„å­˜å‚¨è®¾å¤‡ï¼ˆæˆ–å¯„å­˜å™¨ï¼‰ï¼Œç§°ä¸ºç¨‹åºè®¡æ•°å™¨ï¼ˆPCï¼‰ã€‚åœ¨ä»»ä½•æ—¶åˆ»ï¼ŒPCéƒ½æŒ‡å‘ä¸»å­˜ä¸­æŸæ¡æœºå™¨è¯­è¨€æŒ‡ä»¤ï¼ˆå³å«æœ‰è¯¥æ¡æŒ‡ä»¤çš„åœ°å€ï¼‰ã€‚ å¤„ç†å™¨æ˜¯æŒ‰ç…§ä¸€ä¸ªç®€å•çš„æŒ‡ä»¤æ‰§è¡Œæ¨¡å‹æ¥æ“ä½œçš„ï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯ç”±æŒ‡ä»¤é›†ç»“æ„å†³å®šçš„ã€‚PCè¯»å–æŒ‡ä»¤ï¼Œè§£é‡ŠæŒ‡ä»¤ä¸­çš„ä½ï¼Œæ‰§è¡Œè¯¥æŒ‡ä»¤çš„ç®€å•æ“ä½œï¼Œç„¶åæ›´æ–°PCï¼Œä½¿å…¶æŒ‡å‘ä¸‹ä¸€ä¸ªæŒ‡ä»¤ï¼Œè€Œè¿™æ¡æŒ‡ä»¤å¹¶ä¸ä¸€å®šä¸å­˜å‚¨å™¨ä¸­åˆšåˆšæ‰§è¡Œçš„æŒ‡ä»¤ç›¸é‚»ã€‚ è¿™æ ·çš„æ“ä½œæ˜¯å›´ç»•ä¸»å­˜ã€å¯„å­˜å™¨æ–‡ä»¶(register file)ã€ç®—æœ¯é€»è¾‘å•å…ƒï¼ˆALUï¼‰è¿›è¡Œçš„ã€‚ CPUæŒ‡ä»¤å¯èƒ½ä¼šè¦æ±‚å¦‚ä¸‹æ“ä½œï¼š åŠ è½½ï¼šä¸€ä¸ªå­—èŠ‚æˆ–ä¸€ä¸ªå­—ï¼Œä»ä¸»å­˜åˆ°å¯„å­˜å™¨ å­˜å‚¨ï¼šä¸€ä¸ªå­—èŠ‚æˆ–ä¸€ä¸ªå­—ï¼Œä»å¯„å­˜å™¨åˆ°ä¸»å­˜ æ“ä½œï¼šä¸¤ä¸ªå¯„å­˜å™¨çš„å†…å®¹å¤åˆ¶åˆ°ALUï¼Œè¿›è¡Œç®—æœ¯æ“ä½œ è·³è½¬ï¼šä»æŒ‡ä»¤æœ¬èº«ä¸­æŠ½å–ä¸€ä¸ªå­—ï¼Œå¤åˆ¶åˆ°PCä¸­ã€‚ 1.4.2 è¿è¡Œhelloç¨‹åº å›¾ä¸­é»‘è‰²çš„çº¿å°±æ˜¯æ•´ä¸ªæµç¨‹ã€‚shellå¤–å£³ç¨‹åºå°†å­—ç¬¦è¯»å…¥å¯„å­˜å™¨ï¼Œå†å­˜æ”¾åˆ°å­˜å‚¨å™¨ä¸­ã€‚ä¸€æ—¦ä»£ç å’Œæ•°æ®åŠ è½½åˆ°ä¸»å­˜ä¸­ï¼Œå¤„ç†å™¨å°±å¼€å§‹æ‰§è¡Œhelloç¨‹åºä¸­çš„mainç¨‹åºä¸­çš„æœºå™¨è¯­è¨€æŒ‡ä»¤ã€‚ åˆ©ç”¨å­˜å‚¨å™¨å­˜å–çš„æŠ€æœ¯ï¼Œæ•°æ®å¯ä»¥ä¸é€šè¿‡å¤„ç†å™¨ç›´æ¥ä»ç£ç›˜åˆ°è¾¾ä¸»å­˜ã€‚å¦‚ä¸‹å›¾ï¼š ç„¶åä»ä¸»å­˜å¤åˆ¶åˆ°å¯„å­˜å™¨æ–‡ä»¶ä¸­ï¼Œå†ä»å¯„å­˜å™¨å¤åˆ¶åˆ°æ˜¾ç¤ºè®¾å¤‡ä¸Šã€‚è¿‡ç¨‹å¦‚ä¸‹å›¾ï¼š æœºå™¨æŒ‡ä»¤æœ€åˆæ˜¯æ”¾åœ¨ç£ç›˜ä¸Šçš„ï¼Œ ç„¶ååŠ è½½ç¨‹åºï¼Œå¤åˆ¶åˆ°ä¸»å­˜ å¤„ç†å™¨è¿è¡Œç¨‹åºæ—¶ï¼ŒæŒ‡ä»¤ä»ä¸»å­˜åˆ°å¤„ç†å™¨ åŒæ ·çš„ï¼Œâ€œhello wordâ€ åˆå§‹æ˜¯åœ¨ç£ç›˜ä¸Šçš„ï¼Œç„¶åå¤åˆ¶åˆ°ä¸»å­˜ï¼Œæœ€åä»ä¸»å­˜å¤åˆ¶åˆ°æ˜¾ç¤ºå™¨ã€‚ 1.5 é«˜é€Ÿç¼“å­˜è‡³å…³é‡è¦å¯ä»¥å‘ç°è¿™äº›å¤åˆ¶å‡ç¼“äº†ç¨‹åºçš„é€Ÿåº¦ã€‚ è€Œè¯»å–é€Ÿåº¦ï¼š å­˜å‚¨å™¨ &gt;&gt; å¯„å­˜å™¨ï¼ˆå­˜æ”¾å‡ ç™¾å­—èŠ‚ï¼‰ &gt;&gt; ä¸»å­˜ï¼ˆå­˜æ”¾å‡ åäº¿å­—èŠ‚ï¼‰ &gt;&gt; ç£ç›˜ï¼ˆå¯èƒ½æ¯”ä¸»å­˜å¤§1000å€ï¼Œæ¯”å¦‚2Tçš„ç¡¬ç›˜ï¼Œ2Gçš„å†…å­˜.ï¼‰ å› æ­¤æœ‰äº†é«˜é€Ÿç¼“å­˜å­˜å‚¨å™¨ã€‚ L1ã€L2é«˜é€Ÿç¼“å­˜ç”¨ä¸€ç§é™æ€éšæœºè®¿é—®å­˜å‚¨å™¨ï¼ˆSRAMï¼‰çš„ç¡¬ä»¶æŠ€æœ¯å®ç°çš„ã€‚è®©é«˜é€Ÿç¼“å­˜å­˜æ”¾å¯èƒ½ç»å¸¸è®¿é—®çš„æ•°æ®æ—¶ï¼Œå¯ä»¥å¤§å¤§æé«˜ç¨‹åºçš„æ€§èƒ½ã€‚ 1.6 å­˜å‚¨è®¾å¤‡å½¢æˆå±‚æ¬¡ç»“æ„ å­˜å‚¨å™¨å±‚æ¬¡å­˜å‚¨çš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œé€Ÿåº¦æ›´å¿«çš„ä¸€å±‚ä½œä¸ºä¸‹ä¸€å±‚çš„ç¼“å­˜åŒºã€‚ 1.7 æ“ä½œç³»ç»Ÿç®¡ç†ç¡¬ä»¶ æ“ä½œç³»ç»Ÿå¯ä»¥çœ‹åšåº”ç”¨ç¨‹åºå’Œç¡¬ä»¶ä¹‹é—´çš„ä¸€å±‚è½¯ä»¶ã€‚ 1.7.1 è¿›ç¨‹ä¸€ä¸ªCPUçœ‹ä¸Šå»åƒæ˜¯åœ¨å¹¶å‘çš„æ‰§è¡Œå¤šä¸ªè¿›ç¨‹ï¼Œè¿™æ˜¯é€šè¿‡å¤„ç†å™¨åœ¨è¿›ç¨‹é—´åˆ‡æ¢å®ç°çš„ã€‚æ“ä½œç³»ç»Ÿå®ç°è¿™ç§äº¤é”™æ‰§è¡Œçš„æœºåˆ¶ç§°ä¸ºä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚ç¬¬8ç« ä¼šè¯¦ç»†ä»‹ç»ã€‚ 1.7.2 çº¿ç¨‹ä¸€ä¸ªè¿›ç¨‹å®é™…ä¸Šå¯ä»¥ç”±å¤šä¸ªç§°ä¸ºçº¿ç¨‹çš„æ‰§è¡Œå•å…ƒç»„æˆï¼Œæ¯ä¸ªçº¿ç¨‹éƒ½è¿è¡Œåœ¨è¿›ç¨‹çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå¹¶å…±äº«åŒæ ·çš„ä»£ç å’Œå…¨å±€æ•°æ®ã€‚å¤šçº¿ç¨‹æ¯”å¤šè¿›ç¨‹æ›´å®¹æ˜“å…±äº«æ•°æ®ï¼Œçº¿ç¨‹ä¸€èˆ¬æ¥è¯´æ¯”è¿›ç¨‹æ›´é«˜æ•ˆã€‚ 1.7.3 è™šæ‹Ÿå†…å­˜ ç¨‹åºä»£ç å’Œæ•°æ® å †ï¼šåœ¨è¿è¡Œæ—¶åŠ¨æ€æ‰©å±•å’Œæ”¶ç¼©ã€‚ å…±äº«åº“ æ ˆï¼šç”¨æˆ·æ ˆï¼Œç¼–è¯‘å™¨ç”¨å®ƒæ¥å®ç°å‡½æ•°è°ƒç”¨ã€‚ å†…æ ¸è™šæ‹Ÿå†…å­˜ï¼šä¸ºå†…æ ¸ä¿ç•™çš„ã€‚ 1.7.4 æ–‡ä»¶æ–‡ä»¶å°±æ˜¯å­—èŠ‚åºåˆ—ã€‚æ¯ä¸ªI/Oè®¾å¤‡éƒ½å¯ä»¥çœ‹æˆæ˜¯æ–‡ä»¶ã€‚ç³»ç»Ÿçš„æ‰€æœ‰è¾“å…¥è¾“å‡ºéƒ½æ˜¯é€šè¿‡ä½¿ç”¨ä¸€å°ç»„ç§°ä¸ºUnix I/Oçš„ç³»ç»Ÿå‡½æ•°è°ƒç”¨è¯»å†™æ–‡ä»¶å®ç°çš„ã€‚ç¬¬10ç« ä¼šå­¦ä¹ Unix I/O 1.8 ç³»ç»Ÿä¹‹é—´åˆ©ç”¨ç½‘ç»œé€šä¿¡ ç½‘ç»œä¹Ÿæ˜¯ä¸€ç§I/Oè®¾å¤‡ã€‚ è¿™ç§å®¢æˆ·ç«¯ä¸æœåŠ¡å™¨ä¹‹é—´äº¤äº’çš„ç±»å‹åœ¨æ‰€æœ‰ç½‘ç»œåº”ç”¨ä¸­æ˜¯éå¸¸å…¸å‹çš„ã€‚åœ¨ç¬¬11ç« ä¸­ï¼Œä½ å°†ä¼šå­¦ä¼šå¦‚ä½•æ„é€ ç½‘ç»œåº”ç”¨ç¨‹åºï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç®€å•çš„WebæœåŠ¡å™¨ã€‚Exciting! 1.9 é‡è¦ä¸»é¢˜1.9.1 Amdahlå®šå¾‹ç³»ç»Ÿè¿è¡Œæ—¶é—´ $T_{old}$, ç³»ç»ŸæŸéƒ¨åˆ†æ‰§è¡Œæ—¶é—´å æ¯” $\\alpha(0&lt;\\alpha &lt;1)$, è¯¥éƒ¨åˆ†æå‡æ€§èƒ½æ¯”ä¾‹ $k(k&gt;1)$,åˆ™æ€»çš„æ‰§è¡Œæ—¶é—´ï¼š $$T_{new}=(1-\\alpha)T_{old}+(\\alpha T_{old})/k = T_{old}[(1-\\alpha)+\\alpha/k]$$ $$S=T_{old}/T_{new} = \\dfrac{1}{(1-\\alpha)+\\alpha/k}&lt;k$$ ç®€å•çš„æ•°å­¦è¿ç®—ï¼Œè¯æ˜åªæå‡ç³»ç»ŸæŸä¸€éƒ¨åˆ†çš„æ€§èƒ½ï¼Œå¯¹æ•´ä½“çš„åŠ é€Ÿæ˜æ˜¾å°äºè¿™ä¸€éƒ¨åˆ†æå‡çš„æ¯”ä¾‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¦åŠ é€Ÿæ•´ä¸ªç³»ç»Ÿï¼Œå¿…é¡»åŠ é€Ÿè¿™ä¸ªç³»ç»Ÿä¸­çš„å¤§éƒ¨åˆ†çš„é€Ÿåº¦ã€‚ ç»ƒä¹ é¢˜ 1.1 T_{old}=25 T_{new}=1500/150+(2500-1500)/100=20 åŠ é€Ÿæ¯”ï¼š T_{old}/T_{new}=1.25 1.9.2 å¹¶å‘ï¼ˆconcurrencyï¼‰å’Œå¹¶è¡Œï¼ˆparallelismï¼‰ çº¿ç¨‹çº§å¹¶å‘ å¤šæ ¸å¤„ç†å™¨ï¼š è¶…çº¿ç¨‹ï¼Œæœ‰æ—¶åˆç§°ä¸ºåŒæ—¶å¤šçº¿ç¨‹ï¼ˆsimultaneous multi-threadingï¼‰ï¼Œæ˜¯ä¸€é¡¹å…è®¸ä¸€ä¸ªCPUæ‰§è¡Œå¤šä¸ªæ§åˆ¶æµçš„æŠ€æœ¯ã€‚ç¬¬12ç« ä¼šæ›´æ·±å…¥çš„è®¨è®ºå¹¶å‘ã€‚ æŒ‡ä»¤é›†å¹¶è¡Œ æœ€è¿‘çš„å¤„ç†å™¨å¯ä»¥ä¿æŒæ¯ä¸ªå§‹ç»ˆ2~4æ¡æŒ‡ä»¤çš„æ‰§è¡Œæ•ˆç‡ã€‚å…¶å®æ¯æ¡æŒ‡ä»¤ä»å¼€å§‹åˆ°ç»“æŸéœ€è¦é•¿å¾—å¤šçš„æ—¶é—´ï¼Œå¤§çº¦20ä¸ªæˆ–æ›´å¤šä¸ªå‘¨æœŸã€‚åœ¨ç¬¬4ç« ï¼Œä¼šç ”ç©¶æµæ°´çº¿(pipelining)çš„ä½¿ç”¨ã€‚åœ¨æµæ°´çº¿ä¸­ï¼Œå°†æ‰§è¡Œä¸€æ¡æŒ‡ä»¤æ‰€éœ€è¦çš„æ´»åŠ¨åˆ’åˆ†æˆä¸åŒçš„æ­¥éª¤ï¼Œå°†å¤„ç†å™¨çš„ç¡¬ä»¶ç»„ç»‡æˆä¸€äº›åˆ—çš„é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæ‰§è¡Œä¸€ä¸ªæ­¥éª¤ã€‚ å¤„ç†å™¨è¾¾åˆ°æ¯”ä¸€ä¸ªå‘¨æœŸä¸€æ¡æŒ‡ä»¤æ›´å¿«çš„æ‰§è¡Œé€Ÿç‡ï¼Œå°±ç§°ä¹‹ä¸ºè¶…æ ‡é‡ï¼ˆsuper-scalarï¼‰å¤„ç†å™¨ã€‚ å•æŒ‡ä»¤ã€å¤šæ•°æ®å¹¶è¡Œ ä¸€æ¡æŒ‡ä»¤äº§ç”Ÿå¤šä¸ªå¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ“ä½œï¼Œè¿™ç§æ–¹å¼ç§°ä¸ºå•æŒ‡ä»¤ã€å¤šæ•°æ®ï¼Œå³ SIMD å¹¶è¡Œã€‚ 1.9.3 è®¡ç®—æœºç³»ç»Ÿä¸­æŠ½è±¡çš„é‡è¦æ€§ æ–‡ä»¶æ˜¯å¯¹ I/O è®¾å¤‡çš„æŠ½è±¡ï¼Œè™šæ‹Ÿå†…å­˜æ˜¯å¯¹ç¨‹åºå­˜å‚¨å™¨çš„æŠ½è±¡ï¼Œè€Œè¿›ç¨‹æ˜¯å¯¹ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„ç¨‹åºçš„æŠ½è±¡ã€‚ è™šæ‹Ÿæœºæ˜¯å¯¹æ•´ä¸ªè®¡ç®—æœºçš„æŠ½è±¡ã€‚ 1.10 å°ç»“","link":"/2018/06/08/CSAPP-01-A-tour-of-computer-system/"},{"title":"NLPç®—æ³•-å®ä¹ é¢è¯•ç»éªŒ","text":"æ€å¿…é©°ä¸€é¢çš„å¤§å“¥åº”è¯¥ä¸»è¦æ˜¯åšå›¾åƒï¼Œå¹¶ä¸”åå·¥ç¨‹æ–¹é¢çš„ã€‚åŸºæœ¬ä¸Šæ²¡é—® NLP ç›¸å…³çš„é—®é¢˜ï¼Œä¸»è¦é—®äº†äº›å·¥ç¨‹æ–¹é¢çš„é—®é¢˜å’Œ CNN ç›¸å…³çš„ã€‚ äºŒé¢çš„è€å“¥åˆ™ä¸»è¦é—®äº†ç®€å†ä¸Šç›¸å…³çš„ NLP ç»éªŒã€‚ æ•°æ®ç»“æ„ä¸ç®—æ³• å¿«é€Ÿæ’åº å•å‘æœ‰ç¯é“¾è¡¨æ€ä¹ˆåˆ¤æ–­æ˜¯å¦æœ‰ç¯ æ·±åº¦å­¦ä¹  å·ç§¯æ ¸ä¸º 1 çš„ CNN çš„ä¸»è¦ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ æƒé‡åˆå§‹åŒ–çš„æ–¹å¼æœ‰å“ªäº›ï¼Ÿ Xavier çš„æ¨å¯¼ã€‚ æ‰‹æ¨ BP ç¥ç»ç½‘ç»œã€‚ NLP attention ç†Ÿæ‚‰å—ï¼Ÿå…·ä½“å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ BERT æ¨¡å‹æœ‰å¤šå°‘å±‚ï¼Ÿåœ¨æœºå™¨é˜…è¯»ç†è§£çš„ä»»åŠ¡ä¸Šæ˜¯æ€ä¹ˆè¿›è¡Œ fine-tune çš„ï¼Ÿ Transformer ä¸­ multi-head çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ other SQL ç†Ÿæ‚‰å—ï¼Ÿ C++ ä¸­ æ–°æµªå¾®åšé¢è¯•çš„å¤§å“¥è¶…çº§æœ‰äº²å’ŒåŠ›ï¼Œæ„Ÿè§‰äººéå¸¸å¥½ã€‚é¢å¯¹æˆ‘æ¸£å¦‚ç²ªåœŸçš„ coding èƒ½åŠ›ï¼Œä¾ç„¶è¡¨ç¤ºç†è§£ã€‚å¹¶ä¸”èƒ½è·Ÿä½ ä¸€èµ·æ¢è®¨ç®—æ³•çš„æ€è·¯ï¼Œ å¤§ä½¬å…ˆè¿›è¡Œäº†è‡ªæˆ‘ä»‹ç»ï¼Œè¡¨ç¤ºä»–æ˜¯åœ¨ç¾å›½è¯»çš„ phdï¼Œåœ¨å¾®è½¯å·¥ä½œäº† 10 å¹´ç„¶åè¢«æ–°æµªæŒ–è¿‡æ¥ï¼Œéƒ¨é—¨ä¸»è¦ä»»åŠ¡æ˜¯é€šè¿‡ NLP ç®—æ³•çš„åˆ†æï¼Œæé«˜å¹¿å‘Šçš„ç²¾å‡†æŠ•æ”¾åŠ›åº¦çš„ã€‚ èŠå¤© ä½ åœ¨ç®€å†ä¸Šç›¸å…³é¡¹ç›®ä¸­ï¼Œæœ€ challenge çš„ä¸€æ®µç»å†æ˜¯ä»€ä¹ˆï¼Ÿ éƒ½è¯´æ·±åº¦å­¦ä¹ æ˜¯ç„å­¦è°ƒå‚ï¼Œä½ è§‰å¾—ä¸€ä¸ªä»ä¸šäº”å¹´å’Œä¸€ä¸ªä»ä¸šä¸€å¹´çš„æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆæœ‰ä½•åŒºåˆ«ï¼Ÿ ä½œä¸ºè½¬è¡Œç”Ÿï¼Œä½ è§‰å¾—ä½ ä»¥å‰çš„ç»éªŒå¯¹ç°åœ¨æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ coding çƒé˜ŸæŠ½ç­¾ æœ‰ N åªè¶³çƒé˜Ÿï¼Œçƒé˜Ÿçš„å¼ºå¼±åˆ†åˆ«æ˜¯ 1&lt;2&lt;3&lt;â€¦.&lt;Nï¼Œæ¯æ¬¡æŠ½å–ä¸¤åªçƒé˜Ÿã€‚æœ‰å¦‚ä¸‹ä¸¤ä¸ªæ¡ä»¶ï¼š random æŠ½ç­¾ï¼Œç»è¿‡è¶³å¤Ÿå¤šçš„æ¬¡æ•°ï¼Œæ¯åªçƒé˜Ÿéƒ½ä¼šè¢«æŠ½åˆ° æŠ½åˆ°å¼ºé˜Ÿçš„æ¦‚ç‡æ›´å¤§ æ‰¾æ˜æ˜Ÿ åœ¨ä¸€ä¸ªå­¦æ ¡æœ‰ N ä¸ªäººï¼Œå…¶ä¸­å¯èƒ½æœ‰ä¸€ä½æ˜æ˜Ÿã€‚æ‰¾å‡ºè¿™ä¸ªæ˜æ˜Ÿï¼Œå…¶ä¸­æ˜æ˜Ÿæ»¡è¶³å¦‚ä¸‹ä¸¤ä¸ªæ¡ä»¶ï¼š everybody know him he donnâ€™t know any one else. ç½‘æ˜“æœ‰é“èŠå¤© ä¸ºä»€ä¹ˆä»æœºæ¢°è½¬è¡Œåˆ°è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ ç®€å•ä»‹ç»ä¸‹åœ¨ä¸‰æ˜Ÿç ”ç©¶é™¢çš„å·¥ä½œã€‚è¯¦ç»†è¯´ä¸€ä¸‹æœºå™¨é˜…è¯»ç†è§£çš„æµç¨‹ã€‚ BERT æ¨¡å‹ä¸ºä»€ä¹ˆå¥½ï¼Ÿä»æ›´é«˜å±‚é¢è°ˆè®ºä¸‹ BERT æ¨¡å‹æå‡ºçš„æ„ä¹‰ã€‚ multi-head çš„å…·ä½“å®ç°å’Œä½œç”¨ã€‚ codingå•é¡¹é“¾è¡¨æ˜¯å¦æ˜¯å›æ–‡ã€‚","link":"/2019/01/23/NLP%E7%AE%97%E6%B3%95-%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C/"},{"title":"UCB-cs294-policy gradient","text":"Policy Gradient","link":"/2019/08/12/UCB-cs294-policy-gradient/"},{"title":"FAIR-æ— ç›‘ç£æœºå™¨ç¿»è¯‘","text":"æ— ç›‘ç£æœºå™¨ç¿»è¯‘çš„å‡ ç¯‡paper: Word Translation without Parallel Data - ICLRâ€™18 Unsupervised machine translation using monolingual corpora only, Lample, et al. ICLR 2018a Unsupervised neural machine translation, ICLR 2018 Phrase-based &amp; neural unsupervised machine translation. emnlp 2018b Cross-lingual Language Model Pretraining Neural word embedding as implicit matrix factorization Phrase-Based &amp; Neural Unsupervised Machine Translationå¯¹å‰ä¸¤ç¯‡æ— ç›‘ç£æœºå™¨ç¿»è¯‘è¿›è¡Œäº†ä¸€ä¸ªæ€»ç»“ï¼š carefully initialize the MT system with an inferred bilingual dictionary. é€šè¿‡åŒè¯­å­—å…¸å¯¹MTæ¨¡å‹è¿›è¡Œåˆå§‹åŒ–ã€‚ leverage strong language models, via training the sequence-to-sequence system as a denoising autoencoder. é€šè¿‡è®­ç»ƒseq2eqæ¨¡å‹æ¥åˆ©ç”¨å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ä½œä¸ºé™å™ªè‡ªç¼–ç ã€‚ turn the unsupervised problem into a supervised one by automatic generation of sentence pairs via back-translation.æŠŠæ— ç›‘ç£é—®é¢˜è½¬æ¢ä¸ºæœ‰ç›‘ç£çš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡back-translationè‡ªåŠ¨ç”Ÿæˆè¯­è¨€å¯¹ã€‚ è¿™ç¯‡è®ºæ–‡çš„ä½œè€…å°†ä¸Šè¿°æ–¹æ³•åšäº†ä¸ªæ•´åˆï¼Œå¾—åˆ°çš„NMT ç³»ç»Ÿåœ¨æ— ç›‘ç£ç¿»è¯‘ä¸Šèƒ½è¾¾åˆ° +10 BLEU, å¹¶ä¸”åº”ç”¨åˆ°phrase-based MTä¸Šï¼Œè¾¾åˆ°äº† +12 BLEU. ä½œè€…å°†æ— ç›‘ç£æœºå™¨ç¿»è¯‘æŠ½è±¡æˆä¸Šè¿°è¿‡ç¨‹ã€‚ B. åˆå§‹åŒ– C. è¯­è¨€æ¨¡å‹ D. è¿­ä»£åå‘ç¿»è¯‘ã€‚ Initializationå‰äººçš„åˆå§‹åŒ–æ–¹æ³•ï¼š åˆ©ç”¨æ„æ€ç›¸è¿‘çš„è¯ã€çŸ­è¯­æˆ–æ˜¯ subword bilingual dictionary dictionaries inferred in an unsupervised way. Lample et al. (2018) and Artetxe et al. (2018) è¿™ç§åˆå§‹åŒ–çš„æ–¹å¼å¯¹äºè·ç¦»ç›¸è·è¾ƒè¿œçš„è¯­è¨€å¯èƒ½æ•ˆæœä¸å¤ªå¥½ã€‚æ¯”å¦‚ä¸­è‹±ï¼Ÿ ä½œè€…çš„åˆå§‹åŒ–æ–¹æ³•ï¼š å…ˆå¯¹sourceå’Œtarget languageè¿›è¡Œbpeå¤„ç†(bpeçš„ä¼˜åŠ¿ï¼šå‡å°è¯è¡¨å¤§å°ï¼Œæ¶ˆé™¤unknow word)ï¼Œç„¶åè”åˆèµ·æ¥ï¼ˆè€Œä¸æ˜¯åˆ†å¼€ï¼‰è®­ç»ƒword embedding. join the monolingual corpora apply BPE tokenization on the resulting corpus learn token embeddings (Mikolov et al., 2013) on the same corpus Language ModelingåŸºäºå•è¯­è®­ç»ƒå¾—åˆ°çš„è¯­è¨€æ¨¡å‹ï¼Œä¸»è¦æ˜¯é€šè¿‡ local substitutions and word reorderings æ¥æå‡ç¿»è¯‘çš„è´¨é‡ï¼ˆä¹Ÿå°±æ˜¯ contextual informationï¼‰ã€‚ ä½œè€…çš„ language model training åŸºäº denosing autoencoder. C is a noise model with some words dropped and swapped. $P_{sâ†’s}$ and $P_{tâ†’t}$ are the composition of encoder and decoder both operating on the source and target sides, respectively. Back-translation: Iterative Back-translationDual Learning for Machine Translation $fr \\rightarrow \\hat{en} \\rightarrow fr$ fr æ˜¯ target language. en æ˜¯ source language. å…ˆåˆ©ç”¨åå‘æ¨¡å‹ç¿»è¯‘å¾—åˆ° pesudo en sentence $\\hat{en}$. ç„¶åå°† $(\\hat{en}, fr)$ ä½œä¸ºç¿»è¯‘å¯¹è¿›è¡Œæœ‰ç›‘ç£çš„å­¦ä¹ ã€‚å°½ç®¡ $\\hat{en}$ ä¼šéå¸¸ noisyï¼Œä½†ä¿è¯äº†targetç«¯æ˜¯pure sentenceï¼Œæ•ˆæœç¡®å®ä¸é”™å§ã€‚ ä½œè€…çš„ iteration BT ä¸ä¸Šè¿°æ–¹æ³•ä¸€è‡´ï¼š $u^{* }(y)=argmaxP_{tâ†’s}(u|y)$, $u^{* }(y)$ æ˜¯ pesudo source sentence. $v^{* }(x)=argmaxP_{sâ†’t}(v|x)$, $v^{* }(x)$ æ˜¯ pesudo target sentence. ä½œè€…åœ¨å®éªŒæ—¶ï¼Œå¹¶æ²¡æœ‰å¯¹ $u\\rightarrow u^{* }, v\\rightarrow v^{* }$ è¿™ä¸ªè¿‡ç¨‹è¿›è¡Œä¼˜åŒ–ï¼Œå› ä¸ºè¿™åœ¨å®éªŒä¸­å¹¶æ²¡æœ‰æå‡ã€‚åŒæ—¶åœ¨è®­ç»ƒæ—¶ï¼Œä½œè€…æ˜¯ç®€å•çš„å°† $L^{back}$ å’Œ $L^{lm}$ åŠ èµ·æ¥è¿›è¡Œä¼˜åŒ–ã€‚ ä¸ºäº†é¿å…æ¨¡å‹æ··æ·†ä¸¤ä¸ªè¯­è¨€çš„å‘é‡ç©ºé—´ï¼Œä½œè€…æä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ³•ï¼ŒSharing Latent Representations. ä¹Ÿå°±æ˜¯ denosing aotoencoder å’Œ back-translation å¯ä»¥ç”¨åŒä¸€ä¸ª encoder-decoder æ¨¡å‹ï¼Œä¸åŒè¯­è¨€ä¹‹é—´å…±äº« encoder å’Œ decoder. While sharing the encoder is critical to get the model to work, shar- ing the decoder simply induces useful regularization. å…±äº« encoder å¯¹äºæ— ç›‘ç£æ¨¡å‹éå¸¸å…³é”®ï¼Œè€Œ decoder çš„å…±äº«åˆ™ä¸»è¦æ˜¯æä¾›æœ‰æ•ˆçš„æ­£åˆ™åŒ–ã€‚è¿™ä¸ GNMT æ¨¡å‹ä¸ä¸€æ ·ï¼Œä¸éœ€è¦åŠ ä¸Š tag æ¥æŒ‡å®šç¿»è¯‘æ–¹å‘ã€‚ Result","link":"/2019/11/21/FAIR-%E6%97%A0%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"},{"title":"SCAPP-02-ä¿¡æ¯çš„è¡¨ç¤ºå’Œå¤„ç†","text":"CSAPP ç¬¬äºŒç«  ä¿¡æ¯çš„è¡¨ç¤ºå’Œå¤„ç†ä¸‰ç§æ•°å­—è¡¨ç¤ºï¼š æ— ç¬¦å·(unsigned)ç¼–ç ï¼šå¤§äºæˆ–ç­‰äºé›¶çš„æ•°å­— è¡¥ç ï¼ˆtwoâ€™s-complementï¼‰ç¼–ç ï¼šè¡¨ç¤ºæœ‰ç¬¦å·æ•´æ•°æœ€å¸¸è§çš„æ–¹å¼ æµ®ç‚¹æ•°(floating-point)ç¼–ç ï¼š æº¢å‡ºï¼ˆoverflowï¼‰:è®¡ç®—æœºçš„è¡¨ç¤ºæ³•æ˜¯ç”¨æœ‰é™æ•°é‡çš„ä½æ¥å¯¹ä¸€ä¸ªæ•°å­—ç¼–ç ã€‚å› æ­¤ç»“æœå¤ªå¤§è‡³æ— æ³•è¡¨ç¤ºæ—¶ï¼ŒæŸäº›è¿ç®—å°±ä¼šæº¢å‡ºã€‚ 2.1 ä¿¡æ¯å­˜å‚¨è®¡ç®—æœºä½¿ç”¨8ä½çš„å—ï¼Œæˆ–è€…å­—èŠ‚ï¼ˆbyteï¼‰ã€‚ä½œä¸ºæœ€å°çš„å¯å¯»å€çš„å†…å­˜å•å…ƒï¼Œè€Œä¸æ˜¯å•ç‹¬çš„ä½ã€‚ æœºå™¨çº§ç¨‹åºå°†å†…å­˜è§†ä¸ºä¸€ä¸ªéå¸¸å¤§çš„æ•°å­—æ¥æ ‡è¯†ï¼Œç§°ä¸ºå®ƒçš„å†…å­˜ã€‚å†…å­˜çš„æ¯ä¸€ä¸ªå­—èŠ‚éƒ½ç”±ä¸€ä¸ªå”¯ä¸€çš„æ•°å­—æ¥æ ‡è¯†ï¼Œç§°ä¸ºå®ƒçš„åœ°å€ï¼Œæ‰€æœ‰å¯èƒ½åœ°å€çš„é›†åˆç§°ä¸ºè™šæ‹Ÿåœ°å€ç©ºé—´(virtual address space)ã€‚ æ¥ä¸‹æ¥å‡ ç« å°†è®²è¿°ç¼–è¯‘å™¨å’Œè¿è¡Œæ—¶ç³»ç»Ÿæ˜¯å¦‚ä½•å°†å­˜å‚¨å™¨ç©ºé—´åˆ’åˆ†ä¸ºæ›´å¯ç®¡ç†çš„å•å…ƒï¼Œæ¥å­˜æ”¾ä¸åŒçš„ç¨‹åºå¯¹è±¡ï¼Œå³ç¨‹åºæ•°æ®ã€æŒ‡ä»¤å’Œæ§åˆ¶ä¿¡æ¯ã€‚ 2.1.1 åå…­è¿›åˆ¶è¡¨ç¤ºæ³•ä¸€ä¸ªå­—èŠ‚æ˜¯8ä½ï¼š å¯¹äºäºŒè¿›åˆ¶è¡¨ç¤ºæ³•ï¼Œå€¼åŸŸæ˜¯ $00000000_2-11111111_2$ å¯¹äºåè¿›åˆ¶è¡¨ç¤ºæ³•ï¼Œå€¼åŸŸæ˜¯ $0_{10}-255_{10}$ å¯¹åå…­è¿›åˆ¶è¡¨ç¤ºæ³•ï¼Œå€¼åŸŸæ˜¯ $00_{16}-FF_{16}$ ç»ƒä¹ é¢˜ 2.1 A. å°†ox39A7F8è½¬æ¢ä¸ºäºŒè¿›åˆ¶. 0011 1001 1010 0111 1111 1000 B. å°†äºŒè¿›åˆ¶ 1100 1001 0111 1011 è½¬æ¢ä¸ºåå…­è¿›åˆ¶ã€‚ oxE97D ç»ƒä¹ é¢˜ 2.2 $2^n=2^{i+4j}$ å¯ä»¥å¾ˆå®¹æ˜“å†™æˆåå…­è¿›åˆ¶å°±æ˜¯ 2^iåé¢è·Ÿç€jä¸ª0 æ¯”å¦‚ï¼š $2^9$ -&gt; $2^{1+2x4}$ -&gt; ox200 $2^19$ -&gt; $2^{3+4x4}$ -&gt; ox80000 è¿›åˆ¶çš„è½¬æ¢ å°†åè¿›åˆ¶è½¬æ¢ä¸º16è¿›åˆ¶ï¼Œå…¶å®å¯ä»¥ç†è§£ä¸ºå°†åè¿›åˆ¶è½¬æ¢ä¸ºåè¿›åˆ¶ï¼Œå°±æ˜¯é™¤ä»¥10ï¼Œä½™æ•°åˆ†åˆ«æ˜¯ä¸ªä½ã€ç™¾ä½â€¦. åŒæ ·çš„é“ç†ï¼Œè½¬æ¢ä¸º16è¿›åˆ¶å°±æ˜¯é™¤ä»¥16 åå…­è¿›åˆ¶è½¬æ¢ä¸º10è¿›åˆ¶å°±æ›´ç®€å•äº†ï¼Œ16çš„å¹‚ä¹˜ä»¥æ¯ä¸ªåå…­è¿›åˆ¶æ•°ã€‚ 2.1.2 å­—æ•°æ®å¤§å°å¯¹äºä¸€ä¸ªå­—é•¿ä¸ºwä½çš„æœºå™¨ï¼Œè™šæ‹Ÿåœ°å€çš„èŒƒå›´ä¸º0~$2^w-1$ï¼Œç¨‹åºæœ€å¤šè®¿é—®$2^w$ä¸ªå­—èŠ‚ã€‚ 2.1.3 å¯»å€å’Œå­—èŠ‚é¡ºåºåœ¨å‡ ä¹æ‰€æœ‰çš„æœºå™¨ä¸Šï¼Œå¤šå­—èŠ‚å¯¹è±¡éƒ½è¢«å­˜å‚¨ä¸ºè¿ç»­çš„å­—èŠ‚åºåˆ—ï¼Œå¯¹è±¡çš„åœ°å€ä¸ºæ‰€ä½¿ç”¨å­—èŠ‚ä¸­æœ€å°çš„åœ°å€ã€‚ å‡è®¾å˜é‡xçš„ç±»å‹ä¸ºintï¼Œ ä½äºåœ°å€0x100å¤„ï¼Œå®ƒçš„åå…­è¿›åˆ¶å€¼ä¸º0x01234567. åå…­è¿›åˆ¶çš„ä¸¤ä½å ä¸€ä¸ªå­—èŠ‚ï¼Œå› æ­¤å…¶åœ°å€èŒƒå›´ 0x100~0x103. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include &lt;stdio.h&gt;typedef unsigned char *byte_pointer; // å­—ç¬¦æŒ‡é’ˆ// ä½¿ç”¨ typedef æ¥å‘½åæ•°æ®ç±»å‹void show_bytes(byte_pointer start, size_t len){ size_t i; for (i=0; i&lt;len; i++) printf(&quot; %.2x&quot;, start[i]); printf(&quot;\\n&quot;);}// size_t è¡¨ç¤ºsize typeï¼Œå¤§å°çš„ç±»å‹ï¼Œæ„å‘³ç€å®ƒæ˜¯sizeofè¿ç®—ç¬¦ç»“æœçš„ç±»å‹// åœ¨Cè¯­è¨€ä¸­ï¼Œå¯ä»¥ç”¨æŒ‡é’ˆè¡¨ç¤ºæ³•æ¥å¼•ç”¨æ•°ç»„å…ƒç´ ã€‚å¼•ç”¨start[i]è¡¨ç¤ºæˆ‘ä»¬æƒ³è¦è¯»å–ä»¥startæŒ‡å‘çš„ä½ç½®ä¸ºèµ·å§‹çš„ç¬¬iä¸ªä½ç½®å¤„çš„å­—èŠ‚ã€‚void show_int(int x){ show_bytes((byte_pointer)&amp;x, sizeof(int));}void show_float(float x){ show_bytes((byte_pointer)&amp;x, sizeof(float));}void show_pointer(void *x){ show_bytes((byte_pointer)&amp;x, sizeof(void *));}// C å’Œ C++ ä¸­ç‹¬æœ‰çš„æ“ä½œï¼Œ Cçš„â€œå–åœ°å€â€è¿ç®—ç¬¦ &amp; åˆ›å»ºä¸€ä¸ªæŒ‡é’ˆ &amp;x, è¿™ä¸ªæŒ‡é’ˆçš„ç±»å‹å–å†³äº x çš„ç±»å‹ï¼Œå› æ­¤è¿™ä¸‰ä¸ªæŒ‡é’ˆç±»å‹åˆ†åˆ«ä¸º int*, float*, void**, æ•°æ®ç±»å‹ void * æ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„æŒ‡é’ˆï¼Œæ²¡æœ‰ç›¸å…³è”çš„ç±»å‹ä¿¡æ¯ã€‚// (byte_pointer)&amp;x è¡¨ç¤ºå¼ºåˆ¶è½¬æ¢ï¼Œæ— è®º &amp;x ä¹‹å‰æ˜¯ä»€ä¹ˆç±»å‹ï¼Œç°åœ¨å°±æ˜¯ä¸€ä¸ªæŒ‡å‘ unsigned charçš„æŒ‡é’ˆã€‚ä½†è¿™å¹¶ä¸ä¼šæ”¹å˜çœŸå®çš„æŒ‡é’ˆï¼Œåªæ˜¯ç¼–è¯‘å™¨ä»¥æ–°çš„æ•°æ®ç±»å‹æ¥çœ‹å¾…è¢«æŒ‡å‘çš„æ•°æ®ã€‚void test_show_bytes(int val){ int ival = val; float fval = (float) ival; int * pval = &amp;ival; show_int(ival); show_float(fval); show_pointer(pval);}int main(){ test_show_bytes(12345); return 0;} æˆ‘çš„æœºå™¨æ˜¯Linux64ï¼Œè¿è¡Œç»“æœæ˜¯è¿™æ ·çš„ï¼š åˆ†æä¸‹ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·: 12345ï¼Œåå…­è¿›åˆ¶ä¸º ox3039, å¯¹äºintæ•°æ®é‡‡ç”¨çš„æ˜¯4å­—èŠ‚ï¼š äºŒè¿›åˆ¶æ•°è¡¨ç¤ºæ˜¯: 0000 0000 0000 0000 0011 0000 0011 1001 åœ¨å°ç«¯æ³•æœºå™¨çš„åå…­è¿›åˆ¶è¡¨ç¤ºä¸º: 39 30 00 00 åœ¨å¤§ç«¯æ³•æœºå™¨çš„åå…­è¿›åˆ¶è¡¨ç¤ºä¸º: 00 00 30 39 ä»¥ä¸€ä¸ªå­—èŠ‚ä¸ºå•ä½ï¼Œåå…­è¿›åˆ¶ä¸€ä¸ªå­—èŠ‚å ä¸¤ä½ï¼Œæ‰€ä»¥æ¯ä¸¤ä¸ªæ•°ä¸€ä¸ªå­—èŠ‚ã€‚äºŒè¿›åˆ¶æ˜¯æ¯8ä¸ªæ•°ä¸€ä¸ªå­—èŠ‚ã€‚ ä¸ºä»€ä¹ˆæµ®ç‚¹æ•° 12345.0 çš„åå…­è¿›åˆ¶æ˜¯è¿™æ ·çš„å‘¢ï¼Ÿä¹‹åä¼šè®²åˆ°ï½ ç»ƒä¹ é¢˜ 2.5 åå…­è¿›åˆ¶æ•° ox87654321 å°ç«¯æ³•ï¼š 21 43 65 87 A. å°ç«¯æ³•ï¼š 21 å¤§ç«¯æ³•ï¼š87 B. å°:21 43 å¤§ï¼š 87 65 C. å°:21 43 65 å¤§: 87 65 43 ç»ƒä¹ é¢˜ 2.6 3510593 -&gt; ox00359141 -&gt; 0000 0000 0011 0101 1001 0001 0100 0001 3510593.0 -&gt; ox4A564504 -&gt; 0100 1010 0101 0110 0100 0101 0000 0100 æœ€å¤§åŒ¹é…ä½æ•°ï¼š 2.1.4 è¡¨ç¤ºå­—ç¬¦ä¸²æ¯ä¸ªå­—ç¬¦å¯¹åº”ä¸€ä¸ªASCIIç ã€‚æ€»å…±æœ‰127ä¸ªasciiç ã€‚ åè¿›åˆ¶æ•°xçš„ASCIIç æ­£å¥½æ˜¯ 0x3x, æ¯”å¦‚è¦æ˜¾ç¤ºå­—ç¬¦ 0ï¼Œå…¶ASCIIç æ˜¯48ï¼Œç”¨åå…­è¿›åˆ¶å°±æ˜¯0x30. â€˜Aâ€™â€˜Zâ€™çš„ASCIIç åè¿›åˆ¶è¡¨ç¤ºæ˜¯ 6590ï¼Œâ€˜aâ€™â€˜zâ€™åè¿›åˆ¶æ˜¯ 97122ï¼Œåå…­è¿›åˆ¶æ˜¯ox61~0x7A. nullçš„ASCIIç æ˜¯ 0ï¼Œä¹Ÿå°±æ˜¯0x00. 1234567891011121314151617181920212223void show_bytes(byte_pointer start, size_t len){ size_t i; for (i=0; i&lt;len; i++) printf(&quot; %.2x&quot;, start[i]); printf(&quot;\\n&quot;);}const char *s = &quot;abcdef&quot;;show_bytes((byte_pointer) s, strlen(s)); è¿è¡Œç»“æœï¼š 61 62 63 64 65 66 2.1.5 è¡¨ç¤ºä»£ç  ä¸åŒæœºå™¨ç±»å‹ä½¿ç”¨ä¸åŒçš„ä¸”ä¸å…¼å®¹çš„æŒ‡ä»¤å’Œç¼–ç æ–¹å¼ã€‚ 2.1.6 å¸ƒå°”ä»£æ•°ç®€ä»‹0å’Œ1çš„ç ”ç©¶ï½ éã€ä¸ã€æˆ–ã€å¼‚æˆ–(è¡¨ç¤ºPæˆ–è€…Qä¸ºçœŸï¼Œä½†ä¸åŒæ—¶ä¸ºçœŸæ—¶ï¼ŒP^Qä¸ºçœŸ) å¸ƒå°”è¿ç®—å¯æ‹“å±•åˆ°ä½å‘é‡çš„è¿ç®—ï¼š a=[0110], b=[1100], é‚£ä¹ˆ a&amp;b, a|b, a^b, ~b åˆ†åˆ«ä¸ºï¼š ä½å‘é‡å¯ä»¥ç”¨æ¥è¡¨ç¤ºæœ‰é™é›†åˆã€‚$[a_{w-1},â€¦,a_1,a_0]$ï¼ˆæ³¨æ„æ˜¯ $a_{w-1}$ åœ¨å·¦è¾¹ï¼Œ$a_0$ åœ¨å³è¾¹ï¼‰ ç¼–ç ä»»ä½•å­é›† $A\\in {0,1,2,â€¦,w-1}$. æ¯”å¦‚ a=[00101]å°±è¡¨ç¤º A={0,2}. å…¶å®å¯ä»¥æŠŠ a çœ‹åšå¤šæ ‡ç­¾çš„ one-hotå‘é‡ã€‚ã€‚ã€‚ ç»ƒä¹ é¢˜ 2.9 è“è‰²|ç»¿è‰² = [001]|[010] = [011] = è“ç»¿è‰² çº¢è‰² ^ çº¢ç´«è‰² = [100]^[101] = [001] = è“è‰² 2.1.7 Cè¯­è¨€ä¸­çš„ä½çº§è¿ç®—å¸ƒå°”ä½è¿ç®—ã€‚ ä»åå…­è¿›åˆ¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶ï¼Œè¿›è¡Œè¿ç®—ï¼Œåœ¨è½¬æ¢å›åå…­è¿›åˆ¶ã€‚ ç»ƒä¹ é¢˜ 2.10 ä»»æ„ä¸€ä½å‘é‡ aï¼Œæœ‰ a ^ a = 0. é‚£ä¹ˆ a^a^b=b. 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;stdio.h&gt;void inplace_swap(int *x, int *y){ * y = * x ^ * y; * x = * x ^ * y; * y = * x ^ * y;}int main(){ int a=5, b=4; printf(&quot;%p, %p\\n&quot;,&amp;a, &amp;b); inplace_swap(&amp;a, &amp;b); printf(&quot;%p, %p\\n&quot;,&amp;a, &amp;b); printf(&quot;%i, %i\\n&quot;, a, b); return 0;} è¿è¡Œç»“æœï¼š 12345670x7ffe90686210, 0x7ffe906862140x7ffe90686210, 0x7ffe906862144, 5 æˆ‘ä»¬å‘ç°è¿™é‡Œå…¶å®aå’Œbçš„åœ°å€æ²¡æœ‰æ”¹å˜ï¼Œæ˜¯åœ°å€å¯¹åº”çš„å€¼åœ¨è¿›è¡Œå¼‚æˆ–è¿ç®—ã€‚ 5^4=1, 5^1=4, 4^1=5. è¿™å°±æ˜¯äºŒè¿›åˆ¶è¿ç®—ï¼Œä¹Ÿå°±æ˜¯ä½è¿ç®—ã€‚ 6^5=3, 6^3=5, 5^3=6. ä¸è¿‡ä¸è¦è¯¯ä¼šï¼Œè·ŸåŠ å‡æ³•æ²¡å…³ç³»ï¼ŒåŸç†è¿˜æ˜¯äºŒè¿›åˆ¶ a^a=0. æ‰€ä»¥èƒ½è¿›è¡Œäº¤æ¢ï¼Œå°±æ˜¯å› ä¸º a^b^a=b. ç»“æœæ˜¯ï¼š step1 a a^b step2 a^(a^b)=b a^b step3 b b^(a^b)=a ç»ƒä¹ é¢˜ 2.11 å°† &lt;= æ”¹ä¸º &lt; å³å¯ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;stdio.h&gt;void inplace_swap(int *x, int *y){ * y = * x ^ * y; * x = * x ^ * y; * y = * x ^ * y;}void reverse_array(int a[], int cnt){ int first, last; for (first = 0, last = cnt -1; first &lt; last; first++, last--) inplace_swap(&amp;a[first], &amp;a[last]);}int main(){ int a[] = {1,2,3,4,5}; reverse_array(a, 5); for (int i=0; i&lt;5; i++) printf(&quot;%i&quot;, a[i]); printf(&quot;\\n&quot;); return 0;} 2.1.8 Cè¯­è¨€ä¸­çš„é€»è¾‘è¿ç®—åŒ…æ‹¬ ||, &amp;&amp; å’Œ !, é€»è¾‘è¿ç®—è®¤ä¸ºæ‰€æœ‰éé›¶å‚æ•°ä¸ºTRUEï¼Œå‚æ•°0 è¡¨ç¤º FALSE. è¦æ³¨æ„ä¸ä½è¿ç®—åŒºåˆ†å¼€æ¥ã€‚ ä¹ é¢˜ 2.15 if ((x^y)|(x^y)) return 1 else return 0 2.1.9 Cè¯­è¨€ä¸­çš„ç§»ä½è¿ç®—x &lt;&lt; k: è¡¨ç¤ºå‘å·¦ç§»åŠ¨kä½ï¼Œå³è¾¹è¡¥0 x &gt;&gt; k: æœ‰ä¸¤ç§æƒ…å†µï¼š é€»è¾‘å³ç§»ï¼šå‘å³ç§»åŠ¨kä½ï¼Œå·¦è¾¹è¡¥0 ç®—æœ¯å³ç§»ï¼šå‘å³ç§»åŠ¨kä½ï¼Œå·¦è¾¹è¡¥æœ€é«˜ä½çš„å€¼ã€‚ å¯ä»¥çœ‹åˆ°ä¸€å®šæ˜¯åœ¨ä¸€ä¸ªå­—èŠ‚å†…çš„ï¼Œä¹Ÿå°±æ˜¯ä»¥ä¸€ä¸ªå­—èŠ‚ä¸ºæ•´ä½“ã€‚ å¯¹äºæœ‰ç¬¦å·æ•°ï¼š åœ¨javaä¸­ x&gt;&gt;k æ˜¯ç®—æœ¯å³ç§»ï¼Œ x&gt;&gt;&gt;&gt;k æ˜¯é€»è¾‘å³ç§»ã€‚ Cè¯­è¨€ä¸­æ²¡æœ‰æ˜ç¡®è§„å®šï¼Œä½†å‡ ä¹æ‰€æœ‰çš„ç¼–è¯‘å™¨é‡‡ç”¨ç®—æœ¯å³ç§»ã€‚ä¹Ÿå°±æ˜¯è¡¥æœ€é«˜ä½çš„å€¼ã€‚ å¯¹äºæ— ç¬¦å·æ•°ï¼Œå³ç§»å¿…é¡»æ˜¯é€»è¾‘å³ç§»ã€‚ ç§»ä½è¿ç®—ç¬¦çš„ä¼˜å…ˆçº§ä½äºåŠ æ³•å’Œå‡æ³•ã€‚ 2.2 æ•´æ•°è¡¨ç¤ºæ•´æ•°æ“ä½œæœ¯è¯­ï¼š 2.2.1 æ•´å‹æ•°æ®ç±»å‹32ä½æœºå™¨ï¼š 64ä½æœºå™¨ï¼š è·Ÿæœºå™¨ç›¸å…³çš„æ•´å‹æ•°æ®ç±»å‹å°±åªæœ‰ longï¼Œåœ¨64ä½ä¸­å 8ä¸ªå­—èŠ‚ã€‚åœ¨32ä½ä¸­å 4ä¸ªå­—èŠ‚ã€‚ å¯ä»¥ç¨å¾®è®°ä¸€ä¸‹è¿™äº›æ•°ï¼š æœ‰ç¬¦å·å­—ç¬¦ï¼Œ 1ä¸ªå­—èŠ‚ã€‚ èŒƒå›´ $-2^7=128 \\sim 2^7-1=127$ æ— ç¬¦å·å­—ç¬¦ï¼Œ 1ä¸ªå­—èŠ‚ã€‚ èŒƒå›´ $0 \\sim 2^8-1=255$ æœ‰ç¬¦å·çŸ­æ•´å‹ï¼Œ2ä¸ªå­—èŠ‚ã€‚èŒƒå›´ $-2^15=-32768 \\sim 2^15-1=32767$ æ— ç¬¦å·çŸ­æ•´å‹ï¼Œ2ä¸ªå­—èŠ‚ã€‚èŒƒå›´ $0 \\sim 2^16-1=65535$ æœ‰ç¬¦å·æ•´å‹ï¼Œ 4ä¸ªå­—èŠ‚ã€‚èŒƒå›´ $-2^31=-2 147 483 648 \\sim 2^31-1=2 147 483 647$ æ— ç¬¦å·æ•´å‹ï¼Œ 4ä¸ªå­—èŠ‚ã€‚èŒƒå›´ $0 \\sim 2^32-1=4 294 967 295$ æ— ç¬¦å·çš„æ•°å€¼èŒƒå›´æ˜¯æœ‰ç¬¦å·çš„ä¸¤å€ï¼Œæ˜¯å› ä¸ºæœ‰ç¬¦å·æ•°éœ€è¦æ‹¿å‡ºä¸€ä½æ¥è¡¨ç¤ºç¬¦å·ï¼Œæ‰€ä»¥å°±å°äº†ä¸€å€äº†ã€‚æ¯”å¦‚ï¼šä¸€ä¸ªå­—ç¬¦æ˜¯ä¸€ä¸ªå­—èŠ‚ï¼Œ8ä½ï¼Œæ— ç¬¦å·èŒƒå›´å°±æ˜¯ $2^8-1=255$, æœ‰ç¬¦å·å°±æ˜¯ $2^7-1=127$. ç„¶è€Œåé¢ä»‹ç»äº†è¡¥ç ï¼Œå°‘ä¸€ä½ç¼–ç æ˜¯å¯¹çš„ï¼Œä½†æ˜¯è´Ÿæ•°çš„è®¡ç®—å¹¶ä¸æ˜¯è¿™æ ·å“ˆå“ˆå“ˆï¼Œæ‰“è„¸äº†ï¼Œå¹¶æ²¡æœ‰è¿™ä¹ˆç®€å•å•¦ï½ï½ï½ è¿™ä¹Ÿè§£é‡Šäº†ä¸ºå•¥è´Ÿæ•°æ˜¯ -128ï¼Œ æ­£æ•°æ˜¯ 127. Cè¯­è¨€æ ‡å‡†å®šä¹‰çš„æ¯ç§æ•°æ®ç±»å‹å¿…é¡»è¡¨ç¤ºçš„æœ€å°çš„å–å€¼èŒƒå›´ã€‚ 2.2.2 æ— ç¬¦å·æ•°çš„ç¼–ç å°†äºŒè¿›åˆ¶å†™æˆå‘é‡ã€‚ 2.2.3 è¡¥ç ç¼–ç ä¸ºäº†æ ‡ä¹¦è´Ÿæ•°å€¼ï¼Œæœ€å¸¸è§çš„å°±æ˜¯è¡¥ç ï¼ˆtwoâ€™s-complementï¼‰å½¢å¼ï¼Œ å°†å­—çš„æœ€é«˜æœ‰æ•ˆä½è§£é‡Šä¸ºè´Ÿæƒï¼ˆnegative weightï¼‰. æœ€é«˜ä½çš„æƒé‡æ˜¯ $-2^{w-1}$,å¯¹è¯¥æ•°æ˜¯æ­£æ˜¯è´Ÿå–å†³å®šæ€§ä½œç”¨ã€‚å…¶ä¸º1æ—¶ï¼Œä¸ºè´Ÿï¼›å…¶ä¸º0æ—¶ï¼Œä¸ºæ­£ã€‚ æ‰€ä»¥wä½è¡¥ç èƒ½è¡¨ç¤ºçš„èŒƒå›´ï¼š $$-2^{w-1} \\sim \\sum_{i=1}^{w-1}2^i=2^{w-1}-1$$ é‡è¦çš„æ•°å­—: UMax è¡¨ç¤ºæœ€å¤§æ— ç¬¦å·æ•°ï¼ŒUMinæ²¡æœ‰å†™ï¼Œå°±æ˜¯0å•¦ TMax è¡¨ç¤ºæœ€å¤§æœ‰ç¬¦å·æ•°ï¼ŒTMin è¡¨ç¤ºæœ€å°æœ‰ç¬¦å·æ•°ã€‚ å¯ä»¥å‘ç°ï¼š è¡¥ç ä¸å¯¹ç§°ï¼Œ$|TMin| = |TMax| + 1$ æ— ç¬¦å·æœ€å¤§å€¼æ˜¯æœ‰ç¬¦å·æœ€å¤§å€¼çš„2å€å¤§ä¸€ç‚¹ã€‚æ¯”å¦‚ä¸€ä¸ªå­—èŠ‚8ä½ä¸ºä¾‹ï¼Œ(2^8-1) = 2(2^7-1)+1. ç»ƒä¹ é¢˜2.18 çœ‹ä¸æ‡‚ 2.2.4 æœ‰ç¬¦å·æ•°å’Œæ— ç¬¦å·æ•°ä¹‹é—´çš„è½¬æ¢å¼ºåˆ¶è½¬æ¢ä¿æŒä½å€¼ï¼Œåªæ˜¯è½¬æ¢äº†å…¶è§£é‡Šæ–¹å¼ã€‚ æ¯”å¦‚32ä¸ºçš„æ— ç¬¦å·æœ€å¤§å€¼ 4294967295($UMax_32$)ï¼Œä¸è¡¥ç å½¢å¼çš„-1çš„ä½æ¨¡å¼æ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚ æœ‰ç¬¦å·è½¬æ— ç¬¦å· $T2U_w$12345678910111213int main(){ unsigned u = 4294967295u; int tu = (int) u; printf(&quot;u=%u, tu=%d\\n&quot;, u, tu);} è¿è¡Œç»“æœï¼š 123u=4294967295, tu=-1 è¡¥ç å’Œæ— ç¬¦å·ç ä¹‹é—´çš„å…³ç³»ï¼Œä»¥16ä½ä¸ºä¾‹ï¼š æœ‰ç¬¦å·ç ï¼š 12345çš„äºŒè¿›åˆ¶ï¼š 0011 0000 0011 1001 -12345çš„äºŒè¿›åˆ¶æ•°ï¼š 1100 1111 1100 0111 53191çš„æ— ç¬¦å·ç å’Œ-12345çš„äºŒè¿›åˆ¶è¡¨ç¤ºä¸€æ ·ã€‚ å¯ä»¥å‘ç° $12345 + 53919 = 2^{16}$ å…¶å®å¾ˆå®¹æ˜“æ¨å¯¼å‡ºæ¥, å¯ä»¥å¾—åˆ°å¦‚ä¸‹å…³ç³»ï¼š ä»ç¬¦å·è½¬æ— ç¬¦å·ï¼Œå¦‚æœæ˜¯è´Ÿæ•° u , å‡è®¾é™¤äº†æœ€é«˜ä½çš„æ•°è¡¨ç¤ºä¸º yï¼Œé‚£ä¹ˆï¼š $-2^{w-1} + y = u$ ç°åœ¨è¦æ±‚æ— ç¬¦å·ï¼š $2^{w-1}+y = u+ 2*2^{w-1}= 2^w + u$ æ— ç¬¦å·è½¬æœ‰ç¬¦å· $U2T_w$ $TMax_{w} = 2^{w-1}-1$ å½“ $u &gt; TMax_w$ æ—¶ï¼Œæ˜¾ç„¶è½¬æ¢ä¸ºæœ‰ç¬¦å·åº”è¯¥æ˜¯è´Ÿçš„ã€‚ æ— ç¬¦å·ï¼š $2^{w-1}+y = u$ æœ‰ç¬¦å·ï¼š $-2^{w-1} +y = u-2*2^{w-1}=u-2^w$ 2.2.5 Cè¯­è¨€ä¸­çš„æœ‰ç¬¦å·æ•°å’Œæ— ç¬¦å·æ•°ä¸€èˆ¬éƒ½é»˜è®¤ä¸ºæ˜¯æœ‰ç¬¦å·æ•°ï¼Œè¦åˆ›å»ºä¸€ä¸ªæ— ç¬¦å·æ•°ï¼Œå¿…é¡»åŠ ä¸Šåç¼€ â€˜uâ€™ æˆ– â€˜Uâ€™. æ˜¾ç¤ºçš„å¼ºåˆ¶è½¬æ¢ï¼š éšå¼çš„å¼ºåˆ¶è½¬æ¢ï¼š è¾“å‡º %d, %u, %x åˆ†åˆ«è¡¨ç¤ºåè¿›åˆ¶ã€æ— ç¬¦å·åè¿›åˆ¶å’Œåå…­è¿›åˆ¶ã€‚æ‰€ä»¥åœ¨printfè¾“å‡ºæ—¶ï¼Œä¹Ÿä¼šå‘ç”Ÿè½¬æ¢ã€‚ æ€»ä¹‹ï¼Œè®°ä½è¾“å‡ºçš„ç±»å‹åªæ˜¯ä¸€ç§è§£é‡Šå½¢å¼ï¼Œåº•å±‚çš„ä½å€¼æ˜¯ä¸å˜çš„ã€‚ å½“æ‰§è¡Œè¿ç®—æ—¶ï¼Œä¸€ä¸ªè¿ç®—æ•°æ˜¯æ— ç¬¦å·çš„ï¼Œä¸€ä¸ªæ˜¯æœ‰ç¬¦å·çš„ã€‚é‚£ä¹ˆæœ‰ç¬¦å·çš„ä¼šéšå¼çš„è½¬æ¢ä¸ºæ— ç¬¦å·è¿›è¡Œè®¡ç®—ã€‚ æ¯”å¦‚32ä½æœºå™¨ä¸­ -1 &gt; 0u. è¿™é‡Œçš„-1æ˜¯ 4294967295u 2.2.6 æ‹“å±•ä¸€ä¸ªæ•°å­—çš„ä½è¡¨ç¤º æ— ç¬¦å·æ‹“å±• $B2U_w(\\overrightarrow u)=B2U_{wâ€™}=(\\overrightarrow uâ€™)$ åœ¨è¡¨ç¤ºçš„å¼€å¤´æ·»åŠ  wâ€™-w ä¸ª0 æœ‰ç¬¦å·æ‹“å±• $B2T_w(\\overrightarrow x) = B2T_{wâ€™}(\\overrightarrow xâ€™)$ åœ¨è¡¨ç¤ºå¼€å¤´æ·»åŠ  wâ€™-w ä¸ª $x_{w-1}$ å¯¹äºæ— ç¬¦å·å¾ˆå¥½ç†è§£ï¼Œæ·»åŠ 0åå€¼ä¸ä¼šæ”¹å˜ã€‚ä½†å¯¹äºæœ‰ç¬¦å·çš„è´Ÿæ•°ï¼Œæ·»åŠ 1ä¹‹åï¼Œå…¶å€¼ä¹Ÿæ˜¯æ²¡æœ‰æ”¹å˜çš„ã€‚æ¯”å¦‚æœ‰ç¬¦å· [101] è¡¨ç¤º -4+1=-3. æ‰©å±•ä¸€ä½ [1101]= -8+4+1= -3. è¿™æ˜¯å› ä¸ºæ¯æ‰©å±•ä¸€ä½ $-2^w+2^{w-1}=-2^{w-1}$. æ‰€ä»¥ä¸å˜ï½ï½ 2.2.7 æˆªæ–­æ•°å­—æˆªæ–­æ— ç¬¦å·æ•°ï¼Œå…¶å®å°±æ˜¯å–æ¨¡è¿ç®—$x=B2U_w(\\overrightarrow x), xâ€™=B2U_k(\\overrightarrow xâ€™)$ åˆ™ xâ€™=x mod $2^k$. å³å¯¹ 2^k å–ä½™ã€‚ æˆªæ–­æœ‰ç¬¦å·æ•°$[x_{w-1},x_{w-2},â€¦,x_0]$ å…ˆæŒ‰ç…§æ— ç¬¦å·æ•°æˆªæ–­ï¼Œä¹Ÿå°±æ˜¯å¯¹ $2^k$ å–ä½™ã€‚ç„¶ååœ¨è½¬æ¢ä¸ºæœ‰ç¬¦å·æ•°ã€‚ 2.2.8 å…³äºæœ‰ç¬¦å·æ•°ä¸æ— ç¬¦å·æ•°çš„å»ºè®®ç»ƒä¹ é¢˜ 2.25 å‡ºç°é”™è¯¯æ˜¯å› ä¸º length æ˜¯æ— ç¬¦å·æ•°ï¼Œå½“length=-1æ—¶ï¼Œä¼šè½¬æ¢ä¸º 2^31-1. å¾ªç¯ä¼šæ— é™å¾ªç¯ä¸‹å»ã€‚ å°† unsigned lenght ä¿®æ”¹ä¸º int length å³å¯ã€‚ ç»ƒä¹ é¢˜2.26 strlen è¿”å›çš„ size_t æ˜¯unsigned intï¼Œ æ‰€ä»¥ ç›¸å‡ä¸º-1ä¹‹åä¼šè½¬æ¢ã€‚ æ‰€ä»¥æ”¹ä¸º int (strlen(s)- strlen(t)) &gt; 0; æ— ç¬¦å·æ•°æœ‰æ—¶å€™ä¼šå¾ˆæœ‰ç”¨çš„ã€‚ 2.3 æ•´æ•°è¿ç®—2.3.1 æ— ç¬¦å·åŠ æ³•xï¼Œ yæ˜¯æ— ç¬¦å·æ•°ï¼Œ0 &lt; x,y &lt; $2^w-1$. å¦‚æœæº¢å‡ºï¼Œè¯´æ˜ $2^{w} \\le x+y &lt; 2^{w+1}-2$. éœ€è¦ w+1 ä½æ‰èƒ½è¡¨ç¤ºå‡ºæ¥ã€‚ s = x + y. å½“æº¢å‡ºæ—¶ï¼Œs = x + y - 2^w æº¢å‡ºæ—¶ï¼Œw+1ä½ä¸º1ï¼Œä¸¢å¼ƒå®ƒç›¸å½“äºä»å’Œä¸­å‡å»äº† $2^w$ã€‚æ­£å¸¸æƒ…å†µä¸‹ï¼Œw+1 ä½ä¸º0ï¼Œä¸ä¼šå½±å“ã€‚ æ£€æµ‹æ— ç¬¦å·æº¢å‡ºçš„æ¡ä»¶ï¼š å¦‚æœæº¢å‡ºï¼š$s = x + y -2^w$, é‚£ä¹ˆ $s = x+(y-2^w) &lt;x$, åŒæ · $s = y+(x-2^w)&lt;y$ ä¹ é¢˜ 2.27 1234567891011int uadd_ok(unsigned x, unsigned y){ unsigned s = x + y; return s &gt;= x;} æ— ç¬¦å·æ•°æ±‚åæ¨¡æ•°åŠ æ³•å½¢æˆä¸€ç§æ•°æ®ç»“æ„ï¼Œç§°ä¸ºé˜¿è´å°”ç¾¤ï¼Œä¹Ÿå°±æ˜¯æ±‚ x çš„é€†å…ƒ $-^u_w x$. ä»€ä¹ˆæ„æ€å‘¢ï¼Œå°±æ˜¯ è¿™ä¸ªé€†å…ƒåŠ ä¸Š x å¯¹2^w å–æ¨¡ç­‰äº 0. æ‰€è°“æ¨¡æ•°åŠ æ³•ï¼Œè¶…è¿‡äº†æ¨¡ $2^w$, å°±æ˜¯é™¤ä»¥æ¨¡ $2^w$ çš„ä½™æ•° ç»ƒä¹ é¢˜2.28 | x åå…­è¿›åˆ¶ | x åè¿›åˆ¶ | $-^u_4x$ åè¿›åˆ¶ | $-^u_4x$åå…­è¿›åˆ¶ | | â€”â€”â€”- | â€”â€”â€“ | â€”â€”â€”â€”â€” | â€”â€”â€”â€”â€”- | | 0 | 0 | 0 | 0 | | 5 | 5 | 11 | B | | 8 | 8 | 8 | 8 | | D | 13 | 3 | 3 | | F |15 | 1 | 1 | 2.3.2 è¡¥ç åŠ æ³• æ³¨æ„è·Ÿæ— ç¬¦å·æ•°æ­£æº¢å‡ºçš„åŒºåˆ«ï¼Œæ— ç¬¦å·æ•°æº¢å‡ºä¹‹åæ˜¯å»æ‰ w+1 ä½ï¼Œä½†æ˜¯æœ‰ç¬¦å·æ•°æ­£æº¢å‡ºæ—¶ï¼Œ$x+y&lt;2^w$ï¼Œwä½è¿˜æ˜¯è¦è€ƒè™‘çš„ï¼Œåªæ˜¯å˜ä¸ºè´Ÿæ•°äº†ã€‚æ‰€ä»¥æ˜¯ $x+y-2*2^{w-1}$, è€Œä¸èƒ½è®¤ä¸ºæ˜¯ $x+y-2^{w-1}$ ä½†æ˜¯è´Ÿæº¢å‡ºï¼Œå°±ä¼šæº¢å‡ºåˆ° w+1 ä½äº†ï¼Œé‚£ä¹ˆå»æ‰ w+1 ä½ï¼Œå°±æ˜¯ $x+y+ 2^w$ äº† æ¨å¯¼çš„è¯ï¼Œå› ä¸ºä¸è®ºæœ‰ç¬¦å·è¿˜æ˜¯æ— ç¬¦å·ï¼Œéƒ½æ˜¯ç›¸åŒçš„ä½çº§è¡¨ç¤ºï¼ŒäºŒè¿›åˆ¶çš„åŠ æ³•ä¹Ÿæ˜¯è¿›ä½åŒæ ·çš„æ–¹å¼ã€‚ åªæ˜¯è§£é‡Šæ–¹å¼ä¸ä¸€æ ·è€Œå·²ã€‚æ‰€ä»¥å¯ä»¥å°†æœ‰ç¬¦å·è½¬æ¢ä¸ºæ— ç¬¦å·æ•°è¿›è¡Œ $+^u_w$ çš„æ¨¡ $2^w$ çš„æ¨¡æ•°åŠ æ³•è¿ç®—ï¼Œç„¶ååœ¨è½¬æ¢ä¸ºæœ‰ç¬¦å·æ•°å³å¯ã€‚ è¡¥ç æº¢å‡ºçš„æ¡ä»¶ ä¹ é¢˜ 2.30 è¡¥ç æº¢å‡ºåˆ¤æ–­ï¼š 1234567891011121314151617int tadd_ok(int x, int y){ int sum = x+y; if ((x&gt;0 &amp;&amp; y&gt;0 &amp;&amp; s&lt;=0) || (x&lt;0 &amp;&amp;y&lt;0 &amp;&amp; s&gt;=0)) return 0; return 1;} ä¹ é¢˜ 2.31 123456789101112131415/* Determine whether arguments can be added without overflow *//* WARRING: This code is buggy */int tadd_ok(int x, int y){ int sum = x+y; return (sum-x == y) &amp;&amp; (sum-y ==x);} æ— è®ºæ­£ç¡®ä¸å¦ï¼Œéƒ½æ˜¯è¿”å› 1. æ¨¡æ•°åŠ æ³•å½¢æˆäº†ä¸€ç§æ•°æ®ç»“æ„ï¼Œç§°ä¸ºé˜¿è´å°”ç¾¤(Abelian group). å› æ­¤ä»£ç ä¸­çš„åˆ¤æ–­ï¼Œæ— è®ºæº¢å‡ºä¸å¦ï¼Œéƒ½æ˜¯æˆç«‹çš„ï¼Œéƒ½è¿”å› 1ï¼› å…·ä½“æ¨å¯¼å¦‚ä¸‹ï¼š å¦‚æœ x+y æº¢å‡ºï¼Œ $sum = x+y-2^w$, é‚£ä¹ˆ $sum-x = y-2^w$. å› ä¸º $y &lt; 2^{w-1}-1$, åˆ™ $y-2^w&lt; -2^{w-1} -1$, ä»ç„¶è´Ÿæº¢å‡ºã€‚ æ‰€ä»¥ $sum-x= y-2^w+2^w = y$. ä¹ é¢˜ 2.32 x -y ä¸æº¢å‡ºæ—¶ï¼Œè¿”å›1. 1234567891011/* Determine whether arguments can be subtracted without overflow *//* WARNNING: This code is buggy. */int tsub_ok(int x, int y){ return tadd_ok(x, -y);} x å’Œ y å–ä»€ä¹ˆå€¼æ˜¯ï¼Œä¼šäº§ç”Ÿé”™è¯¯çš„ç»“æœï¼Ÿ å½“ $y = TMin_w = -2^{w-1}$ æ—¶ï¼Œ $-y = 2^{w-1}$ åŸæœ¬åº”è¯¥æ˜¯æ­£æ•°çš„ï¼Œä½†å·²ç»æº¢å‡ºäº†ï¼Œè½¬æ¢æˆäº†è´Ÿæ•°ã€‚ $-y = y$. æ­¤æ—¶å¯¹äº tadd_ok(x, -y) æ¥è¯´ï¼Œå½“ x ä¸ºè´Ÿæ—¶éƒ½è®¤ä¸ºæº¢å‡ºï¼Œè¿”å› 0ï¼Œè€Œ x ä¸ºéè´Ÿæ—¶ï¼Œéƒ½è®¤ä¸ºæ²¡æœ‰æº¢å‡ºè¿”å› 1ã€‚è€Œæƒ…å†µæ°æ°ç›¸åï¼Œtsub_ok(x, TMin)ï¼Œå½“ x ä¸º è´Ÿæ•°æ—¶ï¼Œåº”è®¤ä¸ºæ²¡æœ‰æº¢å‡ºï¼Œè¿”å› 1ï¼Œè€Œ x ä¸ºéè´Ÿæ—¶ï¼Œåº”è®¤ä¸ºæº¢å‡ºè¿”å› 0. 2.3.3 è¡¥ç çš„é å› ä¸ºè¡¥ç åˆè´Ÿæ•°ï¼Œæ‰€ä»¥ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå…¶é€†å…ƒå°±æ˜¯ -xï¼Œ è€Œå¯¹äº $TMin_w$, $-TMin_w$ ä¼šæº¢å‡ºï¼Œå°±ä¸å±äºä¸€èˆ¬æƒ…å†µäº†ã€‚ ç»ƒä¹ é¢˜ 2.33 | x åå…­è¿›åˆ¶ | x åè¿›åˆ¶ | $-^t_4x$ åè¿›åˆ¶ | $-^t_4x$åå…­è¿›åˆ¶ | | â€”â€”â€”- | â€”â€”â€“ | â€”â€”â€”â€”â€” | â€”â€”â€”â€”â€”- | | 0 | 0 | 0 | 0 | | 5 | 5 | -5 | -5 | | 8 | -8 | -8 | 8 | | D | -3 | 3 | 3 | | F | -1 | 1 | 1 | ç¬¬äºŒç§æ–¹æ³•: æ‰§è¡Œä½çº§è¡¥ç éï¼Œç„¶ååŠ 1 å¯¹äºæ— ç¬¦å·æ•°æˆ–æœ‰ç¬¦å·æ•°ï¼Œéï¼ˆæˆ–è€…é€†å…ƒï¼‰ï¼Œä»–ä»¬çš„ä½æ¨¡å¼æ˜¯ä¸€æ ·çš„ã€‚éƒ½æ˜¯ä½æ¨¡å¼å–å+1ï¼Œ å¯¹ä»»æ„æ•´æ•° éï¼ˆé€†å…ƒï¼‰-x å’Œ å…¶ä½æ¨¡å¼å–åï¼ˆä½çº§è¡¥ç éï¼‰åŠ 1 ~x+1 éƒ½æ˜¯ä¸€æ ·çš„ã€‚ 2.3.4 æ— ç¬¦å·ä¹˜æ³•$0\\le x,y \\le 2^w-1$, $0\\le x\\cdot y z^{2w}-2^{w-1}+1$ éœ€è¦2w ä½æ¥è¡¨ç¤ºã€‚å°†å…¶æˆªæ–­ä¸º w ä½ï¼Œç­‰ä»·äºè®¡ç®—è¯¥å€¼æ¨¡ $2^w$. $$x*^u_w y = (x\\cdot y)mod\\ 2^w$$ 2.3.5 è¡¥ç ä¹˜æ³•è¡¥ç ä¹˜æ³•å’Œæ— ç¬¦å·çš„ä¹˜æ³•è¿ç®—çš„ä½çº§è¡¨ç¤ºæ˜¯ä¸€æ ·çš„ã€‚ å¯¹äº $TMin_w\\le x,y\\le TMax_w$ æœ‰ï¼š $$x*^t_wy=U2T_w((x\\cdot y)mod\\ 2^w)$$ åŸç†æ˜¯ï¼š $$T2B_w(x^t_wy)=U2B_w(xâ€™ ^u_w yâ€™)$$ xâ€™,yâ€™ æ˜¯xï¼Œyçš„æ— ç¬¦å·è½¬æ¢åçš„æ•°ã€‚ è¯æ˜ç•¥ã€‚ è™½ç„¶å®Œæ•´çš„ä¹˜ç§¯çš„ä½çº§è¡¨ç¤ºå¯èƒ½ä¸åŒï¼Œä½†æˆªæ–­åæ˜¯ä¸€æ ·çš„ã€‚ ç»ƒä¹ é¢˜ 2.34 | æ¨¡å¼ | x | y | $x\\cdot y$ | æˆªæ–­åçš„ $x\\cdot y$ | | â€”â€” | â€”â€”â€“ | â€”â€”â€“ | â€”â€”â€”â€“ | â€”â€”â€”â€”â€”â€”- | | æ— ç¬¦å· | [100]=4 | [101]=5 | [10100]=20 | [100]=4 | | è¡¥ç  | [100]=-4 | [101]=-3 | [1100] = 12 | [100]=-4 | | æ— ç¬¦å· | [010]=2 | [111]=7 | [1110]=14 | [110]=6 | | è¡¥ç  | [010]=2 | [111]=-1 | [110] =-2 | [110]=-2 | ç»ƒä¹ é¢˜ 2.35 12345678910111213141516171819/* Determine whether arguments can be multiplied without overflow */int tmult_ok(int x, int y){ int p = x*y; /* Either x is 0, or dividing p by x gives y * / return !x || p/x == y;} è§é¢˜ 2.31ï¼Œæˆ‘ä»¬ä¸èƒ½ç”¨å‡æ³•æ¥æ£€éªŒåŠ æ³•æ˜¯å¦æº¢å‡ºï¼Œä½†è¿™é‡Œå¯ä»¥ç”¨é™¤æ³•ä¸ºæ£€éªŒä¹˜æ³•æ˜¯å¦æº¢å‡ºã€‚ è¯æ˜ ï¼Ÿï¼Ÿ ç»ƒä¹ é¢˜ 2.36 1234567891011121314151617int tmul_ok(int x, int y){ /* Compute product without overflow*/ /* è¿™ä¸€è¡Œçš„å¼ºåˆ¶ç±»å‹è½¬æ¢è‡³å…³é‡è¦ã€‚å¦‚æœå†™æˆ int64_t p = x*y; * / /* å°±ä¼šç”¨ 32 ä½å€¼æ¥è®¡ç®—ä¹˜ç§¯(å¯èƒ½æº¢å‡ºï¼‰ï¼Œå†ç¬¦å·æ‰©å±•åˆ° 64 ä½*/ int64_t p = (int64_t) x * y; /* See if casting to int preserves value*/ return (int)p == p; 2.3.6 ä¹˜ä»¥å¸¸æ•°åœ¨å¤§å¤šæ•°æœºå™¨ä¸Šï¼Œæ•´æ•°ä¹˜æ³•æŒ‡ä»¤ç›¸å½“æ…¢ï¼Œéœ€è¦10ä¸ªæˆ–è€…å¤šä¸ªæ—¶é’Ÿå‘¨æœŸã€‚è€ŒåŠ æ³•ã€å‡æ³•ã€ä½çº§è¿ç®—å’Œç§»ä½åªéœ€è¦1ä¸ªæ—¶é’Ÿå‘¨æœŸï¼Œå› æ­¤ç¼–è¯‘é€šå¸¸æŠŠä¹˜æ³•è½¬æ¢ä¸ºç§»ä½å’ŒåŠ æ³•çš„ç»„åˆè¿ç®—ã€‚ 1.å…ˆè€ƒè™‘ä¹˜ä»¥ 2 çš„å¹‚ ä¸2çš„å¹‚çš„æ— ç¬¦å·ä¹˜æ³•ï¼š x &lt;&lt; k äº§ç”Ÿæ•°å€¼ $x*^u_w 2^k$ ä¸2çš„å¹‚çš„è¡¥ç ä¹˜æ³•ï¼š è¡¥ç ä¸æ— ç¬¦å·çš„ä½çº§æ“ä½œç­‰ä»·ï¼Œå› æ­¤è¡¥ç è¿ç®—çš„2çš„å¹‚çš„ä¹˜æ³•ä¹Ÿç±»ä¼¼ã€‚ x &lt;&lt; k äº§ç”Ÿæ•°å€¼ $x*^t_w 2^k$ æ— è®ºæ˜¯è¡¥ç è¿˜æ˜¯æ— ç ï¼Œéƒ½å¯èƒ½æº¢å‡ºã€‚å³ä½¿æº¢å‡ºï¼Œé€šè¿‡ç§»ä½å’Œä¹˜æ³•çš„ç»“æœä¹Ÿæ˜¯ä¸€è‡´çš„ï¼Œæˆªæ–­å°±å¥½äº†ã€‚ å¯¹ä»»æ„æ•°çš„ä¹˜æ³•å¯å†™æˆï¼š $x14 = x(2^3+2^2+2^1)$ æˆ–è€… $x14 = x(2^4-2^1) = (x&lt;&lt;4)-(x&lt;&lt;1)$ ç»ƒä¹ é¢˜ 2.38 (a&lt;&lt;k)+bï¼Œ å…¶ä¸­kå¯ä»¥ä¸º0,1,2,3, bå¯ä¸º0æˆ–a 123456789101112131415a çš„å€æ•° LEA æŒ‡ä»¤2a (a&lt;&lt;1)+03a (a&lt;&lt;1)+a4a (a&lt;&lt;2)+05a (a&lt;&lt;2)+a8a (a&lt;&lt;3)+09a (a&lt;&lt;3)+a ä¹˜æ³•è¿˜å¯ä»¥è¿™ä¹ˆçœ‹ï¼Œ x*K. ç¼–è¯‘å™¨å°†kçš„äºŒè¿›åˆ¶è¡¨è¾¾ä¸ºä¸€ç»„0å’Œ1äº¤æ›¿çš„åºåˆ—ã€‚ nè¡¨ç¤ºç¬¬ä¸€æ®µå¼€å§‹çš„nä¸ªè¿ç»­çš„1ï¼Œn-1è¡¨ç¤ºç¬¬äºŒæ®µè¿ç»­çš„1ã€‚ æ¯”å¦‚ä»¥8ä½ä¸ºä¾‹ï¼Œ14 å¯ä»¥å†™æˆ [(0000)(111)(0)],ä»ç¬¬3ä½å¼€å§‹åˆ°ç¬¬1ä½ä¸ºè¿ç»­çš„1. é‚£ä¹ˆ x*14=(x&lt;&lt;3)+(x&lt;&lt;2)+(x&lt;&lt;1) æˆ–è€… x*14=(x&lt;&lt;4)-(x&lt;&lt;1) å¾ˆç¥å¥‡ã€‚ã€‚ã€‚ ç»ƒä¹ é¢˜2.40 ç»ƒä¹ é¢˜2.41 ç¼–è¯‘å™¨Aã€Bä¸¤ç§å½¢å¼é€‰æ‹©å“ªä¸€ç§ï¼š é€‰æ‹©æ“ä½œæ¬¡æ•°å°‘çš„ã€‚ 2.3.7 é™¤ä»¥2çš„å¹‚é™¤æ³•æ›´æ…¢ï¼Œéœ€è¦30å¤šä¸ªæ—¶é’Ÿå‘¨æœŸã€‚ä¹Ÿå¯ä»¥é‡‡ç”¨ç§»ä½è¿ç®—å®ç°ã€‚åªä¸è¿‡æ˜¯å³ç§»ã€‚æ— ç¬¦å·æ˜¯é€»è¾‘å³ç§»ï¼ˆè¡¥0ï¼‰ï¼Œè¡¥ç æ˜¯ç®—æœ¯å³ç§»ï¼ˆè¡¥1ï¼‰ å®šä¹‰å‘ä¸‹å–æ•´ $\\lfloor a \\rfloor=aâ€™. aâ€™\\le a\\le aâ€™+1$. å¯¹äºéè´Ÿæ•´æ•°æ˜¯æ»¡è¶³é™¤æ³•çš„ï¼Œä½† $\\lfloor -3.14 \\rfloor = -4$ æ˜¯ä¸æ»¡è¶³çš„ã€‚ é™¤ä»¥2çš„å¹‚çš„æ— ç¬¦å·é™¤æ³•x&gt;&gt;k ç­‰ä»·äº $\\lfloor x/2^k\\rfloor$ é™¤ä»¥2çš„å¹‚çš„è¡¥ç é™¤æ³•ç®—æœ¯å³ç§»ï¼Œå¯¹ä¸æ­£æ•°æ¥è¯´ï¼Œæœ€é«˜æœ‰æ•ˆä½æ˜¯0ï¼Œä¸æ— ç¬¦å·çš„é€»è¾‘å³ç§»æ˜¯ä¸€æ ·çš„ã€‚ ä½†å¯¹äºè´Ÿæ•°æ¥è¯´ï¼Œæœ€é«˜æœ‰æ•ˆä½æ˜¯1ï¼Œå³ç§»åæ•ˆæœå¦‚ä¸‹å›¾ï¼š $[x_{w-1},..,x_{w-1},x_{w-2},â€¦,x_k]$ æ˜¯ $\\lfloor x/2^k\\rfloor$ çš„è¡¥ç è¡¨ç¤ºï¼Œä½†æ˜¯å®ƒä¸æ˜¯å‘é›¶èˆå…¥ã€‚éœ€è¦ä½¿ç”¨â€œåç½®â€æ¥ä¿®æ­£è¿™ç§ä¸åˆé€‚çš„èˆå…¥ã€‚ (x+(1&lt;&lt;k)-1)&gt;&gt;k ç­‰ä»·äº $x/2^k$ å‘ä¸Šå–æ•´ã€‚ åç½®æŠ€æœ¯åˆ©ç”¨äº†å¦‚ä¸‹å±æ€§ï¼š $\\lceil x/y \\rceil = \\lfloor (x+y-1)/y \\rfloor$ æ¨å¯¼ï¼š å‡è®¾ x = qy+r. $\\lfloor (x+y-1)/y\\rfloor=q+\\lfloor(r+y-1)/y\\rfloor$ å½“r=0æ—¶ï¼Œä¹Ÿå°±æ˜¯èƒ½æ•´é™¤ï¼Œ$\\lfloor(r+y-1)/y\\rfloor=0$. è¿™ä¹Ÿæ˜¯è¦-1çš„åŸå› ï¼Œä¸ç„¶å¯¹äºæ•´é™¤çš„æ—¶å€™å°±ä¸ç¬¦äº†ã€‚ å½“r&gt;0æ—¶ï¼Œy&gt;r&gt;1,$\\lfloor(r+y-1)/y\\rfloor=1$, ç»“æœç›¸å½“äº q+1 æ‰€ä»¥ï¼Œå½“ y=2^k æ—¶ï¼Œ (x+(1&lt;&lt;k)-1)&gt;&gt;k = $\\lceil x/2^k\\rceil$ æ€»ç»“ï¼š å¯¹äºä½¿ç”¨ç®—æœ¯å³ç§»çš„è¡¥ç æœºå™¨ï¼ŒCè¡¨è¾¾å¼ï¼š (x&lt;0 ? x+(1&lt;&lt;k)-1 : x) &gt;&gt; k ç»ƒä¹ é¢˜2.42 å†™ä¸€ä¸ªå‡½æ•° div16ï¼Œå¯¹äºæ•´æ•°å‚æ•° x è¿”å› x/16 çš„å€¼ã€‚è¦æ±‚ä¸èƒ½ä½¿ç”¨é™¤æ³•ã€æ¨¡è¿ç®—ã€ä¹˜æ³•ã€æ¡ä»¶è¯­å¥ (if å’Œ ? : )ã€æ¯”è¾ƒè¿ç®—ç¬¦ã€å¾ªç¯ç­‰ã€‚å‡è®¾ int ä¸º 32 ä½ï¼Œè¡¥ç ï¼Œç®—æœ¯å³ç§»ã€‚ 1234567891011121314151617int div16(int x){ /* (x+bias) &gt;&gt; k å¯¹äº x ä¸ºæ­£æ•°ï¼Œ bias=0 å¯¹äº x ä¸ºè´Ÿæ•°ï¼Œ bias=(1&lt;&lt;4)-1=15*/ int bias = (x&gt;&gt;31) &amp; 0xf; printf(&quot;%d&quot;, bias); return (x+bias) &gt;&gt; 4;} ç»ƒä¹ é¢˜ 2.43 ä¸‹é¢çš„ä»£ç ä¸­ï¼Œçœç•¥äº†å¸¸æ•° M å’Œ N çš„å®šä¹‰ï¼š 1234567891011121314151617#define M /* Mystery number 1 */#define N /* Mystery number 2 */int arith(int x, int y){ int result = 0; result = x*M + y/N; return result;} ç”¨æŸä¸ª M å’Œ N çš„å€¼ç¼–è¯‘ä¸Šé¢ä»£ç åï¼Œç¼–è¯‘å™¨å°†ä¼˜åŒ–ä¹˜é™¤æ“ä½œã€‚ä¸‹é¢æ˜¯äº§ç”Ÿçš„æœºå™¨ç ç¿»è¯‘å› C çš„ç»“æœï¼š 1234567891011121314151617181920212223/* Translation of assembly code for arith */int optarith(int x, int y){ int t = x; x &lt;&lt;= 5; x -= t; /* x = x*2^5 -xï¼Œä½¿ç”¨äº†å½¢å¼Bçš„ä¼˜åŒ–ï¼š(x&lt;&lt;(n+1))-(x&lt;&lt;m)ï¼Œ è¿™é‡Œ n=4, m=0, ä»è€Œ M = [11111] = 31*/ if (y &lt; 0) y += 7; /* y + (1&lt;&lt;3)-1*/ y &gt;&gt;= 3; /* Arithmetic shiftï¼Œä»è€Œ N ä¸º 2 çš„ 3 æ¬¡æ–¹ï¼Œå³ä¸º 8*/ return x+y;} M=31, N=8 2.3.8 å…³äºæ•´æ•°è¿ç®—çš„æœ€åæ€è€ƒæ•´æ•°è¿ç®—å®é™…ä¸Šæ˜¯ä¸€ç§æ¨¡è¿ç®—å½¢å¼ã€‚ æ•´æ•°è¡¨ç¤ºçš„æœ‰é™å­—é•¿é™åˆ¶äº†è¿ç®—ç»“æœçš„å–å€¼èŒƒå›´ï¼Œä»è€Œå¯èƒ½ä¼šæº¢å‡ºã€‚æ— è®ºæ˜¯è¡¥ç è¡¨ç¤ºè¿˜æ˜¯æ— ç¬¦å·æ•°ï¼Œå®ƒä»¬çš„åŠ å‡ä¹˜é™¤æ“ä½œï¼Œåœ¨ä½çº§ä¸Šéƒ½å®Œå…¨ä¸€æ ·æˆ–éå¸¸ç±»ä¼¼ã€‚ C ä¸­çš„ unsigned æ•°æ®ç±»å‹å˜é‡ï¼Œåœ¨è¿ç®—ä¸­ä¼šéšå¼åœ°è¦æ±‚å°†å…¶å®ƒå‚æ•°å…ˆè½¬æ¢æˆ unsigned æ•°æ®ç±»å‹å†è®¡ç®—ï¼Œä»è€Œä¼šå¯¼è‡´éš¾ä»¥å¯Ÿè§‰çš„ BUGã€‚ ä¹ é¢˜2.44 A. x=TMin_w æ—¶ä¸ºå‡ï¼Œ $x = -2^{w-1}-1 = -2^{w-1}-1+2^w=2^{w-1}-1 &gt;0$ B. ä¸¾åä¾‹å°±æ˜¯ä¸¤è¾¹éƒ½ä¸ºå‡ã€‚å·¦è¾¹ (x&amp;7)=7=[000â€¦0111], é‚£ä¹ˆæœ€åä¸‰ä½éƒ½ä¸º1. å³è¾¹ x&lt;&lt;29 &gt;0,é‚£ä¹ˆå€’æ•°ç¬¬ä¸‰ä½ä¸º0. æ‰€ä»¥ä¸å­˜åœ¨åŒæ—¶ä¸¤è¾¹éƒ½ä¸ºå‡çš„xã€‚ C. å½“ $x=2^31-1$, $x*x=2^62-2$,æº¢å‡ºï¼Œæˆªæ–­ä¹‹åæœ€é«˜ä½ä¸º1ï¼Œè‚¯å®šå°äº0 2.4 æµ®ç‚¹æ•°2.4.1 äºŒè¿›åˆ¶å°æ•° å°æ•°çš„äºŒè¿›åˆ¶è¡¨ç¤ºæ³•åªèƒ½è¡¨ç¤º $x\\times 2^y$ çš„æ•°ï¼Œå…¶ä»–çš„åªèƒ½è¿‘ä¼¼è¡¨ç¤ºã€‚äºŒè¿›åˆ¶è¡¨ç¤ºçš„é•¿åº¦å¯ä»¥æé«˜è¡¨ç¤ºçš„ç²¾åº¦ã€‚ ä¾‹å¦‚ $\\dfrac{1}{5}$ å¯ä»¥ç”¨åè¿›åˆ¶0.20ç²¾ç¡®è¡¨ç¤ºï¼Œä½†å¹¶ä¸èƒ½å‡†ç¡®çš„ç”¨äºŒè¿›åˆ¶è¡¨ç¤ºã€‚ $0.111â€¦1_2$ è¡¨ç¤ºåˆšå¥½å°äº0çš„æ•°ã€‚ç”¨ $1.0-\\epsilon$ è¡¨ç¤º 2.4.2 IEEEæµ®ç‚¹è¡¨ç¤º ç¬¦å·ä½ s é˜¶ç ï¼ˆexponentï¼‰Eçš„ä½œç”¨æ˜¯å¯¹æµ®ç‚¹æ•°åŠ æƒã€‚ nä¸ºå°æ•°å­—æ®µ $frac=f_{n-1}\\cdots f_1f_0$, ç¼–ç å‡ºæ¥çš„ä¹Ÿä¾èµ–ä¸é˜¶ç æ®µçš„å€¼æ˜¯å¦ä¸º0 å•ç²¾åº¦ï¼š s, exp, fracå­—æ®µä¸º 1,8,23 ä½ åŒç²¾åº¦ï¼š s,exp, fracå­—æ®µä¸º 1,11,52 ä½ 1.è§„æ ¼åŒ–çš„å€¼ expçš„ä½æ¨¡å¼ä¸å…¨ä¸º0ï¼Œä¹Ÿä¸å…¨ä¸º1ã€‚ é˜¶ç å­—æ®µè¢«è§£é‡Šä¸ºä»¥åç½®å½¢å¼çš„æœ‰ç¬¦å·æ•´æ•°ã€‚ é˜¶ç çš„å€¼ $E=e-Bias$, eæ˜¯æ— ç¬¦å·æ•°ï¼Œä¸ºè¡¨ç¤ºæ˜¯ $e_{k-1}\\cdots e_1e_0$. Biasç­‰äº $2^{k-1}-1$. æ‰€ä»¥å•ç²¾åº¦èŒƒå›´ï¼š $e_{max}=2^8-2, e_{min}=1$ $E_{max}=2^8-2-2^7+1=127$ $E_{min}=1-(2^7-1)=-126$ 2.éè§„æ ¼åŒ–çš„å€¼ é˜¶ç å…¨ä¸º0ï¼Œ E=1-Bias. 3.ç‰¹æ®Šå€¼ å½“é˜¶ç å…¨ä¸º1æ—¶ï¼Œå°æ•°åŸŸå…¨ä¸º0ï¼Œå¾—åˆ°çš„å€¼è¡¨ç¤ºæ— ç©·ã€‚s=0æ—¶æ˜¯æ­£æ— ç©·ï¼Œs=1æ˜¯è´Ÿæ— ç©·ã€‚ é˜¶ç å…¨ä¸º1ï¼Œ å°æ•°åŸŸé0ï¼Œè¡¨ç¤ºNaN. 2.4.3 æ•°å­—ç¤ºä¾‹","link":"/2018/06/09/CSAPP-02-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/"},{"title":"C plus plus prime-ç±»","text":"ç±»å®šä¹‰ç±»å¤´ã€ç±»ä½“ã€ç±»åŸŸ æ•°æ®æˆå‘˜æ•°æ®æˆå‘˜å£°æ˜åœ¨ç±»ä½“ä¸­ï¼Œå¯ä»¥æ˜¯ä»»ä½•ç±»å‹ï¼Œeg. æŒ‡é’ˆï¼Œç±» åˆ†ä¸ºï¼šéé™æ€ï¼ˆnonstaticï¼‰ï¼Œé™æ€ï¼ˆstaticï¼‰. æˆå‘˜å‡½æ•°æˆå‘˜å‡½æ•°åœ¨ç±»åŸŸä¹‹å¤–æ˜¯ä¸å¯è§çš„ã€‚é€šè¿‡ â€œ.â€ æˆ– â€œ-&gt;â€ æ¥å¼•ç”¨ã€‚ æ³¨æ„åŒºåˆ†å…¨å±€åŸŸ/å…¨å±€å‡½æ•°ï¼Œç±»åŸŸ/æˆå‘˜å‡½æ•°ã€‚ æˆå‘˜è®¿é—®ä¿¡æ¯éšè—ï¼ˆinformation hidingï¼‰ï¼šç±»æˆå‘˜çš„è®¿é—®é™åˆ¶ï¼Œé€šè¿‡è®¿é—®é™å®šç¬¦æ¥å®ç°çš„ã€‚ public å…¬æœ‰æˆå‘˜ï¼Œæä¾›ç»™ç”¨æˆ·ä½¿ç”¨çš„ private åªèƒ½è¢«æˆå‘˜å‡½æ•°å’Œå‹å…ƒè®¿é—® protected å¯¹æ´¾ç”Ÿç±»è¡¨ç°çš„åƒ public, å¯¹å…¶ä»–ç¨‹åºè¡¨ç°çš„åƒ private. å‹å…ƒå…³é”®å­— findï¼Œ å‹å…ƒå¯ä»¥æ˜¯ä¸€ä¸ªåå­—ç©ºé—´å‡½æ•°ã€ä¸€ä¸ªå‰é¢å®šä¹‰çš„ç±»çš„ä¸€ä¸ªæˆå‘˜å‡½æ•°ã€ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç±»ã€‚ å…è®¸ä¸€ä¸ªç±»æˆæƒå…¶ä»–çš„å‡½æ•°è®¿é—®ä»–çš„éå…¬æœ‰æˆå‘˜ï¼Œé€šå¸¸å£°æ˜æ”¾åœ¨ç±»å¤´ä¹‹åã€‚ ç±»å£°æ˜å’Œå®šä¹‰ç±»å£°æ˜ï¼Œæ˜¯åªæœ‰ç±»å¤´ï¼Œæ²¡æœ‰ç±»ä½“ã€‚æ— æ³•ç¡®å®šç±»ç±»å‹çš„å¤§å°ï¼Œç±»æˆå‘˜ä¹Ÿæ˜¯æœªçŸ¥çš„ã€‚ä½†æ˜¯å¯ä»¥å£°æ˜æŒ‡å‘è¯¥ç±»ç±»å‹çš„æŒ‡é’ˆæˆ–å¼•ç”¨ã€‚ ç±»å®šä¹‰ï¼Œæ˜¯å…·æœ‰å®Œæ•´çš„ç±»ä½“ã€‚ ä»€ä¹ˆæ—¶å€™ç”¨ç±»å£°æ˜ï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨ç±»å®šä¹‰ï¼Ÿ ç±»å¯¹è±¡ç±»å®šä¹‰ä¸ä¼šåˆ†é…å­˜å‚¨åŒºï¼Œåªæœ‰å®šä¹‰äº†ä¸€ä¸ªç±»çš„å¯¹è±¡ï¼Œæ‰ä¼šåˆ†é…ã€‚ ç±»ç±»å‹ï¼Œå³å®šä¹‰çš„ä¸€ä¸ªç±»ã€‚é€šè¿‡å®ƒå®šä¹‰çš„å¯¹è±¡æ˜¯æœ‰ç”Ÿå‘½æœŸçš„ï¼Œç”Ÿå‘½æœŸæ ¹æ®å®ƒåœ¨å“ªä¸ªåŸŸä¸­è¢«å£°æ˜çš„ã€‚ ç±»å¯¹è±¡å¯ä»¥è¢«å¦ä¸€ä¸ªå¯¹è±¡åˆå§‹åŒ–æˆ–èµ‹å€¼ï¼Œæ‹·è´ä¸€ä¸ªç±»å¯¹è±¡ä¸æ‹·è´å®ƒçš„æˆå‘˜å‡½æ•°ç­‰ä»·ã€‚ å½“ä¸€ä¸ªç±»å¯¹è±¡è¢«æŒ‡å®šä¸ºå‡½æ•°å®å‚æˆ–å‡½æ•°è¿”å›å€¼æ—¶ï¼Œå®ƒå°±è¢«æŒ‰å€¼ä¼ é€’ã€‚æˆ‘ä»¬å¯ä»¥æŠŠä¸€ä¸ªå‡½æ•°å‚æ•°æˆ–è¿”å›å€¼å£°æ˜ä¸ºä¸€ä¸ªç±»ç±»å‹çš„æŒ‡é’ˆæˆ–å¼•ç”¨ã€‚ï¼ˆ7.3èŠ‚ï¼Œ7.4èŠ‚ï¼‰ å›é¡¾ä¸‹æŒ‡é’ˆï¼š 123456789int i = 100;int *p;int* p = &amp;i;int *p = &amp;i; p æ˜¯æŒ‡é’ˆå˜é‡ï¼Œç”¨æ¥å­˜å‚¨åœ°å€ã€‚æ³¨æ„æ•°ç»„åæ˜¯ const åœ°å€å¸¸é‡ï¼Œä»£è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ çš„åœ°å€ã€‚ æŒ‡é’ˆå’Œå¼•ç”¨çš„åŒºåˆ«ï¼š æŒ‡é’ˆï¼šæŒ‡é’ˆæ˜¯ä¸€ä¸ªå˜é‡ï¼Œåªä¸è¿‡è¿™ä¸ªå˜é‡å­˜å‚¨çš„æ˜¯ä¸€ä¸ªåœ°å€ï¼ŒæŒ‡å‘å†…å­˜çš„ä¸€ä¸ªå­˜å‚¨å•å…ƒï¼›è€Œå¼•ç”¨è·ŸåŸæ¥çš„å˜é‡å®è´¨ä¸Šæ˜¯åŒä¸€ä¸ªä¸œè¥¿ï¼Œåªä¸è¿‡æ˜¯åŸå˜é‡çš„ä¸€ä¸ªåˆ«åè€Œå·²ã€‚å¦‚ï¼š 12345int a=1; int *p=&amp;a;int a=1; int &amp;b=a; ä¸Šé¢å®šä¹‰äº†ä¸€ä¸ªæ•´å½¢å˜é‡å’Œä¸€ä¸ªæŒ‡é’ˆå˜é‡pï¼Œè¯¥æŒ‡é’ˆå˜é‡æŒ‡å‘açš„å­˜å‚¨å•å…ƒï¼Œå³pçš„å€¼æ˜¯aå­˜å‚¨å•å…ƒçš„åœ°å€. è€Œä¸‹é¢2å¥å®šä¹‰äº†ä¸€ä¸ªæ•´å½¢å˜é‡aå’Œè¿™ä¸ªæ•´å½¢açš„å¼•ç”¨bï¼Œäº‹å®ä¸Šaå’Œbæ˜¯åŒä¸€ä¸ªä¸œè¥¿ï¼Œåœ¨å†…å­˜å æœ‰åŒä¸€ä¸ªå­˜å‚¨å•å…ƒã€‚ â€œsizeofå¼•ç”¨â€å¾—åˆ°çš„æ˜¯æ‰€æŒ‡å‘çš„å˜é‡(å¯¹è±¡)çš„å¤§å°ï¼Œè€Œâ€sizeofæŒ‡é’ˆâ€å¾—åˆ°çš„æ˜¯æŒ‡é’ˆæœ¬èº«çš„å¤§å°ï¼› å¯ä»¥æœ‰constæŒ‡é’ˆï¼Œä½†æ˜¯æ²¡æœ‰constå¼•ç”¨ï¼› æŒ‡é’ˆçš„å€¼å¯ä»¥ä¸ºç©ºï¼Œä½†æ˜¯å¼•ç”¨çš„å€¼ä¸èƒ½ä¸ºNULLï¼Œå¹¶ä¸”å¼•ç”¨åœ¨å®šä¹‰çš„æ—¶å€™å¿…é¡»åˆå§‹åŒ–ï¼› æŒ‡é’ˆçš„å€¼åœ¨åˆå§‹åŒ–åå¯ä»¥æ”¹å˜ï¼Œå³æŒ‡å‘å…¶å®ƒçš„å­˜å‚¨å•å…ƒï¼Œè€Œå¼•ç”¨åœ¨è¿›è¡Œåˆå§‹åŒ–åå°±ä¸ä¼šå†æ”¹å˜äº† ç»§ç»­å›åˆ°ç±»å¯¹è±¡ä½œä¸ºå‡½æ•°å‚æ•°æ—¶ï¼Œç”¨æˆå‘˜è®¿é—®æ“ä½œç¬¦æ¥è®¿é—®ç±»å¯¹è±¡çš„æ•°æ®æˆå‘˜æˆ–æˆå‘˜å‡½æ•°ã€‚ç‚¹æ“ä½œç¬¦ä¸ç±»å¯¹è±¡æˆ–å¼•ç”¨è¿ç”¨; ç®­å¤´è®¿é—®æ“ä½œç¬¦ä¸ç±»å¯¹è±¡çš„æŒ‡é’ˆè¿ç”¨ã€‚ 1234567891011121314151617# include &quot;Screen.h&quot;bool isEqual(Screen&amp; s1, Screen *s2){ ... s1.height ... s2-&gt;height ... ...} isEqual æ˜¯éæˆå‘˜å‡½æ•°ï¼Œå¦‚æœè¦ç›´æ¥å¼•ç”¨ s1, s2 ä¸­çš„æ•°æ®æˆå‘˜ height æ˜¯ä¸å¯ä»¥çš„ã€‚æ‰€ä»¥å¿…é¡»å€ŸåŠ©äº Screen ä¸­çš„å…¬æœ‰æˆå‘˜å‡½æ•°ã€‚ 123s2-&gt;height ç­‰ä»·äº (*s2).height ç±»æˆå‘˜å‡½æ•°ä¸€ç»„æ“ä½œçš„é›†åˆã€‚ inline å’Œ é inline æˆå‘˜å‡½æ•°å†…è”å‡½æ•°çš„ä½œç”¨ï¼šå¦‚æœåœ¨ç¨‹åºä¸­è°ƒç”¨æŸä¸ªå‡½æ•°ï¼Œä¸ä½†è¦æ‹·è´å®å‚ï¼Œä¿å­˜æœºå™¨çš„å¯„å­˜å™¨ï¼Œç¨‹åºè¿˜å¿…é¡»è½¬å‘ä¸€ä¸ªæ–°çš„ä½ç½®ï¼Œè¿™æ ·ä¼šé™ä½æ•ˆç‡ã€‚ è€Œå†…è”å‡½æ•°å°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼Œåœ¨ç¨‹åºçš„è°ƒç”¨èŠ‚ç‚¹ä¸Šï¼Œâ€œå†…è”â€çš„å±•å¼€å‡½æ•°çš„æ“ä½œï¼Œä»è€Œé¢å¤–çš„æ‰§è¡Œå¼€é”€è¢«æ¶ˆé™¤äº†ã€‚ ç±»ä½“å†…å®šä¹‰çš„æˆå‘˜å‡½æ•°è‡ªåŠ¨çš„ä½œä¸ºå†…è”å‡½æ•°å¤„ç†ã€‚ ç±»ä½“å†…å£°æ˜ï¼Œç±»ä½“å¤–å®šçš„å‡½æ•°ï¼Œéœ€è¦æ˜¾ç¤ºçš„åŠ ä¸Šå…³é”®å­— inline. åŒæ—¶ç±»ä½“å¤–çš„å®šä¹‰éœ€è¦ç”¨ç±»åé™å®šä¿®é¥°(é™å®šä¿®é¥°ç¬¦ :: ) è®¿é—®ç±»æˆå‘˜æˆå‘˜å‡½æ•°çš„å®šä¹‰å¯ä»¥å¼•ç”¨ä»»ä½•ä¸€ä¸ªç±»æˆå‘˜ï¼Œæ— è®ºè¯¥æˆå‘˜æ˜¯ç§æœ‰è¿˜æ˜¯å…¬æœ‰ã€‚ æˆå‘˜å‡½æ•°å¯ä»¥ç›´æ¥è®¿é—®å®ƒæ‰€å±ç±»çš„æˆå‘˜ï¼Œæ— éœ€è®¿é—®æ“ä½œç¬¦ã€‚è¿™å®é™…ä¸Šæ˜¯é€šè¿‡ this æŒ‡é’ˆå®ç°çš„ã€‚ ç§æœ‰ä¸å…¬æœ‰æˆå‘˜å‡½æ•°å…¬æœ‰å‡½æ•°é›†å®šä¹‰äº†ç±»çš„æ¥å£ã€‚ç§æœ‰æˆå‘˜å‡½æ•°ä¸ºå…¶ä»–æˆå‘˜å‡½æ•°æä¾›æ”¯æŒã€‚ ç‰¹æ®Šçš„æˆå‘˜å‡½æ•°æ„é€ å‡½æ•°ï¼šç®¡ç†ç±»å¯¹è±¡å¹¶å¤„ç†åˆå§‹åŒ–ã€èµ‹å€¼ã€å†…å­˜ç®¡ç†ã€ç±»å‹è½¬æ¢ã€ææ„ç­‰æ´»åŠ¨ã€‚æ¯æ¬¡å®šä¹‰ä¸€ä¸ªç±»å¯¹è±¡æˆ– new è¡¨è¾¾å¼åˆ†é…ä¸€ä¸ªç±»å¯¹è±¡éƒ½ä¼šè°ƒç”¨å®ƒã€‚ æ„é€ å‡½æ•°çš„åå­—å¿…é¡»ä¸ç±»åç›¸åŒã€‚ const å’Œ volatile æˆå‘˜å‡½æ•°åªæœ‰è¢«å£°æ˜ä¸º const çš„æˆå‘˜å‡½æ•°æ‰èƒ½è¢«ä¸€ä¸ª const å¯¹è±¡è°ƒç”¨ã€‚å…³é”®å­— const åœ¨å‡½æ•°ä½“å’Œå‚æ•°è¡¨ä¹‹é—´. é€šå¸¸ä¸€ä¸ªç±»å¦‚æœæƒ³è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå…¶ä¸­ä¸ä¿®æ”¹ç±»æ•°æ®æˆå‘˜çš„æˆå‘˜å‡½æ•°åº”è¯¥å£°æ˜ä¸º const æˆå‘˜å‡½æ•°ã€‚ ä½†æ˜¯å³ä½¿å£°æ˜äº†ï¼Œè¿™ä¸ªæˆå‘˜å‡½æ•°ä¾ç„¶æœ‰å¯èƒ½ä¿®æ”¹æ•°æ®æˆå‘˜ï¼Œå¦‚æœè¿™ä¸ªç±»å«æœ‰æŒ‡é’ˆã€‚ æ¯”å¦‚ï¼š 12345private: char *_text; _text ä¸èƒ½è¢«ä¿®æ”¹ï¼Œä½†æ˜¯å®ƒæŒ‡å‘çš„å­—ç¬¦å´æ˜¯å¯ä»¥è¢«ä¿®æ”¹çš„ã€‚æ‰€ä»¥ç¨‹åºå‘˜è¿™ä¸ªæ—¶å€™å°±éœ€è¦æ³¨æ„äº†ã€‚ã€‚ æ„é€ å‡½æ•°å’Œææ„å‡½æ•°å³ä½¿ä¸æ˜¯ const æˆå‘˜å‡½æ•°ï¼Œä¹Ÿèƒ½è¢« const å¯¹è±¡è°ƒç”¨ã€‚ volatile è·Ÿ const ç”¨æ³•ä¸€æ ·ï¼Œå®ƒç”¨æ¥æç¤ºç¼–è¯‘å™¨è¯¥å¯¹è±¡çš„å€¼å¯èƒ½åœ¨ç¼–è¯‘å™¨æœªè¢«æ£€æµ‹åˆ°çš„æƒ…å†µä¸‹è¢«ä¿®æ”¹ã€‚å› ä¸ºï¼Œç¼–è¯‘å™¨ä¸èƒ½æ­¦æ–­çš„å¯¹å¼•ç”¨è¿™äº›å¯¹è±¡çš„ä»£ç è¿›è¡Œä¼˜åŒ–ã€‚ mutable æ•°æ®æˆå‘˜ä¸€æ—¦ä¸€ä¸ªå¯¹è±¡è¢«å£°æ˜ä¸º constï¼Œå®ƒçš„å†…å®¹å°±ä¸èƒ½ä¿®æ”¹ã€‚ä½†æ˜¯å…¶ä¸­æŸäº›æ•°æ®æˆå‘˜ï¼Œæ¯”å¦‚ç´¢å¼•ï¼Œè¢«ä¿®æ”¹ä¹‹åå¹¶æ²¡æœ‰ä¿®æ”¹å¯¹è±¡æœ¬èº«ï¼Œè¿™ä¸ªæ—¶å€™å°±å¯ä»¥å°†è¯¥æ•°æ®æˆå‘˜ï¼ˆä¹Ÿå°±æ˜¯ç´¢å¼•ï¼‰å£°æ˜ä¸º mutable. é‚£æ ·ï¼Œå³ä½¿ const å¯¹è±¡ï¼Œ const æˆå‘˜å‡½æ•°ä¿®æ”¹äº†è¯¥æ•°æ®æˆå‘˜ï¼Œä¹Ÿä¸ä¼šæœ‰ç¼–è¯‘é”™è¯¯ã€‚ éšå«çš„æŒ‡é’ˆ thisæ¯ä¸ªç±»æˆå‘˜å‡½æ•°éƒ½å«æœ‰ä¸€ä¸ªæŒ‡å‘è¢«è°ƒç”¨å¯¹è±¡çš„æŒ‡é’ˆã€‚ä½†æ˜¯ä¸€èˆ¬ä¸éœ€è¦æ˜¾ç¤ºçš„å†™å‡ºæ¥ï¼Œå¦‚æœå†™å‡ºæ¥ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚ ä½•æ—¶ä½¿ç”¨æŒ‡é’ˆå‘¢ï¼Ÿå½“è¿ç»­ä½¿ç”¨æˆå‘˜å‡½æ•°æ—¶ï¼Œæ¯”å¦‚ myVector.find().sort().insert() æ—¶ï¼Œå¯¹åº”çš„æˆå‘˜å‡½æ•°è¿”å›çš„å€¼åº”è¯¥æ˜¯è¢«è°ƒç”¨å¯¹è±¡æœ¬èº«ï¼Œä¹Ÿå°±æ˜¯ *this. è¿˜æœ‰ä¸€ç§æƒ…å†µï¼Œ copy å‡½æ•°ï¼š 123456789101112131415void Screen::copy(const Screen&amp; sobj){ if (this != &amp;sobj) { // æŠŠ sobj çš„å€¼æ‹·è´åˆ° * this ä¸­ }} è¿™é‡Œçš„ this æŒ‡é’ˆå«æœ‰è¢«è°ƒç”¨å¯¹è±¡çš„åœ°å€ã€‚å¦‚æœ sobj çš„åœ°å€ &amp;sobj ä¸ this ç›¸åŒï¼Œé‚£å°±ä¸éœ€è¦æ‹·è´äº†ã€‚ æ³¨æ„è¿™é‡Œ &amp; ç”¨æ³•ï¼šå¼•ç”¨å’Œå–åœ°å€ã€‚ é™æ€ç±»æˆå‘˜é™æ€æ•°æ®æˆå‘˜å¯¹äºéé™æ€æˆå‘˜ï¼Œæ¯ä¸ªç±»éƒ½æœ‰ä¸€ä¸ªè‡ªå·±çš„æ‹·è´ã€‚è€Œé™æ€æˆå‘˜å¯¹æ¯ä¸ªç±»ç±»å‹åªæœ‰ä¸€ä¸ªæ‹·è´ã€‚é™æ€æ•°æ®æˆå‘˜åªæœ‰ä¸€ä»½ï¼Œç”±è¯¥ç±»ç±»å‹çš„æ‰€æœ‰å¯¹è±¡å…±åŒè®¿é—®ã€‚ ä¸åŒäºå…¨å±€å¯¹è±¡ï¼Œå®ƒå¯ä»¥éšè—ï¼Œå¹¶ä¸”ä¸ä¼šä¸å…¶ä»–å…¨å±€åå­—å†²çªã€‚ å…³é”®å­— static. æ³¨æ„ä¸ const çš„åŒºåˆ«ï¼Œæ²¡æœ‰åŠ  const æ„å‘³ç€æ˜¯å¯ä»¥æ›´æ–°çš„ã€‚åªéœ€è¦æ›´æ–°ä¸€æ¬¡ï¼Œæ‰€æœ‰çš„ç±»å¯¹è±¡å¯¹åº”çš„å€¼éƒ½ä¼šæ›´æ–°ã€‚ é™æ€ç±»æˆå‘˜çš„æ˜¾ç¤ºåˆå§‹åŒ–ï¼Œåœ¨ç±»å®šä¹‰ä¹‹å¤–ï¼Œç”¨ç±»åé™å®šä¿®é¥°ã€‚åœ¨é™æ€æ•°æ®æˆå‘˜çš„å®šä¹‰ä¸­ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ç§æœ‰æˆå‘˜ï¼Œè¿™ä¸åœ¨ç±»æˆå‘˜å‡½æ•°ä¸­ç›´æ¥å¼•ç”¨ç§æœ‰æˆå‘˜æ˜¯ä¸€æ ·çš„ã€‚ å¯¹äºé™æ€æ•°æ®æˆå‘˜ï¼Œé™¤äº†é€šè¿‡ç±»å¯¹è±¡ä½¿ç”¨æˆå‘˜è®¿é—®æ“ä½œç¬¦è®¿é—®ä¹‹å¤–ï¼Œè¿˜å¯ä»¥ç›´æ¥ä½¿ç”¨ç±»ååŠ é™å®šä¿®é¥°ç¬¦è®¿é—® 123Account::_intersetRate é™æ€æˆå‘˜å‡½æ•°é™æ€æˆå‘˜å‡½æ•°ï¼Œåªè®¿é—®é™æ€æ•°æ®æˆå‘˜ï¼Œè€Œä¸è®¿é—®ä»»ä½•å…¶ä»–æ•°æ®æˆå‘˜ã€‚æ‰€ä»¥å®ƒä»¬ä¸å“ªä¸ªå¯¹è±¡æ¥è°ƒç”¨è¿™ä¸ªå‡½æ•°æ— å…³ã€‚ åœ¨ç±»ä½“ä¸­å£°æ˜æ—¶éœ€è¦åŠ å…³é”®å­— static, ç±»ä½“å¤–ä¸èƒ½æŒ‡å®šå…³é”®å­—ã€‚å¹¶ä¸”ä¸èƒ½è®¾å®šä¸º const å’Œ volatile. æŒ‡å‘ç±»æˆå‘˜çš„æŒ‡é’ˆæ™®é€šå‡½æ•°æŒ‡é’ˆ 7.9èŠ‚æˆå‘˜å‡½æ•°æŒ‡é’ˆ","link":"/2018/12/12/Cplusplus-prime/"},{"title":"chapter10-è¯æ€§æ ‡æ³¨","text":"è¯æ€§åˆ†ç±» clossed class, open Classes æ ‡è®°é›† Tagset HMM tagging: ç”Ÿæˆæ¨¡å‹ $p(t_i|w_i)=p(w_i|t_i)p(t_i|t_{i-1})$ æœ‰viterbiç®—æ³•å’Œgreedyç®—æ³•ï½ MEMM: åˆ¤åˆ«æ¨¡å‹ $p(t_i|w_i,t_{i-1},â€¦f;w)$ ä¹Ÿæœ‰Viterbiå’Œgreedyä¸¤ç§ç®—æ³•ï¼Œæ›´æ–°å‚æ•°wï½ åŒå‘æ¨¡å‹ CRFï¼Œchapter20è®² å‰è¨€ï¼š è¯æ€§æ ‡æ³¨ï¼ˆparts-of-speechï¼‰åˆç§° (pos, word classes, or syntactic categories) åˆ†ä¸º8ç±»ï¼š noun, verb, pronoun, preposition, adverb, conjunction, participle, and article.ï¼ˆåè¯ï¼ŒåŠ¨è¯ï¼Œä»£è¯ï¼Œä»‹è¯ï¼Œå‰¯è¯ï¼Œè¿è¯ï¼Œåˆ†è¯å’Œæ–‡ç« ã€‚ï¼‰ è¯æ€§æ ‡æ³¨åœ¨å¯»æ‰¾å‘½åå®ä½“ï¼ˆnamed entitiesï¼‰å’Œå…¶ä»–ä¸€äº› ä¿¡æ¯æå–(information extraction) çš„å·¥ä½œä¸­æ˜¯å¾ˆé‡è¦çš„ç‰¹å¾ã€‚è¯æ€§æ ‡æ³¨ä¹Ÿä¼šå½±å“å½¢æ€è¯ç¼€(morphological affixes)ï¼Œä»è€Œå½±å“è¯å¹²(stemming)çš„ä¿¡æ¯æ£€ç´¢;åœ¨è¯­éŸ³è¯†åˆ«ä¸­å¯¹è¯­éŸ³çš„ç”Ÿæˆä¹Ÿæœ‰å¾ˆé‡è¦çš„ä½œç”¨ï¼Œæ¯”å¦‚CONtentæ˜¯åè¯ï¼Œè€ŒconTENTæ˜¯å½¢å®¹è¯ã€‚ (Mostly) English Word Classesä¼ ç»Ÿä¸Šï¼Œè¯ç±»æ ¹æ®å½¢æ€å’Œè¯­æ³•åŠŸèƒ½æ¥åˆ†ç±»ï¼š distributional properties åˆ†å¸ƒç‰¹å¾:å•è¯å‡ºç°åœ¨ç›¸ä¼¼çš„ç¯å¢ƒä¸­ï¼› morphological properties å½¢æ€ç‰¹å¾ï¼šå•è¯çš„è¯ç¼€å…·æœ‰ç›¸ä¼¼çš„åŠŸèƒ½ã€‚ è¯ç±»å¯åˆ†ä¸ºä¸¤å¤§ç±»ï¼šå°é—­ç±» closed class types å’Œ å¼€æ”¾ç±» open class type. å°é—­ç±»ï¼šprepositions ä»‹è¯ï¼Œfunction wordsï¼Œ likeï¼Œofï¼Œit,â€¦. ä¸€èˆ¬éƒ½å¾ˆçŸ­ï¼Œè€Œä¸”é¢‘ç‡é«˜;å¼€æ”¾ç±»ï¼šnouns, verbs,adjectives, and adverbs noun åè¯: ä¸“æœ‰åè¯(Proper nouns)å’Œæ™®é€šåè¯ï¼ˆcommon nounsï¼‰ã€‚ä¸“æœ‰åè¯ä¸€èˆ¬ä¸å—å† è¯é™åˆ¶ï¼Œä¸”ä¸€åœ°ä¸ªå­—æ¯è¦å¤§å†™ã€‚æ™®é€šåè¯åˆåˆ†ä¸ºå¯æ•°åè¯ï¼ˆcount nounsï¼‰å’Œä¸å¯æ•°åè¯ï¼ˆmass nounsï¼‰. å½“æŸç§ä¸œè¥¿èƒ½åœ¨æ¦‚å¿µä¸ŠæŒ‰ç…§åŒè´¨æ¥åˆ†ç»„æ—¶ï¼Œå°±ä½¿ç”¨ç‰©è´¨åè¯ï¼Œè¿™æ ·çš„è¯æ˜¯ä¸å¯æ•°çš„ã€‚ï¼ˆMass nouns are used when something is conceptualized as a homogeneous groupï¼‰. verb åŠ¨è¯ï¼š ç”¨æ¥è¡¨ç¤ºåŠ¨ä½œæˆ–è¿‡ç¨‹ã€‚åŠ¨è¯æœ‰è‹¥å¹²å½¢æ€ï¼Œ(non-third-person-sgéç¬¬ä¸‰äººç§°å•æ•° (eat), third-person-sgç¬¬ä¸‰äººç§°å•æ•° (eats), è¿›è¡Œæ—¶progressive (eating), è¿‡å»åˆ†è¯past participle (eaten)). adjectives å½¢å®¹è¯ï¼š æè¿°æ€§è´¨å’Œè´¨é‡çš„å•è¯ã€‚ adverbs å‰¯è¯ï¼š æ— è®ºæ˜¯ä»è¯­ä¹‰ä¸Šè¿˜æ˜¯å½¢æ€ä¸Šï¼Œéƒ½æ¯”è¾ƒæ‚ã€‚é€šå¸¸ç”¨æ¥ä¿®é¥°åŠ¨è¯ï¼Œä¹Ÿå¯ä»¥ç”¨æ¥ä¿®é¥°å…¶ä»–å‰¯è¯æˆ–æ˜¯åŠ¨è¯çŸ­è¯­ã€‚æ–¹ä½å‰¯è¯å’Œåœ°ç‚¹å‰¯è¯ Directional adverbs or locative adverbsï¼Œ ç¨‹åº¦å‰¯è¯ degree adverbs (extremely, very, somewhat); æ–¹å¼å‰¯è¯ manner adverbs (slowly, slinkily, delicately); æ—¶é—´å‰¯è¯ temporal adverbs (yesterday, Monday). å°é—­ç±»ï¼š preposition ä»‹è¯: å‡ºç°åœ¨åè¯çŸ­è¯­ä¹‹å‰ï¼Œä»è¯­ä¹‰ä¸Šè®²ï¼Œä»–ä»¬æ˜¯è¡¨ç¤ºå…³ç³»çš„ã€‚ particle å°å“è¯ï¼š ä¸ä»‹è¯æˆ–å‰¯è¯ç›¸ä¼¼ï¼Œç»å¸¸å’ŒåŠ¨è¯ç»“åˆï¼Œå½¢æˆåŠ¨è¯çŸ­è¯­ã€‚ determiner é™å®šè¯: å…¶ä¸­åŒ…æ‹¬ article å† è¯ï¼Œa,an,the. å…¶ä»–çš„é™å®šè¯ï¼Œæ¯”å¦‚ this,that conjunction è¿è¯ï¼š è¿æ¥ä¸¤ä¸ªçŸ­è¯­ã€åˆ†å¥æˆ–å¥å­ã€‚ pronoun ä»£è¯ï¼š ç®€çŸ­åœ°æ´å¼•æŸäº›åè¯çŸ­è¯­ã€å®ä½“æˆ–äº‹ä»¶çš„ä¸€ç§å½¢å¼ã€‚ auxiliary åŠ©åŠ¨è¯ï¼š åŒ…æ‹¬ç³»åŠ¨è¯be,ä¸¤ä¸ªåŠ¨è¯do, have,ä»¥åŠæƒ…æ€åŠ¨è¯ã€‚ è‹±è¯­ä¸­è¿˜æœ‰å¾ˆå¤š å¹è¯(oh,ah,hey,man,alas),å¦å®šè¯(no,not),ç¤¼è²Œæ ‡å¿—è¯(please,thank you). æ˜¯å¦æŠŠè¿™äº›è¯æ”¾åœ¨ä¸€èµ·,å–å†³äºæ ‡è®°çš„ç›®çš„ã€‚ The Penn Treebank Part-of-Speech Tagset æ ‡è®°é›† Peen Treebankæ ‡è®°é›†æ˜¯Brownè¯­æ–™åº“åŸæœ‰çš„87ä¸ªæ ‡è®°é›†ä¸­æŒ‘é€‰å‡ºæ¥çš„ã€‚è¿˜æœ‰ä¸¤ä¸ªæ¯”è¾ƒå¤§çš„æ ‡è®°é›† C5,C7. è¯æ€§æ ‡æ³¨è¯æ€§æ ‡æ³¨(pos tagging)ä¸è®¡ç®—æœºè¯­è¨€çš„ tokenization ï¼ˆè¯å½¢è¿˜åŸï¼Ÿåˆ†è¯ï¼‰ è¿‡ç¨‹æ˜¯ä¸€è‡´çš„ï¼Œä½†è¯æ€§æ ‡æ³¨å…·æœ‰æ›´å¤šçš„æ­§ä¹‰æ€§ã€‚åœ¨Brownè¯­æ–™åº“ä¸­ï¼Œåªæœ‰11.5%çš„è‹±è¯­è¯å‹(word type)æ˜¯å…·æœ‰æ­§ä¹‰çš„,40%ä»¥ä¸Šçš„è¯ä¾‹(word token)æœ‰æ­§ä¹‰çš„ã€‚ chapter2ä¸­å‡ ä¸ªå®¹æ˜“æ··æ·†çš„æ¦‚å¿µï¼š è¯å‹ word type ä¸åŒ…æ‹¬é‡å¤è¯ï¼Œ è¯ä¾‹ word token åŒ…æ‹¬é‡å¤è¯ã€‚ lemma æ˜¯è¯æ„ï¼Œam is areæ˜¯åŒä¸€ä¸ªå•è¯be Wordform æ˜¯è¯çš„å½¢çŠ¶ã€‚ è¯æ€§æ ‡æ³¨æ˜¯æ­§ä¹‰æ¶ˆè§£ï¼ˆdisambiguationï¼‰çš„ä¸€ä¸ªé‡è¦æ–¹é¢ï¼Œå¤§å¤šæ•°çš„æ ‡æ³¨ç®—æ³•åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºè§„åˆ™çš„æ ‡æ³¨ç®—æ³•(rule-based tagger)ï¼Œä¸€ç±»æ˜¯éšæœºæ ‡æ³¨ç®—æ³•(stochastic tagger). HMM Part-of-Speech TaggingåŸºäºéšé©¬å°”å¯å¤«çš„è¯æ€§æ ‡æ³¨ï¼Œè¯­æ–™åº“æ˜¯è§‚å¯Ÿåºåˆ—ï¼Œpart-of-speechæ˜¯éšè—çŠ¶æ€,å¯¹äºPeen Treebankæ ‡è®°é›†æœ‰45ä¸­éšè—çŠ¶æ€ã€‚å› ä¸ºè®­ç»ƒæ•°æ®æ˜¯æœ‰äººå·¥æ ‡æ³¨çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼Œæ ¹æ®å·²çŸ¥çš„è§‚å¯Ÿåºåˆ—å’Œå¯¹åº”çš„éšè—çŠ¶æ€ï¼Œå¯ä»¥æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–è€…ç›¸å¯¹é¢‘ç‡è®¡ç®—å‡ºçŠ¶æ€è½¬ç§»çŸ©é˜µå’Œå‘å°„çŸ©é˜µçš„å‚æ•°ï½ æ‰€ä»¥è¯æ€§æ ‡æ³¨çš„é—®é¢˜å’Œé¢„æµ‹é—®é¢˜æ˜¯ä¸€æ ·çš„ï½å¯ä»¥ä½¿ç”¨viterbiç®—æ³•ï¼Œå¯¹å¸¦æ ‡æ³¨çš„åºåˆ—è¿›è¡Œæ ‡æ³¨ï½ The basic equation of HMM TaggingHMM decoding:æ±‚æ¦‚ç‡æœ€å¤§çš„taggingåºåˆ— $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)$$ ä½¿ç”¨bayeså…¬å¼ï¼š $$\\hat t_1^n=argmax_{t_1^n}\\dfrac{P(w_1^n|t_1^n)P(t_1^n)}{P(w_1^n)}$$ ç›®çš„æ˜¯è®©è§‚å¯Ÿåºåˆ—çš„ä¼¼ç„¶æ¦‚ç‡æœ€å¤§åŒ–ï½å› æ­¤è§‚å¯Ÿåºåˆ—å¯¹äºä»»ä½•éšè—åºåˆ—éƒ½æ˜¯ä¸€æ ·çš„ï½æ•…å¯ä»¥ç›´æ¥å»æ‰åˆ†æ¯ï¼š $$\\hat t_1^n=argmax_{t_1^n}P(w_1^n|t_1^n)P(t_1^n)$$ æ ¹æ®æ¦‚ç‡å›¾æ¨¡å‹ä¸­ï¼Œä¸€ä¸ªwordçš„ç”Ÿæˆåªå–å†³äºå…¶å¯¹åº”çš„tagï¼Œè€Œä¸å…¶ä»–wordå’Œwordæ— å…³ï¼Œä¹Ÿå°±æ˜¯æ¡ä»¶ç‹¬ç«‹å¯å¾—ï¼š $$P(w_1^n|t_1^n)\\approx \\prod_{i=1}^nP(w_i|t_i)$$ æ ¹æ®bigramå‡è®¾ï¼š $$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1})$$ è”ç«‹å¯å¾—ï¼š $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)\\approx argmaxmax_{t_1^n}\\prod_{i=1}^nP(w_i|t_i)P(t_i|t_{i-1})\\tag{10.8}$$ Estimating probabilities æ¦‚ç‡ä¼°è®¡åœ¨HMM taggingä¸­ï¼Œæ¦‚ç‡ä¼°è®¡æ˜¯ç›´æ¥é€šè¿‡å¯¹è®­ç»ƒè¯­æ–™åº“ä¸­è¿›è¡Œè®¡æ•°å¾—åˆ°çš„ã€‚ çŠ¶æ€è½¬ç§»æ¦‚ç‡ transition probabilities: $P(t_i|t_{i-1})$,shape=(45,45) $$P(t_i|t_{i-1})=\\dfrac{C(t_{i-1},t_i)}{C(t_{i-1})}$$ å‘å°„æ¦‚ç‡ emission probabilities: $P(w_i|t_i)$,shape=(45, V) $$P(w_i|t_i)=\\dfrac{C(t_i,w_i)}{C(t_i)}$$ Working through an exampleä¸¾ä¸ªæ —å­ å…¶ä¸­çŠ¶æ€è½¬ç§»æ¦‚ç‡å’Œå‘å°„æ¦‚ç‡ä¾æ®å·²ç»æ ‡è®°å¥½çš„è¯­æ–™åº“WSJ corpusè®¡ç®—å¾—åˆ°ï¼Œå…¶å¯¹åº”çš„çŠ¶æ€è½¬ç§»çŸ©é˜µAå’Œå‘å°„çŸ©é˜µBï¼š é‚£ä¹ˆå¯¹åº”çš„å¯èƒ½çš„çŠ¶æ€åºåˆ—å¦‚ä¸‹å›¾ï¼ŒåŠ ç²—çš„é»‘è‰²è·¯å¾„æ˜¯æ¦‚ç‡æœ€å¤§çš„çŠ¶æ€åºåˆ—ã€‚ Viterbiç®—æ³•å…ˆå›é¡¾ä¸‹éšé©¬å°”ç§‘å¤«æ¨¡å‹ä¸­çš„viterbiç®—æ³•ï¼š è¿™æ˜¯åœ¨ç»™å®šæ¨¡å‹ $\\lambda=(A,B)$ çš„æƒ…å†µä¸‹ï¼Œå¯»æ‰¾çŠ¶æ€åºåˆ—ä½¿å¾—è§‚å¯Ÿåºåˆ—çš„ä¼¼ç„¶æ¦‚ç‡æœ€å¤§ï½ çŸ©é˜µ viterbi [N+2,T], ç¬¬ä¸€è¡Œå’Œç¬¬äºŒè¡Œæ˜¯ states 0 å’Œ $q_F$ ä»state 1 å¼€å§‹ï¼š $$v_t(j)=\\max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t)$$ å¯ä»¥çœ‹åˆ°ï¼Œè¡Œä»£è¡¨çš„æ˜¯[start, NNP, MD,â€¦, DT, end]^T,æ€»å…± N+2 ä¸­çŠ¶æ€ã€‚ åˆ—ä»£è¡¨çš„æ˜¯æ—¶é—´æ­¥ [1,2,3â€¦,t],å› ä¸ºt=0æ—¶åˆ»ï¼Œè‚¯å®šæ˜¯startçŠ¶æ€ï¼Œæ²¡æœ‰emission. ä»ç¬¬ä¸€åˆ— t=1 åˆ° ç¬¬äºŒåˆ— t=2ï¼Œæ¯ä¸€æ­¥éƒ½æ˜¯å‰ä¸€æ­¥çš„Nç§çŠ¶æ€è½¬ç§»åˆ°å½“å‰çŠ¶æ€çš„max. Extending the HMM Algorithm to Trigrams$$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1})$$ æ”¹ä¸ºï¼š $$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1},t_{t-2})$$ æ¯ä¸€æ­¥çš„è®¡ç®—å¤æ‚åº¦ï¼Œä»Nå˜æˆ $N^2$. æœ¬ä¹¦ä¸Šå†™çš„ï¼Œå½“å‰ state-of-art çš„HMM æ ‡æ³¨ç®—æ³•æ˜¯ A statistical part-of-speech tagger. In ANLP 2000, Seattle. è®ºæ–‡ä½œè€…è®©æ ‡æ³¨è€…åœ¨æ¯å¥è¯ç»“å°¾å¤„åŠ ä¸Š end-of-sequence marker for $t_{n+1}$. è¿™é‡Œçš„nè¡¨ç¤ºåºåˆ—é•¿åº¦ã€‚ $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)\\approx argmaxmax_{t_1^n}[\\prod_{i=1}^nP(w_i|t_i)P(t_i|t_{i-1},t_{i-2})]P(t_{n+1}|t_n)$$ å…¶ä¸­ï¼Œè½¬ç§»æ¦‚ç‡å¯ä»¥é€šè¿‡è¯­æ–™åº“è®¡æ•°å¾—åˆ°ï¼š $$P(t_i|t_{i-1},t_{i-2})=\\dfrac{C(t_{i-2},t_{i-1},t_i)}{C(t_{i-2},t_{i-1})}$$ ä½†æ˜¯åœ¨æµ‹è¯•é›†ä¸­ï¼Œå¾ˆå¯èƒ½é‡åˆ°çŠ¶æ€åºåˆ—ï¼ˆä¹Ÿå°±æ˜¯tagåºåˆ—ï¼‰ $t_{i-2},t_{i-1},t_i$ åœ¨è®­ç»ƒé›†ä¸­ä»æœªå‡ºç°è¿‡ï¼Œä¹Ÿå°±æ˜¯å…¶æ¦‚ç‡ä¸ºzerosï¼Œè¿™æ ·å°±æ— æ³•å¯¹æµ‹è¯•é›†ä¸­çš„åºåˆ—è¿›è¡Œæ ‡æ³¨äº†ã€‚ä¹Ÿå°±æ˜¯è¯­è¨€æ¨¡å‹ä¸­æåˆ°çš„zerosæƒ…å†µï½æ¯”å¦‚å›¾10.7ä¸­ï¼ŒæŸä¸ªæ—¶é—´æ­¥æ²¡æœ‰å¯é€‰æ‹©çš„éšè—çŠ¶æ€tag åŒæ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨æ’å€¼æ³•ï½ å…¶ä¸­ $\\lambda$ çš„è®¡ç®—å¯ä»¥ç”¨ deleted interpolation ä¸å¤ªç†è§£ã€‚ã€‚ã€‚ Unknown WordsSamuelsson (1993) and Brants (2000)æ ¹æ®å½¢æ€ï¼ˆmorphologyï¼‰æ¥åˆ¤æ–­unknown wordçš„å¯èƒ½çŠ¶æ€. æ¯”å¦‚ -s é€šå¸¸æ˜¯å¤æ•°åè¯NNS, -edé€šå¸¸æ˜¯è¿‡å»æ—¶æ€VBN, -ableé€šå¸¸æ˜¯å½¢å®¹è¯ JJâ€¦. $$P(t_i|l_{n-i+1}â€¦l_n)$$ è¿™ä¸ªæ¦‚ç‡ä¹Ÿæ˜¯å¯ä»¥é€šè¿‡è¯­æ–™åº“ä¸­ç›¸å¯¹é¢‘ç‡è®¡ç®—å¾—åˆ°ï½ ä»–ä»¬ä½¿ç”¨å°½é‡çŸ­çš„åç¼€ï¼ˆshorter and shorter suffixesï¼‰åº”ç”¨å›é€€smoothingæ–¹æ³•æ¥ä¼°è®¡unknow wordçš„å¯èƒ½çŠ¶æ€ï¼Œä½†è¦å°½é‡é¿å…å…¶çŠ¶æ€ä¸º closed class,æ¯”å¦‚ä»‹è¯ prepositions,å¯ä»¥å°†unknow wordçš„çŠ¶æ€tagé€‰æ‹©ä»é¢‘ç‡ $\\le 10$ ä¸­é€‰æ‹©ï¼Œæˆ–è€…åªä»open classes ä¸­é€‰æ‹©å¯èƒ½çš„tag. Brants(2000) è¿˜é¢å¤–ä½¿ç”¨äº†é¦–å­—æ¯çš„ç‰¹å¾ä¿¡æ¯ï¼Œå°†å…¬å¼(10.21)å¯æ”¹å†™ä¸ºï¼š $$P(t_i,c_i|t_{i-1},c_{i-1},t_{i-2},c_{i-2})$$ è¿™æ ·è¯­æ–™åº“ä¸­æ ‡è®°é›†å°±åŒ…æ‹¬é¦–å­—æ¯å¤§å†™å’Œå°å†™ä¸¤ç§ç‰ˆæœ¬ï¼Œå…¶å¯¹åº”çš„æ ‡è®°é›†tagsetå°±å¢å¤§ä¸º2å€ã€‚ state-of-art HMM taggingä¹Ÿå°±æ˜¯è®ºæ–‡Brants(2000) çš„å‡†ç¡®ç‡è¾¾åˆ° 96.7% åœ¨ä½¿ç”¨Penn Treebankæ ‡è®°é›†ã€‚ Maximum Entropy Markov Modelsæœ€å¤§ç†µéšé©¬å°”å¯å¤«æ¨¡å‹ï¼Œæ˜¯ä¾æ®logisticå›å½’çš„ï¼Œæ‰€ä»¥å®ƒæ˜¯åˆ¤åˆ«æ¨¡å‹ discriminative sequence model, è€ŒHMMæ˜¯ç”Ÿæˆæ¨¡å‹ generative sequence model. sequence of words: W = $w_1^n$ sequence of tags: T = $t_1^n$ é‚£ä¹ˆHMMæ¨¡å‹çš„P(T|W)æ˜¯ä¾æ®bayesè§„åˆ™å’Œæœ€å¤§ä¼¼ç„¶P(w|T)å¾—åˆ°çš„ï¼š è€Œåœ¨æœ€å¤§ç†µéšé©¬å°”ç§‘å¤«æ¨¡å‹ï¼Œç›´æ¥è®¡ç®—åéªŒæ¦‚ç‡ posterior P(T|W). $$\\hat T = argmax_TP(T|W) = argmax_T\\prod_iP(t_i|w_i,t_i-1)$$ å¯¹æ¯”HMMå’ŒMEMMï¼ŒHMMè®¡ç®—æ˜¯åœ¨tagçš„æ¡ä»¶ä¸‹è§‚å¯Ÿåºåˆ—ä¼¼ç„¶æœ€å¤§ï¼Œ MEMMè®¡ç®—æ˜¯åœ¨è§‚å¯Ÿåºåˆ—çš„æ¡ä»¶ä¸‹tagåºåˆ—ä¼¼ç„¶æœ€å¤§ï½æ˜¾ç„¶åœ¨tagå·²ç»æ ‡æ³¨å¥½çš„è®­ç»ƒé›†é‡Œï¼Œåˆ¤åˆ«æ¨¡å‹æ˜¯å®Œå…¨å¯è¡Œçš„ï¼Œä¹Ÿè®¸ä¼šæ˜¯æ›´å¥½çš„ï½ Features in a MEMMåœ¨HMMä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸‹ä¸€ä¸ªtagçš„æ¦‚ç‡åªä¾èµ–ä¸å‰ä¸€ä¸ªæˆ–ä¸¤ä¸ªtagï¼Œä»¥åŠå½“å‰çš„wordï¼Œå¦‚æœæƒ³è€ƒè™‘æ›´å¤šçš„ç‰¹å¾ï¼Œæ¯”å¦‚é¦–å­—æ¯capitalizationï¼Œåç¼€suffixï¼Œé‚£æ ·è®¡ç®—å¤æ‚åº¦ä¼šå¢åŠ å¾ˆå¤šã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMEMMå¯ä»¥è€ƒè™‘æ›´å¤šçš„ç‰¹å¾ï¼š MEMMå¯ä»¥ä¾èµ–çš„ç‰¹å¾å¯ä»¥æ˜¯ wordï¼Œneighboring words, previous tags, and various tags, and various combinations. å¯ä½¿ç”¨ features templates æ¥è¡¨ç¤ºï¼š å¯¹äºä¹‹å‰çš„ä¾‹å­ï¼š Janet/NNP will/MD back/VB the/DT bill/NN å½“ $w_i$ æ˜¯ back æ—¶ï¼Œå¯¹åº”çš„ç‰¹å¾æ¨¡æ¿ features templates,ä¹Ÿå°±æ˜¯ know-words feature: é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰å½“å‰è¯ $w_i$ çš„æ‹¼å†™å’Œå½¢çŠ¶ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯¹äºå¤„ç† unknow word å¾ˆæœ‰å¿…è¦ï½ é‚£ä¹ˆæ ¹æ®ä¸Šè¿°è§„åˆ™ï¼Œå•è¯ well-dressed çš„ word shape ç‰¹å¾å°±æ˜¯ï¼š è¿™æ ·ä¸€æ¥ï¼Œç‰¹å¾å°±å¾ˆå¤šå¾ˆå¤šäº†ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œä¸€å®šçš„cutoff. å…¶ä¸­ $w_{i-l}^{i+l}$ è¡¨ç¤ºè€ƒè™‘å½“å‰è¯å‰å l ä¸ªå•è¯ï¼Œ $t_{i-k}^{i-1}$ è¡¨ç¤ºè€ƒè™‘å‰kä¸ªtags. Decoding and Training MEMMs è®­ç»ƒMEMMsåœ¨MEMMsä¸­ï¼Œæ¯ä¸€æ­¥éƒ½æ˜¯ä¸€ä¸ªlocal classiferï¼Œç„¶åmake a hard decision,é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯ã€‚ã€‚ã€‚ä»¥æ­¤ç±»æ¨ã€‚å› æ­¤ï¼Œè¿™æ˜¯è´ªå¿ƒgreedyç®—æ³•ï½ è™½ç„¶è¿™æ ·ä½¿ç”¨greedyçš„æ–¹æ³•ï¼Œå¾—åˆ°çš„å‡†ç¡®ç‡ä¹Ÿè¿˜ä¸é”™ï½ Viterbiç®—æ³•åŸå§‹Viterbiï¼š $$v_t(j) = max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t)$$ HMM tagging: $$v_t(j) = max_{i=1}^Nv_{t-1}P(s_j|s_i)P(o_t|s_j)$$ MEMM: $$v_t(j) = max_{i=1}^Nv_{t-1}P(s_j|s_i,o_t)$$ ä¹¦ä¸Šå¯¹å¦‚ä½•è®­ç»ƒå‚æ•°ä¸€ç¬”å¸¦è¿‡äº†ï¼Œæˆ‘è‡ªå·±æ€»ç»“å¦‚ä¸‹ï¼š åˆå§‹åŒ–è½¬ç§»çŸ©é˜µå’Œå‘å°„çŸ©é˜µå‚æ•°ï¼Œè®¾ä¸ºw æ ¹æ®Viterbiç®—æ³•å¡«å†™çŸ©é˜µ $N\\times T$, ä¹Ÿå°±æ˜¯æ¯ä¸€ä¸ªç½‘æ ¼ç”¨æ¦‚ç‡ $P(t_i|w_{i-1}^{i+l},t_{i-k}^{i-1})$, å¹¶ä¿ç•™ backpointers æ¯”è¾ƒtagè·¯å¾„ä¸çœŸå®è·¯å¾„ï¼Œç„¶ååå‘ä¼ æ’­ï¼Œæ›´æ–°å‚æ•°æƒé‡w, æ³¨æ„æœ‰æ­£åˆ™åŒ–L1,L2 è¦æ›´æ·±å…¥çš„ç†è§£MEMMï¼Œéœ€è¦å’ŒHMM taggingå¯¹æ¯”æ›´å®¹æ˜“ç†è§£ï½ï½ HMMæ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒæ˜¯åœ¨æ±‚å·²çŸ¥ $t_i$ æ¡ä»¶ä¸‹ç”Ÿæˆ $w_i$ çš„æ¦‚ç‡ $P(w_i|t_i)$,è¿™æ ¹æ®è¯­æ–™åº“ä¸­çš„é¢‘ç‡æ˜¯å¯ä»¥è®¡ç®—å¾—åˆ°çš„ï¼›ç„¶ååéªŒæ¦‚ç‡ $P(t_i|t_{i-1})$ ä¹Ÿæ˜¯å¯ä»¥é€šè¿‡é¢‘ç‡è®¡ç®—å¾—åˆ°çš„ï½ çŸ¥é“ä¼¼ç„¶æ¦‚ç‡å’Œå‘å°„æ¦‚ç‡åæ ¹æ®bayeså…¬å¼å°±å¯ä»¥é¢„æµ‹äº†P(T|W)ï½ MEMMæ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œå®ƒæ˜¯ç›´æ¥æ±‚ P(T|W),æ˜¾ç„¶æ˜¯tagç”Ÿæˆwordï¼Œè€Œä¸æ˜¯wordç”Ÿæˆtagï¼Œå› æ­¤æ— æ³•ç›´æ¥é€šè¿‡é¢‘ç‡ï¼Œä½†æˆ‘ä»¬å°†å®ƒåˆ†è§£ä¸ºæ¯ä¸€ä¸ªæ—¶é—´æ­¥è®¡ç®— $P(t_i|w_i,t_{i-1},â€¦)ï¼ˆå„ç§ç‰¹å¾ï¼‰ï¼Œæ¯ä¸€ä¸ªç‰¹å¾æœ‰å¯¹åº”çš„æƒé‡w,ç„¶åæ ¹æ®æœ€å¤§ç†µåŸç†ï¼Œä¹Ÿå°±æ˜¯å…¬å¼ï¼ˆ10.29ï¼‰æ‰€ç¤ºï¼Œä»å‰ä¸€ä¸ªtag $t_{i-1}$ æœ‰Nä¸­çŠ¶æ€ï¼Œåˆ°ä¸‹ä¸€ä¸ªtag $t_i$, æ˜¯maxçš„è¿‡ç¨‹ï¼Œä½†æ•´ä¸ªè¿‡ç¨‹ä¸æ˜¯greedyçš„ï¼ˆè¿™éœ€è¦å¥½å¥½ç†è§£ï¼Œå…¶å®ä¹Ÿå°±æ˜¯Viterbiä¸€æ ·çš„ã€‚ã€‚ï¼‰ï¼Œç„¶åæ ¹æ®backpointså¾—åˆ°çš„tagåºåˆ—ä¸çœŸå®tagåºåˆ—å¯¹æ¯”ï¼Œä¸æ–­æ›´æ–°æƒé‡å‚æ•°wï¼ Bidirectionalityè¿™é‡Œä¸¾äº†ä¸ªä¾‹å­æ¥é˜è¿°åŒå‘çš„é‡è¦æ€§ã€‚ will/NN to/TO fight/VB é€šå¸¸ to éƒ½æ˜¯æ¥åœ¨åè¯NNåé¢ï¼Œè€Œä¸ä¼šæ˜¯æƒ…æ€åŠ¨è¯ MD åé¢ï¼Œè¿™é‡Œçš„willä¹Ÿåº”è¯¥æ˜¯ NNã€‚ ä½†æ˜¯åœ¨ $P(t_{will}|&lt; s &gt;)$ï¼Œ $t_{will}$ æ›´å€¾å‘äºæ˜¯æƒ…æ€åŠ¨è¯ MDï¼Œè€Œä¸” $P(TO|t_{will},to)$ çš„æ¦‚ç‡æ¥è¿‘äº1,æ— è®º $t_{will}$ æ˜¯å•¥ï¼Œæ‰€ä»¥è¿™æ—¶å€™ $t_{will}$ å°±ä¼šé”™è¯¯çš„æ ‡æ³¨ä¸º MD. è¿™æ ·çš„é”™è¯¯å« label bias or observation bias, æ‰€ä»¥æˆ‘ä»¬éœ€è¦åŒå‘ï½ æ¡ä»¶éšæœºåœº Conditional Random Field or CRF å°±æ˜¯è¿™æ ·çš„ï½ ä»»ä½•sequence modeléƒ½å¯ä»¥æ˜¯åŒå‘çš„ã€‚æ¯”å¦‚ï¼Œå¯¹äºtaggingï¼Œå¯ä»¥åœ¨ç¬¬ä¸€é left to right, è€Œä»äºŒéå¼€å§‹å°±å¯ä»¥åŒå‘äº†ï½ SVMTool system å°±æ˜¯è¿™æ ·çš„ï¼Œä¸è¿‡å®ƒåœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ä½¿ç”¨çš„ä¸æ˜¯æœ€å¤§ç†µåˆ†ç±»å™¨ï¼Œè€Œæ˜¯SVMåˆ†ç±»å™¨ï¼Œç„¶åç”¨Viterbiç®—æ³•æˆ–è€…æ˜¯greedyåº”ç”¨åˆ°æ•´ä¸ªsequence modelï½ Part-of-Speech Tagging for Other Languagesä¸­æ–‡çš„åˆ†è¯å¯ä»¥å’Œæ ‡æ³¨ä¸€èµ·è¿›è¡Œï½ ä¸­æ–‡çš„æœªç™»å½•è¯é—®é¢˜ï½ æ€»ç»“ï¼š","link":"/2018/04/17/chapter10-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"title":"AI challenger å‚ä¼šè®°å½•","text":"ç­”è¾©å¬äº†è§‚ç‚¹å‹é˜…è¯»ç†è§£å’Œç»†ç²’åº¦æƒ…æ„Ÿåˆ†ç±»ä¸¤ä¸ªï¼Œç›¸å¯¹æ¥è¯´åè€…æ›´åŠ å¹²è´§æ»¡æ»¡ï¼Œå¤§ä½¬äº‘é›†çš„ï¼ŒåŸºæœ¬ä¸Šä»£è¡¨äº†å›½å†… NLP çš„å››åº§å¤§å±±ï¼Œæ¸…/åŒ—/ä¸­ç§‘é™¢/å“ˆå·¥å¤§ã€‚é€ æˆå‰è€…å¹²è´§è¾ƒå°‘çš„ä¸»è¦åŸå› ä½œä¸ºä¸»æŒäººçš„æœç‹—å¤§ä½¬ä¹Ÿè¯´äº†ï¼Œ BERT çš„æå‡ºåœ¨é˜…è¯»ç†è§£è¿™æ ·æ›´åŠ éœ€è¦ä¸Šä¸‹æ–‡ç†è§£ä»»åŠ¡çš„æå‡å®åœ¨å¤ªå¤šï¼Œä½¿å¾—é€‰æ‰‹çš„å…¶å®ƒå·¥ä½œéƒ½é»¯ç„¶å¤±è‰²ï¼Œå¯¼è‡´å¤§å®¶çš„æ¨¡å‹éƒ½è¶‹å‘åŒä¸€åŒ–ã€‚è€Œ BERT å¯¹äºåˆ†ç±»ä»»åŠ¡çš„æå‡å°±ç›¸å¯¹è¾ƒå°‘äº†ï¼Œæ‰€ä»¥ä¸‹åˆçš„ç­”è¾©æ˜¾å¾—æ›´åŠ ä¸°å¯Œï¼Œå„ç§æ“ä½œå’Œ trick. ä½†ä¹Ÿæœ‰é€‰æ‰‹è¯´ BERT ä½œä¸ºå•æ¨¡å‹å¯¹è¿™ä¸ªåˆ†ç±»ä»»åŠ¡ä¾ç„¶èƒ½å–å¾—å¾ˆä¸é”™çš„æ•ˆæœï¼Œæ‰€ä»¥ BERT æ˜¯çœŸå¼ºå•Š å› ä¸ºç­”è¾©çš„å±å¹•æ˜¯çœŸå°ï¼Œæ ¹æœ¬çœ‹ä¸æ¸…æ¥šã€‚ã€‚æ‰€ä»¥è®°å½•ä¼šå¾ˆé›¶æ•£ï¼Œä¹Ÿè®¸åªæ˜¯äº›å…³é”®è¯ï¼Œåç»­è¿˜éœ€è¦è‡ªè¡Œ google. è§‚ç‚¹å‹é˜…è¯»ç†è§£ å–å¾—å¥½æˆç»©çš„ä¸»è¦æ“ä½œï¼š é€šè¿‡ç®€å•çš„æ­£åˆ™åŒ¹é…å°†ä¸‰ä¸ªè§‚ç‚¹è½¬åŒ–ä¸ºä½œä¸º â€œæ­£/è´Ÿ/æ— æ³•ç¡®å®šâ€ çš„ä¸‰åˆ†ç±»é—®é¢˜ã€‚è®­ç»ƒé›†ä¸­ 95% çš„æ•°æ®å¯ä»¥å¾ˆå‡†ç¡®çš„è½¬åŒ–ä¸ºè¿™ç§å½¢å¼ï¼Œè¿˜æœ‰ 5% çš„æ˜¯å®ä½“ç±»é—®é¢˜ï¼Œæ¯”å¦‚ â€œéŸ©å›½/ç¾å›½/æ— æ³•ç¡®å®šâ€ï¼Œæœ‰é€‰æ‰‹çš„åšæ³•æ˜¯å°† query ä¸­å¯¹ä¸¤ä¸ªå®ä½“è¿›è¡Œæ’åºï¼Œæ¯”å¦‚éŸ©å›½åœ¨å‰ï¼Œç¾å›½åœ¨åã€‚åŒæ ·å¯¹åº”çš„ answer å°±æ˜¯ â€œéŸ©å›½/ç¾å›½/æ— æ³•ç¡®å®šâ€. å°†æ–‡æœ¬ç†è§£çš„é—®é¢˜ï¼Œè½¬æ¢ä¸ºåˆ†ç±»é—®é¢˜ä¹‹åï¼Œå¯¹æ•´ä¸ªæ¨¡å‹çš„å¤æ‚åº¦éœ€æ±‚å°±é™ä½å¤ªå¤šäº†ã€‚ä½†äº‹å®ä¸Šï¼Œè¿™æ˜¯æ•°æ® bug â€¦ æ¨¡å‹å…³é”®è¯ï¼š BERT multiway attention + R-Net RCZoo æµ™å¤§å¤§ä½¬çš„ï¼šå¤šå±‚ LSTM æ¨¡å‹ï¼Œæµ…å±‚+ä¸»è¦+æ·±å±‚ ä¸‰ä¸ª loss ä¼˜åŒ–ã€‚å…·ä½“å¿˜äº†æ‹ç…§ï¼Œä»¥åŠçœŸçš„çœ‹ä¸æ¸…æ¥šã€‚ã€‚ åŸºäº query çš„ attention è¿˜æ˜¯åŸºäº passage çš„ attention ä½œä¸ºæœ€ç»ˆçš„ answer selection/matching. è¯´å¥ä¸é©¬åç‚®çš„è¯ï¼Œè¿™é‡Œé¢å¤§éƒ¨åˆ†æˆ‘ä¹Ÿéƒ½æƒ³åˆ°äº†å•Šï¼Œåªæ˜¯åšä¸æ²¡åšï¼Œä»¥åŠç”¨ä¸æ²¡ç”¨ BERT ã€‚ã€‚ã€‚ ç»†ç²’åº¦ç”¨æˆ·è¯„è®ºæƒ…æ„Ÿåˆ†æ seq2seq é€‰æ‰‹è¿™ä¹ˆåšçš„åŸå› æ˜¯ ä»–è§‰å¾—å„ä¸ª ç²’åº¦ ä¹‹é—´å­˜åœ¨ä¸€å®šçš„å…³è”ï¼Œæ‰€ä»¥é‡‡ç”¨ decoder çš„å½¢å¼èƒ½æœ‰æ•ˆçš„åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚å¾ˆç¥å¥‡çš„æ“ä½œï¼Œæ˜¯å¦çœŸçš„æœ‰æ•ˆæœ±å°ç‡•è€å¸ˆæœ‰é—®åˆ°ï¼Œå¥½åƒä½œè€…å¹¶æ²¡æœ‰åšå¯¹ç…§å®éªŒã€‚ ELMo æå‡æœ€å¤š æ”¹è¿›çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶å®å°±æ˜¯ multi-head attention PRAUC æŸå¤±å‡½æ•°ï¼Œ è¿™ä¸ªæˆ‘å¥½åƒåœ¨å“ªå„¿è§è¿‡ï¼Œæˆ‘ä¸è®°å¾—äº† å¤§ä½¬æ„Ÿè§‰å¯ä»¥å‘ paper äº†ã€‚ã€‚ otherså…¶ä»–çš„ä¹Ÿå¾ˆå¼ºï¼Œä½†æ²¡æœ‰ seq2seq è¿™ä¹ˆå…·æœ‰ç‰¹æ®Šæ€§ï¼Œæ‰€ä»¥å¯ä»¥ä¸€èµ·è¯´ã€‚ è¯åµŒå…¥éƒ¨åˆ†å¾®è°ƒï¼Œæ²¡å¤ªæ‡‚ï¼Ÿ å“ªä¸€éƒ¨åˆ†å¾®è°ƒï¼Œä»¥åŠéç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä¿è¯å¾®è°ƒçš„ç¨‹åº¦ F1 æŒ‡æ ‡çš„ä¼˜åŒ–ï¼Œè¿™ä¸ªå¯¹äº unbalanced æ•°æ®çœ‹èµ·æ¥æ¯” è¿‡/æ¬  é‡‡æ ·æœ‰æ•ˆã€‚ä»¥åŠåˆ˜æ´‹è€å¸ˆæåˆ°çš„å¯ä»¥åŸºäº rainforce å¯¹ F1 è¿›è¡Œä¼˜åŒ– é™„ä¸Šåˆ˜æ´‹è€å¸ˆç…§ç‰‡ä¸€å¼ ï¼Œä¾§è„¸çœ‹èµ·æ¥çœŸåƒæå¥å•Šï¼Œéƒ½æ˜¯æ¸…åç”·ç¥å§ï½ ä¼ªæœ´ç´ è´å¶æ–¯ç‰¹å¾ï¼ŒPPT é‡Œé¢è¯´çš„å¾ˆæ¸…æ¥šï½æ¯æ¬¡è¾“å…¥å‡ ä¸ªæ ·æœ¬å…¶æå–çš„æ˜¯å±€éƒ¨ç‰¹å¾ï¼Œè€Œä¼ªæœ´ç´ è´å¶æ–¯ç‰¹å¾èƒ½ä½“ç°ä¸€ä¸ªè¯çš„å…¨å±€ç‰¹å¾ã€‚æ„Ÿè§‰å¾ˆæ£’å•Š æ•°æ®å¢å¼ºæ–¹å¼ï¼š drop words éšæœº mask shuffle words æ‰“ä¹±è¯åº ç»„åˆå¢å¼ºç­–ç•¥ å¯¹æŠ—è®­ç»ƒ æ¨¡å‹é›†æˆï¼š è´ªå©ªå¼æ¨¡å‹é€‰æ‹© ç®€å•æ¦‚ç‡å¹³å‡ï¼Œæœ€åé‡‡å–äº†è¿™ç§ã€‚ã€‚ã€‚anyway æ ¹æ®éªŒè¯é›†è°ƒæ•´åˆ†ç±»é˜ˆå€¼ï¼Œå¯¹å½“å‰çš„éªŒè¯é›†å½“ç„¶ä¼šæœ‰è¾ƒå¤§æå‡ã€‚ä½†æ˜¯å¯¹äº æµ‹è¯•é›† å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆï¼Œå¼•å…¥æ­£åˆ™åŒ–å’Œ Ensamble ç­–ç•¥ã€‚ $$b_i^j=\\text{argmax}_b[\\text{marco-}F_1(S^j[:,i]+b)-C|b]$$ ç¬¬ j ä¸ªæƒ…æ„Ÿè¦ç´ ç¬¬ i ç±»åˆ«ä¸Šçš„åç½®ï¼Œ C&gt;0 ä¸ºæ­£åˆ™ç³»æ•°ã€‚ è¿˜æœ‰äº›å…³é”®è¯ï¼Œæœ‰äº›æ¥ä¸åŠæ‹ç…§ã€‚ã€‚ BiSRU æœªå®Œå¾…ç»­ã€‚ã€‚","link":"/2018/12/19/AI-challenger-%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/"},{"title":"UCL-DRL-01-introduction","text":"About reinforcement learningå¼ºåŒ–å­¦ä¹ çš„ç‰¹æ€§ï¼š ä¸åŒäºæœ‰ç›‘ç£å­¦ä¹ ï¼Œæ²¡æœ‰ supervisor, åªæœ‰ä¸€ä¸ª reward signal. The reinforcement learning problemreward å¼ºåŒ–å­¦ä¹ å°±æ˜¯åŸºäº å¥–åŠ±å‡è®¾(reward hypothesis). å†³ç­–çš„é€‰æ‹©æ˜¯ä¸ºäº†è·å¾—æ›´å¤§çš„ future reward. environments statehistory and state state æ˜¯ history çš„å‡½æ•°ï¼š $$S_t = f(H_t)$$ environment state ç¯å¢ƒçŠ¶æ€å¯¹äº agent å¯ä»¥æ˜¯å¯è§çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸å¯è§çš„ã€‚å³ä½¿æ˜¯å¯è§çš„ï¼Œä¹Ÿä¸ä¸€å®šæ˜¯æœ‰ç”¨çš„ä¿¡æ¯ã€‚ Agent state ä»£ç†çŠ¶æ€å°±æ˜¯ agent çš„ä¸­é—´è¡¨ç¤ºã€‚ information state åˆç§° Markov state. çœ‹åšæ˜¯é©¬å°”å¯å¤«é“¾ï¼Œå½“å‰çŠ¶æ€åŒ…å«äº†è¿‡å»æ‰€æœ‰çš„ä¿¡æ¯ã€‚é‚£ä¹ˆä¹‹å‰çš„ history å°±å¯ä»¥æ‰”æ‰äº†ã€‚ Markov decision process é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¡ä»¶ï¼šç¯å¢ƒ state ä¸ agent state ä¸€æ · Agent state = environment state = information state partially observable environments(POMDP) agent å¹¶ä¸èƒ½ç›´æ¥è§‚å¯Ÿç¯å¢ƒï¼š æœºå™¨äººå…·æœ‰æ‘„åƒå¤´çš„è§†è§‰ä¿¡æ¯ï¼Œä½†ä¸çŸ¥é“è‡ªå·±çš„ç»å¯¹ä½ç½® trading agent åªèƒ½è§‚å¯Ÿåˆ°å½“å‰ä»·æ ¼ æ‰‘å…‹ç‰Œé€‰æ‰‹åªèƒ½çœ‹åˆ°å…¬å¼€çš„å¡ç‰Œ agent state $\\ne$ environment state agent å¿…é¡»é‡æ–°æ„å»ºè‡ªå·±çš„çŠ¶æ€è¡¨ç¤º $S_t^a$: æ¯”å¦‚å¾ªç¯ç¥ç»ç½‘ç»œå°±æ˜¯ POMDP $$S_t^a=tanh(s_{t-1}^aW_s+O_tW_o)$$ Inside An RL Agentåœ¨ä¸€ä¸ª agent å†…éƒ¨å…·ä½“æœ‰ä»€ä¹ˆå‘¢?æˆ‘ä»¬æ€ä¹ˆå»å®šä¹‰ä¸€ä¸ª agent: policy: agentâ€™s behaviour function value function: how good is each state and/or action model: agentâ€™s representation of the environment policy ç­–ç•¥ policyï¼š å°±æ˜¯å°† state æ˜ å°„åˆ° action. value function å¦‚ä½•è®¾è®¡ value function æ¥è®¡ç®— future reward æ„Ÿè§‰æ˜¯ä¸ªéš¾åº¦å‘€ï½ model æ¨¡å‹ï¼šç”¨æ¥é¢„æµ‹ç¯å¢ƒå¦‚ä½•å˜åŒ–ï¼Œä¹Ÿå°±æ˜¯æ¨¡æ‹Ÿç¯å¢ƒå§ã€‚æ¯”å¦‚ RNN æ¨¡å‹ï¼Œå°±æ˜¯ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿåºåˆ—å˜åŒ–ã€‚ Problems with Reinforcement LearningLearning and planning å­¦ä¹ å’Œè§„åˆ’çš„åŒºåˆ« environment æ˜¯å¦ known agent ä¸ environment æ˜¯å¦æœ‰ interaction Exploration and Exploitation æ¢ç´¢ä¸å¼€å‘ï¼šä¹‹å‰åœ¨ä¸‰æ˜Ÿå¬åœ¨çº¿å­¦ä¹ çš„è®²åº§æ—¶ï¼Œé€šè¿‡ å¤šè‡‚è€è™æœº å’Œ åœ¨çº¿å¹¿å‘Š è®¨è®ºè¿‡è¿™ä¸ªé—®é¢˜ Explorationï¼ˆæ¢ç´¢ï¼‰ finds more information about the environment Exploitationï¼ˆå¼€å‘ï¼‰ exploits known information to maximise reward It is usually important to explore as well as exploit è¿™æ˜¯ä¸€ä¸ªéœ€è¦æƒè¡¡æˆ–åšå¼ˆçš„é—®é¢˜ã€‚ prediction and control","link":"/2019/01/14/UCL-DRL-01-introduction/"},{"title":"UCL-DRL-02-MDP","text":"Markov ProcessIntroduction to MDPs é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼š ç¯å¢ƒå®Œå…¨å¯è§‚æµ‹ å½“å‰çŠ¶æ€èƒ½å®Œæ•´çš„æè¿°è¿™ä¸ªè¿‡ç¨‹ ç»å¤§å¤šæ•° RL é—®é¢˜éƒ½å¯ä»¥å½¢å¼åŒ–ä¸º MDPs: ä¼˜åŒ–æ§åˆ¶é—®é¢˜ éƒ¨åˆ†å¯è§‚æµ‹é—®é¢˜å¯è½¬åŒ–ä¸º MDPs Banditsï¼ˆè€è™æœºé—®é¢˜ï¼‰ multi-armed bandits problems Markov property ä¸€æ—¦å½“å‰çŠ¶æ€ç¡®å®šåï¼Œæ‰€æœ‰çš„å†å²ä¿¡æ¯éƒ½å¯ä»¥æ‰”æ‰äº†ã€‚è¿™ä¸ªçŠ¶æ€è¶³å¤Ÿå»é¢„æµ‹ future. state transition matrix çŠ¶æ€è½¬ç§»çŸ©é˜µå®šä¹‰äº†æ‰€æœ‰çš„ä»æŸä¸€ä¸ªçŠ¶æ€åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ã€‚æ‰€ä»¥æ¯ä¸€è¡Œç›¸åŠ ä¸º 1. Markov chain é©¬å°”å¯å¤«é“¾å¯ä»¥çœ‹åšæ˜¯ ä¸€ä¸ª tuple(S, P) S æ˜¯æœ‰ç©·çš„çŠ¶æ€çš„é›†åˆ P æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µ $$P_{ssâ€™}=P[S_{t+1}=sâ€™|S_t=s]$$ example: ç®€å•çš„é©¬å°”å¯å¤«é“¾ï¼Œæ²¡æœ‰ reward, action ç­‰ã€‚ Markov reward process åœ¨ Markov chain åŸºç¡€ä¸Šå¢åŠ äº† reward function. return æŠ˜æ‰£å› å­ $\\gamma \\in [0,1]$ è¶Šæ¥è¿‘0ï¼Œ æ„å‘³ç€è¶ŠçŸ­è§† è¶Šæ¥è¿‘1, æ„å‘³ç€æœ‰è¿œè§ Why discount æ•°å€¼è®¡ç®—æ–¹ä¾¿ é¿å…æ— é™å¾ªç¯ å¯¹æœªæ¥çš„ä¸ç¡®å®šæ€§ å¦‚æœrewardæ˜¯ç»æµçš„ï¼Œå³åˆ»çš„rewardèƒ½è·å¾—æ›´å¤šçš„åˆ©ç›Šç›¸æ¯”ä¹‹åçš„reward åŠ¨ç‰©/äººæ›´å€¾å‘äºå³åˆ»çš„å¥–åŠ± æœ‰æ—¶å€™ä¹Ÿä¼šä½¿ç”¨ undiscounted é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ value functionå› ä¸º future æœ‰å¾ˆå¤šä¸­ä¸ç¡®å®šæ€§ï¼Œæ‰€ä»¥éœ€è¦é‡‡æ ·. sample å¾—åˆ°çš„ $G_t$ çš„æœŸæœ›ï¼Œä¹Ÿå°±æ˜¯ value $$v(s)=E[G_t|S_t=s]$$ ä¸Šå›¾ä¸­ $v_1$ åº”è¯¥æ˜¯ $g_1$. $$G_1=R_2+\\gamma R_3+â€¦+\\gamma^{T-2}R_T$$ ä¹Ÿå°±æ˜¯è®¡ç®—ä» t æ—¶åˆ»å¼€å§‹åˆ°ç»“æŸï¼Œæ•´ä¸ªè¿‡ç¨‹å¯èƒ½çš„å¥–åŠ±å€¼ã€‚å› ä¸ºæœªæ¥å¯èƒ½æœ‰å¾ˆå¤šä¸­æƒ…å†µï¼Œæ‰€ä»¥éœ€è¦ sample. å°è±¡ä¸­ï¼Œå¥½åƒ alpha go å°±æ˜¯ç”¨çš„è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿã€‚ Bellman Equation for MRPs åŠ¨æ€è§„åˆ’ã€‚ã€‚ è¿™é‡Œå®¹æ˜“ç†è§£é”™çš„ä¸€ç‚¹å°±æ˜¯ï¼š $R_s$ å®é™…ä¸Šå°±æ˜¯ $R_{t+1}$, s è¡¨ç¤º v(s) ä¸­çš„å½“å‰çŠ¶æ€ã€‚ $P_{ssâ€™}$ æ˜¯çŠ¶æ€è½¬ç§»çš„æ¦‚ç‡ï¼Œ ä» s åˆ° åç»§çŠ¶æ€ sâ€™ çš„æ¦‚ç‡ã€‚ æ‰€ä»¥ï¼š $$\\gamma v(S_{t+1}|S_t=s) = \\gamma\\sum_{sâ€™\\in S} P_{ssâ€™}v(sâ€™)$$ è½¬æ¢æˆçŸ©é˜µå½¢å¼ï¼š äº‹å®ä¸Š: ç³»æ•°çŸ©é˜µ P å°±æ˜¯ çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œæœ‰ n ä¸­çŠ¶æ€ï¼Œé‚£å°±æ˜¯ $n\\times n$ matrix. solving bellman problems ç›´æ¥è®¡ç®—æ˜¯ $O(n^3)$, å› ä¸º n ä¸ªçŠ¶æ€ï¼ŒP çš„è®¡ç®—å°±æ˜¯ $O(n^2)$ å¯¹äºå¾ˆå¤æ‚çš„ MRPsï¼Œå¯ä»¥é‡‡ç”¨ï¼š dynamic programming monte-carlo evalution temporal-difference learning Markov Decision Process æƒ³å¯¹äº Markov reward process å¢åŠ äº† decision. $P_{ssâ€™}^a$ æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œä½†æ˜¯è¿™é‡Œçš„çŠ¶æ€ä¹‹é—´çš„è½¬ç§»æ¦‚ç‡ä¸å†æ˜¯ç¡®å®šå¥½äº†çš„ã€‚è€Œæ˜¯å–å†³äº å½“å‰çŠ¶æ€ï¼Œä»¥åŠå½“å‰çŠ¶æ€ä¸‹çš„ action. $$P_{ssâ€™}^a=P[S_{t+1=sâ€™}|S_t=s, A_t=a]$$ å› æ­¤ R reward functionï¼š $$R_s^a=E[R_{t+1}|S_t=s,A_t=a]$$ å¯ä»¥å‘ç°ï¼Œä¸ä¹‹å‰çš„åŒºåˆ«æ˜¯ï¼Œstate åˆ° state ä¹‹é—´æ˜¯é€šè¿‡ action å†³å®šçš„ã€‚æ¯”å¦‚ ç¬¬ä¸€ä¸ª state ä¸‹ï¼Œå¯ä»¥æ˜¯ study or facebook. policy è¿™ä¸ªå†³ç­–å°±æ˜¯æƒ³å¯¹äº Markov reward process å¤šå‡ºæ¥çš„ä¸€éƒ¨åˆ†ï¼Œä½ éœ€è¦è‡ªå·±å»åšå†³ç­–ã€‚åªä¸è¿‡åœ¨æ¯ä¸€ä¸ªçŠ¶æ€ä¸‹ï¼Œå…¶å†³ç­–ä¹Ÿæ˜¯ä¸€ä¸ª distribution $$\\pi(a|s)=P[A_t=a|S_t=s]$$ äº‹å®ä¸Š Markov decision process ä¹Ÿå¯ä»¥è½¬æ¢æˆ Markov reward process, ä»çŠ¶æ€ s åˆ° sâ€™ çš„æ¦‚ç‡ï¼š $$P_{s,sâ€™}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)P_{ssâ€™}^a$$ value function value æ˜¯å½“å‰çŠ¶æ€ s ä¸‹çš„ reward $G_t$ çš„æœŸæœ›ã€‚ Bellman expectation Equation$v_{\\pi}$ åŸºäº $q_{\\pi}$ çš„è¡¨ç¤º state-value $v_{\\pi}$ æ˜¯ action-value $q_{\\pi}$ åŸºäº action çš„æœŸæœ›ã€‚ $$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)q_{\\pi}(s, a)\\quad \\text{(1)}$$ $q_{\\pi}$ åŸºäº $v_{\\pi}$ çš„è¡¨ç¤º action-valueï¼š $$q_{\\pi}(s,a)=R_s^a+\\gamma\\sum_{sâ€™\\in S}P_{ssâ€™}^av_{\\pi}(sâ€™)\\quad \\text{(2)}$$ ä»çŠ¶æ€ s åˆ° sâ€™ï¼Œ åŸºäº statetransition probability matrix $P_{ssâ€™}^a$. $v_{\\pi}$ åŸºäº $v_{\\pi}$ çš„è¡¨ç¤º å°† ï¼ˆ2ï¼‰å¸¦å…¥ï¼ˆ1ï¼‰å¯å¾—ï¼š $$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)(R_s^a+\\gamma\\sum_{sâ€™\\in S}P_{ssâ€™}^av_{\\pi}(sâ€™))\\quad \\text{(3)}$$ è¿™é‡Œå¾—åˆ°çš„æ˜¯ state-value çš„è¡¨è¾¾å¼ã€‚ç¬¬äºŒä¸ªç±»å’Œç¬¦å·æ˜¯ ä»çŠ¶æ€ s åˆ° sâ€™ æ‰€æœ‰å¯èƒ½çš„åç»§çŠ¶æ€ sâ€™ã€‚ç¬¬ä¸€ä¸ªç±»å’Œç¬¦å·æ˜¯ çŠ¶æ€ s ä¸‹æ‰€æœ‰å¯èƒ½çš„ action. è¿™é‡Œçš„ç±»å’Œéƒ½æ˜¯è¡¨ç¤ºæœŸæœ›ã€‚ $q_{\\pi}$ åŸºäº $q_{\\pi}$ çš„è¡¨ç¤º å°† ï¼ˆ1ï¼‰ å¸¦å…¥ ï¼ˆ2ï¼‰å¯å¾—ï¼š $$q_{\\pi}(s,a) = R_s^a+\\gamma\\sum_{sâ€™\\in S}P_{ssâ€™}^a\\sum_{a\\in A}\\pi(aâ€™|sâ€™)q_{\\pi}(sâ€™,aâ€™)$$ è¿™é‡Œå¾—åˆ°çš„æ˜¯ action-value çš„è¡¨è¾¾å¼ã€‚ç¬¬ä¸€ä¸ªç±»å’Œç¬¦å·æ˜¯åœ¨ action a ä¸‹ï¼Œæ‰€æœ‰å¯èƒ½çš„åç»§çŠ¶æ€ sâ€™ çš„ç±»å’Œã€‚iç¬¬äºŒä¸ªç±»å’Œç¬¦å·æ˜¯ åç»§çŠ¶æ€ sâ€™ ä¸‹æ‰€æœ‰çš„ action çš„ç±»å’Œã€‚ exampleï¼š Matrix form ç±»ä¼¼äºå‰é¢ Markov reward process çš„è¡¨ç¤ºï¼Œä½†æ˜¯è¿™é‡Œçš„çŠ¶æ€è½¬ç§»çŸ©é˜µå˜æˆäº† $P^{\\pi}$, reward å˜æˆäº† $R^{\\pi}$. ä¸ªäººç†è§£ï¼š Markov decision process ä¸ reward process çš„åŒºåˆ«åœ¨äºåŸæœ¬åœ¨ä¸€ä¸ª state s åˆ°ä¸‹ä¸€ä¸ª state sâ€™ çš„æ¦‚ç‡æ˜¯ç»™å®šäº†çš„ã€‚ä½†æ˜¯åœ¨ decision processï¼Œå¹¶ä¸å­˜åœ¨è¿™ä¸ªæ¦‚ç‡ï¼Œè€Œæ˜¯å–å†³äº actionï¼Œè€Œåœ¨å½“å‰ state s ä¸‹ï¼Œæ¯ä¸€ä¸ª action çš„æ¦‚ç‡å–å†³äº policy $\\pi$. optimal value function optimal policy Solving the Bellman Optimality Equation æœ€ä¼˜ policy å°±æ˜¯æ‰¾åˆ° $v_{* }(s)$ å’Œ $q_{* }(s,a)$ æ¨å¯¼è¿‡ç¨‹ä¸å‰é¢ç±»ä¼¼ï¼Œé€šè¿‡ bellman optimally equation å¾—åˆ°ï¼š $$v_*(s)=max_{a}q_*(s,a)$$ $$q_*(s,a)=R_s^a+\\gamma\\sum_{sâ€™\\in S}P_{ssâ€™}^av_*(sâ€™)$$ $$v_*(s)={max}{a}(R_s^a+\\gamma\\sum{sâ€™\\in S}P_{ssâ€™}^av_*(sâ€™))$$ $$q_*(s,a)=R_s^a+\\gamma\\sum_{sâ€™\\in S}P_{ssâ€™}^aq_*(sâ€™,aâ€™)$$ state-value function optimal action-value function optimal Extension to MDP","link":"/2019/01/14/UCL-DRL-02-MDP/"},{"title":"chapter12-å¥æ³•åˆ†æ","text":"ambiguous and disambiguation PCFGs å¦‚ä½•ä»è¯­æ–™åº“ä¸­å¾—åˆ°PCFGsï¼Œæå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œä¹Ÿå°±æ˜¯è®¡ç®—é¢‘ç‡ Chomsky èŒƒå¼ SKYï¼Œä¸€ç§bottom-upçš„dynamic programmingç®—æ³•ï½ å‰è¨€ï¼š ä¸ºä»€ä¹ˆè¦åšå¥æ³•åˆ†æå‘¢ï¼Ÿä¸»è¦æ˜¯é¿å…æ­§ä¹‰ã€‚åœ¨ grammar checkingï¼Œsemantic analysisï¼Œquestion answeringï¼Œ information extraction ä¸­éƒ½æœ‰å¾ˆé‡è¦çš„ä½œç”¨ã€‚ å¦‚ä½•ä»Treebanksä¸­æ‰¾åˆ°æŸä¸€ä¸ªsentenceçš„treeå‘¢ï¼Ÿè¿™ä¸ªç« èŠ‚ä¸­ä¼šä»‹ç»ä¸€ç§åŠ¨æ€è§„åˆ’çš„ç®—æ³• Cocke-Kasami-Younger(CKY). æ­§ä¹‰ Ambiguityåœ¨è¯æ€§æ ‡æ³¨ä¸­æˆ‘ä»¬é‡åˆ°äº† part-of-speech ambiguity and part-of-speech disambiguation, åœ¨å¥æ³•åˆ†æä¸­ä¹Ÿä¼šé‡åˆ°ç±»ä¼¼çš„é—®é¢˜ structure ambiguity. ä¸¾ä¸ªæ —å­ï¼š å…ˆæœ‰è¿™æ ·çš„ä¸€ä¸ªç®€å•çš„ Treebank $L_1$. å¯¹äºåŒæ ·çš„ä¸€å¥è¯ï¼Œå¯ä»¥ä»ä¸Šè¿°Treebankä¸­ç”Ÿæˆä¸¤ç§ parse tree. I shot an elephant in my pajamas. æ˜¾ç„¶å³è¾¹çš„parse treeæ‰æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œä½†å·¦è¾¹çš„ä¹Ÿæ˜¯æ»¡è¶³ $L_1$ çš„è¯­æ³•è§„åˆ™çš„ã€‚ ç»“æ„æ­§ä¹‰é€šå¸¸åˆ†ä¸ºä¸¤ç§ï¼šé™„ç€æ­§ä¹‰(attachment ambiguity),å¹¶åˆ—æ­§ä¹‰(coordination ambiguityã€‚ äº‹å®ä¸Šï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œä¼šæœ‰å¾ˆå¤šè¯­æ³•æ­£ç¡®ï¼Œä½†æ˜¯è¯­ä¹‰ä¸Šä¸åˆç†çš„parseã€‚å› æ­¤æˆ‘ä»¬éœ€è¦å¥æ³•æ¶ˆæ­§ syntactic disambiguation.ä¸€ä¸ªæœ‰æ•ˆçš„å¥æ³•æ¶ˆæ­§ç®—æ³•éœ€è¦å¾ˆå¤šè¯­ä¹‰semanticå’Œè¯­å¢ƒcontextualçŸ¥è¯†, ä½†å¹¸è¿çš„æ˜¯ï¼Œç»Ÿè®¡çš„æ–¹æ³•å°±å¯ä»¥å¾ˆå¥½çš„è§£å†³ï½ Probabilistic Context-Free Grammars (PCFGs)Speech and language Processing è¿™æœ¬ä¹¦ä¸­å…³äºPCFGå†…å®¹å®åœ¨æ˜¯æœ‰ç‚¹å¤šï¼Œè€Œä¸”çº¯è‹±æ–‡çš„éš¾åº¦çœ‹èµ·æ¥çœŸçš„æœ‰ç‚¹å¤§ã€‚ã€‚ã€‚äºæ˜¯æ‰¾åˆ°Columbia Universityï¼ŒMichael Collins çš„NLPè¯¾ç¨‹ï¼Œç›¸æ¯”ä¹‹ä¸‹å†…å®¹å°‘äº†å¾ˆå¤šï¼Œè€Œä¸”è€å¸ˆè®²çš„ä¹Ÿå¾ˆæ¸…æ¥šï½æ‰€ä»¥è¿™éƒ¨åˆ†ä»¥Michael Collinsçš„NLPè®²ä¹‰ä¸ºä¸»ã€‚ context-free grammarsä¸Šä¸‹æ–‡æ— å…³è¯­æ³•CFGå¯å®šä¹‰ä¸º4å…ƒç¥– $G=(N,\\Sigma, R, S)$ (left-most) Derivationsç»™å®šä¸€ä¸ªä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ï¼Œä¸€ä¸ªleft-most derivationæ˜¯ä¸€ä¸ªåºåˆ— $s_1â€¦s_n$,å…¶ä¸­ï¼š $s_1 = S$, the start symbol $s_n\\in \\Sigma^*$, $\\Sigma^*$ è¡¨ç¤ºä»»ä½•ä» $\\Sigma$ ä¸­å¾—åˆ°çš„wordsç»„æˆ $s_i$, è¡¨ç¤ºæ‰¾åˆ° $s_{i-1}$ ä¸­left-mostçš„éç»ˆæç¬¦å·Xï¼Œå¹¶ç”¨ $\\beta$ ä»£æ›¿å®ƒï¼Œå¹¶ä¸” $X\\rightarrow \\beta \\in R$ Probabilistic Context-Free Grammars (PCFGs)ç”¨ $T_G$ è¡¨ç¤ºå¯¹åº”ä¸ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•Gæ‰€æœ‰çš„left-most æ¨å¯¼çš„parse tree. å¦‚æœ $s\\in \\Sigma^* $, é‚£ä¹ˆ $T_G(s)$ è¡¨ç¤ºï¼š $$t:t\\in T_G, yield(t)=s$$ $T_G(s)$ è¡¨ç¤ºåœ¨è¯­æ³•Gä¸‹ï¼Œså¯¹åº”çš„æ‰€æœ‰parse treeçš„é›†åˆã€‚ å¦‚æœä¸€ä¸ªsentenceæ˜¯æœ‰æ­§ä¹‰ambiguousçš„,é‚£ä¹ˆ $|T_G(s)|\\ge 1$ å¦‚æœä¸€ä¸ªsentenceæ˜¯ç¬¦åˆè¯­æ³•Gçš„grammatical, é‚£ä¹ˆ $|T_G(s)|\\ge 0$ ç»™ä¸€ä¸ªå¯èƒ½çš„æ¨å¯¼ï¼Œä¹Ÿå°±æ˜¯parse treeä¸€ä¸ªæ¦‚ç‡,p(t),é‚£ä¹ˆå¯¹äºä»»æ„ $t\\in T_G$: $$p(t)\\ge 0$$ å¹¶ä¸”ï¼š $$\\sum_{t\\in T_G}p(t)=1$$ å’‹ä¸€çœ‹ï¼Œè¿™ä¼¼ä¹å¾ˆå¤æ‚ã€‚æ¯ä¸€ä¸ªparse treeå°±å¾ˆå¤æ‚ï¼Œç„¶åè¿™æ ·çš„parse treeå¯èƒ½æ˜¯infinite.é‚£ä¹ˆå¦‚ä½•å®šä¹‰æ¦‚ç‡åˆ†å¸ƒp(t)å‘¢ï¼Ÿ çŸ¥é“äº†p(t)ï¼Œæˆ‘ä»¬ä¹Ÿå°±çŸ¥é“äº†ä¸€ä¸ªç»™å®šçš„sentenceå¯¹åº”çš„æœ€æœ‰å¯èƒ½çš„parse tree.ä¹Ÿå°±æ˜¯ï¼š $$arg\\ max_{t\\in T_G(s)p(t)}$$ æœ‰äº†è¿™ä¸ªæˆ‘ä»¬å°±èƒ½å¾ˆå¥½çš„æ¶ˆé™¤è¯­è¨€ä¸­å­˜åœ¨çš„ambiguousäº†ï½ï½ï½ é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼š How do we define the function p(t)? How do we learn the parameters of our model of p(t) from training examples? For a given sentence s, how do we find the most likely tree, namely $$arg\\ max_{t\\in T_G(s)p(t)}?$$ definition of PCFGsä¸€ä¸ªPCFGåŒ…æ‹¬ï¼š ä¸Šä¸‹æ–‡æ— å…³è¯­æ³• $G=(N,\\Sigma,S,R)$ å‚æ•° $$q(\\alpha \\rightarrow \\beta)$$ é‚£ä¹ˆå¯¹äº $t\\in T_G$åŒ…å«rules $\\alpha_2\\rightarrow \\beta_1,\\alpha_2\\rightarrow \\beta_2,â€¦,\\alpha_n\\rightarrow \\beta_n$ï¼Œåˆ™æœ‰ï¼š $$p(t)=\\prod_{i=1}^nq(\\alpha_i\\rightarrow \\beta_i)$$ é‚£ä¹ˆå¯¹äºæ‰€æœ‰çš„éç»ˆæç¬¦å· $X\\in N$: $$\\sum_{\\alpha\\rightarrow \\beta \\in R:\\alpha=X}q(\\alpha\\rightarrow \\beta)=1$$ ä¹Ÿå°±æ˜¯ä»ä¸€ä¸ªéç»ˆæç¬¦å·å±•å¼€çš„æ¦‚ç‡ä¹‹å’Œä¸º1. å¯¹äºä¸€ä¸ªparse treeçš„æ¦‚ç‡ï¼Œä¸¾ä¸ªä¾‹å­ï¼š æ ¹æ®ç›´è§‰ï¼Œå¯ä»¥å°†parse treeçš„ç”Ÿæˆçœ‹åšéšæœºè¿‡ç¨‹ stochastical processï¼Œå…¶è¿‡ç¨‹å¦‚ä¸‹ï¼š Deriving a PCFG from a CorpusHaving defined PCFGs, the next question is the following: how do we derive a PCFG from a corpus? We will assume a set of training data, which is simply a set of parse trees $t_1,t_2,â€¦,t_m$. $yield(t_i)$ is the iâ€™th sentence in the corpus. Each parse tree t i is a sequence of context-free rules: we assume that every parse tree in our corpus has the same symbol, S, at its root. We can then define a PCFG (N, Î£, S, R, q) as follows: æœ‰ç‚¹ç–‘æƒ‘ï¼š ä¸€ä¸ªè¯­æ–™åº“å¯¹åº”ä¸€ä¸ªPCFG ä¸€ä¸ªè¯­æ–™åº“ä¸­æ‰€æœ‰çš„parse treeçš„start symbolæ˜¯ç›¸åŒçš„S $t_1,t_2,â€¦,t_m$ å…·ä½“æœ‰å¤šå°‘æˆ‘ä»¬å¹¶ä¸çŸ¥é“ï¼Œä¹Ÿä¸éœ€è¦çŸ¥é“å§ã€‚ã€‚ä½†æ˜¯æœ‰å¤šå°‘sentencesæ˜¯çŸ¥é“çš„ã€‚ æ¯”å¦‚figure5ä¸­æ˜¾ç¤ºçš„é‚£æ ·ï½ Chomsky Normal Form collins æ•™æˆä¸¾çš„ä¾‹å­ï¼š VP -&gt; VT NP PP 0.2 è½¬æ¢æˆ Chomsky Normal Form: VP -&gt; VT-NP 0.2 VP-NT -&gt; VT NP 1 æ€»ç»“ä¸‹è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ï¼š Parsing using the CKY Algorithmåœ¨å‰é¢æœ€å°ç¼–è¾‘è·ç¦»ï¼ŒViterbiï¼ŒFrowardç®—æ³•ä¸­ï¼Œéƒ½æ˜¯å…ˆå¡«å†™è¡¨æ ¼ï¼Œè¡¨æ ¼ä¸­åŒ…å«æœ‰æ‰€æœ‰çš„å­é—®é¢˜ã€‚è€Œåœ¨å¥æ³•åˆ†æä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œå­é—®é¢˜æ˜¯å±•ç¤ºæ‰€æœ‰æˆåˆ†çš„parse treeï¼Œä¹Ÿå¯ä»¥å±•ç¤ºåœ¨è¡¨æ ¼ä¸­ï¼Œä½†å®ƒæ˜¯ä¸‰ç»´çš„ï¼Œå› ä¸ºä¸ä»…ä»…åŒ…å«å¥å­é•¿åº¦è¿™ä¸€ç»´åº¦ï¼Œè¿˜æœ‰å¯¹åº”çš„çŸ­è¯­ç»“æ„. å¯¹äºä¸€ä¸ªsentence $s=x_1\\cdots x_n$,å…¶ä¸­ $x_i$ è¡¨ç¤ºå¥å­ä¸­ç¬¬iä¸ªè¯ã€‚ä¹Ÿå°±æ˜¯æ‰¾å‡ºæ¦‚ç‡æœ€å¤§çš„parse tree $$arg\\ max_{t\\in T_G}p(t)$$ CKYç®—æ³•æ˜¯ä¸€ä¸ªåŠ¨æ€è§„åˆ’ç®—æ³•ï½ å…·ä½“æ€è·¯ï¼š ç»™sentenceæ ‡ä¸Šindex fill the table $(n+1)\\times (n+1)$ï¼Œ æ¯ä¸ª cell å¯¹åº”ä¸€ä¸ªä¸‰ç»´çš„é‡ $\\pi[i,j,X]$,$X\\in N, 1\\le i\\le j\\le n$, å…¶ä¸­non-terminalæ€»ä¸ªæ•°æœ‰Vä¸ªï¼Œé‚£ä¹ˆ table çš„ç»´åº¦æ˜¯ $(n+1)\\times (n+1)\\times V$ sentenceå…¶ä¸­çš„ä¸€éƒ¨åˆ† $x_i\\cdots x_j$ï¼Œå¹¶ä¸”å¯¹åº”çš„rootä¸ºéç»ˆæç¬¦å·X. $$\\pi(i,j,X) = max_{t\\in T(i,j,X)}p(t)$$ é‚£ä¹ˆæˆ‘ä»¬è¦æ±‚çš„sentenceå¯¹åº”çš„parse treeï¼š $$\\pi(1,n,S) = arg\\ max_{t\\in T_G(s)}$$ ç”¨é€’å½’çš„æ–¹æ³•å¡«å†™table è¿™é‡Œæ˜¯ä¸€ç§ bottom-up,ä»ä¸‹åˆ°ä¸Šçš„æ–¹æ³•ï¼Œåªéœ€è¦çŸ©é˜µçš„ä¸Šä¸‰è§’éƒ¨åˆ†ï½ Initialization åˆå§‹å€¼ï¼Œ ä¹Ÿå°±æ˜¯ä¸»å¯¹è§’çº¿çš„å€¼ $$\\pi(i,i,X) =\\begin{cases} q(X\\rightarrow x_i), &amp; \\text{if $X \\rightarrow x_i \\in$ R} \\ 0, &amp; \\text{otherwise} \\end{cases} $$ é€’å½’ recursive $$\\pi(i,j,X)=max_{X\\rightarrow Y Z \\in R,s\\in {iâ€¦(j-1)}}(q(X\\rightarrow Y Z)\\times \\pi(i,s,Y)\\times \\pi(s+1,j,Z))\\tag{1}$$ ä¼ªä»£ç ï¼š å¡«è¡¨çš„è¿‡ç¨‹ï¼š The algorithm fills in the $\\pi$ values bottom-up: first the $\\pi(i, i, X)$ values, using the base case in the recursion; then the values for $\\pi(i, j, X)$ such that j = i + 1; then the values for $\\pi(i, j, X)$ such that j = i + 2; and so on. Justification for the Algorithm è€ƒè™‘ $\\pi(3,8,VP)$ åœ¨è¯­æ³•ä¸­ $VP\\rightarrow Y Z$ åªæœ‰ä¸¤ç§æƒ…å†µ $VP \\rightarrow Vt\\ NP\\ and\\ VP\\rightarrow VP\\ PP$ï¼Œé‚£ä¹ˆ $\\pi(3,3)$ è¿™ä¸ªcellæœ‰ä¸¤ä¸ªç»´åº¦ï¼ŒåŒæ ·çš„ $\\pi(3,4),â€¦,\\pi(8,8)$ éƒ½æœ‰ä¸¤ä¸ªç»´åº¦. æ€»çš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(n^3|N|^3)$,æ€ä¹ˆæ¥çš„å‘¢ï¼Ÿ tableä¸­é€‰æ‹©cell[i,j],å¤æ‚åº¦ $n^2$ å¯¹äºæ¯ä¸ªcell[i,j]è¦è€ƒè™‘ N ä¸ªnon-terminal,æ‰€ä»¥æ˜¯ $n^2N$ ç„¶å $X\\rightarrow Y Z$ Y Zéƒ½æœ‰ N ç§æƒ…å†µï¼Œæ‰€ä»¥ $n^2N^3$ ç„¶å $s\\in {iâ€¦(j-1)}$ æ‰€ä»¥æ˜¯ $O(n^3|N|^3)$ æ•´ä¸ªå›é¡¾ä¸‹ï¼Œå°±æ˜¯æˆ‘ä»¬å…ˆä»ä¸‹åˆ°ä¸Š bottom-up å¡«å†™ä¸‰ç»´table[i,j,V] ï¼Œç„¶åç›´æ¥è®¡ç®— $\\pi(1,n,S)$ï¼Œ åŒæ ·çš„ç±»ä¼¼äºHMMä¸­éœ€è¦backpoints. weaknesses of PCFGs Lack of sensitivity to lexical information Lack of sensitivity to structural frequencies Lack of sensitivity to lexical information åœ¨PCFGsä¸­ $NNP\\rightarrow IBM$ æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„è¿‡ç¨‹ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘åˆ°å…¶ä»–çš„wordsï¼Œè¿™åœ¨NLPä¸­æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚ é™„ç€æ­§ä¹‰(attachment ambiguityï¼‰: å¯ä»¥çœ‹åˆ°å¯¹äºåŒä¸€å¥è¯çš„ä¸¤ç§parse tree,å®ƒä»¬çš„åŒºåˆ«åªæœ‰å›¾ä¸­é»‘è‰²åŠ ç²—çš„ruleï¼Œé‚£ä¹ˆæ¯”è¾ƒä¸¤ä¸ªparse treeé‚£ä¸ªæ›´å¥½ä»…ä»…å–å†³äº $q(NP\\rightarrow NP PP) q(VP\\rightarrow VP PP)$ çš„å¤§å°ã€‚ è¾¹ä¸Šçš„ä¸¾ä¾‹ dumped sacks into bin æ²¡å¤ªæ‡‚ï¼Œå‡†ç¡®ç‡æ˜¯æ€ä¹ˆæ¥çš„ã€‚ã€‚ã€‚ Lack of sensitivity to structural frequencies close attachment: é€šè¿‡ä¸¤ä¸ªä¾‹å­ï¼Œè¯´æ˜PCFGsçš„åŠ£åŠ¿ï¼Œfailed to capture the structural preferences. ä¸¤ä¸ªparse treeçš„ruleså®Œå…¨ç›¸åŒï¼Œé‚£ä¹ˆå¯¹åº”çš„æ¦‚ç‡p(t)ä¹Ÿä¸€æ ·ã€‚PCFGs failed to display a preference for one parse tree or the other and in particular it completely ignores the lexical information. å‚è€ƒï¼š Natural Language Processingï¼ŒColumbia Universityï¼ŒMichael Collins youtobe,Natural Language Processing Speech and language Processing","link":"/2018/04/20/chapter12-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"title":"UCL-DRL-04-policy gradient and Actor Critic Methods","text":"policy gradientbasicDeriving Policy Gradients and Implementing REINFORCE è¿™ç¯‡åšå®¢è¯¦ç»†æ¨å¯¼äº† policy gradients çš„è¿‡ç¨‹ï¼Œè™½ç„¶å…¬å¼å¾ˆå¤šï¼Œä½†å…¶å®è¿˜ç®—ç®€å•ã€‚ æœ€ç»ˆçš„ç»“è®ºå°±æ˜¯ï¼Œä»å…¬å¼(1)æ¨å¯¼ä¸ºå…¬å¼(2): $$J(\\theta)=\\mathbb{E}[\\sum_{t=0}^{T-1}r_{t+1}|\\pi_{\\theta}]=\\sum_{t=1}^{T-1}P(s_t, a_t|\\tau)r_{t+1}\\quad(1)$$ $$\\nabla_{\\theta}J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_{\\theta}log\\pi_{\\theta}(a_t|s_t)(\\sum_{tâ€™=t+1}^T\\gamma^{tâ€™-t-1}\\gamma_{tâ€™})\\quad(2)$$ å…¶ä¸­ $Gt=\\sum_{tâ€™=t+1}^T\\gamma^{tâ€™-t-1}\\gamma_{tâ€™}$ å…¬å¼(2)å¯ç®€åŒ–ä¸º: $$\\nabla_{\\theta}J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_{\\theta}log\\pi_{\\theta}(a_t|s_t)G_t$$ ä¹‹åå°±æ˜¯é’ˆå¯¹ policy gradient çš„å„ç§æ”¹è¿›ã€‚æ¥ä¸‹æ¥çš„å…¬å¼å‚æ•°æ˜¯æ ¹æ®æå®æ¯…è€å¸ˆè¯¾ç¨‹æ¥çš„ã€‚æ‰€ä»¥ç¬¦å·ä¸ä¸Šé¢çš„æœ‰å·®å¼‚ã€‚ add baseline$$\\nabla{\\overline R}{\\theta}=\\dfrac{1}{N}\\sum{n=1}^N\\sum_{t=1}^{T_n}(R(\\tau^n)-b)\\nabla logp_{\\theta}(a_t^n|s_t^n)$$ assign suitable credit$$\\nabla{\\overline R}{\\theta}=\\dfrac{1}{N}\\sum{n=1}^N\\sum_{t=1}^{T_n}(\\sum_{tâ€™=t}^{T_n}r_{tâ€™}^n-b)\\nabla logp_{\\theta}(a_t^n|s_t^n)$$ add discount factor advantage functionaction $a_t$ çš„å¥½æ˜¯ç›¸å¯¹çš„ï¼Œè€Œä¸æ˜¯ç»å¯¹çš„ã€‚å®ƒå¯ä»¥æ˜¯ state-dependent. on-policy and off-policyimportance sampling issue of importance sampling: é‡‡ç”¨ importance samplingï¼Œä¼šå¯¼è‡´sample å¾—åˆ°çš„actionçš„åˆ†å¸ƒæ–¹å·®å‘ç”Ÿå˜åŒ–ã€‚å› æ­¤è¦ä¿è¯actionçš„åˆ†å¸ƒå°½å¯èƒ½ä¸ on-policy to off-policy PPO(proximal policy optimization)add constraint: $\\theta, \\thetaâ€™$ å¹¶ä¸æ˜¯distributionï¼Œè€Œæ˜¯å‚æ•°ã€‚é‚£ä¹ˆè¿™é‡Œçš„ $kl(\\theta, \\thetaâ€™)$ åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ è¿™é‡Œçš„kl divergence å®é™…ä¸Šæ˜¯è¡Œä¸ºä¸Šçš„å·®è·ï¼Œä¹Ÿå°±æ˜¯ action åˆ†å¸ƒçš„è·ç¦»ã€‚é‚£è¿™ä¸ªæ„æ€å°±æ˜¯æˆ‘ä»¬è¿˜æ˜¯å¾—ç”¨ $\\theta$ å»æ±‚å‡º action çš„ distributionï¼Œä½†æ˜¯æˆ‘ä»¬ä¸ç”¨å»sampleå‡ºæ ·æœ¬äº†ã€‚å¯ä»¥ç»§ç»­éœ€è¦ç”¨ $\\thetaâ€™$ sampleå‡ºæ¥çš„æ ·æœ¬ï¼Œä½†æ˜¯éœ€è¦ç»™rewardä¹˜ä»¥ç³»æ•° $\\dfrac{p_{\\theta}(a_t|s_t)}{p_{\\thetaâ€™}(a_t|s_t)}$. PPO2 å› ä¸º $kl(\\theta, \\thetaâ€™)$ çš„è®¡ç®—è¿˜æ˜¯è›®å¤æ‚çš„ï¼Œæ‰€ä»¥ç”¨ppo2æ¥ä»£æ›¿ã€‚æ–¹æ³•ä¹Ÿæ˜¯å¾ˆç›´æ¥ï¼Œç”¨clipçš„æ–¹å¼ä»£æ›¿åŸæ¥çš„æ­£åˆ™é¡¹ã€‚ æ€»ç»“ä¸€ä¸‹ppoï¼š é¦–å…ˆå®ƒæ˜¯off-policyçš„ï¼Œä¸ºä»€ä¹ˆå°†on-policyè½¬æ¢æˆoff-policyå‘¢ï¼Œå› ä¸ºon-policyé€Ÿåº¦å¤ªæ…¢äº†ã€‚åœ¨policy iterationçš„æ—¶å€™å…ˆsampleå°½å¯èƒ½å¤šçš„(state, action)pairsï¼Œç„¶åè®¡ç®—å¯¹åº”çš„rewardï¼Œå†åŸºäºpolicy gradientæ¥æ›´æ–°policyçš„å‚æ•° $\\theta$.è¿™æ˜¯ä¸€æ¬¡è¿­ä»£ã€‚å†ç„¶ååŸºäºupdated policyç”Ÿæˆæ–°çš„(state, action) pairsæˆ–è€…æ–°çš„exampleå§ï¼Œä¾æ¬¡è¿­ä»£ã€‚ã€‚ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¾—åˆ°actionçš„åˆ†å¸ƒï¼Œç„¶åsampleå¾—åˆ°å°½å¯èƒ½å¤šçš„actionsï¼Œè¿™ä¸€æ­¥æ˜¯éå¸¸è€—æ—¶çš„ï¼Œè€Œä¸”æ¯æ¬¡è¿­ä»£éƒ½éœ€è¦é‡æ–°ç”Ÿæˆæ ·æœ¬ã€‚ off-policyæ”¹è¿›çš„å°±æ˜¯æä¸€ä¸ªè¿‘ä¼¼äºå½“å‰policy $\\theta$ çš„ $\\thetaâ€™$. ç”¨è¿™ä¸ª $\\thetaâ€™$ å»é‡‡æ · $(state, action)$ æ ·æœ¬ï¼Œè¿™ä¸ª $\\thetaâ€™$ å¹¶ä¸æ˜¯éšç€ policy $\\theta$ çš„æ›´æ–°è€Œæ›´æ–°çš„ï¼Œæ‰€ä»¥å®ƒsampleå‡ºæ¥çš„æ ·æœ¬å¯ä»¥ç”¨å¾ˆä¹…ã€‚ ä½†æ˜¯policy $\\theta$ æ›´æ–°ä¹‹åï¼Œå…¶å¯¹åº”çš„ action çš„åˆ†å¸ƒä¹Ÿæ˜¯å˜æ¢çš„ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚æ‰€ä»¥æ€ä¹ˆå‡å° $\\thetaâ€™$ å’Œ $\\theta$ å¯¹åº”çš„actionåˆ†å¸ƒçš„å·®å¼‚ï¼Œå°±æœ‰äº†ppoå…¬å¼çš„ç¬¬ä¸€é¡¹ï¼Œä¹Ÿå°±æ˜¯ importance sampling. ä½†æ˜¯importance samplingå¾—åˆ°çš„actionçš„åˆ†å¸ƒå­˜åœ¨åå·®ï¼ˆæ–¹å·®ä¸ä¸€è‡´)ï¼Œæ‰€ä»¥éœ€è¦å°½å¯èƒ½ä¿è¯ $\\theta, \\thetaâ€™$ é‡‡æ ·å¾—åˆ°çš„actionåˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘ã€‚äºæ˜¯æœ‰äº†ppo,ppo2çš„æ–¹æ³•ã€‚ Criticstate-value function$V_{\\pi}(s)$ è¡¨ç¤ºçš„æ˜¯å½“åˆ°è¾¾æŸä¸ªçŠ¶æ€ s ä¹‹åï¼Œå¦‚æœæ¥ä¸‹æ¥ä¸€ç›´æŒ‰ç€ç­–ç•¥ $\\pi$ æ¥è¡ŒåŠ¨ï¼Œèƒ½å¤Ÿè·å¾—çš„æœŸæœ›æ”¶ç›Š. action value function$Q_{\\pi}(s,a)$ è¡¨ç¤ºçš„æ˜¯å½“è¾¾åˆ°æŸä¸ªçŠ¶æ€ s ä¹‹åï¼Œå¦‚æœå¼ºåˆ¶é‡‡å–è¡ŒåŠ¨ a ï¼Œæ¥ä¸‹æ¥å†æŒ‰ç…§ç­–ç•¥ $\\pi$ æ¥è¡ŒåŠ¨ï¼Œèƒ½è·å¾—çš„æœŸæœ›æ”¶ç›Šã€‚ æ˜¾ç„¶åœ°ï¼ŒçŠ¶æ€ä»·å€¼å‡½æ•°å’Œè¡ŒåŠ¨ä»·å€¼å‡½æ•°ä¹‹é—´æœ‰å…³ç³»: $$V_{\\pi}(s)=\\sum_a\\pi(a|s)Q_{\\pi}(s,a)$$ MC v.s. TD sampleåŒæ ·çš„ç»“æœï¼Œé‡‡ç”¨ MC å’Œ TDæœ€ç»ˆè®¡ç®—çš„ç»“æœä¹Ÿæ˜¯ä¸ä¸€æ ·çš„ã€‚ MCè€ƒè™‘çš„æ˜¯ï¼Œå½“å‰state $s_a$ å¯¹æœªæ¥çš„ state $s_b$ å¯èƒ½ä¹Ÿæ˜¯æœ‰å½±å“çš„. æ‰€ä»¥MCå®é™…ä¸Šæ˜¯è¦è®¡ç®—åˆ°æ•´ä¸ªæ¸¸æˆï¼ˆepisodeï¼‰ç»“æŸã€‚","link":"/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/"},{"title":"chapter13 Lexicalized PCFG","text":"Lexicalized PCFGs Lexicalization of a treebankï¼š ç»™treebankæ·»åŠ head Lexicalized probabilistic context-free Grammarsï¼š Lexicalized PCFGsä¸­çš„å‚æ•°ï¼Œä¹Ÿå°±æ˜¯rulesçš„æ¦‚ç‡ Parsing with lexicalized PCFGsï¼šä½¿ç”¨åŠ¨æ€è§„åˆ’CKYå¯¹æµ‹è¯•é›†ä¸­çš„sentenceså¯»æ‰¾æœ€ä¼˜parse tree Parameter estimation in lexicalized probabilistic context-free grammarsï¼š é€šè¿‡è®­ç»ƒé›†ï¼Œä¹Ÿå°±æ˜¯è¯­æ–™åº“corpuså¾—åˆ°Lexicalized PCFGçš„å‚æ•° Accuracy of lexicalized PCFGsï¼šæµ‹è¯•é›†çš„å‡†ç¡®ç‡è®¡ç®— ç›¸å…³ç ”ç©¶ Lexicalized PCFGs è¯æ±‡åŒ–PCFGsLexicalization of a treebank ç»™æ¯ä¸ªrulesæ·»åŠ ä¸€ä¸ªheadï¼Œå‰é¢ä¹Ÿä»‹ç»è¿‡ï½ä¸è¿‡ä¹ˆçœ‹æ‡‚ï¼Œè¿™é‡Œåˆä¸€æ¬¡è®²åˆ°äº†ã€‚ å…³äºæ€ä¹ˆæ‰¾åˆ°è¿™ä¸ªheadï¼Œä¸€ä¸ªruleä¸­æœ€ä¸­å¿ƒçš„éƒ¨åˆ†ï½a core idea in syntax the central sub-constituent of each rule the sementic predicate in each rule åœ¨è¯­æ–™åº“ä¸­æ˜¯æ²¡æœ‰æ ‡æ³¨å‡ºheadçš„ï¼Œé‚£ä¹ˆéœ€è¦ä¸€äº›è§„åˆ™æ¥äººä¸ºé€‰å®šheadã€‚ ä»¥NPä¸ºä¾‹ï¼Œright-most ä»¥VPä¸ºä¾‹ï¼Œ left-most æ·»åŠ headä¹‹åæœ‰å•¥ç”¨å‘¢ï¼Ÿ make the PCFG more sensitive to lexical information. propagate lexical items bottom-up throught these threes using head annotations where each non-terminal receives its head from its head child. Lexicalized probabilistic context-free Grammars Chomsky Normal Form çªç„¶å‘ç°è‡ªå·±ä¸çœ‹å­—æ¯ä¹Ÿèƒ½å¬æ‡‚ Michael Collins çš„è‹±è¯­äº†ï½å‘éŸ³çœŸçš„å¥½å¬è€Œä¸”æ¸…æ™°ï¼ï¼è¿˜æœ‰è‡ªå·±æˆªå›¾å³ä¸‹è§’éƒ½æœ‰ä¸ªé¢„è§ˆçš„æ¡†æ¡†ã€‚ã€‚å› ä¸ºé¼ æ ‡æˆªå›¾æ—¶ä¸å¾—ä¸å›ç¢°åˆ°youtobeé¢„è§ˆçš„è¿›åº¦æ¡ã€‚ã€‚ã€‚ Lexicalized context-free grammars in chomsky normal form è·Ÿregular CFGå¾ˆç›¸ä¼¼ï¼Œä½†å¤šäº†head words Parameters in a Lexicalized PCFG åœ¨PCFGä¸­ï¼Œæ˜¯ $S\\rightarrow$ åœ¨ Lexicalized PCFGä¸­ï¼Œ æ˜¯ $S(saw)\\rightarrow$ å› æ­¤å‚æ•°å¤šäº†å¾ˆå¤šå¾ˆå¤šï¼Œæ¯”å¦‚ $S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$, $S(saw)\\rightarrow_2 NP(women)\\ VP(saw)$ è¿™éƒ½æœ‰å¯¹åº”çš„æ¦‚ç‡ï½we heve every possible lexicalized rule, åŒæ—¶smoothingæŠ€æœ¯åœ¨è¿™ä¼šç›´æ¥ç”¨åˆ°ï½ Parsing with lexicalized PCFGs å¯¹äºä¸€ä¸ªrule grammarï¼Œå…¶non-terminalå±•å¼€æ˜¯ $|\\Sigma|^2\\times |N|^3$ å¯¹äºä¸€ä¸ªé•¿åº¦ä¸ºnçš„sentenceï¼Œä½¿ç”¨dynamic programmingç®—æ³•éœ€è¦çš„è®¡ç®—å¤æ‚åº¦ä¸º $O(n^3|\\Sigma|^2|N|^3)$,ä½†æ˜¯ è¯å…¸ $|\\Sigma|$ å¯èƒ½ä¼šå·¨å¤§ï¼ï¼ï¼ ä½†å¯ä»¥è¿™ä¹ˆç®€åŒ–ï¼Œå°±æ˜¯ä¸ç”¨è€ƒè™‘æ•´ä¸ªè¯å…¸ $|\\Sigma|$, åªéœ€è¦è€ƒè™‘åœ¨sentenceå‡ºç°è¿‡çš„è¯ï¼Œä¹Ÿå°±æ˜¯ n ä¸ªã€‚ é‚£ä¹ˆæ€»çš„è®¡ç®—å¤æ‚åº¦ä¸º $O(n^5|N|^3)$. æ£’å•Šï¼ï½ Parameter estimation in lexicalized probabilistic context-free grammars ä¸ºä»€ä¹ˆ Lexicalized PCFGs è¦å¥½äº PCFGï¼Ÿ ä½¿ç”¨prepositional phrase ambiguityä¸¾ä¾‹è¯´æ˜: rules: 7ä¸ªnon-terminal: $$S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$$ $$NP(man)\\rightarrow_2 DT(the)\\ NN(man)$$ $$VP(saw)\\rightarrow_1 VP(saw)\\ PP(with)\\tag{~}$$ $$VP(saw)\\rightarrow_1 Vt(saw)\\ NP(dog)\\tag{~}$$ $$NP(dog)\\rightarrow_2 DT(the)\\ NN(dog)$$ $$PP(with)\\rightarrow_1 IN(with)\\ NP(telescope)$$ $$NP(telescope)\\rightarrow_2 DT(with)\\ NN(telescope)$$ 9ä¸ªterminalï¼š $$\\cdots$$ 7ä¸ªnon-terminal: $$S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$$ $$NP(man)\\rightarrow_2 DT(the)\\ NN(man)$$ $$VP(saw)\\rightarrow_1 VP(saw)\\ PP(dog)\\tag{~}$$ $$NP(dog)\\rightarrow_1 NP(dog)\\ PP(with)\\tag{~}$$ $$NP(dog)\\rightarrow_2 DT(the)\\ NN(dog)$$ $$PP(with)\\rightarrow_1 IN(with)\\ NP(telescope)$$ $$NP(telescope)\\rightarrow_2 DT(with)\\ NN(telescope)$$ 9ä¸ªterminalï¼š $$\\cdots$$ ä¸¤è€…çš„åŒºåˆ«åœ¨äºï¼ˆï½ï¼‰çš„æ¦‚ç‡å¤§å°ã€‚å¦‚æœæ²¡æœ‰lexicalized,parse treeçš„é€‰æ‹©å°±ä»…ä»…åªå–å†³äºçŸ­è¯­ç»“æ„ï¼Œè€Œå®Œå…¨å¿½è§†äº†è¯­ä¹‰çš„ä¿¡æ¯ã€‚ å¦‚ä½•è®¡ç®—å‚æ•° $q(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))$ æ˜¯æ¡ä»¶æ¦‚ç‡ï¼Œå¯ä»¥çœ‹åšæ˜¯å·²çŸ¥S,saw,ä»CFGè¯­æ³•ä¸­é€‰å‡º $S\\rightarrow_2 NP\\ VP$, å¹¶ä¸”ä»NPçš„ word ä¸­é€‰å‡º man çš„æ¦‚ç‡. $$q(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))=q(S\\rightarrow_2 NP\\ VP|S,saw) \\times q(man|S\\rightarrow_2NP\\ VP,saw)$$ ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡å¯å¾—ï¼š ç¬¬ä¸€é¡¹ï¼š $q(S\\rightarrow_2 NP\\ VP|S,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP\\ VP)}{count(S(saw))}$ ç¬¬äºŒé¡¹ï¼š $q(man|S\\rightarrow_2NP\\ VP,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))}{count(S(saw)\\rightarrow_2 NP\\ VP,saw)}$ ç”¨çº¿æ€§æ’å€¼æ³•ä¼°è®¡å‚æ•°: $$\\lambda_1+\\lambda_2=1$$ $$q_{ML}(S\\rightarrow_2NP\\ VP|S,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP\\ VP)}{count(S(saw))}$$ $$q_{ML}(S\\rightarrow_2NP\\ VP|S) = \\dfrac{count(S\\rightarrow_2 NP\\ VP)}{count(S)}$$ ä¾ç„¶è¿˜æœ‰å¾ˆå¤šéœ€è¦è§£å†³çš„é—®é¢˜ï¼š deal with rules with more child incorporate parts of speech(useful in smoothing) encode preferences for close attachment further reading: MIchael Collins.2003. Head-Driven Statistical Models for Natural Language Parsing. Accuracy of lexicalized PCFGsgold standard and parsing output: Resultä»‹ç»äº†ä¸€äº›ç›¸å…³çš„ç ”ç©¶ï½ reference: MIchael Collins, Natural Language Processing","link":"/2018/04/22/chapter13-Lexicalized-PCFG/"},{"title":"chapter11-ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•CFG","text":"ä¸Šä¸‹æ–‡æ— å…³è¯­æ³• CFGï¼Œåˆå«çŸ­è¯­ç»“æ„è¯­æ³• è‹±è¯­è¯­æ³•ä¸­çš„å„ç§è§„åˆ™ï¼š å¥å­çº§çš„ç»“æ„ï¼Œåˆ†4ç§ åè¯çŸ­è¯­ï¼Œä¸­å¿ƒè¯ï¼Œä»¥åŠå›´ç»•ä¸­å¿ƒè¯çš„å‰åä¿®é¥°è¯­ åŠ¨è¯çŸ­è¯­ å¹¶åˆ—å…³ç³» Treebanksï¼šç»è¿‡å‰–æåçš„è¯­æ–™åº“ï¼ŒåŒ…å«äº†å¥æ³•syntaticä¿¡æ¯å’Œè¯­ä¹‰semanticä¿¡æ¯ Treebanks as Grammars Heads and Head Finding è¯­æ³•ç­‰ä»·ä¸èŒƒå¼ Lexicalized Grammars è¯æ±‡è¯­æ³• å‰è¨€ï¼š ä¸Šä¸‹æ–‡æ— å…³è¯­æ³• context-free gramma. ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•æ˜¯è®¸å¤šè‡ªç„¶è¯­è¨€å¥æ³•å½¢å¼åŒ–æ¨¡å‹çš„æ”¯æŸ±. Constituency ç»„æˆæ€§ä½•ä¸ºç»„æˆæ€§ï¼ˆconstituencyï¼‰ï¼šgroups of words behaving as a single units, or constituents. ä»¥åè¯çŸ­è¯­(noun phrase)ä¸ºä¾‹: ä¸€ä¸ªæ˜æ˜¾çš„è¯æ®æ˜¯åè¯çŸ­è¯­é€šå¸¸åœ¨åŠ¨è¯ verb ä¹‹å‰ã€‚ Context-Free Grammars ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ä¸Šä¸‹æ–‡æ— å…³è¯­æ³• Context-Free Grammar, or CFG, åˆå«çŸ­è¯­ç»“æ„è¯­æ³• Phrase-Structure Grammars, å…¶å½¢å¼åŒ–æ–¹æ³•ç­‰ä»·äº Backus-Naur Form, or BNF. ä¸€ä¸ªä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ï¼Œç”±è§„åˆ™ rules æˆ– äº§ç”Ÿå¼ productions ,ä»¥åŠå•è¯å’Œç¬¦å·çš„ä¸€ä¸ªè¯è¡¨ lexicon. ä¸¾ä¾‹ï¼š ä¸€ä¸ªåè¯çŸ­è¯­ï¼ˆNP or noun phraseï¼‰å¯ä»¥ç”±ä¸€ä¸ªä¸“æœ‰åè¯(ProperNoun)ç»„æˆæˆ–è€…æ˜¯ä¸€ä¸ªé™å®šè¯(Det)åæ¥ä¸€ä¸ªåè¯æ€§æˆåˆ†ï¼ˆNominalï¼‰,è€Œä¸€ä¸ªåè¯æ€§æˆåˆ†åˆå¯ä»¥æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªåè¯ã€‚ parse tree å‰–ææ ‘ NPæ˜¯æ ‘çš„æ ¹ï¼Œä¹Ÿå°±æ˜¯ CFG å®šä¹‰çš„å½¢å¼è¯­è¨€çš„åˆå§‹ç¬¦å· start symbol, ç”¨ S è¡¨ç¤ºï¼Œé€šå¸¸å¯ä»¥è§£é‡Šä¸º â€œå¥å­â€ï¼Œæ‰€ä»¥ç”± S æ¨å¯¼å‡ºæ¥çš„ç¬¦å·ä¸²çš„é›†åˆå°±æ˜¯å¥å­çš„é›†åˆã€‚ a, fight è¿™æ ·çš„å•è¯æ˜¯ç»ˆæç¬¦å· terminal symbol, è¯è¡¨æ˜¯å¼•å…¥ç»ˆæç¬¦å·çš„è§„åˆ™çš„é›†åˆ è¡¨ç¤ºç»ˆæç¬¦å·çš„èšç±»æˆ–æ¦‚æ‹¬çš„ç¬¦å·ç§°ä¸ºéç»ˆæç¬¦å· non-ternimal symbol,æ¯”å¦‚ç®­å¤´å·¦è¾¹çš„ NP.Det, Nounâ€¦. ç”¨ â€œ|â€ è¡¨ç¤ºéç»ˆæç¬¦å·çš„å±•å¼€æ–¹å¼ï¼š è¯­æ³•è§„åˆ™ï¼Œç”¨ $L_0$ è¡¨ç¤ºï½ å‰–ææ ‘ä¹Ÿå¯ä»¥ç”¨æ›´ä¸ºç®€æ´çš„æ–¹å¼è¡¨ç¤ºï¼š Formal Definition of Context-Free Grammar ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•çš„å½¢å¼å®šä¹‰è¯¸å¦‚ $L_0$ çš„CFGå®šä¹‰äº†ä¸€ä¸ªå½¢å¼è¯­è¨€ï¼Œåœ¨chapter2 ä¸­è®²è¿‡ï¼Œå½¢å¼è¯­è¨€æ˜¯ç¬¦å·ä¸²çš„é›†åˆã€‚ä½¿ç”¨å½¢å¼è¯­è¨€æ¥æ¨¡æ‹Ÿè‡ªç„¶è¯­è¨€çš„è¯­æ³•ç§°ä¸º â€œç”Ÿæˆè¯­æ³•ï¼ˆgenerative grammarï¼‰â€. ä¸€ä¸ªä¸Šä¸‹æ–‡æ— å…³è¯­æ³•æœ‰å››ä¸ªå‚æ•°ï¼ˆä¹Ÿç§°ä¸ºå››å…ƒç»„ï¼Œ4-tupleï¼‰ï¼š éç»ˆæç¬¦å·çš„é›†åˆ N ç»ˆæç¬¦å·çš„é›†åˆ $\\Sigma$ ç”Ÿæˆå¼çš„é›†åˆ Rï¼Œ æ¯ä¸ªç”Ÿæˆå¼çš„å½¢å¼ä¸º $A\\rightarrow \\beta$,å…¶ä¸­ A æ˜¯éç»ˆæç¬¦å·ï¼Œ $\\alpha$ æ˜¯ç”±ç¬¦å·ä¸²çš„æ— é™é›† $(\\Sigma \\bigcup N)* $ çš„ç¬¦å·æ„æˆçš„ç¬¦å·ä¸² æŠŠå•è¯çš„ç¬¦å·ä¸²æ˜ å°„åˆ°å‰–ææ ‘çš„é—®é¢˜ç§°ä¸º Syntactic parsing å¥æ³•åˆ†æ. åœ¨chapter12ä¸­ä¼šè®²åˆ°ï½ Some Grammar Rules for Englishæœ¬ä¹¦ä¸­å…³äºè‹±è¯­çŸ­è¯­ç»“æ„åªæ˜¯ç‚¹åˆ°ä¸ºæ­¢ï¼Œéœ€è¦äº†è§£æ›´å¤šå¯ä»¥çœ‹ [Huddleston, R. and Pullum, G. K. (2002). The Cambridge Grammar of the English Language. Cambridge University Press.] Sentence-Level Constructions å¥å­çº§çš„ç»“æ„é™¤äº†ä¸Šæ–‡ä¸­ä»‹ç»çš„é™ˆè¿°å¥ï¼Œè¿˜æœ‰4ä¸­æ¯”è¾ƒå¸¸è§å’Œé‡è¦çš„ç»“æ„ï¼šdeclaratives é™ˆè¿°å¼ç»“æ„, imperatives å‘½ä»¤å¼ç»“æ„, yes-no questions yes-noç–‘é—®å¼ç»“æ„, and wh-questions ç–‘é—®å¼ç»“æ„. imperatives å‘½ä»¤å¼ç»“æ„ åŠ¨è¯çŸ­è¯­å¼€å¤´ï¼Œæ²¡æœ‰ä¸»è¯­ã€‚ yes-no questions yes-noç–‘é—®å¼ç»“æ„ åŠ©åŠ¨è¯å¼€å¤´ï¼Œåé¢è·Ÿä¸€ä¸ªä¸»è¯­NPï¼Œå†è·Ÿä¸€ä¸ªVP. wh-questions ç–‘é—®å¼ç»“æ„ æ¯”è¾ƒå¤æ‚ï¼Œæœ‰ä¸¤ç§æƒ…å†µï¼š whä¸»è¯­ç–‘é—®å¼ï¼ˆwh-subject-questionï¼‰ç»“æ„ï¼Œä¸»è¯­ç–‘é—®å¼ç»“æ„ä¸é™ˆè¿°å¥ç»“æ„ç›¸åŒ whéä¸»è¯­ç–‘é—®å¼ï¼ˆwh-non-subject-questionï¼‰ç»“æ„ï¼ŒwhçŸ­è¯­ä¸æ˜¯å¥å­çš„ä¸»è¯­ ç±»ä¼¼wh-non-subject-question çš„ç»“æ„æˆ‘ä»¬ç§°ä¸º long-distance dependencies,å› ä¸º wh-NP è¿œç¦»åœ¨è¯­ä¹‰ä¸Šå’Œå®ƒç›¸å…³çš„è°“è¯ have. The Noun Phrase åè¯çŸ­è¯­å…³äºåè¯çŸ­è¯­ä¸»è¦æœ‰ï¼š pronounsä»£è¯ï¼Œ proper nounsä¸“æœ‰åè¯ï¼Œå’ŒNP $\\rightarrow det\\ Nominal$. æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸»è¦ä»‹ç»æœ€åä¸€ç§ç»“æ„ NP $\\rightarrow det\\ Nominal$. åœ¨åè¯çŸ­è¯­ä¸­ï¼ŒåŒ…æ‹¬ä¸€ä¸ªä¸­å¿ƒè¯ head, å›´ç»•ä¸­å¿ƒè¯çš„æœ‰ å‰ä¿®é¥°è¯­(prehead modifier) å’Œ åä¿®é¥°è¯­(post-head modifier). The Determiner é™å®šè¯ï¼Œå‡ºç°åœ¨ä¸­å¿ƒè¯ä¹‹å‰ The Nominal $$Nominal \\rightarrow Noun$$ Before the Head Noun: cardinal numbers åŸºæ•°è¯, ordinal numbers åºæ•°è¯, quantifiers æ•°é‡ä¿®é¥°è¯­, and adjectives after the Head Noun: postmodifiers ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼š ä»‹è¯çŸ­è¯­ éé™å®šä»å¥ å…³ç³»ä»å¥ ä»‹è¯çŸ­è¯­ prepositional phrases éé™å®šä»å¥ non-finite postmodifiersï¼Œæœ‰ä¸‰ç§ï¼š åŠ¨åè¯gerundive (-ing), -ed, and infinitive forms. ï¼ˆ1ï¼‰åŠ¨åè¯ gerundive (-ing) åˆ—ä¸¾ä¸€äº›ä¾‹å­ï¼š å…¶å¯¹åº”çš„å½¢å¼è¯­è¨€ï¼š ï¼ˆ2ï¼‰infinitivesä¸å®šå¼ and -edå½¢å¼ å…³ç³»ä»å¥(postnominal relative clause), å‡†ç¡®çš„è¯´æ˜¯ é™åˆ¶æ€§å…³ç³»ä»å¥(restrictive relative clause), é€šå¸¸ç”¨å…³ç³»ä»£è¯ relative pronoun å¼€å¤´. before the Noun Phrase: åœ¨NPsä¹‹å‰çš„è¯ï¼Œé€šå¸¸å« predeterminers, æœ€å¸¸è§çš„å°±æ˜¯ all. å¤æ‚çš„åè¯çŸ­è¯­çš„å‰–ææ ‘ä¸¾ä¾‹ The Verb Phrase åŠ¨è¯çŸ­è¯­æ¯”è¾ƒç®€å•çš„åŠ¨è¯çŸ­è¯­çš„ç»“æ„ï¼š é™¤æ­¤ä¹‹å¤–è¿˜æœ‰æ›´å¤æ‚çš„ï¼Œåœ¨åŠ¨è¯ååµŒå…¥å®Œæ•´çš„å¥å­,è¿™æ ·çš„æˆåˆ†å«åšå¥å­è¡¥è¯­ sentential complements åœ¨VPåæ½œåœ¨çš„æˆåˆ†å¯èƒ½æ˜¯å¦ä¸€ä¸ªVPã€‚åŠ¨è¯å¯ä»¥ä¸ä¸åŒç±»å‹çš„è¡¥è¯­ç›¸å®¹ï¼Œä½†ä¸æ˜¯æ¯ä¸ªåŠ¨è¯éƒ½ä¸æ¯ä¸ªåŠ¨è¯çŸ­è¯­ç›¸å®¹ã€‚æ¯”å¦‚ åŠç‰©åŠ¨è¯(transitive) å’Œ éåŠç‰©åŠ¨è¯(intransitive) ä¹‹åˆ†ã€‚ å°†åŠ¨è¯ æ¬¡èŒƒç•´åŒ–(subcategorize), ä¹Ÿå°±æ˜¯æŒ‰ç…§ NPæˆ–å…¶ä»–è¡¥è¯­ å†åˆ†ç±»ã€‚åŠ¨è¯çš„è¿™äº›å¯èƒ½çš„è¡¥è¯­çš„é›†åˆç§°ä¸ºè¯¥åŠ¨è¯çš„ æ¬¡èŒƒç•´åŒ–æ¡†æ¶ï¼ˆsubcategorize frameï¼‰ã€‚ Coordination å¹¶åˆ—å…³ç³»conjunctions: and, or and but. NP $\\rightarrow$ NP and NP Nominal $\\rightarrow$ Nominal and Nominal VP $\\rightarrow$ VP and VP S $\\rightarrow$ S and S Treebanksä½•ä¸ºTreebanks: a corpus where every sentence in the collection is paired with a corresponding parseï¼Œ Such a syntactically annotated corpus is called a treebank ä¸€ä¸ªè¯­æ–™åº“ä¸­æ‰€æœ‰çš„sentenceséƒ½æœ‰å…¶å¯¹åº”çš„å‰–ææ ‘ã€‚ Penn Treebank project (whose POS tagset we introduced in Chapter 10) has produced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal corpora of English, as well as treebanks in Arabic and Chinese. Penn Treebank æ˜¯ä¸€ä¸ªtreebankï¼Œå…¶è¯­æ–™åº“æ¥è‡ªäº Brownç­‰ã€‚ã€‚ Treebanks as Grammars Viewed as a large grammar in this way, the Penn Treebank III Wall Street Journal corpus, which contains about 1 million words, also has about 1 million non-lexical rule tokens, consisting of about 17,500 distinct rule types. Heads and Head Findingæ¯ä¸€ä¸ªç»„æˆæˆåˆ†constituentï¼Œä¹Ÿå°±æ˜¯æ ‘ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªè¯æ±‡å¤´éƒ¨ ï¼ˆlexical headï¼‰. ä¸ºä»€ä¹ˆéœ€è¦headï¼Ÿ æš‚æ—¶è¿˜ä¸å¤ªæ‡‚ï¼Œå­¦è¿‡åé¢çš„å†…å®¹åº”è¯¥å°±èƒ½ç†è§£äº†å§ï½ ä¸¾ä¸ªæ —å­ï¼š æ¯ä¸ªéç»ˆænon-terminalç¬¦å·ï¼Œä¹Ÿå°±æ˜¯éå¶èŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªhead. é‚£ä¹ˆæ€ä¹ˆæ‰¾æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„headå‘¢ï¼Ÿ Instead of specifying head rules in the grammar itself, heads are identified dynamically in the context of trees for specific sentences. In other words, once a sentence is parsed, the resulting tree is walked to decorate each node with the appropriate head. ä¹Ÿå°±æ˜¯è¯´ï¼Œæ²¡æœ‰å…·ä½“çš„è§„åˆ™ï¼Œæ ¹æ®å…·ä½“çš„å¥å­åŠ¨æ€çš„å®šä¹‰å¯¹åº”çš„headï¼Œè¿™äº›è§„åˆ™ä¹Ÿéƒ½æ˜¯ hand-written rules. ä»¥NPä¸­çš„headä¸ºä¾‹ï¼š Grammar Equivalence and Normal Form è¯­æ³•ç­‰ä»·ä¸èŒƒå¼ å¼ºç­‰ä»· strong equivalence å’Œ å¼±ç­‰ä»· weak equivalence å¦‚æœä¸¤ä¸ªè¯­æ³•ç”Ÿæˆç›¸åŒçš„ç¬¦å·ä¸²é›†åˆï¼Œè€Œä¸”å¯¹æ¯ä¸ªå¥å­éƒ½æŒ‡æ´¾ç›¸åŒçš„çŸ­è¯­ç»“æ„(åªæ”¹å˜ç»ˆæç¬¦å·)ï¼Œè¿™æ ·çš„ä¸¤ä¸ªè¯­æ³•æ˜¯å¼ºç­‰ä»· å¦‚æœç”Ÿæˆçš„ç¬¦å·ä¸²é›†åˆç›¸åŒï¼Œä½†æ˜¯ä¸ç»™æ¯ä¸ªå¥å­æŒ‡æ´¾ç›¸åŒçš„çŸ­è¯­ç»“æ„ï¼Œå°±æ˜¯å¼±ç­‰ä»·ï¼Œå…¶å®å°±æ˜¯å¯¹åŒä¸€ä¸ªå¥å­ï¼Œç†è§£ä¸åŒï½ ä¸¾ä¸ªæ —å­ï¼š ä¸€ä¸ªä¸Šä¸‹æ–‡æ— å…³è¯­æ³•CFGæ˜¯è‡ªç”±çš„ï¼Œå¹¶ä¸”å®ƒçš„æ¯ä¸ªç”Ÿæˆå¼çš„å½¢å¼éƒ½æ˜¯ $A\\rightarrow B C$ æˆ– $A \\rightarrow a$, ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªè§„åˆ™ ruleçš„å³è¾¹è¦ä¹ˆæ˜¯ä¸¤ä¸ªéç»ˆæç¬¦å·ï¼Œè¦ä¹ˆæ˜¯ä¸€ä¸ªç»ˆæç¬¦å·ï¼Œé‚£ä¹ˆè¿™ä¸ªCFGå°±æ˜¯ Chomsky normal form. ä»»ä½•ä¸€ä¸ªCFGéƒ½å¯ä»¥å†™æˆå¼±ç­‰ä»·çš„ChomskyèŒƒå¼è¯­æ³•ã€‚ ä¸¾ä¸ªæ —å­ï¼š VBDæ˜¯åŠ¨è¯çš„è¿‡å»å¼ï¼Œ Lexicalized Grammars è¯æ±‡è¯­æ³•å¯ä»¥çœ‹åˆ°ï¼ŒCFGè¿‡åˆ†å¼ºè°ƒçŸ­è¯­ç»“æ„ï¼Œè€Œå¿½è§†äº†è¯æ±‡çš„ä½œç”¨ï¼Œä¹Ÿå°±æ˜¯å•è¯çš„è¯­ä¹‰ã€‚ä½†è¿™æ ·çš„çŸ­è¯­ç»“æ„éƒ½å¤ªå¤æ‚å’Œç¬¨é‡(cumbersome)äº†ï¼Œè€Œä¸” è¯­æ³•å†—ä½™ï¼Œéš¾ä»¥ç®¡ç†å’Œè„†å¼±ï¼ˆredundant,hard to manage, and brittleï¼‰ã€‚ä¸ºäº†è§£å†³è¿™æ ·çš„é—®é¢˜ï¼Œéœ€è¦æ›´å¥½çš„åˆ©ç”¨lexiconã€‚ æ¥ä¸‹æ¥ä»‹ç»å…¶ä¸­çš„ä¸€ç§æ–¹æ³•ï¼š **Combinatory Categorial Grammarï¼ŒCCG ***,è€ƒè™‘åˆ° å¥æ³• syntactic å’Œ è¯­ä¹‰ semantic çš„é«˜åº¦ è¯æ±‡åŒ–lexicalized çš„æ–¹æ³•. ä¸‹ä¸€ç« ä¼šè¯¦ç»†è®²åˆ°è¯æ±‡åŒ–ï½ æ€»ç»“ å‚è€ƒï¼š Speech and language Processingï¼ŒChapter11 Natural Language Processing, Michael Collins, Columbia University","link":"/2018/04/19/chapter11-%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95/"},{"title":"chapter14 dependency Parsing1","text":"CS224d-Lecture6:Dependency Parsing Dependency ParsingUnlabled Dependency Parses root is a special root symbol Each dependency is a pair (h, m) where h is the index of a head word, m is the index of a modifier word. In the figures, we represent a dependency (h, m) by a directed edge from h to m Dependencies in the above example are (0, 2), (2, 1), (2, 4), and (4, 3). (We take 0 to be the root symbol.) Conditions on Dependency Structures ä»rootåˆ°ä»»ä½•ä¸€ä¸ªwordéƒ½æœ‰ä¸€æ¡ç›´æ¥è·¯å¾„ no crossing (æ¯”å¦‚å³ä¸‹è§’çš„ç¬”è®°å°±ä¸è¡Œï½) å¯¹äº â€œJohn saw Maryâ€ æœ‰5ä¸­dependency parse æœ‰crossingçš„ç»“æ„å« non-projective structure dependency parsing resource: Conll 2007 McDonald dependency banks ä¸€ä¸ª treebank é€šè¿‡ lexicalization å¯ä»¥è½¬æ¢æˆ dependency bank.ä¹Ÿå°±æ˜¯ä¸€ä¸ªlexicalizated PCFGå¯ä»¥è½¬æ¢ä¸ºä¸€ä¸ª dependency bank. efficiency of dependency parsing dynamic programming - Jason Eisner very efficiencyat Parsing very useful representations CS224ä»€ä¹ˆæ˜¯dependency structure describing the structure of a sentence by taking each word and saying what itâ€™s a dependent on.So, if itâ€™s a word that kind of modifies or is an argument of another word that youâ€™re saying, itâ€™s a dependent of that word. ambiguity: PP attachments attachment ambiguities:A key parsing decision is how we â€˜attachâ€™ vairous constituents PPs ä»‹è¯çŸ­è¯­, adverbial or participial phrase å‰¯è¯å’Œåˆ†è¯çŸ­è¯­, infinitives ä¸å®šå¼, coordinations å¹¶åˆ—å…³ç³» Catalan numbers: $C_n=(2n)!/[n+1]!n!$ ??éœ€è¦åœ¨æŸ¥èµ„æ–™!! äººå·¥æ ‡æ³¨ä¹Ÿå°±å¤ªå¤ªéº»çƒ¦äº†ï½ dependency Grammar and Dependency structureuniversal dependency the arrow connects a head (governor, superior, regent) with a dependent (modifier, inferior, subordinate) usually, dependencies from a tree(connected, acyclic éå‘¨æœŸçš„, single-head) Question: compare CFGs and PCFGs and do they, dependency grammars look strongly lexicalized,theyâ€™re between words and does that makes it harder to generalize? dependency conditioning preferenceså…³äºå¦‚ä½•ç¡®å®šdependency? æœ‰ä»¥ä¸‹å‡ ç‚¹perferencesï¼Ÿ dependency parsing è¿™é‡Œéœ€è¦æ³¨æ„ä¸€ç‚¹ï¼šæ˜¯å¦æœ‰äº¤å‰ crossing(non-projective). åœ¨å›¾ä¸­çš„ä¾‹å­ä¸­ï¼Œæ˜¯æœ‰crossingçš„ã€‚ ä½†æ˜¯dependency treeçš„å®šä¹‰æ˜¯ no crossingã€‚ methods of dependency Parsing ç¬¬4ç§æ–¹æ³•æ˜¯ç›®å‰æœ€ä¸»æµçš„æ–¹æ³•ã€‚ paper presentation: Improving distributional similarity with lessions learned from word embeddings, Omer Levy, Yoav Goldberg, Ido Dagan shift reduced parsingï¼Ÿï¼Ÿï¼Ÿ Greedy transition-based parsing The parser: a stack $\\sigma$ï¼Œ written with top to the right which starts with the ROOT symbol a buffer $beta$, written with the top to the left which starts with the input sentence s set of dependency arcs A which starts off empty a set of actions $Left-Arc_r$ $\\sigma|w_i|w_j,\\beta,A \\rightarrow \\sigma|w_j, \\beta, A\\bigcup{r(w_j,w_i)}$ è¡¨ç¤ºstack $\\sigma$ ä¸­çš„ä¸¤ä¸ªå…ƒç´  $w_i\\leftarrow w_j$ MaltParser Question: dependency parsingçš„å‡†ç¡®ç‡æ˜¯å¦ä¼šå‡ºç°waterfallèˆ¬çš„ä¸‹é™ï¼Œbuz one decision will prevent sone other decisions. itâ€™s not bad. dependency parse evaluation suffers much less badly from waterfall effects than CFG parsing when which is worse in that respect. feature representationä¸å¤ªæ‡‚è¿™éƒ¨åˆ†ã€‚ã€‚ã€‚ evaluation of dependency parsing why train a neural dependency parser? Indicator Features Revisited. our approach: learn a dense and compact feature representation. a neural dependency parser A Fast and Accurate Dependency Parser using Neural Networks æ¸…åçš„æœ¬ç§‘ç”Ÿç®€ç›´ç¥ä¸€æ ·çš„å­˜åœ¨ã€‚ã€‚ã€‚ distributed representations Model Architecture Non-linearities between layers: Why they are needed Google announcement of Parsey McParseface, SyntaxNet","link":"/2018/04/23/chapter14-dependency-Parsing/"},{"title":"UCL-DRL-03-planning by DP","text":"Planning by Dynamic ProgrammingIntroductionwhat is dynamic programming? åŠ¨æ€è§„åˆ’æ˜¯ä¸€ç§æ–¹æ³•/æ€æƒ³ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£æˆå­é—®é¢˜ï¼Œå¹¶è§£å†³å­é—®é¢˜ã€‚ DP æ˜¯å…·æœ‰ä»¥ä¸‹ä¸¤ç§ç‰¹è´¨çš„é—®é¢˜å¸¸ç”¨çš„è§£å†³æ–¹æ³•ï¼š å…·æœ‰å¯ä¼˜åŒ–çš„å­ç»“æ„ ä¼˜åŒ–é—®é¢˜å¯ä»¥åˆ†è§£æˆå­é—®é¢˜ é‡å å­é—®é¢˜ å­é—®é¢˜é‡å¤å‡ºç°å¾ˆå¤šæ¬¡ å­é—®é¢˜çš„solutionå¯ä»¥è¢«ç¼“å­˜å’Œreuse é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å°±å…·æœ‰è¿™ä¸¤ç§ç‰¹è´¨ï¼š Bellman å…¬å¼ç»™å‡ºäº†è¿­ä»£åˆ†è§£çš„æ–¹å¼ value funvtion ç”¨æ¥å­˜å‚¨å’Œå†åˆ©ç”¨å­solutions Bellman å…¬å¼çš„å«ä¹‰åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯ç›®å‰çš„ä¸€æ­¥æ˜¯æœ€ä¼˜çš„è¡Œä¸ºï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯ä½™ä¸‹æ¥çš„å…¶ä»–æ­¥éª¤ä¹Ÿæ˜¯æœ€ä¼˜çš„è¡Œä¸ºã€‚ åŠ¨æ€è§„åˆ’éœ€è¦çš„è¾“å…¥ï¼š MDP $&lt;S,A,P,R,\\gamma&gt;$ and policy $\\pi$ or MDP $&lt;S,P^{\\pi},R^{\\pi},\\gamma&gt;$ è¾“å‡ºï¼š value function $v_{\\pi}$ DP é€‚ç”¨çš„ä¸€äº›åœºæ™¯ã€‚æ¯”è¾ƒç†Ÿæ‚‰çš„ sequence alignment, shortest path algorithms, viterbi algorithm. Policy evaluationIterative policy evaluation $v_{k+1}(s)=\\sum_{a\\in A}\\pi(a|s)(R_s^a+\\gamma\\sum_{sâ€™\\in S}P^a_{ssâ€™}v_k(sâ€™))$ example æ–¹å—ä¸­ä»»æ„ä¸€ä¸ªçŠ¶æ€åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„ reward æ˜¯ -1. æ¯è¿­ä»£ä¸€æ­¥æœ‰ north, east, south, weat 4ç§é€‰æ‹©ã€‚ k=0. åˆå§‹çŠ¶æ€ä¸‹ï¼Œæ‰€æœ‰çš„æ–¹å—ç´¯è®¡çš„ reward éƒ½æ˜¯ 0. æ‰€ä»¥åœ¨è¿™ä¸ªçŠ¶æ€ä¸‹æ˜¯ random policy. k=1, è¿­ä»£ä¸€æ­¥ä¹‹åï¼Œé™¤äº† terminal squares, å…¶ä»–çš„ reward éƒ½æ˜¯ 1. å¦‚æœé‡‡ç”¨ greedy policyï¼Œå°±èƒ½ç¡®å®šéƒ¨åˆ†è·¯å¾„äº†ã€‚ k=2, åœ¨ random policy ç­–ç•¥ä¸‹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹ 1.75 æ€ä¹ˆå¾—åˆ°çš„ï¼š $$1+(1+1+1)/4=1.75$$ k=3, åœ¨ random policy ç­–ç•¥ä¸‹ï¼Œæˆ‘ä»¬æ¥çœ‹ 2.4, 2.9 æ€ä¹ˆå¾—åˆ°çš„ï¼š $$(1+2.7+3+3)/4=2.425$$ $$(2.7+3+3+3)/4=2.925$$ è¿™é‡Œçš„ k æ˜¯è¿­ä»£çš„æ¬¡æ•°ï¼Œå¹¶ä¸æ˜¯æ—¶é—´. policy iteration è¿­ä»£æ–¹å¼ï¼š policy evaluation policy improvement: greedy policy improvement $$v_{\\pi}(s)=E[R_{t+1}+\\gamma R_{t+2}+â€¦.|S_t=s]$$ improve the policy by acting greedily. å½“å‰æœ€ä¼˜ policy: $${\\pi}^{â€˜}(s)=greedy(v_{\\pi})=argmax_{a\\in A}q_{\\pi}(s,a)$$ $q_{\\pi}(s,a)$ æ˜¯ action-value function. è¿™é¡µpptä¸­æœ€åçš„å…¬å¼å®é™…è¯æ˜äº†ï¼šé‡‡ç”¨ greedy policy è‡³å°‘èƒ½ä¿è¯ $v_{\\pi^{â€˜}}\\ge v_{\\pi}(s)$. modified policy iteration value iteration ä¼˜åŒ–çš„å®šç†ï¼š ä»»ä½•ä¼˜åŒ–ç­–ç•¥éƒ½å¯ä»¥åˆ†è§£æˆä¸¤éƒ¨åˆ†ï¼š æœ€ä¼˜åŒ–çš„ action A åç»§çŠ¶æ€ Sâ€™ ä¸‹çš„æœ€ä¼˜åŒ–ç­–ç•¥ policy example: summary of DP algorithms In-Place Dynamic Programming Prioritised Sweeping Real-Time Dynamic Programming Extensions to dynamic programming","link":"/2019/03/06/UCL-DRL-03-planning-by-DP/"},{"title":"chapter27-Question Answering","text":"Speech and language Processing, chapter27:Question Answering Question Answering Systemä»€ä¹ˆæ˜¯ Question Answering?ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼š å¤šæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ é’ˆå¯¹ä¸€ç³»åˆ—æ–‡æ¡£æå‡ºçš„é—®é¢˜ ç­”æ¡ˆå¯èƒ½å‡ºç°å¤šæ¬¡ï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰å‡ºç° ä¸»è¦åº”ç”¨åœ¨äºäº’è”ç½‘æœç´¢å¼•æ“ï¼Œæ–‡æœ¬èµ„æ–™åº“çš„æœç´¢ï¼Œæ¯”å¦‚æ–°é—»æ¡£æ¡ˆã€åŒ»å­¦æ–‡çŒ®ã€ç§‘å­¦æ–‡ç« ç­‰ å•ä¸ªæ–‡æ¡£çš„æ™ºèƒ½é—®ç­” Question Type Simple (factoid) questions (most commercial systems): ç®€å•çš„é—®é¢˜ï¼Œå¯ä»¥ç”¨ç®€å•çš„äº‹å®å›ç­”ï¼Œç­”æ¡ˆç®€çŸ­é€šå¸¸æ˜¯ä¸€ä¸ª named entity Who wrote the Declaration of Independence? What is the average age of the onset of autism? Where is Apple Computer based? Complex (narrative) questions: ç¨å¾®å¤æ‚çš„å™è¿°é—®é¢˜ï¼Œç­”æ¡ˆç•¥é•¿ What do scholars think about Jeffersonâ€™s position on dealing with pirates? What is a Hajj? In children with an acute febrile illness, what is the efficacy of single medication therapy with acetaminophen or ibuprofen in reducing fever? Complex (opinion) questions: å¤æ‚çš„é—®é¢˜ï¼Œé€šå¸¸æ˜¯å…³äºè§‚ç‚¹ï¼æ„è§ Was the Gore/Bush election fair? é—®ç­”ç³»ç»Ÿåˆ†ç±»ç°ä»£æ™ºèƒ½é—®ç­”ç³»ç»Ÿçš„ä¸»è¦æœ‰ä¸¤ä¸ªèŒƒå¼(two major modern paradigms of question answering)ä»¥åŠæ··åˆæ–¹æ³•ï¼Œ éƒ½æ˜¯å…³æ³¨äº **äº‹å®æ€§çš„å›ç­”(factoid question)**ã€‚: IR-based question answering åŸºäºä¿¡æ¯æ£€ç´¢çš„æ™ºèƒ½é—®ç­” knowledge-based question answering åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­” Hybrid approaches (IBM Watson) å¯¹æ¯”ä¸‹ä¸‰è€…ï¼Œå…·ä½“äº†è§£ä¹‹åå¯ä»¥å†ä¼šè¿‡å¤´æ¥çœ‹çœ‹ï¼š IR-based Factoid question answeringIR-based factoid AQ çš„æµç¨‹å›¾ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé—®é¢˜å¤„ç†(question processing), ç¯‡ç« æ£€ç´¢(passage retrieval and ranking), å’Œ ç­”æ¡ˆå¤„ç†(answer processing). Question Processing Answer Type Detection: åˆ†æ questionï¼Œå†³å®š answer type Query Formulation: å½¢æˆåˆé€‚çš„æŸ¥è¯¢è¯­å¥è¿›è¡Œæ£€ç´¢ Passagge Retrieval é€šè¿‡æ£€ç´¢å¾—åˆ° top N documents æŠŠ documents æ‹†åˆ†ç§°åˆé€‚çš„å•ä½(unit/passage) Answer Processing å¾—åˆ°å€™é€‰çš„ answer è¿›è¡Œæ’åºï¼Œé€‰å‡ºæœ€ä½³ answer Question Processingä¸¤ä¸ªä»»åŠ¡ï¼š answer type detection what kind of entity (person, place) is the answer? å½¢æˆåˆé€‚çš„ query what is the query to the IR system Answer type: the kind of entity the answer consists of (person, location, time, etc.) Query: the keywords that should be used for the IR system to use in searching for documents Focus: the string of words in the question that are likely to be replaced by the answer in any answer string found åœ¨è¿™ä¸ªé˜¶æ®µéœ€è¦åšçš„äº‹ï¼š ä¸¾ä¸ªæ —å­ï¼š Answering Type Detection (Question classification)é€šå¸¸è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå®ƒå½“ä½œä¸€ä¸ªæœºå™¨å­¦ä¹ çš„åˆ†ç±»é—®é¢˜ã€‚ å®šä¹‰ç±»åˆ« æ³¨é‡Šè®­ç»ƒæ•°æ®ï¼Œç»™æ•°æ®æ‰“ä¸Šåˆ†ç±»æ ‡ç­¾ è®­ç»ƒåˆ†ç±»å™¨ï¼Œæ‰€ç”¨ç‰¹å¾å¯ä»¥åŒ…æ‹¬ hand-written rules. å®šä¹‰åˆ†ç±»ç±»åˆ«å‰äººå·²ç»æä¾›äº†ä¸€äº› answer type çš„å±‚æ¬¡å‹åˆ†ç±»ç»“æ„ï¼Œå¦‚ answer type Taxonomy(from Li &amp; Roth). Two-layered Taxonomy 6 coarse classes:ï¼ˆcoarse-grained tagï¼‰ ABBEVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC_VALUE 50 fine classes:ï¼ˆfine-grained tagsï¼‰ HUMAN: group, individual, title, description ENTITY: animal, body, color, currencyâ€¦ LOCATION: city, country, mountainâ€¦ æå–ç‰¹å¾å°† answer type çœ‹ä½œä¸€ä¸ªç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚questionä¸­å¯ä»¥ç”¨æ¥åˆ†ç±»çš„ç‰¹å¾ï¼š words in the questions, the part-of-speech of each word named entities in the questions answer type word or question headword: é€šå¸¸ wh-word ä¹‹åçš„ç¬¬ä¸€ä¸ª NP å¯ä»¥ç”¨æ¥ä½œä¸ºç‰¹å¾ semantic information about the wordsï¼ŒWordNet synset ID åˆ†ç±»æ–¹æ³•æ€»ç»“ä¸‹å¯ä»¥ç”¨æ¥åˆ†ç±»çš„æ–¹æ³•ï¼š hand-written rules machine learning hybrids å…¶ä¸­ rules åŒ…æ‹¬ï¼š regular expression-baesd rules $\\text{who {is | was | are | were} PERSON}$ PERSONï¼ˆYEAR-YEARï¼‰ Other rules use the question headword headword of first noun phrase after whâ€word å½“ç„¶ä¹Ÿå¯ä»¥æŠŠä¸Šè¿°æŸäº›åˆ†ç±»æ–¹æ³•å½“ä½œç‰¹å¾ä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚å°±åˆ†ç±»æ•ˆæœè€Œè¨€ï¼ŒPERSON, LOCATION, TIME è¿™ç±»çš„é—®é¢˜ç±»å‹æœ‰æ›´é«˜çš„å‡†ç¡®ç‡ï¼ŒREASONï¼ŒDESCRIPTION è¿™ç±»çš„é—®é¢˜æ›´éš¾è¯†åˆ«ã€‚ Query Formulationæ ¹æ® question äº§ç”Ÿä¸€ä¸ª keyword listï¼Œä½œä¸º IR ç³»ç»Ÿçš„è¾“å…¥ queryã€‚å¯èƒ½çš„æµç¨‹æ˜¯å»é™¤ stopwordsï¼Œä¸¢æ‰ question word(where, when, etc.)ï¼Œæ‰¾ noun phrasesï¼Œæ ¹æ® tfidf åˆ¤æ–­ keywords çš„å»ç•™ç­‰ç­‰ã€‚ å¦‚æœ keywords å¤ªå°‘ï¼Œè¿˜å¯ä»¥é€šè¿‡ query expansion æ¥å¢åŠ  query termsã€‚ keyword selection algorithm: select all non-stop words in quotations select all NNP words in recognized named entities select all complex nominals with theor adjectival modifiers select all other complex nominals select all nouns with their adjetival modifiers select all other nouns select all verbs select all adverbs select the QFW word(skipped in all previous steps) select all other words æœ€ç»ˆå¾—åˆ°çš„ query: Passage Retrievalæœ‰äº† queryï¼Œæˆ‘ä»¬è¿›è¡Œæ£€ç´¢ï¼Œä¼šå¾—åˆ° top N çš„æ–‡æ¡£ï¼Œç„¶è€Œæ–‡æ¡£å¹¶ä¸æ˜¯å¾—åˆ° answer çš„æœ€å¥½çš„å•ä½ï¼Œä¸‹ä¸€æ­¥æˆ‘ä»¬éœ€è¦ä»æ–‡æ¡£æŠ½å– potential answer passagesï¼Œæ¥æ–¹ä¾¿åé¢çš„ answer processingã€‚passage å¯ä»¥æ˜¯ sections, paragraphs, sentenceï¼Œå…·ä½“æƒ…å†µå…·ä½“åˆ†æã€‚ 1.rank the documents by relevance 2.extract a set of potential answer passages from the retrieved set of documents. the passage could be sections, paragraphs, sentences. 3.passage retrieval: run a named entity or answer type classification on the retrieved passages. ç„¶åå¯¹å‰©ä¸‹æ¥çš„ passages è¿›è¡Œæ’åºï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œåˆ†ç±»ï¼Œæœ‰ä»¥ä¸‹ä¸€äº›åˆ—ç‰¹å¾ï¼š å¯¹äºåŸºäº web çš„ QA ç³»ç»Ÿï¼Œæˆ‘ä»¬å¯ä»¥ä¾é ç½‘é¡µæœç´¢æ¥åš passage extraction, ç®€å•çš„è¯´ï¼Œå¯ä»¥æŠŠç½‘é¡µæœç´¢äº§ç”Ÿçš„ snippets ä½œä¸º passagesã€‚ æ€»ç»“ä¸‹ passage retrieval: Answer Processingç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»æœ‰äº† answer typeï¼Œä¹Ÿé€‰å‡ºäº†ç›¸å…³çš„ passagesï¼Œå°±å¯ä»¥è¿›ä¸€æ­¥ç¼©å° candidate answer çš„èŒƒå›´ã€‚ answer-extraction task ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³• answer-type pattern extractionåŸºäºæ­£åˆ™åŒ–åŒ¹é…çš„æœºåˆ¶ã€‚å¦‚æœ answer types æ˜¯ HUMAN æˆ–è€… DISTANCE-QUANTITY. å¯ä»¥ç›´æ¥é€šè¿‡æ¨¡å¼åŒ¹é…å¾—åˆ°ï¼š æœ‰æ—¶å€™å…‰ç”¨ pattern-extraction æ–¹æ³•æ˜¯ä¸å¤Ÿçš„ï¼Œä¸€æ–¹é¢æˆ‘ä»¬ä¸èƒ½åˆ›é€ è§„åˆ™ï¼Œå¦ä¸€æ–¹é¢ passage é‡Œä¹Ÿå¯èƒ½æœ‰å¤šä¸ª potential answerã€‚å¦å¤–ï¼Œå¯¹äºæ²¡æœ‰ç‰¹å®šå®šå‘½åå®ä½“ç±»å‹çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼(äººå·¥ç¼–å†™æˆ–è‡ªåŠ¨å­¦ä¹ )ã€‚ çœŸâ€œäººå·¥â€æ™ºèƒ½ï¼ï¼ï¼ å‰ä¸¤å¤©åŒ—ç†å·¥äº¤æµä¼šï¼Œæœ‰ä¸ªè‡ªåŠ¨åŒ–æ‰€çš„åšå£«è®²çš„è®ºæ–‡å°±æ˜¯ç”¨ç¥ç»ç½‘ç»œæ¥ç”Ÿæˆå¾ˆè‡ªç„¶çš„å›ç­”ï¼Œèµï¼ N-gram tiling/redundancy-based approachN-gram tiling åˆè¢«ç§°ä¸º redundancy-based approach(Brill et al. 2002, Lin 2007)ï¼ŒåŸºäºç½‘é¡µæœç´¢äº§ç”Ÿçš„ snippetï¼Œè¿›è¡Œ ngram çš„æŒ–æ˜ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š N-gram mining æå–æ¯ä¸ªç‰‡æ®µä¸­çš„ unigram, bigram, and trigram, å¹¶èµ‹äºˆæƒé‡ N-gram filtering æ ¹æ® ngram å’Œé¢„æµ‹çš„ answer type é—´çš„åŒ¹é…ç¨‹åº¦ç»™ ngram è®¡ç®—åˆ†æ•° N-gram tiling å°†é‡å çš„ ngram è¿æ¥æˆæ›´é•¿çš„ç­”æ¡ˆ **standard greedy method** &gt; 1. start with the highest-scoring candidate and try to tile each other candidate with this candidate 2. add the best-scoring concatenation to the set of candidates 3. remove the lower-scoring candidate 4. continue until a single answer is built å…¶ä¸­æ€ä¹ˆæ±‚è¿™ä¸ª weigthï¼Œ ä¹Ÿå°±æ˜¯ ngram å’Œ answer type çš„åŒ¹é…åº¦ï¼š Ranking Candidate AnswersIn other cases we use machine learning to combine many rich features about which phrase is the answer å¯èƒ½ç”¨åˆ°çš„ feature: æ€»ç»“ IR-based question answering factoid question answering answer type detection query formulation passage retrieval passage ranking answer extration web-based factoid question answering Knowledge-based Question Answeringå½“å¤§é‡çš„ä¿¡æ¯ä»¥ç»“æ„åŒ–çš„å½¢å¼å­˜å‚¨æ—¶ï¼Œ é€šè¿‡è¯­ä¹‰åˆ†æ(semantic parsers) å°† query æ˜ å°„æˆä¸€ä¸ª logical form. æœªå®Œå¾…ç»­ã€‚ã€‚ reference: Question Answering 28.Question Answering NLP ç¬”è®° - Question Answering System","link":"/2018/06/27/chapter27-Question-Answering/"},{"title":"chapter2:æ­£åˆ™è¡¨è¾¾å¼ã€æ–‡æœ¬æ ‡å‡†åŒ–å’Œç¼–è¾‘è·ç¦»","text":"å„ç§æ­£åˆ™è¡¨è¾¾å¼ï¼Œæå–ï¼Œç»„åˆå’Œä¼˜å…ˆå…³ç³» æ–‡æœ¬æ ‡å‡†åŒ–ï¼šå„ç§é¢„å¤„ç†æ–¹æ³• ç¼–è¾‘è·ç¦»ï¼šåŠ¨æ€è§„åˆ’ å‰è¨€ï¼š æ—©æœŸçš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·ELIZAé‡‡ç”¨çš„æ–¹æ³•æ˜¯pattern matching. è€Œå¯¹äºæ–‡æœ¬æ¨¡å¼(text pattern)çš„æè¿°ï¼Œæœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„å·¥å…·ï¼šæ­£åˆ™è¡¨è¾¾å¼(regular expression). å¯¹æ–‡æœ¬å¤„ç†ä»»åŠ¡çš„ç»Ÿç§°ï¼Œå°±æ˜¯æ–‡æœ¬æ ‡å‡†åŒ–(text normalization)ã€‚å…¶ä¸­æœ‰ï¼š tokenizing: åˆ†è¯ï¼Ÿ lemmatization: è¯å½¢è¿˜åŸã€‚ ç¡®å®šè¡¨é¢ä¸ä¸€æ ·çš„è¯æ˜¯å¦æ˜¯åŒä¸€ä¸ªè¯æ ¹ï¼Œé’ˆå¯¹æ—¶æ€æ¯”è¾ƒå¤æ‚çš„è¯­è¨€éå¸¸å¿…è¦ã€‚ stemming:è¯å¹²æå–ã€‚ é’ˆå¯¹å‰ç¼€åç¼€ï¼Œä¸€ç§ç®€å•çš„lemmatization sentence segmantation: å¥å­åˆ†å‰². ç”¨æ—¶æ€æˆ–è€…æ ‡ç‚¹ç¬¦å· æœ€åæåˆ°ç¼–è¾‘è·ç¦»(edit distance):ä¸€ç§ç®—æ³•ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ä¸­éƒ½å¾ˆå¸¸ç”¨ã€‚ æ­£åˆ™è¡¨è¾¾å¼ regular expression ç”¨æ–œçº¿(slashes)æ¥åˆ†éš”æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ–œçº¿ä¸æ˜¯æ­£åˆ™è¡¨è¾¾å¼çš„ä¸€éƒ¨åˆ†ã€‚ æ­£åˆ™è¡¨è¾¾å¼çš„åŒºåˆ†å¤§å°å†™çš„ï¼Œå¯ä»¥ç”¨æ–¹æ‹¬å·(square braces)æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ å¯¹æ‰€æœ‰çš„æ•°å­—(digits)å¯ä»¥ç”¨/[1234567890]/,ä½†å¯¹äºæ‰€æœ‰çš„å­—æ¯è¿™æ ·å°±ä¸å¤ªæ–¹ä¾¿äº†ï¼Œå¯ç”¨è¿å­—ç¬¦(dash)(-)æ¥è¡¨ç¤ºä¸€ä¸ªèŒƒå›´(range). è„±å­—ç¬¦(caret)(^) ç”¨è„±å­—ç¬¦è¡¨ç¤ºå¦å®šï¼Œæˆ–è€…ä»…ä»…è¡¨ç¤ºå®ƒè‡ªèº«ã€‚æ”¾åœ¨æ–¹æ‹¬å·çš„ç¬¬ä¸€ä¸ªä½ç½®æ—¶æ‰æœ‰æ•ˆã€‚ ç”¨é—®å·?è¡¨ç¤ºå‰ä¸€ä¸ªå­—ç¬¦æ˜¯å¯é€‰çš„ã€‚ é—®å·é™¤äº†è¡¨ç¤ºå¯é€‰å¤–ï¼Œè¿˜æœ‰è´ªå©ªå’Œéè´ªå©ªçš„åŒºåˆ«ã€‚/.* ?/ å’Œ /.* / ç”¨ * è¡¨ç¤ºå‰ä¸€ä¸ªå­—ç¬¦çš„é›¶ä¸ªæˆ–å¤šä¸ªï¼›ç”¨ + è¡¨ç¤ºå‰ä¸€ä¸ªå­—ç¬¦çš„ä¸€ä¸ªæˆ–å¤šä¸ª ä¸¾ä¸ªæ —å­ï¼š baa!(è‡³å°‘ä¸¤ä¸ªa) baaa! baaaaaa! ç”¨ /baaa*/ æˆ– /baa+/ å¯åŒ¹é…ä¸Šé¢è¿™ç§å½¢å¼ã€‚ å•ä½æ•°çš„ä»·æ ¼å¯ç”¨ /[0-9]/ï¼Œä¸€ä¸ªæ•´æ•°ï¼ˆå­—ç¬¦ä¸²ï¼‰çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š/[0-9][0-9]* / æˆ–è€… /[0-9]+/ é€šé…ç¬¦(wildcard) /./ /./è¡¨ç¤ºåŒ¹é…ä»»ä½•å­—ç¬¦ï¼Œé‚£ä¹ˆå’Œæ˜Ÿå·ä¸€èµ·ä½¿ç”¨ï¼Œå°±å¯ä»¥è¡¨ç¤ºä»»ä½•å­—ç¬¦ä¸²äº†ã€‚/.* / é”šå·(anchor)æ˜¯ä¸€ç§æŠŠæ­£åˆ™è¡¨è¾¾å¼é”šåœ¨å­—ç¬¦ä¸²ä¸­çš„ç‰¹å®šä½ç½®ï¼Œæœ€æ™®é€šçš„é”šå·æ˜¯è„±å­—ç¬¦^å’Œç¾å…ƒç¬¦$. è„±å­—ç¬¦ ^ ä¸è¡Œçš„å¼€å§‹ç›¸åŒ¹é…ã€‚/^ The/ è¡¨ç¤ºå•è¯åªå‡ºç°åœ¨ä¸€è¡Œçš„å¼€å§‹ï¼Œè¿™æ ·è„±å­—ç¬¦å°±æœ‰ä¸‰ç§ç”¨æ³•äº†ã€‚ ç¾å…ƒç¬¦ \\$ è¡¨ç¤ºä¸€è¡Œçš„ç»“å°¾./^ The dog\\.\\$/è¡¨ç¤ºè¿™ä¸€è¡Œåªæœ‰The dog. å…¶ä¸­ç‚¹å·å‰é¢å¿…é¡»åŠ åæ–œæ ï¼Œå› ä¸ºæˆ‘ä»¬è¦è®©å®ƒè¡¨ç¤ºç‚¹å·ï¼Œè€Œä¸æ˜¯é€šé…ç¬¦ã€‚ è¿˜æœ‰ä¸¤ä¸ªé”šå·ï¼š \\bè¡¨ç¤ºè¯ç•Œï¼Œ \\Bè¡¨ç¤ºéè¯ç•Œ éæ•°å­—ã€ä¸‹åˆ’çº¿æˆ–å­—æ¯ï¼Œå¯ä»¥çœ‹åšè¯ç•Œã€‚ Disjunction,Grouping, and Precedence æå–ï¼Œç»„åˆå’Œä¼˜å…ˆå…³ç³» æå–ç®—ç¬¦(Disjunction operator)(|),æ­£åˆ™è¡¨è¾¾å¼ /cat|dog/ è¡¨ç¤ºå­—ç¬¦ä¸²æ˜¯dogæˆ–è€…cat,å¯¹äºåç¼€guppyå’Œguppies,å¯ä»¥å†™ä½œ /gupp(y|ies)/ åœ†æ‹¬å·ç®—ç¬¦â€œ()â€.æˆ‘ä»¬çŸ¥é“ * åªèƒ½è¡¨ç¤ºå‰ä¸€ä¸ªå­—ç¬¦çš„é‡å¤ï¼Œä½†å¦‚æœè¦é‡å¤ä¸€ä¸ªå­—ç¬¦ä¸²å‘¢ï¼Œé‚£å°±å¾—ç”¨æ‹¬å·äº†ã€‚æ¯”å¦‚ /(column [0-9]+_*)*/ å°±è¡¨ç¤ºcolumnåé¢è·Ÿä¸€ä¸ªæ•°å­—å’Œä»»æ„æ•°ç›®ç©ºæ ¼çš„é‡å¤ï½ è¿ç®—ç¬¦çš„ä¼˜å…ˆçº§ æ­£å¼å› ä¸º * çš„ä¼˜å…ˆçº§é«˜äºåºåˆ—ï¼Œæ‰€ä»¥ /the* / è¡¨ç¤ºä¸theeeeeåŒ¹é…ï¼Œè€Œä¸æ˜¯ä¸thetheåŒ¹é…ã€‚ è¿˜æœ‰æ­£åˆ™è¡¨è¾¾å¼çš„åŒ¹é…æ˜¯è´ªå¿ƒçš„(greedy).æ¯”å¦‚ /[a-z]* / å¯ä»¥åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªå­—æ¯ï¼Œæ‰€ä»¥ç»“æœæ˜¯å°½å¯èƒ½é•¿çš„ç¬¦å·ä¸²ã€‚ æ€ä¹ˆè®©å®ƒä¸è´ªå¿ƒå‘¢ï¼ˆnon-greedyï¼‰ï¼Ÿ å¯ä»¥è¿™ä¹ˆå†™ /[a-z]* ?/ æˆ– /[a-z]+?/ä¼šåŒ¹é…å°½å¯èƒ½å°‘çš„ç¬¦å·ä¸²ã€‚ ä¸€ä¸ªç®€å•çš„æ —å­è¦ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°the /the/ å¹¶ä¸èƒ½æ‰¾åˆ°theä½äºå¥å­å¼€å¤´çš„æƒ…å†µThe /[tT]he/ å½“theåµŒå…¥åœ¨å…¶ä»–å•è¯ä¹‹é—´æ—¶theologyï¼Œä¹Ÿæ˜¯ä¸å¯¹çš„ /\\b[tT]he\\b/ åŠ å…¥è¯ç•Œåä¹Ÿä¸åŒ…æ‹¬the_æˆ–è€…the25äº†ï¼Œä½†å¦‚æœæˆ‘ä»¬ä¹Ÿæƒ³æ‰¾åˆ°è¿™ç§æƒ…å†µä¸­çš„theå‘¢ï¼Ÿé‚£å°±è¯´æ˜ï¼Œåœ¨theä¸¤ä¾§ä¸èƒ½å‡ºç°å­—æ¯ã€‚ /[^a-zA-Z][tT]he[^a-zA-Z]/ è¿™æ ·ä»ç„¶æœ‰é—®é¢˜ï¼Œè¿™æ„å‘³ç€å‰é¢å¿…é¡»æœ‰ä¸ªéå­—æ¯ç¬¦ã€‚æ‰€ä»¥åº”è¯¥è¿™æ ·ï¼š /(^|[^a-zA-Z])[tT]he[^a-zA-Z]/ æ›´å¤šç®—ç¬¦ é€šç”¨å­—ç¬¦é›†çš„åˆ«å(aliases) ç”¨äºè®¡æ•°ç¬¦çš„ç®—ç¬¦ éœ€è¦åŠ åæ–œæ çš„ç‰¹æ®Šç®—ç¬¦ æ­£åˆ™è¡¨è¾¾å¼ä¸­çš„æ›¿æ¢(substitution)ã€å­˜å‚¨å™¨(capture group)å’ŒELIZAs/regexp1/regexp2/ è¡¨ç¤ºç”¨ç¬¬äºŒä¸ªæ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ç¬¬ä¸€ä¸ªçš„å†…å®¹ s/colour/color/ s/([0-9]+)/&lt;\\1&gt;/ å…¶ä¸­ \\1è¡¨ç¤ºå‚ç…§ç¬¬ä¸€ä¸ªæ¨¡å¼ä¸­çš„å†…å®¹ï¼Œä¹Ÿå°±æ˜¯æ‹¬å·çš„å†…å®¹ï¼Œç„¶ååŠ ä¸Š&lt;&gt;åå¯¹å®ƒè¿›è¡Œæ›¿æ¢ã€‚å®é™…ä¸Šå°±æ˜¯æ‰¾åˆ°è¿™æ ·çš„ï¼ŒåŠ ä¸Š&lt;&gt; /the (.* )er they were, the \\1er they will be/ å¯ä»¥åŒ¹é… The bigger they were, the bigger they will be ä½†ä¸èƒ½åŒ¹é… bigger they were, the faster they will be. æ‹¬å·ä¸­ç”¨äºå­˜å‚¨çš„æ¨¡å¼å«åš capture groupï¼Œè€Œç”¨äºå­˜å‚¨çš„æ•°å­—å­˜å‚¨å™¨å«åš å¯„å­˜å™¨(register). è¿™æ ·ä¸€æ¥åœ†æ‹¬å·å°±æœ‰äº†ä¸¤ç§å«ä¹‰äº†ï¼Œå¯ä»¥ç”¨æ¥ä¼˜å…ˆçº§çš„è¿ç®—ç¬¦ï¼Œä¹Ÿå¯ä»¥ç”¨æ¥capture group. æ‰€ä»¥å¿…é¡»åŠ ä»¥åŒºåˆ«ï¼Œç”¨ ?: æ¥è¡¨ç¤º non-capturing group. (?: pattern ) ä¸¾ä¸ªæ —å­: /(?:some|a few) (people|cats) like some \\1/ å¯ä»¥ç”¨æ¥åŒ¹é… some cats like some peopleï¼Œè€Œä¸èƒ½åŒ¹é… some people like some a few. å› ä¸º\\1 è¡¨ç¤ºçš„æ˜¯(people|cats)è¿™ä¸ªæ‹¬å·ä¸­çš„å†…å®¹ã€‚ ELIZAï¼š è¿™å¯çœŸæ˜¯â€œäººå·¥â€æ™ºèƒ½å•Šã€‚ã€‚ã€‚hahha Lookahead assertionsæœ€åï¼Œæœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦é¢„æµ‹æœªæ¥look aheadï¼šåœ¨æ–‡æœ¬ä¸­å‘å‰çœ‹ï¼Œçœ‹çœ‹æœ‰äº›æ¨¡å¼æ˜¯å¦åŒ¹é…ï¼Œä½†ä¸ä¼šæ¨è¿›åŒ¹é…æ¸¸æ ‡(match cursor)ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¤„ç†æ¨¡å¼ã€‚ ä¸æ¨è¿›åŒ¹é…æ¸¸æ ‡æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ lookahead assertions ä½¿ç”¨(?=pattern)å’Œ(?!pattern). The operator (?= pattern) is true if pattern occurs, but is zero-width. è´Ÿå‘é¢„æµ‹ï¼š /(Ë†?!Volcano)[A-Za-z]+/ è¡¨ç¤º è¿™ä¸ªä¸å¤ªç†è§£ï¼Œåˆ°regex.comä¸Šè¯•äº†ä¸‹ï¼š Words and Corporaåœ¨æˆ‘ä»¬å¯¹wordè¿›è¡Œå¤„ç†æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šæ€ä¹ˆæ ·æ‰ç®—ä¸€ä¸ªword. è¯­æ–™åº“ï¼š written texts from different genres (newspaper, fiction, non-fiction, academic, etc.), Brown University in 1963â€“64 (KuÄera and Francis,1967). telephone conversations between strangersï¼Œ(Godfrey et al., 1992). disfluencies, fragment, filled pausesä¸¾ä¸ªæ —å­ï¼š I do uh main- mainly business data processing å¯¹äºè¯­å¥ä¸­å‡ºç°çš„ä¸æµåˆ©çš„åœ°æ–¹ (disfluencies). main- ç§°ä¸ºç‰‡æ®µ (fragment), åƒuhå’Œumè¿™æ ·çš„ç§°ä¸º fillers or filled pauses æˆ‘ä»¬åœ¨å¤„ç†æ–‡æœ¬çš„æ—¶å€™æ˜¯å¦éœ€è¦ä¿ç•™è¿™äº›ä¸æµåˆ©çš„åœ°æ–¹å‘¢ï¼Œè¿™å–å†³äºæˆ‘ä»¬çš„åº”ç”¨ã€‚ Disfluencies like uh or um are actually helpful in speech recognition in predicting the upcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech recognition they are treated as regular words. Because people use different disfluencies they can also be a cue to speaker identification. capitalized tokens or uncapitalized tokensthey å’Œ They æ˜¯å¦éœ€è¦å½“åšåŒä¸€ä¸ªå•è¯å¤„ç†ã€‚ æˆ‘ä»¬çŸ¥é“åœ¨ part-of-speech or named-entity tagging ä¸­é¦–å­—æ¯å¤§å†™æ˜¯å¾ˆæœ‰ç”¨çš„ç‰¹å¾ï¼Œè¿™éœ€è¦ä¿ç•™ä¸‹æ¥ã€‚ lemmaè¯æ„ and wordform ä¸€å¥è¯ä¸­çš„WORDå¯ä»¥ç”¨ä¸¤ç§ä¸åŒçš„æ ‡å‡†æ¥åŒºåˆ†ã€‚ä¸€ç§æ˜¯Lemmaï¼Œä¸€ç§æ˜¯wordformã€‚ wordformå°±æ˜¯è¯çš„å½¢çŠ¶ï¼Œè€Œlemmaåˆ™æ˜¯è¯æ„ã€‚æ¯”å¦‚ am is are ï¼Œéƒ½æ˜¯ä¸€ä¸ªlemmaï¼Œä½†æ˜¯3ä¸ªwordformã€‚åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­ï¼Œéœ€è¦å°†lemmatizationï¼Œå¯èƒ½å› ä¸ºä»–ä»¬åŒä¸€ä¸ªè¯æ„ï¼Œèƒ½ç”¨çš„è¯å¤ªå¤šäº†å§ï¼Œæˆ‘è®°å¾—çœ‹å“ªä¸ªè§†é¢‘çš„æ—¶å€™è¯´è¿‡éª†é©¼ï¼Œæœ‰å››åå¤šç§ã€‚ã€‚ã€‚å¯¹äºè‹±è¯­çš„è¯ï¼Œwordformå°±å¤Ÿäº†ã€‚ word type è¯å‹ and word token è¯ä¾‹å€˜è‹¥ä»¥wordform è¯å½¢çš„å½¢å¼æ¥ç•Œå®šä¸€ä¸ªè¯ï¼Œé‚£ä¹ˆä¸€å¥è¯ä¸­WORDçš„æ•°ç›®è¿˜å¯ä»¥ç”¨ä¸¤ç§ä¸åŒçš„æ ‡å‡†æ¥åŒºåˆ†ã€‚Typeæ˜¯ç›¸åŒçš„è¯éƒ½ç®—ä¸€ä¸ªï¼ŒTokenæ˜¯æ¯ä¸ªè¯å‡ºç°å‡ æ¬¡éƒ½ç®—ã€‚æ‰€ä»¥ â€œno no no â€¦. it is not possibleâ€ è¿™æ ·çš„ä¸€å¥è¯ï¼ŒType æœ‰5ä¸ªï¼ŒToken æœ‰7ä¸ªã€‚TokenåŒ…æ‹¬é‡å¤è¯ã€‚ å…¶ä¸­ Tokens N å’Œ types |V| æœ‰è¿™æ ·çš„å…³ç³»ï¼š $$|V|=kN^{\\beta}$$ $\\beta$ å–å†³äºè¯­æ–™åº“çš„å¤§å°(size)å’Œç±»å‹(genre).å½“è¯­æ–™åº“è‡³å°‘æœ‰ä¸Šå›¾ä¸­çš„å¤§å°æ—¶ï¼Œ $\\beta$ çš„å€¼çš„å¤§å°ä¸º0.67åˆ°0.75ä¹‹é—´ã€‚ Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words. å¦å¤–ä¸€ç§æ˜¯ä»¥lemmasæ¥ç•Œå®šä¸€ä¸ªè¯ï¼Œè€Œä¸æ˜¯wordform. æ–‡æœ¬æ ‡å‡†åŒ– Text Normalizationåœ¨è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ä¹‹å‰ï¼Œéƒ½éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚ Segmenting/tokenizing words from running text åˆ†è¯ Normalizing word formats å•è¯æ ¼å¼å½’ä¸€åŒ– Segmenting sentences in running text. å¥å­åˆ†å‰² Unix tools for crude tokenization and normalizationä»‹ç»äº†ä¸€ä¸ªLinuxå‘½ä»¤ tr å¯ç”¨æ¥ç»Ÿè®¡è¯é¢‘ ä½†è¿™ä¸ªç»Ÿè®¡éå¸¸ç®€å•ç²—æš´ï¼Œå»æ‰äº†æ‰€æœ‰çš„æ ‡ç‚¹ç¬¦å·å’Œæ•°å­— Word Tokenization and Normalizationä»‹ç»äº†æ ‡ç‚¹ç¬¦å·åœ¨å¾ˆå¤šåœ°æ–¹çš„ç”¨é€”: Ph.D,m.p.hâ€¦ æ—¶é—´(09/04/18)..ç­‰ç­‰ email, urls clitic contractions by apostrophes. ç”¨â€™å·è¡¨ç¤ºçš„ç¼©å†™ whatâ€™re,weâ€™re æ ¹æ®åº”ç”¨ä¸åŒï¼Œtokenizeä¹Ÿä¼šä¸åŒï¼Œæ¯”å¦‚New Yorké€šå¸¸ä¹Ÿä¼šæ ‡è®°ä¸ºä¸€ä¸ªè¯ã€‚åœ¨ name entity detection ä¸­Tokenizationä¼šå¾ˆæœ‰ç”¨ã€‚ tokenize standard: Penn Treebank tokenization standard ç”±Linguistic Data Consortium(LDC)å‘å¸ƒã€‚ case folding: everything is mapped to lower case. åœ¨è¯­éŸ³è¯†åˆ«å’Œä¿¡æ¯æ£€ç´¢ä¸­ä¼šæ¯”è¾ƒå¸¸ç”¨ã€‚ ä½†æ˜¯åœ¨sentiment anal- ysis and other text classification tasks, information extraction, and machine transla- tion ä¸­å¤§å°å†™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œå› æ­¤é€šå¸¸ä¸ä¼šä½¿ç”¨case folding. ä¸‹ä¸€ç« ä¸­çš„æœ‰é™çŠ¶æ€è‡ªåŠ¨æœº finite state automata å°±æ˜¯ç”¨åŸºäºæ­£åˆ™è¡¨è¾¾å¼åˆ¤åˆ«ç®—æ³•ç¼–è¯‘è€Œæˆçš„ã€‚ ä¸­æ–‡è¯åˆ†å‰²ï¼šmaximum matching/MaxMatch æœ€å¤§åŒ¹é…ç®—æ³•ä¸€ç§è´ªå¿ƒç®—æ³•ï¼Œéœ€è¦ä¸€ä¸ªå­—å…¸(dictionary/wordlist)è¿›è¡ŒåŒ¹é…. ä¼ªä»£ç ï¼š ä»£ç å‚è€ƒï¼šhttp://www.cnblogs.com/by-dream/p/6429615.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;//å®ï¼Œè®¡ç®—æ•°ç»„ä¸ªæ•°#define GET_ARRAY_LEN(array, len){len=sizeof(array)/sizeof(array[0]);}string dict[] = {&quot;è®¡ç®—&quot;,&quot;è®¡ç®—è¯­è¨€å­¦&quot;,&quot;è¯¾ç¨‹&quot;,&quot;æœ‰&quot;,&quot;æ„æ€&quot;};//æ˜¯å¦ä¸ºè¯è¡¨ä¸­çš„è¯æˆ–è¯è¡¨ä¸­çš„å‰ç¼€bool inDict(string str){ bool res = false; int i; int len = 0; GET_ARRAY_LEN(dict, len); for (i=0; i&lt;len; i++){ if (str == dict[i].substr(0, str.length())) { res = true; } } return res;}int main(){ string sentence = &quot;è®¡ç®—è¯­è¨€å­¦è¯¾ç¨‹æœ‰æ„æ€&quot;; string word = &quot;-&quot;; int wordlen = word.length(); // 1 int i; string s1 = &quot;&quot;; for (i=0; (unsigned)i&lt;sentence.length(); i += wordlen) { string tmp = s1 + sentence.substr(i, wordlen); //æ¯æ¬¡å¢åŠ ä¸€ä¸ªè¯ if (inDict(tmp)) { s1 = s1 + sentence.substr(i, wordlen); } else // å¦‚æœä¸åœ¨è¯è¡¨ä¸­ï¼Œå…ˆæ‰“å°å‡ºä¹‹å‰çš„ç»“æœï¼Œç„¶åä»ä¸‹ä¸€ä¸ªè¯å¼€å§‹ { cout &lt;&lt; &quot;åˆ†è¯ç»“æœï¼š&quot; &lt;&lt; s1 &lt;&lt; endl; s1 = sentence.substr(i, wordlen); } } cout &lt;&lt; &quot;åˆ†è¯ç»“æœï¼š&quot; &lt;&lt; s1 &lt;&lt; endl;} å¦‚æœè¯è¡¨è¶³å¤Ÿå¤§çš„è¯ï¼Œå°±å¯ä»¥å¯¹æ›´å¤šçš„å¥å­è¿›è¡Œåˆ†è¯äº†ã€‚ æˆ‘ä»¬ç”¨ä¸€ä¸ªæŒ‡æ ‡æ¥é‡åŒ–åˆ†è¯å™¨çš„å‡†ç¡®ç‡ï¼Œç§°ä¸º word error rate. æ€ä¹ˆè®¡ç®—word error rate:é€šè¿‡è®¡ç®—æœ€å°ç¼–è¾‘è·ç¦» We compare our output segmentation with a perfect hand-segmented (â€˜goldâ€™) sentence, seeing how many words differ. The word error rate is then the normalized minimum edit distance in words between our output and the gold: the number of word insertionsæ’å…¥, deletionsåˆ é™¤, and substitutionsæ›¿æ¢ divided by the length of the gold sentence in words. ä½œè€…è¿˜æåˆ°æœ€å‡†ç¡®çš„ä¸­æ–‡åˆ†è¯ç®—æ³•æ˜¯é€šè¿‡ç›‘ç£å­¦ä¹ è®­ç»ƒçš„ç»Ÿè®¡ sequence models, åœ¨chapter 10ä¸­ä¼šè®²åˆ°ã€‚ Lemmatization and Stemming è¯å½¢è¿˜åŸå’Œè¯å¹²æå–Lemmatizationï¼š è¯å½¢è¿˜åŸï¼Œam, isï¼Œareæœ‰å…±åŒçš„è¯å…ƒ(Lemma)ï¼šbe ä¸¾ä¾‹è¯´æ˜ï¼š He is reading detective stories. â€“&gt; He be read detective story. é‚£ä¹ˆlemmatizationæ˜¯æ€ä¹ˆå®ç°çš„å‘¢ï¼Ÿ The most sophisticated methods for lemmatization involve complete morphological parsing(å½¢æ€è§£æ) of the word. morphological parsingä¼šåœ¨chapter3ä¸­è®²åˆ°ã€‚ Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes(è¯­ç´ ). è¯­ç´ åŒ…æ‹¬ä¸¤ç±»ï¼š stemsï¼šè¯å¹² affixes: è¯ç¼€ å…³äºè¯å½¢è¿˜åŸçš„å·¥å…·ï¼šblogï¼Œè¯å½¢è¿˜åŸå·¥å…·å¯¹æ¯” Python: NLTK Python: Pattern Python: TextBlob Tree Tagger The Porter Stemmeré€šå¸¸æˆ‘ä»¬ç”¨finite-state transducers æ¥å¤„ç† morphological parser,ä½†æˆ‘ä»¬æœ‰æ—¶å€™ä¹Ÿä¼šä½¿ç”¨ç®€å•ç²—æš´çš„å»æ‰è¯ç¼€çš„æ–¹æ³• stemming. è¿™é‡Œä½œè€…å°±ä»‹ç»äº†ä¸€ç§è¿™æ ·çš„ç®—æ³• Poster algorithm. ç®—æ³•çš„åŸç†ä¸»è¦æ˜¯åŸºäºä¸€äº›è§„åˆ™ cascade. Sentence Segmentation å¥å­åˆ†å‰²ä¸»è¦æ˜¯ç”¨æ ‡ç‚¹ç¬¦å·å•¦ï½ æ¯”è¾ƒunambiguousçš„æ ‡ç‚¹ç¬¦å·æœ‰ï¼šQuestion marks and exclamation points è€ŒPeriodså°±æ¯”è¾ƒambiguousäº†ã€‚ å…·ä½“çš„å¥å­åˆ†å‰²ç®—æ³•å¢é¢chapterä¼šè®²åˆ° Minimum Edit DIstance æœ€å°ç¼–è¾‘è·ç¦»ç”¨æ¥è¡¨ç¤ºä¸¤ä¸ªå¥å­ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ deletion åˆ é™¤ï¼š cost 1 insertion æ’å…¥ï¼š cost 1 substitution æ›¿æ¢ï¼š cost 2 The Minimum Edit Distance Algorithmä¸€ç§åŠ¨æ€è§„åˆ’çš„ç®—æ³•ã€‚ dynamic programming,Bellman, R. (1957). Dynamic Programming. Princeton University Press. that apply a table-driven method to solve problems by combining solutions to sub-problems. source string X[1â€¦iâ€¦n] target string Y[1â€¦jâ€¦m] ç”¨D(i,j)æ¥å®šä¹‰Xä¸­å‰iä¸ªå­—ç¬¦åˆ°Yä¸­å‰jä¸ªå­—ç¬¦çš„ç¼–è¾‘è·ç¦»ï¼Œé‚£ä¹ˆXåˆ°Yçš„ç¼–è¾‘è·ç¦»å°±æ˜¯D(n,m) è®¡ç®—D[i,j],ä¹Ÿå°±æ˜¯é€’æ¨æœ‰ä¸‰ç§æ–¹å¼ï¼š å®šä¹‰cost: åˆå§‹æƒ…å†µï¼š D(i,0) = iï¼Œä¹Ÿå°±æ˜¯ source substring of length i but an empty target string D(o,j) = jï¼Œä¹Ÿå°±æ˜¯ With a target substring of length j but an empty source é‚£ä¹ˆä¼ªä»£ç ï¼š 1234567891011121314151617181920212223242526272829303132333435# åˆ›å»ºçŸ©é˜µ[n+1,m+1]D = np.zeros(n+1, m+1)# 1. Initialization:D[0,0] = 0for each row i for i to n: D[i,0] = D[i-1] + 1for each column j from 1 to m: D[0,j] = D[0,i-1] + 1# 2. Recurrence:for each row i from 1 to n: for each column j from 1 to m: D[i,j] = min(D[i-1,j]+1, D[i-1,j]+1, D[iâˆ’1, jâˆ’1]+2)# 3. Termination:return D[n,m] æˆ‘ä»¬çŸ¥é“äº†æœ€å°ç¼–è¾‘è·ç¦»æ˜¯å¤šå°‘ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æƒ³çŸ¥é“æœ€å°ç¼–è¾‘è·ç¦»å¯¹åº”çš„ä¸¤ä¸ªå­—ç¬¦ä¸²å¯¹é½æ–¹å¼ alignment.æ®è¯´alignmentåœ¨è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ä¸­å¾ˆæœ‰ç”¨ï½ æœ€å°ç¼–è¾‘è·ç¦»å’Œviterbiç®—æ³•ã€å‰å‘ç®—æ³•å¾ˆç›¸ä¼¼ã€‚ æœ€å°ç¼–è¾‘è·ç¦»ï¼šé€’æ¨ä¸€æ­¥æœ‰ä¸‰ç§é€‰æ‹©æ–¹å¼ï¼Œç„¶åå–æœ€å°å€¼ã€‚æ¯ä¸€æ­¥ä¸­ä¸‰ç§æ–¹å¼çš„æƒé‡weightä¹Ÿæ˜¯æœ‰æ„ä¹‰çš„ã€‚ Viterbiç®—æ³•ï¼šé€’æ¨ä¸€æ­¥æœ‰Nä¸ªè·¯å¾„ï¼Œç„¶åå–maxï¼Œå¯ä»¥çœ‹åšæœ€å°ç¼–è¾‘è·ç¦»çš„æ‹“å±•ï¼Œæƒé‡åœ¨è¿™é‡Œå°±æ˜¯æ¦‚ç‡ã€‚ å‰å‘ç®—æ³•ï¼šé€’æ¨æ¯ä¸€æ­¥æœ‰Nä¸ªè·¯å¾„ï¼Œç„¶åå–sum. å…¶ä¸­æœ€å°ç¼–è¾‘è·ç¦»å’ŒViterbiç®—æ³•æœ‰ backtrace. åŒæ ·çš„ï¼Œåœ¨å‰å‘é€’æ¨çš„è¿‡ç¨‹ä¸­å¡«è¡¨ï¼š å¡«è¡¨çš„è¿‡ç¨‹å°±æ˜¯ä»D(0,0)å¼€å§‹ï¼Œæ¯è¿›å…¥ä¸€ä¸ª boldfaced cell(é™¤äº†ç¬¬0è¡Œå’Œç¬¬0åˆ—)éƒ½æœ‰ä¸‰ç§é€‰æ‹©ï¼Œç„¶åé€‰æ‹©æœ€å°çš„ã€‚ è®¡ç®— alignment pathï¼Œåˆ†ä¸ºä¸¤æ­¥éª¤ï¼š åœ¨ç®—æ³•è®¡ç®—çš„è¿‡ç¨‹ä¸­ï¼Œå­˜å‚¨åæŒ‡é’ˆbackpointer backtraceï¼šä»æœ€åä¸€è¡Œæœ€åä¸€åˆ—çš„cellå¼€å§‹ï¼Œæ²¿ç€æŒ‡é’ˆï¼Œæ¯ä¸€æ­¥éƒ½æ˜¯æœ€å°çš„ã€‚ æ€»ç»“ï¼š ä»‹ç»äº†å„ç§æ­£åˆ™è¡¨è¾¾å¼ ç”¨ - è¡¨ç¤ºrange è„±å­—ç¬¦ ^ çš„ä¸‰ç§ç”¨æ³•ï¼šè‡ªèº«ï¼Œæ–¹æ‹¬å·ä¸­çš„å¦å®šï¼Œä¸è¡Œå¼€å¤´åŒ¹é… é—®å· ? è¡¨ç¤ºå‰ä¸€ä¸ªå­—ç¬¦æ˜¯å¯é€‰çš„ è¡¨ç¤ºå‰ä¸€ä¸ªå­—ç¬¦é›¶ä¸ªæˆ–å¤šä¸ªï¼Œ + è¡¨ç¤ºå‰ä¸€ä¸ªå­—ç¬¦ä¸€ä¸ªæˆ–å¤šä¸ª . è¡¨ç¤ºé€šé…ç¬¦ï¼ŒåŒ¹é…ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼Œ/.* /åŒ¹é…ä»»æ„é•¿åº¦å­—ç¬¦ï¼Œä¸”è´ªå¿ƒçš„ é”šå· ^ å’Œ \\$ åŒ¹é…è¡Œå¼€å¤´å’Œç»“å°¾ é”šå· \\bå’Œ\\B è¯ç•Œå’Œéè¯ç•Œ æå–ï¼Œç»„åˆå’Œä¼˜å…ˆå…³ç³» ä¸»è¦æ˜¯æå–ç®—ç¬¦|å’Œåœ†æ‹¬å·()çš„ç”¨æ³•ï¼Œä»¥åŠè¿ç®—ç¬¦ä¼˜å…ˆçº§ æ›¿æ¢å’Œå¯„å­˜å™¨ s/regexp1/regexp2/ \\1 åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„åˆ†è¯å’Œæ–‡æœ¬æ ‡å‡†åŒ– ç”¨äºè¯å¹²æå–stemmingçš„ç®€å•ç²—æš´çš„ç®—æ³• Porter algorithm ç”¨äºæè¿°å­—ç¬¦ä¸²ç›¸ä¼¼åº¦çš„ç®—æ³•ï¼Œæœ€å°ç¼–è¾‘è·ç¦» leetcodeä¸Šæœ‰ä¸ªç¼–è¾‘è·ç¦»çš„é¢˜ç›®ï¼šhttps://leetcode.com/problems/edit-distance/description/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class Solution {public: int minDistance(string word1, string word2) { int n = word1.length(); int m = word2.length(); int a[n+1][m+1]; a[0][0] = 0; for (int i=1; i&lt;=n; i++){ a[i][0] = a[i-1][0] + 1; } for (int j=1; j&lt;=m; j++){ a[0][j] = a[0][j-1] + 1; } for (int i=1; i&lt;=n; i++){ for (int j=1; j&lt;=m; j++){ if (word1[i-1] != word2[j-1]){ int tmp = min(a[i-1][j-1] + 1, a[i-1][j] + 1); a[i][j] = min(tmp, a[i][j-1] + 1); } else { a[i][j] = a[i-1][j-1]; } } } return a[n][m]; }};","link":"/2018/04/09/chapter2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E3%80%81%E6%96%87%E6%9C%AC%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"title":"chapter4:è¯­è¨€æ¨¡å‹å’ŒNå…ƒè¯­æ³•","text":"N-gram: Markov chainæ˜¯å…¶ä¸­ä¸€ä¸ªç‰¹ä¾‹ éªŒè¯N-gramæ¨¡å‹ï¼š å›°æƒ‘åº¦ é¢„å¤„ç†ï¼šæ³›åŒ–å’Œzerosæƒ…å†µ smoothingï¼šç”¨æ¥å¤„ç†zerosï½ å›°æƒ‘åº¦å’Œäº¤å‰ç†µ å‰è¨€: é€šè¿‡å®ä¾‹ï¼Œç®€å•çš„ä»‹ç»äº†assign probabilities to sequences of words çš„é‡è¦æ€§ï¼Œä»¥åŠåœ¨è¿™äº›æ–¹é¢çš„åº”ç”¨ï¼š speech recognition, handwriting recognition, spelling correction, augmentative communication. ç»™å¥å­æˆ–æ˜¯è¯è¯­èµ‹äºˆæ¦‚ç‡çš„æ¨¡å‹å°±æ˜¯è¯­è¨€æ¨¡å‹(language models,LM)ï¼Œè¿™ä¸ªchapterä¸»è¦ä»‹ç»ä¸€ä¸ªç®€å•çš„è¯­è¨€æ¨¡å‹N-grams. 2-gram(bigram), 3-gram(trigram). N-gramsä¸è®ºæ˜¯æ•´ä¸ªå¥å­çš„æ¦‚ç‡è¿˜æ˜¯ä¸€ä¸ªåºåˆ—ä¸­é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡ï¼Œéƒ½è¦ä½¿ç”¨æ¦‚ç‡æ¨¡å‹ã€‚ è®¡ç®—ä¸€ä¸ªå®Œæ•´çš„å¥å­çš„æ¦‚ç‡ $P(w_1,w_2,â€¦,w_n)?$ ï¼Œæˆ‘ä»¬ç”¨ $w_1â€¦w_n æˆ– w_1^n$ æ¥è¡¨ç¤ºNä¸ªå•è¯ç»„æˆçš„å¥å­ã€‚å¯ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¦‚ç‡ï¼š $$P(w_1â€¦w_n) = P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)â€¦P(w_n|w_1^{n-1})$$ $$P(w_1â€¦w_n) = \\prod_{k=1}^nP(w_k|w_1^{k-1})$$ ä½†æ˜¯ $P(w_n|w_1^{n-1})$ å¾ˆéš¾è®¡ç®—,å› ä¸ºï¼š è¿™æ ·æˆ‘ä»¬å¼•å…¥äº†N-gramæ¨¡å‹ï¼Œå°±æ˜¯è€ƒè™‘é¢„æµ‹å•è¯ä¹‹å‰çš„å°‘æ•°å•è¯ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„æ‰€æœ‰è¯ã€‚æ¯”å¦‚ bigram å°±æ˜¯2-gramæ¨¡å‹ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªå•è¯åªä¾èµ–äºå‰ä¸€ä¸ªå•è¯ï¼š $$P(w_n|w_1^{n-1}) = P(w_n|w_{n-1})$$ ä¹Ÿå°±æ˜¯ Markovå‡è®¾ï½é’ˆå¯¹è¯­è¨€æ¥è¯´å¯èƒ½ä¸å¤ªåˆç†å§ï¼Œæ¯•ç«Ÿè¯­è¨€ä¸€ä¸¤ä¸ªè¯åŒ…å«çš„ä¿¡æ¯å¹¶ä¸å¤šï¼Œä½†å¯¹äºå¾ˆå¤šå…¶ä»–çš„ï¼Œæ¯”å¦‚å¤©æ°”ï¼Œåªä¾èµ–äºå‰ä¸¤å¤©æ˜¯å¾ˆé è°±çš„å¯¹å§ï½ é‚£ä¹ˆæ€ä¹ˆä¼°è®¡bigramå’ŒN-gramçš„æ¦‚ç‡ï¼Œä¸€ä¸ªç›´æ¥çš„æ–¹æ³•æ˜¯ æå¤§ä¼¼ç„¶ä¼°è®¡(maximum likelihood estimation, MLE). $$P(w_n|w_{n-1}) = \\dfrac{C(w_{n-1}w_n)}{C(w_{n-1})}$$ å…¶ä¸­æˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªå¥å­åŠ ä¸Šç‰¹æ®Šçš„begin-symbol&lt; s &gt; å’Œ end-symbol &lt; /s &gt;. å¯¹äºä¸€èˆ¬çš„Nå…ƒè¯­æ³•ï¼Œå…¶å‚æ•°ä¼°è®¡ä¸ºï¼š $$P(w_n|w_{n-N+1}^{n-1}) = \\dfrac{C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}$$ è¿™é‡Œå‰é¢N-1ä¸ªå•è¯åŒæ—¶å‡ºç°çš„é¢‘åº¦æ¥é™¤æ•´ä¸ªåºåˆ—å‡ºç°çš„é¢‘åº¦ï¼Œè¿™ä¸ªæ¯”å€¼ç§°ä¸ºç›¸å¯¹é¢‘åº¦(relative frequency).åœ¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸­ï¼Œç›¸å¯¹é¢‘åº¦æ˜¯æ¦‚ç‡ä¼°è®¡çš„ä¸€ç§æ–¹æ³•ï¼Œå³åœ¨ç»™å®šæ¨¡å‹Mçš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å¾—åˆ°çš„æ¨¡å‹å‚æ•°èƒ½ä½¿å¾—è®­ç»ƒé›†Tçš„ä¼¼ç„¶åº¦P(T|M)æœ€å¤§ã€‚ Some pratical issues: é€šå¸¸ä½¿ç”¨ trigranï¼Œ4-gram,ç”šè‡³æ˜¯5-gramæ•ˆæœè¦æ¯”bigramæ›´å¥½ã€‚ä½†è¿™éœ€è¦æ›´å¤§çš„è¯­æ–™åº“ã€‚ ç”±äºç›¸å¯¹æ¦‚ç‡éƒ½æ˜¯å¤§äº0å°äº1çš„,æ¦‚ç‡ç›¸ä¹˜ä¹‹åä¼šæ›´å°ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•°å€¼ä¸‹æº¢(numerical underflow)ã€‚å› æ­¤é‡‡ç”¨å¯¹æ•°æ¦‚ç‡(log probabilities). Evaluating Language Modelså¤–åœ¨è¯„ä¼°, extrinsic evaluation . end-to-end evaluationï¼Œ åº”ç”¨åˆ°å®é™…ï¼Œç„¶åå»è¯„ä¼°æ¨¡å‹çš„å¥½åï¼Œä½†è¿™å¤ªéº»çƒ¦äº†ã€‚ å†…åœ¨è¯„ä¼°, metric: intrinsic evaluation å†…åœ¨è¯„ä¼°: : training set or training corpus. development test set or devset: fresh test set often: 80% training, 10% development, 10% test. Perplexity è¿™é‡Œç®€å•çš„æäº†ä¸€ä¸‹perplexity(å›°æƒ‘åº¦)ï¼šperplexity is a normalized version of the probability of the test setï¼Œå¯ä»¥ç†è§£ä¸º weight average barnching factor. barnching factoræ˜¯æŒ‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å‡ºç°çš„æ¦‚ç‡ã€‚ åœ¨åé¢ç»“åˆä¿¡æ¯ç†µè¿˜ä¼šå†æ¬¡è®²åˆ°ã€‚è¿™é‡Œå¯ä»¥å…ˆç®€å•çš„ç†è§£ä¸ºï¼š æµ‹è¯•é›†çš„æ¦‚ç‡ $P(w_1w_2â€¦w_N)$ è¶Šå¤§ï¼Œä¹Ÿå°±æ˜¯å‡†ç¡®ç‡è¶Šé«˜ï¼Œå…¶å›°æƒ‘åº¦å°±è¶Šå°ã€‚ åŒæ—¶ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼ŒN-gramä¸­Nè¶Šå¤§ï¼Œæ¨¡å‹æå–çš„ä¿¡æ¯è¶Šå¤šï¼Œå…¶å›°æƒ‘åº¦å°±è¶Šå°ã€‚ è¯´åˆ°è¿™å„¿ï¼ŒNå…ƒè¯­æ³•åˆ°åº•ä»å¥å­ä¸­æå–åˆ°äº†ä»€ä¹ˆä¿¡æ¯ï¼Ÿ æ€»ç»“ä¸‹æ¥ï¼Œè¿˜æ˜¯ä¸€ä¸ªæ¦‚ç‡æ€§çš„é—®é¢˜ã€‚ä»€ä¹ˆè¯æ›´å®¹æ˜“å‡ºç°åœ¨ä»€ä¹ˆè¯èº«åï¼Œeatåé¢æ›´å®¹æ˜“å‡ºç°åè¯å’Œå½¢å®¹è¯ï¼Œè¿™æ˜¯å¥æ³•(syntactic)ä¿¡æ¯;åœ¨å¯¹è¯ä¸­ï¼ŒI æ›´å®¹æ˜“å‡ºç°åœ¨å¥é¦–ï¼› è¿˜æœ‰æ–‡åŒ–ç›¸å…³çš„ï¼Œäººä»¬looking for Chinses foodçš„æ¦‚ç‡æ¯”english foodå¤§ã€‚ Generalization and Zeros The N-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a given training corpus. Another implication is that N-grams do a better and better job of modeling the training corpus as we increase the value of N. the value of N éšç€Nçš„å¢åŠ ï¼Œå¤–åœ¨è¯„ä¼°æ•ˆæœä¹Ÿè¶Šæ¥è¶Šå¥½ã€‚ä½†åœ¨4-gramä¸­ it cannot be but so.è¿™ä¸ªæ˜¯ç›´æ¥ä» king henry ä¸­å¾—åˆ°çš„ï¼ŒåŸå› æ˜¯å› ä¸ºåœ¨èå£«æ¯”äºšæ–‡é›†ä¸­ï¼Œit cannot be butä¸‹é¢å¯æŒç»­çš„å•è¯åªæœ‰5ä¸ª(that, I, he, thou, so). æ‰€ä»¥ï¼Œè¯­æ–™åº“ç›¸å¯¹4-gramå¤ªå°äº†ã€‚ åœ¨å›é¡¾ä»¥ä¸‹æ•´ä¸ªè¿‡ç¨‹ï¼š åœ¨training dataä¸­ï¼Œå¯¹è¯­æ–™åº“ä¸­çš„å¥å­åŠ ä¸Š&lt;\\s&gt;ï¼ˆä¸éœ€è¦åŠ  &lt; s &gt;ï¼‰,è¿™é‡Œå¹¶æ²¡æœ‰è®­ç»ƒï¼Œåªæ˜¯è®¡ç®—äº†å¯¹åº”çš„unigram, bigram, trigramçš„æ¦‚ç‡. ç„¶ååœ¨test dataä¸Šè¿›è¡ŒéªŒè¯ï¼Œæ¥åˆ¤æ–­å“ªä¸ªæ¨¡å‹å¥½ï¼Œé‚£ä¸ªåï¼Ÿè¿™æ˜¯å†…åœ¨è¯„ä¼° ç„¶åå¤–åœ¨è¯„ä¼°å°±æ˜¯æ¯ä¸€æ¬¡randomä¸€ä¸ªè¯ï¼Œåœ¨è¿™ä¸ªåŸºç¡€ä¸Šæ ¹æ®æ¦‚ç‡randomä¸‹ä¸€ä¸ªè¯ï¼Œç›´åˆ°ç”Ÿæˆ&lt;\\s&gt;. the corpusä¸åŒçš„è¯­æ–™åº“è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ï¼Œç”Ÿæˆå¾—åˆ°çš„åºåˆ—ä¼šç›¸å·®å¾ˆè¿œã€‚æ€ä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿ How should we deal with this problem when we build N-gram models? One way is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish. To build a language model for translating legal documents, we need a training corpus of legal documents. To build a language model for a question-answering system, we need a training corpus of questions. zeros è¿™é‡Œæ˜¯ä¸ªå¤§å‘æµ‹è¯•é›†ä¸­å‡ºç°äº†è®­ç»ƒé›†ä¸­æ²¡æœ‰å‡ºç°çš„è¯ï¼Œæˆ–è€…äºŒå…ƒç»„ï¼Œè¿™é‡Œä¸æ˜¯,å¯èƒ½offeråœ¨å…¶ä»–åœ°æ–¹å‡ºç°è¿‡ï¼Œä½†åœ¨denied theåé¢å‡ºç°çš„æ¬¡æ•°ä¸º0ï¼Œé‚£ä¹ˆå®ƒçš„æ¦‚ç‡å°±æ˜¯0ï¼Œç„¶åå›°æƒ‘åº¦perplexityå°±æ— ç©·å¤§äº†ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚å¦‚ä¸‹å›¾ä¸­æ‰€ç¤ºã€‚ è¿™æ˜¯ä¸ªå¾ˆé‡è¦çš„é—®é¢˜ï¼åé¢smoothingç®—æ³•éƒ½æ˜¯åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼ Unknow Wordsclosed vocabulary:æµ‹è¯•é›†ä¸­çš„è¯ä¹Ÿéƒ½æ˜¯æ¥è‡ªäºè¯åº“çš„(lexicon),æ¯”å¦‚è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ï¼Œå¹¶ä¸ä¼šå‡ºç°unknownè¯ã€‚ ä½†æˆ‘ä»¬ä¹Ÿä¼šéœ€è¦å¤„ç†ä¸€äº›æˆ‘ä»¬æ²¡æœ‰è§è¿‡çš„è¯ **unknow or out of vocabulary(OOV)**ï¼Œ æˆ‘ä»¬é€šè¿‡ç»™ open vocabulary å¢åŠ ä¸€ä¸ª pseudo-word . è§£å†³è¿™ä¸ªé—®é¢˜æœ‰ä¸¤ç§å¸¸è§çš„æ–¹æ³•ï¼š ç¬¬ä¸€ç§å°±æ˜¯æŠŠé—®é¢˜è½¬æ¢ä¸º closed vocabularyï¼Œä¹Ÿå°±æ˜¯æå‰å»ºç«‹ä¸€ä¸€ä¸ªè¯æ±‡è¡¨ è¯è¯´æˆ‘ä¸æ˜¯å¤ªç†è§£è¿™ä¸ªæ–¹æ³•ï¼Œåº”è¯¥æ˜¯åªå‡ºç°åœ¨test dataä¸­å§ï¼Œé‚£ä¹ˆåœ¨train dataä¸­çš„æ¦‚ç‡å°±æ˜¯0å•Šï¼Œé‚£ä¹ˆå¾—åˆ°çš„æ¨¡å‹çš„æ¦‚ç‡ä¹Ÿæ˜¯0å•Šï¼Œé‚£ä¹ˆåœ¨testæ—¶ï¼Œé¦–å…ˆå®ƒè‚¯å®šä¸ä¼šç”Ÿæˆ,è€Œä¸”ç¢°åˆ°æ—¶ï¼Œä¸‹ä¸€ä¸ªè¯æ€ä¹ˆç”Ÿæˆã€‚ã€‚ã€‚ã€‚ ç¬¬äºŒç§æ–¹æ³•è¿˜å¥½ç†è§£ä¸€ç‚¹ï¼Œå°±æ˜¯åœ¨train dataä¸­æŠŠå‡ºç°å­—æ•°è¾ƒå°‘çš„è¯å½“åšï¼Œé‚£ä¹ˆè¿™ä¸ªæ—¶å€™å°±åƒæ™®é€šçš„è¯ä¸€æ ·ä¹Ÿæ˜¯æœ‰å®ƒçš„æ¦‚ç‡çš„ã€‚ä½†æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼š it takes some time to be convinced. Smoothing å¹³æ»‘ä¸»è¦æ˜¯é’ˆå¯¹å‰é¢é‚£ä¸ªå‘çš„ï¼Œå°±æ˜¯zerosçš„æƒ…å†µã€‚ä¸æ˜¯ï¼Œä½†æ˜¯è¿™ä¸ªäºŒå…ƒç»„æˆ–è€…æ˜¯ä¸‰å…ƒç»„åœ¨test dataä¸­å‡ºç°äº†ï¼Œåœ¨train dataä¸­ä»æ¥æ²¡æœ‰å‡ºç°è¿‡ã€‚ è§£å†³åŠæ³•å°±æ˜¯ï¼šshave off a bit of probability mass from some more frequent events and give it to the events weâ€™ve never seen. This modification is called smoothing or discounting. æœ‰ä»¥ä¸‹è¿™äº›æ–¹æ³•ï¼š**add-1 smoothing, add-k smoothing, Stupid backoff, and Kneser-Ney smoothing.** Laplace Smoothing æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘äº‹å®ä¸Šï¼Œå…³äºå‰é¢ä¸€éƒ¨åˆ†æå¤§ä¼¼ç„¶ä¼°è®¡å’Œlaplace smoothingè¿™ä¸€æ®µæ²¡çœ‹å¤ªæ‡‚ã€‚ä½†æ˜¯æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ•´ä½“è¿˜æ˜¯èƒ½å¤Ÿç†è§£çš„ã€‚ å¯¹äºbigram: ä¸‹å›¾åˆ†åˆ«æ˜¯ä¼¯å…‹åˆ©é¤é¦†å¯¹è¯çš„è¯­æ–™åº“(V=1446)ä»¥åŠåŠ 1å¹³æ»‘åçš„bigramï¼š å¯ä»¥çœ‹åˆ°ï¼Œåˆ†å­æ˜¯æ‰€æœ‰çš„é›¶å’Œéé›¶é¡¹éƒ½åŠ 1ï¼Œæ‰€ä»¥å¯¹äºåˆ†æ¯C(w_{n-1})æ¥è¯´ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢äºŒç»´æ•°ç»„ä¸­æŸä¸€è¡Œéƒ½åŠ 1ï¼Œä¹Ÿå°±æ˜¯å¢åŠ äº†ä¸€ä¸ªè¯è¡¨çš„æ•°é‡Vã€‚ç†è®ºä¸Šæ¥è®²æ˜¯å¯ä»¥çš„ï¼Œä½†æ€»æ„Ÿè§‰æœ‰ç‚¹é—®é¢˜æ˜¯å§ï¼Ÿ $$P(want|I) = \\dfrac{C(want|I)}{C(I)}\\le \\dfrac{C(want|I)+1}{C(I)+V}$$ æ˜¾ç„¶è¿™ä¸ªæ•°å˜å°äº†ï¼å› ä¸ºæœªå‡ºç°è¿‡çš„trigramå äº†ä¸€å®šçš„æ¦‚ç‡ç©ºé—´ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œp(to|i),p(chinese|i)â€¦è¿™äº›æ¦‚ç‡éƒ½è®¤ä¸ºç›¸ç­‰ä¹Ÿæ˜¯å¦åˆç†å‘¢ï¼Ÿ è¿™ä¸ªåšå®¢blogä¸­æåˆ°äº†æˆ‘çš„ç–‘é—®ï¼š å¦‚æ­¤ä¸€æ¥ï¼Œè®­ç»ƒè¯­æ–™ä¸­æœªå‡ºç°çš„n-Gramçš„æ¦‚ç‡ä¸å†ä¸º 0ï¼Œè€Œæ˜¯ä¸€ä¸ªå¤§äº 0 çš„è¾ƒå°çš„æ¦‚ç‡å€¼ã€‚Add-one å¹³æ»‘ç®—æ³•ç¡®å®è§£å†³äº†æˆ‘ä»¬çš„é—®é¢˜ï¼Œä½†æ˜¾ç„¶å®ƒä¹Ÿå¹¶ä¸å®Œç¾ã€‚ç”±äºè®­ç»ƒè¯­æ–™ä¸­æœªå‡ºç°n-Gramæ•°é‡å¤ªå¤šï¼Œå¹³æ»‘åï¼Œæ‰€æœ‰æœªå‡ºç°çš„n-Gramå æ®äº†æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸€ä¸ªå¾ˆå¤§çš„æ¯”ä¾‹ã€‚å› æ­¤ï¼Œåœ¨NLPä¸­ï¼ŒAdd-oneç»™è®­ç»ƒè¯­æ–™ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„ n-Gram åˆ†é…äº†å¤ªå¤šçš„æ¦‚ç‡ç©ºé—´ã€‚æ­¤å¤–ï¼Œè®¤ä¸ºæ‰€æœ‰æœªå‡ºç°çš„n-Gramæ¦‚ç‡ç›¸ç­‰æ˜¯å¦åˆç†å…¶å®ä¹Ÿå€¼å¾—å•†æ¦·ã€‚è€Œä¸”ï¼Œå¯¹äºå‡ºç°åœ¨è®­ç»ƒè¯­æ–™ä¸­çš„é‚£äº›n-Gramï¼Œéƒ½å¢åŠ åŒæ ·çš„é¢‘åº¦å€¼ï¼Œè¿™æ˜¯å¦æ¬ å¦¥ï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½ç»™å‡ºä¸€ä¸ªæ˜ç¡®çš„ç­”æ¡ˆã€‚ Add-k smoothing$$P_{add-k}(w_i|w_{i-n+1}\\cdots w_{i-1}) = \\frac{C(w_{i-n+1}\\cdots w_i)+k}{C(w_{i-n+1}\\cdots w_{i-1})+k|V|}$$ é€šå¸¸ï¼Œadd-kç®—æ³•çš„æ•ˆæœä¼šæ¯”Add-oneå¥½ï¼Œä½†æ˜¯æ˜¾ç„¶å®ƒä¸èƒ½å®Œå…¨è§£å†³é—®é¢˜ã€‚è‡³å°‘åœ¨å®è·µä¸­ï¼Œk å¿…é¡»äººä¸ºç»™å®šï¼Œè€Œè¿™ä¸ªå€¼åˆ°åº•è¯¥å–å¤šå°‘å´è«è¡·ä¸€æ˜¯ã€‚ Backoff and Interpolation å›é€€å’Œæ’å€¼å›é€€é€šå¸¸æˆ‘ä»¬ä¼šè®¤ä¸ºé«˜é˜¶æ¨¡å‹æ›´åŠ å¯é ï¼Œå‰é¢çš„ä¾‹å­ä¹Ÿè¡¨æ˜ï¼Œå½“èƒ½å¤Ÿè·çŸ¥æ›´å¤šå†å²ä¿¡æ¯æ—¶ï¼Œå…¶å®å°±è·å¾—äº†å½“å‰æ¨æµ‹çš„æ›´å¤šçº¦æŸï¼Œè¿™æ ·å°±æ›´å®¹æ˜“å¾—å‡ºæ­£ç¡®çš„ç»“è®ºã€‚æ‰€ä»¥åœ¨é«˜é˜¶æ¨¡å‹å¯é æ—¶ï¼Œå°½å¯èƒ½çš„ä½¿ç”¨é«˜é˜¶æ¨¡å‹ã€‚ä½†æ˜¯æœ‰æ—¶å€™é«˜çº§æ¨¡å‹çš„è®¡æ•°ç»“æœå¯èƒ½ä¸º0ï¼Œè¿™æ—¶æˆ‘ä»¬å°±è½¬è€Œä½¿ç”¨ä½é˜¶æ¨¡å‹æ¥é¿å…ç¨€ç–æ•°æ®çš„é—®é¢˜ã€‚ å›é€€çš„ä¸‰å…ƒè¯­æ³•ï¼š $$\\hat P(w_i|w_{i-2}w_{i-1}) =\\begin{cases}P(w_i|w_{i-2}w_{i-1})&amp;,if\\quad C(w_{i-2}w_{i-1})&gt;0\\ \\alpha_1P(w_i|w_{i-1})&amp;,if\\quad C(w_{i-1})&gt;0\\ \\alpha_2P(w_i)&amp;, otherwise \\end{cases}$$ ä¸€èˆ¬æ€§çš„å›é€€æ¨¡å‹,åˆå« Katz backoff ï¼š $$P_BO(w_n|w_{n-N+1}^{n-1}) =\\begin{cases} \\tilde P(w_n|w_{n-N+1}^{n-1})&amp;, if\\quad C(w_{n-N+1 \\cdots w_{n-1} }&gt;0)\\ \\theta(P(w_n|w_{n-N+1}^{n-1}))\\alpha P_BO(w_n|w_{n-N+2}^{n-1})&amp;,otherwise\\end{cases}$$ å…¶ä¸­ï¼š $$\\theta(x) =\\begin{cases}1&amp;,if\\quad x=0\\ 0&amp;,otherwise\\end{cases}$$ è¯•æƒ³ä¸€ä¸‹ï¼šä¸ºä»€ä¹ˆè¿™é‡Œè¦ç”¨ $\\alpha$ ,æ²¡æœ‰ä¼šæ€ä¹ˆæ ·ï¼Ÿ å¦‚æœæ²¡æœ‰ $\\alpha$ å€¼ï¼Œç­‰å¼å°±ä¸æ˜¯ä¸€ä¸ªæ¦‚ç‡äº†ï¼Œå› ä¸ºä¸‰å…ƒè¯­æ³•æ˜¯æ ¹æ®ç›¸å¯¹é¢‘åº¦æ¥è®¡ç®—çš„ï¼ŒåŸæœ¬ä¸º0çš„æ¦‚ç‡ï¼Œå›é€€åˆ°ä¸€ä¸ªä½é˜¶æ¨¡å‹åï¼Œç›¸å½“äºæŠŠå¤šä½™çš„æ¦‚ç‡é‡åŠ åˆ°ç­‰å¼ä¸­æ¥ï¼Œè¿™æ ·ä¸€æ¥ï¼Œå•è¯çš„æ€»æ¦‚ç‡å°±å°†å¤§äº1. å› æ­¤ï¼Œæ‰€æœ‰çš„å›é€€æ¨¡å‹éƒ½è¦æ‰“æŠ˜ï¼ˆdiscountingï¼‰. å…¶ä¸­ $\\tilde P$ ç”¨äºç»™æœ€å¤§ä¼¼ç„¶ä¼°è®¡MLEçš„æ¦‚ç‡æ‰“æŠ˜,ä¹Ÿå°±æ˜¯ç›´æ¥ä»è®¡æ•°è®¡ç®—å‡ºæ¥çš„æ—§çš„ç›¸å¯¹é¢‘åº¦èŠ‚çœæ¦‚ç‡é‡ã€‚$\\alpha$ ç”¨äºä¿è¯ä½é˜¶Nå…ƒè¯­æ³•æ¦‚ç‡é‡ä¹‹å’Œæ°å¥½ç­‰äºå‰é¢ $\\tilde p$ çœä¸‹æ¥çš„æ¦‚ç‡é‡ã€‚ æ’å€¼æ’å€¼å’Œå›é€€çš„æ€æƒ³å…¶å®éå¸¸ç›¸åƒã€‚è®¾æƒ³å¯¹äºä¸€ä¸ªtrigramçš„æ¨¡å‹ï¼Œæˆ‘ä»¬è¦ç»Ÿè®¡è¯­æ–™åº“ä¸­ â€œlike chinese foodâ€ å‡ºç°çš„æ¬¡æ•°ï¼Œç»“æœå‘ç°å®ƒæ²¡å‡ºç°è¿‡ï¼Œåˆ™è®¡æ•°ä¸º0ã€‚åœ¨å›é€€ç­–ç•¥ä¸­ï¼Œå°†ä¼šè¯•ç€ç”¨ä½é˜¶gramæ¥è¿›è¡Œæ›¿ä»£ï¼Œä¹Ÿå°±æ˜¯ç”¨ â€œchinese foodâ€ å‡ºç°çš„æ¬¡æ•°æ¥æ›¿ä»£ã€‚ åœ¨ä½¿ç”¨æ’å€¼ç®—æ³•æ—¶ï¼Œæˆ‘ä»¬æŠŠä¸åŒé˜¶åˆ«çš„n-Gramæ¨¡å‹çº¿å½¢åŠ æƒç»„åˆåå†æ¥ä½¿ç”¨ã€‚ç®€å•çº¿æ€§æ’å€¼ï¼ˆSimple Linear Interpolationï¼‰ å¯ä»¥ç”¨ä¸‹é¢çš„å…¬å¼æ¥å®šä¹‰ï¼š $$\\hat P(w_n|w_{n-1}w_{n-1})=\\lambda_1P(w_n|w_{n-2}w_{n-1})+\\lambda_2P(w_n|w_{n-1})+\\lambda_3P(w_n)$$ å…¶ä¸­: $\\sum_i\\lambda_i=1$ $\\lambda_i$ å¯ä»¥æ ¹æ®è¯•éªŒå‡­ç»éªŒè®¾å®šï¼Œä¹Ÿå¯ä»¥é€šè¿‡åº”ç”¨æŸäº›ç®—æ³•ç¡®å®šï¼Œä¾‹å¦‚EMç®—æ³•ã€‚ åœ¨ç®€å•å•çº¿å½¢æ’å€¼æ³•ä¸­ï¼Œæƒå€¼ $\\lambda_i$ æ˜¯å¸¸é‡ã€‚æ˜¾ç„¶ï¼Œå®ƒçš„é—®é¢˜åœ¨äºä¸ç®¡é«˜é˜¶æ¨¡å‹çš„ä¼°è®¡æ˜¯å¦å¯é ï¼ˆæ¯•ç«Ÿæœ‰äº›æ—¶å€™é«˜é˜¶çš„Gramè®¡æ•°å¯èƒ½å¹¶æ— ä¸º 0ï¼‰ï¼Œä½é˜¶æ¨¡å‹å‡ä»¥åŒæ ·çš„æƒé‡è¢«åŠ å…¥æ¨¡å‹ï¼Œè¿™å¹¶ä¸åˆç†ã€‚ä¸€ä¸ªå¯ä»¥æƒ³åˆ°çš„è§£å†³åŠæ³•æ˜¯è®© Î»i æˆä¸ºå†å²çš„å‡½æ•°ã€‚ä¹Ÿå°±æ˜¯ æ¡ä»¶æ’å€¼ï¼ˆconditional interpolationï¼‰ åˆ™æœ‰: $$\\hat P(w_n|w_{n-1}w_{n-1})=\\lambda_1(w_{n-2}w_{n-1})P(w_n|w_{n-2}w_{n-1})+\\lambda_2(w_{n-2}w_{n-1})P(w_n|w_{n-1})+\\lambda_3(w_{n-2}w_{n-1})P(w_n)$$ å¯ä½¿ç”¨EMç®—æ³•æ¥è®­ç»ƒ $\\lambda$ çš„å€¼ï¼Œä½¿å¾—ä»ä¸»è¯­æ–™åº“ä¸­åˆ†å‡ºæ¥çš„è¯­æ–™åº“çš„ä¼¼ç„¶åº¦æœ€å¤§ã€‚å…·ä½“æ€ä¹ˆè®­ç»ƒçš„ã€‚ã€‚çœ‹æ–‡çŒ®å§ï¼Œ(Jelinek and Mercer, 1980). absolute discountingæƒ³æƒ³ä¹‹å‰çš„Add-oneï¼Œä»¥åŠAdd-kç®—æ³•ã€‚æˆ‘ä»¬çš„ç­–ç•¥ï¼Œæœ¬è´¨ä¸Šè¯´å…¶å®æ˜¯å°†ä¸€äº›é¢‘ç¹å‡ºç°çš„ N-Gram çš„æ¦‚ç‡åŒ€å‡ºäº†ä¸€éƒ¨åˆ†ï¼Œåˆ†ç»™é‚£äº›æ²¡æœ‰å‡ºç°çš„ N-Gram ä¸Šã€‚å› ä¸ºæ‰€æœ‰å¯èƒ½æ€§çš„æ¦‚ç‡ä¹‹å’Œç­‰äº1ï¼Œæ‰€ä»¥æˆ‘ä»¬åªèƒ½åœ¨å„ç§å¯èƒ½çš„æƒ…å†µä¹‹é—´ç›¸äº’è…¾æŒªè¿™äº›æ¦‚ç‡ã€‚ æ—¢ç„¶æˆ‘ä»¬æ‰“ç®—æŠŠç»å¸¸å‡ºç°çš„ä¸€äº›N-Gramçš„æ¦‚ç‡åˆ†ä¸€äº›å‡ºæ¥ï¼Œå…¶å®ä¹Ÿå°±ç­‰åŒäºå°†å®ƒä»¬å‡ºç°çš„æ¬¡æ•°å‡å»ï¼ˆdiscountï¼‰ä¸€éƒ¨åˆ†ï¼Œé‚£åˆ°åº•è¯¥discountå¤šå°‘å‘¢ï¼ŸChurch &amp; Gale (1991) è®¾è®¡äº†ä¸€ç§éå¸¸å·§å¦™çš„æ–¹æ³•ã€‚é¦–å…ˆä»–ä»¬åœ¨ä¸€ä¸ª ç•™å­˜è¯­æ–™åº“ï¼ˆheld-out corpusï¼‰ è€ƒå¯Ÿé‚£äº›åœ¨è®­ç»ƒé›†ä¸­å‡ºç°äº†4æ¬¡çš„bigramså‡ºç°çš„æ¬¡æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬é¦–å…ˆåœ¨ä¸€ä¸ªæœ‰2200ä¸‡è¯çš„ç•™å­˜è¯­æ–™åº“ä¸­æ£€ç´¢å‡ºæ‰€æœ‰å‡ºç°äº†4æ¬¡çš„bigrams ï¼ˆä¾‹å¦‚ï¼š â€œchinese foodâ€ï¼Œâ€œgood boyâ€ï¼Œâ€œwant toâ€ç­‰ï¼‰ï¼Œç„¶åå†ä»ä¸€ä¸ªåŒæ ·æœ‰2200ä¸‡è¯çš„è®­ç»ƒé›†ä¸­ï¼Œåˆ†åˆ«ç»Ÿè®¡è¿™äº›bigramså‡ºç°çš„æ¬¡æ•°ï¼ˆä¾‹å¦‚ï¼šC(â€œchinese foodâ€)=4ï¼ŒC(â€œgood boyâ€)=3ï¼ŒC(â€œwant toâ€)=3ï¼‰ã€‚æœ€ç»ˆï¼Œå¹³å‡ä¸‹æ¥ï¼Œä»–ä»¬å‘ç°ï¼šåœ¨ç¬¬ä¸€ä¸ª2200ä¸‡è¯çš„è¯­æ–™ä¸­å‡ºç°4æ¬¡çš„bigramsï¼Œåœ¨ç¬¬äºŒä¸ª2200ä¸‡è¯çš„è¯­æ–™ä¸­å‡ºç°äº†3.23æ¬¡ã€‚ä¸‹é¢è¿™å¼ è¡¨ç»™å‡ºäº† c ä»0åˆ°9å–å€¼æ—¶ï¼ˆä¹Ÿå°±æ˜¯å‡ºç°äº† c æ¬¡ï¼‰ï¼Œç»Ÿè®¡çš„bigramsåœ¨ç•™å­˜é›†å’Œè®­ç»ƒé›†ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ å…¶å®ä½ åº”è¯¥å·²ç»å‘ç°å…¶ä¸­çš„è§„å¾‹äº†ã€‚é™¤äº†è®¡æ•°ä¸º0å’Œä¸º1çš„bigramä¹‹å¤–ï¼Œheld-out setä¸­çš„å¹³å‡è®¡æ•°å€¼ï¼Œéƒ½å¤§çº¦ç›¸å½“äºtraining setä¸­çš„è®¡æ•°å€¼å‡å»0.75ã€‚ åŸºäºä¸Šé¢è¿™ä¸ªå®éªŒç»“æœæ‰€è¯±å‘çš„ç›´è§‰ï¼ŒAbsolute discounting ä¼šä»æ¯ä¸€ä¸ªè®¡æ•°ä¸­å‡å»ä¸€ä¸ªï¼ˆç»å¯¹çš„ï¼‰æ•°å€¼ dã€‚è¿™æ ·åšçš„é“ç†å°±åœ¨äºï¼Œå¯¹äºå‡ºç°æ¬¡æ•°æ¯”è¾ƒå¤šçš„è®¡æ•°æˆ‘ä»¬å…¶å®å·²ç»å¾—åˆ°äº†ä¸€ä¸ªç›¸å¯¹æ¯”è¾ƒå¥½çš„ä¼°è®¡ï¼Œé‚£ä¹ˆå½“æˆ‘ä»¬ä»è¿™ä¸ªè®¡æ•°å€¼ä¸­å‡å»ä¸€ä¸ªè¾ƒå°çš„æ•°å€¼ d ååº”è¯¥å½±å“ä¸å¤§ã€‚ä¸Šé¢çš„å®éªŒç»“æœæš—ç¤ºåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå¯¹ä»2åˆ°9çš„è®¡æ•°è¿›è¡Œå¤„ç†ã€‚ $$P_{AbsDiscount}(w_i|w_{i-1})=\\frac{C(w_{i-1}w_i)-d}{C(w_{i-1})}+\\lambda(w_{i-1})P(w_i)$$ å¥½å¥½ç†è§£ä¸‹ï¼š å°±æ˜¯é€šè¿‡ held out set çš„å¯¹æ¯”å¾—åˆ°çš„ç›´è§‰åï¼Œæˆ‘ä»¬åœ¨train dataä¸­ï¼Œå°†æ‰€æœ‰çš„ $C(w_{i-1}w_i)$ å‡å»ä¸€ä¸ªæ•°dï¼Œåˆ†æ¯ä¸å˜ï¼Œæ¦‚ç‡è‚¯å®šå˜å°äº†ï¼Œä½†åªæ˜¯å‡å»ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œå½±å“å¹¶ä¸å¤§ã€‚ç„¶ååŠ ä¸Š $\\lambda(w_{i-1})P(w_i)$ï¼Œè¿™æ ·ä¸€æ¥ï¼ŒåŸæœ¬bigramä¸­ä¸º0çš„æ¦‚ç‡å°±ä¸æ˜¯0äº†ï¼Œå› ä¸ºunigramè‚¯å®šä¸ä¸º0å˜›ï½ä½†åé¢è¿™ä¸€é¡¹æ€ä¹ˆæ±‚ï¼Œæ˜¯æ¥ä¸‹æ¥ä¸€éƒ¨åˆ†çš„å†…å®¹ ä»ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å·²ç»çŸ¥é“ï¼Œå¯¹äºbigram modelè€Œè¨€ï¼Œ$P(w_i|w_{iâˆ’1})=C(w_{iâˆ’1}w_i)/C(w_{iâˆ’1})$ï¼Œæ‰€ä»¥ä¸Šè¿°æ–¹ç¨‹ç­‰å·å³ä¾§ç¬¬ä¸€é¡¹å³è¡¨ç¤ºç»è¿‡ discounting è°ƒæ•´çš„æ¦‚ç‡å€¼ï¼Œè€Œç¬¬äºŒé¡¹åˆ™ç›¸å¯¹äºä¸€ä¸ªå¸¦æƒé‡ Î» çš„ unigram çš„æ’å€¼é¡¹ã€‚é€šå¸¸ï¼Œä½ å¯ä»¥æŠŠ d å€¼å°±è®¾ä¸º 0.75ï¼Œæˆ–è€…ä½ ä¹Ÿå¯ä»¥ä¸ºè®¡æ•°ä¸º 1 çš„ bigram è®¾ç«‹ä¸€ä¸ªå•ç‹¬çš„ç­‰äº 0.5 çš„ d å€¼ï¼ˆè¿™ä¸ªç»éªŒå€¼ä»ä¸Šé¢çš„è¡¨ä¸­ä¹Ÿèƒ½çœ‹å‡ºæ¥ï¼‰ã€‚ Kneser-Ney discountingKneser-Ney discountingæ˜¯åœ¨absolute discountingçš„åŸºç¡€ä¸Šï¼Œå¯¹ä½é˜¶gramè¿›è¡Œä¸€äº›å¤„ç†ï½ è¿™ä¸ªåšå®¢å¯¹è¿™éƒ¨åˆ†è®²çš„éå¸¸æ¸…æ¥šï¼šhttps://blog.csdn.net/baimafujinji/article/details/51297802 ï¼Œæˆ‘å°±ç›´æ¥è´´è¿‡æ¥äº†ã€‚ã€‚ã€‚ å¦‚æœæˆ‘ä»¬ä½¿ç”¨bigramå’Œunigramçš„æ’å€¼æ¨¡å‹æ¥é¢„æµ‹ä¸‹é¢è¿™å¥è¯çš„ä¸‹ä¸€ä¸ªè¯ï¼š I canâ€™t see without my reading _______. æˆ‘ä»¬çš„ç›´è§‰æ˜¯ glassesï¼Œä½†æ˜¯å¦‚æœè¯­æ–™åº“ä¸­å‡ºç° Kong çš„é¢‘ç‡éå¸¸é«˜ï¼Œå› ä¸º Hong Kong æ˜¯é«˜é¢‘è¯ã€‚æ‰€ä»¥é‡‡ç”¨unigramæ¨¡å‹ï¼Œ Kong å…·æœ‰æ›´é«˜çš„æƒé‡ï¼Œæ‰€ä»¥æœ€ç»ˆè®¡ç®—æœºä¼šé€‰æ‹© Kong è¿™ä¸ªè¯ï¼ˆè€Œéglassesï¼‰å¡«å…¥ä¸Šé¢çš„ç©ºæ ¼ï¼Œå°½ç®¡è¿™ä¸ªç»“æœçœ‹èµ·æ¥ç›¸å½“ä¸åˆç†ã€‚è¿™å…¶å®å°±æš—ç¤ºæˆ‘ä»¬åº”è¯¥è°ƒæ•´ä¸€ä¸‹ç­–ç•¥ï¼Œæœ€å¥½ä»…å½“å‰ä¸€ä¸ªè¯æ˜¯ Hong æ—¶ï¼Œæˆ‘ä»¬æ‰ç»™ Kong èµ‹ä¸€ä¸ªè¾ƒé«˜çš„æƒå€¼ï¼Œå¦åˆ™å°½ç®¡åœ¨è¯­æ–™åº“ä¸­ Kong ä¹Ÿæ˜¯é«˜é¢‘è¯ï¼Œä½†æˆ‘ä»¬å¹¶ä¸æ‰“ç®—å•ç‹¬ä½¿ç”¨å®ƒã€‚ å¦‚æœè¯´ P(w) è¡¡é‡äº† w è¿™ä¸ªè¯å‡ºç°çš„å¯èƒ½æ€§ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç°åœ¨æƒ³åˆ›é€ ä¸€ä¸ªæ–°çš„ unigram æ¨¡å‹ï¼Œå«åš $P_{continuation}$ ï¼Œå®ƒçš„æ„æ€æ˜¯å°† w è¿™ä¸ªè¯ä½œä¸ºä¸€ä¸ªæ–°çš„æ¥ç»­çš„å¯èƒ½æ€§ã€‚æ³¨æ„è¿™å…¶å®æš—ç¤ºæˆ‘ä»¬è¦è€ƒè™‘å‰é¢ä¸€ä¸ªè¯ï¼ˆå³å†å²ï¼‰çš„å½±å“ã€‚æˆ–è€…è¯´ï¼Œä¸ºäº†è¯„ä¼° $P_{continuation}$ ï¼ˆæ³¨æ„è¿™æ˜¯ä¸€ä¸ª unigram æ¨¡å‹ï¼‰ï¼Œæˆ‘ä»¬å…¶å®éœ€è¦è€ƒå¯Ÿä½¿ç”¨äº† w è¿™ä¸ªè¯æ¥ç”Ÿæˆçš„ä¸åŒ bigram çš„æ•°é‡ã€‚æ³¨æ„è¿™é‡Œè¯´ä½¿ç”¨äº† w è¿™ä¸ªè¯æ¥ç”Ÿæˆçš„ä¸åŒç±»å‹ bigram çš„æ•°é‡ï¼Œæ˜¯æŒ‡å½“å‰è¯ä¸º w ï¼Œè€Œå‰é¢ä¸€ä¸ªè¯ä¸åŒæ—¶ï¼Œå°±äº§ç”Ÿäº†ä¸åŒçš„ç±»å‹ã€‚ä¾‹å¦‚ï¼šw = â€œfoodâ€, é‚£ä¹ˆä¸åŒçš„ bigram ç±»å‹å°±å¯èƒ½åŒ…æ‹¬ â€œchinese foodâ€ï¼Œâ€œenglish foodâ€ï¼Œâ€œjapanese foodâ€ç­‰ã€‚æ¯ä¸€ä¸ª bigram ç±»å‹ï¼Œå½“æˆ‘ä»¬ç¬¬ä¸€æ¬¡é‡åˆ°æ—¶ï¼Œå°±è§†ä¸ºä¸€ä¸ªæ–°çš„æ¥ç»­ï¼ˆnovel continuationï¼‰ã€‚ ä¹Ÿå°±æ˜¯è¯´ $P_continuation$ åº”è¯¥åŒæ‰€æœ‰æ–°çš„æ¥ç»­ï¼ˆnovel continuationï¼‰æ„æˆçš„é›†åˆä¹‹åŠ¿ï¼ˆcardinalityï¼‰æˆæ¯”ä¾‹ã€‚æ‰€ä»¥ï¼Œå¯çŸ¥: $$P_{continuation}(w_i)\\propto \\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert$$ å¦‚æœä½ å¯¹æ­¤å°šæœ‰å›°æƒ‘ï¼Œæˆ‘å†æ¥è§£é‡Šä¸€ä¸‹ä¸Šé¢è¿™ä¸ªå…¬å¼çš„æ„æ€ã€‚å½“å‰è¯æ˜¯ $w_i$ï¼Œä¾‹å¦‚â€œfoodâ€ï¼Œç”±æ­¤æ„æˆçš„ä¸åŒç±»å‹çš„ bigram å³ä¸º $w_{iâˆ’1}w_i$ï¼Œå…¶ä¸­ $w_{iâˆ’1}$ è¡¨ç¤ºå‰ä¸€ä¸ªè¯ï¼ˆpreceding wordï¼‰ã€‚æ˜¾ç„¶ï¼Œæ‰€æœ‰ç”± $w_{iâˆ’1}w_i$ æ„æˆçš„é›†åˆçš„åŠ¿ï¼Œå…¶å®å°±å–å†³äºå‡ºç°åœ¨ $w_i$ ä¹‹å‰çš„ä¸åŒçš„ $w_{iâˆ’1}$ çš„æ•°é‡ã€‚ ç„¶åï¼Œä¸ºäº†æŠŠä¸Šé¢è¿™ä¸ªæ•°å˜æˆä¸€ä¸ªæ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶é™¤ä»¥ä¸€ä¸ªå€¼ï¼Œè¿™ä¸ªå€¼å°±æ˜¯æ‰€æœ‰ bigram ç±»å‹çš„æ•°é‡ï¼Œå³ $\\lvert {(w_{j-1},w_j):C(w_{j-1}w_j)&gt;0}\\rvert$,è¿™é‡Œå¤§äº0çš„æ„æ€å°±æ˜¯â€œå‡ºç°è¿‡â€ã€‚äºæ˜¯æœ‰: $$P_{continuation}(w_i)=\\frac{\\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert}{\\lvert{(w_{j-1},w_j):C(w_{j-1}w_j)&gt;0}\\rvert}$$ ä¹Ÿå¯ä»¥å†™æˆè¿™ç§å½¢å¼ï¼š $$P_{continuation}(w_i)=\\frac{\\lvert { w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert} {\\sum_{wâ€™_ {i-1}} \\lvert{wâ€™_ {i-1}:C(wâ€™_ {i-1}wâ€™_ i)&gt;0}\\rvert}$$ å…¶å®è¦ç†è§£è¿™ä¸ªå¾ˆç®€å•ï¼Œç»å¯¹å€¼é‡Œé¢çš„çœ‹æˆä¸€ä¸ªå­—å…¸ dict,å‡ºç° $w_{i-1}w_i$ äº†ä¸€æ¬¡å®ƒçš„æ•°é‡å°±æ˜¯1ï¼Œä»¥ $P_continuation(food)$ ä¸ºä¾‹ï¼Œè¯­æ–™åº“ä¸­å‡ºç°äº† chinese food, japan food, english food,é‚£ä¹ˆåˆ†å­å°±æ˜¯è®¡ç®—ä¸åŒ $w_{i-1}$ çš„ä¸ªæ•°ï¼Œå…¶å¤§å°å°±æ˜¯3ã€‚ è€Œåˆ†æ¯å‘¢ï¼Œå°±æ˜¯æ‰€æœ‰çš„bigramï¼Œæ”¾åˆ°dictä¸­ï¼Œç›¸å½“äºå»é‡ï¼Œç„¶åå®ƒçš„æ€»æ•°å°±æ˜¯åˆ†æ¯çš„å€¼ã€‚å³æ‰€æœ‰ä¸åŒçš„ bigram çš„æ•°é‡å°±ç­‰äºå‡ºç°åœ¨å•è¯ $wâ€™_ i$ å‰é¢çš„æ‰€æœ‰ä¸åŒçš„è¯ $wâ€™_ {iâˆ’1}$ çš„æ•°é‡ï¼Œè¿™ä¸ªè®¡ç®—å¤æ‚åº¦åº”è¯¥å°±æ˜¯Vå§ï¼Œéå†æ•´ä¸ªè¯è¡¨å³å¯ã€‚ å¦‚æ­¤ä¸€æ¥ï¼Œä¸€ä¸ªä»…å‡ºç°åœ¨ Hong åé¢çš„é«˜é¢‘è¯ Kong åªèƒ½è·å¾—ä¸€ä¸ªè¾ƒä½çš„æ¥ç»­æ¦‚ç‡ï¼ˆcontinuation probabilityï¼‰ã€‚ç”±æ­¤ï¼Œå†ç»“åˆå‰é¢ç»™å‡ºçš„Absolute Discounting çš„æ¦‚ç‡è®¡ç®—å…¬å¼ï¼Œå°±å¯ä»¥å¾—å‡º Interpolated Kneser-Ney Smoothing çš„å…¬å¼ï¼Œå³ $$P_{KN}(w_i|w_{i-1})=\\frac{max(C(w_{i-1}w_i)-d,0)}{C(w_{i-1})}+\\lambda(w_{i-1})P_{continuation}(w_i)\\tag{4.33}$$ å…¶ä¸­ï¼Œ$max(C(w_{iâˆ’1}w_i)âˆ’d,0)$ çš„æ„æ€æ˜¯è¦ä¿è¯æœ€åçš„è®¡æ•°åœ¨å‡å»ä¸€ä¸ª d ä¹‹åä¸ä¼šå˜æˆä¸€ä¸ªè´Ÿæ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†åŸæ¥çš„ $P(w_i)$ æ›¿æ¢æˆäº† $P_continuation(w_i)$ã€‚æ­¤å¤–ï¼Œ**$\\lambda$ æ˜¯ä¸€ä¸ªæ­£åˆ™åŒ–å¸¸é‡ï¼Œç”¨äºåˆ†é…ä¹‹å‰discountçš„æ¦‚ç‡å€¼**ï¼ˆä¹Ÿå°±æ˜¯ä»é«˜é¢‘è¯ä¸­å‡å»çš„å‡†å¤‡åˆ†ç»™é‚£äº›æœªå‡ºç°çš„ä½é¢‘è¯çš„æ¦‚ç‡å€¼ï¼‰ï¼š $$\\lambda(w_{i-1})=\\frac{d}{C(w_{i-1})}\\cdot \\lvert {w:C(w_{i-1},w)&gt;0}\\rvert\\tag{4.34}$$ ä»¥ $P_{KN}(Kong|Hong)$ ä¸ºä¾‹ï¼Œï¼ˆ4.33ï¼‰å¼ä¸­ç¬¬ä¸€é¡¹æ˜¯å¯¹bigram (Kong|Hong)æ‰“äº†æŠ˜ï¼Œé‚£ä¹ˆåé¢ä¸€é¡¹åº”è¯¥åœ¨æ­¤åŸºç¡€ä¸Šè¡¥å›æ¥ä¸€ç‚¹ã€‚ ï¼ˆ4.34ï¼‰ç¬¬ä¸€é¡¹ $\\dfrac{d}{C(w_{i-1})}$ æ˜¯æŠ˜æ‰£å½’ä¸€åŒ–ï¼ˆnormalized discountï¼‰ï¼›ç¬¬äºŒé¡¹ $\\lvert {w:C(w_{i-1},w)&gt;0}\\rvert$ æ˜¯æ‰“æŠ˜çš„æ¬¡æ•°ã€‚ ä¸å¤ªå¥½ç†è§£ï¼Œçœ‹åŸæ–‡ç« ï¼š å°†ä¸Šè¿°å…¬å¼æ³›åŒ–ï¼Œå¯ç”¨é€’å½’è¡¨ç¤ºä¸ºï¼š $$P_{KN}(w_i|w_{i-n+1}\\cdots w_{i-1})=\\frac{max(0,C_{KN}(w_{i-n+1} \\cdots w_i) - d)}{C_{KN}(w_{i-n+1}\\cdots w_{i-1})} +\\lambda(w_{i-n+1}\\cdots w_{i-1})\\cdot P_{KN}(w_i|w_{i-n+2}\\cdots w_{i-1})$$ å…¶ä¸­ $C_{KN}$ å–å†³äºç”¨ä»€ä¹ˆæ’å€¼æ–¹å¼ï¼Œåœ¨ï¼ˆ4.33ï¼‰å¼ä¸­åªé‡‡ç”¨äº†bigramï¼Œå¦‚æœé‡‡ç”¨trigramï¼Œbigramï¼Œunigramçš„è”åˆæ’å€¼ï¼Œé‚£ä¹ˆå¯¹äºæœ€é«˜é˜¶çš„ trigram åœ¨è®¡æ•°æ—¶å¹¶ä¸éœ€è¦ä½¿ç”¨æ¥ç»­è®¡æ•°ï¼ˆé‡‡ç”¨æ™®é€šè®¡æ•°å³å¯ï¼‰ï¼Œè€Œå…¶ä»–ä½é˜¶ï¼Œå³ bigram å’Œ unigram åˆ™éœ€è¦ä½¿ç”¨æ¥ç»­è®¡æ•°ã€‚è¿™æ˜¯å› ä¸ºåœ¨ unigram ä¸­ï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€ä¸ª Kongï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒå®ƒçš„ bigramï¼ŒåŒç†åœ¨ bigramï¼Œæˆ‘è¿˜å¯ä»¥å†å‚è€ƒå®ƒçš„ trigramï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„æ’å€¼ç»„åˆä¸­æœ€é«˜é˜¶å°±æ˜¯ trigramï¼Œé‚£ä¹ˆç°åœ¨æ²¡æœ‰ 4-gramæ¥ç»™æˆ‘ä»¬åšæ¥ç»­è®¡æ•°ã€‚ç”¨å…¬å¼è¡¨ç¤ºå³ä¸ºï¼š $$C_{KN}(\\cdot)=\\begin{cases}count(\\cdot) &amp;,for\\ the\\ highest\\ order\\ continuationcount(\\cdot) &amp;,for\\ all\\ other\\ lower\\ orders \\end{cases}$$ ä¸ºä»€ä¹ˆä½é˜¶çš„gramè¦é‡‡ç”¨continuationå‘¢ï¼Ÿå½“trigramçš„æ¦‚ç‡ä¸º0æ—¶ï¼Œä½¿ç”¨bigramä¼šå¢å¤§æ¦‚ç‡é‡ï¼Œæ‰€ä»¥éƒ½è¦æ‰“æŠ˜ï¼Œæ‰€ä»¥å¯¹åº”çš„bigramä¹Ÿå¾—ç”¨continuationæ‰å¥½å¯¹å§ï½ æˆ‘ä»¬å‰é¢æåˆ°Kneser-Ney Smoothing æ˜¯å½“å‰ä¸€ä¸ªæ ‡å‡†çš„ã€å¹¿æ³›é‡‡ç”¨çš„ã€å…ˆè¿›çš„å¹³æ»‘ç®—æ³•ã€‚è¿™é‡Œæˆ‘ä»¬æ‰€è¯´çš„å…ˆè¿›çš„å¹³æ»‘ç®—æ³•ï¼Œå…¶å®æ˜¯åŒ…å«äº†å…¶ä»–ä»¥ Kneser-Ney ä¸ºåŸºç¡€æ”¹è¿›ã€è¡ç”Ÿè€Œæ¥çš„ç®—æ³•ã€‚å…¶ä¸­ï¼Œæ•ˆæœæœ€å¥½çš„Kneser-Ney Smoothing ç®—æ³•æ˜¯ç”±Chen &amp; Goodmanï¼ˆ1998ï¼‰æå‡ºçš„ modified Kneser-Ney Smoothing ç®—æ³•ï¼Œå®ƒå¯¹ discount dä½¿ç”¨ä¸åŒçš„å€¼ $d_1,d_2,d_3$ åˆ†åˆ«å¯¹åº”äºunigram, bigram, trigramç­‰ç­‰ã€‚å¾ˆå¤šNLPçš„å¼€å‘åŒ…å’Œç®—æ³•åº“ä¸­æä¾›æœ‰åŸå§‹çš„Kneser-Ney Smoothingï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬å‰é¢ä»‹ç»çš„ï¼‰ï¼Œä»¥åŠmodified Kneser-Ney Smoothing ç®—æ³•çš„å®ç°ã€‚ The Web and Stupid Backoffå¯¹äºç‰¹åˆ«å¤§çš„è¯­è¨€æ¨¡å‹ï¼Œæ•ˆç‡è€ƒè™‘å¾ˆé‡è¦ã€‚ å®ƒä¸æ˜¯å°†æ¯ä¸ªå•è¯å­˜å‚¨ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè€Œæ˜¯é€šå¸¸åœ¨å†…å­˜ä¸­å°†å…¶è¡¨ç¤ºä¸º64ä½æ•£åˆ—å·ï¼Œå¹¶å°†å•è¯æœ¬èº«å­˜å‚¨èµ·æ¥ã€‚é€šå¸¸åªä½¿ç”¨4-8ä½ï¼ˆè€Œä¸æ˜¯8å­—èŠ‚çš„æµ®ç‚¹æ•°ï¼‰å¯¹æ¦‚ç‡è¿›è¡Œé‡åŒ–ï¼Œå¹¶å°†N-gramå­˜å‚¨åœ¨åå‘å°è¯•ä¸­ã€‚ N-gramä¹Ÿå¯ä»¥é€šè¿‡ä¿®å‰ªæ¥ç¼©å°ï¼Œä¾‹å¦‚åªå­˜å‚¨è®¡æ•°å¤§äºæŸä¸ªé˜ˆå€¼çš„N-gramï¼ˆä¾‹å¦‚ç”¨äºGoogle N-gramå‘å¸ƒçš„è®¡æ•°é˜ˆå€¼40ï¼‰æˆ–ä½¿ç”¨ç†µä¿®å‰ªä¸å¤ªé‡è¦çš„N-gramsï¼ˆStolckeï¼Œ1998ï¼‰ã€‚ stupid Backoff æ²¡æœ‰discountingï¼Œæ²¡é‚£ä¹ˆå¤šå¤æ‚çš„æ–¹æ³•ï¼Œå°±æ˜¯ç®€å•çš„å›é€€åˆ°ä¸Šä¸€é˜¶ï¼Œæ‰€ä»¥ä¸æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæ¦‚ç‡åŠ èµ·æ¥å¤§äº1äº†ã€‚å¦‚æœå›é€€åˆ°unigram, $S(w)=\\dfrac{count(w)}{N}$,å¹¶ä¸” $\\lambda$ ä½¿ç”¨çš„0.4. Advanced: Perplexityâ€™s Relation to Entropyç†µ entropyå›°æƒ‘åº¦æ˜¯æ¥è‡ªä¿¡æ¯è®ºä¸­çš„äº¤å‰ç†µcross-entroyçš„æ¦‚å¿µã€‚ å‰é¢æˆ‘çš„ä¸€ç¯‡åšå®¢ä»ä¿¡æ¯è®ºçš„è§’åº¦æåˆ°äº†äº¤å‰ç†µï¼šblog å¯¹äºä¸€ä¸ªéšæœºå˜é‡ $X=(X_1â€¦X_n)$ï¼Œå®ƒçš„åˆ†å¸ƒæ˜¯p(x),é‚£ä¹ˆè¿™ä¸ªéšæœºå˜é‡çš„ç†µå°±æ˜¯ï¼š $$H(X)=-\\sum p(x)logp(x)$$ å¯¹æ•°åº•æ•°å¯ä»¥ä¸ºä»»ä½•æ•°ã€‚$-logp(x)$ æ˜¯é¦™å†œå®šä¹‰çš„ä¿¡æ¯é‡ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾åº•æ•°ä¸º2ï¼Œé‚£ä¹ˆä¿¡æ¯é‡ $-log_2p(x)$ è¡¨ç¤ºæè¿°Xéšæœºå˜é‡å–X_1æ—¶æ‰€éœ€çš„ç¼–ç é•¿åº¦ï¼Œp(X=X_1)æ¦‚ç‡è¶Šå¤§ï¼Œæ‰€éœ€çš„ç¼–ç é•¿åº¦è¶Šå°ã€‚é‚£ä¹ˆç†µH(X)å°±è¡¨ç¤ºæè¿°éšæœºå˜é‡Xç¼–ç é•¿åº¦çš„æœŸæœ›äº†ã€‚ ä½œè€…è¿™é‡Œç”¨ä¸€ä¸ªä¾‹å­æè¿°äº†ç†µè¿™ä¸ªæ¦‚å¿µï¼š æ–‡ç« ä¸­å¯¹æ¦‚ç‡æœ€å¤§çš„ Horse1,å…¶ä¿¡æ¯é‡ $-log_2(1/2)=1$,å…¶ç”¨äºŒè¿›åˆ¶ç¼–ç å°±æ˜¯0ï¼Œ Horse2 å…¶ä¿¡æ¯é‡å°±æ˜¯ $-log_2(1/4)=2$,å…¶ç”¨äºŒè¿›åˆ¶ç¼–ç å°±æ˜¯ 10â€¦. å¯ä»¥çœ‹åˆ°æ¦‚ç‡è¶Šå¤§ï¼Œæ‰€éœ€çš„ç¼–ç é•¿åº¦è¶Šå°ï¼Œä¹Ÿå°±æ˜¯ä¿¡æ¯é‡è¶Šå°ã€‚å¯ä»¥æç«¯ç‚¹æƒ³ï¼Œå¯¹äºä¸€ä¸ªéšæœºäº‹ä»¶ï¼Œå¦‚æœå…¶åˆ†å¸ƒæ˜¯(1,0,0,0)å’Œ(1/4,1/4,1/4,1/4)ï¼ŒåŒæ ·å‘ç”Ÿ10æ¬¡ï¼Œå¯èƒ½å‰è€…çš„åˆ†å¸ƒå°±åˆ¤æ–­å‡ºæ¥äº†ï¼Œä½†åè€…éœ€è¦æ›´å¤šã€‚é‚£ä¹ˆç”¨æ¥æè¿°è¿™ä¸ªéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§çš„åº¦é‡ï¼Œå°±æ˜¯ç†µäº†ï¼Œä¹Ÿå°±æ˜¯ä¿¡æ¯é‡çš„æœŸæœ›ã€‚ æ€»ç»“ä¸‹ï¼šæ‰€è°“ç†µï¼Œå°±æ˜¯ä¿¡æ¯é‡ -logp(x) çš„æœŸæœ›ã€‚ ä¿¡æ¯ç†µä»£è¡¨çš„æ˜¯éšæœºå˜é‡æˆ–æ•´ä¸ªç³»ç»Ÿçš„ä¸ç¡®å®šæ€§ï¼Œç†µè¶Šå¤§ï¼Œéšæœºå˜é‡æˆ–ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§å°±è¶Šå¤§ã€‚ è¿™é‡Œå¼•å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯¹äºä¸€ä¸ªsequenceå¯ä»¥çœ‹åšä¸€ä¸ªéšæœºå˜é‡ï¼Œ$W={ w_0,w_1,â€¦,w_n }$, å¯¹äºä¸€ä¸ªæœ‰é™é•¿åº¦ä¸ºnçš„sequenceï¼Œå…¶ç†µä¸ºï¼š $$H(w_1,w_2,â€¦,w_n)=-log_{w_1^n\\in L}p(w_1^n)logp(w_1^n)$$ entropy rate, ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯per-word entropy: $$\\dfrac{1}{n}H(w_1,w_2,â€¦,w_n)=-\\dfrac{1}{n}log_{w_1^n\\in L}p(w_1^n)logp(w_1^n)$$ äº¤å‰ç†µ cross-entropyä½†æˆ‘ä»¬åœ¨è€ƒè™‘languageçš„ç†µæ—¶ï¼Œéœ€è¦è®¤ä¸ºsequenceæ˜¯æ— é™é•¿åº¦çš„ã€‚å¯ä»¥æŠŠsequencesç”Ÿæˆçš„è¿‡ç¨‹çœ‹æˆæ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹stochastic process L, é‚£ä¹ˆLçš„ç†µï¼š $$H(L)=-lim_{n\\to \\infty}\\dfrac{1}{n}H(w_1,w_2,â€¦,w_n)=-lim_{n\\to \\infty}\\sum_{W\\in L}p(w_1,w_2,â€¦,w_n)logp(w_1,w_2,â€¦,w_n)$$ cross-entropy: ä½†æˆ‘ä»¬ä¸çŸ¥é“ç”Ÿæˆæ•°æ®çš„çœŸå®åˆ†å¸ƒpæ—¶ï¼Œå¯ä»¥ç”¨åˆ†å¸ƒmæ¥è¡¨ç¤ºï¼š $$H(p,m)=lim_{n\\to \\infty}-\\dfrac{1}{n}\\sum_{W\\in L}p(w_1,w_2,â€¦,w_n)logm(w_1,w_2,â€¦,w_n)$$ å¯¹äºé™æ€éšæœºè¿‡ç¨‹(stationary ergodic process),é™æ€å‡è®¾åœ¨HMMä¸­æœ‰è®²åˆ°ï¼šä¸‹ä¸€ä¸ªè¯å¯¹ä¸Šä¸€ä¸ªè¯çš„ä¾èµ–çš„æ¦‚ç‡ï¼Œä¸ä¼šéšæ—¶é—´çš„æ”¹å˜è€Œæ”¹å˜ã€‚é‚£ä¹ˆå¯ä»¥è®¤ä¸º $p(w_1,w_2,â€¦,w_n)$ æ˜¯ä¸€ä¸ªå®šå€¼ã€‚ $$H(p,m)=lim_{n\\to \\infty}-\\dfrac{1}{n}\\sum_{W\\in L}logm(w_1,w_2,â€¦,w_n)\\tag{4.47}$$ åˆæœ‰ï¼š$$H(p)\\le H(p,m)$$ å› æ­¤æˆ‘ä»¬å¯ä»¥ç”¨måˆ†å¸ƒæ¥ä¼°è®¡påˆ†å¸ƒã€‚ å›°æƒ‘åº¦å’Œäº¤å‰ç†µN-gramæ¨¡å‹ $M=P(w_i|w_{i-N+1} \\cdots w_{i-1})$ ç”Ÿæˆåºåˆ—sequences W, æ ¹æ®å…¬å¼(4.47): $$H(W) = -\\dfrac{1}{N}logP(w_1w_2 \\cdots w_N)$$ åŸºäºæ¨¡å‹ $M=P(w_i|w_{i-N+1} \\cdots w_{i-1})$ çš„å›°æƒ‘åº¦perplexityå¯å®šä¹‰ä¸ºäº¤å‰ç†µçš„æŒ‡æ•°å½¢å¼ï¼š ä¼¼ä¹ä¹Ÿæ˜¯æœ‰ç‚¹å·§å•Šï¼Œäº¤å‰ç†µå’Œå›°æƒ‘åº¦éƒ½å¯ä»¥ç”¨æ¥è¡¨å¾påˆ†å¸ƒæ˜¯å¦æ¥è¿‘çœŸå®åˆ†å¸ƒï½å‰å®³äº† æ€»ç»“ ###å‚è€ƒï¼š [1] Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin, 3rd. Chapter 4 [2] è‡ªç„¶è¯­è¨€å¤„ç†ä¸­N-Gramæ¨¡å‹çš„Smoothingç®—æ³•","link":"/2018/04/12/chapter4-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8CN%E5%85%83%E8%AF%AD%E6%B3%95/"},{"title":"chapter6-æœ´ç´ è´å¶æ–¯å’Œæƒ…æ„Ÿåˆ†ç±»","text":"Naive bayes æ¨¡å‹ p(x|y) è®­ç»ƒï¼šæ±‚æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆé¢‘ç‡ä»£æ›¿æ¦‚ç‡ï¼‰ p(y|x),p(x)ï¼Œæ— å‚ä¼°è®¡ ä¼˜åŒ–ï¼šå„ç§é¢„å¤„ç†å’Œç‰¹å¾æå– éªŒè¯æ¨¡å‹ï¼š Precision, Recall, F-measure å¯¹äºå¤šåˆ†ç±»çš„å¤„ç† äº¤å‰éªŒè¯ æ¯”è¾ƒåˆ†ç±»å™¨ï¼šç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯• å‰è¨€ï¼šæ–‡æœ¬åˆ†ç±»(text categorization) çš„åº”ç”¨ï¼š æƒ…æ„Ÿåˆ†æï¼ˆsentiment analysisï¼‰:the extraction of sentiment åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼ˆspam detectionï¼‰ ä½œè€…å½’å±ï¼ˆauthor attributionï¼‰ ä¸»é¢˜åˆ†ç±»ï¼ˆsubject category classificationï¼‰ é™¤äº†æ–‡æœ¬åˆ†ç±»å¤–ï¼Œåˆ†ç±»åœ¨NLPLé¢†åŸŸè¿˜æœ‰å¾ˆå¤šå…¶ä»–åº”ç”¨ï¼š å¥å·æ¶ˆæ­§ï¼ˆperiod disambiguationï¼‰ å•è¯æ ‡è®°ï¼ˆword tokenizationï¼‰ part-of-speech tagger å‘½åå®ä½“æ ‡æ³¨ named-entity tagging æœ¬ç« èŠ‚æ·±å…¥ä»‹ç» multinomial naive Bayes, ä¸‹ä¸€ç« å°† nultinomial logistic regression, åˆå« maximum entropy. åŒæ—¶ï¼Œç®€å•ä»‹ç»äº†ä¸€ä¸‹ç”Ÿæˆç®—æ³•å’Œåˆ¤åˆ«ç®—æ³•çš„åŒºåˆ«ï¼šå¯å‚è€ƒå‰é¢çš„æœºå™¨å­¦ä¹ -ç”Ÿæˆæ¨¡å‹åˆ°é«˜æ–¯åˆ¤åˆ«åˆ†æå†åˆ°GMMå’ŒEMç®—æ³• è¿›ä¸€æ­¥çš„ç†è§£ä¸‹ï¼š åˆ¤åˆ«ç®—æ³•ï¼š $p(y|x;\\theta)$ æ ¹æ®äº¤å‰ç†µ $H(\\hat y, y)$ ç­‰æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰æ¥ä¼˜åŒ–å‚æ•° $theta$. ç”Ÿæˆç®—æ³•:p(x|y)ï¼Œ å¯ä»¥åˆ†ä¸ºä¸¤ç§ï¼Œæœ‰æ ‡ç­¾çš„å’Œæ— æ ‡ç­¾çš„ æœ‰æ ‡ç­¾çš„æ˜¯ç›‘ç£å­¦ä¹ ï¼Œä»¥é«˜æ–¯åˆ¤åˆ«åˆ†æGDAå’Œæœ¬ç« èŠ‚ä¸­çš„Naive Bayesä¸ºä¾‹ï¼Œå°±æ˜¯æ ¹æ® $p(y|x) = \\dfrac{p(x|y)p(y)}{p(x)}$,åœ¨æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥æ ¹æ®MLEï¼ˆç”¨ç›¸å¯¹é¢‘åº¦ä»£æ›¿æ¦‚ç‡ï¼‰æ¥è®¡ç®—å‡º likelihood $p(x|y)$ å’Œå…ˆéªŒæ¦‚ç‡prior $p(y)$,ç„¶åå¸¦å…¥é¢„æµ‹æ ·æœ¬æ•°æ®ï¼Œå¾—åˆ°å¯¹äºæ¯ä¸€ä¸ªåˆ†ç±»p(x|y=0,1,â€¦)çš„æ¦‚ç‡ï¼Œç„¶å $argmax_yp(x|y)$ å°±æ˜¯é¢„æµ‹æ•°æ®çš„ç±»åˆ«ã€‚ æ— æ ‡ç­¾çš„æ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œé«˜æ–¯æ··åˆæ¨¡å‹GMMä¸ºä¾‹ï¼Œéœ€è¦å¼•å…¥éšè—å˜é‡zï¼Œè®¡ç®— $p(x,z)=p(x|z)p(z)$,å‡è®¾p(x|z)æ˜¯å¤šç»´é«˜æ–¯åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨EMç®—æ³•å¯¹é«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ï¼Œä»¥åŠå…ˆéªŒæ¦‚ç‡p(z)è¿›è¡Œä¼°è®¡~ ä½†å› ä¸ºç±»æ ‡ç­¾æ˜¯ä¸å­˜åœ¨çš„ï¼Œæ‰€ä»¥åªèƒ½å¯¹æ ·æœ¬æ•°æ®è¿›è¡Œèšç±»ï¼Œè€Œæ— æ³•ç»™å¯¹é¢„æµ‹æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚ Naive Bayes Classifiers æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ ·æœ¬æ•°æ®ï¼š $(d_1,c_1),â€¦,(d_N,c_N),\\quad c\\in C$ è´å¶æ–¯åˆ†ç±»å™¨æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†ç±»å™¨ï¼š $$\\hat c = argmax_{c\\in C}P(c|d)$$ Bayesian Inference: $$p(x|y) = \\dfrac{p(y|x)p(x)}{p(y)}$$ åˆ™æœ‰ï¼š $$\\hat c = argmax_{c\\in C}P(c|d)=argmax_{c\\in C}\\dfrac{P(d|c)P(c)}{P(d)}$$ å¯ä»¥ç›´æ¥å»æ‰P(d),å› ä¸ºæˆ‘ä»¬è¦è®¡ç®—å¯¹äºæ¯ä¸€ç±» $c\\in C$çš„æ¦‚ç‡ $\\dfrac{P(d|c)P(c)}{P(d)}$, è€ŒP(d)å¯¹äºä»»ä½•ç±»åˆ«éƒ½æ˜¯ä¸å˜çš„ã€‚è€Œæˆ‘ä»¬è¦æ±‚çš„æ˜¯æœ€åå¯èƒ½çš„classï¼Œå…¶å¯¹åº”çš„P(d)éƒ½ä¸€æ ·ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥å»æ‰ã€‚ $$\\hat c = argmax_{c\\in C}P(c|d)=argmax_{c\\in C}P(d|c)P(c)$$ å…¶ä¸­P(d|c)æ˜¯æ ·æœ¬æ•°æ®çš„ ä¼¼ç„¶æ¦‚ç‡likelihood, P(c)æ˜¯ å…ˆéªŒæ¦‚ç‡prior,å¯ä»¥å†™æˆç‰¹å¾çš„å½¢å¼ï¼š $$\\hat c = argmax_{c\\in C}P(f_1,f_2,â€¦,f_n|c)p(c)$$ æ˜¾ç„¶ $P(f_1,f_2,â€¦,f_n|d)$ æ˜¯å¾ˆéš¾è®¡ç®—çš„ï¼Œä»¥è¯­è¨€æ¨¡å‹ä¸ºä¾‹ï¼Œä½ ä¸å¯èƒ½è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„è¯çš„ç»„åˆï¼Œå› æ­¤æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åšäº†ä¸¤ä¸ªå‡è®¾ä»¥ç®€åŒ–æ¨¡å‹ï¼š bag of words assumptionï¼šä¸è€ƒè™‘è¯çš„é¡ºåºï¼Œä¹Ÿå°±æ˜¯è¯´ love è¿™ä¸ªè¯ä¸ç®¡å‡ºç°åœ¨ 1st,20thç­‰ï¼Œå®ƒå¯¹è¿™ä¸ªsequenceæ‰€å±ç±»åˆ«çš„å½±å“æ˜¯ä¸€æ ·çš„ã€‚ naive bayes assumption:æ‰€æœ‰ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ $$P(f_1,f_2,â€¦,f_n|c)=P(f_1|c)cdot P(f_2|c)\\cdots P(f_n|c)$$ å› æ­¤æœ‰ï¼š $$c_{NB}=argmax_{c\\in C}P(c)\\prod_{f\\in F}P(f|c)$$ å°†æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åº”ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘è¯çš„ä½ç½®ï¼Œè¿™é‡Œå…¶å®ä¹Ÿæ²¡æœ‰è€ƒè™‘è¯çš„é¡ºåºå˜›ã€‚ã€‚ $$positions \\leftarrow all\\ word\\ position\\ in\\ test\\ document$$ $$c_{NB}=argmax_{c\\in C}P(c)\\prod_{i\\in position}P(w_i|c)$$ ä¸ºé¿å…æ•°å€¼ä¸‹æº¢ï¼Œåœ¨å¯¹æ•°åŸŸè¿›è¡Œè®¡ç®—ï¼š $$c_{NB}=argmax_{c\\in C}logP(c)+\\sum_{i\\in position}logP(w_i|c)$$ Training the Naive Bayes Classifieråˆ†åˆ«è®¡ç®—æ¦‚ç‡P(c), $P(f_i|c)$ $$\\hat P(c)=\\dfrac{N_c}{N_{doc}}$$ è¿™é‡Œ $N_c$ è¡¨ç¤ºcç±»ä¸­æ‰€æœ‰è¯çš„æ€»æ•°ï¼Œ$N_{doc}$ è¡¨ç¤ºæ‰€æœ‰è¯çš„æ€»æ•°ã€‚ $$\\hat P(w_i|c)=\\dfrac{count(w_i,c)}{\\sum_{w\\in V}count(w,c)}$$ åˆ†æ¯è¡¨ç¤ºcç±»ä¸­æ‰€æœ‰è¯çš„æ€»æ•°ã€‚è¯å…¸Vè¡¨ç¤ºæ‰€æœ‰ç±»åˆ«çš„è¯çš„æ€»æ•°ï¼Œä¸ä»…ä»…æ˜¯cç±»åˆ«çš„ã€‚ æœ‰ä¸ªé—®é¢˜: å¦‚æœè¯â€œfantasticâ€ï¼Œä»æœªå‡ºç°åœ¨ positiveç±»åˆ«ä¸­ï¼Œé‚£ä¹ˆï¼š è¿™æ˜¯ä¸åˆç†çš„ï¼Œæ‰€ä»¥é‡‡ç”¨ add-one smoothing: è¿™é‡Œè¦æƒ³æ¸…æ¥šä¸ºå•¥æ˜¯Vï¼Œè€Œä¸æ˜¯cç±»ä¸­è¯çš„ä¸ªæ•°ï¼Ÿè¦ä¿è¯ $\\sum_iP(w_i|c)=1$ å‚è€ƒä¹ é¢˜ï¼š å¯¹äº unknown words: ç›´æ¥åˆ é™¤ã€‚ã€‚ å¯¹äº stop words: ä¹Ÿå°±æ˜¯ the, a,..,è¿™æ ·é«˜é¢‘è¯ï¼Œå¯ä»¥åˆ æ‰ã€‚ä¹Ÿå¯ä»¥ä¸ç®¡ï¼Œæ•ˆæœå·®ä¸å¤šï½ ç®—æ³•æµç¨‹ æ€»ç»“ä¸‹ï¼š 1234567891011121314151617181920212223for each class in C: è®¡ç®— logprior[c] for each word in V: è®¡ç®— wordåœ¨å½“å‰classä¸‹çš„likelihood loglikelihood[word,c]for each class in C: sum[c] = logprior[c] for each position in testdoc: if word in V: sum[c] += loglikehood[word,c] return argmax(sum[c]) Optimizing for Sentiment Analysisbinary multinominal naive Bayes or binary NB. å°†æ¯ä¸€ä¸ªsequenceæˆ–æ˜¯documentsä¸­çš„é‡å¤è¯çš„æ•°é‡å˜ä¸º1. æ¯”å¦‚ great å‡ºç°åœ¨ä¸¤ä¸ªdocumentsä¸­ï¼Œæœ€åä¸€ä¸ªæœ‰2ä¸ªgreatï¼Œé‚£ä¹ˆpositiveä¸­greatæ€»æ•°ä¸º2ï¼Œæœ‰ç‚¹ç±»ä¼¼äºå½’ä¸€åŒ–ã€‚ä»»ä½•ä¸€ä¸ªè¯çš„æ•°é‡ä¸ä¼šè¶…è¿‡documentçš„æ•°é‡ã€‚ deal with negationé‡åˆ°logical negation(nâ€™t, not, no, never)åï¼Œå‰ç½®å‰ç¼€ NOT_ ç»™æ¯ä¸€ä¸ªå•è¯ï¼Œç›´åˆ°é‡åˆ°æ ‡ç‚¹ç¬¦å·ã€‚ sentiment lexiconså¦‚æœæ²¡æœ‰è¶³å¤Ÿå¤šçš„å·²ç»æ ‡æ³¨å¥½çš„æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨é¢„æ³¨é‡Šå¥½çš„æƒ…æ„Ÿè¯ã€‚å››ä¸ªéå¸¸æµè¡Œçš„è¯åº“ï¼š General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). ä»¥ MPQA ä¸ºä¾‹ï¼Œ6885 words, 2718 positive and 4912 negative è¯è¯´æ€ä¹ˆä½¿ç”¨ã€‚ã€‚ Evaluation: Precision, Recall, F-measure å¯¹äºæ•°æ®ä¸å‡è¡¡æ˜¯ï¼Œä½¿ç”¨accuracyæ˜¯ä¸å‡†ç¡®äº†ã€‚ ä¸¾ä¸ªæ —å­ï¼š a million tweet: 999,900æ¡ä¸æ˜¯å…³äºpieçš„ï¼Œåªæœ‰100æ¡æ˜¯å…³äºpieçš„ å¯¹äºä¸€ä¸ªstupidåˆ†ç±»å™¨ï¼Œä»–è®¤ä¸ºæ‰€æœ‰çš„tweetéƒ½æ˜¯è·Ÿpieæ— å…³çš„ï¼Œé‚£ä¹ˆå®ƒçš„å‡†ç¡®ç‡æ˜¯99.99%ï¼ä½†è¿™ä¸ªåˆ†ç±»å™¨æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œå› è€Œaccuracyä¸æ˜¯ä¸€ä¸ªå¥½çš„metricï¼Œå½“å®ƒç›®æ ‡æ˜¯rareï¼Œæˆ–æ˜¯complete unbalanced. å¼•å…¥å¦å¤–ä¸¤ä¸ªæŒ‡æ ‡ï¼š ç²¾åº¦ precision: æ˜¯åˆ†ç±»ç³»ç»Ÿé¢„æµ‹å‡ºæ¥çš„positiveçš„ä¸­çœŸå®positiveçš„æ¯”ä¾‹ å¬å› callï¼š æ˜¯çœŸå®positiveä¸­åˆ†ç±»ç³»ç»Ÿä¹Ÿé¢„æµ‹ä¸ºpositiveçš„æ¯”ä¾‹ã€‚ ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç›®çš„æ˜¯è¦æ‰¾åˆ°tweetä¸­å’Œpieç›¸å…³çš„ä¸€ç±»ï¼Œé‚£ä¹ˆå…¶å¬å›ç‡å°±æ˜¯ 0/10 = 0ï¼Œ å…¶precisionæ˜¯ 0/0 ï¼¦-measure $$F_{\\beta}=\\dfrac{(\\beta^2+1)PR}{\\beta^2P+R}$$ å½“ $\\beta&gt;1$æ—¶ï¼ŒRecallçš„æ¯”é‡æ›´å¤§;å½“ $\\beta&lt;1$æ—¶ï¼Œprecisionçš„æ¯”é‡æ›´å¤§ã€‚ä½¿ç”¨æœ€å¤šçš„æ˜¯ $\\beta=1$,ä¹Ÿå°±æ˜¯ $F_{\\beta=1}, F_1$. $\\beta$ çš„çš„å–å€¼å–å†³äºå®é™…åº”ç”¨ã€‚ $$F_1 = \\dfrac{2PR}{P+R}$$ ï¼¦-measure æ˜¯ precision å’Œ recall çš„ **åŠ æƒè°ƒå’Œå¹³å‡å€¼(weighted harmonic mean)**ã€‚ è°ƒå’Œå¹³å‡å€¼æ˜¯å€’æ•°çš„ç®—æœ¯å¹³å‡å€¼çš„å€’æ•°ã€‚ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è°ƒå’Œå¹³å‡å€¼å‘¢ï¼Ÿå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªæ›´ ä¿å®ˆçš„åº¦é‡(conservative metric). ç›¸æ¯”ç›´æ¥è®¡ç®— P å’Œ R çš„å¹³å‡å€¼ï¼Œ F-measureçš„å€¼æ›´çœ‹é‡ä¸¤è€…ä¸­çš„è¾ƒå°å€¼ã€‚ More than two classeså¤šåˆ†ç±»æœ‰ä¸¤ç§æƒ…å†µï¼š ä¸€ç§æ˜¯ä¸€ä¸ªdocumentå¯¹åº”å¤šæ ‡ç­¾ï¼Œ**ï¼ˆany-of or multi-label classificationï¼‰** ï¼šè§£å†³æ–¹æ³•æ˜¯å¯¹æ¯ä¸€ä¸ªç±»åˆ«ä½¿ç”¨äºŒåˆ†ç±»ï¼Œæ¯”å¦‚å¯¹äºç±»åˆ«cï¼Œå¯åˆ†ç±» positive labeled c å’Œ negative not labeled c. å¦å¤–ä¸€ç§æ˜¯ä¸€ä¸ªdocumentåªå¯¹åº”ä¸€ä¸ªæ ‡ç­¾ï¼Œä½†æ€»çš„ç±»åˆ«æ•°å¤§äº2. (one-of or multinomial classification) confusion metrix: å°†ä¸‰ä¸ªåˆ†ç±»åˆ†å¼€æ¥çœ‹ï¼Œpooledè¡¨ç¤ºæ±‡æ€» Test sets and Cross-validation æµ‹è¯•å’Œäº¤å‰éªŒè¯10æŠ˜äº¤å‰éªŒè¯ï¼š Statistical Significance Testing ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•æ¯”è¾ƒä¸¤ä¸ªåˆ†ç±»å™¨Aå’ŒBçš„å¥½åã€‚ å‡è®¾æ£€éªŒå¯¹äºä¸¤ä¸ªåˆ†ç±»å™¨ classifier A and B. æˆ‘ä»¬éœ€è¦çŸ¥é“Açš„æ€§èƒ½ä¸€å®šæ¯”Bå¥½å—ï¼Ÿè¿˜æ˜¯åªæ˜¯åœ¨ç‰¹å®šçš„æ•°æ®é›†ä¸Šè¡¨ç°æ¯”Bå¥½ï¼Ÿ null hypothesis: A is not really better than Bï¼ŒA å’Œ Båœ¨æŒ‡æ ‡F-measureçš„å·®è·æ˜¯ $\\delta(x)$ éšæœºå˜é‡Xæ˜¯æµ‹è¯•é›†çš„é›†åˆã€‚ æ¥å—åŸŸï¼š $H_0: P(\\delta (X)&gt; \\delta (x)|H_0)$ å½“p-value(X)&lt;0.05 or 0.01,æ—¶æˆ‘ä»¬æ‹’ç»å‡è®¾ $H_0$ bootstrap teståœ¨NLPä¸­é€šå¸¸ä¸ä½¿ç”¨ä¼ ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•ï¼Œå› ä¸ºmetricså¹¶ä¸æ˜¯æ­£æ€åˆ†å¸ƒ(normal distribution),è¿åäº†æµ‹è¯•çš„å‡è®¾ã€‚ å¯¹äºBootstrapçŸ¥ä¹ä¸Šæœ‰ä¸ªæ¯”è¾ƒæ¸…æ¥šçš„ç­”æ¡ˆï¼šhttps://zhuanlan.zhihu.com/p/24851814 æœ¬è´¨ä¸Šï¼ŒBootstrapæ–¹æ³•ï¼Œæ˜¯å°†ä¸€æ¬¡çš„ä¼°è®¡è¿‡ç¨‹ï¼Œé‡å¤ä¸Šåƒæ¬¡ä¸Šä¸‡æ¬¡ï¼Œä»è€Œä¾¿å¾—åˆ°äº†å¾—åˆ°ä¸Šåƒä¸ªç”šè‡³ä¸Šä¸‡ä¸ªçš„ä¼°è®¡å€¼ï¼Œäºæ˜¯åˆ©ç”¨è¿™ä¸æ­¢ä¸€ä¸ªçš„ä¼°è®¡å€¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä¼°è®¡å¾…ä¼°è®¡å€¼çš„å‡å€¼ä»¥å¤–çš„å…¶ä»–ç»Ÿè®¡é‡ï¼šæ¯”å¦‚æ ‡å‡†å·®ã€ä¸­ä½æ•°ç­‰ã€‚ åº”ç”¨åœ¨è¿™é‡Œæ˜¯å› ä¸ºä½†æ ·æœ¬æ•°æ®è¾ƒå°‘æ—¶ï¼Œä¸€æ¬¡æ ·æœ¬ä¼°è®¡æ— æ³•å‡†ç¡®çš„è®¡ç®—å‡ºä¸¤ä¸ªåˆ†ç±»å™¨Aå’ŒBçš„å·®å€¼ï¼Œå› è€Œé‡‡ç”¨bootstrap. å‚è€ƒï¼š Speech and Language Processing ,3rd, Chapter 6 çŸ¥ä¹ï¼šhttps://zhuanlan.zhihu.com/p/24851814","link":"/2018/04/14/chapter6-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%92%8C%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"title":"chapter8-ç¥ç»ç½‘ç»œå’Œè‡ªç„¶è¯­è¨€æ¨¡å‹","text":"Embeddings è¿™éƒ¨åˆ†ç®€å•çš„çœ‹ä¸€ä¸‹å§ï½æ¯•ç«Ÿå·²ç»å¾ˆç†Ÿäº†ã€‚ã€‚ã€‚ chapter 8 - neural networks chapter 15 - semantic representations for words called embeddings chapter 25 - sequence-to-sequence (seq2seq model), applied to language generation: machine translation, conversation agents and summarization. Neural Language Modelsç›¸æ¯”å‰é¢chapter4ä»‹ç»çš„paradigm(the smoothed N-grams),ä¾æ®ç¥ç»ç½‘ç»œçš„è¯­è¨€æ¨¡å‹æœ‰å¾ˆå¤šä¼˜åŠ¿ï½ ä¸éœ€è¦smoothingï¼Ÿ èƒ½å¤Ÿä¾èµ–æ›´é•¿çš„å†å²ä¾æ® handle much longer histories æ³›åŒ–èƒ½åŠ›æ›´å¼º é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æ˜¯ç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ï½ Embeddingsè¯å‘é‡ï¼š ä¸ºä»€ä¹ˆè¦ç”¨å‘é‡è¡¨ç¤ºè¯ï¼Ÿ Vectors turn out to be a really powerful representation for words, because a distributed representation allows words that have similar meanings, or similar grammatical properties, to have similar vectors. ç›¸è¿‘æ„æ€å’Œç›¸ä¼¼è¯­æ³•æ€§è´¨çš„è¯ï¼Œå…·æœ‰ç›¸ä¼¼çš„å‘é‡ã€‚ ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒè¯å‘é‡ï¼Œè¿™é‡Œæ˜¯ç”¨çš„4-gramï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªè¯å–å†³äºå‰ä¸‰ä¸ªè¯ï¼š input layer: 1x|V| embedding vector matrix: dx|V| projection layer: 1x3d hidden layer: W.shape (dhx3d) -&gt; 1xdh output layer: U.shape (|V|xdh) -&gt; 1x|V| softmax : $P(w_t=i|w_{t-1},w_{t-2},w_{t-3})$","link":"/2018/04/17/chapter8-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"title":"cs224d-lecture1-è¯å‘é‡è¡¨ç¤º","text":"Word Vectors Skip-gram Continuous Bag of words(CBOW) Negative Sampling Hierarchical SoftmaxM Word2Vec 1. How to represent words?With word vectors, we can quite easily encode this ability in the vectors themselves (using distance measures such as Jaccard, Cosine, Eu-clidean, etc). 2. Word Vectorsencode word tokens into some vector(N-dimensional space, N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfre using speech. one-hot vector: V is the size of vocabulary. each word is a completely independent entity. 3. Iteration Based Methods - Word2Vec 2 algorithms: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary. language model Unigram model : $$P(w_1,w_2,â€¦,w_n) = \\prod_{i=1}^nP(w_i)$$ bigram model: $$P(w_1,w_2,â€¦,w_n) = \\prod_{i=1}^nP(w_i|w_{i-1})$$ 4. Continuous bag of words model(CBOW)predict center word from the context. $$ \\prod_{c=1}^{n}P(w^{(c)}|w^{(c-m)},â€¦,w^{(c-1)},w^{(c+1)},â€¦,w^{(c+m)})$$ negative log likelihood: $$J(\\theta)= -\\sum_{c=1}^{n}logP(w^{(c)}|w^{(c-m)},â€¦,w^{(c-1)},w^{(c+1)},â€¦,w^{(c+m)})$$ the words of context to generate the center word is dependent: $$J(\\theta) = \\dfrac{1}{n}\\sum_{c=1}^T\\sum_{-m\\le j\\le m}logp(w_c|w_{c+j})$$ how to present this probability??? To one sentence: $$ \\begin{align} minimize J &amp;= -logP(w_c|w_{c-m},..,w_{c-1},w_{c+1},â€¦,w_{c+m})\\ &amp;= -log P(u_c|\\hat v)\\tag{1}\\ &amp;= -log \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{2}\\ &amp;= -u_c^T\\hat v + log\\sum_{j=1}^{|V|}exp(u_j^T\\hat v) \\end{align} $$ important: from word to vector the (1) to (2), using the softmax to present the probability $$P(u_c|\\hat v) = \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{* }$$ å…¶å®word2vecå¯ä»¥ç†è§£ä¸ºä¸¤ä¸ªwordï¼Œä»–ä»¬çš„ä¸Šä¸‹æ–‡è¶Šç›¸ä¼¼ï¼Œé‚£ä¹ˆä»–ä»¬ä¿©çš„è¯å‘é‡è¡¨ç¤ºä¹Ÿå°±è¶Šç›¸ä¼¼.æ¯”å¦‚ he å’Œ she å¤§å¤šæ•°æƒ…å†µä¸‹ä»–ä»¬çš„è¯­å¢ƒï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸‹æ–‡å‡ºç°çš„å•è¯véƒ½æ˜¯å¾ˆæ¥è¿‘çš„ï¼Œé‚£ä¹ˆåŒæ ·ä¸è¿™äº›è¯å†…ç§¯å¾—åˆ°çš„æ¦‚ç‡å°±ä¼šå·®ä¸å¤šï½è¯´åˆ°åº•ï¼Œä¹Ÿæ˜¯ä¸ªé¢‘ç‡ç»Ÿè®¡çš„æ–¹æ³•ï¼Œåªä¸è¿‡ç”¨äº†æ— ç›‘ç£å­¦ä¹ è¿™ä¸ªæ–¹å¼æ¥å¾—åˆ°distribution vectoräº†ï½ä»è¿™ä¸ªè§’åº¦ç†è§£å°±å¾ˆåˆç†äº†ã€‚é”™è¯¯çš„ç†è§£æ˜¯ u å’Œv å‡ºç°åœ¨åŒä¸€ä¸ªçª—å£ï¼Œä»–ä»¬çš„å†…ç§¯çš„æ¦‚ç‡å°±è¶Šå¤§ï¼Œè¿™æ— æ³•è§£é‡Šä»»ä½•ä¸œè¥¿ã€‚ 4.1 We can use an simple neural networt to train this matrix weightsinput: $x^{(c)}\\in R^{|V|\\times 1}$, the input one-hot vector of context labels: $y^{(c)}\\in R^{|V|\\times 1}$, the one hot vector of the known center word. parameters: $w_i$: word i from vocabulary V $V \\in R^{n\\times |V|}$ input word matrix $v_i$:i-th column of $V$, the input vector representation of word $w_i$ $U\\in R^{|V|\\times n}$: output word matrix $u_i$: i-th row of $U$, the output vector representation of word $w_i$ n is an arbitrary size which defines the size of our embedding space there are some differences with the figureâ€¦.$W_1^{n\\times |V|}$, $W_2^{|V|\\times n}$ input : $x_1.shape = (|V|, 1)$, $x_2.shape = (|V|, 1)$,â€¦,$x_{2m}.shape = (|V|, 1)$ $W_1$ : input matrix V, $W_1.shape = (n, |V|)$ each column is the representation of $w_i$ hidden layer: $\\hat v = \\dfrac{V.dot(x_1)+â€¦+V.dot(x_{2m})}{2m}$, $\\hat v.shape = (n,1)$ $W_2$ : output matrix U, $W_2.shape = (|V|, n)$ each row is the representation of $w_i$ score: $u.shape = (|V|, 1)$ output: $\\hat y = softmax(u)$, $\\hat y.shape=(|V|,1)$ cross entropy: $$H(\\hat y, y) = -\\sum_{j=1}^{|V|}y_jlog(\\hat y_j)$$ Because y is the one hot vector, and i is the index whose value is 1. $$H(\\hat y, y) = -y_ilog(\\hat y_i)$$ look at the paper word2vec Parameter Learning Explained, it is very cautious, very wonderful!!! The symbols are different from the above. 4.2 one word context inference 4.3 one word context backpropagation 4.4 multi-words context 5. Skip-gram$$ \\begin{align} minimize J &amp;=-logP(w_{c-m},â€¦,w_{c-1},w_{c+1},..,w_{c+m}|w_c)\\ &amp;=-log\\prod_{j=0,j\\neq m }^{2m}P(w_{c-m+j}|w_c)\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}logP(w_{c-m+j}|w_c)\\tag{3}\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}log\\dfrac{exp(u_{c-m+j}^Tv_c)}{\\sum_{k=1}^{|V|}exp(u_{k}^Tv_c)}\\tag{4}\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}u_{c-m+j}^Tv_c+2m\\ log\\sum_{k=1}^{|V|}exp(u_{k}^Tv_c) \\end{align} $$ important: from word to vector the (1) to (2), using the softmax to present the probability $$P(u_{c-m+j}|w_c) = \\dfrac{exp(u_{c-m+j}^Tv_c)}{\\sum_{j=1}^{|V|}exp(u_j^Tv_c)}\\tag{* }$$ V is the input matrix, U is the output matrix 5.1 We can use the simple neural networks to train matrix weights 5.2 inference and backpropagation Skip-gram treats each context word equally: the models computes the probability for each word of appearing in the context independently of its distance to the center word. 6. Optimizing Computational Efficiency6.1 Hirarchical Softmax 6.2 Negative Samplingloss function: $$E = -log\\sigma(vâ€™^T_{w_O}h)-\\sum_{w_j\\in W_{neg}}log\\sigma(-vâ€™^T_{w_j}h)$$ in the CBOW, $h=\\dfrac{1}{C}\\sum_{c=1}^Cv_{w_c^T}$ in the skip-gram, $h=v_{w_I}^T$ how to choose the K negative samples? As described in (Mikolov et al., 2013b), word2vec uses a unigram distribution raised to the 3/4th power for the best quality of results. å…³äºè´Ÿé‡‡æ ·çš„åŸç†çš„ç†è§£ï¼š Intuitive explanation of Noise Contrastive Estimation (NCE) loss? The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead. For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of kk randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors. This is important: instead of predicting the next word (the â€œstandardâ€ training technique), the optimized classifier simply predicts whether a pair of words is good or bad. reference: word2vec Parameter Learning Explained word2vecåŸç†æ¨å¯¼ä¸ä»£ç åˆ†æ","link":"/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"},{"title":"cs224d-lecture10 æœºå™¨ç¿»è¯‘å’Œæ³¨æ„åŠ›æœºåˆ¶","text":"ä¸»è¦å†…å®¹ï¼š Seq2Seq åŸºç¡€æ¨¡å‹ seq2seq encoder and decoder Attention Mechanisms: ä»‹ç»äº†ä¸‰ç§attention Bahdanau et al. NMT model: é‡ç‚¹æ˜¯æ€ä¹ˆè®¡ç®— context vector $c_i$ å‰è¨€å¯¹äºNERæ ‡æ³¨ï¼Œæ˜¯é€šè¿‡previous wordsé¢„æµ‹ä¸‹ä¸€ä¸ªword.è¿˜æœ‰ä¸€ç±»NLPä»»åŠ¡ï¼Œæ˜¯é’ˆå¯¹sequential output or outputs that are sequences of potentially varying length. æ¯”å¦‚ï¼š Translation: taking a sentence in one language as input and outputting the same sentence in another language Conversation: taking a statement or question as input and responding to it. Summarization: taking a large body of text as input and outputting a summary of it. è¿™ä¸€èŠ‚å†…å®¹è®²çš„å°±æ˜¯æ ¹æ®ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶sequence-tosequenceæ¨¡å‹ï¼Œç”¨æ¥å¤„ç†åºåˆ—ç”Ÿæˆçš„é—®é¢˜ã€‚ åºåˆ—ç”Ÿæˆçš„å†å²æ–¹æ³•: word-based system: æ— æ³•captureå¥å­ä¸­çš„è¯åº phrase-based system: æ— æ³•è§£å†³é•¿æ—¶é—´ä¾èµ–çš„é—®é¢˜ Seq2Seqæ¨¡å‹ï¼šcan generate arbitrary output sequences after seeing the entire input. They can even focus in on specific parts of the input automatically to help generate a useful translation. sequence-to-sequence Basics Sutskever et al. 2014, â€œSequence to Sequence Learning with Neural Networksâ€ Seq2Seq-encoder encoderçš„ç›®çš„å°±æ˜¯å°†input sentenceè¯»å…¥åˆ°æ¨¡å‹ä¸­ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªå›ºå®šç»´åº¦ context vector C. æ˜¾ç„¶ï¼Œå°±ä¸€ä¸ªä»»æ„é•¿åº¦çš„å¥å­çš„ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šç»´åº¦çš„å‘é‡ä¸­ï¼Œè¿™æ˜¯å¾ˆå›°éš¾çš„ã€‚æ‰€ä»¥encoderé€šå¸¸ä½¿ç”¨ stacked LSTMs. é€šå¸¸ä¼šå°†sentenceç¿»è½¬ä½œä¸ºè¾“å…¥ï¼Œä»¥æœºå™¨ç¿»è¯‘ä¸ºä¾‹ï¼Œç¿»è½¬åè¾“å…¥çš„æœ€åä¸€ä¸ªè¯å¯¹åº”çš„ç¿»è¯‘ï¼Œä¹Ÿå°±æ˜¯outputçš„ç¬¬ä¸€ä¸ªè¯ã€‚ æ˜æ˜¾æ„Ÿè§‰æ•ˆæœä¼šä¸å¤ªå¥½å¯¹å§ï¼Œæ‰€ä»¥ä¹Ÿå°±æœ‰äº†åæ¥çš„attention ä¸¾ä¸ªä¾‹å­ï¼š input sentenceï¼šâ€what is your nameâ€ é‚£ä¹ˆå¾—åˆ°çš„context vector å°±æ˜¯ a vector space representation of the notion of asking someone for their name. Seq2Seq-decoder decoderçš„ç›®çš„æ˜¯ç”Ÿæˆsentenceï¼Œåœ¨æœ€ä¸Šé¢ä¸€å±‚LSTMä¸Šæ¥ç€softmaxç”¨æ¥ç”Ÿæˆå½“å‰æ—¶é—´æ­¥çš„outputè¯ã€‚ç„¶åç”¨è¿™ä¸ªè¯ä½œä¸ºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„input word. ä¸€æ—¦å¾—åˆ°output sentence,é€šè¿‡æœ€å°åŒ–äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œæ¥è®­ç»ƒencoderå’Œdecoderä¸­çš„å‚æ•°ã€‚ Bidirectional RNNs Attention MechanismMotivationåœ¨seq2seqæ¨¡å‹ä¸­ï¼Œä½¿ç”¨å•ä¸€çš„ context vectorï¼šdifferent parts of an input have different levels of significance. Moreover, different parts of the output may even consider different parts of the input â€œimportant.â€ ä¹Ÿå°±æ˜¯è¯´è¾“å…¥sentenceä¸­ï¼Œæ¯ä¸ªè¯å¹¶ä¸æ˜¯å…·æœ‰åŒæ ·çš„é‡è¦ç¨‹åº¦çš„ã€‚æ¯”å¦‚ â€œthe ball is on the fieldâ€,æ˜¾ç„¶â€ballâ€ â€œonâ€ â€œfieldâ€æ¯”è¾ƒé‡è¦ã€‚ è€Œä¸”ï¼Œåœ¨è¾“å‡ºçš„æŸä¸€éƒ¨åˆ†ä¹Ÿå¯èƒ½æ›´çœ‹ä¸­inputä¸­çš„æŸä¸€éƒ¨åˆ†ã€‚é€šå¸¸outputä¸­çš„å‰å‡ ä¸ªè¯ä¸»è¦å´å–å†³äºinputä¸­çš„å‰å‡ ä¸ªè¯ï¼Œoutputä¸­çš„åå‡ ä¸ªè¯ä¸»è¦å–å†³äºinputä¸­çš„åå‡ ä¸ªè¯ã€‚ é‚£ä¹ˆAttention mechanismsé‡‡ç”¨çš„æ–¹æ³•æ˜¯ï¼š providing the decoder network with a look at the entire input sequence at every decoding step; the decoder can then decide what input words are important at any point in time. åœ¨decoderæ—¶é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®å®šåœ¨ä»»ä½•æ—¶åˆ»ç”Ÿæˆè¯æ—¶è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„æƒé‡ã€‚ Bahdanau et al. NMT modelåŸè®ºæ–‡ï¼š Bahdanau et al. Neural Machine Translation by Jointly Learning to Align and Translate Decoder: General description decoderä¸­ç”Ÿæˆä¸‹ä¸€ä¸ªè¯çš„æ¡ä»¶æ¦‚ç‡ï¼š $$P(y_i|y_1,â€¦,y_{i-1},X)=g(y_{i-1},s_i,c_i)$$ å…¶ä¸­ï¼Œå½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ $s_i$ è¡¨ç¤ºä¸ºï¼š $$s_i=f(s_{i-1},y_{i-1},c_i)$$ ä¹Ÿå°±æ˜¯ï¼š iæ—¶åˆ»ç”Ÿæˆæ­¤ $y_i$ å–å†³äº ä¸Šä¸€ä¸ªç”Ÿæˆè¯ $y_{i-1}$ (åœ¨ç”Ÿæˆåºåˆ—æ—¶ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥) å’Œ i-1æ—¶åˆ»çš„éšè—çŠ¶æ€ $s_{i-1}$ ä»¥åŠcontext vectorå¯¹åº”çš„å€¼ $c_i$. é‡ç‚¹æ˜¯æ€ä¹ˆè®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„context vector $c_i$ï¼š åœ¨æ ‡å‡†çš„seq2seqæ¨¡å‹ä¸­ï¼Œcontext vectoråªæœ‰ä¸€ä¸ªï¼Œä½†åœ¨attentionæ¨¡å‹ä¸­ï¼Œæ¯ä¸ªæ—¶é—´æ­¥éƒ½æœ‰å•ç‹¬çš„context vector $c_i$,å®ƒä¾èµ–äºè¾“å…¥åºåˆ—ä¸­çš„æ‰€æœ‰annotation $(h_1; Â· Â·Â· ; h_{T_x})$,å¹¶èµ‹äºˆä»–ä»¬ä¸€å®šçš„æƒé‡ã€‚ä¹Ÿå°±æ˜¯ï¼š $$c_i=\\sum_{j=1}^{T_x}\\alpha_{ij}h_j$$ å…¶ä¸­iè¡¨ç¤ºè¾“å‡ºåºåˆ—çš„ç¬¬iæ—¶åˆ»ï¼Œjè¡¨ç¤ºè¾“å…¥åºåˆ—çš„ç¬¬jä¸ªwordçš„annotation. å…¶ä¸­å¯¹äºæ¯ä¸ªè¾“å…¥è¯çš„annotationå³ $h_j$ çš„æƒé‡ $a_{ij}$ æ˜¯è¿™ä¹ˆè®¡ç®—çš„ï¼š $$\\alpha_{ij}=\\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$$ å…¶ä¸­ï¼š $$e_{ij}=a(s_{i-1},h_j)$$ æ˜¯å¯¹å…¶æ¨¡å‹(alignment model),$s_{i-1}$ è¡¨ç¤ºè¾“å‡ºåºåˆ—çš„éšè—çŠ¶æ€ï¼Œ $h_j$ è¡¨ç¤ºè¾“å…¥åºåˆ—çš„éšè—çŠ¶æ€ï¼Œæ‰€ä»¥ç”¨æ¥è®¡ç®—è¾“å…¥sentenceä¸­ç¬¬jä¸ªä½ç½®å’Œè¾“å‡ºåºåˆ—ä¸­ç¬¬iä¸ªä½ç½®åŒ¹é…çš„å¾—åˆ†(score).è¿™ä¸ªå¾—åˆ†æ˜¯åŸºäºdecoderä¸­çš„å‰ä¸€ä¸ªæ—¶åˆ»çš„éšè—çŠ¶æ€ $s_{i-1}$, å’Œè¾“å…¥åºåˆ—ä¸­çš„ç¬¬jä¸ªannotation $h_j$ã€‚ aæ˜¯ä»»æ„å‡½æ•°ï¼Œä¸”å¾—åˆ°çš„å€¼æ˜¯Rã€‚æ¯”å¦‚å¯ä»¥æ˜¯ä¸€ä¸ªå•å±‚çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œå¾—åˆ°äº†åºåˆ— $e_{i,1},â€¦,e_{i,n}$ï¼Œ ç„¶åä½¿ç”¨softmaxå¾—åˆ° $\\alpha_i=(\\alpha_{i,1},â€¦,\\alpha_{i,n})$. ç–‘é—®ï¼šçŸ¥é“äº† $e_{ij}$ çš„æ„ä¹‰ï¼Œä½†ä¸å¤ªæ˜ç™½æ€ä¹ˆè®¡ç®—ã€‚ã€‚è¿˜æ²¡çœ‹è®ºæ–‡ï¼ŒçŒœæƒ³æ—¢ç„¶aæ˜¯ä»»æ„å‡½æ•°ï¼Œé‚£ä¹ˆåº”è¯¥å°±æ˜¯ç”¨ç¥ç»ç½‘ç»œæ¥è¡¨ç¤ºäº†ã€‚ æ€»ç»“ä¸‹æ¥ï¼š Let $\\alpha_{ij}$ be a probability that the target word yi is aligned to, or translated from, a source word $x_j$. Then, the i-th context vector $c_i$ is the expected annotation over all the annotations with probabilities $\\alpha_{ij}$. æ‰€ä»¥ $\\alpha_{ij}$ è¡¨ç¤ºçš„å°±æ˜¯ä»è¯ $x_j$ translate to (or align to) è¯ $y_i$ çš„æ¦‚ç‡ã€‚ä¹Ÿå°±æ˜¯è¯´ outputä¸­ç¬¬iä¸ªè¯ $y_i$ å¯èƒ½ç”± inputä¸­çš„ä»»æ„ä¸€ä¸ªè¯å¯¹é½è€Œæ¥çš„ï¼Œä¹Ÿä¸ä¸€å®šå°±æ˜¯ç¿»è¯‘ï¼Œå°±æ˜¯ ç¿»è¯‘ $y_i$ çš„æ—¶å€™ï¼Œinputä¸­æ¯ä¸€ä¸ªå¯¹å®ƒçš„å½±å“ç¨‹åº¦ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªæƒé‡å€¼ $\\alpha_{ij}$. è€Œè¿™ä¸ªæ¦‚ç‡ $\\alpha_{ij}$ ä»¥åŠå…¶ associated energy $e_{ij}$ åæ˜ äº† $s_{i-1}$ å’Œ $h_j$ å¯¹ç”Ÿæˆä¸‹ä¸€ä¸ªwordçš„é‡è¦æ€§ã€‚ ç–‘é—®ï¼šåœ¨è®­ç»ƒçš„æ—¶å€™å¯ä»¥é€šè¿‡åå‘ä¼ æ’­ï¼Œå¾—åˆ°å‚æ•° $c_i$ï¼Œä½†æ˜¯è¿™ä¸ªæƒé‡ä¹Ÿåªèƒ½ç”¨äºå½“å‰çš„åºåˆ—å§ã€‚ã€‚æµ‹è¯•çš„æ—¶å€™ï¼Œè¿™äº›è®­ç»ƒçš„å‚æ•°è¿˜èƒ½ç”¨ä¹ˆï¼Ÿ Encoder: bidirectional RNN for annotation sequencesåœ¨encoderæ—¶ï¼Œå°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºannotation $(h_1,h_2,â€¦,h_{T_x})$. ä¸ºäº†æ—¢è€ƒè™‘preceding wordsï¼Œåˆè€ƒè™‘ following wordsï¼Œé‡‡ç”¨åŒå‘RNNï¼ˆBiRNNï¼‰. forward RNN $\\overrightarrow f$ reads input sentence (from $x_1$ to $x_{T_x}$): $$(\\overrightarrow h_1,â€¦,\\overrightarrow h_{T_x})$$ backward RNN $\\overleftarrow f$ reads input sentence in the reverse order (from $x_{T_x}$ to $x_1$): $$(\\overleftarrow h_1,â€¦,\\overleftarrow h_{T_x})$$ annotation for $x_j$: $$h_j=[\\overrightarrow {h_{T_j}^T},\\overleftarrow {h_{T_j}^T}]$$ Connection with translation alignmentåœ¨è®­ç»ƒçš„decoderè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å¾—åˆ°è¿™æ ·çš„ä¸€ä¸ªalignment table, a table mapping words in the source to corresponding words in the target sentence.ä½¿ç”¨attention score $\\alpha_{i,j}$ å¡«å……è¿™ä¸ªè¡¨æ ¼ã€‚ è¿™å°±è§£å†³äº†ä¹‹å‰çš„ç–‘æƒ‘äº†ï¼Œåœ¨æµ‹è¯•çš„æ—¶å€™ï¼Œcontext vectorçš„æƒé‡ $\\alpha_i$ ç›´æ¥é€šè¿‡æŸ¥è¡¨å¾—åˆ°ï½ Huong et al. NMT modelåŸè®ºæ–‡ï¼š Effective Approaches to Attentionbased Neural Machine Translation by Minh-Thang Luong, Hieu Pham an Christopher D. Manning Global attentionencoder éšè—çŠ¶æ€åºåˆ—ï¼š $h_1,â€¦,h_n$ ï¼Œnè¡¨ç¤ºåºåˆ—é•¿åº¦ decoder éšè—çŠ¶æ€åºåˆ—ï¼š $\\overline h_1,â€¦,\\overline h_n$ å¯¹äºæ¯ä¸€ä¸ªdecoderä¸­çš„éšè—çŠ¶æ€ $\\overline h_i$ï¼Œè®¡ç®—å…¶åŸºäºæ‰€æœ‰encoderéšè—çŠ¶æ€çš„attention vector $c_i$. $$ score(h_i^T\\overline h_j)=\\begin{cases} h_i^T\\overline h_j \\ h_i^TW\\overline h_j \\quad &amp; \\text{$\\in R$}\\ W[h_i,\\overline h_j] \\end{cases} $$ ç±»ä¼¼äºBahdanau et al. NMT modelä¸­çš„ $e_{ij}$, åŒæ ·çš„éœ€è¦å¾—åˆ°çš„æƒé‡ $\\alpha_{ij}$ æ˜¯æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯encoderä¸­çš„ $h_i$ ä¸ decoderä¸­çš„ $h_j$ åŒ¹é…çš„æ¦‚ç‡, attention vector $\\alpha_{i,j}$ï¼š $$\\alpha_{i,j}=\\dfrac{exp(score(h_i^T\\overline h_j))}{\\sum_{k=1}^nexp(score(h_k^T\\overline h_j))}$$ é‚£ä¹ˆcontext vector: $$c_i=\\sum_{j=1}^n \\alpha_{i,j}h_j$$ é‚£ä¹ˆä½¿ç”¨context vectorå’Œéšè—çŠ¶æ€ $\\overline h_i$ ç”Ÿæˆæ–°çš„decoderä¸­ç¬¬iæ—¶é—´æ­¥çš„æ–°çš„ vector $$\\tilde h_i=f[\\overline h_i,c_i]$$ Local Attentionthe model predicts an aligned position in the input sequence. Then, it computes a context vector using a window centered on this position. The computational cost of this attention step is constant and does not explode with the length of the sentence. windowæ€ä¹ˆé€‰ï¼Ÿï¼Ÿ Christopher å¥½åƒè¯´ç”¨å¼ºåŒ–å­¦ä¹ ã€‚ã€‚ Googleâ€™s new NMT6 Johnson et el. 2016, â€œGoogleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translationâ€ The new multilingual model not only improved their translation performance, it also enabled â€œzero-shot translation,â€ in which we can translate between two languages for which we have no translation training data. For instance, if we only had examples of Japanese-English translations and Korean-English translations, Googleâ€™s team found that the multilingual NMT system trained on this data could actually generate reasonable Japanese-Korean translations. The powerful implication of this finding is that part of the decoding process is not language-specific, and the model is in fact maintaining an internal representation of the input/output sentences independent of the actual languages involved. More advanced papers using attention Show, Attend and Tell: Neural Image Caption Generation with Visual Attention by Kelvin Xu, Jimmy Lei Ba,Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel and Yoshua Bengio. This paper learns words/image alignment. Modeling Coverage for Neural Machine Translation by Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu and Hang Li. Their model uses a coverage vector that takes into account the attention history to help future attention. Incorporating Structural Alignment Biases into an Attentional Neural Translation Model by Cohn, Hoang, Vymolova, Yao, Dyer, Haffari. This paper improves the attention by incorporating other traditional linguistic ideas. Sequence model decodersä½¿ç”¨ç»Ÿè®¡çš„æ–¹æ³•æ‰¾åˆ°æœ€åˆé€‚çš„sequence $\\hat s*$ï¼š $$\\hat s* = argmax_{\\hat s}(P(\\hat s|s))$$ Exhaustive search: NPé—®é¢˜ Ancestral sampling $$x_t \\sim P(x_t|x_1,..,x_n)$$ Greedy search $$x_t=argmax_{\\tilde x_t}P(\\tilde x_t|x_1,â€¦,x_n)$$ å¦‚æœå…¶ä¸­ä¸€æ­¥é”™äº†ï¼Œå¯¹æ¥ä¸‹æ¥çš„åºåˆ—å½±å“å¾ˆå¤§ã€‚ Beam searchï¼š the idea is to maintain K candidates at each time step. PresentationGoogleâ€™s Multilingual Neural Machine Translation System: Enabling zero-short Translation","link":"/2018/05/08/cs224d-lecture10-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"title":"cs224d-lecture11 å†çœ‹GRUå’ŒNMT","text":"ä¸»è¦å†…å®¹ï¼š GRUè¿›ä¸€æ­¥ç†è§£ GRUå’ŒLSTMå¯¹æ¯” LSTMçš„è¿›ä¸€æ­¥ç†è§£ è®­ç»ƒRNNçš„ä¸€äº›tips Ensemble MT Evaluation ç”Ÿæˆè¯ä½¿ç”¨softmaxå¯¼è‡´çš„è®¡ç®—é‡è¿‡å¤§çš„é—®é¢˜ presentation GRU è¿›ä¸€æ­¥ç†è§£shortcut connection adaptive shortcut connection ä½¿ç”¨update gate è‡ªé€‚åº”çš„å¢åŠ shortcut connection prune unnecessary connections adaptively ä½¿ç”¨reset gateè‡ªé€‚åº”çš„ä¿®å‰ªä¸å¿…è¦çš„è¿æ¥ã€‚ çªç„¶æƒ³åˆ°ä¸ªé—®é¢˜ï¼Œä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œå…·æœ‰è‡ªé€‚åº”æ€§ï¼Ÿæˆ‘ä¸ªäººçš„ç†è§£æ˜¯ï¼Œç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªå‚æ•°å­¦ä¹ å’Œæ‹Ÿåˆçš„è¿‡ç¨‹ï¼Œåœ¨æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¾—åˆ°ä¼˜åŒ–ä½¿å…¶å…·æœ‰è‡ªé€‚åº”æ€§ã€‚ question1:how you select the readable subset based on this reset gate? $$r_t=\\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})\\tag{reset gate}$$ $$\\tilde h_t=tanh(Wx_t+r_t\\circ Uh_{t-1})\\tag{new memory}$$ the reset gate decides which parts of the hidden state to read to update the hidden state. So, the reset gate calculates which parts to read based on the current input and the previous hidden state. So itâ€™s gonna say, okay, I wanna pay a lot of attention to dimensions 7 and 52. And so, those are the ones and a little to others. And so those are the ones that will be being read here and used in the calculation of the new candidate update, which is then sort of mixed together with carrying on what you had before. å¯¹æ­¤ï¼ŒChristopherè€å¤´å„¿è¿˜ä¸¾äº†ä¸ªä¾‹å­ï¼Œåœ¨éšè—çŠ¶æ€ä¸­åŠ¨è¯ä¿å­˜åœ¨47-52 dimensions,å½“é‡åˆ°æ–°çš„verbæ˜¯ï¼Œéšè—çŠ¶æ€çš„è¿™éƒ¨åˆ†ç»´åº¦å°±ä¼šå¾—åˆ°æ›´æ–°ã€‚çœ‹åˆ°è¿™ï¼ŒçœŸæƒ³è¯•è¯•æ‰“å°å‡º $r_t$ çœ‹çœ‹å®ƒéšæ—¶é—´æ­¥çš„å˜åŒ–æƒ…å†µã€‚ã€‚ question2:how you select the writable subset based on this update gate? $$u_t=\\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})\\tag{update gate}$$ $$h_t=(1-u_t)\\circ \\tilde h_t+u_t\\circ h_{t-1} \\tag{Hidden state}$$ some of the hidden state weâ€™re just gonna carry on from the past. Weâ€™re only now going to edit part of the register. And saying part of the register, I guess is a lying and simplifying a bit, because really, youâ€™ve got this vector of real numbers and some said the part of the register is 70% updating this dimension and 20% updating this dimension that values could be one or zero but normally they wonâ€™t be. So I choose the writable subset And then itâ€™s that part of it that Iâ€™m then updating with my new candidate update which is then written back, adding on to it. And so both of those concepts in the gating, the one gate is selecting what to read for your candidate update. And the other gate is saying, which parts of the hidden state to overwrite? æ„Ÿè§‰æ„æ€æ˜¯ï¼Œupdate gateä¸»è¦æ˜¯ä¸ºäº†æ§åˆ¶ç”Ÿæˆå½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ $h_t$ï¼Œå¦‚æœæ›´æ–°é—¨çš„å€¼éƒ½æ˜¯1ï¼Œé‚£å°±ä»¥ä¸ºç€ä¿å­˜æ‰€æœ‰çš„ä»¥å‰çš„ä¿¡æ¯ã€‚ question3:how does these gates avoid gradient vanishing? $$h_t=f(h_{t-1},x_t)=u_t \\circ \\tilde h_t + (1-u_t)\\circ h_{t-1}$$ the secret is this plus sign. åœ¨å›è¿‡å¤´æ¥çœ‹ä¸€ä¸‹æ¢¯åº¦æ¶ˆå¤±çš„é‚£ä¸ªå…¬å¼ï¼š $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}\\dfrac{\\partial h_t}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}$$ ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬è¦æ±‚t=3æ—¶åˆ»çš„æŸå¤±å‡½æ•°å¯¹ $W_{hh}$ çš„å¯¼æ•°ï¼Œé‚£ä¹ˆï¼š $$\\begin{align} \\dfrac{\\partial E_3}{\\partial W} &amp;=\\sum_{k=1}^3\\dfrac{\\partial E_3}{\\partial y_3}\\dfrac{\\partial y_3}{\\partial h_3}\\dfrac{\\partial h_3}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}\\ &amp;=\\dfrac{\\partial E_3}{\\partial y_3}\\dfrac{\\partial y_3}{\\partial h_3}(\\dfrac{\\partial h_3}{\\partial W}+\\dfrac{\\partial h_3}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial W}+\\dfrac{\\partial h_3}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial h_1}\\dfrac{\\partial h_1}{\\partial W}) \\end{align}$$ å¯ä»¥çœ‹åˆ°å¾ˆæ—©ä¹‹å‰çš„éšè—çŠ¶æ€ $h_1$ éšç€æ—¶é—´çš„å¢é•¿ï¼Œå¯¹å½“å‰æ—¶åˆ»çš„å½±å“è¶Šæ¥è¶Šå°ã€‚è€Œåœ¨GRUä¸­ï¼Œå½“update gate $u_t=0$ æ—¶ï¼Œ$h_3=h_2$,è¿™è¯´æ˜ä¹‹å‰çš„éšè—çŠ¶æ€å­˜å‚¨çš„ä¿¡æ¯èƒ½æœ‰æ•ˆçš„ä¼ é€’ä¸‹æ¥ï¼Œ thatâ€™s why it can carry information for a very long time.ã€‚ question4:how long does a GRU actually end up remembering for? Answer: I kind of think order of magnitude the kind number you want in your head is 100 steps. So they donâ€™t remember forever I think thatâ€™s something people also get wrong. question5:Does GRU train faster than lstm? Answer: LSTMs have a slight edge on speed. No huge difference. GRUå’ŒLSTMçš„åŒºåˆ« question6:LSTMsä¸­ä¸ºä»€ä¹ˆ $h_t=o_t\\circ tanh(c_t)$ ä¸­è¦ç”¨åˆ°tanhï¼Ÿ TA Richardçš„è§£é‡Šæ˜¯ï¼Œå¯¹äº new memory cell $\\tilde c = f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$ è¿™æ˜¯ä¸€ä¸ªçº¿æ€§çš„layerï¼ŒåŠ ä¸Štanhéçº¿æ€§å› ç´ ï¼Œèƒ½è®©lstmæ›´powerful. LSTM ç›´è§‚å›¾è§£ å¯ä»¥è¯´æ˜¯å¾ˆæ¸…æ¥šäº†ï½ï½ä¸è¿‡è¿™é‡Œæœ‰ç‚¹åŒºåˆ«,å°† $h_{t-1}$ å’Œ $x_t$ concatåœ¨ä¸€èµ·äº†ï¼Œæ¯”å¦‚ä¸‰ä¸ªgate: $$i_t = \\sigma (W_i[h_{t-1},x_t]+b_i)\\tag{input/update gate}$$ $$o_t = \\sigma (W_o[h_{t-1},x_t]+b_o)\\tag{output gate}$$ $$f_t = \\sigma (W_f[h_{t-1},x_t]+b_f)\\tag{forget gate}$$ è€Œæ›´æ–°çš„ new memory cell $\\tilde c_t$: $$\\tilde c_t=\\tanh(W_c[h_{t-1}, x_t]+b_c)$$ æœ€ç»ˆçš„è®°å¿†ç»†èƒçŠ¶æ€ $c_t$: $$c_t= f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$$ æœ€ç»ˆçš„éšè—çŠ¶æ€ $h_t$: $$h_t=o_t\\circ tanh(c_t)$$ LSTMçš„æ ¸å¿ƒï¼Œç±»ä¼¼äºresnet: ç”¨åŠ å’Œï¼Œä¹Ÿå°±æ˜¯å›¾ä¸­çš„plus signï¼ŒåŸæœ¬çš„rnnçš„ä»…æœ‰matrix multiplyï¼Œä½¿å¾—ç½‘ç»œå…·æœ‰ long dependency. è®­ç»ƒrnnçš„ä¸€äº›ç»éªŒ ç¬¬7ç‚¹ï¼Œä¸‡ä¸‡ä¸èƒ½dropout horizontally,é‚£æ ·ä¼šä¸¢å¤±å¾ˆå¤šä¿¡æ¯ã€‚ Ensemble ä¹‹å‰çœ‹åˆ°åœ¨cnné‡Œé¢ï¼Œdropoutå¯ä»¥çœ‹åšæ˜¯å¾ˆå¤šæ¨¡å‹çš„é›†æˆï¼Œä¸çŸ¥é“rnnæ˜¯å¦ä¹Ÿå¯ä»¥ã€‚ MT Evaluationå…³äºæœºå™¨ç¿»è¯‘çš„æ¨¡å‹éªŒè¯ notoriously tricky and subjective taskï¼Œè‡­åæ˜­è‘—çš„æ£˜æ‰‹ä»¥åŠéå¸¸å…·æœ‰ä¸»è§‚æ€§ã€‚ BLEU: a Method for Automatic Evaluation of Machine Translation åŸç†ï¼š n-gram matches $p_n$ = # matched n-grams / # n-grams in candidate translation å…¶å®å°±æ˜¯ precision $p_n$ è¡¨ç¤º n-gram çš„precision score. å¹¶ä¸”ï¼Œä½¿ç”¨ $w_n=1/2^n$ ä½œä¸ºå¯¹åº”çš„æƒé‡ã€‚ brevity penaltyï¼šçŸ­è¯‘å¥å®¹æ˜“å¾—é«˜åˆ†ï¼Œå› æ­¤éœ€è¦ç»™äºˆæƒ©ç½š $$BP=\\begin{cases} 1, &amp; \\text{if c &gt; r}\\ e^{1-r/c}, &amp; \\text{if c $\\le r$} \\end{cases}$$ BLEU: $$BLEU=BP\\cdot exp(\\sum_{n=1}^Nw_nlogp_n)$$ åœ¨å¯¹æ•°åŸŸï¼š $$log BLEU=min(1-\\dfrac{r}{c},0)+\\sum_{n=1}^Nw_nlogp_n$$ åˆæ˜¯softmaxçš„é—®é¢˜ åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œä»éšè—çŠ¶æ€åˆ°è¯è¡¨ï¼Œä½¿ç”¨softmaxè¿™ä¸€æ­¥éå¸¸æ¶ˆè€—è®¡ç®—åŠ›ã€‚ è§£å†³æ–¹æ³•ï¼š Not GPU-friendly! ä¸çŸ¥é“ä¸ºå•¥ã€‚ã€‚æ„Ÿè§‰éœ€è¦å¥½å¥½äº†è§£ä¸‹GPU åŸè®ºæ–‡ï¼š On Using Very Large Target Vocabulary for Neural Machine Translation Word and character-based models??? presentation","link":"/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/"},{"title":"cs224d-lecture13 å·ç§¯ç¥ç»ç½‘ç»œ","text":"ä¸»è¦å†…å®¹ï¼š ä¸ºä»€ä¹ˆè¦ä½¿ç”¨CNN single layer connection pooling: max-pooling Multiple-Filters: multiple n-grams Multiple-Channels: ä¸¤ä¸ªè¯å‘é‡ static, dynamic lassification after one CNN layer ä¸ºä»€ä¹ˆè¦ä½¿ç”¨CNNï¼ŸCNNæ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ä¸è¯­éŸ³è¯†åˆ«æ–¹é¢å–å¾—äº†å“è¶Šçš„æˆå°±. åœ¨ NLP ä¹Ÿæ˜¯å¯ä»¥çš„. å·ç§¯å…·æœ‰å±€éƒ¨ç‰¹å¾æå–çš„åŠŸèƒ½, æ‰€ä»¥å¯ç”¨ CNN æ¥æå–å¥å­ä¸­ç±»ä¼¼ n-gram çš„å…³é”®ä¿¡æ¯. Single layer connectionConvolution Neural Networks for Sentence Classification å¯¹äºå•ä¸ªè¯ï¼Œå…¶è¯å‘é‡ï¼š$x_i\\in R^k$, kç»´è¯å‘é‡ nä¸ªè¯concatenateåœ¨ä¸€èµ·è¡¨ç¤ºä¸ºï¼š$x_{1:n}=x_1\\bigoplus x_2 \\bigoplusâ€¦\\bigoplus x_n$ å·ç§¯è¿‡æ»¤å™¨filter: $w\\in R^{hk}$ hè¡¨ç¤ºè¿‡æ»¤å™¨çš„å°ºå¯¸ï¼Œä¹Ÿå°±æ˜¯è¦†ç›–å¤šå°‘ä¸ªè¯. ä¸Šå›¾figure13è¡¨ç¤ºï¼š k=2,n=5,h=3 éœ€è¦æ³¨æ„çš„æ˜¯filteræ˜¯ä¸€ä¸ªé•¿åº¦ä¸º hk çš„ vectorï¼Œä¸inputçš„æ¯ä¸€ä¸ªæ—¶é—´æ­¥åšä¸€æ¬¡å·ç§¯ï¼ˆåŠ æƒå’Œï¼‰å¾—åˆ°ä¸€ä¸ªå®æ•°: $$c_i=f(W^Tx_{i:i|h-1}+b)$$ é‚£ä¹ˆè¾“å‡ºå‘é‡ï¼š$c=[c_1,c_2,â€¦,c_{n-h+1}]\\in R^{n-h+1}$ å¦‚æœh=3ï¼Œé‚£ä¹ˆæœ€åä¸¤ä¸ªè¯ my birth å°±ä¸å¤Ÿå·ç§¯ï¼Œå¯ä»¥å¦‚å›¾figure14æ‰€ç¤ºé‡‡ç”¨ h-1 zero-vector padding. poolingmax-pooling: $$\\hat c=max{c}, c\\in R$$ filterå¯ä»¥çœ‹åšå’Œå›¾åƒä¸­çš„filterä¸€æ ·ï¼Œä¸€ä¸ªå›¾åƒç‰¹å¾æå–å™¨ï¼Œé‚£ä¹ˆåœ¨æ–‡æœ¬ä¸­ï¼Œä¸€ä¸ªfilterä¹Ÿå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªn-gramç‰¹å¾æå–å™¨ï¼Œæ¯”å¦‚ä¸€ä¸ªç”¨æ¥è¡¨ç¤ºpositive bigram,é‚£ä¹ˆè¿™ä¸ªfilterå’Œå¥å­ä¸­åŒæ ·è¡¨ç¤ºpositive bigramåšå·ç§¯çš„è¯ï¼Œå…¶å€¼å°±ä¼šå¾ˆå¤§ï½é‚£ä¹ˆä½¿ç”¨max-poolingå°±æ˜¯æ‰¾åˆ°è¿™ä¸ªå€¼ å½“ç„¶ä½¿ç”¨min-poolingä¹Ÿæ˜¯å¯ä»¥çš„.ä½†æ›´å¤šçš„æ—¶å€™æˆ‘ä»¬é€‰æ‹©ç”¨ reluä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œé‚£ä¹ˆä½¿ç”¨min-poolingçš„è¯ï¼Œå°±ä¼šå‡ºç°æ›´å¤šçš„ 0. Multiple-FiltersWe can use multiple bi-gram filters because each filter will learn to recognize a different kind of bi-gram. Even more generally, we are not restricted to using just bi-grams, we can also have filters using tri-grams, quad-grams and even higher lengths. Each filter has an associated max-pool layer. Thus, our final output from the CNN layers will be a vector having length equal to the number of filters. åŒæ—¶ä½¿ç”¨å¥½å‡ ä¸ªbigramï¼Œç”¨ä»¥è·å–ä¸åŒçš„pattern. é™¤äº†bigramï¼Œè¿˜ä¼šä½¿ç”¨trigram,unigramç­‰ç­‰ã€‚ Multiple-Channelsæˆ‘ä»¬æœ‰æ—¶å€™ä¼šéœ€è¦åœ¨ç‰¹å®šçš„åœºæ™¯ä¸‹æ›´æ–°è¯å‘é‡ï¼Œä¹Ÿå°±æ˜¯ä¹Ÿè®­ç»ƒè¯å‘é‡å‚æ•°ï¼Œè¿™æ ·èƒ½å¤Ÿé€‚åº”æ›´ç‰¹æ®Šçš„ä»»åŠ¡ã€‚ä½†å¦‚æœåœ¨testä¸­å‡ºç°äº†trainä¸­æ²¡æœ‰å‡ºç°çš„è¯unseen wordï¼Œ é‚£ä¹ˆè¿™ä¸ªè¯çš„è¯å‘é‡è¿˜ä¼šä¿æŒåˆå§‹è¯å‘é‡çš„å€¼ï¼ˆGloveç­‰ï¼‰ã€‚ä½†æ˜¯ä¸è¿™ä¸ªè¯è¯­ä¹‰ç›¸å…³çš„è¯çš„è¯å‘é‡å´å‘ç”Ÿäº†å˜åŒ–ï¼Œè¿™æ ·å°±é€ æˆäº†ç›¸ä¼¼çš„è¯çš„çš„è¯å‘é‡ç›¸å·®è¾ƒå¤§ã€‚è¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚ æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªè¯å‘é‡ï¼Œone â€™staticâ€™ (no gradient flow into them) and one â€™dynamicâ€™, which are updated via SGD. Backprop into only one set, keep other â€œstaticâ€ Both channels are added to $c_i$ before max-pooling. Classification after one CNN layer First one convolution, followed by one max-pooling To obtain final feature vector: $z=[\\hat c_1,â€¦,\\hat c_m]$ å‡è®¾æœ‰mä¸ªè¿‡æ»¤å™¨filter Simple final softmax layer $y=softmax(W^{(S)}z+b)$ Convolution Neural Networks for Sentence Classification è®ºæ–‡ä¸­çš„æ¨¡å‹ï¼š ç¬¬ä¸€å±‚ï¼š two word-vector channels $n\\times k\\times 2$ ç¬¬äºŒå±‚ï¼š mä¸ªfilterå¾—åˆ°çš„måˆ—feature mapsï¼Œç”±äºæœ‰å¤šç§filterå°ºå¯¸ï¼Œå¦‚æœæ²¡æœ‰zero-paddingçš„è¯ï¼Œé‚£ä¹ˆå¾—åˆ°çš„feature mapsé•¿åº¦æ˜¯ä¸ä¸€è‡´çš„ã€‚ ç¬¬ä¸‰å±‚ï¼šmax-pooling $z=[\\hat c_1,â€¦,\\hat c_m]$ ç¬¬å››å±‚ï¼šfully connectioned layer with dropout and softmax output. Tricks: Dropout$$y=softmax(W^{(S)}(r\\circ z)+b)$$ é’ˆå¯¹dropoutï¼Œåœ¨trainå’Œtestæ—¶ï¼Œå¤„ç†æ–¹å¼æ˜¯ä¸ä¸€æ ·çš„ï¼š æ¨¡å‹å‚æ•°é€‰æ‹©é—®é¢˜ CNNæ›´å®¹æ˜“å®ç°åœ¨GPUä¸Šçš„å¹¶è¡Œå¤„ç†ã€‚ CNNçš„å˜ç§ä»¥åŠåº”ç”¨ Narrow vs Wide Narrowå°±æ˜¯æ²¡æœ‰zero-padding: é‚£ä¹ˆoutputé•¿åº¦å°±æ˜¯ $n-h+1$ Wideå°±æ˜¯å‰åä¸¤ç«¯éƒ½æœ‰h-1 zero-padding: é‚£ä¹ˆoutputé•¿åº¦å°±æ˜¯ï¼š $[n+2\\times (h-1)]- h+1=n+h-1$ k-max pooling ç›¸æ¯”max-poolingï¼Œ k-max poolingæ˜¯é€‰å‡ºæœ€å¤§çš„kä¸ªå€¼ æ¨¡å‹å¯¹æ¯”æ€»ç»“ PresentationCharacter-Aware neural language models,Yon Kim at al.","link":"/2018/05/14/cs224d-lecture13-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"chapter3-æœ‰é™çŠ¶æ€è‡ªåŠ¨æœº","text":"æœ‰é™çŠ¶æ€æœº FSA ç¡®å®šæ€§çš„è¯†åˆ«å™¨ DFSA ç¡®å®šæ€§çš„è¯†åˆ«å™¨ NFSAï¼šæ·±åº¦ä¼˜å…ˆæœç´¢ or å¹¿åº¦ä¼˜å…ˆæœç´¢ å½¢å¼è¯­è¨€ æœ‰é™çŠ¶æ€è‡ªåŠ¨æœº finite-state automation, FSAå‰é¢ä¸€ç« èŠ‚ä¸­çš„æ­£åˆ™è¡¨è¾¾å¼åªæ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬æœç´¢çš„æ–¹ä¾¿çš„å…ƒè¯­è¨€ï¼Œå®ƒæ˜¯æè¿°æœ‰é™çŠ¶æ€æœºçš„ä¸€ç§æ–¹æ³•ã€‚ ä»»ä½•æ­£åˆ™è¡¨è¾¾å¼åˆå¯ä»¥ç”¨æœ‰é™çŠ¶æ€æœºæ¥å®ç°ï¼ˆé™¤äº†ä½¿ç”¨å­˜å‚¨ç‰¹æ€§çš„é‚£äº›æ­£åˆ™è¡¨è¾¾å¼ï¼‰ã€‚æ¯”å¦‚ä¸Šä¸€ç« ä¸­ç”¨äºæè¿°ç¾Šçš„è¯­è¨€çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š /baa+!/ å°±å¯ä»¥è¡¨ç¤ºä¸ºè¿™æ ·çš„æœ‰é™è‡ªåŠ¨æœºï¼š FSAå¯ä»¥ç”¨æ¥è¯†åˆ«ï¼ˆæˆ–æ¥å—ï¼‰ç¬¦å·ä¸²ã€‚æ¥å—æ–¹å¼å¦‚ä¸‹ï¼šæŠŠè¾“å…¥æƒ³è±¡æˆä¸€ä¸ªé•¿é•¿çš„å¸¦å­(tape),å¸¦å­ä¸Šçš„ä¸€ä¸ªå•å…ƒæ ¼(cell)å¯ä»¥å†™ä¸€ä¸ªç¬¦å·ï¼š æ€ä¹ˆç†è§£è¿™ä¸ªè¾“å…¥å¸¦å­å’Œè‡ªåŠ¨æœºå‘¢ï¼Ÿå°±æ˜¯å½“è¾“å…¥å¸¦ä¸Šçš„å­—æ¯å’Œè‡ªåŠ¨æœºä¸­ç¦»å¼€å½“å‰çŠ¶æ€çš„å¼§ç›¸åŒ¹é…æ—¶ï¼Œå°±ç©¿è¿‡è¿™ä¸ªå¼§ï¼Œè¿›å…¥åˆ°è‡ªåŠ¨æœºä¸­çš„ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¸¦å­ä¸Šçš„ä¸‹ä¸€ä¸ªç¬¦å·ã€‚æ¯”å¦‚ä¸Šå›¾ä¸­åˆå§‹çŠ¶æ€ $q_0$ åªæœ‰é‡åˆ°bæ—¶æ‰ä¼šåŒ¹é…æˆåŠŸè€Œè¿›å…¥ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œè¾“å…¥ç¬¦å·ï¼Œæ‰€ä»¥ä¸Šå›¾ä¸­çš„è‡ªåŠ¨æœºä¸èƒ½æ¥å—è¾“å…¥ã€‚å¯¹å›¾2.10ä¸­ $q_3$ çŠ¶æ€ï¼Œä»–å¯ä»¥åŒ¹é… aæˆ–!. æˆ‘ä»¬å¯ä»¥ç”¨çŠ¶æ€è½¬ç§»è¡¨(state-transition table)æ¥è¡¨ç¤ºè‡ªåŠ¨æœºã€‚çŠ¶æ€è½¬ç§»è¡¨å¯ä»¥è¡¨ç¤ºåˆå§‹çŠ¶æ€ã€æ¥æ”¶çŠ¶æ€å’Œç¬¦å·åœ¨çŠ¶æ€ä¹‹é—´çš„è½¬ç§»æƒ…å†µã€‚ ä¸€ä¸ªæœ‰é™è‡ªåŠ¨æœºå¯ä»¥ç”¨ä¸‹é¢5ä¸ªå‚æ•°æ¥å®šä¹‰ï¼š å…¶ä¸­Qè¡¨ç¤ºNç§çŠ¶æ€çš„é›†åˆï¼Œ$\\Sigma$ è¡¨ç¤ºæœ‰é™çš„è¾“å…¥ç¬¦å·å­—æ¯è¡¨ï¼Œ $q_0$æ˜¯åˆå§‹çŠ¶æ€ï¼Œ $F\\in Q$ æ˜¯ç»ˆæçŠ¶æ€ã€‚ $\\delta(q,i)$ æ˜¯è½¬ç§»çŸ©é˜µï¼Œå…¶ä¸­ $q\\in Q,i\\in \\Sigma$. $\\delta(q,i)$ è¿”å›ä¸€ä¸ªæ–°çš„çŠ¶æ€ï¼Œ$qâ€™\\in Q$ï¼Œåˆ™ $\\delta(q,i)$ æ˜¯ä» $Q \\times \\Sigma$ åˆ° $Q$ çš„ä¸€ä¸ªå…³ç³». åœ¨HMMä¸­ï¼ŒçŠ¶æ€è½¬ç§»çŸ©é˜µæ˜¯N*Nçš„ï¼Œè·Ÿè¿™é‡Œæœ‰åŒºåˆ«ã€‚ DFSAå¯ä»¥ç”¨çŠ¶æ€è½¬ç§»è¡¨æ¥è¯†åˆ«å­—ç¬¦ä¸²ï¼Œå°±æ˜¯æŠŠä¸€ä¸ªå­—ç¬¦ä¸²è¾“å…¥è¿›å»ï¼Œç„¶åç”¨è‡ªåŠ¨æœºæ¥åˆ¤åˆ«æ˜¯acceptè¿˜æ˜¯rejectã€‚è¿™ç§ç®—æ³•æ˜¯â€œç¡®å®šæ€§çš„æ˜¯è¯†åˆ«å™¨â€ï¼Œdeterministicç®—æ³•ï¼Œç®€ç§° D-RECOGNIZE. ä¼ªä»£ç ï¼š åœ¨å›¾2.13ä¸­ï¼ŒçŠ¶æ€ä¹‹é—´çš„è½¬ç§»çŸ©é˜µæ˜¯ transition-table[current-state, tape[index]]. current-stateè¡¨ç¤ºè¡Œï¼Œtape[index]è¡¨ç¤ºåˆ—ã€‚å…¶ä¸­index = index + 1. å¦‚æœå°†çŠ¶æ€è½¬ç§»çŸ©é˜µä¸­çš„ç©ºçŠ¶æ€ç§°ä¸ºå¤±è´¥çŠ¶æ€(fail state)æˆ–è€…å¸æ”¶çŠ¶æ€(sink state).é‚£ä¹ˆå¯¹ä»»ä½•çŠ¶æ€å’Œä»»ä½•è¾“å…¥ï¼Œè‡ªåŠ¨æœºéƒ½èƒ½æ‰¾åˆ°å¯ä»¥è½¬ç§»çš„åœ°æ–¹ã€‚ NFSAå­˜åœ¨æŸä¸ªçŠ¶æ€ï¼Œéš¾ä»¥åˆ¤æ–­æ¥ä¸‹æ¥æ€ä¹ˆèµ°çš„è‡ªåŠ¨æœºç§°ä¸ºéç¡®å®šçš„FSA(æˆ–NFSA).å¦‚ä¸‹å›¾ï¼š è¿˜æœ‰ä¸€ç§NFSA,ç”¨åˆ°äº† $\\epsilon-$ è½¬ç§»çš„æ–°å¼§ã€‚å¦‚æœåˆ°è¾¾äº†çŠ¶æ€3ï¼Œé‚£ä¹ˆå¯ä»¥è¿›å…¥å¸¦æœ‰æ ‡è®°!çš„å¼§ï¼Œä¹Ÿå¯ä»¥ä¸çœ‹å¸¦å­ä¸Šçš„ç¬¦å·ï¼Œç›´æ¥è½¬ç§»åˆ°çŠ¶æ€2. é‚£ä¹ˆå…¶å¯¹åº”çš„çŠ¶æ€è½¬ç§»è¡¨å¦‚ä¸‹å›¾ï¼š ç”¨NFSAæ¥æ¥å—ç¬¦å·ä¸²: æœ‰ä¸‰ç§æ–¹æ³•ï¼š å›é€€ å‰ç» å¹¶è¡Œ è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨å›é€€æ³•ï¼š ä»£ç é˜…è¯»ï¼š function ND-RECOGNIZE(tape, machine) return accept or reject åˆ›å»ºè¿›ç¨‹è¡¨(agenda): å…¶ä¸­çš„å…ƒç´ æ˜¯current-search-state, æ˜¯ç”±è‡ªåŠ¨æœºçš„ä¸€ä¸ªç»“ç‚¹ï¼ˆçŠ¶æ€ï¼‰å’Œå¸¦å­ä¸Šçš„ä¸€ä¸ªä½ç½®ç»„åˆè€Œæˆçš„ã€‚ agenda = {(Initial state of machine, beginning of tape)} current-search-state = NEXT(agenda) ä¸»å›è·¯ï¼šç”¨æ¥ç¡®å®šè¾“å…¥å¸¦å­ä¸Šçš„å…¨éƒ¨å†…å®¹æ˜¯å¦éƒ½è¢«è‡ªåŠ¨æœºå…¨éƒ¨è¯†åˆ«äº†ï¼Œè¿™å¯ä»¥ç”¨ACCEPT-STATE?å‡½æ•°å®ç°ï¼Œå¦‚æœå½“å‰æœç´¢çŠ¶æ€æ—¢åŒ…æ‹¬ä¸€ä¸ªæ¥å—çš„æœºå™¨çŠ¶æ€,ä¹ŸåŒ…å«ä¸€ä¸ªå­ç»“å°¾çš„æŒ‡é’ˆï¼Œé‚£ä¹ˆå°±è¿”å›accept.è¿™ä¸€æ­¥ç±»ä¼¼äº D-RECOGNIZE. å¦‚æœä¸è¡Œï¼Œé‚£å°±è°ƒç”¨ GENERATE-NEW-STATE æ¥ç”Ÿæˆä¸€ç³»åˆ—å¯èƒ½çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚ 1234567891011121314151617if ACCEPT-STATE?(current-search-state) return ture : return ACCEPTelse agenda = agenda and GENERATE-NEW-STATE(current-search-state) //è¿™ä¸€æ­¥ä¸å¤ªç†è§£ï¼Ÿï¼Ÿï¼Ÿif agenda is empty: return rejectelse current-search-state = NEXT(agenda) NEXTå‡½æ•°ä»è¿›ç¨‹è¡¨è¿”å›ä¸€ä¸ªä¸ºæ¢æµ‹è¿‡çš„çŠ¶æ€ã€‚é‚£ä¹ˆæ€ä¹ˆå®šä¹‰å‡½æ•°NEXTï¼Œè¿™æ˜¯ä¸€ä¸ªå€¼å¾—ç ”ç©¶çš„é—®é¢˜ã€‚ è¿›ç¨‹è¡¨ç”¨stackå®ç°ï¼Œé‚£å°±æ˜¯æ·±åº¦ä¼˜å…ˆæœç´¢(depth-first search)æˆ–åè¿›å…ˆå‡º(Last In First Out, LIFO). è¿›ç¨‹è¡¨ç”¨queueå®ç°ï¼Œé‚£å°±æ˜¯å¹¿åº¦ä¼˜å…ˆæœç´¢(breadth-first search)æˆ–å…ˆè¿›å…ˆå‡º(First In First Out, FIFO). ACCEPT-STATE? å‡½æ•°ç”¨æ¥ç¡®å®šè¾“å…¥å¸¦å­ä¸Šçš„å…¨éƒ¨å†…å®¹æ˜¯å¦éƒ½è¢«è‡ªåŠ¨æœºæˆåŠŸçš„è¯†åˆ«äº†ã€‚ 123456789101112131415function ACCEPT-STATE? (search-state) returns ture of false current-node = the node search-state is in index = the point on the tape search-state is looking at if index is at the end of the tape and current-node is an accept state of machine: return ture else return false GENERATE-NEW-STATEå‡½æ•°ç”¨ä¸é€‰å–ä¸€ç§ä¸ºè®¿é—®è¿‡çš„è·¯å¾„ éœ€è¦ç†è§£çš„æ˜¯ï¼šåœ¨NEXT(agenda)ä¸­,agendaé‡‡ç”¨çš„æ˜¯stackæˆ–queue, é‚£ä¹ˆæ„å‘³ç€åœ¨åªæœ‰ä¸€ç§é€‰æ‹©çš„ç»“ç‚¹å¤„ï¼Œèµ°è¿‡å°±åˆ æ‰äº†ï¼Œåªæœ‰æœ‰ä¸¤ç§é€‰æ‹©çš„åœ°æ–¹ï¼Œå¦å¤–ä¸€ç§ä¼šå‚¨å­˜åœ¨agendaä¸­ã€‚æ‰€ä»¥current-search-state = NEXT(agenda)ç”Ÿæˆçš„æ˜¯æœ‰ä¸¤ç§é€‰æ‹©å¤„çš„æœªè®¿é—®è¿‡çš„è·¯å¾„ã€‚é‚£ä¹ˆGENERATE-NEW-STATEå‡½æ•°å°±æ˜¯ç”Ÿæˆè¿™æ ·çš„ä¸€ä¸ªè·¯å¾„ï¼Œç„¶ååœ¨ACCEPT-STATE?ä¸­åˆ¤æ–­ã€‚ è¯†åˆ«å°±æ˜¯æœç´¢ï¼š æŠŠç¬¦å·ä¸²è¾“å…¥å½“åšè¾“å…¥å¸¦ï¼Œç„¶åç”¨è‡ªåŠ¨æœºæ¥è¯†åˆ«ã€‚å…¶å®ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯æœç´¢çš„è¿‡ç¨‹ã€‚ æ·±åº¦ä¼˜å…ˆæœç´¢ï¼š å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼š å½¢å¼è¯­è¨€å½¢å¼è¯­è¨€æ˜¯æ—©æœŸå¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ä¸­ï¼Œå½“æ—¶å‡ ä¹æ˜¯å”¯ä¸€çš„æ–¹æ³•ï¼Œå¯ä»¥ç”¨äºæè¿°è‡ªç„¶è¯­è¨€çš„è¯­æ³•è§„å¾‹ï¼Œèƒ½æœ€å¤§é™åº¦çš„é€¼è¿‘è‡ªç„¶è¯­è¨€ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“ç”Ÿæˆè¯­è¨€å†…å®¹ã€‚ å½¢å¼è¯­è¨€å’Œè‡ªåŠ¨æœºä¹‹é—´å­˜åœ¨çš„å¯¹åº”å…³ç³»ï¼Œä½¿å…¶å¤©ç”Ÿå°±å®¹æ˜“è¢«è®¡ç®—æœºå¤„ç†ã€‚ è™½ç„¶ç°åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸»è¦æ˜¯åŸºäºç»Ÿè®¡æ¨¡å‹æ¥å¤„ç†çš„ï¼Œä½†å½¢å¼è¯­è¨€ä»ç„¶ä¼šå‡ºç°åœ¨å¾ˆå¤šè®ºæ–‡ä¸­ã€‚ å›¾è®º æ— å‘å›¾ G = (V, E), V è¡¨ç¤ºå®šç‚¹ï¼Œ Eè¡¨ç¤ºæ— å‘è¾¹ã€‚ æœ‰å‘å›¾ G = (V, E), V æ˜¯å®šç‚¹ï¼ŒE æ˜¯æœ‰å‘è¾¹ è¿é€šå›¾å’Œå›è·¯ æ–‡æ³•ï¼Œå½¢å¼è¯­æ³•åœ¨ç”¨è®¡ç®—æœºç³»ç»Ÿ,æ¥åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦æ˜¯æŸè¯­è¨€çš„åˆæ³•å¥å­æ—¶ï¼Œä»å¥å­å’Œè¯­è¨€çš„ç»“æ„ç‰¹å¾ä¸Šç€æ‰‹æ˜¯éå¸¸é‡è¦çš„ã€‚ä¸€èˆ¬å¯ä»¥é€šè¿‡è¿™ä¸ªå¥å­æ˜¯å¦èƒ½æœ‰ç»™å®šè¯­è¨€å¯¹åº”çš„æ–‡æ³•äº§ç”Ÿæ¥ä½œå‡ºåˆ¤æ–­ï¼Œå¦‚æœèƒ½ï¼Œä»–å°±æ˜¯åˆæ³•çš„ï¼Œå¦åˆ™ï¼Œä»–å°±æ˜¯éæ³•çš„ã€‚å¯¹ä¸€ç±»è¯­è¨€ï¼Œå¯ä»¥åœ¨å­—æ¯è¡¨ä¸ŠæŒ‰ç…§ä¸€å®šçš„è§„åˆ™ï¼Œæ ¹æ®è¯­è¨€çš„ç»“æ„ç‰¹ç‚¹ï¼Œå®šä¹‰ä¸€ä¸ªæ–‡æ³•ã€‚ç”¨æ–‡æ³•ä½œä¸ºç›¸åº”è¯­è¨€çš„æœ‰ç©·æè¿°ä¸ä»…å¯ä»¥æè¿°å‡ºè¯­è¨€çš„ç»“æ„ç‰¹æ€§ï¼Œè€Œä¸”è¿˜å¯ä»¥äº§ç”Ÿå‡ºè¿™ä¸ªè¯­è¨€çš„æ‰€æœ‰å¥å­ã€‚","link":"/2018/04/11/chapter3-%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"title":"cs224d-lecture3 åŸºäºWindowçš„åˆ†ç±»ä¸ç¥ç»ç½‘ç»œ","text":"åˆ†ç±»é—®é¢˜ window classification å¦‚ä½•è‡ªå·±å¼€å§‹ä¸€ä¸ªé¡¹ç›® åˆ†ç±»é—®é¢˜sentiment, named-entity recognition(NER)éƒ½å¯ä»¥çœ‹ä½œæ˜¯åˆ†ç±»é—®é¢˜ã€‚ç»™å®šä¸€ä¸ªè¯çš„è¯å‘é‡ï¼Œé¢„æµ‹å…¶æ‰€å±çš„ç±»ã€‚è¿™ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ éƒ½æ˜¯ä¸€æ ·çš„ï½ $${x^{(i)},y^{(i)}}_1^N$$ å…¶ä¸­ $x^{(i)}$ æ˜¯ä¸€ä¸ª d-ç»´å‘é‡ï¼Œ$y^(i)$ æ˜¯ä¸€ä¸ª C-ç»´one-hotå‘é‡ï¼ŒNæ˜¯æ€»æ•°ã€‚ softmaxsoftmax: $$p(y_i=1|x)=\\dfrac{exp(W_jx)}{\\sum_{c=1}^Cexp(W_cx)}$$ softmaxä¸äº¤å‰ç†µæŸå¤±è®­ç»ƒæ—¶å¯ä»¥ç›´æ¥æœ€å°åŒ–æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡çš„è´Ÿå¯¹æ•°ï¼š $$-log(\\dfrac{exp(W_kx)}{\\sum_{c=1}^Cexp(W_cx)})$$ å…¶å®è¿™ä¸ªæŸå¤±å‡½æ•°ç­‰æ•ˆäºäº¤å‰ç†µ $H(\\hat y, y)=-logy_ilog(\\hat y)$,å…¶ä¸­yæ˜¯one-hotå‘é‡ã€‚ å¯¹äºNä¸ªæ•°æ®ç‚¹ $$-\\sum_{i = 1}^N\\log \\bigg(\\frac{\\exp(W_{k{(i)}\\cdot}x^{(i)})}{\\sum_{c=1}^C\\exp(W_{c\\cdot}x^{(i)})}\\bigg)$$ åŠ ä¸Šæ­£åˆ™åŒ–ï¼š $$-\\sum_{i = 1}^N\\log \\bigg(\\frac{\\exp(W_{k{(i)}\\cdot}x^{(i)})}{\\sum_{c=1}^C\\exp(W_{c\\cdot}x^{(i)})}\\bigg) + \\lambda \\sum_{k=1}^{C\\cdot d + |V|\\cdot d} \\theta_k^2$$ åœ¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦è®­ç»ƒæƒé‡å‚æ•°å³å¯ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬è¿˜å¯ä»¥ä»¥é‡æ–°è®­ç»ƒè¯å‘é‡ä¸­çš„æƒé‡å‚æ•°ã€‚é‚£ä¹ˆéœ€è¦è®­ç»ƒçš„å‚æ•°æ•°é‡ï¼šNxd+dxV retrain embeddingä½†å½“ä½ çš„è®­ç»ƒé›†å¾ˆå°æ—¶ï¼Œå¯èƒ½ä¼šä½¿è¯å‘é‡å¤±å»æ³›åŒ–æ•ˆæœã€‚ This is because Word2Vec or GloVe produce semantically related words to be located in the same part of the word space. When we retrain these words over a small set of the vocabulary, these words are shifted in the word space and as a result, the performance over the final task could actually reduce. Window classificationé€šè¿‡ç›‘ç£å­¦ä¹ æ¥å¯¹ä¸€ä¸ªsingle wordè¿›è¡Œåˆ†ç±»æ˜¾ç„¶æ˜¯ä¸ç¬¦åˆè‡ªç„¶è¯­è¨€çš„ç‰¹æ€§çš„ã€‚å› ä¸ºä¸€ä¸ªwordå…·æœ‰å¤šä¹‰æ€§å’Œå¤šè¯æ€§ã€‚éœ€è¦ç»“åˆä¸Šä¸‹æ–‡æ¥åˆ¤æ–­ã€‚ ç”¨$X^{i}_{window}$ ä»£æ›¿å•ä¸ªè¯ä½œä¸ºè¾“å…¥ $W_i$ cs224d-lecture4 åå‘ä¼ æ’­å’Œé¡¹ç›®æŒ‡å¯¼Project QA: A neural network for Factoid Question Answering over Paragraph sentiment: http://nlp.standford.edu/sentiment/ æ¥ä¸‹æ¥éƒ½æ˜¯å›´ç»•ç€è¯¾ç¨‹é¡¹ç›®çš„æŒ‡å¯¼ä¸å»ºè®®ï¼Œå°±ä¸å•°å—¦äº†ã€‚ç®€å•å†™å†™ä¸€äº›ä½“ä¼šï¼š ä¸è¦æƒ³ç€ä¸€ä¸Šæ¥å°±å‘æ˜ä¸ªæ–°æ¨¡å‹æä¸ªå¤§æ–°é—» ä¹Ÿä¸è¦æµªè´¹å¤§éƒ¨åˆ†æ—¶é—´åœ¨çˆ¬è™«ä¸Šé¢ï¼Œæœ¬æœ«å€’ç½® æŠŠæ—§æ¨¡å‹ç”¨äºæ–°é¢†åŸŸ\\æ–°æ•°æ®ä¹Ÿæ˜¯ä¸é”™çš„é¡¹ç›® å…ˆè¦æŒ‰éƒ¨å°±ç­åœ°ç†Ÿæ‚‰æ•°æ®ã€ç†Ÿæ‚‰è¯„æµ‹æ ‡å‡†ã€å®ç°åŸºçº¿æ–¹æ³• å†æ ¹æ®åŸºçº¿æ–¹æ³•çš„ä¸è¶³ä¹‹å¤„æ€è€ƒæ·±åº¦å­¦ä¹ å¦‚ä½•èƒ½å¸¦æ¥æ”¹è¿› å†å®ç°ä¸€ä¸ªå·²æœ‰çš„è¾ƒä¸ºå‰æ²¿çš„æ¨¡å‹ è§‚å¯Ÿè¯¥æ¨¡å‹çŠ¯çš„é”™è¯¯ï¼Œæ€è€ƒå¦‚ä½•æ”¹è¿› è¿™æ—¶æ‰èƒ½æ²¡å‡†å°±ç¦è‡³å¿ƒçµå‘æ˜ä¸€ä¸ªæ–°æ–¹æ³•","link":"/2018/04/30/cs224d-lecture3-Word-Window%E5%88%86%E7%B1%BB%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"cs224d-lecture9 æœºå™¨ç¿»è¯‘","text":"ä¸»è¦å†…å®¹ï¼š RNN Translation Model GRU LSTM Towards a Better Language Modeling RNN Translation Model encoder: $$h_t=\\phi(h_{t-1},x_t)=f(W^{(hh)}h_{t-1}+W^{(hx)}x_t)$$ encoder: $$h_t=\\phi(h_{t-1})=f(W^{(hh)}h_{t-1})$$ $$y_t=softmax(W^{(S)}h_t)$$ corss entropy function: $$max_{\\theta}\\dfrac{1}{N}\\sum_{n=1}^Nlogp_{\\theta}(y^{(n)}|x^{(n)})$$ rnnçš„å‡ ç‚¹æ‰©å±•1.åœ¨encoderå’Œdecoderä¸­ï¼Œ$W^{(hh)}$ æ˜¯ä¸ä¸€æ ·çš„ 2.è®¡ç®—decoderä¸­çš„éšè—ç¥ç»å…ƒæ—¶ï¼Œå¯ä»¥ä¸ä»…ä»…åªä½¿ç”¨ä¸Šä¸€ä¸ªéšè—å±‚çš„ä¿¡æ¯ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸‰ç§inputæ¥è·å–æ›´å¤šçš„ä¿¡æ¯ The previous hidden state (standard) Last hidden layer of the encoder (c = hT) Previous predicted output word, $y^{tâˆ’1}$ $$h_t=\\phi(h_{t-1},c,y_{t-1})$$ 3.ä½¿ç”¨deep rnn: è¿™éœ€è¦æ›´å¤§çš„è¯­æ–™åº“ 4.ä½¿ç”¨bi-directional encoder 5.é¢ å€’è¯åºè¿›è¡Œè®­ç»ƒ rnn åˆ°åº•åšäº†ä»€ä¹ˆï¼Ÿwe never gave this model an explicit grammar for the source language, or the target language, right? Itâ€™s essentially trying, in some really deep, clever, continuous function, general function approximation kind of way, just correlation, basically, right? And it doesnâ€™t have to know the grammar, but as long as youâ€™re consistent and you just reverse every sequence, the same way. Itâ€™s still grammatical if you read it from the other side. And the model reads it from potentially both sides, and so on. RNNçš„ç¼ºç‚¹æˆ‘ä»¬çŸ¥é“åœ¨ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œä¼ é€’ä¸­$a^{} = g(W_{a}\\cdot[a{},x{}] + b_a)$, å¾ˆå®¹æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œå¹¶ä¸”ç¥ç»ç½‘ç»œä¸æ“…é•¿å¤„ç†é•¿æœŸä¾èµ–çš„é—®é¢˜ã€‚ä»¥è¯­è¨€æ¨¡å‹ä¸ºä¾‹ï¼Œå³åºåˆ—å¾ˆéš¾åå‘ä¼ æ’­åˆ°æ¯”è¾ƒé å‰çš„éƒ¨åˆ†ï¼Œä¹Ÿå°±éš¾ä»¥è°ƒæ•´åºåˆ—å‰é¢çš„è®¡ç®—ã€‚ GRU(gated recurrent units)åŸè®ºæ–‡ï¼šhttps://arxiv.org/pdf/1406.1078v3.pdf è¿™ä¸ªå›¾ç”»çš„ç®—æ˜¯å¾ˆå¥½äº†çš„å§ã€‚ã€‚ä½†æ˜¯è¿˜æ˜¯å¤æ‚äº†ä¸€ç‚¹ï¼Œå¿…é¡»å¯¹ç€å…¬å¼æ‰èƒ½çœ‹æ‡‚ã€‚å¯ä»¥çœ‹ç®€åŒ–å›¾ï¼š $$r_t=\\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})\\tag{reset gate}$$ $$u_t=\\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})\\tag{update gate}$$ $$\\tilde h_t=tanh(Wx_t+r_t\\circ Uh_{t-1})\\tag{new memory}$$ $$h_t=(1-u_t)\\circ \\tilde h_t+u_t\\circ h_{t-1} \\tag{Hidden state}$$ ä¸»è¦å°±æ˜¯ä¸¤ä¸ªgateï¼š é‡ç½®é—¨rï¼šå†³å®šäº†å¦‚ä½•å°†æ–°çš„è¾“å…¥ä¿¡æ¯ä¸å‰é¢çš„è®°å¿†ç›¸ç»“åˆã€‚æ‰€ä»¥å®ƒçš„ä½œç”¨å¯¹è±¡æ˜¯ $\\tilde h_t$ ä¹Ÿå°±æ˜¯new memory cell. æ›´æ–°é—¨uï¼šå®šä¹‰äº†å‰é¢è®°å¿†ä¿å­˜åˆ°å½“å‰æ—¶é—´æ­¥çš„é‡ã€‚æ‰€ä»¥å®ƒçš„ä½œç”¨å¯¹è±¡æ˜¯ $h_t$.ä¹Ÿå°±æ˜¯å½“å‰memory cellä¿å­˜ $h_{t-1}$ å’Œ $\\tilde h_t$ å¤šå°‘ä¿¡æ¯é‡ã€‚ GRU èƒŒåçš„åŸç†ï¼š å¦‚æœæˆ‘ä»¬å°†é‡ç½®é—¨è®¾ç½®ä¸º 1ï¼Œæ›´æ–°é—¨è®¾ç½®ä¸º 0ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å†æ¬¡è·å¾—æ ‡å‡† RNN æ¨¡å‹ã€‚ å¦‚æœé‡ç½®é—¨è®¾ä¸º0ï¼Œé‚£ä¹ˆå°†å¿½è§†ä¹‹å‰çš„éšè—çŠ¶æ€ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥ä¸¢æ‰ä¹‹å‰çš„ä¿¡æ¯ï¼Œå½“å®ƒä»¬ä¸æœªæ¥çš„ä¿¡æ¯ä¸ç›¸å…³æ—¶ã€‚ æ›´æ–°é—¨uæ§åˆ¶ç€è¿‡å»çš„çŠ¶æ€å¯¹ç°åœ¨çš„å½±å“ã€‚If z close to 1, then we can copy information in that units through many time steps!è¿™æ„å‘³ç€ Less vanishing gradient! Units with short-term dependencies often have reset gate very active. è¿™å‡ å¥æ€»ç»“å¯ä»¥è¯´æ˜¯é“å‡ºäº†GRUçš„ç²¾é«“äº†ï¼ ä»éœ€è¦ç†è§£çš„å‡ ä¸ªé—®é¢˜ï¼š æ¿€æ´»å‡½æ•°ä¸ºä»€ä¹ˆæ˜¯tanh ,sigmoidï¼Œå¹¶ä¸èƒ½åƒæ¦‚ç‡å›¾æ¨¡å‹é‚£æ ·ï¼Œç”¨æ•°å­¦æ¥è§£é‡Šï¼Œå°±æ˜¯å¾ˆç„å­¦å§ã€‚ã€‚ è¿™ç¯‡æ–‡ç« å†™çš„ä¸é”™ï½ç»å…¸å¿…è¯»ï¼šé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰çš„åŸºæœ¬æ¦‚å¿µä¸åŸç† LSTM è¿™ä¸ªå›¾æœ‰ç‚¹æŠ½è±¡ã€‚ è¿™ä¸ªå›¾æ˜¯æ¥è‡ªNgçš„è¯¾ï¼Œå°†å›¾ä¸­çš„ $a^{&lt; t &gt;}$ æ¢æˆ $h_t$ å°±å¯ä»¥äº†ï½ ä¸‰ä¸ªgateä»¥åŠæ–°çš„è®°å¿†ç»†èƒï¼Œä¸‰ä¸ªsigmoidå’Œä¸€ä¸ªtanh $$i_t = \\sigma(W^{(i)}x_t+U^{(i)}h_{t-1})\\tag{Input\\update gate}$$ $$f_t = \\sigma(W^{(f)}x_t+U^{(f)}h_{t-1})\\tag{forget gate}$$ $$o_t = \\sigma(W^{(o)}x_t+U^{(o)}h_{t-1})\\tag{Output/Exposure gate}$$ $$\\tilde c_t = tanh(W^{(c)}x_t+U^{(c)}h_{t-1})\\tag{New memory cell}$$ è¾“å…¥é—¨å’Œé—å¿˜é—¨ä½œç”¨äºæ–°çš„è®°å¿†ç»†èƒå¾—åˆ°æœ€ç»ˆçš„è®°å¿†ç»†èƒ: $$c_t=f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$$ è¾“å‡ºé—¨ä½œç”¨äºæ–°çš„è®°å¿†ç»†èƒå¾—åˆ°æœ€ç»ˆçš„éšè—çŠ¶æ€ï¼š $$h_t=o_t\\circ tanh(c_t)$$ è¿™é‡Œè¦ç†è§£æ¯ä¸ªgateçš„ç›®çš„åˆ°åº•æ˜¯å•¥ï¼Ÿè™½ç„¶å¾ˆéš¾ç”¨æ•°å­¦æ¥è§£é‡Šï¼Œä½†æ˜¯ä»intuitiveä¸Šæ¥ç†è§£ä¸‹è¿˜æ˜¯å¯ä»¥çš„ï½ New memory cell: åœ¨GRUä¸­ä¹Ÿå­˜åœ¨ï¼Œä½†æ˜¯æ˜¯æœ‰åŒºåˆ«çš„ï¼Œè¿™é‡Œæ˜¯é€šè¿‡input word $x_t$ å’Œ è¿‡å»çš„éšè—çŠ¶æ€ $h_{t-1}$ å¾—åˆ°çš„ã€‚GRUä¸­è™½ç„¶ä¹Ÿæ˜¯ï¼Œä½†ç›´æ¥ä½¿ç”¨äº†reset gate Input gate: ä¹Ÿå«æ›´æ–°é—¨ï¼Œå› ä¸ºæ–°çš„è®°å¿†ç»†èƒ $\\tilde c_t$ ç”Ÿæˆæ—¶å¹¶æœªè€ƒè™‘current wordæ˜¯å¦æœ‰ä¿ç•™çš„æ„ä¹‰ï¼Œå› æ­¤ $i_t$ ä½œç”¨äº $\\tilde c_t$. Forget gate: è·Ÿè¾“å…¥é—¨æ˜¯åŒæ ·çš„é“ç†ï¼Œæ–°çš„è®°å¿†ç»†èƒ $\\tilde c_t$ ç”Ÿæˆæ—¶å¹¶æœªè€ƒè™‘past memory cellæ˜¯å¦æœ‰ä¿ç•™çš„æ„ä¹‰ï¼Œå› æ­¤ $f_i$ ä½œç”¨äº $c_{t-1}$ Final memory generation: ç»¼åˆè€ƒè™‘äº†é—å¿˜é—¨ä½œç”¨åçš„ $c_{t-1}$ å’Œ è¾“å…¥é—¨ä½œç”¨åçš„ $\\tilde c_t$ Output/Exposure Gate: åœ¨GRUä¸­ä¸å­˜åœ¨ï¼Œè¿™é‡Œæ˜¯ç”¨æ¥åŒºåˆ†æœ€ç»ˆçš„è®°å¿†ç»†èƒ $c_t$ å’Œ æœ€ç»ˆçš„éšè—çŠ¶æ€ $h_t$ çš„ã€‚å› ä¸ºè®°å¿†ç»†èƒä¸­åŒ…å«æœ‰å¾ˆå¤šå¯¹äºéšè—çŠ¶æ€æ¥è¯´ä¸å¿…è¦çš„ä¿¡æ¯ã€‚ ä½¿ç”¨LSTMçš„æœºå™¨ç¿»è¯‘æ•ˆæœå¾ˆç¥å¥‡ï¼ï¼ï¼ PresentationTowards a Better Language Modeling. Better inputs: word $\\rightarrow$ subword $\\rightarrow$ char Better regularization/Processing Better Model","link":"/2018/05/07/cs224d-lecture9-machine-translation/"},{"title":"cs224d-lecture8-RNN","text":"ä¸»è¦å†…å®¹ï¼š è¯­è¨€æ¨¡å‹ Language model å¾ªç¯ç¥ç»ç½‘ç»œ recurrent neural network æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜çš„åŸå› ä»¥åŠè§£å†³æ–¹æ³• åŒå‘rnnï¼Œ deep bi-RNNs å…³äºä¾å­˜åˆ†æçš„presentation è¯­è¨€æ¨¡å‹ Language Modelè¯­è¨€æ¨¡å‹æ˜¯è®¡ç®—ä¸€ç³»åˆ—è¯ä»¥ç‰¹å®šåºåˆ—å‡ºç°çš„æ¦‚ç‡ã€‚ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹æ˜¯åŸºäºé¢‘ç‡ï¼Œè®¡ç®—åœ¨å‰nä¸ªè¯çš„æ¡ä»¶ä¸‹ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ $w_i$ çš„æ¦‚ç‡ã€‚ $$P(w_1,â€¦,w_m)=\\prod_{i=1}^{i=m}P(w_i|w_1,â€¦,w_i-1)\\approx\\prod_{i=1}^{i=m}P(w_i|w_{i-n},â€¦,w_{i-1})$$ å…¶ä¸­ï¼š $$P(w_2|w_1)=\\dfrac{count(w_1,w_2)}{count(w_1)}$$ $$P(w_3|w_1,w_2)=\\dfrac{count(w_1,w_2,w_3)}{count(w_1,w_2)}$$ ä½†åŸºäºæ¦‚ç‡çš„è¯­è¨€æ¨¡å‹å¹¶ä¸èƒ½æ•æ‰åˆ°ä¸€äº›è¯­ä¹‰ä¿¡æ¯ã€‚ For instance, consider a case where an article discusses the history of Spain and France and somewhere later in the text, it reads â€œThe two countries went on a battleâ€; clearly the information presented in this sentence alone is not sufficient to identify the name of the two countries. äºæ˜¯å‡ºç°äº†ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œçš„è¯­è¨€æ¨¡å‹ï¼Œ learning a distributed representation of words $$\\hat y=softmax(W^{(2)}tanh(w^{(1)}x+b^{(1)})+w^{(3)}x+b^{(3)})$$ W^{(1)} åº”ç”¨äºè¯å‘é‡ï¼ˆsolid green arrowsï¼‰ W^{(2)} åº”ç”¨äºéšè—å±‚ W^{(3)} åº”ç”¨äºè¯å‘é‡ï¼ˆdashed green arrowsï¼‰ ä½†å¦‚æœè¦è®°å¿†æ›´å¤šçš„è¯ï¼Œå¿…é¡»è¦å¢å¤§windows size nï¼Œè¿™ä¼šé€ æˆè®¡ç®—é‡å¤ªå¤§è€Œæ— æ³•è®¡ç®—ã€‚ å¾ªç¯ç¥ç»ç½‘ç»œ Recurrent Neural Network language model $$h_t = \\sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$ å…¶ä¸­+è¡¨ç¤ºconcatenateè¿˜æ˜¯ç›´æ¥ç›¸åŠ ï¼Ÿé€šè¿‡ä½œä¸šå®ç°ï¼Œæ˜¯ç›¸åŠ ï½ shapes: $h_0\\in R^{D_h}$ is some initialization vector for the hidden layer at time step 0, $x\\in R^{d}$ is the column vector for L at index [t] at time step t $W^{hh}\\in R^{D_h\\times D_h}$ $W^{hx}\\in R^{D_h\\times d}$ $W^{(S))}\\in R^{|V|\\times D_h}$ å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼š $\\hat y \\in R^{|V|}$ é€šè¿‡softmaxå¾—åˆ°çš„åœ¨è¯è¡¨Vä¸Šçš„æ¦‚ç‡åˆ†å¸ƒã€‚ é‚£ä¹ˆå½“å‰æ—¶é—´æ­¥çš„æŸå¤±å€¼ï¼š $$J^{(t)}(\\theta) = -\\sum_{j=1}^{|V|}y_{t,j}log\\hat y_{t,j}$$ $y_{t,j}$ è¡¨ç¤ºå½“å‰æ—¶é—´æ­¥çš„actual word,æ˜¯ one-hot vector. åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œ$\\hat y_t$ ç”¨æ¥è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„æŸå¤±å€¼,ä»è€Œè®­ç»ƒå‚æ•°ã€‚è€Œåœ¨æµ‹è¯•é›†ä¸­æ—¶ï¼Œä¹Ÿå°±æ˜¯ç”Ÿæˆsentenceæ—¶ï¼Œç”¨æ¥ä½œä¸ºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ã€‚ é‚£ä¹ˆå¯¹æ•´ä¸ªsentenceçš„é¢„æµ‹çš„æŸå¤±å€¼ï¼š $$J=\\dfrac{1}{T}\\sum_{t=1}^T(\\theta)=-\\dfrac{1}{T}\\sum_{t=1}^T\\sum_{j=1}^{|V|}y_{t,j}\\times log(\\hat y_{t,j})$$ å›°æƒ‘åº¦ï¼š $$Perplexity=2^J$$ æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜Training RNNs is incredibly hard! Buz of gradient vanishing and explosion problems. è¿™ç¯‡æ–‡ç« å¯¹rnnä¸­æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜è¯´çš„æ¯”è¾ƒæ¸…æ¥šï¼ŒRNNæ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸çš„åŸå›  è¿™é‡Œå°†rnnç®€åŒ–äº†,åŸæœ¬åº”è¯¥æ˜¯ï¼š $$h_t=\\sigma (Wf(h_{t-1})+W^{(hx)}x_{|t|})$$ $$\\hat y = softmax(W^{(S)}f(h_t))$$ è¿™é‡Œå°±æŒ‰ç…§ç®€åŒ–çš„æ¥æ¨å¯¼å§ï¼Œtæ—¶é—´æ­¥çš„æŸå¤±å€¼å¯¹ $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}\\dfrac{\\partial h_t}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}$$ å…¶å®ä¸»è¦æ˜¯è¿™ä¸ªå¼å­çš„é—®é¢˜ $\\dfrac{\\partial h_t}{\\partial h_k}$, $h_t$ æ˜¯W å’Œ $h_t-1$ çš„å‡½æ•°ï¼Œ $h_{t-1}$ åˆæ˜¯ W å’Œ $h_{t-2}$ çš„å‡½æ•°â€¦.. ä¹Ÿå°±æ˜¯è¯´ $h_t$ æ˜¯ä¹‹å‰æ‰€æœ‰æ—¶åˆ» $h_k$ çš„å‡½æ•°ï¼Œè€Œ $h_k$ ä¹Ÿæ˜¯æƒé‡ W çš„å‡½æ•° $$\\dfrac{\\partial h_t}{\\partial h_k} = \\prod_{j=k+1}^k\\dfrac{\\partial h_j}{\\partial h_{j-1}}=\\prod_{j=k+1}^tW^T\\times diag[fâ€™(j_{j-1})]$$ å…¶ä¸­ $h\\in R^{D_h}$, å› æ­¤å…¶å¯¼æ•° $\\partial h_j/\\partial h_{j-1}$ æ˜¯ä¸€ä¸ª $D_h \\times D_h$ çš„é›…å…‹æ¯”çŸ©é˜µã€‚ æ‰€ä»¥æœ‰ï¼š $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}(\\prod_{j=k+1}^t\\dfrac{\\partial h_j}{\\partial h_{j-1}})\\dfrac{\\partial h_k}{\\partial W}$$ å®šä¹‰ $\\beta$ ä¸ºèŒƒå¼çš„ä¸‹ç•Œï¼Œé‚£ä¹ˆ $||\\dfrac{\\partial h_j}{\\partial h_{j-1}}||$ å¾ˆå®¹æ˜“å˜å¾—å¾ˆå°æˆ–å¾ˆå¤§ã€‚ è§£å†³æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±çš„ä¸€äº›tricksæ¢¯åº¦è£å‰ª gradient clippingå¯¹äºgradient explodingï¼Œæœ‰ä¸ªå¾ˆç®€å•çš„trick:gradient clipping å¯ä»¥åŠ¨æ‰‹å®è·µä¸‹ï¼Œä¹Ÿè®¸å¯¹æ¢¯åº¦ä¼šæœ‰æ›´æ·±çš„ç†è§£ï½ å®çº¿Solid linesè¡¨ç¤º standard gradient descent trajectories è™šçº¿Dashed linesè¡¨ç¤º gradients rescaled to fixed size å°†errorçœ‹ä½œæ˜¯å¾ˆå¤šç»´å‚æ•°ç©ºé—´çš„å‡½æ•°,å¦‚æœæ˜¯äºŒç»´çš„è¯ï¼Œé‚£error surfaceå°±æ˜¯ä¸€ä¸ªæ›²é¢ã€‚åœ¨æ›²é¢ä¸Šé«˜æ›²ç‡çš„åœ°æ–¹(high curvature walls)ï¼Œå…¶æ¢¯åº¦ä¹Ÿå°±å¾ˆå¤§ã€‚ è¯¦ç»†çš„è¿˜æ˜¯çœ‹æ–‡çŒ®å§On the difficulty of training Recurrent Neural Networks, Pascanu å¯¹äºæ¢¯åº¦æ¶ˆå¤± vanishing gradients å‚æ•°åˆå§‹åŒ– Initialization relus, Rectified Relus å¾ˆéš¾ç†è§£ä¸ºå•¥ç”¨reluèƒ½å¾ˆå¥½çš„è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œçš„ç¡®reluçš„æ¢¯åº¦ä¸º1ï¼Œä½†å®ƒçš„éçº¿æ€§ä¹Ÿå¤ªç®€å•äº†å§ã€‚ã€‚ã€‚æ‰€ä»¥å¾—çœ‹çœ‹åŸè®ºæ–‡ A Simple Way to Initialize Recurrent Networks of Rectified Linear Units softmaxè®¡ç®—é‡å¤ªå¤§çš„é—®é¢˜å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä»éšè—å±‚åˆ°è¾“å‡º $W^{(S)} \\in R^{D_h, V}$ ,å¦‚æœè¯è¡¨å¾ˆå¤§çš„è¯ï¼Œè¿™ä¸ªçŸ©é˜µä¹Ÿå°±å¾ˆå¤§äº†ï½ åºåˆ—æ¨¡å‹çš„ä¸€äº›å…¶ä»–ä»»åŠ¡Classify each word into: NER Entity level sentiment in context opinion expression extraction Opinion Mining with Deep Recurrent Neural Networks åŒå‘ RNNs å…¶å®è·Ÿrnnæ²¡æœ‰å¤ªå¤šå˜åŒ–ï¼Œæœ‰ä¸¤ä¸ªéšè—å±‚ï¼Œå¹¶ä¸”éšè—å±‚çš„é€’å½’åˆ†åˆ«æ˜¯ä»è¯­æ–™åº“çš„ä¸¤ä¸ªä¸åŒçš„æ–¹å‘ã€‚ Deep bidirectional RNNs $$\\overrightarrow {h_t^{(i)}}=f(\\overrightarrow{W^{(i)}}h_t^{(i-1)}+\\overrightarrow{V^{(i)}}h_{t-1}^{(i)}+\\overrightarrow{b^{(i)}})$$ å…¶ä¸­ $h_t^{(i-1)}$ è¡¨ç¤ºä¸Šä¸€å±‚éšè—å±‚çš„è¾“å…¥ï¼Œ $h_{t-1}^{(i)}$ è¡¨ç¤ºå½“å‰éšè—å±‚å±‚çš„ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ã€‚ $$\\overleftarrow {h_t^{(i)}}=f(\\overleftarrow{W^{(i)}}h_t^{(i-1)}+\\overleftarrow{V^{(i)}}h_{t-1}^{(i)}+\\overleftarrow{b^{(i)}})$$ éœ€è¦è®­ç»ƒçš„å‚æ•°æœ‰ï¼š$\\overrightarrow{W^{(i)}},\\overleftarrow{W^{(i)}}$ $\\overrightarrow{V^{(i)}},\\overleftarrow{V^{(i)}}$ $$\\hat y_t=g(Uh_t+c)=g(U[\\overrightarrow{h_t^{(L)}};\\overleftarrow{h_t^{(L)}}]+c)$$ data evalution PresentationStructured Training for Neural Network Transition-Based Parsing, David Weiss, Chris Alberti, Michael Collins, Slav Petrov è¡¨ç¤ºæ ¹æœ¬å¬ä¸æ‡‚ï¼ŒåªçŸ¥é“ä½¿ç”¨deeplearningåšä¾å­˜åˆ†æã€‚ã€‚ç”¨state-of-artçš„SyntaxNetå’Œå‰äººå‡ ç¯‡æœ‰å½±å“åŠ›çš„è¿›è¡Œäº†å¯¹æ¯”ï½","link":"/2018/05/04/cs224d-lecture8-RNN/"},{"title":"chapter9 éšé©¬å°”å¯å¤«æ¨¡å‹","text":"chapter6: éšé©¬å°”ç§‘å¤«æ¨¡å‹ standford tutorial, å¯ä»¥è¯´æ˜¯çœ‹è¿‡çš„å…³äºéšé©¬å°”ç§‘å¤«æœ€å¥½çš„æ•™ç¨‹äº†ã€‚è¦æ˜¯çœ‹è‹±æ–‡åŸç‰ˆèƒ½å’Œçœ‹ä¸­æ–‡ä¸€æ ·easyè¯¥å¤šå¥½ï¼Œå¯æƒœæ–‡åŒ–å·®å¼‚çš„æƒ…å†µä¸‹ï¼Œå³ä½¿æ˜¯å•è¯éƒ½è®¤è¯†ï¼Œç†è§£èµ·æ¥ä¹Ÿä¼šæœ‰ç‚¹å›°éš¾ã€‚å¯¹æ­¤ï¼Œåªèƒ½è‡ªå·±é‡æ–°æ€»ç»“ä¸€ä¸‹å§ï½ å¯ä»¥è¯´ï¼Œæ–¯å¦ç¦ä»æœªè®©äººå¤±æœ›è¿‡ï¼Œå¤ªèµäº†ï¼ ä¹Ÿæ˜¯æ— æ„ä¸­åœ¨googleä¸Šçœ‹åˆ°è¿™ç¯‡æ–‡ç« ï¼Œæ‰å‘ç°äº†è¿™ä¹ˆå¥½çš„ä¸€æœ¬ä¹¦, Speech and language processing. å‰è¨€ï¼š éšé©¬å°”å¯å¤«æ¨¡å‹é©¬å°”å¯å¤«æ¨¡å‹çš„åè£”ï¼Œå®ƒæ˜¯ä¸€ä¸ªåºåˆ—æ¨¡å‹(sequence model). å¯¹äºä¸€ä¸ªåºåˆ—æ¨¡å‹ï¼Œå®ƒçš„å·¥ä½œæ˜¯ç»™åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªå°å•å…ƒåˆ†é…æ ‡ç­¾labelæˆ–æ˜¯ç±»åˆ«class.å…¶ä¸­åŒ…æ‹¬ï¼špart-of-speech tagging, named entity tagging, and speech recognition. é©¬å°”å¯å¤«é“¾ Markov chainsé©¬å°”å¯å¤«é“¾å’Œéšé©¬å°”ç§‘å¤«æ¨¡å‹éƒ½æ˜¯æœ‰é™è‡ªåŠ¨æœº(finite automation)çš„æ‹“å±•ã€‚å¯ä»¥å°†ä»–ä»¬çœ‹ä½œæ˜¯æœ‰æƒé‡çš„æœ‰é™è‡ªåŠ¨æœº(weighted finite automation),åŒ…æ‹¬ä¸€äº›åˆ—çŠ¶æ€å’ŒçŠ¶æ€ä¹‹é—´çš„è½¬ç§»å…³ç³»ï¼Œå…¶ä¸­ä»ä¸€ä¸ªçŠ¶æ€åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„è½¬ç§»å¼§çº¿æ˜¯æœ‰æƒé‡çš„ã€‚åœ¨é©¬å°”å¯å¤«é“¾ä¸­ï¼Œè¿™æ ·çš„æƒé‡ä»£è¡¨ç€æ¦‚ç‡ï¼Œä¸”ä»ä¸€ä¸ªèŠ‚ç‚¹å‡ºæ¥çš„æ¦‚ç‡ä¹‹å’Œä¸º1. ä¸Šå›¾ä¸­ä¸ä»…åŒ…æ‹¬äº†çŠ¶æ€ä¹‹é—´çš„è½¬ç§»æ¦‚ç‡ï¼Œè¿˜åŒ…æ‹¬äº†startå’Œendä¸¤ç§ç‰¹å®šçš„çŠ¶æ€ã€‚ | states | hot1 | cold2 | warm3 | | â€”â€” | â€”â€”â€“ | â€”â€”â€“ | â€”â€”â€“ | | hot1 | $a_{11}$ | $a_{12}$ | $a_{13}$ | | cold2 | $a_{21}$ | $a_{22}$ | $a_{23}$ | | warm3 | $a_{31}$ | $a_{32}$ | $a_{33}$ | start0: {$a_{01},a_{02},a_{03}$} end4: {$a_{14},a_{24},a_{34}$} æ ¹æ®å›¾9.1ï¼Œé©¬å°”å¯å¤«é“¾å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªæ¦‚ç‡å›¾æ¨¡å‹ï¼Œå…¶ä¸­åŒ…æ‹¬ä»¥ä¸‹å‡ éƒ¨åˆ†ï¼š Qä»£è¡¨çŠ¶æ€é›†åˆï¼ŒAæ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œ $a_{ij}$ è¡¨ç¤ºä»çŠ¶æ€ $q_i$ åˆ°çŠ¶æ€ $q_j$ çš„æ¦‚ç‡,é‚£ä¹ˆæœ‰ $\\sum_{j=1}^na_{ij}=1,i=1,2â€¦N$ $q_0$ å’Œ $q_F$æ˜¯åˆå§‹çŠ¶æ€å’Œç»ˆæ­¢çŠ¶æ€ã€‚ åšä¸¤ä¸ªé‡è¦çš„å‡è®¾ï¼š ä»¥ç®€åŒ–æ¨¡å‹ (1) The Limited horiqon assumption é½æ¬¡å‡è®¾ å¯¹äºtæ—¶åˆ»çš„çŠ¶æ€ï¼Œåªå–å†³äºä¹‹å‰çš„ä¸€ä¸ªçŠ¶æ€ã€‚ $$ Markov Assumption: P(q_i|q_1â€¦q_{i-1})=P(q_i|q_{i-1})$$ ä¹Ÿå°±æ˜¯ä¸€é˜¶é©¬å°”ç§‘å¤«é“¾(a first-order Markov)ã€‚ å›¾ä¸­çš„ $x_i$ è¡¨ç¤ºå¯è§‚æµ‹æ—¶çš„ $q_i$ (2) Stationary process assumption é™æ€è¿‡ç¨‹å‡è®¾ the conditional distribution over next state given current state does not change over time. å¯¹äºçŠ¶æ€ä¹‹é—´çš„æ¡ä»¶æ¦‚ç‡ä¸ä¼šéšæ—¶é—´çš„å˜åŒ–è€Œæ”¹å˜ï¼Œä¹Ÿå°±æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µåªæœ‰ä¸€ä¸ªï½ $$P(q_t|q_{t-1})=P(q_2|q_1);t \\in 2â€¦T$$ åœ¨å¾ˆå¤šå…¶ä»–çš„è®ºæ–‡ä¸­ï¼Œç”¨ $\\pi$ æ¥è¡¨ç¤ºåˆå§‹çŠ¶æ€ éƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¸è¿‡è¿™ç¯‡æ•™ç¨‹ä¸­ç”¨ç¬¬ä¸€ç§è¡¨ç¤ºæ–¹æ³•ã€‚ éšé©¬å°”å¯å¤«æ¨¡å‹ä¸ºäº†æ›´å½¢è±¡çš„æè¿°éšé©¬å°”å¯å¤«æ¨¡å‹ï¼Œæ–‡ç« ä¸¾äº†è¿™æ ·çš„ä¸€ä¸ªæ —å­ï¼š Imagine that you are a climatologist in the year 2799 studying the history of global warming. You cannot find any records of the weather in Baltimore, Maryland, for the summer of 2007, but you do find Jason Eisnerâ€™s diary, which lists how many ice creams Jason ate every day that summer. Our goal is to use these observations to estimate the temperature every day. Weâ€™ll simplify this weather task by assuming there are only two kinds of days: cold (C) and hot (H). So the Eisner task is as follows: Given a sequence of observations O, each observation an integer corresponding to the number of ice creams eaten on a given day, figure out the correct â€˜hiddenâ€™ sequence Q of weather states (H or C) which caused Jason to eat the ice cream. æ€»ç»“ä¸‹å°±æ˜¯ï¼Œè§‚å¯Ÿåˆ°çš„åºåˆ—æ˜¯æ¯å¤©åƒçš„å†°æ·‡æ·‹æ•°ç›®2,3,1,2,3,â€¦.ï¼Œä»è€Œåˆ¤æ–­æ¯å¤©çš„å¤©æ°”æ˜¯hot or coldè¿™ä¸¤ç§çŠ¶æ€ä¸­çš„å“ªä¸€ç§ã€‚ å®šä¹‰ä¸€ä¸ªéšé©¬å°”å¯å¤«æ¨¡å‹ï¼Œæœ‰ä»¥ä¸‹ç»„æˆéƒ¨åˆ†ï¼š ä¸¤ä¸ªå‡è®¾ (1)ä¸€é˜¶é©¬å°”ç§‘å¤«æ¨¡å‹ï¼š $$ Markov Assumption: P(q_i|q_1â€¦q_{i-1})=P(q_i|q_{i-1})$$ (2)æ¡ä»¶ç‹¬ç«‹ï¼Œåœ¨çŠ¶æ€ $q_i$ çš„æ¡ä»¶ä¸‹ï¼Œè§‚æµ‹ $o_i$ åªå–å†³äº $q_i$ï¼Œ è€Œä¸å…¶ä»–çš„çŠ¶æ€å’Œè§‚æµ‹å€¼å¤Ÿæ— å…³ $$Output Indepence: P(o_i|q_1â€¦q_i,â€¦,q_T,o_1,â€¦,o_i,â€¦,o_T)=P(o_i|q_i)$$ å¯¹ice cream task.é—®é¢˜çš„æè¿°å¦‚ä¸‹å›¾ï¼š æ³¨æ„åˆ°ï¼Œå›¾ä¸­æ‰€æœ‰çš„æ¦‚ç‡éƒ½ä¸ä¸ºé›¶ï¼Œè¿™ç§HMMæ¨¡å‹å«åš fully connected æˆ–æ˜¯ ergodic HMM. ä½†å¹¶ä¸æ˜¯æ‰€æœ‰çŠ¶æ€éƒ½å¯ä»¥äº’ç›¸è½¬ç§»çš„ï¼Œæ¯”å¦‚ left-to-right HMM,åˆç§°Bakis HMMs,é€šå¸¸ç”¨äºè¯­éŸ³å¤„ç†ã€‚ å·¦å›¾è¡¨ç¤ºBakis HMMï¼Œå³å›¾æ˜¯ergodic HMM. å…³äºéšé©¬å°”å¯å¤«æ¨¡å‹çš„ä¸‰ä¸ªé—®é¢˜ï¼š é—®é¢˜1ï¼šè®¡ç®—ä¼¼ç„¶æ¦‚ç‡ã€‚ å‰å‘/åå‘ç®—æ³• é—®é¢˜2ï¼šè§£ç é—®é¢˜ï¼Œåˆç§°é¢„æµ‹é—®é¢˜ï¼Œå·²çŸ¥æ¨¡å‹å‚æ•°å’Œè§‚æµ‹åºåˆ—ï¼Œæ±‚æœ€æœ‰å¯èƒ½çš„çŠ¶æ€åºåˆ— ç»´ç‰¹æ¯”ç®—æ³• é—®é¢˜3ï¼šå­¦ä¹ é—®é¢˜ï¼Œå·²çŸ¥è§‚æµ‹åºåˆ—ï¼Œä¼°è®¡æ¨¡å‹å‚æ•°ä½¿å¾—è¯¥æ¨¡å‹ä¸‹è§‚æµ‹åºåˆ—çš„æ¦‚ç‡æœ€å¤§ã€‚ æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ŒBaum-Welch, EMç®—æ³• æ¦‚ç‡è®¡ç®—ï¼šå‰å‘ç®—æ³•çŠ¶æ€å·²çŸ¥çš„è¯ï¼Œæ˜¯ç›‘ç£å­¦ä¹ ä»¥å›¾9.3ä¸ºä¾‹ï¼Œè§‚æµ‹åºåˆ—ä¸ºï¼ˆ3,1,3ï¼‰å‡å¦‚æˆ‘ä»¬çŸ¥é“éšè—çŠ¶æ€æ˜¯ï¼ˆhot, hot, coldï¼‰çš„è¯ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè®¡ç®—ä¼¼ç„¶æ¦‚ç‡å°±å¾ˆç®€å•äº†ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼Œå·²çŸ¥è§‚æµ‹åºåˆ— $O=o_1,o_2,â€¦,o_T$. ä¸”éšè—çŠ¶æ€åºåˆ—å·²çŸ¥ $Q=q_0,q_1,â€¦,q_T$,é‚£ä¹ˆä¼¼ç„¶æ¦‚ç‡ä¸ºï¼š $$P(O|Q)=\\prod_{i=1}^TP(o_i|q_i)$$ çŠ¶æ€æ— æ³•è§‚æµ‹ï¼Œæ— ç›‘ç£å­¦ä¹ ä½†å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒçŠ¶æ€æ˜¯ä¸çŸ¥é“çš„ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å–è®¡ç®—å‡ºç°è§‚æµ‹åºåˆ— (3,1,3) çš„æ‰€æœ‰å¯èƒ½çš„éšè—çŠ¶æ€åºåˆ—ã€‚ è§‚æµ‹åºåˆ— $O=o_1,o_2,â€¦,o_T$ï¼Œå‡å®šå­˜åœ¨ä¸€ä¸ªç‰¹å®šçš„éšè—çŠ¶æ€åºåˆ— $Q=q_0,q_1,â€¦,q_T$ï¼Œé‚£ä¹ˆè”åˆæ¦‚ç‡åˆ†å¸ƒï¼š $$P(O,Q)=P(O|Q)\\times P(Q)=\\prod_{i=1}^Tp(o_i|q_i)\\times \\prod_{i=1}^TP(q_i|q_{i-1})\\tag{9.10}$$ æ‰€ä»¥éšé©¬å°”ç§‘å¤«æ˜¯ä¸€ä¸ªåŒé‡éšæœºè¿‡ç¨‹ã€‚ é‚£ä¹ˆè§‚å¯Ÿåºåˆ—ä¸ºï¼ˆ3,1,1ï¼‰å’ŒçŠ¶æ€åºåˆ—ä¸ºï¼ˆhot, hot, coldï¼‰çš„è”åˆæ¦‚ç‡ä¸ºï¼š è¿™æ ·æˆ‘ä»¬çŸ¥é“äº†æ€ä¹ˆæ±‚ä¸€ä¸ªç‰¹å®šçš„éšè—åºåˆ—å’Œè§‚æµ‹åºåˆ—çš„è”åˆæ¦‚ç‡ï¼Œé‚£ä¹ˆæ‰€æœ‰å¯èƒ½éšè—åºåˆ—çš„ç±»å’Œå°±æ˜¯è§‚æµ‹åºåˆ—çš„æ€»ä¼¼ç„¶æ¦‚ç‡äº†ã€‚ $$P(O)=\\sum_QP(O,Q)=\\sum_QP(O|Q)P(Q)\\tag{9.12}$$ å¯¹å†°æ·‡æ·‹çš„ä¾‹å­ï¼Œå¦‚æœè§‚æµ‹åºåˆ—æ˜¯ï¼ˆ3,1,3ï¼‰ï¼Œé‚£ä¹ˆä¼¼ç„¶æ¦‚ç‡ä¸ºï¼š å¦‚æœæœ‰Nä¸­çŠ¶æ€çš„è¯ï¼Œå¯¹äºé•¿åº¦ä¸ºTçš„åºåˆ—ï¼Œå…¶è®¡ç®—å¤æ‚åº¦å°±æ˜¯ $O(N^T)$ è¿™å°±å¤ªå¤§äº†ï¼Œæ‰€ä»¥å¾—å¯»æ±‚æ›´ç®€å•çš„è§£æ³•ã€‚ forward algorithmå‰å‘ç®—æ³•æ˜¯ä¸€ç§åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œå…¶è®¡ç®—å¤æ‚åº¦æ˜¯ $O(N^2T)$ The forward algorithm is a kind of dynamic programming algorithm, that is, an algorithm that uses a table to store intermediate values as it builds up the probability of the observation sequence. The forward algorithm computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a single forward trellis. ä¹‹æ‰€ä»¥é«˜æ•ˆçš„åŸå› ï¼Œæ˜¯å®ƒå°†æ‰€æœ‰çš„è·¯å¾„éƒ½éšå¼çš„æŠ˜å åˆ°ä¸€ä¸ªå‰å‘ç½‘æ ¼ä¸­ã€‚ã€‚ å‰å‘ç½‘æ ¼(forward trellis)ä¸­çš„æ¯ä¸€ä¸ªå•å…ƒ(cell) $\\alpha_t(j)$ è¡¨ç¤ºç»™å®šæ¨¡å‹ $\\lambda$ï¼Œtæ—¶åˆ»è§‚æµ‹åºåˆ—ä¸º $o_1,o_2,â€¦,o_t$,çŠ¶æ€ä¸ºjçš„æ¦‚ç‡ã€‚ $$\\alpha_t(j)=P(o_1,o_2,â€¦,o_t,q_t=j|\\lambda)\\tag{9.13}$$ å…¶ä¸­ï¼Œ $\\alpha_t(j)$ çš„è®¡ç®—æ˜¯å åŠ æ‰€æœ‰å¯èƒ½çš„è·¯å¾„ã€‚ $$\\alpha_t(j)=[\\sum_{i=1}^N\\alpha_{t-1}a_{ij}]b_j(o_t)$$ è¡¨ç¤ºtæ—¶åˆ»çŠ¶æ€ä¸ºj,ä»t-1æ—¶åˆ»åˆ°tæ—¶åˆ»ï¼Œæœ‰Næ¡è·¯å¾„ï¼Œå åŠ ï½ ä»¥å›¾9.7ä¸­çš„ç¬¬äºŒæ—¶é—´æ­¥å’ŒçŠ¶æ€2ä¸ºä¾‹ï¼Œ $$\\alpha_2(1)=\\alpha_1(1)Ã—P(H|H)Ã—P(1|H) + Î±_1(2)Ã—P(H|C)Ã—P(1|H)$$ ä¸‹å›¾æè¿°äº†å‰å‘ç½‘æ ¼ä¸­è®¡ç®—ä¸€ä¸ªcellçš„æ­¥éª¤ï¼š æ•´ä¸ªä¼¼ç„¶æ¦‚ç‡çš„è®¡ç®—è¿‡ç¨‹ï¼Œä¼ªä»£ç ï¼š çœŸçš„è®²çš„å¤ªå¥½äº†å¤ªæ¸…æ¥šäº†ï¼ï¼ï¼æ„ŸåŠ¨å“­ã€‚ã€‚ã€‚è¯´çœŸçš„ï¼Œè¦ä¸ä¸­å›½çš„å¤§å­¦éƒ½æ”¹æˆè‹±æ–‡æ•™å­¦å§ã€‚ã€‚çœ‹ç€é‚£äº›ç¿»è¯‘è¿‡æ¥çš„ä¹¦ç±éƒ½å¤´ç–¼ã€‚ã€‚è¿™ä¹ˆå¥½çš„è‹±æ–‡æ•™å­¦ææ–™ï¼Œä¸ºä»€ä¹ˆç¿»è¯‘è¿‡æ¥ä¹‹åå°±é‚£ä¹ˆéš¾ç†è§£äº†ã€‚ã€‚æ„Ÿè§‰å¾ˆå¤šè€å¸ˆå¯èƒ½è‡ªå·±æ‡‚äº†ï¼Œä½†æ˜¯è®²å‡ºæ¥çš„è¯¾æˆ–æ˜¯å†™å‡ºæ¥çš„ä¹¦ï¼Œå®Œå…¨å°±æ˜¯åº”ä»˜ä»»åŠ¡çš„å§ã€‚ã€‚ å¥½å§ï¼Œå›åˆ°ä¸»é¢˜ã€‚ä¼ªä»£ç ä¸­ï¼š æ¦‚ç‡çŸ©é˜µ forward [N+2,T] æ˜¯åŒ…æ‹¬äº†åˆå§‹çŠ¶æ€startå’Œç»“æŸçŠ¶æ€end,é‚£ä¹ˆ forward[s,t]è¡¨ç¤º $\\alpha_t(s)$ initialization step: $$\\alpha_1(j)=a_{0j}b_j(o_1),1\\le j \\le N$$ ä¼ªä»£ç ä¸­ï¼šforward[s,1] &lt;â€“ $a_{0,s}* b_s(o_1)$ æ˜¯ä»çŠ¶æ€0åˆ°t=1æ—¶åˆ»çš„çŠ¶æ€s Recursion (since states 0 anf F are non-emittinf): $$\\alpha_t(j)=[\\sum_{i=1}^N\\alpha_{t-1}(i)a_{ij}]b_j(o_t)$$ åœ¨ä¼ªä»£ç ä¸­æœ‰ä¸¤ä¸ªå¾ªç¯ï¼Œåˆ†åˆ«æ˜¯å¯¹æ—¶é—´æ­¥2åˆ°Tï¼Œä»¥åŠæ¯ä¸ªæ—¶é—´æ­¥ï¼Œå…¶ä¸­çš„çŠ¶æ€ä»1åˆ°N 1234567for each time step from 2 to T do: for each state s from 1 to N do: forward[s,t] = sum_{s'=1}^N forward[s', t-1] * a_{s',s} * b_s{o_t} $forward[s,t] = sum_{sâ€™=1}^N forward[sâ€™, t-1] * a_{sâ€™,s}$ å¯ä»¥å‘ç°ï¼Œæ¦‚ç‡çŸ©é˜µforwardä¸­çš„æ¯ä¸€ä¸ªå€¼è¡¨ç¤ºçš„æ˜¯tæ—¶åˆ»çŠ¶æ€ä¸ºsçš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯ $\\alpha_t(s)$ Termination: $$P(O|\\lambda)=\\alpha_T(q_F)=\\sum_{i=1}^N\\alpha_T(i)a_{iF}$$ ä¼ªä»£ç ä¸­ï¼š forward[$q_F$,T] = $\\sum_{s=1}^N$ forward[s,T] * $a_{s,q_F}$ return forward[$q_F$,T] Tæ—¶åˆ»çŠ¶æ€ä¸º $q_F$çš„æ¦‚ç‡ã€‚æ„Ÿè§‰åº”è¯¥æ˜¯T+1æ—¶åˆ»å§ã€‚ã€‚ã€‚ï¼Ÿï¼Ÿï¼Ÿ é¢„æµ‹é—®é¢˜ï¼šç»´ç‰¹æ¯”ç®—æ³•Decoding: The Viterbi Algorithm è§£ç é—®é¢˜ï¼ˆé¢„æµ‹é—®é¢˜ï¼‰ï¼šç»™å®šHMMæ¨¡å‹ $\\lambda=(A,B)$ å’Œè§‚å¯Ÿåºåˆ— $O=o_1,o_2,â€¦,o_T$, æ‰¾å‡ºæ¦‚ç‡æœ€å¤§çš„éšè—çŠ¶æ€åºåˆ— $Q=q_1q_2â€¦q_T$ åœ¨å‰å‘ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“äº†æ€ä¹ˆè®¡ç®—ç‰¹å®šéšè—çŠ¶æ€åºåˆ—å’Œè§‚æµ‹åºåˆ—çš„è”åˆæ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯å…¬å¼ï¼ˆ9.13ï¼‰ï¼Œç„¶åæ‰¾å‡ºå…¶ä¸­æ¦‚ç‡æœ€å¤§çš„åºåˆ—å°±å¯ä»¥äº†å¯¹å§ï¼Ÿä½†æ˜¯æˆ‘ä»¬çŸ¥é“çŠ¶æ€åºåˆ—æœ‰ $N^2$ ä¸ªï¼Œè¿™æ ·è®¡ç®—å°±å¤ªå¤æ‚äº†ã€‚äºæ˜¯ï¼Œæœ‰äº† Viterbi algorithm. ç»´ç‰¹æ¯”ç®—æ³•æ˜¯ä¸€ç§åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œç±»ä¼¼äºæœ€å°åŒ–ç¼–è¾‘è·ç¦»ã€‚ ä¸Šå›¾å±•ç¤ºäº†ï¼Œå†°æ·‡æ·‹ä¾‹å­ä¸­ï¼ŒHMMæ¨¡å‹å‚æ•°å·²çŸ¥ï¼Œè§‚æµ‹åºåˆ—ä¸ºï¼ˆ3,1,3ï¼‰çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—æœ€å¤§éšè—çŠ¶æ€åºåˆ—çš„è¿‡ç¨‹ã€‚Viterbiç½‘æ ¼ä¸­æ¯ä¸€ä¸ªcellä¸º $v_T(j)$ è¡¨ç¤ºtæ—¶åˆ»ï¼Œè§‚å¯Ÿåºåˆ—ä¸º $o_1,o_2,â€¦,o_t$ï¼Œ éšè—çŠ¶æ€ä¸ºjï¼Œå‰t-1çš„çŠ¶æ€åºåˆ—ä¸º $q_1q_2â€¦q_{t-1}$ çš„æ¦‚ç‡ã€‚ $$v_t(j)=P(q_0q_1â€¦q_{t-1},o_1,o_2,â€¦,o_t,q_t=j|\\lambda)$$ æ¢å¥è¯è¯´ï¼Œt-1æ—¶åˆ»çš„çŠ¶æ€åºåˆ—æ˜¯è¿™æ · $q_1q_2â€¦q_{t-1}$ï¼Œæœ‰Nä¸ªè¿™æ ·çš„åºåˆ—ï¼ˆå› ä¸ºt-1æ—¶åˆ»çš„çŠ¶æ€æœ‰Nä¸ªï¼‰ï¼Œç„¶åè®¡ç®—å‡ºè¿™Nä¸ªåºåˆ—ä¸­åˆ°tæ—¶åˆ»çŠ¶æ€ä¸ºjçš„æ¦‚ç‡ï¼Œæ‰¾å‡ºå…¶ä¸­æœ€å¤§å€¼ï¼Œå°±æ˜¯ä»å¼€å§‹åˆ°tæ—¶åˆ»çŠ¶æ€ä¸ºjçš„æœ€å¤§æ¦‚ç‡åºåˆ—ã€‚ $$v_t(j)=max_{i=1}^N v_{t-1}(i)a_{ij}b_j(o_t)$$ å¯¹åº”åˆ°å›¾ï¼ˆ9.10ï¼‰ä¸­ï¼Œä»¥ $v_2(2)$ä¸ºä¾‹ï¼Œ $$v_2(2)=max(v_1(1)* a_{12}* b_2(1),v_1(2)* a_{22}* b_2(1))$$ $$v_2(2)=max(v_1(1)* P(H|C)* P(1|H), v_1(2)* P(H|H)* P(1|H))$$ ç»´ç‰¹æ¯”ç®—æ³•çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œä¼ªä»£ç å¦‚ä¸‹ï¼š æˆ‘ä»¬å‘ç°Viterbiç®—æ³•è·Ÿå‰å‘ç®—æ³•éå¸¸ç›¸ä¼¼ï¼Œé™¤äº†å‰å‘ç®—æ³•æ˜¯è®¡ç®—çš„sumï¼Œè€ŒViterbiæ˜¯è®¡ç®—çš„maxï¼ŒåŒæ—¶æˆ‘ä»¬ä¹Ÿå‘ç°ï¼ŒViterbiç›¸æ¯”å‰å‘ç®—æ³•å¤šäº†ä¸€ä¸ªéƒ¨åˆ†ï¼šbackpointers. åŸå› æ˜¯å› ä¸ºå‰å‘ç®—æ³•åªéœ€è¦è®¡ç®—å‡ºæœ€åçš„ä¼¼ç„¶æ¦‚ç‡ï¼Œä½†Viterbiä¸ä»…è¦è®¡ç®—å‡ºæœ€å¤§çš„æ¦‚ç‡ï¼Œè¿˜è¦å¾—åˆ°å¯¹åº”çš„çŠ¶æ€åºåˆ—ã€‚å› æ­¤ï¼Œåœ¨ç±»ä¼¼äºå‰å‘ç®—æ³•è®¡ç®—æ¦‚ç‡çš„åŒæ—¶ï¼Œè®°å½•ä¸‹è·¯å¾„ï¼Œå¹¶åœ¨æœ€åbacktracingæœ€å¤§æ¦‚ç‡çš„è·¯å¾„ã€‚ The Viterbi backtrace. initialization: $$v_1(j)=a_{0j}b_j(o_1)\\tag{9.20}$$ $$b_{t1}(j)=0\\tag{9.21}$$ åˆå§‹çŠ¶æ€åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹startï¼Œå¯ç¡®å®šä¸º0 Recursion(recall that states 0 and $q_F$ are non-emitting): $$v_t(j)=max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t);1\\le j \\le N, 1\\le t\\le T\\tag{9.22}$$ $$b_{t_t}(j)=argmax_{i=1}^Nv_{t-1}(i)a_{ij}b_i(o_t);1\\le j \\le N, 1\\le t\\le T\\tag{9.23}$$ $b_{t_t}(j)è¡¨ç¤ºtæ—¶åˆ»çŠ¶æ€ä¸ºjçš„æ‰€æœ‰Nä¸ªè·¯å¾„ $(q_1,q_2,â€¦,q_{t-1})$$ æ¦‚ç‡æœ€å¤§çš„è·¯å¾„çš„ç¬¬k-1ä¸ªèŠ‚ç‚¹ã€‚ ä»¥å›¾9.12ä¸ºä¾‹ï¼Œå¯¹äºt=2ï¼ŒçŠ¶æ€ä¸º1çš„èŠ‚ç‚¹ï¼Œå…¶max()ä¸­æœ‰2é¡¹ï¼Œåˆ†åˆ«æ˜¯ $v_1(1)* a_{11}* b_1(1)$ å’Œ $v_1(2)* a_{21}* b_1(1)$,å…¶ä¸­è¾ƒå¤§çš„é¡¹çš„èŠ‚ç‚¹å°±æ˜¯ $max_{i=1}^Nv_{t-1}(i)a_{ij}b_i(o_t)$, ä½†tæ—¶åˆ»ä¹Ÿæœ‰Nä¸ªèŠ‚ç‚¹ï¼Œæ‰€ä»¥ä¹Ÿéœ€è¦è®°å½•ï¼Œå› æ­¤ç”¨arg Termination: The best score: $$P*=v_T(q_F)=max_{i=1}^Nv_T(i)* a_{iF}\\tag{9.24}$$ è®¡ç®—å‡ºTæ—¶åˆ»åˆ°endçŠ¶æ€çš„æœ€å¤§æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰è·¯å¾„ä¸­çš„æœ€å¤§æ¦‚ç‡ã€‚ The start of backtrace: $$q_T*=b_{t_T}(q_F)=argmax_{i=1}^Nv_T(i)* a_{iF}\\tag{9.25}$$ è¡¨ç¤ºTæ—¶åˆ»ä¸­Nä¸ªè·¯å¾„ä¸­æ¦‚ç‡æœ€å¤§çš„ç»“ç‚¹ã€‚ å…¶å®Viterbiç®—æ³•çš„è·¯å¾„æ•°é‡å’Œå‰å‘ç®—æ³•çš„è·¯å¾„æ•°é‡æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ï¼Œåªæ˜¯ä¸€ä¸ªæ˜¯maxï¼Œä¸€ä¸ªæ˜¯sumï¼Œå› æ­¤ä¹Ÿå¯ä»¥å‚è€ƒå›¾9.8. å­¦ä¹ é—®é¢˜ï¼šHMM Training: The Forward-Backward AlgorithmLearning: Given an observation sequence O and the set of possible states in the HMM, learn the HMM parameters A and B. ç¬¬ä¸‰ä¸ªé—®é¢˜ï¼Œç»™å®šè§‚æµ‹åºåˆ— $O=(o_1,o_2,â€¦,o_T)$, ä¼°è®¡æ¨¡å‹å‚æ•°ä½¿å¾—åœ¨è¯¥HMMæ¨¡å‹ä¸‹ï¼Œè§‚æµ‹åºåˆ—çš„æ¦‚ç‡ $P(O|\\lambda)$ æ¦‚ç‡æœ€å¤§ï¼Œå³ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•ä¼°è®¡å‚æ•°ã€‚ å…ˆè€ƒè™‘é©¬å°”å¯å¤«é“¾é©¬å°”å¯å¤«é“¾å…¶çŠ¶æ€æ˜¯å¯è§‚å¯Ÿçš„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯é€€åŒ–çš„éšé©¬å°”å¯å¤«æ¨¡å‹ã€‚å³æ²¡æœ‰å‘å°„æ¦‚ç‡(emmision probablities) B.å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å­¦ä¹ çš„å‚æ•°åªæœ‰çŠ¶æ€è½¬ç§»çŸ©é˜µï¼ˆprobability matrixï¼‰A. å…¶ä¸­ $a_ij$ è¡¨ç¤ºä»çŠ¶æ€iè½¬ç§»åˆ°çŠ¶æ€jçš„æ¦‚ç‡ï¼Œå¯ä»¥ç”¨å¤§æ•°å®šå¾‹æ¥è®¡ç®—ã€‚ $C(i\\rightarrow)$ è¡¨ç¤ºè§‚å¯Ÿåˆ°çš„åºåˆ—ä¸­ä»çŠ¶æ€iè½¬ç§»åˆ°çŠ¶æ€jçš„æ•°é‡ã€‚ç„¶åé™¤ä»¥æ‰€æœ‰ä»çŠ¶æ€iè½¬ç§»çš„æ€»æ•°é‡ã€‚ $$a_{ij}=\\dfrac{C(i\\rightarrow j)}{\\sum_{q\\in Q}C(i\\rightarrow q)}\\tag{9.26}$$ æ˜¾ç„¶åˆ†æ¯ä¸åŒ…æ‹¬æœ€å T æ—¶åˆ»å‡ºç°çŠ¶æ€ iï¼Œå› ä¸ºendä¸å±äºQ. éšé©¬å°”å¯å¤«æ¨¡å‹ï¼š Baum-Welchç®—æ³• The Baum-Welch algorithm uses two neat intuitions to solve this problem. The first idea is to iteratively estimate the counts. We will start with an estimate for the transition and observation probabilities and then use these estimated probabilities to derive better and better probabilities. The second idea is that we get our estimated probabilities by computing the forward probability for an observation and then dividing that probability mass among all the different paths that contributed to this forward probability. å…¶å®å°±æ˜¯EMç®—æ³•ï½ åœ¨æ­¤ä¹‹å‰ï¼Œå…ˆäº†è§£ä¸€ä¸‹åå‘ç®—æ³•ï¼Œå¯ä»¥çœ‹åšåå‘çš„å‰å‘ç®—æ³•ã€‚ åå‘ç®—æ³•å¯¹åº”çš„åå‘æ¦‚ç‡ï¼ˆBackward probabilityï¼‰$\\beta_t(i)$ è¡¨ç¤ºç»™å®šhmmæ¨¡å‹ $\\lambda$, åœ¨ t æ—¶åˆ»çŠ¶æ€ä¸º i çš„æ¡ä»¶ä¸‹ï¼Œt+1 æ—¶åˆ»çš„è§‚æµ‹åºåˆ—ä¸º $o_{t+1},o_{t+2},â€¦,o_T$çš„æ¦‚ç‡. $$\\beta_t(i)=P(o_{t+1},o_{t+2},â€¦,0_T|q_t=i,\\lambda)\\tag{9.27}$$ initialization: $$\\beta_T(i)=a_{iF},1\\le i \\le N$$ åœ¨æèˆªè€å¸ˆçš„ã€Šç»Ÿè®¡å­¦ä¹ å¯¼è®ºã€‹è¿™æœ¬ä¹¦ä¸Š $\\beta_T(i)=1$. $\\beta_T(i)$ çš„å®šä¹‰è¡¨ç¤ºåœ¨Tæ—¶åˆ»çŠ¶æ€ä¸ºiï¼Œè§‚æµ‹åˆ°åºåˆ—ä¸º $o_{T+1}$, è¿™ä¸œè¥¿ä¸å­˜åœ¨ï¼Œå¯ä»¥çœ‹åšæ˜¯endå§ï¼Œæ‰€ä»¥ä»Tåˆ°endå…¶æ¦‚ç‡åº”è¯¥æ˜¯ $a_{iF}$,ä½†æ˜¯åœ¨æèˆªè€å¸ˆçš„ä¹¦ä¸Šåªæœ‰åˆå§‹çŠ¶æ€çš„æ¦‚ç‡ $\\pi=a_{01}$,è€Œæ²¡æœ‰ $a_{iF}$. æ„Ÿè§‰è·Ÿå…·ä½“åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æœ‰å…³ç³»ã€‚ã€‚ Recursion(again since stetes 0 and $q_F$ are non-emitting) $$\\beta_t(i)=\\sum_{j=1}^N\\beta_{t+1}(j)a_{ij}b_j(o_{t+1}),1\\le j \\le N, 1\\le t\\le T$$ æ ¹æ®å®šä¹‰å¾ˆå¥½ç†è§£ã€‚$\\beta_{t+1}(j)$ è¡¨ç¤ºç»™å®šhmmæ¨¡å‹ $\\lambda$, åœ¨t+1æ—¶åˆ»çŠ¶æ€ä¸ºjçš„æ¡ä»¶ä¸‹ï¼Œt+1æ—¶åˆ»çš„è§‚æµ‹åºåˆ—ä¸º $o_{t+2},o_{t+3},â€¦,o_T$çš„æ¦‚ç‡.é‚£ä¹ˆå°±å¯ä»¥å¾—åˆ° $\\beta_t(i)$ äº†ã€‚ Termination: $$P(O|\\lambda)=\\alpha_T(q_F)=\\beta_1(q_0)=\\sum_{j=1}^N\\beta_1(j)a_{0j}b_j(o_1)$$ åœ¨åˆå§‹æ¨¡å‹å‚æ•°ä¸‹ï¼Œç”¨å¤§æ•°å®šå¾‹ä¼°è®¡æ–°çš„æ¨¡å‹å‚æ•°ï¼Œä¹Ÿå°±æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡æ ¹æ®å…¬å¼(9.26)æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼ŒçŠ¶æ€ï½‰åˆ°ï½Šçš„æ¦‚ç‡ï¼š $$\\hat a_{ij}=\\dfrac{expected\\ number\\ of\\ transitions\\ from\\ state\\ i \\ to\\ state\\ j}{expected\\ number\\ of\\ transitions\\ from\\ state\\ i}$$ ç„¶è€Œæ€ä¹ˆè®¡ç®—è¿™äº›numeratorï¼Ÿè¯•æƒ³ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ç‰¹å®šæ—¶åˆ»tï¼Œä»çŠ¶æ€iè½¬ç§»åˆ°jçš„æ¦‚ç‡ï¼Œé‚£ä¹ˆå°±èƒ½è®¡ç®—æ‰€æœ‰æ—¶åˆ»tçš„ä»iè½¬ç§»åˆ°jçš„æ•°é‡ã€‚ å®šä¹‰ $\\zeta_t$ è¡¨ç¤ºåœ¨ç»™å®šæ¨¡å‹å‚æ•°å’Œè§‚å¯Ÿåºåˆ—æ¡ä»¶ä¸‹ï¼Œtæ—¶åˆ»çŠ¶æ€ä¸ºiï¼Œt+1æ—¶åˆ»çŠ¶æ€ä¸ºjçš„æ¦‚ç‡ã€‚ $$\\zeta_t(i,j)=P(q_t=i,q_{t+1}=j|O,\\lambda)\\tag{9.32}$$ ä½†æ˜¯æ¨¡å‹å‚æ•°æˆ‘ä»¬ä¸çŸ¥é“å‘€ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬éœ€è¦å­¦ä¹ å¾—åˆ°çš„ã€‚ è¿™æ ·æˆ‘ä»¬å…ˆè®¡ç®—ä¸€ä¸ªå’Œ $\\zeta_t$ ç›¸ä¼¼çš„æ¦‚ç‡ã€‚ $$not-quite-\\zeta_t(i,j)=P(q_t=i,q_{t+1}=j,O|\\lambda)\\tag{9.33}$$ $\\alpha_t(i)$ å’Œ $\\beta_t(j)$ æ˜¯å‰å‘/åå‘ç®—æ³•ä¸­çš„å®šä¹‰ã€‚æˆ‘ä»¬å…ˆçœ‹ä¸‹å‰å‘ç®—æ³•è®¡ç®—çš„æ¡ä»¶ï¼š ä¹Ÿå°±æ˜¯problem1ä¸­çš„æ¡ä»¶ï¼Œç»™å®š $\\lambda$ å’Œ è§‚å¯Ÿåºåˆ—ï¼Œæ±‚ $P(O|\\lambda)$ æˆ‘ä»¬å¯ä»¥ç”¨ $\\alpha_t(i)$ å’Œ $\\beta_t(j)$ ï¼Œæ¥è¡¨ç¤º $\\zeta_t$, æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨è®¡ç®— $\\zeta_t$ æ—¶æ˜¯å…ˆå‡å®šæœ‰è¿™ä¸ªä¸€ä¸ªæ¨¡å‹å‚æ•°ï¼Œæ¯”å¦‚åˆå§‹å‚æ•°ï½ é‚£ä¹ˆï¼š $$not-quite-\\zeta_t=\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)\\tag{9.34}$$ æ ¹æ®bayeså…¬å¼ï¼š $$P(X|Y,Z)=\\dfrac{P(X,Y,Z)}{P(Y,Z)}=\\dfrac{P(X,Y,Z)}{P(Z)P(Y|Z)}=\\dfrac{P(X,Y|Z)}{P(Y|Z)}\\tag{9.35}$$ å¯¹åº”èµ·æ¥å°±æ˜¯ï¼š $$P(q_t=i,q_{t+1}=j|O,\\lambda)=\\dfrac{P(q_t=i,q_{t+1}=j,O|\\lambda)}{P(O|\\lambda)}=\\dfrac{not-quite-\\zeta_t}{P(O|\\lambda)}\\tag{9.36}$$ å…¶ä¸­ï¼š $$P(O|\\lambda)=\\alpha_T(q_F)=\\beta_1(q_0)=\\sum_{j=1}^N\\alpha_t(j)\\beta_t(j)\\tag{9.37}$$ è¿™ä¸€æ­¥æœ€åé¢ä¸€ä¸ªå¼å­çš„ç†è§£å¯ä»¥çœ‹åšæ˜¯å‰å‘ç®—æ³•å’Œåå‘ç®—æ³•åœ¨æ—¶åˆ»tç›¸é‡ã€‚ çœ‹åˆ°è¿™é‡Œä¼šå‘ç°ï¼Œæèˆªè€å¸ˆä¹¦ä¸­179é¡µï¼Œå…¬å¼25-26çš„æ¨å¯¼å°±æœ‰ç‚¹é€»è¾‘ä¸é€šäº†ã€‚ å› æ­¤ï¼Œç°åœ¨å°±å¯ä»¥æ¨å¯¼å‡º $\\zeta_t$ï¼š $$\\zeta_t{i,j}=\\dfrac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\alpha_T(q_F)}\\tag{9.37}$$ $\\zeta_t$ è¡¨ç¤ºçš„æ˜¯æŸä¸€ä¸ªæ—¶åˆ»tï¼Œé‚£ä¹ˆå¯¹äºå‚æ•° $a_{ij}$ çš„ä¼°è®¡å°±æ˜¯æ‰€æœ‰çš„æ—¶åˆ»ä¸­iåˆ°jçš„æ€»æ•°é™¤ä»¥iåˆ°k(k=1,2â€¦N)çš„æ€»æ•° $$\\hat a_{ij}=\\dfrac{\\sum_{t=1}^{T-1}\\zeta_t(i,j)}{\\sum_{t=1}^{T-1}\\sum_{k=1}^N\\zeta_t(i,k)}\\tag{9.38}$$ åŒæ ·çš„é“ç†ï¼Œæˆ‘ä»¬å¯ä»¥æ¨ç†å¾—åˆ°å‘å°„çŸ©é˜µBçš„å‚æ•°ä¼°è®¡ $b_j(v_k)$ çŠ¶æ€ä¸ºjï¼Œè§‚å¯Ÿå¾—åˆ° $v_k,v_k\\in V$çš„æ¦‚ç‡ï¼š åœ¨tæ—¶åˆ»çŠ¶æ€ä¸ºjçš„æ¦‚ç‡ï¼Œå®šä¹‰ä¸º $\\gamma_t(j)$ $$\\gamma_t(j)=P(q_t=j|O,\\lambda)\\tag{9.40}$$ åŒæ ·çš„é“ç†ï¼š $$\\gamma_t(j)=\\dfrac{P(q_t=j,O|\\lambda)}{P(O|\\lambda)}\\tag{9.41}$$ åŒå…¬å¼ï¼ˆ9.37ï¼‰ä¸€æ ·ï¼Œå‰å‘åå‘ç®—æ³•åœ¨tæ—¶åˆ»ç›¸é‡ï¼š $$\\gamma_t(j)=\\dfrac{\\alpha_t(j)\\beta_t(j)}{P(O|\\lambda)}$$ ç„¶åæ±‚æ•´ä¸ªæ—¶é—´æ®µå†…çš„jåˆ° $v_k$çš„æ€»æ•°ï¼Œé™¤ä»¥çŠ¶æ€ä¸ºjçš„æ€»æ•°ã€‚ $$\\hat b_j(v_k)=\\dfrac{\\sum_{t=1,o_t=v_k}^T}{\\sum_{t=1}^T}\\gamma_t(j)$$ ä»”ç»†å›é¡¾ä»¥ä¸‹è¿™ä¸ªè¿‡ç¨‹ï¼Œåœ¨åˆå§‹æ¨¡å‹å‚æ•°å’Œè§‚æµ‹åºåˆ—çš„æ¡ä»¶ä¸‹ï¼Œæ ¹æ®å¤§æ•°å®šå¾‹å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚å…¶å®å°±æ˜¯åœ¨åˆå§‹æ¨¡å‹å‚æ•°ä¸‹ï¼Œæ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡æ±‚å¾—è§‚æµ‹åºåˆ—çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œç„¶ååœ¨ä¼¼ç„¶æ¦‚ç‡æœ€å¤§çš„æ¡ä»¶ä¸‹æ±‚å¾—ç›¸åº”çš„æ¨¡å‹å‚æ•°ã€‚ å¯ä»¥çœ‹åˆ°è¿™é‡Œç›´æ¥ç”¨å¤§æ•°å®šå¾‹å’Œæèˆªè€å¸ˆä¹¦ä¸Šï¼Œä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œç„¶åæ±‚å¯¼å¾—åˆ°çš„å…¬å¼æ˜¯ä¸€æ ·çš„ã€‚ EMç®—æ³• E step: æ ¹æ®åˆå§‹æ¨¡å‹å‚æ•°æˆ–æ˜¯M stepå¾—åˆ°çš„æ¨¡å‹å‚æ•°ï¼Œå¾—åˆ°åéªŒæ¦‚ç‡ã€‚ M step: æ ¹æ®E stepä¸­å¾—åˆ°çš„æ¦‚ç‡ï¼Œä¼°è®¡å‡ºæ–°çš„æ¨¡å‹å‚æ•°ã€‚è¿™é‡Œç›´æ¥ç”¨å¤§æ•°å®šå¾‹å¾—åˆ°çš„ï¼Œå…¶å®å…¶æœ¬è´¨åŸç†å°±æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œä¹Ÿå°±æ˜¯æ±‚å‡ºä½¿å¾—æ¦‚ç‡æœ€å¤§çš„æ¨¡å‹å‚æ•°ã€‚ é‚£ä¹ˆè¿­ä»£æ¡ä»¶å‘¢ï¼Ÿä»€ä¹ˆæƒ…å†µä¸‹ç»ˆæ­¢ï¼Ÿåœ¨GMMä¸­æœ‰logå‡½æ•°ï¼Œè¿™é‡Œå‘¢ã€‚ã€‚ è¿™é‡Œåº”è¯¥å°±æ˜¯ $P(O|\\lambda)$ å§ï¼Œåœ¨å‰å‘ç®—æ³•ä¸­æœ‰è®¡ç®—åˆ°åœ¨æ¨¡å‹å‚æ•°å’Œè§‚å¯Ÿåºåˆ—æ¡ä»¶ä¸‹çš„æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚ æ ¹æ®GMMå’ŒHMMå¯¹ä½¿ç”¨EMç®—æ³•è¿›è¡Œå‚æ•°ä¼°è®¡çš„ä¸€ç‚¹æƒ³æ³•ï¼šæ‰€ä»¥EMç®—æ³•ä¸­çš„E stepå¹¶ä¸æ˜¯æ±‚æœŸæœ›ï¼Œè€Œæ˜¯åœ¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼°è®¡æ—¶ï¼Œåœ¨åˆå§‹æ¨¡å‹æˆ–previousæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ±‚å¾—åŸºäºè§‚æµ‹åºåˆ—æˆ–æ˜¯è®­ç»ƒæ ·æœ¬çš„ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æ˜¯å¤§æ•°å®šå¾‹æ±‚å¾—åéªŒæ¦‚ç‡ã€‚ ç„¶åM STEPå°±æ˜¯è®©è¿™ä¸ªæ¦‚ç‡æœ€å¤§çš„æ¡ä»¶ä¸‹æ›´æ–°æ¨¡å‹å‚æ•°ã€‚ æ€»ç»“ï¼š åœ¨å›é¡¾ä¸‹éšé©¬å°”å¯å¤«æ¨¡å‹çš„ä¸‰ä¸ªé—®é¢˜ï¼š ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œè®¡ç®—æ¦‚ç‡ å·²çŸ¥æ¨¡å‹å‚æ•° $\\lambda$ å’Œè§‚æµ‹åºåˆ— $O$ï¼Œæ±‚åœ¨è¯¥æ¨¡å‹ä¸‹ï¼Œå‡ºç°è§‚æµ‹åºåˆ—çš„æ¦‚ç‡ã€‚ ä½¿ç”¨å‰å‘ç®—æ³•ï¼Œä¸€ä¸ªåŠ¨æ€å›å½’çš„ç®—æ³•ï¼ŒæŠŠæ±‚é•¿åº¦ä¸ºTçš„æ¦‚ç‡è½¬æ¢ä¸ºtåˆ°t+1çš„æ¦‚ç‡sum è¿™ä¸€é—®é¢˜å…¶å®ä¸»è¦æ˜¯ä¸ºåé¢ä¸¤ä¸ªé—®é¢˜é“ºå«çš„ï¼Œå› ä¸ºä¸€èˆ¬çš„åœºæ™¯éƒ½æ˜¯çŠ¶æ€æœªçŸ¥ï¼Œæ›´ä¸å¯èƒ½çŸ¥é“æ¨¡å‹å‚æ•°äº†ã€‚ é¢„æµ‹é—®é¢˜ï¼Œåˆç§°è§£ç é—®é¢˜ å·²çŸ¥æ¨¡å‹å‚æ•° $\\lambda$ å’Œè§‚æµ‹åºåˆ— $O$, æ±‚æ¦‚ç‡æœ€å¤§çš„çŠ¶æ€åºåˆ—ã€‚ ä½¿ç”¨Viterbiç®—æ³•ï¼Œç±»ä¼¼äºå‰å‘ç®—æ³•ï¼Œä¸è¿‡æ¯ä¸€æ­¥ä¸æ˜¯sumï¼Œè€Œæ˜¯maxï¼Œå¹¶ä¸”éœ€è¦å›æº¯backpointers è¿™ä¸ªé—®é¢˜çš„åº”ç”¨åœºæ™¯å°±æ¯”è¾ƒå¹¿äº†ã€‚ å­¦ä¹ é—®é¢˜ï¼šæ¨¡å‹å‚æ•°ä¼°è®¡ å·²çŸ¥è§‚æµ‹åºåˆ— $O$ï¼Œä¼°è®¡æ¨¡å‹å‚æ•° $\\lambda$, ä½¿å¾—è§‚æµ‹åºåˆ—çš„æ¦‚ç‡ $P(O|\\lambda)$ æœ€å¤§ã€‚ ä½¿ç”¨Baum-Welch(æå¤§ä¼¼ç„¶ä¼°è®¡)æˆ–forward-backward(å¤§æ•°å®šå¾‹)ç®—æ³•ï¼Œå¹¶ä½¿ç”¨EMç®—æ³•è¿­ä»£ï¼Œå¯¹å‚æ•°è¿›è¡Œä¼°è®¡ï¼Œ","link":"/2018/04/06/chapter9-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-standford/"},{"title":"chapter7-logisticå›å½’","text":"logistic regression æ¨¡å‹ p(y|x,w) é’ˆå¯¹è¯­è¨€æ¨¡å‹çš„ç‰¹å¾å¤„ç† $f_i(c,x)$ è®­ç»ƒæ¨¡å‹ æ­£åˆ™åŒ– ç‰¹å¾é€‰æ‹©ï¼šä¿¡æ¯å¢ç›Š åˆ†ç±»å™¨é€‰æ‹©ï¼šbias-variance å‰è¨€: åˆ†ç±»ç®—æ³•ï¼šmultinomial logistic regression, å½“åº”ç”¨åˆ°NLPæ—¶ï¼Œåˆç§° maximum entropy, MaxEnt. å†ä¸€æ¬¡è¯´åˆ°äº†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹ã€‚ä¸Šä¸€ç« å·²ç»è¯´äº†äº†ï¼Œå°±ä¸å†™äº†ï½ logistic regression$$\\hat y=argmax_yP(y|x)$$ è¿™å¥è¯å¯ä»¥è¯´è§£é‡Šçš„å¾ˆæ¸…æ¥šäº†ï½ ç„¶é¹…ï¼Œèƒ½å¤Ÿç›´æ¥è®¡ç®—å‡ºå¯¹åº”çš„æ¦‚ç‡P(y|x)å—ï¼Ÿåƒè¿™æ ·ï¼š $$P(y|x)?=\\sum_{i=1}^Nw_if_i$$ $$?=w\\cdot f$$ æ˜¾ç„¶ï¼Œè¿™è®¡ç®—å‡ºæ¥çš„å¹¶ä¸æ˜¯ä¸€ä¸ªåˆç†çš„æ¦‚ç‡ï¼Œå› ä¸º $\\sum_{i=1}^N$ çš„èŒƒå›´æ˜¯ $-\\infty\\ to\\ \\infty$. æ€ä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿå°±æ˜¯è®©å¾—åˆ°çš„æ¦‚ç‡åœ¨0-1ä¹‹é—´ã€‚ å¯¹äºäºŒåˆ†ç±»ï¼š$y\\in {0,1}$å¯ä»¥ä½¿ç”¨sigmoidå‡½æ•°ï¼š $$\\sigma(z)=\\dfrac{1}{1+e^{-z}}$$ å°†zå‹ç¼©åˆ°0-1èŒƒå›´å†…ã€‚ åˆ™æœ‰ï¼š $$\\hat y = \\dfrac{1}{1+e^{-w^Tx}}$$ $$p(y=1|x) = \\dfrac{1}{1+e^{-w^Tx}}$$ $$p(y=0|x) = \\dfrac{1}{1+e^{w^Tx}}$$ æ ¹æ®cross-entroyå‡½æ•°ï¼š $$L = -p(x)logq(x)$$ å¯¹äºå•ä¸ªæ ·æœ¬æœ‰ï¼š $çœŸå®åˆ†å¸ƒp(x):(y,1-y),å¯¹åº”çš„é¢„æµ‹åˆ†å¸ƒ(\\hat y, 1-\\hat y)$ å¸¦å…¥å¯å¾—ï¼š $$L(\\hat y, y)=-ylog(\\hat y)-(1-y)log(1-\\hat y)$$ å¯¹äºå¤šåˆ†ç±»softmaxåˆ†ç±»å™¨ï¼š $$p(c|x)=\\dfrac{exp(\\sum_{i=1}^Nw_if_i(c,x))}{\\sum_{câ€™\\in C}exp(\\sum_{i=1}^Nw_if_i(câ€™,x))}$$ å…¶ä¸­ $f_i(c,x)$ å°±æ˜¯è¡¨ç¤ºåœ¨ç»™å®šæ ·æœ¬æ¡ä»¶ä¸‹ç±»åˆ«cå¯¹åº”çš„è¾“å…¥æ•°æ®å¤„ç†åçš„ç‰¹å¾iã€‚ æ€ä¹ˆç†è§£ $w_if_i(c,x)$ å‘¢ï¼Ÿhow to convince myself~ å‡è®¾å•ä¸ªæ ·æœ¬ $X:$ (3072,1) æ€»å…±æœ‰10ä¸ªç±»åˆ« $c\\in C$ (10,) åˆ™å¯¹åº”çš„æƒé‡ï¼š $W$ (10, 3072) å…¶å®å¯ä»¥è®¤ä¸ºæƒé‡Wçš„æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªåˆ†ç±»å™¨ $w_i$ï¼Œä¹Ÿå°±æ˜¯ç‰¹å¾æå–å™¨ã€‚$f_i(c,x)$ åº”è¯¥å°±æ˜¯å¯¹ä¸åŒç±»åˆ«çš„è¾“å…¥æ•°æ®è¿›è¡Œç‰¹å¾å¤„ç†å§ã€‚ åœ¨å›¾åƒå¤„ç†ä¸­ï¼Œå¯èƒ½å¹¶ä¸éœ€è¦æå‰å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†ï¼Œä½†åœ¨NLPä¸­å…ˆå¯¹è¾“å…¥æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹æ˜¯å¾ˆé‡è¦çš„ã€‚ Features in Multinomial Logistic Regressionå‡è®¾document xå«æœ‰è¯great,å…¶classæ˜¯ +($f_1$).åˆ™å¯¹åº”çš„ $f_1(c,x)$ $w_1(x)$ è¡¨ç¤ºgreatä½œä¸º class + çš„æƒé‡ã€‚ Classification in Multinomial Logistic Regressionè¿™æ˜¯ä¸€ä¸ªç®€å•çš„äºŒåˆ†ç±»positive or negativeï¼Œå…¶å®ä¹Ÿå¯ä»¥çœ‹èµ·æ¥è·Ÿå›¾åƒæ˜¯ä¸€æ ·çš„ï¼Œæ¯”å¦‚è¿™é‡Œæœ‰4ä¸ªè¯ï¼Œä¹Ÿå°±æ˜¯4ä¸ªç‰¹å¾, $x = (1,1,1,1)^T$ $w_+=(0,0,0,1.9)$,$w__ =(0.7,0.9,-0.8)$ åœ¨é¢„æµ‹å±äºå“ªä¸€ç±»æ˜¯ï¼Œå¯ä»¥ç®€åŒ–è®¡ç®—ï¼š Learning Logistic Regressionæ€ä¹ˆè®¡ç®—æƒé‡å‘¢ï¼Ÿ è®­ç»ƒæ ·æœ¬æ•°æ®é€šè¿‡ æ¡ä»¶æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆconditional maximum likelihood estimation.ï¼‰ å¯¹äºå•ä¸ªæ ·æœ¬ $(x^{(j)},y^{(j)})$ ï¼Œä¼˜åŒ–æƒé‡ï¼š $$\\hat w = argamx_w logP(y^{(j)}|x^{(j)})$$ é‚£ä¹ˆå¯¹äºæ•´ä¸ªæ ·æœ¬é›†ï¼š $$\\hat w = argamx_w \\sum_jlogP(y^{(j)}|x^{(j)})$$ é€šè¿‡ä¼˜åŒ–ä¼¼ç„¶æ¦‚ç‡ $L(w)$ æ¥å­¦ä¹ å¾—åˆ°å‚æ•°w $$L(w) = \\sum_jlogP(y^{(j)}|x^{(j)})\\tag{7.12}$$ è¿™é‡Œå’Œæˆ‘ä»¬å‰é¢è®²è¿‡çš„softmaxå›å½’æœ‰ç‚¹åŒºåˆ«ï¼Œsoftmaxæ˜¯å…ˆæ±‚å‡º $\\hat y$,ç„¶åä¸çœŸå®åˆ†å¸ƒyè¿›è¡Œæ¯”è¾ƒï¼Œæœ€å°åŒ–å·®å¼‚ï¼›è€Œmultinomial logistic regression æ˜¯ç›´æ¥å°†çœŸå®åˆ†å¸ƒyå’Œè§‚æµ‹æ•°æ®xè”åˆåœ¨ä¸€èµ·æœ€å¤§åŒ–p(y|x). åŒæ ·ä¹Ÿæ˜¯ä¸€ä¸ªå‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆconvex optimization problemï¼‰ï¼Œé€šè¿‡é‡‡ç”¨éšæœºæ¢¯åº¦ä¸Šå‡ï½ $Lâ€™(w)å…³äºæƒé‡æ±‚å¯¼$ æ­£åˆ™åŒ–å½“æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰æ—¶ï¼Œç»™å…¬å¼ï¼ˆ7.12ï¼‰å¢åŠ æ­£åˆ™åŒ–é¡¹ï¼Œç”¨æ¥æƒ©ç½šæƒé‡è¾ƒå¤§çš„é¡¹ã€‚ L2æ­£åˆ™åŒ–Euclidean distance L1æ­£åˆ™åŒ–Manhattan distance L1æ­£åˆ™åŒ–å’ŒL2æ­£åˆ™åŒ–éƒ½å¯ä»¥é€šè¿‡è´å¶æ–¯æ¥è§£é‡Šï½ L1æ­£åˆ™åŒ–å¯ä»¥çœ‹ä½œæ˜¯æƒé‡æ»¡è¶³Laplaceåˆ†å¸ƒ. L2æ­£åˆ™åŒ–å¯ä»¥çœ‹åšæ˜¯æƒé‡æ»¡è¶³å‡å€¼ä¸º0çš„é«˜æ–¯åˆ†å¸ƒ ä»¥L2æ­£åˆ™åŒ–ä¸ºä¾‹ï¼Œ$w_j$æœä»é«˜æ–¯åˆ†å¸ƒ ç„¶åå‡è®¾å‡å€¼ $\\mu=0$ï¼Œ$2\\sigma^2=1$,åœ¨å¯¹æ•°åŸŸå¯¹wæ±‚å¯¼å¯å¾—ï¼š è¿™ä¸å…¬å¼ï¼ˆ7.17ï¼‰ä¸€è‡´ï½ çŸ¥ä¹ä¸Šæœ‰ä¸€ç¯‡æ–‡ç« å¾ˆå¥½çš„è§£é‡Šäº†L1æ­£åˆ™åŒ–ä¸L2æ­£åˆ™åŒ– Feature Selection ç‰¹å¾é€‰æ‹©å¯¹äºç”Ÿæˆæ¨¡å‹å¦‚ naive bayes æ— æ³•ä½¿ç”¨æ­£åˆ™åŒ–ï¼Œå› æ­¤éœ€è¦ feature selection å¦‚ä½•è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå°±æ˜¯é€šè¿‡ä¸€äº›metricå¯¹ç‰¹å¾è¿›è¡Œæ’åºï¼Œé€‰æ‹©é‡è¦çš„ç‰¹å¾ã€‚ information gain è¿™éƒ¨åˆ†å‚è€ƒå®—æˆåº†è€å¸ˆçš„ã€Šç»Ÿè®¡è‡ªç„¶è¯­è¨€å¤„ç†ã€‹ ä¿¡æ¯å¢ç›Šï¼ˆIGï¼‰æ³•ä¾æ®æŸé¡¹ç‰¹å¾ $w_i$ ä¸ºæ•´ä¸ªåˆ†ç±»æ‰€èƒ½æä¾›çš„ä¿¡æ¯é‡çš„å¤šå°‘æ¥è¡¡é‡è¯¥ç‰¹å¾é¡¹çš„é‡è¦ç¨‹åº¦ã€‚æŸä¸ªç‰¹å¾çš„ä¿¡æ¯å¢ç›ŠæŒ‡çš„æ˜¯æœ‰è¯¥ç‰¹å¾å’Œæ²¡æœ‰è¯¥ç‰¹å¾æ—¶ï¼Œä¸ºæ•´ä¸ªåˆ†ç±»æ‰€èƒ½æä¾›çš„ä¿¡æ¯é‡çš„å·®åˆ«ã€‚å…¶ä¸­ï¼Œä¿¡æ¯é‡çš„å¤šå°‘ç”±ç†µæ¥è¡¡é‡ã€‚ å› æ­¤ä¿¡æ¯å¢ç›Šå³ä¸è€ƒè™‘ä»»ä½•ç‰¹å¾æ—¶æ–‡æ¡£çš„ç†µå’Œè€ƒè™‘è¯¥ç‰¹å¾åæ–‡æ¡£çš„ç†µçš„æ’å€¼ï¼š $P(c_i)$ è¡¨ç¤ºè®­ç»ƒæ ·æœ¬ä¸­ $c_i$ ç±»æ–‡æ¡£çš„æ¦‚ç‡ã€‚ $P(w)$ è¡¨ç¤ºè®­ç»ƒæ ·æœ¬ä¸­åŒ…å«ç‰¹å¾wçš„æ–‡æ¡£å æ€»æ–‡æ¡£æ•°çš„æ¦‚ç‡ã€‚å‡è®¾æŸä¸€ä¸ªæ–‡æ¡£ä¸­æœ‰ä¸¤ä¸ªè¯ â€˜greatâ€™,é‚£ä¹ˆéœ€è¦å°†å®ƒæ•°é‡å˜ä¸º1ï¼Œè¿™åœ¨chapter6ä¸­æœ‰è®²åˆ°ã€‚ $P(c_i|w)$ è¡¨ç¤ºæ–‡æ¡£ä¸­åŒ…å«ç‰¹å¾wä¸”ç±»åˆ«ä¸º $c_i$ çš„æ¦‚ç‡ã€‚ åœ¨æèˆªè€å¸ˆçš„ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ä¸­ï¼Œå†³ç­–æ ‘è¿™ä¸€ç« ä¸­ä¹Ÿæœ‰è®²åˆ°ä½¿ç”¨ä¿¡æ¯å¢ç›Šæ¥è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚ $$H(C|w) = P(w)\\sum_{i=1}^CP(c_i|w)logP(c_i|w)$$ è¡¨ç¤ºåœ¨ç‰¹å¾wæ¡ä»¶ä¸‹å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œåˆ†ç±»çš„ä¸ç¡®å®šæ€§ï¼Œä¹Ÿå°±æ˜¯æ¡ä»¶ç†µçš„æœŸæœ›ã€‚ å…¬å¼ï¼ˆ7.23ï¼‰ä¸­ç¬¬ä¸€é¡¹æ˜¯ç»éªŒç†µï¼Œå°±æ˜¯å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œä¸ç¡®å®šæ€§çš„åº¦é‡ã€‚ç¬¬äºŒé¡¹æ˜¯ç»éªŒæ¡ä»¶ç†µï¼Œä¹Ÿå°±æ˜¯åœ¨ç‰¹å¾wç»™å®šçš„æ¡ä»¶ä¸‹å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œåˆ†ç±»çš„ä¸ç¡®å®šæ€§ã€‚ Choosing a classifier and featuresæ˜¾ç„¶logisticå›å½’è¦æ¯”naive bayesè¦å¥½ï¼Œå› ä¸ºnaive bayesä¸­å‡è®¾ç‰¹å¾ $f_1,f_2,â€¦,f$ ç›¸äº’ç‹¬ç«‹ï¼Œå¦‚æœç‰¹å¾ $f_1 å’Œ f_2$ å…·æœ‰ä¸€å®šçš„ç›¸å…³æ€§ï¼Œé‚£ä¹ˆnaive bayeså°±overestimateè¿™ä¸ªç‰¹å¾ã€‚è€Œlogisticç›¸æ¯”ä¹‹ä¸‹å¯¹å…·æœ‰ç›¸å…³æ€§çš„ç‰¹å¾çš„å¤„ç†é²æ£’æ€§è¦å¼ºå¾ˆå¤šï¼Œå¦‚æœ $f_1,f_2$ å®Œå…¨æ­£ç›¸å…³ï¼Œé‚£ä¹ˆä»–ä»¬çš„æƒé‡éƒ½ä¼šèµ‹å€¼å‡å°‘ä¸ºåŸæ¥çš„ 1/2. The overly strong conditional independence assumptions of Naive Bayes mean that if two features are in fact correlated naive Bayes will multiply them both in as if they were independent, overestimating the evidence. Logistic regression is much more robust to correlated features; if two features f 1 and f 2 are perfectly correlated, regression will simply assign half the weight to w 1 and half to w 2 . å½“ç‰¹å¾å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§æ—¶ï¼Œlogisticçš„å‡†ç¡®ç‡è¦é«˜äºNaive bayesã€‚ä½†å½“æ•°æ®é›†è¾ƒå°æ—¶ï¼Œnaive bayesçš„å‡†ç¡®ç‡è¦é«˜äºlogisticå’ŒSVMï¼Œè€Œä¸”naive bayesæ›´å®¹æ˜“è®­ç»ƒã€‚ bias-variance tradeoff åå·®bias è¾ƒé«˜: æ¬ æ‹Ÿåˆ underfitting æ–¹å·®variance è¾ƒé«˜ï¼š è¿‡æ‹Ÿåˆ overfitting å¦‚ä½•é€‰æ‹©å„ç§åˆ†ç±»å™¨classifier: low bias : SVM with polynomial or RBF kernels, downweighting or removing features low variance: naive bayes, add more features feature interactions :ç‰¹å¾å·¥ç¨‹å¾ˆé‡è¦ã€‚ å¸¸è§çš„åˆ†ç±»å™¨æœ‰ï¼šSupport Vector Machines (SVMs) with polynomial or RBF kernels, and random forests. æ€»ç»“ å‚è€ƒï¼š Speech and language Processingï¼ŒChapter7 çŸ¥ä¹ï¼šL1æ­£åˆ™åŒ–ä¸L2æ­£åˆ™åŒ– å®—æˆåº†ï¼Œã€Šç»Ÿè®¡è‡ªç„¶è¯­è¨€å¤„ç†ã€‹ï¼Œç¬¬13ç«  æèˆªï¼Œã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ï¼Œç¬¬5ç« ","link":"/2018/04/16/chapter7-logistic%E5%9B%9E%E5%BD%92/"},{"title":"cs224d lecture16 dynamic Memory network","text":"ä¸»è¦å†…å®¹ï¼š paper:Ask Me Anything: Dynamic Memory Networks for Natural Language Processing æ˜¯å¦æ‰€æœ‰NLPä»»åŠ¡éƒ½å¯è§†ä½œQAï¼Ÿ åœ¨old-school NLPç³»ç»Ÿä¸­ï¼Œå¿…é¡»æ‰‹å·¥æ•´ç†ä¸€ä¸ªâ€œçŸ¥è¯†åº“â€ï¼›ç„¶ååœ¨è¿™ä¸ªçŸ¥è¯†åº“ä¸Šåšè§„åˆ™æ¨æ–­ã€‚è¿™èŠ‚è¯¾ä»‹ç»çš„DMNå®Œå…¨ä¸åŒäºè¿™ç§å°ä½œåŠï¼Œå®ƒèƒ½å¤Ÿç›´æ¥ä»é—®ç­”è¯­æ–™ä¸­å­¦ä¹ æ‰€æœ‰å¿…è¦çš„çŸ¥è¯†è¡¨è¾¾ã€‚ DMNè¿˜å¯ä»¥åœ¨é—®ç­”ä¸­åšæƒ…æ„Ÿåˆ†æã€è¯æ€§æ ‡æ³¨å’Œæœºå™¨ç¿»è¯‘ã€‚ æ‰€ä»¥æ„å»ºä¸€ä¸ªjoint modelç”¨äºé€šç”¨QAæˆä¸ºç»ˆæç›®æ ‡ã€‚ è¦å®ç°è¿™ä¸ªç›®æ ‡ï¼Œæœ‰ä¸¤ä¸ªéšœç¢ã€‚ æ²¡æœ‰ä»»ä½•å·²æœ‰ç ”ç©¶æ¢è®¨è¿‡å¦‚ä½•è®©å•ä¸ªæ¨¡å‹å­¦ä¼šè¿™ä¹ˆå¤šçš„ä»»åŠ¡ã€‚æ¯ç§ä»»åŠ¡éƒ½æœ‰ç‹¬ç‰¹çš„ç‰¹ç‚¹ï¼Œé€‚åˆä¸åŒçš„ç¥ç»ç½‘ç»œæ¥åšï¼š ç¬¬äºŒä¸ªéšœç¢ Fully joint multitask learningï¼ˆåŒä¸€ä¸ªdecoder/classifierï¼Œä¸ä»…ä»…å…±äº«è¯å‘é‡ï¼Œè€Œåº”è¯¥å…±äº«å…¨éƒ¨å‚æ•°ï¼‰éå¸¸å›°éš¾ã€‚ æœ‰äº›ä¸æˆåŠŸçš„ç ”ç©¶å‘ç°ï¼Œåªèƒ½åœ¨ä½å±‚ï¼ˆè¯å‘é‡ï¼‰å…±äº«å‚æ•°ã€å¦‚æœä»»åŠ¡ä¹‹é—´æ²¡æœ‰ç›´æ¥å…³è”åˆ™ä¼šé¡¾æ­¤å¤±å½¼ã€‚ æ„Ÿè§‰å°±æ˜¯è¿ç§»å­¦ä¹ æ— æ³•åº”ç”¨äº nlp ä¸Šï¼ŒåŒæ—¶åº”åˆ°åˆ°ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šï¼Œç„¶åå¯¹ç›¸åŒçš„å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œå¾€å¾€ä¼šå¾—åˆ°å¾ˆå·®çš„æ•ˆæœã€‚ So if youâ€™re trying to train two tasks together in one model, say you just have two softmaxes on the same Hidden state of your LSTMs. It turns to actually get worse in many cases, too. Dynamic Memory Networksä»Šå¤©ä»‹ç»çš„DMNä»…ä»…è§£å†³äº†ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚è™½ç„¶æœ‰äº›è¶…å‚æ•°è¿˜æ˜¯å¾—å› ä»»åŠ¡è€Œå¼‚ï¼Œä½†æ€»ç®—æ˜¯ä¸ªé€šç”¨çš„æ¶æ„äº†ã€‚ å›ç­”ç‰¹åˆ«éš¾çš„é—®é¢˜ ä½ æ— æ³•è®°ä½å…¨æ–‡ï¼Œä½†çœ‹äº†é—®é¢˜ä¹‹åï¼Œåªè¦å¸¦ç€é—®é¢˜æ‰«å‡ çœ¼åŸæ–‡ï¼Œä½ å°±èƒ½æ‰¾å‡ºç­”æ¡ˆã€‚ è¿™ç§ç°è±¡å¯å‘äº†DMNã€‚ Dynamic Memory Networks å…ˆæ¥çœ‹big pictureï¼ˆæ¥ä¸‹æ¥ä¼šå¯¹æ¯ä¸ªæ¨¡å—å•ç‹¬è®²è§£ï¼‰ï¼š ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªmoduleï¼š semantic memory module: è¯å‘é‡ input moduleï¼š ä½¿ç”¨GRUæˆ–LSTMå¯¹åŸæ–‡è¿›è¡Œencoderï¼Œæ¯ä¸€ä¸ªwordå¯¹åº”ä¸€ä¸ªhidden vectorï¼ŒåŒquestion mudule å…±äº« GRU çš„æƒé‡å‚æ•° Question Module å’Œ Episodoc memory Moduleï¼š è®¡ç®—å‡ºä¸€ä¸ªQuestion Vector qï¼Œæ ¹æ®qåº”ç”¨attentionæœºåˆ¶ï¼Œå›é¡¾inputçš„ä¸åŒæ—¶åˆ»ã€‚æ ¹æ®attentionå¼ºåº¦çš„ä¸åŒï¼Œå¿½ç•¥äº†ä¸€äº›inputï¼Œè€Œæ³¨æ„åˆ°å¦ä¸€äº›inputã€‚è¿™äº›inputè¿›å…¥Episodic Memory Moduleï¼Œæ³¨æ„åˆ°é—®é¢˜æ˜¯å…³äºè¶³çƒä½ç½®çš„ï¼Œé‚£ä¹ˆæ‰€æœ‰ä¸è¶³çƒåŠä½ç½®çš„inputè¢«é€å…¥è¯¥æ¨¡å—ã€‚è¯¥æ¨¡å—æ¯ä¸ªéšè—çŠ¶æ€è¾“å…¥Answer moduleï¼Œsoftmaxå¾—åˆ°ç­”æ¡ˆåºåˆ—ã€‚ attention çš„è¿‡ç¨‹å®é™…ä¸Šæ˜¯ä¸€ä¸ª transitive reasoning ä¼ é€’å…³ç³»çš„è¿‡ç¨‹ï¼š æ¯”å¦‚ä¸Šå›¾ä¸­ï¼Œé—®é¢˜æ˜¯å…³äºfootballçš„è¯­ä¹‰ question vector qï¼Œé‚£ä¹ˆå¸¦ç€ q å›é¡¾ä¸€éåŸæ–‡ï¼Œæ‰¾åˆ° John put down the footballï¼Œ å¾—åˆ°Episodic memory modeluçš„è¾“å‡º $m^1$, ç„¶åå¸¦ç€q å’Œ $m_1$ å†å›é¡¾ä¸€æ¬¡åŸæ–‡ï¼Œç„¶ååˆæ‰¾åˆ° John moved to the bedroom å’Œ John went to the hallwayï¼Œ è®¡ç®—å¾—åˆ° $m^2$. å…·ä½“æ€ä¹ˆå®ç°çœ‹æ¥ä¸‹æ¥å…·ä½“æ¨¡å—çš„è®²è§£ã€‚ input Module è¾“å…¥æ¨¡å—æ¥å— $T_I$ ä¸ªè¾“å…¥å•è¯ï¼Œè¾“å‡º $T_C$ ä¸ªâ€œäº‹å®â€çš„è¡¨ç¤ºã€‚å¦‚æœè¾“å‡ºæ˜¯ä¸€ç³»åˆ—è¯è¯­ï¼Œé‚£ä¹ˆæœ‰ $T_C=T_I$ï¼›å¦‚æœè¾“å‡ºæ˜¯ä¸€ç³»åˆ—å¥å­ï¼Œé‚£ä¹ˆçº¦å®š $T_C$ è¡¨ç¤ºå¥å­çš„æ•°é‡ï¼Œ$T_I$ è¡¨ç¤ºå¥å­ä¸­å•è¯çš„æ•°é‡ã€‚æˆ‘ä»¬ä½¿ç”¨ç®€å•çš„GRUè¯»å…¥å¥å­ï¼Œå¾—åˆ°éšè—çŠ¶æ€ $h_t=GRU(x_t,h_{tâˆ’1})$ï¼Œå…¶ä¸­ $x_t=L[w_t]$ï¼ŒLæ˜¯embedding matrixï¼Œ$w_t$ æ˜¯æ—¶åˆ» t çš„è¯è¯­ã€‚ äº‹å®ä¸Šï¼Œè¿˜å¯ä»¥å°†è¿™ä¸ªUni-GRUå‡çº§ä¸ºBi-GRUï¼š Question Module Episodic Memory Mudule $$h_i^t=g_i^tGRU(s_i,h_{i-1}^t)+(1-g_i^t)h_{i-1}^t$$ å…¶ä¸­: $g_i^t$ is just a single scalar number. Should I pay attention to this sentence.ä¹Ÿç›¸å½“äºä¸€ä¸ªgateæœºåˆ¶ï¼Œå½“ $g_i^t=0$ æ—¶è¡¨ç¤ºä¸ $s_i$ æ— å…³ã€‚ ä¸Šæ ‡tè¡¨ç¤º $t^{th}$ time that we went over the entire input. å¦‚ä½•è®¡ç®— $g_i^t$ ï¼Œä¹Ÿå°±æ˜¯æ€ä¹ˆåˆ¤æ–­å½“å‰è¿­ä»£ä¸inputä¸­çš„æ¯ä¸ªsentenceæ˜¯å¦ç›¸å…³ã€‚ ç›¸å½“ç®€å•å’Œç›´æ¥ï¼š sentence similarity: element-wise product or subtraction of sentence vector. è®¡ç®—sentenceç›¸ä¼¼æ€§ï¼š $$z_i^t=[s_i\\circ q; s_i\\circ c^{t-1}; |s_i-q|; |s_i-m^{t-1}|]$$ ä¸€ä¸ªåŒå±‚neural network: $$Z_i^t = W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)}$$ softmaxè®¡ç®—å½“å‰è¿­ä»£æ¬¡æ•°ä¸‹æ¯ä¸ªsentenceæ‰€å çš„æ¯”é‡ï¼š $$g_i^t=\\dfrac{exp(Z_i^t)}{\\sum_{k=1}^{M_i}exp(Z_i^t)}$$ Answer Module ç›¸å…³å·¥ä½œæœ‰å¾ˆå¤šå·²æœ‰å·¥ä½œåšäº†ç±»ä¼¼ç ”ç©¶ï¼š Sequence to Sequence (Sutskever et al. 2014) Neural Turing Machines (Graves et al. 2014) Teaching Machines to Read and Comprehend (Hermann et al. 2015) Learning to Transduce with Unbounded Memory (Grefenstette 2015) Structured Memory for Neural Turing Machines (Wei Zhang 2015) Memory Networks (Weston et al. 2015) End to end memory networks (Sukhbaatar et al. 2015) åŒ MemNet å¯¹æ¯” ç›¸åŒç‚¹ éƒ½æœ‰input, scoring, attention and responseæ¨¡å— ä¸åŒç‚¹ MemNetså¯¹äºinput representations ä½¿ç”¨è¯è¢‹ï¼Œç„¶åæœ‰ä¸€äº›embeddingå»encodeä½ç½®ï¼Œ DMN ä½¿ç”¨ GRU MemNetsè¿­ä»£è¿è¡Œattentionå’Œresponse è¿™äº›ä¸åŒç‚¹éƒ½æ˜¯ç”±äºMemNetsæ˜¯ä¸ªésequenceæ¨¡å‹é€ æˆçš„ã€‚è€ŒDMNæ˜¯ä¸ªè¡€ç»Ÿçº¯æ­£çš„neural sequence modelï¼Œå¤©ç„¶é€‚åˆåºåˆ—æ ‡æ³¨ç­‰ä»»åŠ¡ï¼Œæ¯”MemNetsåº”ç”¨èŒƒå›´æ›´å¹¿ã€‚ DMNçš„sequenceèƒ½åŠ›æ¥è‡ªGRUï¼Œè™½ç„¶ä¸€å¼€å§‹ç”¨çš„æ˜¯LSTMï¼Œåæ¥å‘ç°GRUä¹Ÿèƒ½è¾¾åˆ°ç›¸åŒçš„æ•ˆæœï¼Œè€Œä¸”å‚æ•°æ›´å°‘ã€‚ï¼ˆè¿™å›ç­”äº†GRUå’ŒLSTMé‚£èŠ‚è¯¾æœ‰ä¸ªå­¦ç”Ÿçš„é—®é¢˜ï¼šå“ªä¸ªè®¡ç®—å¤æ‚åº¦æ›´ä½ã€‚Manningå½“æ—¶å›ç­”åº”è¯¥æ˜¯ä¸€æ ·çš„ï¼Œè¿˜ä¸å¤ªç›¸ä¿¡Richardçš„ç­”æ¡ˆã€‚è¯´æ˜åœ¨å·¥ç¨‹ä¸Šï¼Œè¿˜æ˜¯åšå®éªŒçš„ä¸€çº¿åšå£«æ›´æœ‰ç»éªŒï¼‰ Evaluation è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„QAè¯­æ–™åº“ï¼Œé‡Œé¢éƒ½æ˜¯ä¸€äº›ç®€å•çš„é—®ç­”ã€‚éƒ¨åˆ†NLPå­¦è€…å¾ˆåŒæ¶æœºå™¨ç”Ÿæˆçš„è¯­æ–™ï¼Œä½†å¦‚æœè¿æœºå™¨ç”Ÿæˆçš„è¯­æ–™éƒ½æ— æ³•è§£å†³ï¼Œä½•è°ˆè§£å†³çœŸå®çš„å¤æ‚é—®é¢˜ã€‚ æƒ…æ„Ÿåˆ†æ ä¾ç„¶æ‹¿åˆ°æœ€é«˜åˆ†æ•°ã€‚ æ­¤æ—¶é—®é¢˜æ°¸è¿œæ˜¯ç›¸åŒçš„ï¼Œå…¶å‘é‡æ˜¯å›ºå®šçš„ã€‚ é—æ†¾çš„æ˜¯ï¼Œå¯¹äºä¸åŒçš„ä»»åŠ¡ï¼Œè¶…å‚æ•°ä¾ç„¶å¿…é¡»ä¸åŒæ‰èƒ½æ‹¿åˆ°æœ€ä½³ç»“æœã€‚ Episodesæ•°é‡ï¼š å…¶ä¸­task3æ˜¯ä¸‰æ®µè®ºï¼Œç†è®ºåªéœ€è¦3ä¸ªpassï¼Œä½†æ¨¡å‹ä¾ç„¶éœ€è¦5ä¸ªã€‚è€ƒè™‘åˆ°è¿™æ˜¯ä¸ªend to endè®­ç»ƒï¼Œæ²¡æœ‰ç›‘ç£ä¿¡å·æŒ‡ç¤ºé‚£äº›factæ˜¯é‡è¦çš„ï¼Œæ‰€ä»¥è¿™ä¸ªè¡¨ç°è¿˜æŒºå¥½ã€‚æƒ…æ„Ÿåˆ†æçš„NAæ˜¯å› ä¸ºï¼Œè®¡ç®—å¤æ‚åº¦å®åœ¨å¤ªé«˜äº†ã€‚åˆ†æ•°å·²ç»åœ¨é™ä½ï¼Œæ‰€ä»¥å¹²è„†æ²¡è·‘ã€‚ æƒ…æ„Ÿåˆ†æçš„ä¸€äº›ä¾‹å­ VQA vision question answeringEverthing is Question Answering, è¿™ä¹Ÿå¤ªé…·äº†å§ï½ input module ä¸å¤ªä¸€æ ·ï¼Œè¿™é‡Œæ˜¯é€šè¿‡CNNæå–ç‰¹å¾ï¼Œå°†å›¾åƒä¸­çš„æ¯ä¸€å—åŒºåŸŸç”¨å‘é‡è¡¨ç¤ºï¼Œç„¶åä½œä¸ºGRUçš„è¾“å…¥ã€‚ç”±äºå·ç§¯ç‰¹å¾å¹¶ä¸æ˜¯åºåˆ—çš„ï¼Œæ‰€ä»¥è¾“å…¥æ¨¡å—çš„è¾“å‡ºç‰¹å¾åªæ˜¯æ‰€æœ‰æ—¶åˆ»éšè—çŠ¶æ€å‘é‡çš„æ‹¼æ¥ã€‚ æ˜¾ç„¶è¿™éœ€è¦å¾ˆå¥½çš„æ•°æ®é›†å•Šã€‚ã€‚ è¿™å°±çœŸçš„å¾ˆåŠäº†ï½ï¼ æœ€é«˜åˆ†ã€‚ã€‚ æ€»ç»“ å‚è€ƒ hankcsçš„åšå®¢CS224nç¬”è®°16 DMNä¸é—®ç­”ç³»ç»Ÿ","link":"/2018/05/21/cs224d-lecture16-dynamic-neural-network/"},{"title":"ctc loss and decoder","text":"æƒ³è¦è¯¦ç»†çš„äº†è§£ctc lossï¼Œå»ºè®®ç›´æ¥è½¬è‡³å¤§ä½¬çš„åšå®¢ $\\rightarrow$ https://xiaodu.io/ctc-explained/. æˆ‘ä¸ªäººå†™è¿™ä¸ªç¬”è®°çš„ç›®çš„åœ¨äºæœ€è¿‘è¦å¯¹ctc lossè¿›è¡Œé­”æ”¹æ—¶ï¼Œå‘ç°ä¹‹å‰çš„ç»†èŠ‚éƒ½å¿˜äº†ã€‚æ‰€ä»¥æŒ‰ç…§è‡ªå·±å·²æœ‰çš„åŸºç¡€ä¸Šé‡æ–°æ•´ç†äº†ä¸€éã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¤§ä½¬çš„åšå®¢é‡Œé¢å¹¶æ²¡æœ‰ä»£ç è§£æã€‚è¿™é‡Œæœ‰ctc loss å’Œ ctc decodeçš„pythonä»£ç å®ç°ï¼Œæ‰€ä»¥æƒ³è¦å¯¹ctc lossè¿›è¡Œé­”æ”¹çš„ï¼Œå¯ä»¥å†è¿‡ä¸€éæˆ‘è¿™ç¯‡æ–‡ç« ~Why ctc loss, ctc loss vs cross entropy ç°å®ä¸­æœ‰å¾ˆå¤šä»»åŠ¡éƒ½å¯ä»¥çœ‹ä½œæ˜¯åºåˆ—åˆ°åºåˆ—çš„å¯¹é½è®­ç»ƒã€‚ä¸»è¦å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼š NLPé¢†åŸŸå¸¸è§çš„æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ã€‚å¯¹äºè¿™ç±»ä»»åŠ¡ï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨cross-entropy + teacher forcingæ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ç±»ä»»åŠ¡çš„ç‰¹ç‚¹æ˜¯æºåºåˆ—å’Œç›®æ ‡åºåˆ—æ²¡æœ‰ä¸¥æ ¼çš„å¯¹é½å…³ç³»ã€‚ä»–ä»¬æœ¬è´¨ä¸Šå¯ä»¥çœ‹ä½œæ˜¯ conditional language model. ä¹Ÿå°±æ˜¯ç›®æ ‡åºåˆ—ä½œä¸ºæ¡ä»¶è¯­è¨€æ¨¡å‹ï¼Œæ›´çœ‹é‡è¿è´¯æ€§ã€æµåˆ©åº¦ï¼Œå…¶æ¬¡æ˜¯ä¸æºåºåˆ—çš„å¯¹åº”å…³ç³»ï¼ˆæ‰€ä»¥ä»–ä»¬ä¼šæœ‰å¤šæ ·æ€§ç ”ç©¶ï¼‰ã€‚ è¯†åˆ«é¢†åŸŸå¸¸è§çš„è¯­éŸ³è¯†åˆ«ï¼ŒOCRï¼Œæ‰‹è¯­è¯†åˆ«ã€‚å¯¹äºè¿™ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ™ä¸»è¦ä½¿ç”¨ctc lossä½œä¸ºæŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ç±»ä»»åŠ¡çš„ç‰¹ç‚¹æ˜¯æºåºåˆ—å’Œç›®æ ‡åºåˆ—æœ‰ä¸¥æ ¼çš„å¯¹é½å…³ç³»ã€‚å¯¹äºè¯­éŸ³æˆ–æ‰‹è¯­ï¼Œç›®æ ‡åºåˆ—æœ‰è¯­è¨€æ¨¡å‹çš„ç‰¹ç‚¹ï¼Œä½†æ˜¯æ›´çœ‹é‡ä¸æºåºåˆ—çš„å‡†ç¡®çš„å¯¹åº”å…³ç³»ã€‚ç¬¬äºŒç±»ä»»åŠ¡å…¶å®ä¹Ÿå¯ä»¥ç”¨cross entropyæ¥è®­ç»ƒï¼Œä½†æ˜¯å¾€å¾€æ•ˆæœä¸å¤ªå¥½ã€‚æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå¯¹äºç¬¬äºŒç±»ä»»åŠ¡ï¼Œæœ€ç†æƒ³çš„æƒ…å†µæ˜¯å°†æºåºåˆ—å…ˆè¿›è¡Œåˆ†å‰²ï¼Œè¿™æ ·å•ç‹¬çš„å¯¹æŸä¸€ä¸ªéŸ³èŠ‚ï¼Œæ‰‹è¯­æˆ–è€…å­—ç¬¦è¿›è¡Œè¯†åˆ«ï¼Œå‡†ç¡®ç‡å°±ä¼šå¾ˆé«˜äº†ã€‚ä½†æ˜¯ç°å®æ˜¯ï¼Œæºåºåˆ—æ›´å¤šçš„æ˜¯æœªåˆ†å‰²çš„æƒ…å†µã€‚é’ˆå¯¹è¿™ç±»ä»»åŠ¡ï¼Œ[Alex Graves, 2006] æå‡ºäº†Connectionist Temporal Classification. ä½¿ç”¨ctcè¿›è¡Œè®­ç»ƒæœ‰ä¸¤ä¸ªè¦æ±‚ï¼š æºåºåˆ—é•¿åº¦ &gt;&gt; ç›®æ ‡åºåˆ—é•¿åº¦ æºåºåˆ—çš„orderä¸ç›®æ ‡åºåˆ—çš„orderä¸€è‡´ï¼Œä¸”å­˜åœ¨é¡ºåºå¯¹é½çš„å…³ç³» ctc trainingå¦‚ä½•è®¡ç®— ctc loss, è¿™ç¯‡åšå®¢CTC Algorithm Explained Part 1å†™çš„éå¸¸éå¸¸èµ(ä»¥ä¸‹ç®€ç§°ä¸ºctc explain blog)ï¼Œç»†èŠ‚çœ‹è¿™ä¸ªå°±å¥½äº†ã€‚ è¿™é‡Œç®€å•çš„æ¦‚æ‹¬ä¸‹æ€æƒ³ã€‚ ç»™å®šæºåºåˆ— $X={x_1,x_2,..,x_T}$ å’Œç›®æ ‡åºåˆ— $Y={y_1,..y_N}$ . å…¶ä¸­ $T&gt;&gt;N$.æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡åŸç†ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿å¾— P(Y|X;W) æœ€å¤§åŒ–çš„Wã€‚è¿™é‡Œ X,Y å­˜åœ¨å¤šå¯¹ä¸€çš„å…³ç³»ã€‚æˆ‘ä»¬å‡è®¾å­˜åœ¨è¿™æ ·çš„è·¯å¾„ $\\pi={\\pi_1, â€¦,\\pi_T}$, $\\pi_i\\in |Vâ€™=V+blank|$ ä¸æºåºåˆ—ä¸€ä¸€å¯¹åº”ï¼ˆVæ˜¯è¯è¡¨ï¼‰ã€‚å¹¶ä¸”å­˜åœ¨è¿™æ ·çš„æ˜ å°„å…³ç³» $\\beta(\\pi) = Y$ , å…¶æ˜ å°„æ³•åˆ™å°±æ˜¯å»é‡å’Œå»æ‰blankï¼ˆè¿™ä¸ªæ˜ å°„æ³•åˆ™æœ‰å…¶ç‰©ç†æ„ä¹‰ï¼šå°±æ˜¯ä¸€ä¸ªéŸ³èŠ‚æˆ–æ‰‹è¯­åŠ¨ä½œä¼šåŒ…å«å¤šå¸§ï¼Œä»¥åŠå­˜åœ¨ä¸­é—´åœé¡¿æˆ–æ— æ„ä¹‰å¸§ç­‰æƒ…å†µ.ï¼‰ å› æ­¤ï¼Œä¼˜åŒ–çš„ç›®æ ‡æ¨¡å‹å¯ä»¥è½¬æ¢ä¸ºï¼š$P(Y|X)=\\sum_{\\pi\\in \\beta^{-1}(Y)}P(\\pi|X;W)$ æ‰€ä»¥æˆ‘ä»¬çš„ç›®æ ‡ç°åœ¨è½¬æ¢æˆäº†æœ€å¤§åŒ–æ»¡è¶³ $\\pi\\in \\beta^{-1}(Y)$ çš„æ‰€æœ‰è·¯å¾„çš„æ¦‚ç‡ã€‚ç°åœ¨é—®é¢˜å°±è½¬å˜æˆå¦‚ä½•æ‰¾åˆ° Y å¯¹åº”çš„æ‰€æœ‰è·¯å¾„ã€‚è¿™æ˜¯ä¸€ä¸ªåŠ¨æ€è§„åˆ’çš„é—®é¢˜ã€‚Graves æ ¹æ®HMMçš„å‰å‘åå‘ç®—æ³•ï¼Œåˆ©ç”¨åŠ¨æ€è§„åˆ’çš„æ–¹æ³•æ¥æ±‚è§£ã€‚æ ¹æ®ç›®æ ‡åºåˆ—Yå’ŒXçš„å¸§æ•°æ„å»ºä¸€ä¸ªè¡¨æ ¼ï¼š çºµè½´æ˜¯å°†ç›®æ ‡åºåˆ—æ‰©å±•ä¸ºå‰åéƒ½æœ‰blankçš„åºåˆ— $lâ€™=(-, l_1, -, l_2,â€¦,- ,l_N, -)$ ã€‚å¦‚æœ T=N æ—¶ï¼Œé‚£ä¹ˆYå’ŒXå°±æ˜¯ä¸€ä¸€å¯¹åº”äº†ã€‚Xçš„åºåˆ—è¶Šé•¿ï¼Œè¿™ä¸ªè¡¨æ ¼çš„æœç´¢ç©ºé—´è¶Šå¤§ï¼Œå­˜åœ¨çš„å¯èƒ½çš„è·¯å¾„å°±è¶Šå¤šã€‚ å¦‚ä½•æ‰¾åˆ°æ‰€æœ‰çš„åˆæ³•è·¯å¾„ï¼Œå…ˆå®šä¹‰è·¯å¾„è§„åˆ™ï¼Œç„¶åæ‰¾åˆ°é€’å½’æ¡ä»¶ä¾¿èƒ½é€šè¿‡åŠ¨æ€è§„åˆ’çš„æ–¹æ³•è§£å†³ï¼Œå…·ä½“ç»†èŠ‚å‚è§ctc explain blog. è·¯å¾„è§„åˆ™ï¼š è½¬æ¢åªèƒ½å¾€å³ä¸‹æ–¹å‘ï¼Œå…¶ä»–æ–¹å‘ä¸å…è®¸ ç›¸åŒçš„å­—ç¬¦ä¹‹é—´è‡³å°‘æœ‰ä¸€ä¸ªç©ºæ ¼ éç©ºå­—ç¬¦ä¸èƒ½è¢«è·³è¿‡ï¼ˆä¸ç„¶æœ€ç»ˆå°±ä¸æ˜¯appleäº† èµ·ç‚¹å¿…é¡»ä»å‰ä¸¤ä¸ªå­—ç¬¦å¼€å§‹ é‡ç‚¹å¿…é¡»è½åœ¨ç»“å°¾ä¸¤ä¸ªå­—ç¬¦ è¿™é‡Œä»¥å‰å‘ç®—æ³•ä¸ºä¾‹æ¥è§£é‡Šï¼š å…¶ä¸­çš„ç¬¦å·ï¼šxè¡¨ç¤ºè¾“å…¥åºåˆ—ï¼Œzè¡¨ç¤ºè¾“å‡ºåºåˆ—ï¼Œsè¡¨ç¤ºçºµè½´çš„èŠ‚ç‚¹(2T+1ä¸ª)åˆå§‹æ¡ä»¶ï¼Œ t=1æ—¶åˆ»åªèƒ½æ˜¯ blank æˆ– $l^{â€˜}_ {2}$ $\\alpha_1(1)=y_{-}^1$ è¡¨ç¤º t=1 æ—¶åˆ»ä¸ºblankçš„æ¦‚ç‡. $\\alpha_1(2)=y_{l_2â€™}^1$ è¡¨ç¤º t=1 æ—¶åˆ»ä¸ºsä¸­çš„ç¬¬äºŒä¸ªèŠ‚ç‚¹ s_2 çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯è¾“å‡ºåºåˆ—çš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ $\\alpha_1(s)=0, \\forall s&gt;2$ è¡¨ç¤ºt=1æ—¶åˆ»å…¶ä»–èŠ‚ç‚¹æ¦‚ç‡ä¸º0 $\\alpha_t(s)=0, \\forall s &lt; |lâ€™|-2(T-t)-1$ å¯¹äºä»»ä½•æ—¶åˆ»éƒ½æœ‰éƒ¨åˆ†èŠ‚ç‚¹æ˜¯å®Œå…¨ä¸å¯èƒ½çš„ t=T æ—¶åˆ»ï¼Œåªæœ‰æœ€åä¸¤ä¸ªèŠ‚ç‚¹å¯è¡Œã€‚ t=0 æ—¶åˆ»ï¼Œå¯¹äºèŠ‚ç‚¹ $s&lt;|lâ€™|-2T -1(|lâ€™|=2N+1)$ . å…¶æ¦‚ç‡ä¸º 0. å¦‚æœè¾“å…¥åºåˆ—çš„é•¿åº¦ T=Nï¼ˆä¸labelç­‰é•¿ï¼‰ï¼Œåˆ™ $|lâ€™|=2T+1$ . ä¸€èˆ¬æƒ…å†µä¸‹ T&gt;&gt;Nï¼Œs&lt;0 0&lt;t&lt;T æ—¶åˆ»ï¼Œä»¥ç‰¹ä¾‹ T=N ä¸ºä¾‹ï¼Œ $s&lt;2N+1-2N+2t -1 \\rightarrow s&lt;2t$ . ä¹Ÿå°±æ˜¯ s&lt;2t çš„èŠ‚ç‚¹æ¦‚ç‡éƒ½ä¸º0. å‰å‘é€’æ¨å…¬å¼ï¼Œå¦‚æœ t æ—¶åˆ»ä¸º s èŠ‚ç‚¹ï¼Œé‚£ä¹ˆ t-1 æ—¶åˆ»å¯èƒ½çš„èŠ‚ç‚¹ä¸ s èŠ‚ç‚¹æ˜¯å¦ä¸º blank æœ‰å…³ã€‚ å¦‚æœsèŠ‚ç‚¹ä¸ºblank. ä¸èƒ½è·³è¿‡éç©ºå­—ç¬¦ï¼Œæ‰€ä»¥ $\\alpha_t$ ä»…ä¾èµ– $\\alpha_{t-1}(s)$, $\\alpha_{t-1}(s-1)$ , ä¸ä¾èµ–äº $\\alpha_{t-1}(s-2)$ å¦‚æœ s = s-2. ç›¸åŒå­—ç¬¦ä¹‹é—´å¿…é¡»æœ‰ç©ºæ ¼ã€‚å…¬å¼åŒä¸Šã€‚ ä¸å±äºä¸Šè¿°ä¸¤ç§æƒ…å†µï¼Œ $\\alpha_t$ ä¹Ÿèƒ½ä¾èµ–äº $\\alpha_{t-1}(s-2)$ æœ€ç»ˆé€šè¿‡å…¬å¼è®¡ç®—loss, é€šè¿‡è¿­ä»£çš„æ–¹æ³•è®¡ç®—Tæ—¶åˆ»æœ€åä¸¤ä¸ªèŠ‚ç‚¹çš„æ¦‚ç‡:$-ln(p(l|x)) = -ln(\\alpha_T(|lâ€™|) + \\alpha_T(|lâ€™|-1))$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232ä»£ç è§£æimport numpy as npfrom six.moves import xrangeimport numpy as npimport editDistance as edimport heapq as hqfrom six.moves import xrangedef ctc_loss(params, seq, blank=0, is_prob=True): &quot;&quot;&quot; params: [vocab_size, T], logits.softmax(-1). T æ˜¯è¾“å…¥åºåˆ—çš„é•¿åº¦ï¼Œvocab_sizeæ˜¯è¯è¡¨å¤§å°ã€‚ seq: [seq_len] è¾“å‡ºåºåˆ—çš„é•¿åº¦ã€‚ CTC loss function. params - n x m matrix of n-D probability distributions over m frames. seq - sequence of phone id's for given example. is_prob - whether params have already passed through a softmax Returns objective and gradient. &quot;&quot;&quot; seqLen = seq.shape[0] # Length of label sequence (# phones) numphones = params.shape[0] # Number of labels L = 2 * seqLen + 1 # Length of label sequence with blanks, æ‹“å±•åçš„ l'. T = params.shape[1] # Length of utterance (time) # å»ºç«‹è¡¨æ ¼ l' x T. alphas = np.zeros((L, T)) # å‰å‘æ¦‚ç‡ betas = np.zeros((L, T)) # åå‘æ¦‚ç‡ # è¿™é‡Œdpçš„mapï¼š # æ¨ªè½´ä¸º 2*seq_len+1, ä¹Ÿå°±æ˜¯ ground truth labelä¸­æ¯ä¸ªtokenå‰åæ’å…¥ blank # çºµè½´æ˜¯ T frames # logits è½¬æ¢ä¸ºæ¦‚ç‡ if not is_prob: # if not probs, params is logits without softmax. params = params - np.max(params, axis=0) params = np.exp(params) params = params / np.sum(params, axis=0) # Initialize alphas and forward pass # åˆå§‹æ¡ä»¶ï¼šT=0æ—¶ï¼Œåªèƒ½ä¸º blank æˆ– seq[0] alphas[0, 0] = params[blank, 0] alphas[1, 0] = params[seq[0], 0] # T=0ï¼Œ alpha[:, 0] å…¶ä»–çš„å…¨éƒ¨ä¸º 0 c = np.sum(alphas[:, 0]) alphas[:, 0] = alphas[:, 0] / c # è¿™é‡Œ T=0 æ—¶åˆ»æ‰€æœ‰å¯èƒ½èŠ‚ç‚¹çš„æ¦‚ç‡è¦å½’ä¸€åŒ– llForward = np.log(c) # è½¬æ¢ä¸ºlogåŸŸ for t in xrange(1, T): # ç¬¬ä¸€ä¸ªå¾ªç¯ï¼š è®¡ç®—æ¯ä¸ªæ—¶åˆ»æ‰€æœ‰å¯èƒ½èŠ‚ç‚¹çš„æ¦‚ç‡å’Œ start = max(0, L - 2 * (T - t)) # å¯¹äºæ—¶åˆ» t, å…¶å¯èƒ½çš„èŠ‚ç‚¹.ä¸å…¬å¼2ä¸€è‡´ã€‚ end = min(2 * t + 2, L) # å¯¹äºæ—¶åˆ» tï¼Œæœ€å¤§èŠ‚ç‚¹èŒƒå›´ä¸å¯èƒ½è¶…è¿‡ 2t+2 for s in xrange(start, L): l = (s - 1) / 2 # blankï¼ŒèŠ‚ç‚¹såœ¨å¶æ•°ä½ç½®ï¼Œæ„å‘³ç€sä¸ºblank if s % 2 == 0: if s == 0: # åˆå§‹ä½ç½®ï¼Œå•ç‹¬è®¨è®º alphas[s, t] = alphas[s, t - 1] * params[blank, t] else: alphas[s, t] = (alphas[s, t - 1] + alphas[s - 1, t - 1]) * params[blank, t] # sä¸ºå¥‡æ•°ï¼Œéç©º # l = (s-1/2) å°±æ˜¯ s æ‰€å¯¹åº”çš„ lable ä¸­çš„å­—ç¬¦ã€‚ # ((s-2)-1)/2 = (s-1)/2-1 = l-1 å°±æ˜¯ s-2 å¯¹åº”çš„lableä¸­çš„å­—ç¬¦ elif s == 1 or seq[l] == seq[l - 1]: alphas[s, t] = (alphas[s, t - 1] + alphas[s - 1, t - 1]) * params[seq[l], t] else: alphas[s, t] = (alphas[s, t - 1] + alphas[s - 1, t - 1] + alphas[s - 2, t - 1]) \\ * params[seq[l], t] # normalize at current time (prevent underflow) c = np.sum(alphas[start:end, t]) alphas[start:end, t] = alphas[start:end, t] / c llForward += np.log(c) return llForwardctc_beam_search è§£ç def ctc_beam_search_decode(probs, beam_size=5, blank=0): &quot;&quot;&quot; :param probs: The output probabilities (e.g. post-softmax) for each time step. Should be an array of shape (time x output dim). :param beam: :param blank: :return: &quot;&quot;&quot; # Tè¡¨ç¤ºæ—¶é—´ï¼ŒSè¡¨ç¤ºè¯è¡¨å¤§å° T, S = probs.shape # æ±‚æ¦‚ç‡çš„å¯¹æ•° probs = np.log(probs) # Elements in the beam are (prefix, (p_blank, p_no_blank)) # Initialize the beam with the empty sequence, a probability of # 1 for ending in blank and zero for ending in non-blank # (in log space). # æ¯æ¬¡æ€»æ˜¯ä¿ç•™beam_sizeæ¡è·¯å¾„ beam = [(tuple(), ((0.0, NEG_INF), tuple()))] for t in range(T): # Loop over time # A default dictionary to store the next step candidates. next_beam = make_new_beam() for s in range(S): # Loop over vocab # print(s) p = probs[t, s] # tæ—¶åˆ»ï¼Œç¬¦å·ä¸ºsçš„æ¦‚ç‡ # The variables p_b and p_nb are respectively the # probabilities for the prefix given that it ends in a # blank and does not end in a blank at this time step. for prefix, ((p_b, p_nb), prefix_p) in beam: # Loop over beam # p_bè¡¨ç¤ºå‰ç¼€æœ€åä¸€ä¸ªæ˜¯blankçš„æ¦‚ç‡ï¼Œp_nbæ˜¯å‰ç¼€æœ€åä¸€ä¸ªéblankçš„æ¦‚ç‡ # If we propose a blank the prefix doesn't change. # Only the probability of ending in blank gets updated. if s == blank: # å¢åŠ çš„å­—æ¯æ˜¯blank # å…ˆå–å‡ºå¯¹åº”prefixçš„ä¸¤ä¸ªæ¦‚ç‡ï¼Œç„¶åæ›´åç¼€ä¸ºblankçš„æ¦‚ç‡n_p_b (n_p_b, n_p_nb), _ = next_beam[prefix] # -inf, -inf n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p) # æ›´æ–°åç¼€ä¸ºblankçš„æ¦‚ç‡ next_beam[prefix] = ((n_p_b, n_p_nb), prefix_p) # s=blankï¼Œ prefixä¸æ›´æ–°ï¼Œå› ä¸ºblankè¦å»æ‰çš„ã€‚ # print(next_beam[prefix]) continue # Extend the prefix by the new character s and add it to # the beam. Only the probability of not ending in blank # gets updated. end_t = prefix[-1] if prefix else None n_prefix = prefix + (s,) # æ›´æ–° prefix, å®ƒæ˜¯ä¸€ä¸ªtuple n_prefix_p = prefix_p + (p,) # å…ˆå–å‡ºå¯¹åº” n_prefix çš„ä¸¤ä¸ªæ¦‚ç‡, è¿™ä¸ªæ˜¯æ›´æ–°äº†blankæ¦‚ç‡ä¹‹åçš„ new æ¦‚ç‡ (n_p_b, n_p_nb), _ = next_beam[n_prefix] # -inf, -inf if s != end_t: # å¦‚æœsä¸å’Œä¸Šä¸€ä¸ªä¸é‡å¤ï¼Œåˆ™æ›´æ–°éç©ºæ ¼çš„æ¦‚ç‡ n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p) else: # å¦‚æœså’Œä¸Šä¸€ä¸ªé‡å¤ï¼Œä¹Ÿè¦æ›´æ–°éç©ºæ ¼çš„æ¦‚ç‡ # We don't include the previous probability of not ending # in blank (p_nb) if s is repeated at the end. The CTC # algorithm merges characters not separated by a blank. n_p_nb = logsumexp(n_p_nb, p_b + p) # If s is repeated at the end we also update the unchanged # prefix. This is the merging case. if s == end_t: (n_p_b, n_p_nb), n_prefix_p = next_beam[prefix] n_p_nb = logsumexp(n_p_nb, p_nb + p) # å¦‚æœæ˜¯s=end_tï¼Œåˆ™prefixä¸æ›´æ–° next_beam[prefix] = ((n_p_b, n_p_nb), n_prefix_p) else: # *NB* this would be a good place to include an LM score. next_beam[n_prefix] = ((n_p_b, n_p_nb), n_prefix_p) # print(t, next_beam.keys()) # Sort and trim the beam before moving on to the # next time-step. # æ ¹æ®æ¦‚ç‡è¿›è¡Œæ’åºï¼Œæ¯æ¬¡ä¿ç•™æ¦‚ç‡æœ€é«˜çš„beam_sizeæ¡è·¯å¾„ beam = sorted(next_beam.items(), key=lambda x: logsumexp(*x[1][0]), reverse=True) beam = beam[:beam_size] # best = beam[0] # return best[0], -logsumexp(*best[1][0]), best[1][1] pred_lens = [len(beam[i][0]) for i in range(beam_size)] max_len = max(pred_lens) pred_seq, scores, pred_pobs = np.zeros((beam_size, max_len), dtype=np.int32), \\ [], np.zeros((beam_size, max_len)) for bs in range(beam_size): pred_seq[bs][:pred_lens[bs]] = beam[bs][0] scores.append(-logsumexp(*beam[bs][1][0])) pred_pobs[bs][:pred_lens[bs]] = np.exp(beam[bs][1][1]) return pred_seq, scores, pred_pobs# å› ä¸ºä»£ç ä¸­ä¸ºäº†é¿å…æ•°æ®ä¸‹æº¢ï¼Œéƒ½é‡‡ç”¨çš„æ˜¯å¯¹æ•°æ¦‚ç‡ï¼Œæ‰€ä»¥çœ‹èµ·æ¥æ¯”è¾ƒç¹çdef logsumexp(*args): &quot;&quot;&quot; Stable log sum exp. &quot;&quot;&quot; if all(a == NEG_INF for a in args): return NEG_INF a_max = max(args) lsp = math.log(sum(math.exp(a - a_max) for a in args)) # æ¦‚ç‡ç›¸åŠ å†å–logï¼Œä¸ºé¿å…æ•°å€¼ä¸‹æº¢ return a_max + lsp# åˆ›å»ºä¸€ä¸ªæ–°çš„beamdef make_new_beam(): fn = lambda: ((NEG_INF, NEG_INF), tuple()) return collections.defaultdict(fn)if __name__ == &quot;__main__&quot;: import ctcdecode, time np.random.seed(3) seq_len = 50 output_dim = 20 probs = np.random.rand(seq_len, output_dim) # probs = np.random.rand(time, output_dim) # probs = np.random.rand(time, output_dim) probs = probs / np.sum(probs, axis=1, keepdims=True) start_time = time.time() labels, score, labels_p = MPGenerate.ctc_beam_search_decode(probs, beam_size=5, blank=0) print(&quot;labels:&quot;, labels[0], len(labels[0])) print(&quot;labels_p: &quot;, labels_p[0], len(labels_p[0])) print(&quot;Score {:.3f}&quot;.format(score[0])) print(&quot;First method time: &quot;, time.time() - start_time) dec_logits = torch.FloatTensor(probs).unsqueeze(0) len_video = torch.LongTensor([seq_len]) decoder_vocab = [chr(x) for x in range(20000, 20000 + output_dim)] second_time = time.time() decoder = ctcdecode.CTCBeamDecoder(decoder_vocab, beam_width=5, blank_id=0, num_processes=10) pred_seq, scores, _, out_seq_len = decoder.decode(dec_logits, len_video) # pred_seq: [batch, beam, length] # out_seq_len: [batch, beam] print(pred_seq[0, 0, :][:out_seq_len[0, 0]]) print(out_seq_len[0, 0]) print(&quot;Score {:.3f}&quot;.format(scores[0, 0])) print(&quot;Second method time: &quot;, time.time() - second_time) References Sequence Modeling With CTC, Awni HannunCTCç®—æ³•è¯¦è§£ä¹‹è®­ç»ƒç¯‡ CTC Algorithm Explained Part 2ï¼šDecoding the Networkï¼ˆCTCç®—æ³•è¯¦è§£ä¹‹è§£ç ç¯‡","link":"/2020/09/13/ctc-loss-and-decoder/"},{"title":"è®ºæ–‡ç¬”è®° Deep Transition Architecture","text":"paper 1paper: Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers, 2018 WASSA@EMNLP å°† self-attention å’Œ LSTMï¼ŒCNN è¿›è¡Œå¯¹æ¯”. å› ä¸ºå•çº¯åªæ˜¯ä¸ºäº†æ¯”è¾ƒä¸‰è€…ä½œä¸ºä¸€ä¸ª building block çš„æ€§èƒ½ï¼Œæ‰€ä»¥ä¿è¯å…¬å¹³æ€§çš„æƒ…å†µä¸‹ï¼Œåªç”¨äº† 1 layer å’Œ 2-layer. æ•´ç¯‡æ–‡ç« å¹¶æ²¡æœ‰çœ‹åˆ°å…¶ä»–çš„åˆ›æ–°æ€§ï¼Œä¸çŸ¥é“æ˜¯ä¸æ˜¯çœ‹é”™äº†ã€‚ã€‚æ‰€ä»¥è¿™æ ·ä¹Ÿèƒ½å‘ EMNLP? ä½†æ˜¯çœ‹äº†è¿˜æ˜¯æœ‰ç‚¹æ”¶è·ï¼Œå¤ä¹ äº†ä¸€ä¸‹ multi-head self-attention. ä»¥åŠæ–‡ä¸­æåˆ°äº†ä¸‰ç§ position encoding: Sinusoidal Position Encoding Learned Position Encoding Relative Position Representations Sinusoidal æ˜¯åœ¨ Transformer ä¸­ä½¿ç”¨çš„, å¥½å¤„åœ¨äºå³ä½¿æ˜¯æµ‹è¯•é›†ä¸­å‡ºç° sentence çš„é•¿åº¦æ¯”è®­ç»ƒé›†ä¸­æ‰€æœ‰çš„ sentence éƒ½è¦é•¿ï¼Œä¹Ÿèƒ½è®¡ç®—å…¶ position encoding. Relative æ˜¯æ•ˆæœæœ€å¥½çš„,ä½œè€…å’Œ Tansformer çš„ä½œè€…æ˜¯ä¸€æ ·çš„ï¼Œå€¼å¾—ä¸€çœ‹ã€‚Self-attention with relative position representations For this method, the self-attention mechanism is modified to explicitly learn the relative positional information between every two sequence positions. As a result, the input sequence is modeled as a labeled, directed, fully-connected graph, where the labels represent positional information. A tunable parameter k is also introduced that limits the maximum distance considered between two sequence positions. [Shaw et al., 2018] hypothesized that this will allow the model to generalize to longer sequences at test time. paper 2DTMT: A Novel Deep Transition Architecture for Neural Machine Translation AAAI 2019 è…¾è®¯ WeChat AI çš„ä¸€ç¯‡æœºå™¨ç¿»è¯‘çš„ paperï¼ŒåŒæ ·ä¹Ÿæ˜¯ encoder-decoder, ä½†æ˜¯æ”¹è¿›äº† Deep transition RNN. å…ˆäº†è§£ä½•ä¸º deep transition RNNï¼Œ æ³¨æ„ä¸ deep stacked architectures çš„åŒºåˆ«ã€‚ ä¹Ÿå°±æ˜¯åœ¨ä¼ ç»Ÿçš„ RNN è¿­ä»£ä¹‹å‰ï¼Œå…ˆå¯¹ state è¿›è¡Œ transitionã€‚è€Œè¿™ç¯‡ paper æ”¹è¿›çš„å°±æ˜¯è¿™ç§ transition çš„æ–¹å¼ï¼Œç›®çš„å°±æ˜¯ä¸ºäº†å‡è½»å…¶ä¸­å› ä¸ºéçº¿æ€§æ“ä½œå¸¦æ¥çš„æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚ æ¨¡å‹ç»“æ„å›¾ï¼š encoder transition multi-head attention query transition decoder transition å…¶ä¸­æå‡ºäº† GRU ä»¥åŠå…¶å˜ä½“ T-GRU å’Œ L-GRU. GRUå›é¡¾ä¸‹ GRU $$h_t = (1-z_t)\\odot h_{t-1} + z_t\\odot \\tilde h_t$$ å…¶ä¸­ï¼š candidate state: $$\\tilde h_t = tanh(W_{xh}x_t + r_t\\odot (W_{hh}h_{t-1}))$$ reset gate: $$r_t = \\sigma(W_{xr}x_t+W_{hr}h_{t-1})$$ update gate: $$z_t=\\sigma(W_{xz}x_t+W_{hz}h_{t-1})$$ T-GRU (transition GRU)å¯¹ GRU åšäº†ç®€åŒ–ï¼Œå› ä¸ºåªé’ˆå¯¹äº state çš„å˜åŒ–ï¼Œå»æ‰äº† $x_t$ çš„è¾“å…¥. $$h_t = (1-z_t)\\odot h_{t-1} + z_t\\odot \\tilde h_t$$ candidate state: $$\\tilde h_t = tanh(r_t\\odot (W_{hh}h_{t-1}))$$ reset gate: $$r_t = \\sigma(W_{hr}h_{t-1})$$ update gate: $$z_t=\\sigma(W_{hz}h_{t-1})$$ L-GRU( Linear Transformation enhanced GRU)$$h_t = (1-z_t)\\odot h_{t-1} + z_t\\odot \\tilde h_t$$ candidate state: $$\\tilde h_t = tanh(W_{xh}x_t + r_t\\odot (W_{hh}h_{t-1}))+ l_t\\odot H(x_t)$$ å¢åŠ äº†ä¸€ä¸ªåŸºäº x_t çš„çº¿æ€§çš„å˜æ¢ H(x_t), å¹¶ç”± $l_t$ æ§åˆ¶ä¿¡æ¯é‡çš„å¤šå°‘ã€‚ $$H(x_t)=W_xx_t$$ $$l_t=\\sigma(W_{xl}x_t+W_{hl}h_{t-1})$$ è¿™ä¸ªæ¨¡å—ç›¸å¯¹äº $W_{xh}x_t$ å¤šäº†ä¸€ä¸ªçº¿æ€§æ§åˆ¶å™¨ï¼Œå…¶å®ä»–ä¸ç¬¬äºŒä¸ªæ¨¡å— $r_t\\odot (W_{hh}h_{t-1}))$ å€’æ˜¯å¯¹ç§°çš„ï¼Œåªä¸è¿‡ $h_{t-1}$ æ¢æˆäº† $x_t$. æ‰€ä»¥æˆ‘å¯ä¸å¯ä»¥å†åŠ ä¸ªè¿™æ ·çš„ï¼Ÿ $\\tilde h_t = tanh(W_{xh}x_t +W_{hh2}h_{t-1} +l_t\\odot W_{xh2}x_t+ r_t\\odot (W_{hh}h_{t-1}))$ DNMTDecoder$L_s$ è¡¨ç¤º encoder transition çš„æ·±åº¦ depth. $j$ è¡¨ç¤º current time step. $$\\overrightarrow h_{j,0}=L-GRU(x_j, \\overrightarrow h_{j-1,L_s})$$ $$\\overrightarrow h_{j,k}=T-GRU(\\overrightarrow h_{j, k-1}),\\text{ for } 1\\le k\\le L_s$$ å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯ k=0 transition çš„ç¬¬ä¸€æ­¥éœ€è¦è¾“å…¥ $x_t$ï¼Œä½¿ç”¨ L-GRU. transition çš„å‰©ä½™æ­¥éª¤ä½¿ç”¨ T-GRU. åŒå‘ bi-direction GRU: $C=[\\overrightarrow h_{j, L_s}, \\overleftarrow h_{j, L_s}]$ Decoder query transition: depth $L_q$ decoder transition: depth $L_d$ å¯¹äº query transition: è¾“å…¥æ˜¯ä¸Šä¸€æ­¥çš„è¾“å‡º $y_{t-1}$ å’Œ ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ $s_{t-1}$. ç„¶åå¾—åˆ° $S_{t, L_q}$ ä¸ encoder è¿›è¡Œäº¤äº’, multi-head attention: å¾—åˆ° attention vector $c_t$ ä¹‹åï¼Œ$c_t$ ç±»ä¼¼äºè¾“å…¥ï¼Œ $s_{t,L_q}$ æ˜¯éšè—çŠ¶æ€ï¼Œè¿›ä¸€æ­¥è¿›è¡Œ transition state è½¬æ¢ã€‚ å¾—åˆ° $s_{t, L_q+L_d+1}$ å°±æ˜¯å½“å‰æ—¶é—´æ­¥çš„æœ€ç»ˆéšè—çŠ¶æ€ï¼Œé€šè¿‡æ˜ å°„åˆ°è¯è¡¨ç©ºé—´ï¼Œå³å¯é¢„æµ‹å½“å‰æ—¶é—´æ­¥çš„è¯ã€‚ Tricks è¿˜æ˜¯æœ‰å¾ˆå¤šå¯ä»¥å€Ÿé‰´çš„åœ°æ–¹çš„ï½","link":"/2018/12/21/paper-reading-12-21/"},{"title":"cs224d-lecture14 Tree-RNN and Constituency Parsing","text":"ä¸»è¦å†…å®¹ï¼š è¯­è¨€çš„è¯­ä¹‰è§£é‡Š å¦‚æœå°†çŸ­è¯­ç»“æ„æ˜ å°„åˆ°å‘é‡ç©ºé—´ä¸­ï¼šåˆ©ç”¨è¯­ä¹‰çš„åˆæˆæ€§ å¯¹æ¯” RNN å’Œ CNN Recursive neural networks Parsing a sentence with an RNN ä½¿ç”¨tree-rnn è¿›è¡Œåˆ†ç±»ï¼š assignment3 æƒ…æ„Ÿåˆ†ç±» è¯­è¨€çš„è¯­ä¹‰è§£é‡Šâ€“å¹¶ä¸åªæ˜¯è¯å‘é‡ è¯å‘é‡åªæ˜¯è¯è¯­çº§åˆ«çš„å‘é‡ï¼Œäººä»¬å¯ä»¥ç”¨æ›´å¤§é¢—ç²’åº¦çš„æ–‡æœ¬æ¥è¡¨è¾¾è‡ªå·±çš„æ„æ€ï¼Œè€Œä¸ä»…ä»…æ˜¯è¯è¢‹ä¸­çš„æŸä¸ªå•è¯ã€‚ æ¯”å¦‚: the country of my birth, the place where I was born Question: how can we represent the meaning of longer phrases? Answer: By mapping them into the same vector space. How should we map phrases into a vector space?åˆ©ç”¨è¯­ä¹‰çš„åˆæˆæ€§ï¼š use principle of Compositionality the meanings of its words the rules that combine them å…¶å®æƒ³æƒ³RNNä¹Ÿæ˜¯å°†ä¸€ä¸ªsentenceæˆ–è€…æ˜¯phraseå‹ç¼©åˆ°ä¸€ä¸ªå‘é‡ä¸­å»ã€‚åé¢ä¼šä»‹ç»å®ƒä»¬çš„åŒºåˆ«ã€‚ é€šè¿‡åŒæ—¶å­¦ä¹ å¥æ³•æ ‘å’Œå¤åˆæ€§å‘é‡è¡¨ç¤ºï¼Œå°±å¯ä»¥å¾—åˆ°çŸ­è¯­çš„å‘é‡è¡¨ç¤ºäº†ã€‚ å¥æ³•æ ‘ï¼š å¥æ³•æ ‘ç»“æ„å’Œå‘é‡è¡¨ç¤ºLearn Structure and Representationï¼š é—®é¢˜æ˜¯ï¼šæˆ‘ä»¬çœŸçš„éœ€è¦å­¦ä¹ è¿™ç§æ ‘ç»“æ„å—ï¼ŸDo we really need to learn this structure? ä»ä¸¤ä¸ªè§’åº¦æ¥è¯´æ˜è¿™ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯å¯¹æ¯”recursive å’Œ rnnï¼Œ è€Œæ˜¯ä»è¯­è¨€çš„æœ¬è´¨æ¥è§£é‡Šã€‚ Recursive vs. RNN Richard mentioned that the recurrent models are really sort of capturing representations of whole prefixes and youâ€™re not getting any representations of smaller units than that. ä¸¤è€…éƒ½æ˜¯é€’å½’ç¥ç»ç½‘ç»œï¼Œåªä¸è¿‡å‰è€…åœ¨ç©ºé—´ä¸Šé€’å½’ï¼Œåè€…åœ¨æ—¶é—´ä¸Šé€’å½’ã€‚ä¸­æ–‡æœ‰æ—¶ä¼šæŠŠåè€…ç¿»è¯‘ä¸ºâ€œå¾ªç¯ç¥ç»ç½‘ç»œâ€ï¼Œä½†è¿™æ˜æ˜¾æ··æ·†äº†ç­‰çº§ï¼Œä»¤äººè¯¯è§£ã€‚ å®ƒä»¬å„æœ‰å„çš„ä¼˜ç¼ºç‚¹ï¼ŒRecursive neural netéœ€è¦åˆ†æå™¨æ¥å¾—åˆ°å¥æ³•æ ‘ï¼Œè€ŒRecurrent neural netåªèƒ½æ•æ‰â€œå‰ç¼€â€â€œä¸Šæ–‡â€æ— æ³•æ•æ‰æ›´å°çš„å•ä½ã€‚ ä½†äººä»¬è¿˜æ˜¯æ›´å€¾å‘äºç”¨åè€…ï¼ŒLSTMä¹‹ç±»ã€‚å› ä¸ºè®­ç»ƒRecursive neural netä¹‹å‰ï¼Œä½ éœ€è¦å¥æ³•æ ‘ï¼›å¥æ³•æ ‘æ˜¯ä¸€ä¸ªç¦»æ•£çš„å†³ç­–ç»“æœï¼Œæ— æ³•è¿ç»­åœ°å½±å“æŸå¤±å‡½æ•°ï¼Œä¹Ÿå°±æ— æ³•ç®€å•åœ°åˆ©ç”¨åå‘ä¼ æ’­è®­ç»ƒRecursive neural netã€‚å¦å¤–ï¼Œå¤æ‚çš„ç»“æ„ä¹Ÿå¯¼è‡´Recursive neural netä¸æ˜“åœ¨GPUä¸Šä¼˜åŒ–ã€‚ è¯­è¨€æœ¬è´¨æ˜¯é€’å½’çš„å—ï¼Ÿ åœ¨è®¤çŸ¥ç§‘å­¦ä¸Šè™½ç„¶æœ‰äº›äº‰è®®ï¼Œå› ä¸ºä¸€èˆ¬ä¸€ä¸ªå¥å­æ˜¯æœ‰é•¿åº¦é™åˆ¶çš„ï¼Œäººä»¬å‡ ä¹ä»ä¸è¯´300ä¸ªè¯ä»¥ä¸Šçš„å¥å­ã€‚ä½†æ˜¯é€’å½’æ˜¯æè¿°è¯­è¨€çš„æœ€ä½³æ–¹å¼ï¼Œæ¯”å¦‚ [The man from [the company that you spoke with about [the project] yesterday]] è¿™é‡Œé¢ä¸€ä¸ªåè¯çŸ­è¯­å¥—ä¸€ä¸ªåè¯çŸ­è¯­ï¼Œä¸€çº§çº§ä¸‹å»ã€‚ä»å®ç”¨çš„è§’åº¦è®² 1ã€é€šè¿‡é€’å½’åœ°æè¿°å¥å­ï¼ˆå¥æ³•æ ‘ï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ¶ˆæ­§ï¼š 2ã€ä¾¿äºæŒ‡ä»£ç›¸æ¶ˆç­‰ä»»åŠ¡ã€‚ 3ã€ä¾¿äºåˆ©ç”¨è¯­æ³•æ ‘ç»“æ„ï¼ˆåŸºäºçŸ­è¯­çš„æœºå™¨ç¿»è¯‘ï¼‰ ä» RNNs åˆ° CNNsRNNåªä¼šä¸ºæ»¡è¶³è¯­æ³•çš„çŸ­è¯­è®¡ç®—å‘é‡ï¼Œè€ŒCNNä¸ºæ¯ä¸ªå¯èƒ½çš„çŸ­è¯­è®¡ç®—å‘é‡ã€‚ä»è¯­è¨€å­¦å’Œè®¤çŸ¥ç§‘å­¦çš„è§’åº¦æ¥è®²ï¼ŒCNNå¹¶ä¸åˆç†ã€‚ç”šè‡³recurrent neural networkä¹Ÿæ¯”tree modelå’ŒCNNæ›´åˆç†ã€‚ ä¸¤è€…çš„å…³ç³»å¯ä»¥è¿™æ ·æƒ³è±¡ï¼ŒRNNå°†CNNæ•æ‰çš„ä¸æ˜¯çŸ­è¯­çš„éƒ¨åˆ†åˆ é™¤äº†ï¼š å¾—åˆ°ï¼š So the sort of picture is that for the CNN, youâ€™re sort of making a representation of every pair of words, every triple of words, every four words. Where as the tree recursive neural network is saying well some of those representations donâ€™t correspond to a phrase and so weâ€™re gonna delete them out. So that for the convolultional neural network, you have a representation for every bigram. So you have a representation for there speak and trigram there speak slowly. Whereas for the recursive neural network, you only have representations for the sort of semantically meaningful phrases like people there and speaks slowly going together to give a representation for the whole sentence. Recursive Neural Networks for Structure Prediction è¾“å…¥ï¼š ä¸¤ä¸ªå­èŠ‚ç‚¹çš„å‘é‡è¡¨ç¤º è¾“å‡ºï¼š ä¸¤ä¸ªå­èŠ‚ç‚¹åˆå¹¶åçš„æ–°èŠ‚ç‚¹è¯­ä¹‰è¡¨ç¤ºï¼Œä»¥åŠæ–°èŠ‚ç‚¹æˆç«‹çš„åˆ†å€¼ Recursive Neural Network Definitionå¯ä»¥åŒæ—¶å¾—åˆ°å¥æ³•æ ‘å’Œå‘é‡è¡¨ç¤ºçš„ä¸€ç§ä»»åŠ¡ã€‚é€šè¿‡socreæ¥å¾—åˆ°å¥æ³•æ ‘ã€‚ é¡ºä¾¿æä¸€ä¸‹assignment3: åœ¨ assignment3 æ˜¯è¿™æ ·çš„tree-RNN $$h=relu([h^{(1)}{left},h^{(1)}{right}]W+b^{(1)})$$ $$\\hat y = softmax(h^{(1)}U+b^{(s)})$$ $L\\in R^{|V|\\times d},W^{(1)}\\in R^{2d\\times d}, b^{(1)}\\in R^{1\\times d}, U\\in R^{(d\\times 5)}, b^{(s)}\\in R^{1\\times 5}$ åœ¨assignment3ä¸­ç”¨tree-rnnè¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œæ˜¯å·²ç»é€šè¿‡å¥æ³•åˆ†æå¾—åˆ°äº†å¥æ³•æ ‘çš„ï¼Œæ‰€ä»¥åªéœ€è¦ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œé€’å½’æ‰¾åˆ°å­èŠ‚ç‚¹ï¼Œå¹¶è®¡ç®—å‡ºå¯¹åº”çš„å‘é‡è¡¨ç¤ºï¼Œå¹¶å½’ä¸€åŒ–softmaxï¼Œç„¶åä¸æ¯ä¸ªèŠ‚ç‚¹ï¼ˆåŒ…æ‹¬å¶èŠ‚ç‚¹ï¼‰çœŸå®æ ‡ç­¾å¯¹æ¯”ï¼Œè®¡ç®—å¾—åˆ°æŸå¤±å€¼ã€‚ç„¶åç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å¾—åˆ°æ¨¡å‹å‚æ•°ã€‚ é‚£ä¹ˆè¿™é‡Œçš„å‚æ•°æ€ä¹ˆç†è§£ï¼Ÿåœ¨ä¼ ç»Ÿçš„rnnä¸­ $W_{hh}$ å¯ä»¥çœ‹åšæ˜¯éšè—çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œè¿™é‡Œå‘¢ï¼Ÿï¼Ÿï¼Ÿå…³äº $W_{hh}$ çš„ç†è§£ï¼Œå¯ä»¥çœ‹çŸ¥ä¹ HMMå’ŒRNNæ˜¯ä»€ä¹ˆå…³ç³»ï¼ŸåŠŸæ•ˆä¸Šä¸¤è€…æœ‰å†²çªé‡å ï¼Ÿ Parsing a sentence with an RNNgreedily incrementally building up parse structure. è®¡ç®—ä»»æ„ä¸¤ä¸ªå•è¯åˆå¹¶çš„å¾—åˆ†ï¼ˆè™½ç„¶ä¸‹å›¾æ˜¯ç›¸é‚»ä¸¤ä¸ªï¼Œä½†æˆ‘è§‰å¾—é‚£åªæ˜¯ç»˜å›¾æ–¹ä¾¿ï¼›å°±ç®—æ˜¯æˆ‘ç¬¬ä¸€æ¬¡å†™çš„ç©å…·çº§åˆ«çš„ä¾å­˜å¥æ³•åˆ†æå™¨ï¼Œä¹Ÿæ˜¯ä»»æ„ä¸¤ä¸ªå•è¯ä¹‹é—´è®¡ç®—ï¼‰ï¼š ç„¶åè´ªå¿ƒåœ°é€‰æ‹©å¾—åˆ†æœ€å¤§çš„ä¸€å¯¹åˆå¹¶ï¼š é‡å¤è¿™ä¸€è¿‡ç¨‹:è®¡ç®—ä»»æ„ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œåˆå¹¶å¾—åˆ†æœ€å¤§çš„ä¸€å¯¹ ç›´åˆ°å¾—åˆ°æ ¹èŠ‚ç‚¹ï¼š æ¨¡å‹ä¸­åªæœ‰ä¸€ä¸ªåˆæˆå‡½æ•°ï¼Œä½¿ç”¨åŒä¸€ä¸ªæƒå€¼çŸ©é˜µWå¤„ç†NPã€VPã€PPâ€¦â€¦è¿™æ˜æ˜¾æ˜¯ä¸åˆç†çš„ã€‚ Max-Margin Framework-DetailsæŸå¤±å‡½æ•°ä½¿ç”¨æœ€å¤§é—´éš” å†å›é¡¾ä¸€ä¸‹å¤šåˆ†ç±»æ”¯æŒå‘é‡æœºæŸå¤± Multiclass Support Vector Machine Loss æˆ‘çš„cs231nç¬”è®° å¯¹äºå•ä¸ªèŠ‚ç‚¹ï¼Œå¯ä»¥çœ‹åšå•ä¸ªæ ·æœ¬æŸå¤± $$L_i=\\sum_{j\\ne y_j}^N max(0, s_j-(s_{y_i}-\\Delta))$$ å…¶ä¸­ $s_{y_j}$ è¡¨ç¤ºçœŸå®æ ‡ç­¾å¯¹åº”çš„å€¼ï¼ŒéçœŸå®åˆ†ç±»çš„å¾—åˆ†ä¸èƒ½è¶…è¿‡ $s_{y_j}-\\Delta$ï¼Œå‡¡æ˜¯è¶…è¿‡çš„éƒ½ä¼šå¯¹ $L_i$ äº§ç”Ÿå½±å“ã€‚æ¯”è¿™ä¸ªå€¼å°±æ²¡äº‹ï½ å¯¹äºæ•´ä¸ªsentenceï¼š $$J=\\sum_imax(0, s(x_i,y_j)-max_{y\\in A(x_i)}(s(x_i,y)+\\Delta(y,y_i)))$$ $\\Delta$ è¡¨ç¤ºå¯¹æ‰€æœ‰éæ­£ç¡®åˆ†ç±»çš„æƒ©ç½š max è¡¨ç¤ºè´ªå¿ƒæœç´¢å¾—åˆ°çš„syntactic treeçš„å¾—åˆ† æœ‰æ—¶å€™ä¹Ÿå¯ç”¨beam search ä½¿ç”¨ tree-RNN è¿›è¡Œåˆ†ç±»ä»»åŠ¡è¿™é‡Œçš„rnnæŒ‡çš„æ˜¯ é€’å½’recursive neural networks.ç©ºé—´ç»“æ„ä¸Šçš„é€’å½’ï¼Œè€Œä»¥å‰å­¦çš„RNNä¹Ÿæ˜¯é€’å½’ï¼Œä¸è¿‡æ˜¯æ—¶é—´åºåˆ—ä¸Šçš„é€’å½’ã€‚ ä»¥assignment3ä¸­çš„æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œè¿›è¡Œå‰å‘ã€åå‘ä¼ æ’­æ¨å¯¼ã€‚ ç”±äºå‰å‘ä¼ æ’­æ—¶æ¯ä¸ªèŠ‚ç‚¹çš„ä¿¡å·æ¥è‡ªæ‰€æœ‰å­èŠ‚ç‚¹ï¼Œæ‰€ä»¥æ¢¯åº¦ä¹Ÿæ¥è‡ªæ‰€æœ‰å­èŠ‚ç‚¹ã€‚å¹¶ä¸”å‰å‘ä¼ æ’­æ—¶çˆ¶èŠ‚ç‚¹çš„ä¿¡å·æ˜¯åˆ©ç”¨å­èŠ‚ç‚¹ä¿¡å·çš„æ‹¼æ¥è®¡ç®—çš„ï¼Œæ‰€ä»¥æ¢¯åº¦éœ€è¦é’ˆå¯¹å­èŠ‚ç‚¹çš„ä¿¡å·è®¡ç®—ï¼š è¿™ä¸ªé—®é¢˜å…¶å®åœ¨TensorFlowé‚£ä¸€è¯¾å·²ç»è®²è¿‡äº†ï¼Œå›¾è®¡ç®—ï¼šå‰å‘ä¼ æ’­ä¿¡å·æµå…¥æŸèŠ‚ç‚¹ï¼Œåå‘ä¼ æ’­è¯¯å·®å°±å¾—ä»æŸèŠ‚ç‚¹åˆ†æµåˆ°æ‰€æœ‰æºèŠ‚ç‚¹ã€‚æ ‘åªæ˜¯å›¾çš„ä¸€ä¸ªç‰¹ä¾‹ï¼š Richard Socher çš„ä»£ç æ¯”å¦‚softmaxä¹‹ç±»çš„å¯çœŸç†Ÿç»ƒï½ 123456789101112131415161718192021222324252627def forwardProp(self, node): # Recursive ... # This is node's hidden activation node.h = np.dot(self.W, np.hstack([node.left.h, node.right.h])) + self.b # [1,d] # Relu node.h[node.h&lt;0] = 0 # Softmax node.score = np.dot(self.Ws, node.h) + self.bs # [1, 5] node.score -= np.max(node.probs) node.probs = np.exp(node.score)/np.sum(np.exp(node.score)) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def backProp(self.node, error=None): # softmax`grad deltas = node.probs deltas[node.label] -= 1.0 self.dWs = np.outer(deltas, node.h) # Compute the outer product of two vectors. è®­ç»ƒæ—¶ä¸€ä¸ªbatchåªæœ‰ä¸€ä¸ªsentenceï¼Œæ‰€æœ‰hæ˜¯å‘é‡ã€‚ self.dbs += deltas # å¯¹éšè—çŠ¶æ€æ±‚å¯¼ dh = np.dot(self.Ws.T, deltas) # Add deltas from above if error is not None: dh += error # f'(z) now: # relu åå‘ä¼ æ’­ dh *= (node.h != 0) # Updata word vector if leaf node: # å¦‚æœæ˜¯å¶èŠ‚ç‚¹ï¼Œh å°±æ˜¯ Lï¼Œè¯å‘é‡. if node.isLeaf: self.dL[node.word] += deltas return # Recursively backProp # å¦‚æœå½“å‰èŠ‚ç‚¹ä¸æ˜¯å¶èŠ‚ç‚¹ï¼Œé‚£ä¹ˆéœ€è¦æ›´æ–°æƒé‡Wå’Œbï¼ŒåŒæ—¶å°†error if not node.isLeaf: self.dW += np.outer(deltas, np.hstack([node.left.h, node.right.h])) self.db += deltas # Error signal to children dh = np.dot(self.W.T, dh) # å°±æ˜¯å…¬å¼ h=relu([h^{(1)}_{left},h^{(1)}_{right}]W+b^{(1)}) # é€’å½’è®¡ç®—å·¦å­èŠ‚ç‚¹ï¼Œnode.lefç”¨æ¥è®¡ç®—å·¦å­èŠ‚ç‚¹è‡ªèº«çš„æŸå¤±ï¼Œ dh[:self.hiddenDim]ç”¨æ¥è®¡ç®—çˆ¶èŠ‚ç‚¹ä¼ é€’ä¸‹æ¥çš„æŸå¤± self.backProp(node.left, dh[:self.hiddenDim]) self.backProp(node.right, dh[self.hiddenDim:]) okï¼å®Œå…¨å¼„æ‡‚äº†å§ï¼Ÿï¼ï½ Syntactically-Untied RNN Version3 Presentation[Deep reinforcement learning for dialogue generation] reference CS224nç¬”è®°14 Tree RNNä¸çŸ­è¯­å¥æ³•åˆ†æ Recursive Neural Networks Can Learn Logical Semantics","link":"/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/"},{"title":"dockerå­¦ä¹ å’Œä½¿ç”¨","text":"docker å®‰è£…docker tensorflowdocker é•œåƒè¿ç§»æœ‰æ—¶å€™éœ€è¦å°†é•œåƒä»ä¸€å°æœåŠ¡å™¨ç§»åŠ¨åˆ°å¦å¤–ä¸€å°æœåŠ¡å™¨ã€‚å¯ä½¿ç”¨ nc è¿›è¡Œå¤§æ–‡ä»¶ä¼ è¾“ docker æ–‡ä»¶æ˜ å°„1234567sudo docker run -itd -p 8888:8888 -v container_path:/usr/home/wuwei7/tmp_data image_id /bin/bashsudo docker exec -it container_id /bin/bash -v å‚æ•°è¡¨ç¤ºå®¹å™¨å†…æ–‡ä»¶ä¸å®¿ä¸»æœºå™¨æ–‡ä»¶æ˜ å°„ docker é©»å®ˆçŠ¶æ€1234567sudo docker run -itd -p 8888:8888 -v container_path:/usr/home/wuwei7/tmp_data image_id /bin/bashsudo docker exec -it container_id /bin/bash -itd è¡¨ç¤ºé©»å®ˆçŠ¶æ€ exec é€€å‡ºä¹‹åå®¹å™¨ä¾ç„¶åœ¨è¿è¡Œ docker å¸¸ç”¨å‘½ä»¤123456789101112131415sudo docker ps æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„å®¹å™¨sudo docker images æŸ¥çœ‹é•œåƒsudo docker rm -rf Container_ID åˆ é™¤æ­£åœ¨è¿è¡Œçš„å®¹å™¨sudo docker rmi -f Image_ID åˆ é™¤é•œåƒï¼Œ-f å¼ºåˆ¶åˆ é™¤åœ¨å®¹å™¨ä¸­è¿è¡Œçš„é•œåƒ","link":"/2019/02/21/docker%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BD%BF%E7%94%A8/"},{"title":"è®ºæ–‡ç¬”è®°-baseline for OOD","text":"paper: A baseline for detecting misclassified and out-of-distribution examples Motivationæ–‡ç« å¼€å¤´å…ˆè¯´åˆ°ï¼Œé€šè¿‡ softmax é¢„æµ‹å¾—åˆ°çš„å„ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ (prediction probability) å’Œ ç½®ä¿¡åº¦ä¹‹é—´çš„å¯¹åº”å…³ç³»å¹¶ä¸æ˜¯å¾ˆç›´æ¥ (have a poor direct correspondence to confidence)ã€‚è¿™æ˜¯å› ä¸º softmax çš„è®¡ç®—ä½¿ç”¨äº†æŒ‡æ•°å‡½æ•°(fast-growing exponential function),è¿™å°±ä¼šå¯¼è‡´ä¸€ä¸ªå¾ˆå°çš„é¢å¤–é™„åŠ è¾“å…¥ï¼Œéƒ½ä¼šä½¿å¾—æ¥ä¸‹æ¥çš„è¾“å‡ºåˆ†å¸ƒ(output distribution) å‘ç”Ÿæ”¹å˜ã€‚äº‹å®ä¸Šï¼Œä¹‹å‰ä¹Ÿæœ‰äººåšäº†ç›¸å½“ä¸€éƒ¨åˆ†å®éªŒï¼Œè¯æ˜äº†ä¸€ä¸ªé«˜æ–¯éšæœºå™ªå£°åŠ å…¥åˆ°ä¸€ä¸ª MNIST å›¾åƒä¹‹åï¼Œä¼šè®©è¿™ä¸ªå›¾åƒè·å¾— 91% çš„é¢„æµ‹æ¦‚ç‡ã€‚ å°½ç®¡å¦‚æ­¤ï¼Œç„¶è€Œï¼Œè¿™ç¯‡æ–‡ç« ä½œè€…ä¾æ—§è®¤ä¸º é”™è¯¯çš„ç±»åˆ« æˆ– OOD(incorrect and out-of-distribution) æ›´å€¾å‘äºå…·æœ‰è¾ƒä½çš„é¢„æµ‹æ¦‚ç‡ï¼Œç›¸å¯¹äºæ­£ç¡®çš„æ ·æœ¬ã€‚å› æ­¤ï¼Œè·å¾—å…³äºæ­£ç¡®çš„æˆ–è€… in-sample æ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡çš„ç»Ÿè®¡ï¼Œé€šå¸¸æ¥è¯´è¶³å¤Ÿå»æ£€æµ‹å‡º é”™è¯¯æˆ–ä¸æ­£å¸¸(error or abnormal),å³ä½¿å•ç‹¬æ¥çœ‹é¢„æµ‹æ¦‚ç‡å¯èƒ½ä¼šæœ‰è¯¯å¯¼ã€‚ä¹Ÿå°±æ˜¯ä»ç»Ÿè®¡çš„è§’åº¦ï¼Œsoftmax å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒè¿˜æ˜¯å¯ä¿¡çš„ã€‚ è¿™ç¯‡æ–‡ç« åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶ä¸éƒ½æ˜¯ SOTA, æ‰€ä»¥åªæ˜¯æä¾›äº†ä¸€ä¸ªæ–°çš„æ–¹æ³•ç”¨æ¥éªŒè¯ä¸€ä¸ªç¥ç»ç½‘ç»œèƒ½å¦æœ‰æ•ˆçš„åŒºåˆ†å‡º abnormalï¼Œä½œä¸º baseline method. é™¤äº†è¿™ä¸ª baseline method,ä½œè€…è¿˜åˆ¶å®šäº†æ ‡å‡†çš„ä»»åŠ¡å’ŒæŒ‡æ ‡ï¼Œç”¨æ¥è¯„ä»·å¯¹ é”™è¯¯å’ŒOOD çš„è‡ªåŠ¨æ£€æµ‹ . æ‰€ä»¥çœ‹è¿™ç¯‡æ–‡ç« çš„ç›®çš„å°±æ˜¯ baseline mathod æ˜¯ä»€ä¹ˆï¼Ÿä¹Ÿå°±æ˜¯æ€ä¹ˆå»è¯„ä»·ä¸€ä¸ªæ¨¡å‹è‡ªåŠ¨åŒºåˆ†å‡º OOD çš„èƒ½åŠ›ã€‚ ä½œè€…ç»™å‡ºçš„æ ‡å‡†ä»»åŠ¡å’Œæ•°æ® Baseline methodè¿™ç¯‡æ–‡ç« çš„ä¸»è¦è§£å†³çš„ä¸¤ä¸ªé—®é¢˜ï¼š error and success prediction: èƒ½å¦æ­£ç¡®çš„å¯¹ä¸€ä¸ªæ ·æœ¬åˆ†ç±» in- and out-of-distribution detectionï¼š èƒ½å¦æ­£ç¡®çš„æ£€æµ‹å‡º OOD é€šå¸¸æ¥è¯´ï¼ŒOOD å’Œ in sample æ¥è¯´ï¼Œæ ·æœ¬æ•°é‡å·®å¼‚ä¼šå¾ˆå¤§ï¼Œæ¯”å¦‚ç–¾ç—…æ£€æµ‹ï¼Œæœªè§è¿‡çš„ç½•è§ç–¾ç—… OOD å°±å¾ˆå°‘; åˆæ¯”å¦‚å¯¹ä¸€ä¸ªçŒ«ç‹—åˆ†ç±»å™¨ï¼Œä»–åœ¨æµ‹è¯•æ—¶ï¼ŒOOD å°±å¾ˆå¤§ã€‚æ‰€ä»¥å¯¹äºè¿™ç§æ•°æ®ä¸å‡è¡¡çš„é—®é¢˜ï¼Œaccuracy å·²ç»æ— æ³•æ»¡è¶³è¿™ç±»é—®é¢˜äº†ã€‚ metric1: AUROCä½œè€…ä½¿ç”¨äº† AUROC(Area Under the Receiver Operating Characteristic curve). å…¶å®è¿™ä¸ªåœ¨å‰é¢çš„ç¬”è®°ä¸­æœ‰è¯¦ç»†ä»‹ç»è¿‡ï¼Œè¿™é‡Œå†å¤ä¹ éï½ æœºå™¨å­¦ä¹ -å¸¸ç”¨æŒ‡æ ‡æ€»ç»“ ROC æ›²çº¿æ˜¯ä¾èµ–äºé˜ˆå€¼çš„æ€§èƒ½éªŒè¯æŒ‡æ ‡ (metric which is a threshold-independent performance evalution). å› ä¸ºåœ¨è¿™ç±»ä¸å‡è¡¡é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯ positive label. æ‰€ä»¥æˆ‘ä»¬å…³æ³¨çš„æŒ‡æ ‡æ˜¯ çœŸæ­£ç±»ç‡ TPR, è´Ÿæ­£ç±»ç‡ FPR. TPR, çœŸæ­£ç±»ç‡(true positive rate ,TPR),ï¼š å¦‚æœä¸€ä¸ªå®ä¾‹æ˜¯æ­£ç±»å¹¶ä¸”ä¹Ÿè¢« é¢„æµ‹æˆæ­£ç±»ï¼Œå³ä¸ºçœŸæ­£ç±»ï¼ˆTrue positiveï¼‰ï¼ŒçœŸæ­£ç±»ç‡æ˜¯æŒ‡åˆ†ç±»å™¨æ‰€è¯†åˆ«å‡ºæ¥çš„ æ­£å®ä¾‹å æ‰€æœ‰æ­£å®ä¾‹çš„æ¯”ä¾‹ã€‚å°±æ˜¯æ­£ç±»çš„ Recall å§ï½ TPR = TP / (TP + FN) FPR, è´Ÿæ­£ç±»ç‡ï¼š åˆ†ç±»å™¨é”™è®¤ä¸ºæ­£ç±»çš„è´Ÿå®ä¾‹å æ‰€æœ‰è´Ÿå®ä¾‹çš„æ¯”ä¾‹ï¼ŒFPR = FP / (FP + TN) è¿˜æ˜¯ä¸å¤ªæ˜ç™½ä¸ºå•¥ TPR + FPR = 1ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ metric2: AUPRArea Under the Precision-Recall curve (AUPR) The PR curve plots the precision (tp=(tp+fp)) and recall (tp=(tp + fn)) against each other. å¯¹äº PR æ›²çº¿ï¼Œé€‰æ‹©å“ªä¸ªç±»åˆ«ä½œä¸º positive ç±»ï¼Œéå¸¸é‡è¦ã€‚","link":"/2018/12/24/paper-reading-12-24/"},{"title":"pytorch-Tensor","text":"Tensorä»æ¥å£çš„è§’åº¦æ¥è®²ï¼Œå¯¹tensorçš„æ“ä½œå¯åˆ†ä¸ºä¸¤ç±»ï¼š torch.functionï¼Œå¦‚torch.saveç­‰ã€‚ å¦ä¸€ç±»æ˜¯tensor.functionï¼Œå¦‚tensor.viewç­‰ã€‚ è€Œä»å­˜å‚¨çš„è§’åº¦æ¥è®²ï¼Œå¯¹tensorçš„æ“ä½œåˆå¯åˆ†ä¸ºä¸¤ç±»ï¼š ä¸ä¼šä¿®æ”¹è‡ªèº«çš„æ•°æ®ï¼Œå¦‚ a.add(b)ï¼Œ åŠ æ³•çš„ç»“æœä¼šè¿”å›ä¸€ä¸ªæ–°çš„tensorã€‚ ä¼šä¿®æ”¹è‡ªèº«çš„æ•°æ®ï¼Œå¦‚ a.add_(b)ï¼Œ åŠ æ³•çš„ç»“æœä»å­˜å‚¨åœ¨aä¸­ï¼Œaè¢«ä¿®æ”¹äº†ã€‚ è¡¨3-1: å¸¸è§æ–°å»ºtensorçš„æ–¹æ³• |å‡½æ•°|åŠŸèƒ½| |:â€”:|:â€”:| |Tensor(*sizes)|åŸºç¡€æ„é€ å‡½æ•°| |ones(*sizes)|å…¨1Tensor| |zeros(*sizes)|å…¨0Tensor| |eye(*sizes)|å¯¹è§’çº¿ä¸º1ï¼Œå…¶ä»–ä¸º0| |arange(s,e,step|ä»såˆ°eï¼Œæ­¥é•¿ä¸ºstep| |linspace(s,e,steps)|ä»såˆ°eï¼Œå‡åŒ€åˆ‡åˆ†æˆstepsä»½| |rand/randn(*sizes)|å‡åŒ€/æ ‡å‡†åˆ†å¸ƒ| |normal(mean,std)/uniform(from,to)|æ­£æ€åˆ†å¸ƒ/å‡åŒ€åˆ†å¸ƒ| |randperm(m)|éšæœºæ’åˆ—| å…¶ä¸­ä½¿ç”¨Tensorå‡½æ•°æ–°å»ºtensoræ˜¯æœ€å¤æ‚å¤šå˜çš„æ–¹å¼ï¼Œå®ƒæ—¢å¯ä»¥æ¥æ”¶ä¸€ä¸ªlistï¼Œå¹¶æ ¹æ®listçš„æ•°æ®æ–°å»ºtensorï¼Œä¹Ÿèƒ½æ ¹æ®æŒ‡å®šçš„å½¢çŠ¶æ–°å»ºtensorï¼Œè¿˜èƒ½ä¼ å…¥å…¶ä»–çš„tensor. b.tolist() æŠŠ tensor è½¬ä¸º list b.numel() b ä¸­å…ƒç´ æ€»æ•°ï¼Œç­‰ä»·äº b.nelement() torch.Tensor(b.size()) åˆ›å»ºå’Œ b ä¸€æ ·çš„ tensor é™¤äº†tensor.size()ï¼Œè¿˜å¯ä»¥åˆ©ç”¨tensor.shapeç›´æ¥æŸ¥çœ‹tensorçš„å½¢çŠ¶ï¼Œtensor.shapeç­‰ä»·äºtensor.size() 1234567891011121314151617181920212223# ç”¨listçš„æ•°æ®åˆ›å»ºtensorb = torch.Tensor([[1,2,3],[4,5,6]])print(b)print(b.tolist())print(b.numel())# åˆ›å»ºä¸€ä¸ªå’Œbå½¢çŠ¶ä¸€æ ·çš„tensorc = torch.Tensor(b.size())print(c)# åˆ›å»ºä¸€ä¸ªå…ƒç´ ä¸º2å’Œ3çš„tensord = torch.Tensor((2, 3))print(d) tensor([[ 1., 2., 3.], [ 4., 5., 6.]]) [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]] 6 tensor(1.00000e-15 * [[-3.4942, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]]) tensor([ 2., 3.]) å¸¸ç”¨Tensoræ“ä½œview, squeeze, unsqueeze, resize é€šè¿‡tensor.viewæ–¹æ³•å¯ä»¥è°ƒæ•´tensorçš„å½¢çŠ¶ï¼Œä½†å¿…é¡»ä¿è¯è°ƒæ•´å‰åå…ƒç´ æ€»æ•°ä¸€è‡´ã€‚viewä¸ä¼šä¿®æ”¹è‡ªèº«çš„æ•°æ®ï¼Œè¿”å›çš„æ–°tensorä¸æºtensorå…±äº«å†…å­˜ï¼Œä¹Ÿå³æ›´æ”¹å…¶ä¸­çš„ä¸€ä¸ªï¼Œå¦å¤–ä¸€ä¸ªä¹Ÿä¼šè·Ÿç€æ”¹å˜ã€‚åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½ç»å¸¸éœ€è¦æ·»åŠ æˆ–å‡å°‘æŸä¸€ç»´åº¦ï¼Œè¿™æ—¶å€™squeezeå’Œunsqueezeä¸¤ä¸ªå‡½æ•°å°±æ´¾ä¸Šç”¨åœºäº†ã€‚ tensorflow é‡Œé¢æ˜¯ tf.expand_dim å’Œ tf.squeeze. resizeæ˜¯å¦ä¸€ç§å¯ç”¨æ¥è°ƒæ•´sizeçš„æ–¹æ³•ï¼Œä½†ä¸viewä¸åŒï¼Œå®ƒå¯ä»¥ä¿®æ”¹tensorçš„å¤§å°ã€‚å¦‚æœæ–°å¤§å°è¶…è¿‡äº†åŸå¤§å°ï¼Œä¼šè‡ªåŠ¨åˆ†é…æ–°çš„å†…å­˜ç©ºé—´ï¼Œè€Œå¦‚æœæ–°å¤§å°å°äºåŸå¤§å°ï¼Œåˆ™ä¹‹å‰çš„æ•°æ®ä¾æ—§ä¼šè¢«ä¿å­˜ï¼Œçœ‹ä¸€ä¸ªä¾‹å­ã€‚ 12345a = torch.arange(0, 6)a.view(2, 3) tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 12345b = a.view(-1, 3) # å½“æŸä¸€ç»´ä¸º-1çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨è®¡ç®—å®ƒçš„å¤§å°b tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 123b.unsqueeze(1) # æ³¨æ„å½¢çŠ¶ï¼Œåœ¨ç¬¬1ç»´ï¼ˆä¸‹æ ‡ä»0å¼€å§‹ï¼‰ä¸Šå¢åŠ â€œï¼‘â€ tensor([[[ 0., 1., 2.]], [[ 3., 4., 5.]]]) 123b.unsqueeze(-2) # -2è¡¨ç¤ºå€’æ•°ç¬¬äºŒä¸ªç»´åº¦ tensor([[[ 0., 1., 2.]], [[ 3., 4., 5.]]]) 12345c = b.view(1, 1, 1, 2, 3)c.squeeze(0) # å‹ç¼©ç¬¬0ç»´çš„â€œï¼‘â€ tensor([[[[ 0., 1., 2.], [ 3., 4., 5.]]]]) 123c.squeeze() # æŠŠæ‰€æœ‰ç»´åº¦ä¸ºâ€œ1â€çš„å‹ç¼© tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 12345a[1] = 100b # aä¿®æ”¹ï¼Œbä½œä¸ºviewä¹‹åçš„ï¼Œä¹Ÿä¼šè·Ÿç€ä¿®æ”¹ tensor([[ 0., 100., 2.], [ 3., 4., 5.]]) 12345b.resize_(1, 3)b tensor([[ 0., 100., 2.]]) 12345b.resize_(3, 3) # æ—§çš„æ•°æ®ä¾æ—§ä¿å­˜ç€ï¼Œå¤šå‡ºçš„å¤§å°ä¼šåˆ†é…æ–°ç©ºé—´b tensor([[ 0.0000, 100.0000, 2.0000], [ 3.0000, 4.0000, 5.0000], [ -0.0000, 0.0000, 0.0000]]) ç´¢å¼•æ“ä½œTensoræ”¯æŒä¸numpy.ndarrayç±»ä¼¼çš„ç´¢å¼•æ“ä½œï¼Œè¯­æ³•ä¸Šä¹Ÿç±»ä¼¼ï¼Œä¸‹é¢é€šè¿‡ä¸€äº›ä¾‹å­ï¼Œè®²è§£å¸¸ç”¨çš„ç´¢å¼•æ“ä½œã€‚å¦‚æ— ç‰¹æ®Šè¯´æ˜ï¼Œç´¢å¼•å‡ºæ¥çš„ç»“æœä¸åŸtensorå…±äº«å†…å­˜ï¼Œä¹Ÿå³ä¿®æ”¹ä¸€ä¸ªï¼Œå¦ä¸€ä¸ªä¼šè·Ÿç€ä¿®æ”¹ã€‚ å…¶å®ƒå¸¸ç”¨çš„é€‰æ‹©å‡½æ•°å¦‚è¡¨3-2æ‰€ç¤ºã€‚ è¡¨3-2å¸¸ç”¨çš„é€‰æ‹©å‡½æ•° å‡½æ•°|åŠŸèƒ½| :â€”:|:â€”:| index_select(input, dim, index)|åœ¨æŒ‡å®šç»´åº¦dimä¸Šé€‰å–ï¼Œæ¯”å¦‚é€‰å–æŸäº›è¡Œã€æŸäº›åˆ— masked_select(input, mask)|ä¾‹å­å¦‚ä¸Šï¼Œa[a&gt;0]ï¼Œä½¿ç”¨ByteTensorè¿›è¡Œé€‰å– non_zero(input)|é0å…ƒç´ çš„ä¸‹æ ‡ gather(input, dim, index)|æ ¹æ®indexï¼Œåœ¨dimç»´åº¦ä¸Šé€‰å–æ•°æ®ï¼Œè¾“å‡ºçš„sizeä¸indexä¸€æ · gatheræ˜¯ä¸€ä¸ªæ¯”è¾ƒå¤æ‚çš„æ“ä½œï¼Œå¯¹ä¸€ä¸ª2ç»´tensorï¼Œè¾“å‡ºçš„æ¯ä¸ªå…ƒç´ å¦‚ä¸‹ï¼š 12345out[i][j] = input[index[i][j]][j] # dim=0out[i][j] = input[i][index[i][j]] # dim=1 ä¸‰ç»´tensorçš„gatheræ“ä½œåŒç†ï¼Œä¸‹é¢ä¸¾å‡ ä¸ªä¾‹å­ã€‚ index_select(input, dim, index) æŒ‡å®šç»´åº¦ä¸Šé€‰å–æŸäº›è¡Œå’Œåˆ—, è¿”å›çš„æ˜¯æŸè¡Œå’ŒæŸåˆ—1234567a = torch.randn(3, 4)print(a)print(a[0,1]) # ç¬¬ 0 è¡Œï¼Œ ç¬¬ 1 åˆ— tensor([[ 0.5948, -0.5760, 1.3726, -0.9664], [ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) tensor(-0.5760) è¿”å›è¡Œçš„å››ç§æ–¹å¼123print(a[torch.LongTensor([1,2])]) # ç¬¬ 0 è¡Œ å’Œ ç¬¬ 1 è¡Œ tensor([[ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) 12345index = torch.LongTensor([1,2])a.index_select(dim=0, index=index) tensor([[ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) 123a[1:3] # åªèƒ½æ˜¯è¿ç»­çš„è¡Œ tensor([[ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) 123print(a[torch.LongTensor([[1],[2]])]) # è¿˜æ˜¯ç¬¬ 0 è¡Œ å’Œ ç¬¬ 1 è¡Œ tensor([[[ 0.5705, 1.0374, -1.1780, 0.0635]], [[-0.1195, 0.6657, 0.9583, -1.8952]]]) è¿”å›åˆ—çš„ä¸¤ç§æ–¹å¼123a.index_select(dim=1, index=index) tensor([[-0.5760, 1.3726], [ 1.0374, -1.1780], [ 0.6657, 0.9583]]) 123a[:, 1:3] # è¿ç»­çš„åˆ— tensor([[-0.5760, 1.3726], [ 1.0374, -1.1780], [ 0.6657, 0.9583]]) masked_selected(input, mask) ä½¿ç”¨ ByteTensor è¿›è¡Œé€‰å–mask is ByteTensor, ç±»ä¼¼äº a[a&gt;1] 123456789a = torch.randn(3, 4)print(a)print(a[a&gt;0])a.masked_select(a&gt;0) tensor([[ 0.3464, 1.4499, 0.7417, -1.9551], [-0.0042, -0.0141, 1.2861, 0.0691], [ 0.5843, 1.6635, -1.2771, -1.4623]]) tensor([ 0.3464, 1.4499, 0.7417, 1.2861, 0.0691, 0.5843, 1.6635]) tensor([ 0.3464, 1.4499, 0.7417, 1.2861, 0.0691, 0.5843, 1.6635]) 123a&gt;0 # è¿”å›ä¸€ä¸ª ByteTensor tensor([[ 1, 1, 1, 0], [ 0, 0, 1, 1], [ 1, 1, 0, 0]], dtype=torch.uint8) 12345b = torch.ByteTensor(3,4)b tensor([[ 80, 235, 127, 167], [ 199, 85, 0, 0], [ 0, 0, 0, 0]], dtype=torch.uint8) 123a[b] tensor([ 0.3464, 1.4499, 0.7417, -1.9551, -0.0042, -0.0141]) gather(input, dim, index) æ ¹æ® index åœ¨ dim ç»´åº¦ä¸Šé€‰å–æ•°æ®ï¼Œè¾“å‡º size ä¸ index ä¸€æ ·.123456789a = torch.arange(0, 20).view(4,5)print(a)index = torch.LongTensor([[0,1,2,1,3]])print(index, index.shape) tensor([[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.], [ 10., 11., 12., 13., 14.], [ 15., 16., 17., 18., 19.]]) tensor([[ 0, 1, 2, 1, 3]]) torch.Size([1, 5]) 123a.gather(dim=0, index=index) tensor([[ 0., 6., 12., 8., 19.]]) æ‰€ä»¥ gather å°±æ˜¯ index ä¸ input ä¸­æŸä¸€ä¸ªç»´åº¦ä¸€è‡´ï¼Œæ¯”å¦‚è¿™é‡Œ input.size()=[4,5]. é‚£ä¹ˆ dim=0, index.size()=[1,5]. ç„¶ååœ¨æ¯åˆ—å¯¹åº”çš„ index é€‰å–å¯¹åº”çš„æ•°æ®ã€‚æœ€åè¾“å‡º size ä¸ index ä¸€è‡´ã€‚ 12345index2 = torch.LongTensor([[1],[2],[3],[4]])print(index2.shape) torch.Size([4, 1]) 123a.gather(dim=1, index=index2) tensor([[ 1.], [ 7.], [ 13.], [ 19.]]) list è½¬æ¢æˆ one-hot å‘é‡1234567891011### list è½¬æ¢æˆ one-hot å‘é‡label = [1, 2, 3, 4, 5]label = torch.LongTensor(label).view(-1, 1)one_hot = torch.zeros(5, 10).scatter_(dim=1, index=label, value=1)one_hot tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]) Tensor ç±»å‹Tensoræœ‰ä¸åŒçš„æ•°æ®ç±»å‹ï¼Œå¦‚è¡¨3-3æ‰€ç¤ºï¼Œæ¯ç§ç±»å‹åˆ†åˆ«å¯¹åº”æœ‰CPUå’ŒGPUç‰ˆæœ¬(HalfTensoré™¤å¤–)ã€‚é»˜è®¤çš„tensoræ˜¯FloatTensorï¼Œå¯é€šè¿‡t.set_default_tensor_type æ¥ä¿®æ”¹é»˜è®¤tensorç±»å‹(å¦‚æœé»˜è®¤ç±»å‹ä¸ºGPU tensorï¼Œåˆ™æ‰€æœ‰æ“ä½œéƒ½å°†åœ¨GPUä¸Šè¿›è¡Œ)ã€‚Tensorçš„ç±»å‹å¯¹åˆ†æå†…å­˜å ç”¨å¾ˆæœ‰å¸®åŠ©ã€‚ä¾‹å¦‚å¯¹äºä¸€ä¸ªsizeä¸º(1000, 1000, 1000)çš„FloatTensorï¼Œå®ƒæœ‰1000*1000*1000=10^9ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ å 32bit/8 = 4Byteå†…å­˜ï¼Œæ‰€ä»¥å…±å å¤§çº¦4GBå†…å­˜/æ˜¾å­˜ã€‚HalfTensoræ˜¯ä¸“é—¨ä¸ºGPUç‰ˆæœ¬è®¾è®¡çš„ï¼ŒåŒæ ·çš„å…ƒç´ ä¸ªæ•°ï¼Œæ˜¾å­˜å ç”¨åªæœ‰FloatTensorçš„ä¸€åŠï¼Œæ‰€ä»¥å¯ä»¥æå¤§ç¼“è§£GPUæ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œä½†ç”±äºHalfTensoræ‰€èƒ½è¡¨ç¤ºçš„æ•°å€¼å¤§å°å’Œç²¾åº¦æœ‰é™[^2]ï¼Œæ‰€ä»¥å¯èƒ½å‡ºç°æº¢å‡ºç­‰é—®é¢˜ã€‚ ^2: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste è¡¨3-3: tensoræ•°æ®ç±»å‹ æ•°æ®ç±»å‹| CPU tensor |GPU tensor| :â€”:|:â€”:|:â€“:| 32-bit æµ®ç‚¹| torch.FloatTensor |torch.cuda.FloatTensor 64-bit æµ®ç‚¹| torch.DoubleTensor| torch.cuda.DoubleTensor 16-bit åŠç²¾åº¦æµ®ç‚¹| N/A |torch.cuda.HalfTensor 8-bit æ— ç¬¦å·æ•´å½¢(0~255)| torch.ByteTensor| torch.cuda.ByteTensor 8-bit æœ‰ç¬¦å·æ•´å½¢(-128~127)| torch.CharTensor |torch.cuda.CharTensor 16-bit æœ‰ç¬¦å·æ•´å½¢ | torch.ShortTensor| torch.cuda.ShortTensor 32-bit æœ‰ç¬¦å·æ•´å½¢ |torch.IntTensor |torch.cuda.IntTensor 64-bit æœ‰ç¬¦å·æ•´å½¢ |torch.LongTensor |torch.cuda.LongTensor å„æ•°æ®ç±»å‹ä¹‹é—´å¯ä»¥äº’ç›¸è½¬æ¢ï¼Œtype(new_type)æ˜¯é€šç”¨çš„åšæ³•ï¼ŒåŒæ—¶è¿˜æœ‰floatã€longã€halfç­‰å¿«æ·æ–¹æ³•ã€‚CPU tensorä¸GPU tensorä¹‹é—´çš„äº’ç›¸è½¬æ¢é€šè¿‡tensor.cudaå’Œtensor.cpuæ–¹æ³•å®ç°ã€‚Tensorè¿˜æœ‰ä¸€ä¸ªnewæ–¹æ³•ï¼Œç”¨æ³•ä¸t.Tensorä¸€æ ·ï¼Œä¼šè°ƒç”¨è¯¥tensorå¯¹åº”ç±»å‹çš„æ„é€ å‡½æ•°ï¼Œç”Ÿæˆä¸å½“å‰tensorç±»å‹ä¸€è‡´çš„tensorã€‚ torch.set_sefault_tensor_type(â€˜torch.IntTensor) é€å…ƒç´ æ“ä½œè¿™éƒ¨åˆ†æ“ä½œä¼šå¯¹tensorçš„æ¯ä¸€ä¸ªå…ƒç´ (point-wiseï¼Œåˆåelement-wise)è¿›è¡Œæ“ä½œï¼Œæ­¤ç±»æ“ä½œçš„è¾“å…¥ä¸è¾“å‡ºå½¢çŠ¶ä¸€è‡´ã€‚å¸¸ç”¨çš„æ“ä½œå¦‚è¡¨3-4æ‰€ç¤ºã€‚ è¡¨3-4: å¸¸è§çš„é€å…ƒç´ æ“ä½œ |å‡½æ•°|åŠŸèƒ½| |:â€“:|:â€“:| |abs/sqrt/div/exp/fmod/log/pow..|ç»å¯¹å€¼/å¹³æ–¹æ ¹/é™¤æ³•/æŒ‡æ•°/æ±‚ä½™/æ±‚å¹‚..| |cos/sin/asin/atan2/cosh..|ç›¸å…³ä¸‰è§’å‡½æ•°| |ceil/round/floor/trunc| ä¸Šå–æ•´/å››èˆäº”å…¥/ä¸‹å–æ•´/åªä¿ç•™æ•´æ•°éƒ¨åˆ†| |clamp(input, min, max)|è¶…è¿‡minå’Œmaxéƒ¨åˆ†æˆªæ–­| |sigmod/tanh..|æ¿€æ´»å‡½æ•° å¯¹äºå¾ˆå¤šæ“ä½œï¼Œä¾‹å¦‚divã€mulã€powã€fmodç­‰ï¼ŒPyTorchéƒ½å®ç°äº†è¿ç®—ç¬¦é‡è½½ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥ä½¿ç”¨è¿ç®—ç¬¦ã€‚å¦‚a ** 2 ç­‰ä»·äºtorch.pow(a,2), a * 2ç­‰ä»·äºtorch.mul(a,2)ã€‚ å…¶ä¸­clamp(x, min, max)çš„è¾“å‡ºæ»¡è¶³ä»¥ä¸‹å…¬å¼ï¼š $$ y_i = \\begin{cases} min, &amp; \\text{if } x_i \\lt min \\ x_i, &amp; \\text{if } min \\le x_i \\le max \\ max, &amp; \\text{if } x_i \\gt max\\ \\end{cases} $$ clampå¸¸ç”¨åœ¨æŸäº›éœ€è¦æ¯”è¾ƒå¤§å°çš„åœ°æ–¹ï¼Œå¦‚å–ä¸€ä¸ªtensorçš„æ¯ä¸ªå…ƒç´ ä¸å¦ä¸€ä¸ªæ•°çš„è¾ƒå¤§å€¼ã€‚ 123456789import torcha = torch.arange(0,6).view(2,3)print(a)torch.clamp(a, min=3, max=5) tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) tensor([[ 3., 3., 3.], [ 3., 4., 5.]]) å½’å¹¶æ“ä½œæ­¤ç±»æ“ä½œä¼šä½¿è¾“å‡ºå½¢çŠ¶å°äºè¾“å…¥å½¢çŠ¶ï¼Œå¹¶å¯ä»¥æ²¿ç€æŸä¸€ç»´åº¦è¿›è¡ŒæŒ‡å®šæ“ä½œã€‚å¦‚åŠ æ³•sumï¼Œæ—¢å¯ä»¥è®¡ç®—æ•´ä¸ªtensorçš„å’Œï¼Œä¹Ÿå¯ä»¥è®¡ç®—tensorä¸­æ¯ä¸€è¡Œæˆ–æ¯ä¸€åˆ—çš„å’Œã€‚å¸¸ç”¨çš„å½’å¹¶æ“ä½œå¦‚è¡¨3-5æ‰€ç¤ºã€‚ è¡¨3-5: å¸¸ç”¨å½’å¹¶æ“ä½œ |å‡½æ•°|åŠŸèƒ½| |:â€”:|:â€”:| |mean/sum/median/mode|å‡å€¼/å’Œ/ä¸­ä½æ•°/ä¼—æ•°| |norm/dist|èŒƒæ•°/è·ç¦»| |std/var|æ ‡å‡†å·®/æ–¹å·®| |cumsum/cumprod|ç´¯åŠ /ç´¯ä¹˜| ä»¥ä¸Šå¤§å¤šæ•°å‡½æ•°éƒ½æœ‰ä¸€ä¸ªå‚æ•° **dim**ï¼Œç”¨æ¥æŒ‡å®šè¿™äº›æ“ä½œæ˜¯åœ¨å“ªä¸ªç»´åº¦ä¸Šæ‰§è¡Œçš„ã€‚å…³äºdim(å¯¹åº”äºNumpyä¸­çš„axis)çš„è§£é‡Šä¼—è¯´çº·çº­ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªç®€å•çš„è®°å¿†æ–¹å¼ï¼š å‡è®¾è¾“å…¥çš„å½¢çŠ¶æ˜¯(m, n, k) å¦‚æœæŒ‡å®šdim=0ï¼Œè¾“å‡ºçš„å½¢çŠ¶å°±æ˜¯(1, n, k)æˆ–è€…(n, k) å¦‚æœæŒ‡å®šdim=1ï¼Œè¾“å‡ºçš„å½¢çŠ¶å°±æ˜¯(m, 1, k)æˆ–è€…(m, k) å¦‚æœæŒ‡å®šdim=2ï¼Œè¾“å‡ºçš„å½¢çŠ¶å°±æ˜¯(m, n, 1)æˆ–è€…(m, n) sizeä¸­æ˜¯å¦æœ‰â€1â€ï¼Œå–å†³äºå‚æ•°keepdimï¼Œkeepdim=Trueä¼šä¿ç•™ç»´åº¦1ã€‚æ³¨æ„ï¼Œä»¥ä¸Šåªæ˜¯ç»éªŒæ€»ç»“ï¼Œå¹¶éæ‰€æœ‰å‡½æ•°éƒ½ç¬¦åˆè¿™ç§å½¢çŠ¶å˜åŒ–æ–¹å¼ï¼Œå¦‚cumsumã€‚ 12345a = torch.arange(0, 6).view(2,3)a tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 123a.norm(dim=0, p=1), a.norm(dim=0, p=2), a.norm(dim=0, p=3) (tensor([ 3., 5., 7.]), tensor([ 3.0000, 4.1231, 5.3852]), tensor([ 3.0000, 4.0207, 5.1045])) 123torch.norm?? $||x||{p} = \\sqrt[p]{x{1}^{p} + x_{2}^{p} + \\ldots + x_{N}^{p}}$ torch.dist??dist(input, other, p=2) -&gt; Tensor Returns the p-norm of (:attr:input - :attr:other) 123torch.dist(torch.ones(4), torch.zeros(4), 2) tensor(2.) 123torch.var(torch.randn(10,3), dim=0) tensor([ 0.7617, 1.0060, 1.6778]) æ¯”è¾ƒæ¯”è¾ƒå‡½æ•°ä¸­æœ‰ä¸€äº›æ˜¯é€å…ƒç´ æ¯”è¾ƒï¼Œæ“ä½œç±»ä¼¼äºé€å…ƒç´ æ“ä½œï¼Œè¿˜æœ‰ä¸€äº›åˆ™ç±»ä¼¼äºå½’å¹¶æ“ä½œã€‚å¸¸ç”¨æ¯”è¾ƒå‡½æ•°å¦‚è¡¨3-6æ‰€ç¤ºã€‚ è¡¨3-6: å¸¸ç”¨æ¯”è¾ƒå‡½æ•° |å‡½æ•°|åŠŸèƒ½| |:â€“:|:â€“:| |gt/lt/ge/le/eq/ne|å¤§äº/å°äº/å¤§äºç­‰äº/å°äºç­‰äº/ç­‰äº/ä¸ç­‰| |topk|æœ€å¤§çš„kä¸ªæ•°| |sort|æ’åº| |max/min|æ¯”è¾ƒä¸¤ä¸ªtensoræœ€å¤§æœ€å°å€¼| è¡¨ä¸­ç¬¬ä¸€è¡Œçš„æ¯”è¾ƒæ“ä½œå·²ç»å®ç°äº†è¿ç®—ç¬¦é‡è½½ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨a&gt;=bã€a&gt;bã€a!=bã€a==bï¼Œå…¶è¿”å›ç»“æœæ˜¯ä¸€ä¸ªByteTensorï¼Œå¯ç”¨æ¥é€‰å–å…ƒç´ ã€‚max/minè¿™ä¸¤ä¸ªæ“ä½œæ¯”è¾ƒç‰¹æ®Šï¼Œä»¥maxæ¥è¯´ï¼Œå®ƒæœ‰ä»¥ä¸‹ä¸‰ç§ä½¿ç”¨æƒ…å†µï¼š t.max(tensor)ï¼šè¿”å›tensorä¸­æœ€å¤§çš„ä¸€ä¸ªæ•° t.max(tensor,dim)ï¼šæŒ‡å®šç»´ä¸Šæœ€å¤§çš„æ•°ï¼Œè¿”å›tensorå’Œä¸‹æ ‡ t.max(tensor1, tensor2): æ¯”è¾ƒä¸¤ä¸ªtensorç›¸æ¯”è¾ƒå¤§çš„å…ƒç´  è‡³äºæ¯”è¾ƒä¸€ä¸ªtensorå’Œä¸€ä¸ªæ•°ï¼Œå¯ä»¥ä½¿ç”¨clampå‡½æ•°ã€‚ä¸‹é¢ä¸¾ä¾‹è¯´æ˜ã€‚ max/min sort topk 12345a = torch.rand(3,4)a tensor([[ 0.1845, 0.4101, 0.1470, 0.0083], [ 0.7520, 0.8871, 0.9494, 0.2504], [ 0.3879, 0.4554, 0.4080, 0.1703]]) 123torch.max(a, dim=0) (tensor([ 0.7326, 0.6784, 0.9791, 0.9011]), tensor([ 1, 2, 1, 1])) 123a.sort(dim=0) (tensor([[ 0.1424, 0.5681, 0.1833, 0.1654], [ 0.4556, 0.6418, 0.3242, 0.5120], [ 0.7326, 0.6784, 0.9791, 0.9011]]), tensor([[ 2, 0, 0, 2], [ 0, 1, 2, 0], [ 1, 2, 1, 1]])) 123a.topk(k=2, dim=0) (tensor([[ 0.7326, 0.6784, 0.9791, 0.9011], [ 0.4556, 0.6418, 0.3242, 0.5120]]), tensor([[ 1, 2, 1, 1], [ 0, 1, 2, 0]])) çº¿æ€§ä»£æ•°PyTorchçš„çº¿æ€§å‡½æ•°ä¸»è¦å°è£…äº†Blaså’ŒLapackï¼Œå…¶ç”¨æ³•å’Œæ¥å£éƒ½ä¸ä¹‹ç±»ä¼¼ã€‚å¸¸ç”¨çš„çº¿æ€§ä»£æ•°å‡½æ•°å¦‚è¡¨3-7æ‰€ç¤ºã€‚ è¡¨3-7: å¸¸ç”¨çš„çº¿æ€§ä»£æ•°å‡½æ•° |å‡½æ•°|åŠŸèƒ½| |:â€”:|:â€”:| |trace|å¯¹è§’çº¿å…ƒç´ ä¹‹å’Œ(çŸ©é˜µçš„è¿¹)| |diag|å¯¹è§’çº¿å…ƒç´ | |triu/tril|çŸ©é˜µçš„ä¸Šä¸‰è§’/ä¸‹ä¸‰è§’ï¼Œå¯æŒ‡å®šåç§»é‡| |mm/bmm|çŸ©é˜µä¹˜æ³•ï¼Œbatchçš„çŸ©é˜µä¹˜æ³•| |addmm/addbmm/addmv/addr/badbmm..|çŸ©é˜µè¿ç®— |t|è½¬ç½®| |dot/cross|å†…ç§¯/å¤–ç§¯ |inverse|æ±‚é€†çŸ©é˜µ |svd|å¥‡å¼‚å€¼åˆ†è§£ å…·ä½“ä½¿ç”¨è¯´æ˜è¯·å‚è§å®˜æ–¹æ–‡æ¡£^3ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒçŸ©é˜µçš„è½¬ç½®ä¼šå¯¼è‡´å­˜å‚¨ç©ºé—´ä¸è¿ç»­ï¼Œéœ€è°ƒç”¨å®ƒçš„.contiguousæ–¹æ³•å°†å…¶è½¬ä¸ºè¿ç»­ã€‚ 1234567b.contiguous(), b.size()b.contiguous().is_contiguous()print(a.matmul(b.contiguous())) tensor([[ 0.8260, 1.3392, 0.5944], [ 1.3392, 2.7192, 1.0062], [ 0.5944, 1.0062, 0.6130]]) 123456789b = a.t()print(a.size(), b.shape)print(a.mm(b))b.is_contiguous() torch.Size([3, 4]) torch.Size([4, 3]) tensor([[ 0.8260, 1.3392, 0.5944], [ 1.3392, 2.7192, 1.0062], [ 0.5944, 1.0062, 0.6130]]) False 123b, b.diag() (tensor([[ 0.4556, 0.7326, 0.1424], [ 0.5681, 0.6418, 0.6784], [ 0.1833, 0.9791, 0.3242], [ 0.5120, 0.9011, 0.1654]]), tensor([ 0.4556, 0.6418, 0.3242])) 12345a = torch.randn(5,5)a.triu(1) tensor([[ 0.0000, 1.5959, -0.2253, 0.2349, -0.5151], [ 0.0000, 0.0000, -0.0366, -0.0867, 0.2737], [ 0.0000, 0.0000, 0.0000, 0.9904, -1.4889], [ 0.0000, 0.0000, 0.0000, 0.0000, -1.1053], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) Tensorå’ŒNumpyTensorå’ŒNumpyæ•°ç»„ä¹‹é—´å…·æœ‰å¾ˆé«˜çš„ç›¸ä¼¼æ€§ï¼Œå½¼æ­¤ä¹‹é—´çš„äº’æ“ä½œä¹Ÿéå¸¸ç®€å•é«˜æ•ˆã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒNumpyå’ŒTensorå…±äº«å†…å­˜ã€‚ç”±äºNumpyå†å²æ‚ ä¹…ï¼Œæ”¯æŒä¸°å¯Œçš„æ“ä½œï¼Œæ‰€ä»¥å½“é‡åˆ°Tensorä¸æ”¯æŒçš„æ“ä½œæ—¶ï¼Œå¯å…ˆè½¬æˆNumpyæ•°ç»„ï¼Œå¤„ç†åå†è½¬å›tensorï¼Œå…¶è½¬æ¢å¼€é”€å¾ˆå°ã€‚ æ³¨æ„ï¼š å½“numpyçš„æ•°æ®ç±»å‹å’ŒTensorçš„ç±»å‹ä¸ä¸€æ ·çš„æ—¶å€™ï¼Œæ•°æ®ä¼šè¢«å¤åˆ¶ï¼Œä¸ä¼šå…±äº«å†…å­˜ã€‚ 123456789import numpy as npa = np.ones([2,3])print(a.dtype)a float64 array([[1., 1., 1.], [1., 1., 1.]]) 1234567b = torch.Tensor(a)print(b.type())b torch.FloatTensor tensor([[ 1., 100., 1.], [ 1., 1., 1.]]) 123torch.from_numpy?? 1234567c = torch.from_numpy(a)print(c.type())c torch.DoubleTensor tensor([[ 1., 1., 1.], [ 1., 1., 1.]], dtype=torch.float64) 12345a[0,1] = 100b # bä¸aä¸é€šå‘å†…å­˜ï¼Œæ‰€ä»¥å³ä½¿aæ”¹å˜äº†ï¼Œbä¹Ÿä¸å˜ tensor([[ 1., 1., 1.], [ 1., 1., 1.]]) 123c # c ä¸ a å…±äº«å†…å­˜ tensor([[ 1., 100., 1.], [ 1., 1., 1.]], dtype=torch.float64) BroadCastå¹¿æ’­æ³•åˆ™(broadcast)æ˜¯ç§‘å­¦è¿ç®—ä¸­ç»å¸¸ä½¿ç”¨çš„ä¸€ä¸ªæŠ€å·§ï¼Œå®ƒåœ¨å¿«é€Ÿæ‰§è¡Œå‘é‡åŒ–çš„åŒæ—¶ä¸ä¼šå ç”¨é¢å¤–çš„å†…å­˜/æ˜¾å­˜ã€‚ Numpyçš„å¹¿æ’­æ³•åˆ™å®šä¹‰å¦‚ä¸‹ï¼š è®©æ‰€æœ‰è¾“å…¥æ•°ç»„éƒ½å‘å…¶ä¸­shapeæœ€é•¿çš„æ•°ç»„çœ‹é½ï¼Œshapeä¸­ä¸è¶³çš„éƒ¨åˆ†é€šè¿‡åœ¨å‰é¢åŠ 1è¡¥é½ ä¸¤ä¸ªæ•°ç»„è¦ä¹ˆåœ¨æŸä¸€ä¸ªç»´åº¦çš„é•¿åº¦ä¸€è‡´ï¼Œè¦ä¹ˆå…¶ä¸­ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™ä¸èƒ½è®¡ç®— å½“è¾“å…¥æ•°ç»„çš„æŸä¸ªç»´åº¦çš„é•¿åº¦ä¸º1æ—¶ï¼Œè®¡ç®—æ—¶æ²¿æ­¤ç»´åº¦å¤åˆ¶æ‰©å……æˆä¸€æ ·çš„å½¢çŠ¶ PyTorchå½“å‰å·²ç»æ”¯æŒäº†è‡ªåŠ¨å¹¿æ’­æ³•åˆ™ï¼Œä½†æ˜¯ç¬”è€…è¿˜æ˜¯å»ºè®®è¯»è€…é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªå‡½æ•°çš„ç»„åˆæ‰‹åŠ¨å®ç°å¹¿æ’­æ³•åˆ™ï¼Œè¿™æ ·æ›´ç›´è§‚ï¼Œæ›´ä¸æ˜“å‡ºé”™ï¼š unsqueezeæˆ–è€…viewï¼šä¸ºæ•°æ®æŸä¸€ç»´çš„å½¢çŠ¶è¡¥1ï¼Œå®ç°æ³•åˆ™1 expandæˆ–è€…expand_asï¼Œé‡å¤æ•°ç»„ï¼Œå®ç°æ³•åˆ™3ï¼›è¯¥æ“ä½œä¸ä¼šå¤åˆ¶æ•°ç»„ï¼Œæ‰€ä»¥ä¸ä¼šå ç”¨é¢å¤–çš„ç©ºé—´ã€‚ æ³¨æ„ï¼Œrepeatå®ç°ä¸expandç›¸ç±»ä¼¼çš„åŠŸèƒ½ï¼Œä½†æ˜¯repeatä¼šæŠŠç›¸åŒæ•°æ®å¤åˆ¶å¤šä»½ï¼Œå› æ­¤ä¼šå ç”¨é¢å¤–çš„ç©ºé—´ã€‚ 12345a = torch.ones(3, 2)b = torch.zeros(2, 3, 1) 12345678910111213# è‡ªåŠ¨å¹¿æ’­æ³•åˆ™# ç¬¬ä¸€æ­¥ï¼šaæ˜¯2ç»´,bæ˜¯3ç»´ï¼Œæ‰€ä»¥å…ˆåœ¨è¾ƒå°çš„aå‰é¢è¡¥1 ï¼Œ# å³ï¼ša.unsqueeze(0)ï¼Œaçš„å½¢çŠ¶å˜æˆï¼ˆ1ï¼Œ3ï¼Œ2ï¼‰ï¼Œbçš„å½¢çŠ¶æ˜¯ï¼ˆ2ï¼Œ3ï¼Œ1ï¼‰,# ç¬¬äºŒæ­¥: aå’Œbåœ¨ç¬¬ä¸€ç»´å’Œç¬¬ä¸‰ç»´å½¢çŠ¶ä¸ä¸€æ ·ï¼Œå…¶ä¸­ä¸€ä¸ªä¸º1 ï¼Œ# å¯ä»¥åˆ©ç”¨å¹¿æ’­æ³•åˆ™æ‰©å±•ï¼Œä¸¤ä¸ªå½¢çŠ¶éƒ½å˜æˆäº†ï¼ˆ2ï¼Œ3ï¼Œ2ï¼‰a+b tensor([[[ 1., 1.], [ 1., 1.], [ 1., 1.]], [[ 1., 1.], [ 1., 1.], [ 1., 1.]]]) 123a.unsqueeze(0).expand(2,3,2) + b.expand(2,3,2) tensor([[[ 1., 1.], [ 1., 1.], [ 1., 1.]], [[ 1., 1.], [ 1., 1.], [ 1., 1.]]]) 12345# expandä¸ä¼šå ç”¨é¢å¤–ç©ºé—´ï¼Œåªä¼šåœ¨éœ€è¦çš„æ—¶å€™æ‰æ‰©å……ï¼Œå¯æå¤§èŠ‚çœå†…å­˜e = a.unsqueeze(0).expand(10000000000000, 3,2) å†…éƒ¨ç»“æ„tensorçš„æ•°æ®ç»“æ„å¦‚å›¾3-1æ‰€ç¤ºã€‚tensoråˆ†ä¸ºå¤´ä¿¡æ¯åŒº(Tensor)å’Œå­˜å‚¨åŒº(Storage)ï¼Œä¿¡æ¯åŒºä¸»è¦ä¿å­˜ç€tensorçš„å½¢çŠ¶ï¼ˆsizeï¼‰ã€æ­¥é•¿ï¼ˆstrideï¼‰ã€æ•°æ®ç±»å‹ï¼ˆtypeï¼‰ç­‰ä¿¡æ¯ï¼Œè€ŒçœŸæ­£çš„æ•°æ®åˆ™ä¿å­˜æˆè¿ç»­æ•°ç»„ã€‚ç”±äºæ•°æ®åŠ¨è¾„æˆåƒä¸Šä¸‡ï¼Œå› æ­¤ä¿¡æ¯åŒºå…ƒç´ å ç”¨å†…å­˜è¾ƒå°‘ï¼Œä¸»è¦å†…å­˜å ç”¨åˆ™å–å†³äºtensorä¸­å…ƒç´ çš„æ•°ç›®ï¼Œä¹Ÿå³å­˜å‚¨åŒºçš„å¤§å°ã€‚ ä¸€èˆ¬æ¥è¯´ä¸€ä¸ªtensoræœ‰ç€ä¸ä¹‹ç›¸å¯¹åº”çš„storage, storageæ˜¯åœ¨dataä¹‹ä¸Šå°è£…çš„æ¥å£ï¼Œä¾¿äºä½¿ç”¨ï¼Œè€Œä¸åŒtensorçš„å¤´ä¿¡æ¯ä¸€èˆ¬ä¸åŒï¼Œä½†å´å¯èƒ½ä½¿ç”¨ç›¸åŒçš„æ•°æ®ã€‚ä¸‹é¢çœ‹ä¸¤ä¸ªä¾‹å­ã€‚ 12345a = torch.arange(0,6)a tensor([ 0., 1., 2., 3., 4., 5.]) 123a.storage() 0.0 1.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 12345b = a.view(2, 3)b.storage() 0.0 1.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 1234567# ä¸€ä¸ªå¯¹è±¡çš„idå€¼å¯ä»¥çœ‹ä½œå®ƒåœ¨å†…å­˜ä¸­çš„åœ°å€# storageçš„å†…å­˜åœ°å€ä¸€æ ·ï¼Œå³æ˜¯åŒä¸€ä¸ªstorageid(b.storage()) == id(a.storage()) True 12345c = torch.arange(0, 6)c.storage() 0.0 1.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 1234567# ä¸€ä¸ªå¯¹è±¡çš„idå€¼å¯ä»¥çœ‹ä½œå®ƒåœ¨å†…å­˜ä¸­çš„åœ°å€# storageçš„å†…å­˜åœ°å€ä¸€æ ·ï¼Œå³æ˜¯åŒä¸€ä¸ªstorageid(c.storage()) == id(a.storage()) True 1234567# aæ”¹å˜ï¼Œbä¹Ÿéšä¹‹æ”¹å˜ï¼Œå› ä¸ºä»–ä»¬å…±äº«storage, ä½†æ˜¯ c æ²¡æœ‰æ”¹å˜å•Šï¼Œå¾ˆç¥å¥‡a[1] = 100b, c (tensor([[ 0., 100., 2.], [ 3., 4., 5.]]), tensor([ 0., 1., 2., 3., 4., 5.])) 1234567# ä¸€ä¸ªå¯¹è±¡çš„idå€¼å¯ä»¥çœ‹ä½œå®ƒåœ¨å†…å­˜ä¸­çš„åœ°å€# storageçš„å†…å­˜åœ°å€ä¸€æ ·ï¼Œå³æ˜¯åŒä¸€ä¸ªstorageid(c[1].storage()), id(c.storage()) (139719200619016, 139719200619016) 1234567c = a[2:]print(c)c.storage() tensor([ 2., 3., 4., 5.]) 0.0 100.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 12345c.data_ptr(), a.data_ptr() # data_ptrè¿”å›tensoré¦–å…ƒç´ çš„å†…å­˜åœ°å€# å¯ä»¥çœ‹å‡ºç›¸å·®8ï¼Œè¿™æ˜¯å› ä¸º2*4=8--ç›¸å·®ä¸¤ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ å 4ä¸ªå­—èŠ‚(float) (94551854283064, 94551854283056) 12345c[0]=-100 # c[0]çš„å†…å­˜åœ°å€å¯¹åº” a[2] çš„å†…å­˜åœ°å€a tensor([ 0., 100., -100., 3., 4., 5.]) 1234567d = torch.Tensor(c.storage())d[0] = 6666b tensor([[ 6666., 100., -100.], [ 3., 4., 5.]]) 12345# ä¸‹é¢ï¼”ä¸ªtensorå…±äº«storageid(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage()) True 123a.storage_offset(), c.storage_offset(), d.storage_offset() (0, 2, 0) 12345e = b[::2, ::2] # éš”2è¡Œ/åˆ—å–ä¸€ä¸ªå…ƒç´ id(e.storage()) == id(a.storage()) True 123e.is_contiguous() False å¯è§ç»å¤§å¤šæ•°æ“ä½œå¹¶ä¸ä¿®æ”¹tensorçš„æ•°æ®ï¼Œè€Œåªæ˜¯ä¿®æ”¹äº†tensorçš„å¤´ä¿¡æ¯ã€‚è¿™ç§åšæ³•æ›´èŠ‚çœå†…å­˜ï¼ŒåŒæ—¶æå‡äº†å¤„ç†é€Ÿåº¦ã€‚åœ¨ä½¿ç”¨ä¸­éœ€è¦æ³¨æ„ã€‚ æ­¤å¤–æœ‰äº›æ“ä½œä¼šå¯¼è‡´tensorä¸è¿ç»­ï¼Œè¿™æ—¶éœ€è°ƒç”¨tensor.contiguousæ–¹æ³•å°†å®ƒä»¬å˜æˆè¿ç»­çš„æ•°æ®ï¼Œè¯¥æ–¹æ³•ä¼šä½¿æ•°æ®å¤åˆ¶ä¸€ä»½ï¼Œä¸å†ä¸åŸæ¥çš„æ•°æ®å…±äº«storageã€‚ å¦å¤–è¯»è€…å¯ä»¥æ€è€ƒä¸€ä¸‹ï¼Œä¹‹å‰è¯´è¿‡çš„é«˜çº§ç´¢å¼•ä¸€èˆ¬ä¸å…±äº«stroageï¼Œè€Œæ™®é€šç´¢å¼•å…±äº«storageï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿï¼ˆæç¤ºï¼šæ™®é€šç´¢å¼•å¯ä»¥é€šè¿‡åªä¿®æ”¹tensorçš„offsetï¼Œstrideå’Œsizeï¼Œè€Œä¸ä¿®æ”¹storageæ¥å®ç°ï¼‰ã€‚ æŒä¹…åŒ–Tensorçš„ä¿å­˜å’ŒåŠ è½½ååˆ†çš„ç®€å•ï¼Œä½¿ç”¨t.saveå’Œt.loadå³å¯å®Œæˆç›¸åº”çš„åŠŸèƒ½ã€‚åœ¨save/loadæ—¶å¯æŒ‡å®šä½¿ç”¨çš„pickleæ¨¡å—ï¼Œåœ¨loadæ—¶è¿˜å¯å°†GPU tensoræ˜ å°„åˆ°CPUæˆ–å…¶å®ƒGPUä¸Šã€‚ 123456789101112131415161718192021if torch.cuda.is_available(): a = a.cuda() # æŠŠaè½¬ä¸ºGPU1ä¸Šçš„tensor, torch.save(a,'a.pth') # åŠ è½½ä¸ºb, å­˜å‚¨äºGPU1ä¸Š(å› ä¸ºä¿å­˜æ—¶tensorå°±åœ¨GPU1ä¸Š) b = torch.load('a.pth') # åŠ è½½ä¸ºc, å­˜å‚¨äºCPU c = torch.load('a.pth', map_location=lambda storage, loc: storage) # åŠ è½½ä¸ºd, å­˜å‚¨äºGPU0ä¸Š d = torch.load('a.pth', map_location={'cuda:1':'cuda:0'}) 12345a = torch.load(&quot;a.pth&quot;)print(a) tensor([ 6666., 100., -100., 3., 4., 5.], device='cuda:0') å‘é‡åŒ–å‘é‡åŒ–è®¡ç®—æ˜¯ä¸€ç§ç‰¹æ®Šçš„å¹¶è¡Œè®¡ç®—æ–¹å¼ï¼Œç›¸å¯¹äºä¸€èˆ¬ç¨‹åºåœ¨åŒä¸€æ—¶é—´åªæ‰§è¡Œä¸€ä¸ªæ“ä½œçš„æ–¹å¼ï¼Œå®ƒå¯åœ¨åŒä¸€æ—¶é—´æ‰§è¡Œå¤šä¸ªæ“ä½œï¼Œé€šå¸¸æ˜¯å¯¹ä¸åŒçš„æ•°æ®æ‰§è¡ŒåŒæ ·çš„ä¸€ä¸ªæˆ–ä¸€æ‰¹æŒ‡ä»¤ï¼Œæˆ–è€…è¯´æŠŠæŒ‡ä»¤åº”ç”¨äºä¸€ä¸ªæ•°ç»„/å‘é‡ä¸Šã€‚å‘é‡åŒ–å¯æå¤§æé«˜ç§‘å­¦è¿ç®—çš„æ•ˆç‡ï¼ŒPythonæœ¬èº«æ˜¯ä¸€é—¨é«˜çº§è¯­è¨€ï¼Œä½¿ç”¨å¾ˆæ–¹ä¾¿ï¼Œä½†è¿™ä¹Ÿæ„å‘³ç€å¾ˆå¤šæ“ä½œå¾ˆä½æ•ˆï¼Œå°¤å…¶æ˜¯forå¾ªç¯ã€‚åœ¨ç§‘å­¦è®¡ç®—ç¨‹åºä¸­åº”å½“æåŠ›é¿å…ä½¿ç”¨PythonåŸç”Ÿçš„forå¾ªç¯ã€‚ 1234567891011def for_loop_add(x, y): result = [] for i,j in zip(x, y): result.append(i + j) return torch.Tensor(result) 123456789x = torch.zeros(100)y = torch.ones(100)%timeit -n 10 for_loop_add(x, y)%timeit -n 10 x + y 351 Âµs Â± 9.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each) The slowest run took 16.46 times longer than the fastest. This could mean that an intermediate result is being cached. 4.24 Âµs Â± 7.12 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each) å¯è§äºŒè€…æœ‰è¶…è¿‡40å€çš„é€Ÿåº¦å·®è·ï¼Œå› æ­¤åœ¨å®é™…ä½¿ç”¨ä¸­åº”å°½é‡è°ƒç”¨å†…å»ºå‡½æ•°(buildin-function)ï¼Œè¿™äº›å‡½æ•°åº•å±‚ç”±C/C++å®ç°ï¼Œèƒ½é€šè¿‡æ‰§è¡Œåº•å±‚ä¼˜åŒ–å®ç°é«˜æ•ˆè®¡ç®—ã€‚å› æ­¤åœ¨å¹³æ—¶å†™ä»£ç æ—¶ï¼Œå°±åº”å…»æˆå‘é‡åŒ–çš„æ€ç»´ä¹ æƒ¯ã€‚ æ­¤å¤–è¿˜æœ‰ä»¥ä¸‹å‡ ç‚¹éœ€è¦æ³¨æ„ï¼š å¤§å¤šæ•°torch.functionéƒ½æœ‰ä¸€ä¸ªå‚æ•°outï¼Œè¿™æ—¶å€™äº§ç”Ÿçš„ç»“æœå°†ä¿å­˜åœ¨outæŒ‡å®štensorä¹‹ä¸­ã€‚ torch.set_num_threadså¯ä»¥è®¾ç½®PyTorchè¿›è¡ŒCPUå¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—æ—¶å€™æ‰€å ç”¨çš„çº¿ç¨‹æ•°ï¼Œè¿™ä¸ªå¯ä»¥ç”¨æ¥é™åˆ¶PyTorchæ‰€å ç”¨çš„CPUæ•°ç›®ã€‚ torch.set_printoptionså¯ä»¥ç”¨æ¥è®¾ç½®æ‰“å°tensoræ—¶çš„æ•°å€¼ç²¾åº¦å’Œæ ¼å¼ã€‚ ä¸‹é¢ä¸¾ä¾‹è¯´æ˜ã€‚ 1234567a = torch.randn(2,3)torch.set_printoptions(precision=10)a tensor([[-0.3306640089, -0.0507176071, -0.4223535955], [-0.8678948879, -0.0437202156, 0.0183448847]]) çº¿æ€§å›å½’çº¿æ€§å›å½’æ˜¯æœºå™¨å­¦ä¹ å…¥é—¨çŸ¥è¯†ï¼Œåº”ç”¨ååˆ†å¹¿æ³›ã€‚çº¿æ€§å›å½’åˆ©ç”¨æ•°ç†ç»Ÿè®¡ä¸­å›å½’åˆ†æï¼Œæ¥ç¡®å®šä¸¤ç§æˆ–ä¸¤ç§ä»¥ä¸Šå˜é‡é—´ç›¸äº’ä¾èµ–çš„å®šé‡å…³ç³»çš„ï¼Œå…¶è¡¨è¾¾å½¢å¼ä¸º$y = wx+b+e$ï¼Œ$e$ä¸ºè¯¯å·®æœä»å‡å€¼ä¸º0çš„æ­£æ€åˆ†å¸ƒã€‚é¦–å…ˆè®©æˆ‘ä»¬æ¥ç¡®è®¤çº¿æ€§å›å½’çš„æŸå¤±å‡½æ•°ï¼š $$ loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2 $$ ç„¶ååˆ©ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°$\\textbf{w}$å’Œ$\\textbf{b}$æ¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œæœ€ç»ˆå­¦å¾—$\\textbf{w}$å’Œ$\\textbf{b}$çš„æ•°å€¼ã€‚ 123456789import torch as t%matplotlib inlinefrom matplotlib import pyplot as pltfrom IPython import display 12345678910111213141516171819202122232425# è®¾ç½®éšæœºæ•°ç§å­ï¼Œä¿è¯åœ¨ä¸åŒç”µè„‘ä¸Šè¿è¡Œæ—¶ä¸‹é¢çš„è¾“å‡ºä¸€è‡´t.manual_seed(1000)def get_fake_data(batch_size=8): ''' äº§ç”Ÿéšæœºæ•°æ®ï¼šy=x*2+3ï¼ŒåŠ ä¸Šäº†ä¸€äº›å™ªå£°''' x = t.rand(batch_size, 1) * 20 y = x * 2 + (1 + t.randn(batch_size, 1))*3 return x, y# æ¥çœ‹çœ‹äº§ç”Ÿçš„x-yåˆ†å¸ƒx, y = get_fake_data()plt.scatter(x.squeeze().numpy(), y.squeeze().numpy()) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# éšæœºåˆå§‹åŒ–å‚æ•°w = torch.randn(1,1)b = torch.zeros(1,1)lr = 0.001 # å­¦ä¹ ç‡for epoch in range(20000): x, y = get_fake_data(batch_size=8) # forward y_pred = x.mm(w) + b.expand_as(y) loss = 0.5 * (y_pred - y) ** 2 loss = loss.sum() # backward: æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦ dloss = 1 dy_pred = dloss * (y_pred - y) dw = x.t().contiguous().mm(dy_pred) db = dy_pred.sum() # æ›´æ–°å‚æ•° w.sub_(lr * dw) b.sub_(lr * db) if epoch % 1000 == 0: print(&quot;epoch:{}, loss:{}&quot;.format(epoch, loss)) # ç”»å›¾ display.clear_output(wait=True) x = torch.arange(0, 20).view(-1, 1) # [20, 1] y = x.mm(w) + b.expand_as(x) # predicted data plt.plot(x.numpy(), y.numpy()) x2, y2 = get_fake_data(batch_size=20) # true data plt.scatter(x2.numpy(), y2.numpy()) plt.xlim(0,20) plt.ylim(0,41) plt.show() plt.pause(0.5)print(w.squeeze()[0], b.squeeze()[0]) tensor(2.0264241695) tensor(2.9323694706)","link":"/2018/12/01/pytorch-book-1-Tensor/"},{"title":"rejectionç³»åˆ—1-overview","text":"å…³äº open set recognition çš„ä¸€ç‰‡ç»¼è¿°ã€‚ paper: Recent Advances in Open Set Recognition: A Survey Motivation In real-world recognition/classification tasks, limited by various objective factors, it is usually difficult to collect training samples to exhaust all classes when training a recognizer or classifier. A more realistic scenario is open set recognition (OSR), where incomplete knowledge of the world exists at training time, and unknown classes can be submitted to an algorithm during testing, requiring the classifiers not only to accurately classify the seen classes, but also to effectively deal with the unseen ones. ç°å®ä¸­å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä¸å¯èƒ½åœ¨è®­ç»ƒé›†ä¸­ç©·å°½æ‰€æœ‰ç±»åˆ«ã€‚æ›´å®é™…çš„æƒ…å†µæ˜¯ open set recognition (OSR). åœ¨è®­ç»ƒé˜¶æ®µåŒ…å«çš„æ˜¯ä¸å®Œæ•´çš„ knowledge of world. åœ¨æµ‹è¯•é˜¶æ®µä¼šå‡ºç° unknown ç±»åˆ«ã€‚è¿™éœ€è¦åˆ†ç±»å™¨ä¸ä»…èƒ½å‡†ç¡®çš„è¯†åˆ«åœ¨è®­ç»ƒé˜¶æ®µå·²ç»è§åˆ°è¿‡çš„ç±»åˆ«ï¼Œä¹Ÿèƒ½æœ‰æ•ˆçš„å¤„ç†æ²¡æœ‰è§è¿‡çš„ç±»åˆ«, æ¯”å¦‚ rejection æˆ–è€…å½’ç±»ä¸º unknown. This paper provides a comprehensive survey of existing open set recognition techniques covering various aspects ranging from related definitions, representations of models, datasets, experiment setup and evaluation metrics. Furthermore, we briefly analyze the relationships between OSR and its related tasks including zero-shot, one-shot (few-shot) recognition/learning techniques, classification with reject option, and so forth. Additionally, we also overview the open world recognition which can be seen as a natural extension of OSR. Importantly, we highlight the limitations of existing approaches and point out some promising subsequent research directions in this field. è¿™ç¯‡ç»¼è¿°è¦†ç›–äº†ç›¸å…³çš„å®šä¹‰ã€æ¨¡å‹ã€å®éªŒä»¥åŠéªŒè¯æŒ‡æ ‡ã€‚æ›´å¤šåœ°ï¼Œè¿˜åˆ†æäº† ä¸OSR ç›¸å…³çš„ä»»åŠ¡ zero-shot, one-shot è¯†åˆ«ï¼Œä»¥åŠ rejection. é¢å¤–åœ°ï¼Œè¿˜æ¦‚è¿°äº† open world recognition å¯ä»¥çœ‹ä½œ OSR çš„æ‰©å±•ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä½œè€…è¯´æ˜äº†å½“å‰ä¸€äº›æ–¹æ³•çš„é™åˆ¶ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„ä¸€äº›æ–¹å‘ã€‚ Introduction a more realistic scenario is usually open and non-stationary such as driverless, fault/medical diagnosis, etc., where unseen situations can emerge unexpectedly, which drastically weakens the robustness of these existing methods. æ›´ç°å®çš„åœºæ™¯æ˜¯ å¼€æ”¾çš„å’Œéé™æ€ çš„ã€‚ To meet this challenge, several related research directions actually have been explored including lifelong learning [1], [2], transfer learning [3]â€“[5], domain adaptation [6], zero-shot [7]â€“[9], one-shot (few-shot) [10]â€“[16] recognition/learning and open set recognition/classification [17]â€“[19], and so forth. æ¶‰åŠåˆ°çš„é¢†åŸŸï¼š lifelong learning, transfer learning, domain adaption, zero-shot, one-shot, open set recogntion. recognition should consider four basic categories of classes as follows: known known: train/dev ä¸­æœ‰æ ‡ç­¾çš„æ ·æœ¬ï¼ŒåŒ…æ‹¬æ­£è´Ÿç±»åˆ«ï¼Œå¹¶ä¸”æœ‰ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ã€‚ known unknown: train/dev ä¸­æœ‰æ ‡ç­¾çš„æ ·æœ¬ï¼Œè´Ÿç±»ï¼Œæ²¡æœ‰ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ã€‚ unknown known: test ä¸­æ²¡æœ‰å‡ºç°åœ¨ train ä¸­çš„æ ·æœ¬ï¼Œä½†æ˜¯æœ‰ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ã€‚æ¯”å¦‚ï¼Œtrain ä¸­æœ‰çŒ«ï¼Œç„¶å test ä¸­æœ‰å¦å¤–ä¸€ç§çŒ«ç§‘åŠ¨ç‰©ï¼Œé‚£ä¹ˆåŠ¨ç‰©è¿™ä¸ªæ ·æœ¬æ˜¯æœ‰æ„ä¹‰çš„å§ï¼Ÿï¼Ÿï¼Ÿ unknown unknown: test ä¸­æ²¡æœ‰å‡ºç°åœ¨ train ä¸­çš„æ ·æœ¬ï¼Œå¹¶ä¸”æ²¡æœ‰ä»»ä½•ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ã€‚ Unlike the traditional classification, zero-shot learning (ZSL) can identify unseen classes which have no available observations in training. However, the available semantic/attribute information shared among all classes including seen and unseen ones are needed. zero-shot æ˜¯é’ˆå¯¹ unknown known, ä¹Ÿå°±æ˜¯åŒ…å«äº†è¯­ä¹‰ä¿¡æ¯ã€‚ The ZSL mainly focuses on the recognition of the unknown known classes defined above. Actually, such a setting is rather restrictive and impractical, since we usually know nothing about the testing samples which may come from known known classes or not. unknown known è¿™ç§è®¾å®šå¾ˆæœ‰é™ï¼Œå¹¶ä¸”ä¸åˆ‡å®é™…ã€‚å› ä¸ºæˆ‘ä»¬å¾ˆéš¾çŸ¥é“ test ä¸­çš„æ ·æœ¬æ˜¯å¦æ˜¯åŒ…å«äº†è¯­ä¹‰ä¿¡æ¯ï¼Œæ— æ³•åˆ¤æ–­æ˜¯ unknown known or unknown unknown. comparision between open set recognition and traditional classification Via these decision boundaries, samples from an unknown unknown class are labeled as â€unknownâ€ or rejected rather than misclassified as known known classes. Basic notation and related definitionç»éªŒé£é™©å‡½æ•°ï¼š $L(x, y, f(x)) \\ge 0$ æ˜¯ loss function. P(x,y) æ˜¯å¯¹åº”æ ·æœ¬ (x, y) çš„æ¦‚ç‡ï¼Œé€šå¸¸è¿™ä¸ªè”åˆåˆ†å¸ƒçš„æ¦‚ç‡æˆ‘ä»¬æ˜¯ä¸çŸ¥é“çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•ç¡®å®šè‡ªç„¶ç•Œä¸­æ ·æœ¬ç©ºé—´(label space)åˆ°åº•æ˜¯ä¸ªä»€ä¹ˆåˆ†å¸ƒã€‚ [æèˆªï¼Œæœºå™¨å­¦ä¹ ] ä¸­å…³äºé£é™©å‡½æ•°çš„å®šä¹‰ï¼š æŸå¤±å‡½æ•°åº¦é‡ä¸€æ¬¡æ¨¡å‹é¢„æµ‹çš„å¥½åï¼Œé£é™©å‡½æ•°åº¦é‡å¹³å‡æ„ä¹‰ä¸‹æ¨¡å‹é¢„æµ‹çš„å¥½åã€‚ ä»¥å‰çœ‹ä¸æ‡‚è¿™ä¸€éƒ¨åˆ†ï¼Œç°åœ¨åªæƒ³è¯´ï¼š Perfect! Therefore, traditional recognition/classification approaches minimize the empirical risk instead of the ideal risk RI by using other knowledge, such as assuming that the label space is at least locally smooth and regularizing the empirical risk minimization. ä¼ ç»Ÿçš„åˆ†ç±»æ–¹æ³•æ˜¯æ ¹æ®å…¶ä»–çš„å¤–éƒ¨çŸ¥è¯†æ¥æœ€å°åŒ–ç»éªŒé£é™©ï¼Œæ¯”å¦‚ label space æ˜¯å…‰æ»‘çš„ï¼Œç„¶åä½¿ç”¨æ­£åˆ™åŒ–æœ€å°ç»éªŒé£é™©ï¼ˆä¹Ÿå°±æ˜¯ä¸Šé¢æ‰€è¯´çš„ç»“æ„é£é™©å‡½æ•°ï¼‰ã€‚ Note that traditional recognition problem is usually performed under the closed set assumption. When the assumption switches to open environment/set scenario with the open space, other things should be added since intuitively there is some risk in labeling sample in the open space as any known known classes. This gives such an insight for OSR that we do know something else: we do know where known known classes exist, and we know that in open space we do not have a good basis for assigning labels for the unknown unknown classes. ä¼ ç»Ÿçš„è¯†åˆ«æ˜¯å‡è®¾åœ¨å›ºå®šçš„æ ·æœ¬ç©ºé—´ä¸‹(known known). å½“è½¬æ¢åˆ°å¼€æ”¾å¼åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¾ˆæ•æ„Ÿçš„æ„è¯†åˆ°éœ€è¦ç»™ label space åŠ ç‚¹ riskã€‚ã€‚æˆ‘ä»¬çŸ¥é“ known known classes æ˜¯å­˜åœ¨çš„ï¼Œæˆ‘ä»¬ä¹ŸçŸ¥é“æˆ‘ä»¬å¹¶æ²¡æœ‰è¿™æ ·ä¸€ä¸ª basis å»ç»™ unknown unknown æ‰“æ ‡ç­¾ã€‚ open space riskè¿™éƒ¨åˆ†çš„å†…å®¹ä¸»è¦å¼•è‡ªè¿™ç¯‡ paper: 17. Toward Open Set Recognition è¿™ç¯‡ paper æ˜¯æŠŠ class of interest å½“ä½œä¸€ä¸ªç±»ï¼Œç„¶åæ‰€æœ‰çš„ unknown/known å½“ä½œå¾ˆå¤š classes, ä¹Ÿå°±æ˜¯ 1-vs-set. To improve the overall open set recognition error, our 1-vs-set formulation balances the unknown classes by obtaining a core margin around the decision boundary A from the base SVM, specializing the resulting half-space by adding another plane $\\Omega$ and then generalizing or specializing the two planes (shown in Fig. 2) to optimize empirical and open space risk. This process uses the open set training data and the risk model to define a new â€œopen set margin.â€ The second plane $\\Omega$ allows the 1-vs-set machine to avoid the overgeneralization that would misclassify the raccoon in Fig. 2. The overall optimization can also adjust the original margin with respect to A to reduce open space risk, which can avoid negatives such as the owl. ä½¿ç”¨äº†ä¸¤ä¸ªè¶…å¹³é¢ï¼Œå»åˆ†éš” Negatives/positivecs/unknown. While we do not know the joint distribution $P(x, y)$ in, one way to look at the open space risk is as a weak assumption: Far from the known data the Principle of Indifference [8] suggests that if there is no known reason to assign a probability, alternatives should be given equal probability. In our case, this means that at all points in open space, all labels (both known and unknown) are equally likely, and risk should be computed accordingly. However, we cannot have constant value probabilities over infinite spacesâ€”the distribution must be integrable and integrate to 1. We must formalize open space differently (e.g., by ensuring the problem is well posed and then assuming the probability is proportional to relative Lebesgue measure [9]). Thus, we can consider the measure of the open space to the full space, and define our risk penalty proportional to such a ratio. æ— æ³•çŸ¥é“è”åˆåˆ†å¸ƒ $P(x, y)$, ä½œè€…å‡è®¾æ‰€æœ‰çš„æ ·æœ¬æ¦‚ç‡æ˜¯ç›¸ç­‰çš„ï¼Œä½†æ˜¯å‘é‡ç©ºé—´ä¸­æ ·æœ¬æ€»æ•°æ˜¯ä¸ç¡®å®šçš„ï¼Œæ‰€ä»¥ä½œè€…å®šä¹‰ä¸€ä¸ªæ¯”ä¾‹æ¥æè¿° åœ¨ open space ä¸­å‡ºç° unknown çš„å±é™©æƒ©ç½šç³»æ•°ã€‚ where open space risk is considered to be the fraction (in terms of Lebesgue measure) of positively labeled open space compared to the overall measure of positively labeled space (which includes the space near the positive examples). open space risk æ˜¯å¼€æ”¾ç©ºé—´ä¸­ positive label çš„æ€»æ•°ä¸æ€»ä½“ç©ºé—´ä¸­ positive label çš„æ€»ä½“åº¦é‡ã€‚ ä¸å¤ªæ‡‚ã€‚ã€‚é—®é¢˜è¿˜æ˜¯ä¸çŸ¥é“æ€ä¹ˆåº¦é‡ï¼Ÿ unknown çš„ç±»åˆ«èƒ½ç¡®å®šï¼Ÿï¼Ÿï¼Ÿ opennessopennessï¼Œç”¨æ¥è¡¨å¾æ•°æ®é›†çš„å¼€æ”¾ç¨‹åº¦ï¼š $C_{TR}$ æ˜¯è®­ç»ƒé›†ä¸­çš„ç±»åˆ«æ•°ï¼Œè¶Šå¤§ï¼Œå¼€æ”¾ç¨‹åº¦è¶Šå°ã€‚ $C_{TE}$ æµ‹è¯•é›†ä¸­çš„ç±»åˆ«æ•°ã€‚ The Open Set Recognition Problem our goal is to balance the risk of the unknown in open space with the empirical (known) risk. In this sense, we formally define the open set recognition problem as follows: æˆ‘ä»¬çš„ç›®çš„æ˜¯å¹³è¡¡ the risk of unknown å‡ºç°åœ¨åŸºäº known classes è®¡ç®—çš„åˆ°çš„ open space çš„ empirical riskã€‚ æ€ä¹ˆç†è§£å‘¢ï¼Ÿå°±æ˜¯ä¼ ç»Ÿçš„é£é™©å‡½æ•°éƒ½æ˜¯åªè€ƒè™‘äº†ç»éªŒé£é™©ï¼Œä¹Ÿå°±æ˜¯å®Œå…¨åŸºäºè®­ç»ƒæ•°æ®çš„ã€‚ä½†æ˜¯åœ¨ open space é‡Œé¢ï¼Œæˆ‘ä»¬è¿˜è¦æµ‹è¯•æ—¶ä¼šå‡ºç°çš„ unknownï¼Œæ‰€ä»¥åœ¨ é£é™©å‡½æ•°çš„è®¾ç½®çš„åŒæ—¶ï¼Œå°±è¦è€ƒè™‘åˆ° unknown çš„å­˜åœ¨ã€‚ä¹Ÿå°±æ˜¯å‰é¢çš„ open space risk. a categorization of OSR techniquesé—®é¢˜çš„å…³é”®åœ¨äºå¦‚ä½•å°† å…¬å¼ï¼ˆ4ï¼‰open space risk åˆå¹¶åˆ°æ¨¡å‹ä¸­å»ã€‚ç„¶åå¤§ä½¬ä»¬æå‡ºå„å¼å„æ ·çš„æ¨¡å‹ï¼Œä¸»è¦åˆ†ä¸º discriminative model and generative models. æ›´è¿›ä¸€æ­¥ï¼Œå¯ä»¥åˆ†ä¸ºï¼šfive categories (Table II): Traditional ML-based Deep Network-based Adversarial Learning-based EVT-based Dirichlet Process-based OSR models Deep Neural Network-based OSR Modelså¤§ä½¬ä»¬çš„æ°ä½œï¼Œæ„Ÿè§‰éƒ½æŒºæ–°çš„ï¼Œæ–°å‘ï¼Ÿ æå‡ºäº† OpenMax,ä½¿ç”¨ deep networks, è¿˜æ˜¯ç”¨ softmax æŸå¤±å‡½æ•°æ¥æœ€å°åŒ– äº¤å‰ç†µ cross entropy loss. ç„¶ååœ¨ç½‘ç»œçš„å€’æ•°ç¬¬äºŒå±‚ï¼ˆsoftmax çš„å‰ä¸€å±‚ï¼Ÿï¼‰å¾—åˆ°æ¯ä¸€ä¸ªæ­£åˆ†ç±»çš„ mean activate vector(MAV). ç„¶åæ˜¯æ ¹æ® Weibull districution å» redistribution ä»¥åŠé‡æ–°åˆ†ç±»ç­‰ç­‰æ¥ä¸‹æ¥çš„æ“ä½œè¿˜æ˜¯çœ‹ç›¸åº” çš„ paper å§ã€‚ the OpenMax effectively addressed the challenge of the recognition for fooling/rubbish and unrelated open set images. However, as discussed in [71], the OpenMax fails to recognize the adversarial images which are visually indistinguishable from training samples but are designed to make deep networks produce high confidence but incorrect answers [96], [98]. OpenMax æœ‰æ•ˆçš„è§£å†³äº† ä¸ç›¸å…³çš„ open set images çš„é—®é¢˜ï¼Œä½†æ˜¯å´æ— æ³•æœ‰æ•ˆåŒºåˆ†å¯¹æŠ—ç”Ÿæˆæ ·æœ¬ã€‚ Actually, the authors in [72] have indicated that the OpenMax is susceptible to the adversarial generation techniques directly working on deep representations. Therefore, the adversarial samples are still a serious challenge for open set recognition. Furthermore, using the distance from MAV, the cross entropy loss function in OpenMax does not directly incentivize projecting class samples around the MAV. In addition to that, the distance function used in testing is not used in training, possibly resulting in inaccurate measurement in that space [73]. To address this limitation, Hassen and Chan [73] learned a neural network based representation for open set recognition, which is similar in spirit to the Fisher Discriminant, where samples from the same class are closed to each other while the ones from different classes are further apart, leading to larger space among known known classes for unknown unknown classesâ€™ samples to occupy. äº¤å‰ç†µå¹¶ä¸èƒ½æœ‰æ•ˆçš„å°†ç±»åˆ«æ˜ å°„åˆ°ç›¸åº”çš„ MAV ä¸­ï¼Œå› ä¸ºåœ¨æµ‹è¯•é›†ä¸­çš„ distence function è·Ÿåœ¨ training set é‡Œé¢æ˜¯ä¸ä¸€æ ·çš„ï¼Œè¿™ä¼šå¯¼è‡´ä¸å‡†ç¡®çš„åˆ¤åˆ«ã€‚åŸºäºæ­¤ï¼Œ[73]æå‡ºäº† Fisher åˆ¤åˆ«ï¼Œä»åŒä¸€ä¸ªç±»åˆ«ä¸­é‡‡æ ·ï¼Œä½¿å¾—unknown unknown å’Œ known known çš„é—´è·å¾ˆå¤§ã€‚ OpenMax to text classification Deep Open classifier tWiSARD hidden unknown unknown classes Adversarial Learning-based OSR Models Note that the main challenge for open set recognition is the incomplete class knowledge existing in training, leading to the open space risk when classifiers encounter unknown unknown classes during testing. Fortunately, the adversarial learning technique can account for open space to some extent by adversarially generating the unknown unknown class data according to the known known class knowledge, which undoubtedly provides another way to tackle the challenging multiclass OSR problem. open set recognition æœ€å¤§çš„æŒ‘æˆ˜æ˜¯ training ä¸­ä¸å®Œæ•´çš„ knowledgeï¼Œ åœ¨ testing ä¸­é‡åˆ° unknown unknown å¯¼è‡´ open space risk. è€Œå¯¹æŠ—è®­ç»ƒç½‘ç»œåœ¨æŸç§ç¨‹åº¦ä¸Šæ ¹æ® known known ç”Ÿæˆ unknown unknownï¼Œæä¾›äº†å¦å¤–ä¸€ç§æ–¹å¼è§£å†³ OSR é—®é¢˜ã€‚ EVT-based OSR Models As a powerful tool to increase the classification performance, the statistical Extreme Value Theory (EVT) has recently achieved great success due to the fact that EVT can effectively model the tails of the distribution of distances between training observations using the asymptotic theory[100]. ä¸æ˜¯å¾ˆæ‡‚è¿™ä¸ªç†è®ºï¼Œç»™å‡ºå‡ ç¯‡ paper å§ Remark: As mentioned above, almost all existing OSR methods adopt the threshold-based classification scheme, where recognizers in decision either reject or categorize the input samples to some known known class using empirically set threshold. Thus the threshold plays a key role. However, the selection for it usually depends on the knowledge of known known classes, inevitably incurring risks due to lacking available information from unknown unknown classes [57]. This indicates the threshold-based OSR methods still face serious challenges. åŸºäº known known å¾—åˆ°çš„ threshold å› ä¸ºç¼ºä¹ unknown unknown çš„ä¿¡æ¯ï¼Œä¸å¯é¿å…çš„ä¼šé€ æˆ risk, è¿™ä¹Ÿæ˜¯åŸºäº threshold è¿™ç±»æ–¹æ³•æ‰€é¢ä¸´çš„å›°éš¾ã€‚ Dirichlet Process-based OSR Models (ç”Ÿæˆæ¨¡å‹) Dirichlet process (DP) [104]â€“[108] considered as a distribution over distributions is a stochastic process, which has been widely applied in clustering and density estimation problems as a nonparametric prior defined over the number of mixture components. Furthermore, this model does not overly depend on training samples and can achieve adaptive change as the data changes, making it naturally adapt to the open set recognition scenario. In fact, researchers have begun the related research Dirichlet è¿‡ç¨‹ä½œä¸ºä¸€ç§åŸºäºæ··åˆæ¨¡å‹çš„éå‚æ•°æ–¹æ³•å¹¿æ³›ç”¨äºèšç±»ï¼Œå‚æ•°ä¼°è®¡ã€‚è¿™ç§æ¨¡å‹ä¸éœ€è¦ä¾èµ–äº trainingï¼Œå¯ä»¥éšç€ dataset çš„å˜åŒ–è€Œè‡ªé€‚åº”çš„å˜åŒ–ï¼Œè¿™ä½¿å¾—å®ƒèƒ½æœ‰æ•ˆçš„é€‚ç”¨äº open set çš„åœºæ™¯ã€‚ å¯¹ç”Ÿæˆæ¨¡å‹ä¸æ˜¯å¾ˆç†Ÿã€‚ã€‚ Remark: Instead of addressing the OSR problem from the discriminative model perspective, CD-OSR actually reconsiders this problem from the generative model perspective due to the use of HDP, which provides another research direction for open set recognition. Furthermore, the collective decision strategy for OSR is also worth further exploring since it not only takes the correlations among the testing samples into account but also provides a possibility for new class discovery, whereas single-sample decision strategy2 adopted by other existing OSR methods can not do such a work since it can not directly tell whether the single rejected sample is an outlier or from new class. Beyond open set Recognitionå…³äº open set recognition å¦‚æœä»…ä»…è€ƒè™‘é™æ€çš„ setï¼Œæ„ä¹‰ä¸æ˜¯å¾ˆå¤§ã€‚ä»¥åŠï¼Œåªå¯¹ unknown unknown è¿›è¡Œ rejection ä¹Ÿæ˜¯ä¸å¤Ÿçš„ã€‚ä¸ºæ­¤ï¼Œæœ‰äººæå‡º open world recognition. open world recognition (OWR), where a recognition system should perform four tasks: detecting unknown unknown classes choosing which samples to label for addition to the model labelling those samples updating the classifier Remark: As a natural extension of OSR, the OWR faces more serious challenges which require it not only to have the ability to handle the OSR task, but also to have minimal downtime, even to continuously learn, which seems to have the flavor of lifelong learning to some extent. Besides, although some progress regarding the OWR has been made, there is still a long way to go. ç»ˆèº«å­¦ä¹ ã€‚ã€‚666 Dataset and evalution metricsdataset https://dx.doi.org/10.6084/m9.figshare.1097614 https://www.csie.ntu.edu.tw/âˆ¼cjlin/libsvmtools/datasets/multi-class.html Experiment Setup: In open set recognition, most existing experiments are carried out on a variety of recastes multi-class benchmark datasets. Specifically, taking the Usps dataset as an example, when it is used for OSR problem, one can randomly choose S distinct labels as the known known classes, and vary openness by adding a subset of the remaining labels. å¯ä»¥å¢åŠ å‡å°‘ç±»åˆ«æ•°æ¥æ”¹å˜ openness. Evaluation Metrics for Open Set Recognition TPï¼š true positive FP: false positive TN: true negative FN: false negative TU: true unknown FU: false unknown accuracyå¯¹äº closed set : $$\\text{accuracy}=\\dfrac{TP+TN}{TP+TN+FP+FN}$$ å¯¹äº open set: $$\\text{accuracy}_O=\\dfrac{(TP+TN)+TU}{(TP+TN+FP+FN)+(TU+FU)}$$ å¯¹äºä¸å‡è¡¡æƒ…å†µï¼Œaccuracy å¹¶ä¸èƒ½å®¢è§‚çš„è¯„ä»·æ¨¡å‹å¥½åã€‚æ¯”å¦‚åœ¨testing ä¸­ï¼Œunknown unknown æ ·æœ¬æ•°é‡å¾ˆå¤š,é‚£ä¹ˆå¦‚æœåˆ†ç±»å™¨æŠŠæ‰€æœ‰çš„ç±»åˆ«éƒ½åˆ¤ä¸º unknown unknownï¼Œå®ƒçš„å‡†ç¡®ç‡ä¾æ—§å¾ˆé«˜ã€‚ äºæ˜¯ï¼Œæœ‰äººæå‡ºäº† normalized accuracy(NA). $0\\le \\lambda \\le 1$ æ˜¯æ­£åˆ™åŒ–å¸¸æ•°ã€‚ F-measureF1: $$F1=\\dfrac{2\\text{precision} \\text{recall}}{\\text{precision}+\\text{recall}}$$ $$precision=\\dfrac{TP}{TP+FP}$$ ç²¾åº¦ï¼š é¢„æµ‹å¾—åˆ°çš„ positive ä¸­çœŸæ­£æ˜¯ positive çš„æ¦‚ç‡ã€‚ $$recall=\\dfrac{TP}{TP+FN}$$ å¬å›ï¼š æ‰€æœ‰çœŸæ­£ positive çš„æ ·æœ¬è¢«é¢„æµ‹ä¸º positive çš„æ¦‚ç‡ã€‚ åœ¨ open set åœºæ™¯ä¸‹ï¼ŒF1 å€¼æ— æ³•è€ƒè™‘ unknown unknown. Instead, the computations of Precision and Recall in it are only for available known known classes. Additionally, the work [67] has indicated that although the computations of Precision and Recall are only for available known known classes, the FN and FP also consider the false unknown unknown classes and false known known classes by taking into account the false negative and the false positive, and we refer the reader to [67] for more details. äº‹å®ä¸Šï¼Œåœ¨ FP å’Œ FN ä¸­å¯èƒ½ä¹ŸåŒ…æ‹¬ false unknown unknown, è¿™å°±æœ‰é—®é¢˜äº†æ˜¯å§ã€‚ã€‚ è¯¦ç»†å‚è€ƒè¿™ç¯‡ paper Nearest neighbors distance ratio open-set classifier Note that the Precision contains the macro-Precision and micro-Precision while Recall includes the macro-Recall and micro-Recall, which leads to the corresponding macro-F-measure and micro-F-measure. Nevertheless, whether it is macro-F-measure or micro-F-measure, the higher their values, the better the performance of the corresponding OSR model. Youdenâ€™s index for OSR$$J= \\text{Recall}+S-1$$ å…¶ä¸­ S æ˜¯çœŸè´Ÿç±»ç‡ï¼š $S=\\dfrac{TN}{TN+FP}$ future research directionsAbout modeling å¤§éƒ¨åˆ†å·¥ä½œéƒ½æ˜¯åŸºäºåˆ¤åˆ«æ¨¡å‹æ¥åšçš„ï¼Œåªæœ‰å°‘éƒ¨åˆ†æ˜¯åŸºäºç”Ÿæˆæ¨¡å‹ï¼Œä¹Ÿè®¸ç”Ÿæˆæ¨¡å‹ä¼šæ›´æœ‰æ¢ç´¢ç©ºé—´ã€‚ OSR çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä¼ ç»Ÿçš„åˆ†ç±»å™¨æ˜¯åœ¨ closed-set åœºæ™¯ä¸‹è·å¾—çš„ï¼Œä¸€æ—¦ unknown unknown class è½å…¥è¿™ä¸ªç©ºé—´ï¼Œå°†æ°¸è¿œæ— æ³•è¢«æ­£ç¡®çš„åˆ†ç±»ã€‚ modeling known known classeså¦‚æœå¾—åˆ°çš„ known known class æ²¡æœ‰è¢«è¿‡æ‹Ÿåˆï¼Œé‚£ä¹ˆè¿™æ ·çš„åˆ†ç±»å™¨å°±èƒ½æœ‰æ•ˆçš„åŒºåˆ†å‡º unknown unknown. æ‰€ä»¥èšç±»å’Œåˆ†ç±»ç®—æ³•çš„ç»“åˆä¼šæ˜¯ä¸é”™çš„æ–¹å‘ã€‚å…³äº clustering å’Œ classification çš„ unified learning framework: è¿™ä¸¤ç¯‡ paper ä¾æ—§æ˜¯åœ¨ closed-set ä¸‹åšçš„ï¼Œæ‰€ä»¥éœ€è¦ä½ å»å°è¯•ã€‚ã€‚ã€‚ modeling unknown unknown classesä¼¼ä¹åœ¨åªæœ‰ known known classes çš„æƒ…å†µä¸‹æ˜¯å¾ˆéš¾å»å­¦ä¹  unknown unknown çš„ç±»çš„æ€§è´¨çš„ã€‚ä½†æ˜¯å¯ä»¥é€šè¿‡å¯¹æŠ—å­¦ä¹ æ¥ç”Ÿæˆ unknown unknown ä¹Ÿæ˜¯ä¸é”™çš„æ–¹å‘ã€‚ é¡ºä¾¿ä½œè€…è¿˜æäº†ä¸‹ transductive leanringï¼Œä»¥åŠåŸºäº Dirichlet process çš„è‡ªé€‚åº”è¡Œï¼ŒCD-OSRã€Dirichlet processed-based OSR ä¹Ÿæ˜¯å€¼å¾—æ¢ç´¢çš„ã€‚ About rejectingå¤§éƒ¨åˆ†çš„å·¥ä½œéƒ½æ˜¯ reject unknown unknown classesï¼Œè€Œæ²¡æœ‰åç»­çš„å·¥ä½œäº†ã€‚åªæœ‰å°‘é‡çš„ [66][67]è¿›è¡Œäº†åç»­çš„å·¥ä½œï¼Œæ¯”å¦‚ new classes discovery. About the decisionæ‰€æœ‰çš„ OSR æ¨¡å‹éƒ½æ˜¯ç”¨æ¥è¯†åˆ«å•ä¸ªæ ·æœ¬çš„ï¼Œä½†æ˜¯ä¸€ä¸ªå†³ç­–çš„å†³å®šå¹¶æ²¡æœ‰è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ‰€ä»¥ collective decision ä¸ä»…åœ¨ testing æ—¶è€ƒè™‘ç›¸å…³æ€§ï¼ŒåŒæ—¶è¿˜èƒ½å‘ç° new classes. Open set + â€˜sthâ€™As open set scenario is a more practical assumption for the real-world classification/recognition tasks, it can naturally be combined with various fields involving classification/recognition such as semi-supervised learning, domain adaptation, active learning, multi-task learning, multi-label learning, multi-view learning, and so forth. For example, [124]â€“[126] recently introduced this scenario into domain adaptation, while [127] explored the open set classification in active learning field. Therefore, many interesting works are worth looking forward to. çœ‹èµ·æ¥æ˜¯ä¸ªä¸é”™çš„æ–¹å‘ã€‚ã€‚ Generalized Open Set Recognitionåˆ©ç”¨ side-information,æ¯”å¦‚ unknown unknwon å’Œ known known ä¼šæœ‰å…±åŒçš„è¯­ä¹‰ä¿¡æ¯(semantic/attribute information). Appending semantic/attribute information In fact, a lot of semantic/attibute information is shared between the known known and the unknown unknown classes. Therefore, we can fully utilize this kind of information to â€™cognizeâ€™ the unknown unknown classes, or at least to provide a rough semantic/attribute description for the corresponding unknown unknown classes instead of simply rejecting them. åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯å»æ„è¯†åˆ° unknown unknwonï¼Œè€Œä¸æ˜¯ç®€å•çš„ reject. ä½†æ˜¯è¦æ³¨æ„åŒºåˆ† open set recognition å’Œ ZSL(zero-shot learning) çš„åŒºåˆ«ï¼š The $\\text{side-information}^1$ in ZSL denotes the semantic/attribute information shared among all classes including known known and unknown known classes. where the $\\text{side-information}^4$ denotes the available semantic/attribute information only for known known classes æ„Ÿè§‰è¿™ä¸ª side-information çš„ç•Œé™å¾ˆéš¾ç¡®å®šå•Šï¼ŸGeneralized Open Set Recognition çš„è¿™ä¸ªèŒƒå›´ä¼¼ä¹å¾ˆéš¾å®ç°ï¼Œ æ€ä¹ˆå¯èƒ½å‡ºç°åœ¨ training ä¸­çš„ semantice information å®Œå…¨ä¸å‡ºç°åœ¨ unknown unknown ä¸­å‘¢ã€‚ã€‚ è¿˜æœ‰ä¸€äº›ç›¸ä¼¼çš„å·¥ä½œï¼š Using other available side-information **The main reason for open space risk is that the traditional classifiers trained under closed set scenario usually divide over-occupied space for known known classes, thus inevitably resulting in misclassifications once the unknown unknown class samples fall into the space divided for some known known class.** From this perspective, the open space risk will be reduced as the space divided for those known known classes decreases by using other side-information like universum [135], [136] to shrink their regions as much as possible. è™½ç„¶æ„Ÿè§‰å¾ˆæ‰¯æ·¡ã€‚ã€‚ä½†æ˜¯è¿˜æ˜¯æœ‰äººåšå•Šï¼Œä¸è¿‡å…³äº open space risk çš„å®šä¹‰å¯ä»¥åœ¨çœ‹ä¸€éã€‚ã€‚ Relative Open Set Recognitionæ„Ÿè§‰è¿™ä¸ªè¿˜æŒºæœ‰æ„æ€çš„ã€‚ç–¾ç—…çš„è¯Šæ–­ï¼Œæ‰€æœ‰çš„æ ·æœ¬ç©ºé—´éƒ½å¯ä»¥åŒºåˆ†ä¸º sick or no sick, æ‰€ä»¥ä»…ä»…æ˜¯åˆ¤æ–­æœ‰æ²¡æœ‰ç—…ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸ª closed set é—®é¢˜ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬è¦è¿›ä¸€æ­¥åˆ¤æ–­ç–¾ç—…çš„ç±»å‹ï¼Œé‚£ä¹ˆæœ‰å¯èƒ½å‡ºç° unseen disease in training. Knowledge Integration for Open Set Recognition In fact, the incomplete knowledge of the world is universal, especially for the single individuals: something you know does not mean I also know. how to integrate the classifiers trained on each sub-knowledge set to further reduce the open space risk will be an interesting yet challenging topic in the future work, especially for such a situation: we can only obtain the classifiers trained on corresponding sub-knowledge sets, yet these sub-knowledge sets are not available due to the privacy protection of data. åˆ©ç”¨çŸ¥è¯†åº“æ¥å‡å° open space riskã€‚ ä¼¼ä¹è¿™ä¸ªçœ‹èµ·æ¥æ¯”è¾ƒé è°±ï¼Œå› ä¸º unknown èŒƒå›´ç¡®å®å¾ˆéš¾å®šä¹‰ï¼Œå¦‚æœç»™ä¸ªå¤–éƒ¨çŸ¥è¯†åº“ç»™ä½ ï¼ŒæŠŠè·ŸçŸ¥è¯†åº“ç›¸å…³çš„ unknown è¯†åˆ«å‡ºæ¥ï¼Œå°±å¾ˆæ£’äº†å§ ç›¸å…³çš„ä¸€äº›å¼€æºå·¥å…·å’Œä»£ç ï¼š","link":"/2018/12/09/rejection%E7%B3%BB%E5%88%971-overview-1/"},{"title":"pytorch-æŸå¤±å‡½æ•°","text":"pytorch loss function. Cross Entropyç®€å•æ¥è¯´ï¼Œäº¤å‰ç†µæ˜¯ç”¨æ¥è¡¡é‡åœ¨ç»™å®šçš„çœŸå®åˆ†å¸ƒ $p_k$ ä¸‹ï¼Œä½¿ç”¨éçœŸå®åˆ†å¸ƒ $q_k$ æ‰€æŒ‡å®šçš„ç­–ç•¥ f(x) æ¶ˆé™¤ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§æ‰€éœ€è¦ä»˜å‡ºçš„åŠªåŠ›çš„å¤§å°ã€‚äº¤å‰ç†µçš„è¶Šä½è¯´æ˜è¿™ä¸ªç­–ç•¥è¶Šå¥½ï¼Œæˆ‘ä»¬æ€»æ˜¯ minimize äº¤å‰ç†µï¼Œå› ä¸ºäº¤å‰ç†µè¶Šå°ï¼Œå°±è¯æ˜ç®—æ³•æ‰€äº§ç”Ÿçš„ç­–ç•¥è¶Šæ¥è¿‘æœ€ä¼˜ç­–ç•¥ï¼Œä¹Ÿå°±é—´æ¥è¯æ˜æˆ‘ä»¬çš„ç®—æ³•æ‰€è®¡ç®—å‡ºçš„éçœŸå®åˆ†å¸ƒè¶Šæ¥è¿‘çœŸå®åˆ†å¸ƒã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°ä»ä¿¡æ¯è®ºçš„è§’åº¦æ¥è¯´ï¼Œå…¶å®æ¥è‡ªäº KL æ•£åº¦ï¼Œåªä¸è¿‡æœ€åæ¨å¯¼çš„æ–°å¼ç­‰ä»·äºäº¤å‰ç†µçš„è®¡ç®—å…¬å¼ï¼š ä»ä¿¡æ¯è®ºçš„è§†è§’æ¥ç†è§£ï¼š ä¿¡æ¯é‡/ä¿¡æ¯ç†µï¼ˆç†µï¼‰/äº¤å‰ç†µ/æ¡ä»¶ç†µ ä¿¡æ¯é‡ï¼š ä¸€ä¸ªäº‹ä»¶çš„ä¿¡æ¯é‡å°±æ˜¯è¿™ä¸ªæ—¶é—´å‘ç”Ÿçš„æ¦‚ç‡çš„è´Ÿå¯¹æ•°ï¼Œæ¦‚ç‡è¶Šå¤§ï¼Œæ‰€å¸¦æ¥çš„ä¿¡æ¯å°±è¶Šå°‘å˜›ã€‚è‡³äºä¸ºä»€ä¹ˆæ˜¯è´Ÿå¯¹æ•°ï¼Œå°±è¦é—®é¦™å†œäº†ã€‚ã€‚èµ·ç è¦æ»¡è¶³$P(X)=1$æ—¶ä¿¡æ¯é‡ä¸º0ï¼Œä¸”å§‹ç»ˆå¤§äº0 $$-\\log P(X)$$ ä¿¡æ¯ç†µï¼Œ ä¹Ÿå°±æ˜¯ç†µï¼Œæ˜¯éšæœºå˜é‡ä¸ç¡®å®šæ€§çš„åº¦é‡ï¼Œä¾èµ–äºäº‹ä»¶Xçš„æ¦‚ç‡åˆ†å¸ƒã€‚å³ä¿¡æ¯ç†µæ˜¯ä¿¡æ¯é‡çš„æœŸæœ›ã€‚å³æ±‚ç¦»æ•£åˆ†å¸ƒåˆ—çš„æœŸæœ›ï½ï½ $$H(p) = -\\sum_{i=1}^np_i\\log p_i$$ äº¤å‰ç†µï¼š å›å½’åˆ°åˆ†ç±»é—®é¢˜æ¥ï¼Œæˆ‘ä»¬é€šè¿‡score functionå¾—åˆ°ä¸€ä¸ªç»“æœï¼ˆ10ï¼Œ1ï¼‰ï¼Œé€šè¿‡softmaxå‡½æ•°å‹ç¼©æˆ0åˆ°1çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬ç§°ä¸º $q_i=\\dfrac{e^{f_{y_i}}}{\\sum_je^{f_j}}$ å§ï¼Œ $$H(p,q) = -\\sum_{i=1}^np_i\\log q_i$$ è¿™å°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„äº¤å‰ç†µï¼Œé€šè¿‡ Gibbsâ€™ inequality çŸ¥é“ï¼š$H(p,q)&gt;=H(p)$ æ’æˆç«‹ï¼Œå½“ä¸”ä»…å½“ $q_i$ åˆ†å¸ƒå’Œ $p_i$ ç›¸åŒæ—¶ï¼Œä¸¤è€…ç›¸ç­‰ã€‚ ç›¸å¯¹ç†µï¼š è·Ÿäº¤å‰ç†µæ˜¯åŒæ ·çš„æ¦‚å¿µï¼Œ$D(p||q)=H(p,q)-H(p)=-\\sum_{i=1}^np(i)\\log {\\dfrac{q(i)}{p(i)}}$ï¼Œåˆç§°ä¸ºKLæ•£åº¦ï¼Œè¡¨å¾ä¸¤ä¸ªå‡½æ•°æˆ–æ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚æ€§ï¼Œå·®å¼‚è¶Šå¤§åˆ™ç›¸å¯¹ç†µè¶Šå¤§. æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€Negative Log Liklihood(NLL)ã€KLæ•£åº¦ä¸Cross Entropyå…¶å®æ˜¯ç­‰ä»·çš„ï¼Œéƒ½å¯ä»¥è¿›è¡Œäº’ç›¸æ¨å¯¼ï¼Œå½“ç„¶MSEä¹Ÿå¯ä»¥ç”¨Cross Entropyè¿›è¡Œæ¨å¯¼å‡ºï¼ˆè¯¦è§Deep Learning Book P132ï¼‰ã€‚ BCELossCreates a criterion that measures the Binary Cross Entropy between the target and the output ç”¨äºäºŒåˆ†ç±»çš„æŸå¤±å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯ logistic å›å½’çš„æŸå¤±å‡½æ•°ã€‚ å¯¹äºäºŒåˆ†ç±»ï¼Œæˆ‘ä»¬åªéœ€è¦é¢„æµ‹å‡ºæ­£åˆ†ç±»çš„æ¦‚ç‡ pï¼Œå¯¹åº”çš„ (1-p) åˆ™æ˜¯è´Ÿåˆ†ç±»çš„æ¦‚ç‡ã€‚å…¶ä¸­ p å¯ä½¿ç”¨ sigmoid å‡½æ•°å¾—åˆ°ã€‚ $$sigmoid(x) = \\dfrac{1}{1+e^{(-x)}}$$ å¯¹åº”çš„æŸå¤±å‡½æ•°å¯é€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡æ¨å¯¼å¾—åˆ°ï¼š å‡è®¾æœ‰ n ä¸ªç‹¬ç«‹çš„è®­ç»ƒæ ·æœ¬ ${(x_1,y_1), â€¦,(x_n, y_n)}$ y æ˜¯çœŸå®æ ‡ç­¾ï¼Œ$y\\in {0,1}$, é‚£ä¹ˆå¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡ä¸ºï¼š $$P(x_i, y_i)=P(y_i=1|x_i)^{y_i}P(y_i=0|x_i)^{1-y_i}$$ $$=P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}$$ å–è´Ÿå¯¹æ•°å³å¯å¾—ï¼š $$-y_iP(y_i=1|x_i)-(1-y_i)(1-P(y_i=1|x_i))$$ ä¸éš¾çœ‹å‡ºï¼Œè¿™ä¸å¸¸è§çš„ softmax å¤šåˆ†ç±»çš„ loss è®¡ç®—æ˜¯ä¸€è‡´çš„ã€‚ 12345678910111213141516171819202122232425262728293031class BCELoss(_WeightedLoss): def __init__(self, weight=None, size_average=None, reduce=None, reduction='elementwise_mean'): &quot;&quot;&quot; - weight: æ‰‹åŠ¨è°ƒæ•´æƒé‡ï¼Œä¸å¤ªæ˜ç™½æœ‰å•¥ç”¨ï¼Œç”¨åˆ°åœ¨çœ‹å§ - size_average, reduce å¼ƒç”¨ï¼Œç›´æ¥çœ‹ reduction å³å¯ - reductionï¼š &quot;elementwise_mean&quot;|&quot;sum&quot;|&quot;none&quot;ï¼Œçœ‹åå­—å°±çŸ¥é“å•¥æ„æ€äº† &quot;&quot;&quot; super(BCELoss, self).__init__(weight, size_average, reduce, reduction)def forward(self, input, target): return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction) &quot;&quot;&quot; - input: é¢„æµ‹æ¦‚ç‡ï¼Œä»»æ„ shape, ä½†æ˜¯å€¼å¿…é¡»åœ¨ 0-1 ä¹‹é—´ - target: çœŸå®æ¦‚ç‡ï¼Œ shape ä¸ input ç›¸åŒ &quot;&quot;&quot; $$loss(p,t)=âˆ’\\dfrac{1}{N}\\sum_{i=1}^{N}=\\dfrac{1}{N}[t_iâˆ—log(p_i)+(1âˆ’t_i)âˆ—log(1âˆ’p_i)]$$ example: 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchimport torch.nn as nnloss = nn.BCELoss(reduction=&quot;elementwise_mean&quot;)input = torch.randn(5)target = torch.ones(5)loss = loss(torch.sigmoid(input), target)my_loss = torch.mean(-target * torch.log(torch.sigmoid(input)) - (1-target) * torch.log((1-torch.sigmoid(input))))# test weight parameterloss1 = F.binary_cross_entropy(torch.sigmoid(input), target, reduction=&quot;none&quot;, weight=torch.Tensor([0,0,0,0,1]))loss2 = F.binary_cross_entropy(torch.sigmoid(input), target, weight=torch.Tensor([0,0,0,0,1]))print(my_loss, loss)print(loss1, loss2*5)# tensor(0.7590) tensor(0.7590)# tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3104]) tensor(0.3104) é€šå¸¸ä½¿ç”¨ sigmoid å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬é¢„æµ‹å¾—åˆ°æ­£åˆ†ç±»çš„æ¦‚ç‡ï¼Œç„¶åéœ€è¦äººä¸ºè®¾ç½® threshold æ¥åˆ¤æ–­æ¦‚ç‡è¾¾åˆ° threshold æ‰æ˜¯æ­£åˆ†ç±»ï¼Œæœ‰ç‚¹ç±»ä¼¼äº hingle loss å“¦ã€‚ torch.nn.CrossEntropyLossThis criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ binary_cross_entropy çš„æ‹“å±•ã€‚è®¡ç®—è¿‡ç¨‹å¯ä»¥åˆ†ä¸ºä¸¤æ­¥ï¼Œlog_softmax() å’Œ nn.NLLloss() It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. åœ¨ä¸å‡è¡¡æ•°æ®é›†ä¸­ï¼Œå‚æ•° weight ä¼šå¾ˆæœ‰ç”¨ã€‚ 1234567891011121314151617181920212223class CrossEntropyLoss(_WeightedLoss): def __init__(): &quot;&quot;&quot; - weights: ç»™æ¯ä¸€ä¸ªç±»åˆ«ä¸€ä¸ªæƒé‡ã€‚ - reduction: &quot;elementwise_mean&quot;|&quot;sum&quot;|&quot;none&quot;. &quot;&quot;&quot; def forward(): &quot;&quot;&quot; - input: [batch, C] or [batch, C, d_1, d_2, ..., d_k] - target: [batch], 0 &lt;= targte[i] &lt;= C-1, or [batch, d_1, d_2, ..., d_k], K &gt;= 2. &quot;&quot;&quot; example: 12345678910111213141516171819202122232425262728293031323334353637383940414243input = torch.randn(2, 3)target = torch.Tensor([0, 2]).long()# use loss functionloss_fn = nn.CrossEntropyLoss()loss = loss_fn(input, target)# compute loss step by stepscore = torch.log_softmax(input, dim=1)score1 = torch.log(F.softmax(input, dim=1))print(score)print(score1)# use nll lossnll_loss_fn = nn.NLLLoss()nll_loss = nll_loss_fn(score, target)# computer nll loss step by stepmy_nll = torch.mean(-score[0][0] - score[1][2])print(nll_loss, loss, my_nll) 1234567891011tensor([[-0.8413, -0.7365, -2.4073], [-0.4626, -2.0660, -1.4120]])tensor([[-0.8413, -0.7365, -2.4073], [-0.4626, -2.0660, -1.4120]])tensor(1.1266) tensor(1.1266) tensor(1.1266) torch.nn.NLLlossThe negative log likelihood loss. It is useful to train a classification problem with C class. input æ˜¯å·²ç»é€šè¿‡ log_softmax å±‚çš„è¾“å…¥ã€‚loss æ˜¯å¯¹åº”æ ·æœ¬ä¸­çœŸå®æ ‡ç­¾å¯¹åº”çš„å€¼çš„è´Ÿæ•°ã€‚ 12345678910111213class NLLLoss(_WeightedLoss): def __init__(): &quot;&quot;&quot; å‚æ•°è®¾ç½®è·Ÿ CrossEntropyLoss åŸºæœ¬ä¸€è‡´ã€‚ &quot;&quot;&quot; NLLloss $$\\ell(x, y) = L = {l_1,\\dots,l_N}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}{c \\not= \\text{ignore_index}}$$ exampleï¼š 12345678910111213141516171819202122232425262728293031loss = nn.NLLLoss()# input is of size N x C = 3 x 5input = torch.randn(3, 5, requires_grad=True)# each element in target has to have 0 &lt;= value &lt; Ctarget = torch.tensor([1, 0, 4])output = loss(torch.log_softmax(input, dim=1), target)score = torch.log_softmax(input, dim=1)output2 = (-score[0, 1]-score[1, 0]-score[2, 4])/3output.backward()# output2.backward()print(output, output2)# tensor(1.5658, grad_fn=&lt;NllLossBackward&gt;) tensor(1.5658, grad_fn=&lt;DivBackward0&gt;) MultiMarginLoss$loss = \\dfrac{1}{N}\\sum_{j\\ne y_i}^{N}max(0,s_j - s_{y_i}+\\Delta)$ $s_{yi}$ è¡¨ç¤ºå…¶çœŸå®æ ‡ç­¾å¯¹åº”çš„å€¼ï¼Œé‚£ä¹ˆå…¶ä»–éçœŸå®åˆ†ç±»çš„ç»“æœå‡¡æ˜¯å¤§äº $s_{yi}âˆ’\\Delta$ è¿™ä¸ªå€¼çš„ï¼Œéƒ½å¯¹æœ€åçš„ç»“æœ $loss$ äº§ç”Ÿå½±å“ï¼Œæ¯”è¿™ä¸ªå€¼å°çš„å°±æ²¡äº‹ï½ æ˜¾ç„¶æƒ³å¯¹äº softmax æŸå¤±å‡½æ•°æ¥è¯´ï¼Œsoftmax è€ƒè™‘åˆ°äº†æ‰€æœ‰çš„é”™åˆ†ç±»ï¼Œè€Œ marginloss åªè€ƒè™‘æ¦‚ç‡è¾ƒå¤§çš„é”™åˆ†ç±»ã€‚ 1234567891011121314151617181920212223242526272829303132333435class MultiMarginLoss(_WeightedLoss): def __init__(self, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='elementwise_mean'): &quot;&quot;&quot; - p (int, optional): Has a default value of `1`. `1` and `2` are the only supported values - margin (float, optional): Has a default value of `1`. &quot;&quot;&quot; super(MultiMarginLoss, self).__init__(weight, size_average, reduce, reduction) if p != 1 and p != 2: raise ValueError(&quot;only p == 1 and p == 2 supported&quot;) assert weight is None or weight.dim() == 1 self.p = p self.margin = margin def forward(self, input, target): return F.multi_margin_loss(input, target, p=self.p, margin=self.margin, weight=self.weight, reduction=self.reduction) example: 1234567891011121314151617loss = nn.MultiMarginLoss()input = torch.FloatTensor([[0, 3, 1], [0, 4, 2], [1, 5, 2], [3, 5, 1]])target = torch.ones(4).long()out = loss(input, target)print(out) # æ˜¾ç„¶åº”è¯¥æ˜¯ 0,å› ä¸ºè´Ÿåˆ†ç±»ä¸çœŸå®æ ‡ç­¾çš„ socre å·®å€¼éƒ½å¤§äºç­‰äº 1.# tensor(0.) nn.L1loss$$L1(\\hat{y}, y)=\\dfrac{1}{m}\\sum|\\hat{y}_iâˆ’y_i|$$ nn.MSEloss$$L2(\\hat{y}, y)=\\dfrac{1}{m}\\sum|\\hat{y}_iâˆ’y_i|^2$$ 1234567891011121314151617181920212223loss = nn.L1Loss()loss2 = nn.MSELoss()input = torch.FloatTensor([1,2,3])target = torch.FloatTensor([1,2,9])output = loss(input, target)output2 = loss2(input, target)print(output, output2)# tensor(2.) tensor(12.)","link":"/2018/12/07/pytorch-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"title":"Tensorflow Attention API æºç é˜…è¯»1","text":"è¿™èŠ‚å†…å®¹æ˜¯è¯¦ç»†äº†è§£ä¸‹ tensorflow å…³äº attention çš„apiï¼Œç”±äºå°è£…çš„å¤ªå¥½ï¼Œä¹‹å‰ä½¿ç”¨è¿‡å‘ç°æŠ¥é”™äº†å¾ˆéš¾å»æ‰¾å‡ºå“ªå„¿ bugï¼Œæ‰€ä»¥ç”¨ eager execution æ¥çœ‹å…·ä½“ç»†èŠ‚ã€‚ æŒ‰ç…§å®˜æ–¹æ•™ç¨‹ Seq2seq Library (contrib) è¿™é‡Œçš„æµç¨‹é€æ­¥æ·±å…¥ã€‚ This library is composed of two primary components: New attention wrappers for tf.contrib.rnn.RNNCell objects. A new object-oriented dynamic decoding framework. ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ˜¯æ–°çš„åŸºäº attention çš„ RNNCell å¯¹è±¡ï¼Œä¸€ä¸ªé¢å‘å¯¹è±¡çš„åŠ¨æ€è§£ç æ¡†æ¶ã€‚ AttentionAttention wrappers are RNNCell objects that wrap other RNNCell objects and implement attention. The form of attention is determined by a subclass of tf.contrib.seq2seq.AttentionMechanism. These subclasses describe the form of attention (e.g. additive vs. multiplicative) to use when creating the wrapper. An instance of an AttentionMechanism is constructed with a memory tensor, from which lookup keys and values tensors are created. attenion wrapper ä¹Ÿæ˜¯ RNNCell å¯¹è±¡ï¼Œçˆ¶ç±»æ˜¯ tf.contrib.seq2seq.AttentionMechanism,ç„¶åå…¶å­ç±»æ˜¯é’ˆå¯¹ä¸åŒ attention å½¢å¼ï¼ˆadditive vs. multiplicativeï¼‰çš„å®ç°ã€‚AttentionMechanism çš„æ„é€ æ˜¯åœ¨ memory çš„åŸºç¡€ä¸Šï¼Œmemory ä¹Ÿå°±æ˜¯ attention è¿‡ç¨‹ä¸­çš„ keys values. Attention Mechanismsattention çš„æå‡ºæ¥è‡ªäºï¼š paper: Neural Machine Translation by Jointly Learning to Align and Translate paper:Effective Approaches to Attention-based Neural Machine Translation encoder é‡‡ç”¨å•å±‚æˆ–å¤šå±‚çš„å•å‘æˆ–åŒå‘çš„ rnn å¾—åˆ°source sentence çš„éšè—çŠ¶æ€è¡¨ç¤º $H=[h_1,â€¦,h_T]$ã€‚ decoder çš„ t æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä¸º $s_t$, åœ¨ decoder é˜¶æ®µä¹Ÿæ˜¯ rnnï¼Œå…¶ä¸­éšè—çŠ¶æ€çš„æ›´æ–°ä¸ºï¼š $s_i=f(s_{i-1},y_{i-1},c_i)$ å…¶ä¸­ $s_{i-1}$ æ˜¯ä¸Šä¸€ä¸ªéšè—çŠ¶æ€ï¼Œ$y_{i-1}$ æ˜¯ä¸Šä¸€æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œ$c_i$ æ˜¯å½“å‰æ—¶é—´æ­¥çš„ attention vector. é‚£ä¹ˆç°åœ¨å°±æ˜¯æ€ä¹ˆè®¡ç®—å½“å‰æ—¶é—´æ­¥çš„ $c_i$. å½“å‰æ—¶é—´æ­¥çš„ $e_t=a(s_{i-1}, h_j)$, è¿™æ˜¯å¯¹é½æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯è®¡ç®—ä¸Šä¸€ä¸ªéšè—çŠ¶æ€ $s_{i-1}$ ä¸ encoder ä¸­æ¯ä¸€ä¸ª hidden çš„ match ç¨‹åº¦ï¼Œè®¡ç®—è¿™ä¸ª score æœ‰å¾ˆå¤šä¸­æ–¹å¼ï¼Œå…¶ä¸­æœ€å¸¸è§çš„ï¼Œä¹Ÿæ˜¯ tf api ä¸­ä½¿ç”¨çš„ä¸¤ç§ BahdanauAttention å’Œ LuongAttention. $$\\text{BahdanauAttention:}\\quad e_{ij}=v_a^Ttanh(W^as_{i-1}+U_ah_j)$$ $$\\text{LuongAttention:}\\quad e_{ij}=h_j^TW^as_i$$ ç„¶åå¯¹å¾—åˆ°çš„å¯¹é½ score ä½¿ç”¨ softmax å¾—åˆ°ç›¸åº”çš„æ¦‚ç‡: $$a_{ij}=\\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$$ softmax å®é™…ä¸Šç›¸æ¯”ä¸Šé¢çš„å…¬å¼æœ‰ç‚¹åŒºåˆ«ï¼Œå°±æ˜¯ $exp(e^{ij}-max(e^{ik}))$ é˜²æ­¢æ•°å€¼æº¢å‡ºã€‚ å°†å¾—åˆ°çš„ $s_{i-1}$ ä¸ encoder ä¸­çš„ $h_j$ è®¡ç®—å¾—åˆ°çš„æ¦‚ç‡ä¸ $h_j$ åšåŠ æƒå’Œå¾—åˆ°å½“å‰æ—¶é—´æ­¥çš„ attention vector $c_i$ åœ¨ç„¶åä½¿ç”¨ $c_{i-1},s_{i-1},y_{i-1}$ æ›´æ–°decoder ä¸­çš„éšè—çŠ¶æ€ï¼Œå¾ªç¯ä¸‹å»ã€‚ã€‚ã€‚ æ ¹æ®å½“å‰çš„éšè—çŠ¶æ€ $s_i$ è®¡ç®—å¾—åˆ°å½“å‰æ—¶é—´æ­¥çš„è¾“å‡º $y_t$ $$y_t=Ws_{i}+b$$ å…ˆçœ‹çˆ¶ç±» tf.contrib.seq2seq.AttentionMechanismæºç ï¼š 12345678910111213141516171819class AttentionMechanism(object): @property def alignments_size(self): raise NotImplementedError @property def state_size(self): raise NotImplementedError ä¸¤ä¸ªå±æ€§ï¼š alignments_size å’Œ state_size åˆ†åˆ«å¯¹åº” sequence çš„é•¿åº¦ï¼Œæ‰€ä»¥è¿™ä¸ª alignment_size æ˜¯è¡¨ç¤º mask ä¹‹åçš„é•¿åº¦å§ï¼Ÿæ¥ä¸‹æ¥çœ‹æºç ã€‚ state_size è¡¨ç¤ºéšè—å±‚çš„çŠ¶æ€ã€‚æ˜¾ç„¶è¿™é‡Œçš„ attention ä¹Ÿæ˜¯ä¸€ä¸ªæ—¶é—´æ­¥å†…çš„è®¡ç®—ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261class _BaseAttentionMechanism(AttentionMechanism): &quot;&quot;&quot;A base AttentionMechanism class providing common functionality. Common functionality includes: 1. Storing the query and memory layers. 2. Preprocessing and storing the memory. &quot;&quot;&quot; def __init__(self, query_layer, memory, probability_fn, memory_sequence_length=None, memory_layer=None, check_inner_dims_defined=True, score_mask_value=None, name=None): &quot;&quot;&quot;Construct base AttentionMechanism class. Args: query_layer: Callable. Instance of `tf.layers.Layer`. The layer's depth must match the depth of `memory_layer`. If `query_layer` is not provided, the shape of `query` must match that of `memory_layer`. memory: The memory to query; usually the output of an RNN encoder. This tensor should be shaped `[batch_size, max_time, ...]`. probability_fn: A `callable`. Converts the score and previous alignments to probabilities. Its signature should be: `probabilities = probability_fn(score, state)`. memory_sequence_length (optional): Sequence lengths for the batch entries in memory. If provided, the memory tensor rows are masked with zeros for values past the respective sequence lengths. memory_layer: Instance of `tf.layers.Layer` (may be None). The layer's depth must match the depth of `query_layer`. If `memory_layer` is not provided, the shape of `memory` must match that of `query_layer`. check_inner_dims_defined: Python boolean. If `True`, the `memory` argument's shape is checked to ensure all but the two outermost dimensions are fully defined. score_mask_value: (optional): The mask value for score before passing into `probability_fn`. The default is -inf. Only used if `memory_sequence_length` is not None. name: Name to use when creating ops. &quot;&quot;&quot; if (query_layer is not None and not isinstance(query_layer, layers_base.Layer)): raise TypeError( &quot;query_layer is not a Layer: %s&quot; % type(query_layer).__name__) if (memory_layer is not None and not isinstance(memory_layer, layers_base.Layer)): raise TypeError( &quot;memory_layer is not a Layer: %s&quot; % type(memory_layer).__name__) self._query_layer = query_layer self._memory_layer = memory_layer self.dtype = memory_layer.dtype if not callable(probability_fn): raise TypeError(&quot;probability_fn must be callable, saw type: %s&quot; % type(probability_fn).__name__) if score_mask_value is None: score_mask_value = dtypes.as_dtype( self._memory_layer.dtype).as_numpy_dtype(-np.inf) self._probability_fn = lambda score, prev: ( # pylint:disable=g-long-lambda probability_fn( _maybe_mask_score(score, memory_sequence_length, score_mask_value), prev)) with ops.name_scope( name, &quot;BaseAttentionMechanismInit&quot;, nest.flatten(memory)): self._values = _prepare_memory( memory, memory_sequence_length, check_inner_dims_defined=check_inner_dims_defined) self._keys = ( self.memory_layer(self._values) if self.memory_layer # pylint: disable=not-callable else self._values) self._batch_size = ( self._keys.shape[0].value or array_ops.shape(self._keys)[0]) self._alignments_size = (self._keys.shape[1].value or array_ops.shape(self._keys)[1]) @property def memory_layer(self): return self._memory_layer @property def query_layer(self): return self._query_layer @property def values(self): return self._values @property def keys(self): return self._keys @property def batch_size(self): return self._batch_size @property def alignments_size(self): return self._alignments_size @property def state_size(self): return self._alignments_size def initial_alignments(self, batch_size, dtype): &quot;&quot;&quot;Creates the initial alignment values for the `AttentionWrapper` class. This is important for AttentionMechanisms that use the previous alignment to calculate the alignment at the next time step (e.g. monotonic attention). The default behavior is to return a tensor of all zeros. Args: batch_size: `int32` scalar, the batch_size. dtype: The `dtype`. Returns: A `dtype` tensor shaped `[batch_size, alignments_size]` (`alignments_size` is the values' `max_time`). &quot;&quot;&quot; max_time = self._alignments_size return _zero_state_tensors(max_time, batch_size, dtype) def initial_state(self, batch_size, dtype): &quot;&quot;&quot;Creates the initial state values for the `AttentionWrapper` class. This is important for AttentionMechanisms that use the previous alignment to calculate the alignment at the next time step (e.g. monotonic attention). The default behavior is to return the same output as initial_alignments. Args: batch_size: `int32` scalar, the batch_size. dtype: The `dtype`. Returns: A structure of all-zero tensors with shapes as described by `state_size`. &quot;&quot;&quot; return self.initial_alignments(batch_size, dtype) è¿™ä¸ªç±» _BaseAttentionMechanism æ˜¯æœ€åŸºæœ¬çš„ attention ç±»äº†ã€‚å¯ä»¥çœ‹åˆ° self._keys å’Œ self._values çš„è®¡ç®—æ–¹å¼éƒ½æ˜¯éœ€è¦è€ƒè™‘ memory_sequence_length è¿™ä¸ªå‚æ•°çš„ã€‚ æœ‰è¿™å‡ ä¸ªå±æ€§ï¼š values: å…¶è®¡ç®—ä½¿ç”¨äº† _prepare_memory å‡½æ•°å¯¹åº”çš„æ˜¯æŠŠè¾“å…¥åºåˆ— memory çš„è¶…è¿‡å¯¹åº”å®é™…é•¿åº¦çš„éƒ¨åˆ†çš„å€¼å˜ä¸º 0 keysï¼š self._keys = self.memory_layer(self._values) æ˜¯åœ¨å¾—åˆ°äº† values ä¹‹åè¿›è¡Œå…¨é“¾æ¥çš„å€¼ï¼Œå…¶shape=[batch, max_times, num_units] state_size å’Œ alignment_size æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯ max_times self._probability_fn(score, prev) ä½¿ç”¨äº† _maybe_mask_score è¿™ä¸ªå‡½æ•°è®¡ç®—å¾—åˆ° score ä¹‹åå¹¶ mask çš„æ¦‚ç‡ï¼Œç„¶åè¿˜è¦åˆ©ç”¨ prev state? _maybe_mask_scoreæºç ï¼š 123456789101112131415161718192021def _maybe_mask_score(score, memory_sequence_length, score_mask_value): if memory_sequence_length is None: return score message = (&quot;All values in memory_sequence_length must greater than zero.&quot;) with ops.control_dependencies( [check_ops.assert_positive(memory_sequence_length, message=message)]): score_mask = array_ops.sequence_mask( memory_sequence_length, maxlen=array_ops.shape(score)[1]) score_mask_values = score_mask_value * array_ops.ones_like(score) return array_ops.where(score_mask, score, score_mask_values) 12345score = tf.random_uniform(shape=[2,10])tf.shape(score).numpy() array([ 2, 10], dtype=int32) 12345678910111213141516171819score = tf.random_uniform(shape=[2,10])memeory_sequence_len = [5,8]score_mask_value = -100000000score_mask = tf.sequence_mask(lengths=memeory_sequence_len, maxlen=tf.shape(score)[1])print(&quot;true or false: %s\\n&quot; %score_mask)score_mask_values = score_mask_value * tf.ones_like(score)print(&quot;-inf: %s\\n&quot;%score_mask_values)ans = tf.where(score_mask, score, score_mask_values)print(ans) true or false: tf.Tensor( [[ True True True True True False False False False False] [ True True True True True True True True False False]], shape=(2, 10), dtype=bool) -inf: tf.Tensor( [[-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08] [-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08]], shape=(2, 10), dtype=float32) tf.Tensor( [[ 2.3987615e-01 4.9896538e-01 7.2822869e-01 4.7516704e-02 1.6099060e-01 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08] [ 3.5503960e-01 2.5502288e-01 8.1264114e-01 4.3110681e-01 1.1858845e-01 2.5748730e-02 4.8437893e-01 2.8339624e-02 -1.0000000e+08 -1.0000000e+08]], shape=(2, 10), dtype=float32) _prepare_memory\\12345self._keys = _prepare_memory(memory, memory_sequence_length,check_inner_dims_defined=check_inner_dims_defined) å…¶ä¸­ _prepare_memory è¿™ä¸ªå‡½æ•°,ä¹Ÿå°±æ˜¯æ€ä¹ˆè®¡ç®— mask çš„ï¼Œå…¶è®¡ç®—å¦‚ä¸‹ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107def _prepare_memory(memory, memory_sequence_length, check_inner_dims_defined): &quot;&quot;&quot;Convert to tensor and possibly mask `memory`. Args: memory: `Tensor`, shaped `[batch_size, max_time, ...]`. memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`. check_inner_dims_defined: Python boolean. If `True`, the `memory` argument's shape is checked to ensure all but the two outermost dimensions are fully defined. Returns: A (possibly masked), checked, new `memory`. Raises: ValueError: If `check_inner_dims_defined` is `True` and not `memory.shape[2:].is_fully_defined()`. &quot;&quot;&quot; memory = nest.map_structure( lambda m: ops.convert_to_tensor(m, name=&quot;memory&quot;), memory) if memory_sequence_length is not None: memory_sequence_length = ops.convert_to_tensor( memory_sequence_length, name=&quot;memory_sequence_length&quot;) if check_inner_dims_defined: def _check_dims(m): if not m.get_shape()[2:].is_fully_defined(): raise ValueError(&quot;Expected memory %s to have fully defined inner dims, &quot; &quot;but saw shape: %s&quot; % (m.name, m.get_shape())) nest.map_structure(_check_dims, memory) if memory_sequence_length is None: seq_len_mask = None else: seq_len_mask = array_ops.sequence_mask( memory_sequence_length, maxlen=array_ops.shape(nest.flatten(memory)[0])[1], dtype=nest.flatten(memory)[0].dtype) seq_len_batch_size = ( memory_sequence_length.shape[0].value or array_ops.shape(memory_sequence_length)[0]) def _maybe_mask(m, seq_len_mask): rank = m.get_shape().ndims rank = rank if rank is not None else array_ops.rank(m) extra_ones = array_ops.ones(rank - 2, dtype=dtypes.int32) m_batch_size = m.shape[0].value or array_ops.shape(m)[0] if memory_sequence_length is not None: message = (&quot;memory_sequence_length and memory tensor batch sizes do not &quot; &quot;match.&quot;) with ops.control_dependencies([ check_ops.assert_equal( seq_len_batch_size, m_batch_size, message=message)]): seq_len_mask = array_ops.reshape( seq_len_mask, array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0)) return m * seq_len_mask else: return m return nest.map_structure(lambda m: _maybe_mask(m, seq_len_mask), memory) _prepare_memory å…¶å®å¾ˆç®€å•ï¼Œå°±æ˜¯æ ¹æ® batch ä¸­æ¯ä¸ªæ ·æœ¬çš„å®é™…é•¿åº¦ï¼Œå°†è¶…å‡ºéƒ¨åˆ†è®¾ç½®ä¸º 0 tf.contrib.seq2seq.BahdanauAttentionè¿™é‡Œæ¶‰åŠåˆ°äº†ä¸¤ç¯‡ paper: [Neural Machine Translation by Jointly Learning to Align and Translate.â€ ICLR 2015. ](https://arxiv.org/abs/1409.0473) [Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.â€](https://arxiv.org/abs/1602.07868) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class BahdanauAttention(_BaseAttentionMechanism): &quot;&quot;&quot;Implements Bahdanau-style (additive) attention. This attention has two forms. The first is Bahdanau attention, as described in: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot; ICLR 2015. https://arxiv.org/abs/1409.0473 The second is the normalized form. This form is inspired by the weight normalization article: Tim Salimans, Diederik P. Kingma. &quot;Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.&quot; https://arxiv.org/abs/1602.07868 To enable the second form, construct the object with parameter `normalize=True`. &quot;&quot;&quot; def __init__(self, num_units, memory, memory_sequence_length=None, normalize=False, probability_fn=None, score_mask_value=None, dtype=None, name=&quot;BahdanauAttention&quot;): &quot;&quot;&quot;Construct the Attention mechanism. Args: num_units: The depth of the query mechanism. memory: The memory to query; usually the output of an RNN encoder. This tensor should be shaped `[batch_size, max_time, ...]`. memory_sequence_length (optional): Sequence lengths for the batch entries in memory. If provided, the memory tensor rows are masked with zeros for values past the respective sequence lengths. normalize: Python boolean. Whether to normalize the energy term. probability_fn: (optional) A `callable`. Converts the score to probabilities. The default is @{tf.nn.softmax}. Other options include @{tf.contrib.seq2seq.hardmax} and @{tf.contrib.sparsemax.sparsemax}. Its signature should be: `probabilities = probability_fn(score)`. score_mask_value: (optional): The mask value for score before passing into `probability_fn`. The default is -inf. Only used if `memory_sequence_length` is not None. dtype: The data type for the query and memory layers of the attention mechanism. name: Name to use when creating ops. &quot;&quot;&quot; num_units æ˜¯query mechanism çš„ç»´åº¦. å®ƒå¯ä»¥æ—¢ä¸æ˜¯ query çš„ç»´åº¦,ä¹Ÿå¯ä»¥ä¸æ˜¯ memory çš„ç»´åº¦å¯¹å§? query çš„ç»´åº¦è¦å’Œ memory(ä¹Ÿå°±æ˜¯ keys/values) çš„ç»´åº¦ä¸€è‡´å—?æ˜¯ä¸éœ€è¦çš„.åœ¨ BahdanauAttention çš„å®ç°ä¸­æ¯”è¾ƒå¥½ç†è§£,ä¸¤ä¸ªå…¨é“¾æ¥æœ€åçš„ç»´åº¦ä¸€è‡´å³å¯ç›¸åŠ .ä½†æ˜¯åœ¨ LuongAttention ä¸­çŸ©é˜µçŸ©é˜µç›¸ä¹˜æ—¶éœ€è¦æ³¨æ„ç»´åº¦å˜åŒ–. memory_sequence_length: è¿™ä¸ªå‚æ•°å¾ˆé‡è¦, mask æ¶ˆé™¤ padding çš„å½±å“. score_mask_value: ä¸Šä¸€ä¸ªå‚æ•°å­˜åœ¨æ—¶,è¿™ä¸ªå‚æ•°æ‰ä¼šä½¿ç”¨,é»˜è®¤ä¸º -inf. ç»§ç»­çœ‹æºç çš„å®ç°: 12345678910111213141516171819202122232425262728293031323334353637if probability_fn is None: probability_fn = nn_ops.softmaxif dtype is None: dtype = dtypes.float32wrapped_probability_fn = lambda score, _: probability_fn(score)super(BahdanauAttention, self).__init__( query_layer=layers_core.Dense( num_units, name=&quot;query_layer&quot;, use_bias=False, dtype=dtype), memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name)self._num_units = num_unitsself._normalize = normalizeself._name = name ç°åœ¨ç†è§£äº† _BaseAttentionMechanism è¿™ä¸ªç±»ä¸­ query_layer å’Œ memory_layer çš„æ„ä¹‰äº†. score_mask_value æ²¿ç”¨çˆ¶ç±»ä¸­çš„è®¡ç®—æ–¹å¼. ç»§ç»­çœ‹ call å‡½æ•°,ä¹Ÿå°±æ˜¯ attention çš„è®¡ç®—æ–¹å¼ 123456789101112131415161718192021222324252627282930313233343536373839def __call__(self, query, state): &quot;&quot;&quot;Score the query based on the keys and values. Args: query: Tensor of dtype matching `self.values` and shape `[batch_size, query_depth]`. state: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). Returns: alignments: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). &quot;&quot;&quot; with variable_scope.variable_scope(None, &quot;bahdanau_attention&quot;, [query]): processed_query = self.query_layer(query) if self.query_layer else query score = _bahdanau_score(processed_query, self._keys, self._normalize) alignments = self._probability_fn(score, state) next_state = alignments return alignments, next_state ç„¶åçœ‹æ€ä¹ˆè®¡ç®—çš„ score. score = _bahdanau_score(processed_query, self._keys, self._normalize) å…¶ä¸­ processed_query å’Œ self._keys éƒ½æ˜¯é€šè¿‡å…¨é“¾æ¥å±‚åå¾—åˆ°çš„, [batch, alignments_size, num_units] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def _bahdanau_score(processed_query, keys, normalize): &quot;&quot;&quot;Implements Bahdanau-style (additive) scoring function. &quot;&quot;&quot; dtype = processed_query.dtype # Get the number of hidden units from the trailing dimension of keys num_units = keys.shape[2].value or array_ops.shape(keys)[2] # Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting. processed_query = array_ops.expand_dims(processed_query, 1) v = variable_scope.get_variable( &quot;attention_v&quot;, [num_units], dtype=dtype) if normalize: # Scalar used in weight normalization g = variable_scope.get_variable( &quot;attention_g&quot;, dtype=dtype, initializer=init_ops.constant_initializer(math.sqrt((1. / num_units))), shape=()) # Bias added prior to the nonlinearity b = variable_scope.get_variable( &quot;attention_b&quot;, [num_units], dtype=dtype, initializer=init_ops.zeros_initializer()) # normed_v = g * v / ||v|| normed_v = g * v * math_ops.rsqrt( math_ops.reduce_sum(math_ops.square(v))) return math_ops.reduce_sum( normed_v * math_ops.tanh(keys + processed_query + b), [2]) else: return math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [2]) æºç ä¸­è®¡ç®— score çš„æœ€åä¸€æ­¥ä¸æ˜¯å…¨é“¾æ¥ï¼Œè€Œæ˜¯è¿™æ ·çš„ï¼š 12345v = tf.get_variable(&quot;attention_v&quot;, [num_units])score = tf.reduce_sum(v * tanh(keys + processed_query), [2]) 1234567891011121314151617181920212223import tensorflow as tfimport numpy as npimport tensorflow.contrib.eager as tfetf.enable_eager_execution()print(tfe.executing_eagerly())memory = tf.ones(shape=[1, 10, 5]) # batch=1, max_sequence_len=10, embed_size=5memory_sequence_len = [5] # æœ‰æ•ˆé•¿åº¦ä¸º 5attention_mechnism = tf.contrib.seq2seq.BahdanauAttention(num_units=32, memory=memory, memory_sequence_length=memory_sequence_len) True 123print(attention_mechnism.state_size, attention_mechnism.alignments_size) 10 10 123memory &lt;tf.Tensor: id=3, shape=(1, 10, 5), dtype=float32, numpy= array([[[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]], dtype=float32)&gt; 123attention_mechnism.values # å¯ä»¥å‘ç° values å°±æ˜¯æŠŠ memory ä¸­è¶…è¿‡memory_sequence_length çš„éƒ¨åˆ†å˜ä¸º 0 &lt;tf.Tensor: id=30, shape=(1, 10, 5), dtype=float32, numpy= array([[[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]], dtype=float32)&gt; 12345print(attention_mechnism.keys.shape) # ç»è¿‡äº†å…¨é“¾æ¥ä¹‹åçš„attention_mechnism.keys.numpy()[0,1,:] (1, 10, 32) array([ 0.09100786, 0.18448338, -0.7751561 , 0.00775184, 0.467805 , 0.9172474 , 0.57645243, -0.3915946 , -0.22213435, 0.76866853, 0.3591721 , 0.8922573 , 0.15866229, 0.6033571 , 0.51816225, 0.3820553 , -0.39130217, 0.04532939, -0.02089322, 0.6878175 , -0.28697258, 0.59283376, -0.37825382, -0.5865691 , 0.17466056, -0.5915747 , 0.6070496 , -0.18531135, -0.821724 , 1.2838829 , 0.15700272, -0.2608306 ], dtype=float32) 123print(attention_mechnism.query_layer, attention_mechnism.memory_layer) &lt;tensorflow.python.layers.core.Dense object at 0x7fa0464da908&gt; &lt;tensorflow.python.layers.core.Dense object at 0x7fa0464dab38&gt; 123456789# åˆ©ç”¨ call å‡½æ•°æ¥è®¡ç®—ä¸‹ä¸€ä¸ª state å’Œ attention vectorquery = tf.ones(shape=[1, 8]) # query_depth = 10state_h0 = attention_mechnism.initial_alignments(batch_size=1, dtype=tf.float32)attention_vector = attention_mechnism(query=query, state=state_h0) 123print(attention_vector) (&lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;, &lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;) tf.contrib.seq2seq.LuongAttentionpaper: Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class LuongAttention(_BaseAttentionMechanism): &quot;&quot;&quot;Implements Luong-style (multiplicative) attention scoring. &quot;&quot;&quot; def __init__(self, num_units, memory, memory_sequence_length=None, scale=False, probability_fn=None, score_mask_value=None, dtype=None, name=&quot;LuongAttention&quot;): if probability_fn is None: probability_fn = nn_ops.softmax if dtype is None: dtype = dtypes.float32 wrapped_probability_fn = lambda score, _: probability_fn(score) super(LuongAttention, self).__init__( query_layer=None, memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name) self._num_units = num_units self._scale = scale self._name = name å¯ä»¥å‘ç° query æ²¡æœ‰ç»è¿‡ query_layer çš„å¤„ç†ï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰å…¨é“¾æ¥ã€‚ä½†æ˜¯ memory è¿˜æ˜¯è¦ç”¨å…¨é“¾æ¥å¤„ç†çš„ï¼Œå¾—åˆ° [batch, max_times, num_units] å†çœ‹ä½¿ç”¨ call å‡½æ•°è®¡ç®—å¯¹å…¶æ¦‚ç‡ alignment å’Œ next_state. 12345678910111213141516171819202122232425262728293031323334353637def __call__(self, query, state): &quot;&quot;&quot;Score the query based on the keys and values. Args: query: Tensor of dtype matching `self.values` and shape `[batch_size, query_depth]`. state: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). Returns: alignments: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). &quot;&quot;&quot; with variable_scope.variable_scope(None, &quot;luong_attention&quot;, [query]): score = _luong_score(query, self._keys, self._scale) alignments = self._probability_fn(score, state) next_state = alignments return alignments, next_state æ¥ä¸‹æ¥çœ‹æ€ä¹ˆè®¡ç®—çš„ score 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def _luong_score(query, keys, scale): &quot;&quot;&quot;Implements Luong-style (multiplicative) scoring function. Args: query: Tensor, shape `[batch_size, num_units]` to compare to keys. keys: Processed memory, shape `[batch_size, max_time, num_units]`. scale: Whether to apply a scale to the score function. Returns: A `[batch_size, max_time]` tensor of unnormalized score values. Raises: ValueError: If `key` and `query` depths do not match. &quot;&quot;&quot; depth = query.get_shape()[-1] key_units = keys.get_shape()[-1] if depth != key_units: raise ValueError( &quot;Incompatible or unknown inner dimensions between query and keys. &quot; &quot;Query (%s) has units: %s. Keys (%s) have units: %s. &quot; &quot;Perhaps you need to set num_units to the keys' dimension (%s)?&quot; % (query, depth, keys, key_units, key_units)) dtype = query.dtype # Reshape from [batch_size, depth] to [batch_size, 1, depth] # for matmul. query = array_ops.expand_dims(query, 1) # Inner product along the query units dimension. # matmul shapes: query is [batch_size, 1, depth] and # keys is [batch_size, max_time, depth]. # the inner product is asked to **transpose keys' inner shape** to get a # batched matmul on: # [batch_size, 1, depth] . [batch_size, depth, max_time] # resulting in an output shape of: # [batch_size, 1, max_time]. # we then squeeze out the center singleton dimension. score = math_ops.matmul(query, keys, transpose_b=True) score = array_ops.squeeze(score, [1]) if scale: # Scalar used in weight scaling g = variable_scope.get_variable( &quot;attention_g&quot;, dtype=dtype, initializer=init_ops.ones_initializer, shape=()) score = g * score return score é€šè¿‡æºç å¯ä»¥å‘ç° LuongAttention è°ƒç”¨ call å‡½æ•°æ—¶ï¼Œå…¶ query çš„ç»´åº¦å¿…é¡»æ˜¯ num_units. è€Œ BahdanauAttention å¹¶ä¸éœ€è¦ã€‚ å…¶æ˜¯è®¡ç®— score çš„æ–¹å¼å¦‚ä¸‹ï¼š 123456789101112131415161718192021222324252627282930313233343536373839batch_size = 2query_depth = num_units = 32memory_depth = 15max_times = 10embed_size = 5scale = Truequery = tf.random_normal(shape=[batch_size, num_units])# memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])# values = self._prepaer_memory(memory)# keys = memory_layer(values)values = tf.random_normal(shape=[batch_size, max_times, memory_depth])keys = tf.layers.dense(inputs=values, units=num_units) # [batch, max_times, num_units]query = tf.expand_dims(query, axis=1) # [batch, 1, num_units]score = tf.matmul(query, keys, transpose_b=True) # [batch, 1, max_times]score = tf.squeeze(score, axis=1) # [batch, max_times]print(score.shape) (2, 10) 12345678910111213141516171819202122232425### å®Œæ•´çš„è¿‡ä¸€émemory = tf.random_normal(shape=[batch_size, max_times, memory_depth])memory_sequence_len = [5,8]query_len = 5query = tf.random_normal(shape=[batch_size, num_units])state = tf.zeros(shape=[batch_size, max_times])attention_mechnism = tf.contrib.seq2seq.LuongAttention(num_units=num_units, memory=memory, memory_sequence_length=memory_sequence_len)attention_vector = attention_mechnism(query, state)attention_vector[0], attention_vector[1] # attention_vector å’Œ state (&lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy= array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02, 6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06, 3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04, 0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;, &lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy= array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02, 6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06, 3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04, 0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;) 123tf.reduce_sum(attention_vector[0][1]).numpy() 1.0 è¿™åªæ˜¯é’ˆå¯¹å•ä¸ª query çš„æƒ…å†µï¼Œä½†å®é™…ä¸Š query ä¸€èˆ¬æ˜¯è¿™æ ·çš„ [batch, query_len, num_units]ï¼Œé‚£æ€ä¹ˆåŠå‘¢ï¼Ÿ æ€»ç»“æœ€åæ€»ç»“ä¸€ä¸‹å†çœ‹ä¸€éä¸¤ä¸ª attention åˆå§‹åŒ–çš„å·®å¼‚ 123456789101112131415161718192021super(BahdanauAttention, self).__init__( query_layer=layers_core.Dense( num_units, name=&quot;query_layer&quot;, use_bias=False, dtype=dtype), memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name) 12345678910111213141516171819super(LuongAttention, self).__init__( query_layer=None, memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name) ä½œä¸ºä¸€ä¸ªç±»å¯¹è±¡æ—¶ï¼ŒAttentionMechanismï¼ŒBahdanauAttentionï¼ŒLuongAttentionå®ƒä»¬å…·æœ‰å¦‚ä¸‹å±æ€§ï¼š query_layer: åœ¨ BahdanauAttention ä¸­ä¸€èˆ¬æ˜¯ tf.layer.dense çš„å®ä¾‹å¯¹è±¡ï¼Œå…¶ç»´åº¦æ˜¯ num_units. æ‰€ä»¥ BahdanauAttention ä¸­ query çš„ç»´åº¦å¯ä»¥æ˜¯ä»»æ„å€¼ã€‚è€Œ LuongAttention ä¸­ query_layer ä¸º Noneï¼Œæ‰€ä»¥ query çš„ç»´åº¦åªèƒ½æ˜¯ num_units. memory_layer: åœ¨ä¸¤ä¸ª attention ä¸­éƒ½æ˜¯ä¸€æ ·çš„ï¼Œtf.layer.dense,ä¸”ç»´åº¦ä¸º num_units. alignments_size: å¯¹é½sizeï¼Œæ˜¯ memory çš„ max_times. batch_size: æ‰¹é‡å¤§å° values: æ˜¯ç»è¿‡ mask å¤„ç†åçš„ memory. [batch, max_times, embed_size] keys: æ˜¯ç»è¿‡ memory_layer å…¨é“¾æ¥å¤„ç†åçš„ã€‚ [batch, max_times, num_units]. state_size: ç­‰äº alignment_size. ç„¶åæ˜¯å¯¹åº”çš„æ–¹æ³•ï¼š init: åˆå§‹åŒ–ç±»å®ä¾‹ï¼Œé‡Œé¢çš„å‚æ•°ï¼š num_units: åœ¨ Bahdanau ä¸­è¿™ä¸ªå‚æ•°å…¶å®æ˜¯ä¸ªä¸­é—´å€¼ï¼Œå°† query å’Œ keys è½¬åŒ–ä¸ºè¿™ä¸ªç»´åº¦ï¼Œå åŠ ï¼Œä½†æœ€åè¿˜æ˜¯è¦åœ¨è¿™ä¸ªç»´åº¦ä¸Š reduce_sumï¼ ä½†æ˜¯åœ¨ LuongAttention ä¸­å®ƒå¿…é¡»å’Œ query çš„ç»´åº¦ä¸€è‡´ï¼Œç„¶åå’Œ memory_layer å¤„ç†åçš„ memory åšçŸ©é˜µç›¸ä¹˜ã€‚ memory: [batch, max_times, embed_size] normalize: æ˜¯ä½›æœ‰å½’ä¸€åŒ– probability_fn: tf.nn.softmaxï¼Œtf.contrib.seq2seq.hardmaxï¼Œ tf.contrib.sparsemax.sparsemax memory_sequence_lengthï¼š æ²¡æœ‰ç»è¿‡ padding æ—¶ memory çš„é•¿åº¦ã€‚å…¶ç»´åº¦åº”è¯¥æ˜¯ [1, batch_size] call(query, state) è°ƒç”¨è¯¥å®ä¾‹ query: [batch_size, query_length]. åœ¨ LuongAttention ä¸­ query_length å¿…é¡»ç­‰äº num_units. state: [batch_size, alignments_size]. ä¸€ç›´ä¸å¤ªç†è§£ state æœ‰å•¥ç”¨ï¼Ÿåœ¨æºç ä¸­æ˜¯ç”¨æ¥è®¡ç®— alignments çš„ï¼š 123456789101112131415161718192021222324252627282930313233alignments = self._probability_fn(score, state)self._probability_fn = lambda score, prev: ( # pylint:disable=g-long-lambda probability_fn( _maybe_mask_score(score, memory_sequence_length, score_mask_value), prev))# å…¶ä¸­ score æ˜¯å¯èƒ½éœ€è¦ mask çš„. probability_fn æ˜¯ tf.nn.softmax. æ‰€ä»¥å‘¢ï¼Ÿï¼Ÿï¼Ÿï¼Ÿä¸éœ€è¦ prev å•Šï¼Ÿ# ç„¶åå‘ç°ç¡®å®ä¸éœ€è¦å•Šã€‚ã€‚ã€‚ä¸€æ­¥æ­¥å¾€ä¸Šæ‰¾probability_fn=wrapped_probability_fnwrapped_probability_fn = lambda score, _: probability_fn(score) initial_alignments(batch_size, dtype) åˆå§‹åŒ–å¯¹é½ Args: batch_size: int32 scalar, the batch_size. dtype: The dtype. Returns: A dtype tensor shaped [batch_size, alignments_size] initial_state(batch_size, dtype)ï¼š Creates the initial state values for the AttentionWrapper class. batch_size: int32.","link":"/2018/09/01/tensorflow-Attention-API/"},{"title":"ç±»å’Œæ–¹æ³•","text":"Pythonçš„OOPæ¨¡å‹ä¸»è¦æ€æƒ³ï¼šåœ¨ä¸€å †å¯¹è±¡ä¸­æŸ¥æ‰¾å±æ€§ï¼Œå¹¶ä¸ºå‡½æ•°å®šä¸€ä¸ªç‰¹æ®Šçš„ç¬¬ä¸€ä¸ªå‚æ•°ã€‚ 1. ç±»ä»£ç ç¼–å†™åŸºç¡€1.1 ç±»å’Œå®ä¾‹åŒ–pythoné¢å‘å¯¹è±¡ä¸­æœ‰ä¸¤ç§å¯¹è±¡ï¼šç±»å¯¹è±¡å’Œå®ä¾‹å¯¹è±¡ã€‚ ç±»å¯¹è±¡æä¾›é»˜è®¤è¡Œä¸ºï¼Œå®ä¾‹å¯¹è±¡æ˜¯ç¨‹åºå¤„ç†çš„å®é™…å¯¹è±¡ï¼šå„è‡ªéƒ½æœ‰è‡ªå·±çš„å‘½åç©ºé—´ã€‚ åœ¨classè¯­å¥å†…ï¼Œä»»ä½•èµ‹å€¼è¯­å¥éƒ½ä¼šäº§ç”Ÿç±»å±æ€§ï¼š 123456789101112131415161718192021class ShareDate: spam = 42 # ä¸æ˜¯åœ¨__init__å‡½æ•°ä¸­ï¼Œæœ‰ç‚¹ç±»ä¼¼äºC++çš„é™æ€æˆå‘˜å˜é‡x = ShareDate()y = ShareDate()print(x.spam) # 42print(y.spam) # 42ShareDate.spam = 89print(x.spam) # 89print(y.spam) # 89 1.2 ç±»æ–¹æ³•ç±»æ–¹æ³•ï¼šä¸æ™®é€šçš„defå‡½æ•°ä¸åŒçš„æ˜¯ï¼Œç±»ä¸­çš„æ–¹æ³•ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯selfï¼Œå³å¼•ç”¨æ­£å¤„ç†çš„å®ä¾‹å¯¹è±¡ã€‚selfå‚æ•°åŒC++çš„thisæŒ‡é’ˆå¾ˆç›¸ä¼¼ã€‚ 1.3 ç±»ç»§æ‰¿ï¼Œé‡è½½é™¤äº†ç»§æ‰¿å’Œé‡è½½ï¼Œåœ¨å­ç±»ä¸­è¿˜å¯ä»¥é‡æ–°å®šåˆ¶æ„é€ å‡½æ•°ï¼Œè¿™ä¹Ÿæ˜¯å¾ˆå¸¸è§çš„ã€‚åœ¨1.8ä¸­ä¼šé€šè¿‡å®Œæ•´çš„ä¾‹å­æ¥è®²å®šåˆ¶æ„é€ å‡½æ•°ã€‚ 123456789101112131415161718192021222324252627class FirstClass: def setdata(self, value): self.data = value def display(self): print(self.data)class SecondClass(FirstClass): # ç±»ç»§æ‰¿ def display(self): # ç±»å‡½æ•°çš„é‡è½½ print('current value = &quot;%s&quot; '%self.data) z = SecondClass()z.setdata(42)z.display() # current value = &quot;42&quot; 1.4 ç±»æ˜¯æ¨¡å—çš„å±æ€§ç±»åç§°æ€»æ˜¯å­˜åœ¨ä¸æ¨¡å—æ–‡ä»¶ä¸­çš„ï¼Œç±»æ˜¯æ¨¡å—å¯¹è±¡çš„å±æ€§ã€‚ç±»å’Œæ¨¡å—éƒ½æ˜¯å‘½åç©ºé—´ï¼Œä½†ç±»å¯¹åº”äºè¯­å¥ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªæ–‡ä»¶ï¼‰ï¼Œè€Œä¸”æ”¯æŒå¤šä¸ªå®ä¾‹ã€ç»§æ‰¿ä»¥åŠè¿ç®—ç¬¦é‡è½½è¿™äº›OOPæ¦‚å¿µã€‚ éœ€ç†è§£_init_.pyæ–‡ä»¶ã€‚ 1.5 è¿ç®—ç¬¦é‡è½½ï¼Œç±»å¯ä»¥æˆªè·Pythonè¿ç®—ç¬¦å®é™…ä¸Šï¼Œâ€œè¿ç®—ç¬¦é‡è½½â€åªæ˜¯æ„å‘³ç€åœ¨ç±»æ–¹æ³•ä¸­æ‹¦æˆªå†…ç½®çš„æ“ä½œâ€¦â€¦å½“ç±»çš„å®ä¾‹å‡ºç°åœ¨å†…ç½®æ“ä½œä¸­ï¼ŒPythonè‡ªåŠ¨è°ƒç”¨ä½ çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä½ çš„æ–¹æ³•çš„è¿”å›å€¼å˜æˆäº†ç›¸åº”æ“ä½œçš„ç»“æœã€‚ä»¥ä¸‹æ˜¯å¯¹é‡è½½çš„å…³é”®æ¦‚å¿µçš„å¤ä¹ ï¼š è¿ç®—ç¬¦é‡è½½è®©ç±»æ‹¦æˆªå¸¸è§„çš„Pythonè¿ç®—ã€‚ ç±»å¯é‡è½½æ‰€æœ‰Pythonè¡¨è¾¾å¼è¿ç®—ç¬¦ ç±»å¯ä»¥é‡è½½æ‰“å°ã€å‡½æ•°è°ƒç”¨ã€å±æ€§ç‚¹å·è¿ç®—ç­‰å†…ç½®è¿ç®— é‡è½½ä½¿ç±»å®ä¾‹çš„è¡Œä¸ºåƒå†…ç½®ç±»å‹ã€‚ é‡è½½æ˜¯é€šè¿‡ç‰¹æ®Šåç§°çš„ç±»æ–¹æ³•æ¥å®ç°çš„ã€‚ ä¸¾ä¸ªæ —å­ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class ThirdClass(SecondClass): def __init__(self, value): self.data = value def __add__(self, other): return ThirdClass(self.data + other) def __str__(self): print(&quot;__str__ is called&quot;) return &quot;[ThirdClass: %s]&quot; % self.data def mul(self, other): self.data *= othera = ThirdClass(&quot;abc&quot;)a.display()print(a) # è‡ªåŠ¨è°ƒç”¨__str__b = a + &quot;xyz&quot; # è‡ªåŠ¨è°ƒç”¨__add__b.display()a.mul(3) # æ˜¾ç¤ºç»“æœä¸pythonå†…ç½®ç±»å‹çš„è¡Œä¸ºä¸åŒa.display()# è¿è¡Œç»“æœï¼šcurrent value = &quot;abc&quot;__str__ is called[ThirdClass: abc]current value = &quot;abcxyz&quot;current value = &quot;abcabcabc&quot; æ„æ€å°±æ˜¯ï¼Œå½“ä½ çš„ç±»å‡ºç°åœ¨å†…ç½®å‡½æ•°æ¯”å¦‚+ï¼Œprintè¿™æ ·çš„è¡¨è¾¾å¼ä¸­ï¼Œå°±ä¼šè¢«_add_,__str__è¿™æ ·çš„ç±»æ–¹æ³•æ‹¦æˆªã€‚ä½†å¦‚æœåœ¨ä½ çš„ç±»ä¸­ï¼Œæ²¡æœ‰å®šä¹‰è¿™æ ·çš„æ–¹æ³•ï¼Œå°±ä¼šå¾ˆæœ‰å¯èƒ½æŠ¥é”™äº†ï½é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆä¼šæ‹¦æˆªå‘¢ï¼Ÿè¿˜éœ€è¦çœ‹åº•å±‚ä»£ç ï½çŒœæƒ³åº”è¯¥æ˜¯å¯¹è‡ªå®šä¹‰çš„ç±»æ‰§è¡Œ+æ“ä½œæ—¶ï¼Œå°±ä¼šè‡ªåŠ¨ä»ç±»æ ‘ä¸­å¯»æ‰¾__add__æ–¹æ³•å§ï½ | æ–¹æ³• | é‡è½½ | è°ƒç”¨ | | â€”â€”â€”â€”â€”| â€”â€”â€”| â€”â€”â€”â€”â€”â€”â€”â€”â€“| | _init_ | æ„é€ å‡½æ•° | å¯¹è±¡å»ºç«‹ï¼šX = Classï¼ˆargsï¼‰| |_del_ |ææ„å‡½æ•°| Xå¯¹è±¡æ”¶å›| |_add_ |è¿ç®—ç¬¦ + | å¦‚æœæ²¡æœ‰__iadd__ï¼Œ X + Y, X += Y |_or_ |è¿ç®—ç¬¦ï¼ˆä½ORï¼‰| å¦‚æœæ²¡æœ‰_ior_ |_repr_, _str_| æ‰“å°ã€è½¬æ¢| print(X), repr(X), str(X) |_call_ |å‡½æ•°è°ƒç”¨| X(*args, **kargs) |_getattr_| ç‚¹å·è¿ç®—| X.undefined |_setattr_| å±æ€§èµ‹å€¼è¯­å¥| X.any = value |_delattr_| å±æ€§åˆ é™¤| del X.any |_getattribute_| å±æ€§è·å– |X.any |_getitem_| ç´¢å¼•è¿ç®—|X[key],X[i:j]ï¼Œæ²¡__iter__æ—¶çš„forå¾ªç¯å’Œå…¶ä»–è¿­ä»£å™¨ |_setitem_ |ç´¢å¼•èµ‹å€¼è¯­å¥| X[key] = value, X[i:j] = sequence |_delitem_|ç´¢å¼•å’Œåˆ†ç‰‡åˆ é™¤| del X[key], del X[i:j] |_len_| é•¿åº¦ |len(X), å¦‚æœæ²¡æœ‰__bool__ï¼Œ çœŸå€¼æµ‹è¯• |_bool_| å¸ƒå°”æµ‹è¯•| bool(X), çœŸæµ‹è¯•ï¼ˆåœ¨Python 2.6ä¸­å«åš__nonzero__ï¼‰ |_lt_, _gt_,_lt_,_ge_,_eq_, _ne|ç‰¹å®šæ¯”è¾ƒ| X&lt;Y, X&gt;Y, X&lt;=Y, X&gt;=Y, X == Y, X != Y(æˆ–è€…åœ¨Python 2.6ä¸­åªæœ‰_cmp) |_radd_| å³ä¾§åŠ æ³•| Other + X |_iadd| å®åœ°ï¼ˆå¢å¼ºçš„ï¼‰åŠ æ³•| X += Y(or else _add) |_iter_, _next_ |è¿­ä»£ç¯å¢ƒ| I = iter(X), next(I); for loops, in if no _contains_, all comprehensions, map(F, X), å…¶ä»–(__next__åœ¨Python2.6ä¸­æˆä¸ºnext) |_contains_ |æˆå‘˜å…³ç³»æµ‹è¯•| item in X(ä»»ä½•å¯è¿­ä»£çš„) |_index_| æ•´æ•°å€¼ |hex(X), bin(X), oct(X), O[X], O[X:]ï¼ˆæ›¿ä»£Python 2ä¸­çš„_oct_,__hex__ï¼‰ |_enter_, _exit_ |ç¯å¢ƒç®¡ç†å™¨| with obj as var: |_get_, _set_,_delete_|æè¿°ç¬¦å±æ€§| X.attr, X.attr = value, del X.attr |_new_ |åˆ›å»º |åœ¨__init__ä¹‹å‰åˆ›å»ºå¯¹è±¡ æ‰€æœ‰é‡è½½æ–¹æ³•çš„åç§°å‰åéƒ½æœ‰ä¸¤ä¸ªä¸‹åˆ’çº¿ï¼Œä»¥ä¾¿æŠŠåŒç±»ä¸­å®šä¹‰çš„å˜é‡ååŒºåˆ«å¼€æ¥ã€‚ç‰¹æ®Šæ–¹æ³•åç§°å’Œè¡¨è¾¾å¼æˆ–è¿ç®—çš„æ˜ å°„å…³ç³»ï¼Œæ˜¯ç”±Pythonè¯­è¨€é¢„å…ˆå®šä¹‰å¥½çš„ï¼ˆåœ¨æ ‡å‡†è¯­è¨€æ‰‹å†Œä¸­æœ‰è¯´æ˜ï¼‰ã€‚ä¾‹å¦‚åç§°ï¼Œ__add__æŒ‰ç…§Pythonè¯­è¨€çš„å®šä¹‰ï¼Œæ— è®º__add__æ–¹æ³•çš„ä»£ç å®é™…åœ¨åšäº›ä»€ä¹ˆï¼Œæ€»æ˜¯å¯¹åº”åˆ°äº†è¡¨è¾¾å¼ + ã€‚ å¦‚æœæ²¡æœ‰å®šä¹‰è¿ç®—ç¬¦é‡è½½æ–¹æ³•çš„è¯ï¼Œå®ƒå¯èƒ½ç»§æ‰¿è‡ªè¶…ç±»ï¼Œå°±åƒä»»ä½•å…¶ä»–çš„æ–¹æ³•ä¸€æ ·ã€‚è¿ç®—ç¬¦é‡è½½æ–¹æ³•ä¹Ÿéƒ½æ˜¯å¯é€‰çš„â€¦â€¦å¦‚æœæ²¡æœ‰ç¼–å†™æˆ–ç»§æ‰¿ä¸€ä¸ªæ–¹æ³•ï¼Œä½ çš„ç±»ç›´æ¥ä¸æ”¯æŒè¿™äº›è¿ç®—ï¼Œå¹¶ä¸”è¯•å›¾ä½¿ç”¨å®ƒä»¬ä¼šå¼•å‘ä¸€ä¸ªå¼‚å¸¸ã€‚ä¸€äº›å†…ç½®æ“ä½œï¼Œæ¯”å¦‚æ‰“å°ï¼Œæœ‰é»˜è®¤çš„é‡è½½æ–¹æ³•ï¼ˆç»§æ‰¿è‡ªPython 3.xä¸­éšå«çš„objectç±»ï¼‰ï¼Œä½†æ˜¯ï¼Œå¦‚æœæ²¡æœ‰ç»™å‡ºç›¸åº”çš„è¿ç®—ç¬¦é‡è½½æ–¹æ³•çš„è¯ï¼Œå¤§å¤šæ•°å†…ç½®å‡½æ•°ä¼šå¯¹ç±»å®ä¾‹å¤±è´¥ã€‚ ä¹Ÿå°±æ˜¯è™½ç„¶è‡ªå®šä¹‰çš„ç±»é‡Œé¢æ²¡æœ‰é‡è½½ç±»æ–¹æ³•__str__æˆ–æ˜¯_add_,ä½†Python3æ‰€æœ‰è‡ªå®šä¹‰çš„ç±»éƒ½æ˜¯ç»§æ‰¿äº†objectç±»çš„ï¼Œå«æœ‰é»˜è®¤çš„_add_,_str_,ä½†å¤§éƒ¨åˆ†ä¼šå¯¹å®ä¾‹å¤±è´¥ã€‚ è¿˜æ˜¯ä¸¾ä¸ªä¹‹å‰çš„é‚£ä¸ªæ —å­ï¼Œè¿™æ¬¡æˆ‘ä»¬ä¸å®šä¹‰_add_,_str_: 123456789101112131415161718192021class ThirdClass(SecondClass): def __init__(self, value): self.data = valueprint(dir(ThirdClass))print(&quot;***************&quot;)a = ThirdClass(&quot;abc&quot;)print(a)b = a + &quot;xyz&quot;b.display() è¿è¡Œç»“æœï¼š 123456789101112131415161718192021222324252627282930313233['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'display', 'setdata']***************&lt;__main__.ThirdClass object at 0x7f30d8472da0&gt;---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-40-8717227036a8&gt; in &lt;module&gt;() 7 a = ThirdClass(&quot;abc&quot;) 8 print(a)----&gt; 9 b = a + &quot;xyz&quot; 10 b.display()TypeError: unsupported operand type(s) for +: 'ThirdClass' and 'str' æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒThirdClassç±»æœ¬èº«ç»§æ‰¿äº†objectçš„å¾ˆå¤šå†…ç½®æ–¹æ³•ï¼ŒåŒ…æ‹¬__str__ï¼Œåªæ˜¯æ˜¾ç¤ºçš„ä¸æˆ‘ä»¬ä¹‹å‰è‡ªå®šä¹‰çš„__str__ä¸ä¸€æ ·è€Œå·²ã€‚è€Œ__add__å°±ä¼šå‡ºç°å¼‚å¸¸äº†ã€‚ 1.6 ç±»å’Œå­—å…¸çš„å…³ç³»pythonçš„ç±»æ¨¡å‹ç›¸å½“åŠ¨æ€ã€‚ç±»å’Œå®ä¾‹åªæ˜¯å‘½åç©ºé—´ï¼Œå±æ€§æ˜¯é€šè¿‡èµ‹å€¼è¯­å¥åŠ¨æ€å»ºç«‹çš„ã€‚ æ¨¡å—çš„å‘½åç©ºé—´å®é™…ä¸Šæ˜¯ä»¥å­—å…¸çš„å½¢å¼å‡ºç°çš„ï¼Œç±»å’Œå®ä¾‹å¯¹è±¡ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å¯ç”¨__dict__æ¥æ˜¾ç¤ºè¿™ä¸€ç‚¹ï¼Œå±æ€§ç‚¹å·è¿ç®—å…¶å®å°±æ˜¯å­—å…¸å†…çš„ç´¢å¼•è¿ç®—ï¼Œè€Œå±æ€§ç»§æ‰¿å…¶å®å°±æ˜¯æœç´¢é“¾æ¥çš„å­—å…¸ã€‚ 1.7 æµ‹è¯•è„šæœ¬æ–‡ä»¶ä»£ç 1234567if __name__ == &quot;__main__&quot;: # self-test pass åªæœ‰åœ¨å½“å‰è„šæœ¬æ–‡ä»¶ä¸‹è¿è¡Œæ—¶ï¼Œä¸Šè¿°ifè¯­å¥æ¡ä»¶æ‰ä¸ºçœŸã€‚å› æ­¤ï¼Œè¿™æ ·å°±å¯ä»¥åœ¨æ–‡ä»¶åº•éƒ¨è¿è¡Œæµ‹è¯•è¯­å¥ï¼Œè€Œä¸ä¼šåœ¨å¯¼å…¥æ–‡ä»¶çš„æ—¶å€™è¿è¡Œã€‚ 1.8 å®šåˆ¶æ„é€ å‡½æ•°1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Person(): def __init__(self, name, job=None, pay=0): self.name = name self.job = job self.pay = pay def LastName(self): return self.name.spilt()[-1] def giveRaise(self, persent): self.pay = int(self.pay * (1 + persent)) def __str__(self): return self.__class__.__name__ + ':{0} {1}'.format(self.name, self.pay)class Manager(Person): def __init__(self, name, pay): # å®šåˆ¶æ„é€ å‡½æ•° Person.__init__(self, name, 'mgr', pay) # å¿…é¡»æ‰‹åŠ¨è°ƒç”¨è¶…ç±» def giveRaise(self, persent, bonus=0.1): Person.giveRaise(self, persent + bonus)if __name__ == &quot;__main__&quot;: tom = Manager('Tom Jones', 5000) print(tom) print(tom.__class__.__name__) print(Manager.__bases__) print(tom.__dict__.keys()) print(getattr(tom, 'name') == tom.name) # è™½ç„¶ç±»å’Œå­—å…¸ç±»ä¼¼ï¼Œä½†æ˜¯ä¸èƒ½å†™tom['name']# è¿è¡Œç»“æœï¼šManager:Tom Jones 5000Manager(&lt;class '__main__.Person'&gt;,)dict_keys(['name', 'job', 'pay'])True åœ¨åˆå§‹åŒ–ä¸­self.name=nameè¿™é‡Œçœ‹èµ·æ¥æœ‰äº›å¤šä½™ï¼Œä½†å®é™…ä¸Šname, jobåœ¨__init__å‡½æ•°ä¸­åªæ˜¯æœ¬åœ°å˜é‡ä½†self.jobæ˜¯å®ä¾‹ä¸­çš„ä¸€ä¸ªå±æ€§ï¼Œè¿™ä¸¤ä¸ªæ˜¯ä¸åŒçš„å˜é‡ï¼Œåªæ˜¯æ°å¥½åå­—ä¸€æ ·__init__å‡½æ•°æ²¡ä»€ä¹ˆå¥‡å¦™ä¹‹å¤„ï¼Œåªæ˜¯åœ¨äº§ç”Ÿä¸€ä¸ªå®ä¾‹æ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ï¼Œå¹¶ä¸”æœ‰ç‰¹æ®Šçš„ç¬¬ä¸€ä¸ªå‚æ•° è¿™å…¶ä¸­ç”¨åˆ°äº†ç‰¹æ®Šçš„ç±»å±æ€§ï¼Œ _class_ _name_ _bases_ _dict_ 1.9 æŠŠå¯¹è±¡å­˜å‚¨åˆ°æ•°æ®åº“ä¸­ pickle ä»»æ„Pythonå¯¹è±¡å’Œå­—ç¬¦ä¸²ä¹‹é—´çš„åºåˆ—åŒ– dbm å®ç°ä¸€ä¸ªå¯é€šè¿‡é”®è®¿é—®çš„æ–‡ä»¶æŒ‰ç³»ç»Ÿï¼Œä»¥å­˜å‚¨å­—ç¬¦ä¸² shelve ä½¿ç”¨å¦ä¸¤ä¸ªæ¨¡å—æŠŠpythonå¯¹è±¡å­˜å‚¨åˆ°æ–‡ä»¶ä¸­ 123456789101112131415161718192021222324252627282930313233343536373839404142434445bob = Person('bob smith')sue = Person('Sue Jones', job='dev', pay=10000)tom = Manager('Tom Jones', pay=5000)import shelvedb = shelve.open('Persondb')for object in (bob, sue, tom): db[object.name] = objectdb.close()db = shelve.open('Persondb')print(db)print(len(db))print(list(db.keys()))print(db['bob smith'])#è¿è¡Œç»“æœï¼š&lt;shelve.DbfilenameShelf object at 0x7f915bdce588&gt;3['bob smith', 'Sue Jones', 'Tom Jones']Person:bob smith 0 1.10 æŠ½è±¡è¶…ç±»å¯¹äºæŠ½è±¡åŸºç±»ï¼Œéœ€è¦å­ç±»æ¥å¡«å……ã€‚å½“è¡Œä¸ºæ— æ³•é¢„æµ‹ï¼Œéå¾—ç­‰åˆ°æ›´ä¸ºå…·ä½“çš„å­ç±»ç¼–å†™æ—¶æ‰çŸ¥é“ï¼Œé€šå¸¸å¯ç”¨è¿™ç§æ–¹å¼æŠŠç±»é€šç”¨åŒ–ã€‚OOPè½¯ä»¶æ¡†æ¶ä¹Ÿä½¿ç”¨è¿™ç§æ–¹å¼ä½œä¸ºå®¢æˆ·ç«¯å®šä¹‰ã€å¯å®šåˆ¶çš„è¿ç®—çš„å®ç°æ–¹æ³•ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from abc import ABCMeta, abstractmethodclass Super(metaclass=ABCMeta): def delegate(self): self.action() @abstractmethod def action(self): passclass Sub1(Super): # æ²¡æœ‰å®šä¹‰actionå‡½æ•°ï¼Œå°±ä¼šæŠ¥é”™ passclass Sub2(Super): def action(self): print('spam')X2 = Sub2()X2.delegate()X1 = Sub1()X1.delegate()# è¿è¡Œç»“æœspamTypeError: Can't instantiate abstract class Sub1 with abstract methods action 1.11 Pythonå‘½åç©ºé—´ç›´æ¥çœ‹ä»£ç ï¼Œçœ‹æ‡‚äº†ä¹Ÿå°±äº†è§£äº†Pythonçš„å‘½åç©ºé—´äº†ï½ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# manyname.pyX = 11def f(): print(X)def g(): X = 22 print(X)class C: X = 33 def m(self): X = 44 self.X = 55print(X) # 11 module,æ¨¡å—å±æ€§f() # 11 globalg() # 22 local,å‡½æ•°å†…çš„æœ¬åœ°å˜é‡print(X) # 11 æ¨¡å—å±æ€§æ²¡å˜obj = C()print(obj.X) # 33 ç±»å±æ€§obj.m()print(obj.X) # 55 å®ä¾‹å±æ€§print(C.X) # 33 1.12 super()è¶…ç±»é¦–å…ˆè¦å¼„æ¸…æ¥šä¸ºä»€ä¹ˆè¦ä½¿ç”¨super()ã€‚æ˜¯å› ä¸ºæˆ‘ä»¬æƒ³è¦è°ƒç”¨çˆ¶ç±»ä¸­å·²ç»è¢«å­ç±»è¦†ç›–çš„æ–¹æ³•ã€‚æœ€å¸¸è§çš„æ¯”å¦‚ init()å‡½æ•°ï¼Œåœ¨å­ç±»ä¸­å¿…ç„¶ä¼šè¦†ç›–çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œè¿™æ ·çˆ¶ç±»ä¸­å¾ˆå¤šå±æ€§å°±æ²¡æœ‰äº†ã€‚ ä¸¾ä¸ªæ —å­ï¼š 12345678910111213141516171819202122232425class A: def __init__(self, batch_size, vocab_size): self._batch_size = batch_size self._vocab_size = vocab_size print('A.spam')class B(A): def __init__(self): print('B.spam')b = B()print(b._batch_size) è¿è¡Œç»“æœ: 1234567891011B.spamTraceback (most recent call last): File &quot;/home/panxie/Documents/NLPå®æˆ˜/text classification/06-memory-networks/test.py&quot;, line 25, in &lt;module&gt; print(b._batch_size)AttributeError: 'B' object has no attribute '_batch_size' ç»“æœæŠ¥é”™ï¼šå­ç±»ä¸­ä¸å­˜åœ¨_batch_sizeè¿™ä¸ªå±æ€§ã€‚æ‰€ä»¥å¿…é¡»åœ¨å­ç±»çš„ init()å‡½æ•°ä¸­è°ƒç”¨çˆ¶ç±»çš„ init()å‡½æ•°ã€‚ ä¹Ÿå°±æ˜¯è¿™æ ·ï¼š 123456789101112131415161718192021222324252627class A: def __init__(self, batch_size, vocab_size): self._batch_size = batch_size self._vocab_size = vocab_size print('A.spam')class B(A): def __init__(self): super(B, self).__init__(8, 1000) print('B.spam')b = B()print(b._batch_size) è¿è¡Œç»“æœï¼š 1234567A.spamB.spam8 é‚£æˆ‘ä»¬å¯ä¸å¯ä»¥ä¸ç”¨super()å‘¢ï¼Œç›´æ¥è°ƒç”¨Aç±»çš„æ–¹æ³•ä¹Ÿè¡Œå‘€ï¼Œæ¯”å¦‚ï¼š 123456789101112131415161718192021222324252627class A: def __init__(self, batch_size, vocab_size): self._batch_size = batch_size self._vocab_size = vocab_size print('A.spam')class B(A): def __init__(self): A.__init__(self, 8, 10) print('B.spam')b = B()print(b._batch_size) è¿è¡Œç»“æœè·Ÿä¸Šé¢ä¸€æ ·ï¼Œè²Œä¼¼ä¹Ÿæ²¡æ¯›ç—…ã€‚ä½†æ˜¯åœ¨å¤šç»§æ‰¿çš„æ—¶å€™ï¼Œçˆ¶ç±»ä¼šé‡å¤è°ƒç”¨ä»€ä¹ˆçš„ï¼Œæ¯”å¦‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Base: def __init__(self): print('Base.__init__')class A(Base): def __init__(self): Base.__init__(self) print('A.__init__')class B(Base): def __init__(self): Base.__init__(self) print('B.__init__')class C(A,B): def __init__(self): A.__init__(self) B.__init__(self) print('C.__init__')c = C()print(C.__mro__) è¿è¡Œç»“æœï¼š 12345678910111213Base.__init__A.__init__Base.__init__B.__init__C.__init__(&lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.B'&gt;, &lt;class '__main__.Base'&gt;, &lt;class 'object'&gt;) å¯ä»¥å‘ç°åŸºç±»è¢«é‡å¤è°ƒç”¨äº†ä¸¤æ¬¡ã€‚è¿™æ˜¾ç„¶æ˜¯ä¸å¥½çš„ã€‚ä½†æ˜¯æ¢æˆ super() æ¯ä¸ªç±»çš„ init()æ–¹æ³•ä¿è¯åªä¼šè°ƒç”¨ä¸€æ¬¡ã€‚å…·ä½“åŸç†å¯ä»¥å‚è€ƒ python-cookbook","link":"/2018/03/25/python-%E7%B1%BB%E5%92%8C%E6%96%B9%E6%B3%95/"},{"title":"ä»0å¼€å§‹GAN-1-from-GAN-to-WGAN","text":"From GAN to WGANReference: From GAN to WGAN ä»¤äººæ‹æ¡ˆå«ç»çš„Wasserstein GAN å¬æå®æ¯…è€å¸ˆè®²æ®µå­ä¹‹ GAN è¿™æ˜¯ä¸€ç¯‡ copy + translate + understand çš„å­¦ä¹ ç¬”è®°. NLP é€‰æ‰‹æ€»æ˜¯ä¼šå¬è¯´ GAN ä¸é€‚åˆè‡ªç„¶è¯­è¨€å¤„ç†è¿™ç±»ä»»åŠ¡ï¼Œä½†å­¦äº†æ‰å‘ç°ï¼Œemmmmï¼ŒçœŸé¦™ã€‚ã€‚ ä¸ç®¡æ˜¯å¦é€‚åˆï¼Œä½†çœŸçš„å¥½ç©ï¼ çºµæ‰€å‘¨çŸ¥ï¼ŒGANéå¸¸éš¾è®­ç»ƒï¼Œåœ¨è®­ç»ƒæ—¶æ€»æ˜¯ä¼šé¢ä¸´è®­ç»ƒä¸ç¨³å®šï¼Œä»¥åŠéš¾ä»¥æ”¶æ•›çš„æƒ…å†µã€‚è¿™é‡Œï¼Œä½œè€…å°è¯•é€šè¿‡é˜è¿°GANèƒŒåçš„æ•°å­¦åŸç†ï¼Œæ¥è§£é‡Šä¸ºä»€ä¹ˆGANä¸å¥½è®­ç»ƒï¼Œå¹¶ä¸”ä»‹ç»äº†GANçš„å¦ä¸€ä¸ªç‰ˆæœ¬æ¥æ›´å¥½çš„è§£å†³è¿™äº›è®­ç»ƒéš¾é¢˜ã€‚ Kullbackâ€“Leibler and Jensenâ€“Shannon Divergenceåœ¨å­¦ä¹ GANä¹‹å‰ï¼Œå…ˆå›é¡¾ä¸€ä¸‹å¦‚ä½•è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒç›¸ä¼¼åº¦çš„æ ‡å‡†ã€‚ KL (Kullbackâ€“Leibler) divergenceå¦‚ä½•é€šä¿—çš„è§£é‡Šäº¤å‰ç†µä¸ç›¸å¯¹ç†µ? ç†µ: ä¿¡æ¯é‡å¯è¡¨ç¤ºä¸º $log\\dfrac{1}{p}$ï¼Œå…¶å¯ç†è§£ä¸ºæ¦‚ç‡ä¸ºpçš„éšæœºäº‹ä»¶æ‰€åŒ…å«çš„ä¿¡æ¯é‡ã€‚æ¯”å¦‚â€œå¤ªé˜³æ˜å¤©æ—©ä¸Šåœ¨ä¸œè¾¹å‡èµ·â€ï¼Œè¿™ä¸ªæ¦‚ç‡p=1ï¼Œé‚£ä¹ˆå…¶æ‰€åŒ…å«çš„ä¿¡æ¯å°±ä¸º0äº†ï¼Œæ„æ€å°±æ˜¯è¿™ä¸æ˜¯å¥å±è¯å˜›ã€‚ã€‚æ‰€ä»¥ä¿¡æ¯é‡ä¸æ¦‚ç‡pæˆåæ¯”ã€‚è‡³äºä¸ºä»€ä¹ˆå°±æ˜¯ $log\\dfrac{1}{p}$ è¿™ç§å½¢å¼ï¼Œä¸ºå•¥ä¸æ˜¯ $1/p$ï¼Œè¿™éœ€è¦å»é—®é¦™å†œäº†ã€‚ã€‚ è€Œç†µåˆ™æ˜¯ ä¿¡æ¯é‡çš„æœŸæœ›ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸º éšæœºæ€§çš„åº¦é‡ã€‚éšæœºæ€§è¶Šå¤§ï¼Œç†µè¶Šå¤§ã€‚ äº¤å‰ç†µ ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒpå’Œqï¼Œpä¸ºçœŸå®åˆ†å¸ƒï¼Œqä¸ºéçœŸå®åˆ†å¸ƒã€‚æŒ‰ç…§çœŸå®åˆ†å¸ƒæ¥è¡¡é‡è¯†åˆ«ä¸€ä¸ªæ ·æœ¬æˆ–è€…æ˜¯åˆ¤æ–­éšæœºäº‹ä»¶çš„å‡†ç¡®æ€§çš„åº¦é‡ï¼Œå°±æ˜¯ç†µï¼Œä¹Ÿå°±æ˜¯ä¿¡æ¯é‡çš„æœŸæœ› $H(p)=\\sum_ip(i) * log\\dfrac{1}{p(i)}$,ä½†æ˜¯äº‹å®æ˜¯ï¼Œæˆ‘ä»¬æ— æ³•å¾—çŸ¥è¿™ä¸ªçœŸå®çš„åˆ†å¸ƒï¼Œåªèƒ½é€šè¿‡ç»Ÿè®¡æ¥é¢„æµ‹è¿™ä¸ªåˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯ç”¨éçœŸå®åˆ†å¸ƒqå»è¡¡é‡è¿™ä¸ªç†µï¼Œ$H(p,q)=\\sum_ip(i) * log\\dfrac{1}{q(i)}$, æ³¨æ„è¿™é‡Œçš„æ¦‚ç‡æ˜¯çœŸå®åˆ†å¸ƒ p(i). H(p,q)å°±æ˜¯æˆ‘ä»¬çš„â€œäº¤å‰ç†µâ€ã€‚ å½“ç”¨æ¥é¢„æµ‹çš„éçœŸå®åˆ†å¸ƒqè¶Šæ¥è¿‘çœŸå®åˆ†å¸ƒï¼Œå…¶éšæœºæ€§è¶Šå°ï¼Œå‡†ç¡®ç‡ä¹Ÿå°±è¶Šé«˜ã€‚ ç›¸å¯¹ç†µ/KLæ•£åº¦ æ ¹æ®Gibbsâ€™ inequalityä¸Šè¿°ä¾‹å­ä¸­çš„ $H(p,q) &gt;= H(p)$ æ’æˆç«‹ã€‚å½“ä¸”ä»…å½“q=pæ—¶ï¼Œè¿™ä¸ªç­‰å·æ‰æˆç«‹ã€‚é‚£ä¹ˆç†µH(p,q)ç›¸æ¯”ç†µH(q)å¤šå‡ºæ¥çš„éƒ¨åˆ†å°±æ˜¯ç›¸å¯¹ç†µ $D(p||q)=H(p,q)-H(p)=\\sum_ip(i)* log\\dfrac{p(i)}{q(i)}$ï¼Œä¹Ÿç§°ä¸ºKLæ•£åº¦(Kullbackâ€“Leibler divergenceï¼ŒKLD). ä»æœºå™¨å­¦ä¹ çš„è§’åº¦å»æ€è€ƒï¼Œæˆ‘ä»¬é¢„æµ‹å¾—åˆ°çš„éçœŸå®åˆ†å¸ƒå°±æ˜¯qï¼Œå½“æ¨¡å‹è¶Šå¥½æ—¶ï¼Œqä¸pè¶Šæ¥è¿‘ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹çš„å‡†ç¡®åº¦è¶Šé«˜ï¼Œéšæœºæ€§è¶Šå°ï¼Œæ‰€ä»¥äº¤å‰ç†µ/ç›¸å¯¹ç†µä¹Ÿå°±è¶Šå°ã€‚åè¿‡æ¥ï¼Œå°±å¯ä»¥é€šè¿‡äº¤å‰ç†µ/ç›¸å¯¹ç†µæ¥è®­ç»ƒæˆ‘ä»¬æ‰€éœ€çš„æ¨¡å‹äº†ï½ æ‰€ä»¥ï¼š $$D_{KL}(p||q)=H(p,q)-H(p)=\\sum_ip(i)* log\\dfrac{p(i)}{q(i)}=\\int_x{p(x)}log\\dfrac{p(x)}{q(x)}dx$$ ä½†æ˜¯ï¼Œè¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼Œpå’Œqå¹¶ä¸æ˜¯å®Œå…¨å¯¹ç§°çš„ã€‚æ˜¾ç„¶å½“p(x)ä¸º0ï¼Œq(x)ä¸ºéé›¶å€¼æ—¶ï¼Œq(x)çš„å½±å“å°±ä¸å­˜åœ¨äº†ã€‚åè¿‡æ¥å‘¢ï¼Œqä¸å¯èƒ½ä¸ºé›¶ã€‚æ‰€ä»¥å½“ä¸¤ä¸ªæ¦‚ç‡å®Œå…¨ç›¸ç­‰æ—¶ï¼Œç”¨KLæ•£åº¦æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡çš„ç›¸ä¼¼åº¦å°±ä¼šå­˜åœ¨é—®é¢˜äº†ã€‚ Jensenâ€“Shannon DivergenceJSæ•£åº¦çš„èŒƒå›´æ˜¯[0,1],å¹¶ä¸”æ˜¯å®Œå…¨å¯¹ç§°çš„ã€‚ $$D_{JS}(p | q) = \\frac{1}{2} D_{KL}(p | \\frac{p + q}{2}) + \\frac{1}{2} D_{KL}(q | \\frac{p + q}{2})$$ pæ˜¯å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1 çš„æ­£æ€åˆ†å¸ƒï¼Œqæ˜¯å‡å€¼ä¸º 1ï¼Œæ–¹å·®ä¸º 1 çš„æ­£æ€åˆ†å¸ƒã€‚ä¸¤è€…çš„å‡å€¼çš„åˆ†å¸ƒæ˜¯ m=(p+q)/2.å¯ä»¥çœ‹åˆ° $D_{Kl}$ æ˜¯éå¯¹ç§°çš„ï¼Œè€Œ $D_{JS}$ æ˜¯å¯¹ç§°çš„ã€‚ [How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? ](https://arxiv.org/pdf/1511.05101.pdf)è®¤ä¸º GAN èƒ½æˆåŠŸçš„å¾ˆå¤§ä¸€éƒ¨åˆ†åŸå› æ˜¯ç”¨JSæ•£åº¦ä»£æ›¿äº†ä¼ ç»Ÿçš„åŸºäºæå¤§ä¼¼ç„¶ä¼°è®¡çš„KLæ•£åº¦ã€‚ Generative Adversarial Network (GAN)GAN åŒ…å«ä¸¤ä¸ªæ¨¡å‹ï¼š Discrimator D: åˆ¤åˆ«å™¨ D ç”¨æ¥ä¼°è®¡æ¥è‡ª $p_r$ æˆ– $p_g$ çš„æ ·æœ¬æ˜¯çœŸå®æ ·æœ¬çš„æ¦‚ç‡ Generator G: ç»™å®šéšæœºè¾“å…¥å˜é‡ zï¼ˆéšæœº z å¸¦æ¥äº†å¤šæ ·æ€§, $z\\sim p_z$ï¼‰ï¼Œè¾“å‡ºå¾—åˆ°åˆæˆçš„æ ·æœ¬ã€‚G çš„è®­ç»ƒæ˜¯é€šè¿‡æ•æ‰çœŸå®æ ·æœ¬çš„åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆå°½å¯èƒ½çœŸå®çš„æ ·æœ¬ ($G(z)=x\\sim p_g$)ï¼Œæ¢å¥è¯è¯´ï¼Œå°±æ˜¯æ¬ºéª—åˆ¤åˆ«å™¨ D ä½¿å¾—ç”Ÿæˆçš„æ ·æœ¬è·å¾—è¾ƒé«˜çš„æ¦‚ç‡ã€‚ $p_z$ noiseï¼Œå¯ä»¥æ˜¯æ­£æ€åˆ†å¸ƒï¼Œä¹Ÿå¯ä»¥æ˜¯å‡åŒ€åˆ†å¸ƒ $p_g$ é€šè¿‡ sample ç”Ÿæˆå™¨ç”Ÿæˆçš„æ ·æœ¬å¾—åˆ°çš„åˆ†å¸ƒ $p_r$ é€šè¿‡ sanple çœŸå®æ ·æœ¬çš„ database å¾—åˆ°çš„åˆ†å¸ƒ æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹æ˜¯è¿­ä»£è¿›è¡Œçš„ï¼š å›ºå®šç”Ÿæˆå™¨çš„å‚æ•°ï¼Œè®­ç»ƒåˆ¤åˆ«å™¨ å›ºå®šåˆ¤åˆ«å™¨å‚æ•°ï¼Œè®­ç»ƒä¼˜åŒ–å™¨ iterationâ€¦ ä½•æ—¶åœæ­¢ï¼Œä»¥åŠå¦‚ä½•åˆ¤æ–­ä½•æ—¶åœæ­¢ï¼Œè¿™ä¹Ÿæ˜¯ GAN éœ€è¦è§£å†³çš„é—®é¢˜ã€‚ Generatorå®é™…ä¸Šï¼Œå¯ä»¥æŠŠç¥ç»ç½‘ç»œ G çœ‹ä½œæ˜¯ç”¨æ¥å®šä¹‰ä¸€ä¸ªåˆ†å¸ƒï¼Œ$p_g$, ä½¿å¾—è¿™ä¸ªåˆ†å¸ƒå°½å¯èƒ½çš„æ¥è¿‘çœŸå®æ ·æœ¬çš„å›¾åƒåœ¨é«˜ç»´ç©ºé—´ä¸­çš„åˆ†å¸ƒ $p_r$. æ‰€ä»¥å¯¹äºç”Ÿæˆå™¨çš„ç›®æ ‡å‡½æ•°æ˜¯ $G^* =\\argmax_{G}Div(P_g,p_r)$ ä½†æ˜¯é—®é¢˜åœ¨äºï¼Œå¦‚ä½•å»è¯„åˆ¤ä¸¤ä¸ª distributin çš„æ¥è¿‘ç¨‹åº¦å‘¢ï¼Œä¹Ÿå°±æ˜¯ $Div(p_g,p_{data})$ æ€ä¹ˆè®¡ç®—ï¼Ÿ DiscriminatorGAN ç‰›é€¼çš„åœ°æ–¹å°±æ˜¯ç”¨å¦ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥åˆ¤æ–­è¿™ä¸¤ä¸ª distribution. æ‰€ä»¥å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜äº†ã€‚ å½“ä¸¤ä¸ªåˆ†å¸ƒå¾ˆæ¥è¿‘çš„æ—¶å€™ï¼Œåˆ¤åˆ«å™¨å°±å¾ˆéš¾å»åŒºåˆ†æ¥è‡ªäº $p_g$ å’Œ $p_r$ çš„æ ·æœ¬ã€‚ æ‰€ä»¥å¯¹äºåˆ¤åˆ«å™¨ï¼Œå…¶ç›®æ ‡æ˜¯å°½å¯èƒ½çš„å»åŒºåˆ†å‡º $p_g$ å’Œ $p_r$ï¼Œå½“è®¡ç®—å‡ºçš„ divergence è¶Šå¤§æ—¶ï¼ŒD è¶Šå¥½ $D^* =\\argmax_{D}Div(D,G)$. æ‰€ä»¥ï¼ŒG å’Œ D ä¸¤ä¸ªæ¨¡å‹åœ¨è®­ç»ƒä¸­æ˜¯ç›¸äº’åšå¼ˆçš„è¿‡ç¨‹ã€‚G å°½å¯èƒ½çš„å»æ¬ºéª— Dï¼Œè€Œ D åˆ™å°½å¯èƒ½çš„ä¸è¢«æ¬ºéª—ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„zero-sumæ¸¸æˆã€‚ loss functionä»æå¤§ä¼¼ç„¶ä¼°è®¡çš„è§’åº¦æ¥åˆ†ææ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ŒäºŒåˆ†ç±»åˆ¤åˆ«å™¨ D çš„è¾“å…¥æ ·æœ¬é›† ${(x_1, y_1),(x_2,y_2),â€¦,(x_N, y_N)}$ çš„æ¦‚ç‡æœ€å¤§ï¼Œè€Œè¾“å…¥åˆ°åˆ¤åˆ«å™¨ D çš„æ ·æœ¬å¯èƒ½æ¥è‡ª real data, $x\\sim p_r(x)$ï¼Œä¹Ÿå¯èƒ½æ¥è‡ªç”Ÿæˆå™¨ G, $x\\sim p_g(x)$. å…¶ä¸­å¯¹åº”çš„ label: $$ y= \\begin{cases} 1, &amp; \\text {$x\\sim p_r(x)$} \\ 0, &amp; \\text{$x\\sim p_g(x)$} \\end{cases} $$ ä¼¼ç„¶å‡½æ•°ï¼ˆæ ·æœ¬é›†çš„æ¦‚ç‡æœ€å¤§ï¼‰: $$L(\\theta)=\\prod_iD(y_i=1|x_i)^{y_i}(1-D(y_i=1|x_i))^{(1-y_i)}$$ å¯¹äº $x\\sim p_r$, $y_i=1$,æ‰€ä»¥ $$logL=\\sum_{x\\sim p_r} logD(x)$$ å¯¹äº $x\\sim p_g$, $y_i=0$, å¯ä»¥å¾—åˆ°ï¼š $$logL=\\sum_{x\\sim p_g}log(1-D(x))$$ æ‰€ä»¥å¯¹äºåˆ¤åˆ«å™¨D, åœ¨ç”Ÿæˆå™¨Gå›ºå®šå‚æ•°æ—¶æœ€ä¼˜çš„åˆ¤åˆ«å™¨ D å°±æ˜¯æœ€å¤§åŒ–ä¸‹é¢è¿™ä¸ªç›®æ ‡å‡½æ•°ï¼š $$E_{x\\sim p_r(x)}[logD(x)]+E_{x\\sim p_g}[log(1-D(x)]\\qquad\\text{(1)}$$ äº‹å®ä¸Šï¼Œæˆ‘ä»¬å‘ç°ï¼Œè¿™ä¸ªç›®æ ‡å‡½æ•°è·Ÿ logistic regression æ˜¯ä¸€æ ·çš„ã€‚ã€‚ã€‚ ä»ç†µçš„è§’åº¦æ¥åˆ†ææˆ‘ä»¬é€šè¿‡æœ€å¤§åŒ– $E_{x\\sim p_r(x)}[logD(x)]$ æ¥ä¿è¯åˆ¤åˆ«å™¨ D åœ¨ real data $p_r$ä¸Šçš„å‡†ç¡®ç‡ã€‚ä¸æ­¤åŒæ—¶ï¼ŒG ç”Ÿæˆå¾—åˆ°çš„ fake æ ·æœ¬ï¼ŒG(z), $z\\sim p_z(z)$ï¼Œåˆ¤åˆ«å™¨DæœŸæœ›å¯¹äº fake æ ·æœ¬çš„æ¦‚ç‡ D(G(z)) è¶Šæ¥è¿‘äº 0 è¶Šå¥½ï¼Œä¹Ÿå°±æ˜¯æœ€å¤§åŒ– $E_{z\\sim p_z(z)}[log(1-D(G(z)))]$. å¯¹äºç”Ÿæˆå™¨ï¼Œå…¶ç›®çš„å°±æ˜¯è®©åˆ¤åˆ«å™¨Dåœ¨ fake æ ·æœ¬ä¸Šå¾—åˆ°çš„æ¦‚ç‡æ›´å¤§ï¼ŒGoodfellowä¸€å¼€å§‹æå‡ºæ¥ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œåæ¥åˆæå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼Œåˆ†åˆ«æ˜¯ $$E_{z\\sim p_z}[log(1-D(G(z))]=E_{x\\sim p_g}[log(1-D(x)]\\qquad\\text{(2)}$$ $$E_{z\\sim p_z}[-logD(G(z)]=E_{x\\sim p_g}[-logD(x)]\\qquad\\text{(3)}$$ è¿™ä¸ªç›´è§‚ä¸Šä¹Ÿå¾ˆå¥½ç†è§£~å›ºå®šäº†åˆ¤åˆ«å™¨ Gï¼Œç„¶åè®© $E_{x\\sim p_g}[logD(x)]$ å°½å¯èƒ½å¤§ï¼Œä¹Ÿå°±æ˜¯ $E_{x\\sim p_g}[log(1-D(x)]$ æˆ–è€… $E_{x\\sim p_g}[-logD(x)]$ å°½å¯èƒ½å°ã€‚ ç„¶åæŠŠä¸¤è€…ï¼ˆ1ï¼‰å’Œ ï¼ˆ2ï¼‰åˆå¹¶èµ·æ¥ï¼ˆå®ƒä»¬æœ‰å…±åŒçš„ç¬¬äºŒé¡¹ï¼‰ï¼ŒDå’ŒGæ­£åœ¨è¿›è¡Œçš„å°±æ˜¯ä¸€ä¸ª minimax gameï¼Œè€Œæˆ‘ä»¬æ‰€éœ€ä¼˜åŒ–çš„loss functionå°±æ˜¯ï¼š $$% &lt;![CDATA[ \\begin{aligned} \\min_G \\max_D L(D, G) &amp; = \\mathbb{E}{x \\sim p{r}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))] \\ &amp; = \\mathbb{E}{x \\sim p{r}(x)} [\\log D(x)] + \\mathbb{E}_{x \\sim p_g(x)} [\\log(1 - D(x)] \\quad (1.5) \\end{aligned} %]]&gt;$$ å¯¹äºç”Ÿæˆå™¨ï¼Œéœ€è¦æœ€å°åŒ–è¿™ä¸ªç›®æ ‡å‡½æ•°ã€‚å¯¹äºåˆ¤åˆ«å™¨ï¼Œéœ€è¦æœ€å¤§åŒ–è¿™ä¸ªå‡½æ•°ã€‚ å¦‚ä½•æ±‚å…³äºåˆ¤åˆ«å™¨ D çš„æœ€ä¼˜è§£å®šä¹‰å¥½äº† loss functionï¼Œæ¥ä¸‹æ¥æ¨å¯¼å¯¹äº D çš„æœ€ä¼˜è§£. ä¸Šå¼å¯ä»¥å†™æˆç§¯åˆ†å‡½æ•°ï¼š $$L(G,D)=\\int_x(p_r(x)log(D(x))+p_g(x)log(1-D(x)))dx\\quad (4)$$ å¯¹äºåˆ¤åˆ«å™¨ Dï¼Œæˆ‘ä»¬è¦æ±‚æœ€å¤§åŒ–ä¸Šè¿°ç›®æ ‡å‡½æ•°ã€‚å‡è®¾ D(x) å¯ä»¥æ¨¡æ‹Ÿä»»ä½•å‡½æ•°ï¼ˆäº‹å®ä¸Š neural network ä¹Ÿæ˜¯å¯ä»¥çš„ï¼‰ï¼Œé‚£ä¹ˆæœ€å¤§åŒ–ä¸Šè¿°å‡½æ•°ç­‰åŒäºæœ€å¤§åŒ–ç§¯åˆ†å†…çš„å‡½æ•°ã€‚ ä¹Ÿå°±æ˜¯ given $\\forall$ x ï¼Œæ±‚è§£å…¶æœ€ä¼˜çš„åˆ¤åˆ«å™¨ D*. $$p_r(x)log(D(x))+p_g(x)log(1-D(x))$$ ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œå‡è®¾ $$\\tilde x=D(x), A=p_r(x), B=p_g(x)$$ å¯¹ç§¯åˆ†å†…éƒ¨æ±‚å¯¼ï¼ˆè¿™é‡Œå¯ä»¥å¿½ç•¥ç§¯åˆ†ï¼Œå› ä¸ºxæ˜¯é‡‡æ ·ä»»ä½•å¯èƒ½çš„å€¼ï¼‰ï¼š $$% &lt;![CDATA[ \\begin{aligned} f(\\tilde{x}) &amp; = A log\\tilde{x} + B log(1-\\tilde{x}) \\ \\frac{d f(\\tilde{x})}{d \\tilde{x}} &amp; = A \\frac{1}{ln10} \\frac{1}{\\tilde{x}} - B \\frac{1}{ln10} \\frac{1}{1 - \\tilde{x}} \\ &amp; = \\frac{1}{ln10} (\\frac{A}{\\tilde{x}} - \\frac{B}{1-\\tilde{x}}) \\ &amp; = \\frac{1}{ln10} \\frac{A - (A + B)\\tilde{x}}{\\tilde{x} (1 - \\tilde{x})} \\ \\end{aligned} %]]&gt;$$ ç„¶åï¼Œä»¤ $\\dfrac{df(\\tilde x)}{d\\tilde x}=0$,å¯ä»¥å¾—åˆ°D(x)çš„æœ€ä¼˜è§£ï¼š $D^* (x) = \\tilde{x}^* = \\frac{A}{A + B} = \\frac{p_{r}(x)}{p_{r}(x) + p_g(x)} \\in [0, 1]\\qquad\\text{(5)}$ è¿™ä¸ªç»“æœä»ç›´è§‚ä¸Šå¾ˆå®¹æ˜“ç†è§£ï¼Œå°±æ˜¯çœ‹ä¸€ä¸ªæ ·æœ¬xæ¥è‡ªçœŸå®åˆ†å¸ƒå’Œç”Ÿæˆåˆ†å¸ƒçš„å¯èƒ½æ€§çš„ç›¸å¯¹æ¯”ä¾‹ã€‚å¦‚æœ $P_r(x) = 0$ ä¸” $P_g(x) \\neq 0$ï¼Œæœ€ä¼˜åˆ¤åˆ«å™¨å°±åº”è¯¥éå¸¸è‡ªä¿¡åœ°ç»™å‡ºæ¦‚ç‡0ï¼›å¦‚æœ $P_r(x) = P_g(x)$ï¼Œè¯´æ˜è¯¥æ ·æœ¬æ˜¯çœŸæ˜¯å‡çš„å¯èƒ½æ€§åˆšå¥½ä¸€åŠä¸€åŠï¼Œæ­¤æ—¶æœ€ä¼˜åˆ¤åˆ«å™¨ä¹Ÿåº”è¯¥ç»™å‡ºæ¦‚ç‡0.5ã€‚ å¦‚ä½•å¾—åˆ°ç”Ÿæˆå™¨ G çš„æœ€ä¼˜è§£ï¼Œä¹Ÿå°±æ˜¯å…¨å±€æœ€ä¼˜è§£è®°ä½æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®­ç»ƒå¾—åˆ°ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä½¿å¾—å…¶ç”Ÿæˆçš„ $P_g$ åˆ†å¸ƒèƒ½å°½å¯èƒ½çš„æ¥è¿‘äº $P_r$. æ‰€ä»¥å½“åˆ¤åˆ«å™¨æœ€ä¼˜æ—¶ï¼Œæœ€å°åŒ–ç›®æ ‡å‡½æ•°å°±èƒ½å¾—åˆ° G*ï¼Œå°† (4) å¸¦å…¥ (5) å¼å¯ä»¥å¾—åˆ°ï¼š $$\\begin{aligned} L(G, D^* ) &amp;= \\int_x \\bigg( p_{r}(x) \\log(D^* (x)) + p_g (x) \\log(1 - D^* (x)) \\bigg) dx \\ &amp;= \\int_x (p_r(x)log\\dfrac{p_r}{p_r+p_g} + p_g(x)log\\dfrac{p_g}{p_r+p_g})dx \\ &amp;= \\int_x(p_xlog\\dfrac{\\dfrac{1}{2}p_r}{\\dfrac{1}{2}(p_r+p_g)} + p_glog\\dfrac{\\dfrac{1}{2}p_g}{\\dfrac{1}{2}(p_r+p_g)})dx\\quad\\text{ä¸Šä¸‹åŒæ—¶ä¹˜ä»¥$\\dfrac{1}{2}$}\\ &amp;= -2log2 + \\int_xp_r(x)log\\dfrac{p_r(x)}{\\dfrac{1}{2}(p_r(x)+p_g)}dx + \\int_xp_g(x)log\\dfrac{p_g(x)}{\\dfrac{1}{2}(p_r(x)+p_g)}dx \\quad\\text{(6)}\\ &amp;= -2log2 + D_{KL}(p_r||\\dfrac{p_r+p_g}{2}) + D_{KL}(p_g||\\dfrac{p_r+p_g}{2})\\quad\\text{å¸¦å…¥ KL æ•£åº¦å…¬å¼} \\ &amp;= -2log2 + D_{JS}(p_{r} | p_g)\\quad\\text{å¸¦å…¥ JS æ•£åº¦å…¬å¼} \\end{aligned}$$ æˆ‘ä»¬çªç„¶å‘ç°ï¼Œè¯¶ï¼Œå§æ§½ï¼Œå‰å®³äº†ã€‚given D* çš„æ¡ä»¶ä¸‹ï¼Œå½“ç”Ÿæˆå™¨ G æœ€ä¼˜æ—¶ï¼Œé€šè¿‡æ¨å¯¼å‘ç°ï¼Œæœ€å°åŒ–ç›®æ ‡å‡½æ•°ç­‰åŒäºæœ€å°åŒ– $p_r$ å’Œ $p_g$ çš„ JS æ•£åº¦ã€‚æ‰€ä»¥å•Šï¼Œé€šè¿‡ç†è®ºè¯æ˜ï¼Œè®©ä¸¤ä¸ªåˆ†å¸ƒæ›´æ¥è¿‘çš„è¯ï¼Œä½¿ç”¨ JS æ•£åº¦æ˜æ˜¾è¦æ¯”æˆ‘ä»¬ä¼ ç»Ÿä¸Šä½¿ç”¨çš„ KL æ•£åº¦è¦åˆç†å‘€~ æ‰€ä»¥å¦‚ä½•åˆ¤åˆ«ä¸¤ä¸ªåˆ†å¸ƒçš„ Divergence, é€šè¿‡æ¨å¯¼å‘Šè¯‰æˆ‘ä»¬ï¼ŒJS æ•£åº¦æ›´å¥½~ æ•´ä¸ªç®—æ³•æµç¨‹ï¼š è¿™ä¸ªä¸ºä»€ä¹ˆ D è®­ç»ƒæ—¶æ˜¯å¤šæ¬¡, è€Œ G è®­ç»ƒæ—¶åªéœ€è¦ä¸€æ¬¡å‘¢ï¼Ÿ å›ºå®šåˆ¤åˆ«å™¨ä¸º $D^* $ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™è®­ç»ƒ $G_0 \\rightarrow G_1$, è¿™é‡Œ $G_0$ å’Œ $G_1$ ä¸èƒ½å·®è·å¤ªå¤§. å› ä¸ºå¦‚æœ G å˜åŒ–å¤ªå¤§ï¼Œé‚£ä¹ˆå¯¹åº”çš„ JS divergence å˜åŒ–å¯èƒ½å°±å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¼šçªç„¶å˜å¾—å¾ˆå¤§ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬æ‰€é¢„æƒ³çš„å‡å°äº†ã€‚ Problems in GANsç†è®ºä¸Šï¼Œæ»¡è¶³ JS æ•£åº¦è¶Šå°ï¼Œä¸¤ä¸ªåˆ†å¸ƒè¶Šæ¥è¿‘æ˜¯å¯ä»¥çš„ã€‚ä½†æ˜¯è¦ä½¿å¾— JS æ•£åº¦è¶Šæ¥è¶Šå°è¿™ä¸ªæœ‰ç‚¹éš¾åº¦ï¼Œå› ä¸ºå›¾åƒæ˜¯é«˜ç»´ç©ºé—´é‡Œé¢çš„ä½ç»´ mainfold. è¿™ä¹Ÿæ˜¯æ¥ä¸‹æ¥è¦è®²çš„é—®é¢˜ã€‚ å°½ç®¡ GAN åœ¨å›¾åƒç”Ÿæˆä¸Šå–å¾—äº†å¾ˆå¤§çš„æˆåŠŸï¼Œä½†æ˜¯å…¶è®­ç»ƒå¹¶ä¸å®¹æ˜“ï¼Œè¿‡ç¨‹å¾ˆæ…¢å¹¶ä¸”ä¸ç¨³å®šã€‚ä»¤äººæ‹æ¡ˆå«ç»çš„Wasserstein GAN å°†åŸå§‹ GAN çš„é—®é¢˜ä¸»è¦åˆ†æˆä¸¤éƒ¨åˆ†ã€‚ åˆ¤åˆ«å™¨çš„é—®é¢˜ï¼šD è¶Šå¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±è¶Šä¸¥é‡ã€‚ ç”Ÿæˆå™¨çš„é—®é¢˜ï¼šæœ€å°åŒ–ç”Ÿæˆå™¨losså‡½æ•°ï¼Œä¼šç­‰ä»·äºæœ€å°åŒ–ä¸€ä¸ªä¸åˆç†çš„è·ç¦»è¡¡é‡ï¼Œå¯¼è‡´ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯æ¢¯åº¦ä¸ç¨³å®šï¼ŒäºŒæ˜¯collapse modeå³å¤šæ ·æ€§ä¸è¶³ã€‚ åˆ¤åˆ«å™¨çš„é—®é¢˜ï¼šåˆ¤åˆ«å™¨è¶Šå¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±è¶Šä¸¥é‡å¯¹äºå‰é¢è¯´åˆ°çš„ JS æ•£åº¦ä¸Šã€‚æˆ‘ä»¬ä¼šå¸Œæœ›å¦‚æœä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´è¶Šæ¥è¿‘å®ƒä»¬çš„JSæ•£åº¦è¶Šå°ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–JSæ•£åº¦å°±èƒ½å°† $p_g$ â€œæ‹‰å‘â€ $p_r$ï¼Œæœ€ç»ˆä»¥å‡ä¹±çœŸã€‚è¿™ä¸ªå¸Œæœ›åœ¨ä¸¤ä¸ªåˆ†å¸ƒæœ‰æ‰€é‡å çš„æ—¶å€™æ˜¯æˆç«‹çš„ï¼Œä½†æ˜¯å¦‚æœä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨æ²¡æœ‰é‡å çš„éƒ¨åˆ†ï¼ˆè¦ä¹ˆæ˜¯ $x\\sim p_r$, è¦ä¹ˆæ˜¯ $x\\sim p_g$ï¼‰ï¼Œæˆ–è€…å®ƒä»¬é‡å çš„éƒ¨åˆ†å¯å¿½ç•¥ï¼ˆç­‰ä¼šå„¿è§£é‡Šä»€ä¹ˆå«å¯å¿½ç•¥ï¼‰ï¼Œå®ƒä»¬çš„JSæ•£åº¦æ˜¯å¤šå°‘å‘¢ï¼Ÿ $p_1(x)=0ä¸”p_2(x)=0$ $p_1(x)\\ne0ä¸”p_2(x)\\ne0$ $p_1(x)=0ä¸”p_2(x)\\ne 0$ $p_1(x)\\ne 0ä¸”p_2(x)=0$ ç¬¬ä¸€ç§å¯¹è®¡ç®—JSæ•£åº¦æ— è´¡çŒ® ç¬¬äºŒç§æƒ…å†µç”±äºé‡å éƒ¨åˆ†å¯å¿½ç•¥,ï¼ˆ$p_rå’Œ p_g$ éƒ½æ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ä½ç»´æµå½¢ï¼‰ï¼Œæ‰€ä»¥è´¡çŒ®ä¹Ÿä¸º0. ç¬¬ä¸‰ç§æƒ…å†µï¼Œå¸¦å…¥å…¬å¼ï¼ˆ6ï¼‰å€’æ•°ç¬¬ä¸‰æ­¥çš„çš„åä¸¤é¡¹ï¼ŒJS çš„æ•£åº¦è®¡ç®—å¯ä»¥å¾—åˆ°å…¶å€¼ä¸º log2. ç¬¬å››ç§æƒ…å†µåŒç†ã€‚ æ¢å¥è¯è¯´ï¼Œæ— è®ºè·Ÿæ˜¯è¿œåœ¨å¤©è¾¹ï¼Œè¿˜æ˜¯è¿‘åœ¨çœ¼å‰ï¼Œåªè¦å®ƒä»¬ä¿©æ²¡æœ‰ä¸€ç‚¹é‡å æˆ–è€…é‡å éƒ¨åˆ†å¯å¿½ç•¥ï¼ŒJSæ•£åº¦å°±å›ºå®šæ˜¯å¸¸æ•°ï¼Œè€Œè¿™å¯¹äºæ¢¯åº¦ä¸‹é™æ–¹æ³•æ„å‘³ç€â€”â€”æ¢¯åº¦ä¸º0ï¼æ­¤æ—¶å¯¹äºæœ€ä¼˜åˆ¤åˆ«å™¨æ¥è¯´ï¼Œç”Ÿæˆå™¨è‚¯å®šæ˜¯å¾—ä¸åˆ°ä¸€ä¸ç‚¹æ¢¯åº¦ä¿¡æ¯çš„ï¼›å³ä½¿å¯¹äºæ¥è¿‘æœ€ä¼˜çš„åˆ¤åˆ«å™¨æ¥è¯´ï¼Œç”Ÿæˆå™¨ä¹Ÿæœ‰å¾ˆå¤§æœºä¼šé¢ä¸´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚ ä½†æ˜¯ $P_r$ ä¸ $P_g$ ä¸é‡å æˆ–é‡å éƒ¨åˆ†å¯å¿½ç•¥çš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿä¸ä¸¥è°¨çš„ç­”æ¡ˆæ˜¯ï¼šéå¸¸å¤§ã€‚æ¯”è¾ƒä¸¥è°¨çš„ç­”æ¡ˆæ˜¯ï¼šå½“ $P_r$ ä¸ $P_g$ çš„æ”¯æ’‘é›†ï¼ˆsupportï¼‰æ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ä½ç»´æµå½¢ï¼ˆmanifoldï¼‰æ—¶ï¼Œ$P_r$ ä¸ $P_g$ é‡å éƒ¨åˆ†æµ‹åº¦ï¼ˆmeasureï¼‰ä¸º0çš„æ¦‚ç‡ä¸º1ã€‚ ä¹Ÿå°±æ˜¯æ¥ä¸‹æ¥è¦è¯´çš„: low dimensional support å’Œ gradient vanishing. Low dimensional supportsæœ‰ä¸¤ä¸ªæ•°å­¦æ¦‚å¿µï¼š Manifold: A topological space that locally resembles Euclidean space near each point. Precisely, when this Euclidean space is of dimension n, the manifold is referred as n-manifold. æ‹“æ‰‘ç©ºé—´ï¼Œåœ¨æ¯ä¸ªç‚¹é™„è¿‘å±€éƒ¨ç±»ä¼¼äºæ¬§å‡ é‡Œå¾·ç©ºé—´ã€‚ ç¡®åˆ‡åœ°è¯´ï¼Œå½“è¯¥æ¬§å‡ é‡Œå¾·ç©ºé—´å…·æœ‰nç»´æ—¶ï¼Œè¯¥æµå½¢è¢«ç§°ä¸ºn-æµå½¢ã€‚ Support: In mathematics, the support of a real-valued function f is the subset of the domain containing those elements which are not mapped to zero. åœ¨æ•°å­¦ä¸­ï¼Œå®å€¼å‡½æ•°fçš„æ”¯æŒæ˜¯åŒ…å«é‚£äº›æœªæ˜ å°„åˆ°é›¶çš„å…ƒç´ çš„åŸŸçš„å­é›† â€œTowards principled methods for training generative adversarial networksâ€. è¿™ç¯‡éå¸¸ç†è®ºçš„è®ºæ–‡è®¨è®ºäº†å¯¹äº $p_r$ å’Œ $p_g$ çš„supportæ˜¯å¤„äºä½ç»´çš„ç©ºé—´ï¼Œå¹¶ä¸”è¿™å¯¼è‡´äº†GANçš„è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§ã€‚ çœŸå®æ ·æœ¬ç©ºé—´å…·æœ‰é«˜åº¦çš„äººå·¥ç‰¹å¾ï¼Œå› ä¸ºå®ƒçš„ä¸»é¢˜ä¸€æ—¦ç¡®å®šï¼Œå…¶åŒ…å«çš„å¯¹è±¡ä¹Ÿå°±å›ºå®šäº†ã€‚æ¯”å¦‚dogåº”è¯¥æœ‰two earså’Œa tail.ä¸€ä¸ªSkyscraperåº”è¯¥æœ‰straightå’Œtallçš„èº«ä½“ã€‚è¿™äº›é™åˆ¶ä½¿å¾—å›¾åƒä¸å…·å¤‡é«˜ç»´ç©ºé—´çš„å½¢å¼ã€‚ åŒæ ·çš„ $p_g$ ä¹Ÿæ˜¯åœ¨ä½ç»´æµå½¢ç©ºé—´ã€‚å½“ç»™å®šåˆå§‹çš„å™ªå£°è¾“å…¥å˜é‡ä¸º100ç»´ï¼Œç”Ÿæˆå™¨å°†å…¶ä½œä¸ºè¾“å…¥ç”Ÿæˆè¾ƒå¤§çš„å›¾åƒ $64\\times 64$ï¼Œå¯¹äºè¾“å‡ºçš„åˆ†å¸ƒ 4096 pixelså·²ç»è¢«100ç»´éšæœºçš„å‘é‡å®šä¹‰äº†ï¼Œæ‰€ä»¥å®ƒä¹Ÿå¾ˆéš¾å»å¡«æ»¡æ•´ä¸ªé«˜ç»´ç©ºé—´ã€‚ â€œæ’‘ä¸æ»¡â€å°±ä¼šå¯¼è‡´çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒéš¾ä»¥â€œç¢°åˆ°é¢â€ï¼Œè¿™å¾ˆå®¹æ˜“åœ¨äºŒç»´ç©ºé—´ä¸­ç†è§£ï¼šä¸€æ–¹é¢ï¼ŒäºŒç»´å¹³é¢ä¸­éšæœºå–ä¸¤æ¡æ›²çº¿ï¼Œå®ƒä»¬ä¹‹é—´åˆšå¥½å­˜åœ¨é‡å çº¿æ®µçš„æ¦‚ç‡ä¸º0ï¼›å¦ä¸€æ–¹é¢ï¼Œè™½ç„¶å®ƒä»¬å¾ˆå¤§å¯èƒ½ä¼šå­˜åœ¨äº¤å‰ç‚¹ï¼Œä½†æ˜¯ç›¸æ¯”äºä¸¤æ¡æ›²çº¿è€Œè¨€ï¼Œäº¤å‰ç‚¹æ¯”æ›²çº¿ä½ä¸€ä¸ªç»´åº¦ï¼Œé•¿åº¦ï¼ˆæµ‹åº¦ï¼‰ä¸º0ï¼Œå¯å¿½ç•¥ã€‚ä¸‰ç»´ç©ºé—´ä¸­ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œéšæœºå–ä¸¤ä¸ªæ›²é¢ï¼Œå®ƒä»¬ä¹‹é—´æœ€å¤šå°±æ˜¯æ¯”è¾ƒæœ‰å¯èƒ½å­˜åœ¨äº¤å‰çº¿ï¼Œä½†æ˜¯äº¤å‰çº¿æ¯”æ›²é¢ä½ä¸€ä¸ªç»´åº¦ï¼Œé¢ç§¯ï¼ˆæµ‹åº¦ï¼‰æ˜¯0ï¼Œå¯å¿½ç•¥ã€‚ä»ä½ç»´ç©ºé—´æ‹“å±•åˆ°é«˜ç»´ç©ºé—´ï¼Œå°±æœ‰äº†å¦‚ä¸‹é€»è¾‘ï¼šå› ä¸ºä¸€å¼€å§‹ç”Ÿæˆå™¨éšæœºåˆå§‹åŒ–ï¼Œæ‰€ä»¥å‡ ä¹ä¸å¯èƒ½ä¸æœ‰ä»€ä¹ˆå…³è”ï¼Œæ‰€ä»¥å®ƒä»¬çš„æ”¯æ’‘é›†ä¹‹é—´çš„é‡å éƒ¨åˆ†è¦ä¹ˆä¸å­˜åœ¨ï¼Œè¦ä¹ˆå°±æ¯”å’Œçš„æœ€å°ç»´åº¦è¿˜è¦ä½è‡³å°‘ä¸€ä¸ªç»´åº¦ï¼Œæ•…è€Œæµ‹åº¦ä¸º0ã€‚æ‰€è°“â€œé‡å éƒ¨åˆ†æµ‹åº¦ä¸º0â€ï¼Œå°±æ˜¯ä¸Šæ–‡æ‰€è¨€â€œä¸é‡å æˆ–è€…é‡å éƒ¨åˆ†å¯å¿½ç•¥â€çš„æ„æ€ã€‚ å› ä¸º $p_r$ å’Œ $p_g$ éƒ½æ˜¯å¤„äºä½ç»´æµå½¢ï¼Œä»–ä»¬å¾ˆå¤§å¯èƒ½æ€§æ˜¯ä¸ç›¸äº¤çš„ã€‚å½“ä»–ä»¬å…·å¤‡ä¸ç›¸äº¤çš„ç‰¹æ€§æ—¶ï¼Œæˆ‘ä»¬å°±å¾ˆå®¹æ˜“æ‰¾åˆ°ä¸€ä¸ªå®Œç¾çš„åˆ¤åˆ«å™¨æ¥å‡†ç¡®çš„100%åŒºåˆ†fakeæ ·æœ¬å’ŒçœŸå®æ ·æœ¬ã€‚ å·¦ä¾§å›¾æ˜¯ä¸¤æ¡çº¿åœ¨ä¸‰ç»´ç©ºé—´ã€‚å³ä¾§æ˜¯ä¸¤ä¸ªå¹³é¢åœ¨ä¸‰ç»´ç©ºé—´ã€‚é€šè¿‡ç»´åº¦çš„å¯¹æ¯”æ¥è¡¨æ˜ç›¸äº¤çš„å¯èƒ½æ€§ã€‚ Vanishing gradientå½“åˆ¤åˆ«å™¨éå¸¸å®Œç¾çš„æ—¶å€™ï¼Œ$D(x)=1,\\forall x\\in p_r$, $D(x)=0, \\forall x\\in p_g$. $$L(G,D)=\\int_x(p_r(x)log(D(x))+p_g(x)log(1-D(x)))$$ å¸¦å…¥è¿™ä¸ªå…¬å¼å¯ä»¥å‘ç°ï¼Œloss function L ä¼šé™ä¸º0ï¼Œåœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦ä¹Ÿå°±æ— æ³•æ›´æ–°ã€‚ä¸‹å›¾è¯æ˜äº†ï¼Œå½“åˆ¤åˆ«å™¨è¶Šå¥½çš„æ—¶å€™ï¼Œæ¢¯åº¦æ¶ˆå¤±è¶Šå¿«ã€‚ WGANå‰ä½œFigure 2ã€‚å…ˆåˆ†åˆ«å°†DCGANè®­ç»ƒ1ï¼Œ20ï¼Œ25ä¸ªepochï¼Œç„¶åå›ºå®šç”Ÿæˆå™¨ä¸åŠ¨ï¼Œåˆ¤åˆ«å™¨é‡æ–°éšæœºåˆå§‹åŒ–ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¯¹äºç¬¬ä¸€ç§å½¢å¼çš„ç”Ÿæˆå™¨lossäº§ç”Ÿçš„æ¢¯åº¦å¯ä»¥æ‰“å°å‡ºå…¶å°ºåº¦çš„å˜åŒ–æ›²çº¿ï¼Œå¯ä»¥çœ‹åˆ°éšç€åˆ¤åˆ«å™¨çš„è®­ç»ƒï¼Œç”Ÿæˆå™¨çš„æ¢¯åº¦å‡è¿…é€Ÿè¡°å‡ã€‚æ³¨æ„yè½´æ˜¯å¯¹æ•°åæ ‡è½´ã€‚ ç”Ÿæˆå™¨çš„é—®é¢˜ï¼šæœ€å°åŒ–ç”Ÿæˆå™¨losså‡½æ•°ï¼Œä¼šç­‰ä»·äºæœ€å°åŒ–ä¸€ä¸ªä¸åˆç†çš„è·ç¦»è¡¡é‡è¿™æ ·ä¼šå¯¼è‡´ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯æ¢¯åº¦ä¸ç¨³å®šï¼ŒäºŒæ˜¯ mode collapse/dropping å³å¤šæ ·æ€§ä¸è¶³ã€‚ å‰é¢è¯´åˆ° Goodfellow ç»™äº†ä¸¤ä¸ª generator çš„ loss functionï¼Œä¹Ÿå°±æ˜¯å…¬å¼ ï¼ˆ2ï¼‰å’Œï¼ˆ3ï¼‰. Goodfellow æ¢æˆç¬¬äºŒç§ loss function çš„ç†ç”±å¦‚ä¸Šå›¾ï¼Œå› ä¸ºåœ¨è®­ç»ƒç”Ÿæˆå™¨ G æ˜¯ï¼ŒD(x) è‚¯å®šæ˜¯å¾ˆå°çš„ï¼Œæ‰€ä»¥è§‚å¯Ÿä¸Šå›¾å¯ä»¥çœ‹åˆ° log(1-D(x)) åœ¨ D(x) åå°çš„åŒºåŸŸæ¢¯åº¦å¾ˆå°ï¼Œæ‰€ä»¥å¯¼è‡´è®­ç»ƒå¾ˆæ…¢ã€‚ ä½†æ˜¯ï¼Œæ¢æˆç¬¬äºŒç§ loss ä¼šå¯¼è‡´ mode collapse. æ¥ä¸‹æ¥é€šè¿‡å…¬å¼æ¨å¯¼è¯æ˜è¿™ä¿©é—®é¢˜ã€‚ é€šè¿‡å…¬å¼ï¼ˆ1.5ï¼‰å’Œå…¬å¼ï¼ˆ6ï¼‰å¯ä»¥å¾—åˆ°åœ¨ $D^* $ çš„æ¡ä»¶ä¸‹ï¼š $$ \\mathbb{E}{x \\sim p{r}(x)} [\\log D^* (x)] + \\mathbb{E}{x \\sim p_g(x)} [\\log(1 - D^* (x)]=-2log2 + D{JS}(p_{r} | p_g)\\quad\\text{(7)}$$ æˆ‘ä»¬åœ¨ç®—ä¸€ä¸ª KL æ•£åº¦: $$% &lt;![CDATA[ \\begin{aligned} KL(p_g||p_r) &amp;=\\mathbb{E}_{x\\sim p_g}log(\\dfrac{p_g(x)}{p_r(x)})\\ &amp;=\\mathbb{E}_{x\\sim p_g}[log\\dfrac{p_g(x)/(p_g(x)+p_r(x))}{p_r(x)/p_g(x)+p_r(x)}]\\ &amp;=\\mathbb{E}_{x\\sim p_g}[log\\dfrac{1-D^* (x)}{D^* (x)}]\\ &amp;=\\mathbb{E}{x\\sim p_g}log[1-D^* (x)]-\\mathbb{E}{x\\sim p_g}logD^* (x)\\quad\\text{(8)} \\end{aligned} %]]&gt;$$ å°†å…¬å¼ï¼ˆ7ï¼‰å’Œ ï¼ˆ8ï¼‰å¸¦å…¥åˆ°ç¬¬äºŒç§ lossï¼ˆ3ï¼‰ä¸­å¯ä»¥å¾—åˆ°ï¼š $$\\begin{aligned} \\mathbb{E}_{x\\sim p_g}logD^* (x) &amp;=KL(p_g||p_r)-\\mathbb{E}_{x\\sim p_g}log[1-D^* (x)]\\ &amp;=KL(p_g||p_r)-D_{JS}(p_{r} | p_g)+2log2-\\mathbb{E}{x \\sim p{r}(x)}\\quad\\text{(7)} \\end{aligned}$$ ä¸Šå¼åä¸¤é¡¹ä¸ G æ— å…³ï¼Œæ‰€ä»¥æœ€å°åŒ– lossï¼ˆ3ï¼‰ç­‰ä»·äºæœ€å°åŒ–: $$KL(p_g||p_r)-D_{JS}(p_{r} | p_g)$$ è¿™ä¸ªç­‰ä»·æœ€å°åŒ–ç›®æ ‡å­˜åœ¨ä¸¤ä¸ªä¸¥é‡çš„é—®é¢˜ã€‚ç¬¬ä¸€æ˜¯å®ƒåŒæ—¶è¦æœ€å°åŒ–ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„KLæ•£åº¦ï¼Œå´åˆè¦æœ€å¤§åŒ–ä¸¤è€…çš„JSæ•£åº¦ï¼Œä¸€ä¸ªè¦æ‹‰è¿‘ï¼Œä¸€ä¸ªå´è¦æ¨è¿œï¼è¿™åœ¨ç›´è§‚ä¸Šéå¸¸è’è°¬ï¼Œåœ¨æ•°å€¼ä¸Šåˆ™ä¼šå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šï¼Œè¿™æ˜¯åé¢é‚£ä¸ªJSæ•£åº¦é¡¹çš„æ¯›ç—…ã€‚ ç¬¬äºŒï¼Œå³ä¾¿æ˜¯å‰é¢é‚£ä¸ªæ­£å¸¸çš„KLæ•£åº¦é¡¹ä¹Ÿæœ‰æ¯›ç—…ã€‚å› ä¸ºKLæ•£åº¦ä¸æ˜¯ä¸€ä¸ªå¯¹ç§°çš„è¡¡é‡ï¼ŒKL(P_g || P_r)ä¸KL(P_r || P_g)æ˜¯æœ‰å·®åˆ«çš„ã€‚ä»¥å‰è€…ä¸ºä¾‹: å½“ $P_g(x)\\rightarrow 0$ è€Œ $P_r(x)\\rightarrow 1$ æ—¶ï¼Œ$P_g(x) \\log \\frac{P_g(x)}{P_r(x)} \\rightarrow 0$ï¼Œå¯¹ $KL(P_g || P_r)$ è´¡çŒ®è¶‹è¿‘0 å½“ $P_g(x)\\rightarrow 1$ è€Œ $P_r(x)\\rightarrow 0$ æ—¶ï¼Œ$P_g(x) \\log \\frac{P_g(x)}{P_r(x)} \\rightarrow +\\infty$ï¼Œå¯¹ $KL(P_g || P_r)$ è´¡çŒ®è¶‹è¿‘æ­£æ— ç©· æ¢è¨€ä¹‹ï¼ŒKL(P_g || P_r)å¯¹äºä¸Šé¢ä¸¤ç§é”™è¯¯çš„æƒ©ç½šæ˜¯ä¸ä¸€æ ·çš„ï¼Œç¬¬ä¸€ç§é”™è¯¯å¯¹åº”çš„æ˜¯â€œç”Ÿæˆå™¨æ²¡èƒ½ç”ŸæˆçœŸå®çš„æ ·æœ¬â€ï¼Œæƒ©ç½šå¾®å°ï¼›ç¬¬äºŒç§é”™è¯¯å¯¹åº”çš„æ˜¯â€œç”Ÿæˆå™¨ç”Ÿæˆäº†ä¸çœŸå®çš„æ ·æœ¬â€ ï¼Œæƒ©ç½šå·¨å¤§ã€‚ç¬¬ä¸€ç§é”™è¯¯å¯¹åº”çš„æ˜¯ç¼ºä¹å¤šæ ·æ€§ï¼Œç¬¬äºŒç§é”™è¯¯å¯¹åº”çš„æ˜¯ç¼ºä¹å‡†ç¡®æ€§ã€‚è¿™ä¸€æ”¾ä¸€æ‰“ä¹‹ä¸‹ï¼Œç”Ÿæˆå™¨å®å¯å¤šç”Ÿæˆä¸€äº›é‡å¤ä½†æ˜¯å¾ˆâ€œå®‰å…¨â€çš„æ ·æœ¬ï¼Œä¹Ÿä¸æ„¿æ„å»ç”Ÿæˆå¤šæ ·æ€§çš„æ ·æœ¬ï¼Œå› ä¸ºé‚£æ ·ä¸€ä¸å°å¿ƒå°±ä¼šäº§ç”Ÿç¬¬äºŒç§é”™è¯¯ï¼Œå¾—ä¸å¿å¤±ã€‚è¿™ç§ç°è±¡å°±æ˜¯å¤§å®¶å¸¸è¯´çš„collapse modeã€‚ ç¬¬ä¸€éƒ¨åˆ†å°ç»“ï¼šåœ¨åŸå§‹GANçš„ï¼ˆè¿‘ä¼¼ï¼‰æœ€ä¼˜åˆ¤åˆ«å™¨ä¸‹ï¼Œç¬¬ä¸€ç§ç”Ÿæˆå™¨lossé¢ä¸´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œç¬¬äºŒç§ç”Ÿæˆå™¨lossé¢ä¸´ä¼˜åŒ–ç›®æ ‡è’è°¬ã€æ¢¯åº¦ä¸ç¨³å®šã€å¯¹å¤šæ ·æ€§ä¸å‡†ç¡®æ€§æƒ©ç½šä¸å¹³è¡¡å¯¼è‡´mode collapseè¿™å‡ ä¸ªé—®é¢˜ã€‚ è¿™ä½è€å“¥è®²çš„å¤ªå¥½äº†ã€‚ã€‚ç›´æ¥ copy äº†ã€‚ã€‚ Mode collapsemode collapse: é‡å¤ç”Ÿæˆä¸€å¼ å›¾ç‰‡ mode dropping: G åœ¨è¿­ä»£æ—¶åªèƒ½ç”Ÿæˆä¸€ç±»å›¾ç‰‡ã€‚ è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼šLack of a proper evaluation metricGAN æ²¡æœ‰ä¸€ä¸ªå¥½çš„ç›®æ ‡å‡½æ•°æ¥æè¿°è®­ç»ƒè¿‡ç¨‹ã€‚æ²¡æœ‰å¥½çš„éªŒè¯æŒ‡æ ‡ï¼Œå°±å¥½æ¯”åœ¨é»‘æš—ä¸­work. æ²¡æœ‰ä¿¡å·æ¥æç¤ºè¯¥åœ¨ä»€ä¹ˆæ—¶å€™åœæ­¢ï¼Œä¹Ÿæ²¡æœ‰å¥½çš„æŒ‡æ ‡æ¥è¯„ä»·å¤šç§æ¨¡å‹çš„å¥½åã€‚ Improving GAN Training Improve Techniques for Training GANs Towards principled methods for training generative adversarial networks Wasserstein GAN (WGAN)Wasserstein Distance æ˜¯ä¸€ç§æµ‹é‡ä¸¤ä¸ªåˆ†å¸ƒè·ç¦»çš„æ–¹å¼ã€‚å¯ä»¥ç±»æ¯”æˆ earth moverâ€™s distance. ä¸ºäº†ç®€å•çš„ç†è§£ï¼Œå¯ä»¥æŠŠä¸¤ä¸ªåˆ†å¸ƒçœ‹ä½œæ˜¯ç¦»æ•£çš„ã€‚è¿™é‡Œçœ‹ä½œæ˜¯ä¸¤å †åœŸï¼ŒWasserstein distance å°±æ˜¯è®¡ç®—å¦‚ä½•ç§»åŠ¨æœ€å°‘é‡çš„åœŸä½¿å¾—ä¸¤ä¸ªåˆ†å¸ƒä¸€è‡´ã€‚ æ‰€ä»¥ç›¸æ¯” JS æ•£åº¦ï¼Œä»åˆ†ç±»ä»»åŠ¡å˜æˆäº†å›å½’ä»»åŠ¡ã€‚å³ä½¿ä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨æ— äº¤é›†ï¼Œä¹Ÿæ˜¯å­˜åœ¨ divergence æ›´å¤§æˆ–è€…æ›´å°çš„é—®é¢˜ï¼Œæ‰€ä»¥ä¹Ÿå°±ä¸å­˜åœ¨æ¢¯åº¦ä¸ºé›¶çš„æƒ…å†µäº†ã€‚ ä½†æ˜¯é—®é¢˜æ¥äº†ï¼Œæ€ä¹ˆè®¡ç®— Wasserstein distance å‘¢ï¼Ÿ step1: $p_1 \\underrightarrow{2} p_2$, ä½¿å¾— $p_1å’ŒQ_1$ match. step2: $p_2 \\underrightarrow{2} p_3$, ä½¿å¾— $p_2å’ŒQ_2$ match. step3: $Q_3 \\underrightarrow{1} Q_4$, ä½¿å¾— $p_3å’ŒQ_3$ match. æ‰€ä»¥æ€»çš„ W=5. å¯¹äºè¿ç»­åˆ†å¸ƒï¼ŒWasserstein distanceï¼š $$W(p_r, p_g) = \\inf_{\\gamma \\sim \\Pi(p_r, p_g)} \\mathbb{E}_{(x, y) \\sim \\gamma}[| x-y |]$$ $\\Pi(p_r, p_g)$ æ˜¯æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒ. å…¶ä¸­ä¸€ç§è”åˆåˆ†å¸ƒ $\\gamma \\sim \\Pi(p_r, p_g)$ è¡¨ç¤ºä¸€ç§ move plan, å°±æ¯”å¦‚ä¸Šå›¾ä¸­çš„ç¤ºä¾‹ã€‚ å…¶ä¸­,å¯¹äºä»»ä½•ä¸€ä¸ªè”åˆåˆ†å¸ƒ $\\gamma$,å…¶è¾¹ç¼˜åˆ†å¸ƒåˆ†åˆ«æ˜¯ $p_g(x)=\\sum_x\\gamma(x,y)$, $p_r(y)=\\sum_y\\gamma(x,y)$. åœ¨æ­¤åˆ†å¸ƒä¸‹çš„ç§»åŠ¨è·ç¦»æ˜¯ $||x-y||$. é‚£ä¹ˆå½“å‰è”åˆåˆ†å¸ƒä¸‹çš„ cost æ˜¯ $\\gamma(x, y) \\cdot | x-y |$. å…¶æœŸæœ›å°±æ˜¯ï¼š $$\\sum_{x, y} \\gamma(x, y) | x-y |= \\mathbb{E}_{x, y \\sim \\gamma} | x-y |$$ è€Œæˆ‘ä»¬éœ€è¦æ±‚çš„æ˜¯æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒä¸­çš„ä¸‹ç•Œ, å°±å®šä¹‰ä¸º Wasserstein distance. Why Wasserstein is better than JS or KL divergence?Wasserstein è·ç¦»çš„ä¼˜åŠ¿åœ¨äºï¼Œå³ä½¿ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰äº¤é›†ï¼Œä¹Ÿèƒ½å¹³æ»‘çš„è¡¨ç¤ºä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„æ•£åº¦ã€‚ $$\\forall (x, y) \\in P, x = 0 \\text{ and } y \\sim U(0, 1)\\ \\forall (x, y) \\in Q, x = \\theta, 0 \\leq \\theta \\leq 1 \\text{ and } y \\sim U(0, 1)\\$$ å½“ $\\theta\\ne 0$ æ—¶ï¼Œåˆ†åˆ«è®¡ç®— KL,JSï¼ŒWS æ•£åº¦ï¼š $$% &lt;![CDATA[ \\begin{aligned} D_{KL}(P | Q) &amp;= \\sum_{x=0, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{0} = +\\infty \\ D_{KL}(Q | P) &amp;= \\sum_{x=\\theta, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{0} = +\\infty \\ D_{JS}(P, Q) &amp;= \\frac{1}{2}(\\sum_{x=0, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{1/2} + \\sum_{x=0, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{1/2}) = \\log 2\\ W(P, Q) &amp;= |\\theta| \\end{aligned} %]]&gt;$$ å½“ $\\theta = 0$ æ—¶ï¼Œåˆ†åˆ«è®¡ç®— KL,JSï¼ŒWS æ•£åº¦ï¼š $$% &lt;![CDATA[ \\begin{aligned} D_{KL}(P | Q) &amp;= D_{KL}(Q | P) = D_{JS}(P, Q) = 0\\ W(P, Q) &amp;= 0 = \\lvert \\theta \\rvert \\end{aligned} %]]&gt;$$ å½“ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰äº¤é›†æ—¶ï¼ŒKL æ•£åº¦çš„å€¼æ˜¯ inifity. JS æ•£åº¦åˆ™å­˜åœ¨ä¸è¿ç»­çš„é—®é¢˜ï¼Œå¯¹äºè¿™ä¸ªä¾‹å­è€Œè¨€ï¼Œå½“ $\\theta=0$ æ—¶ï¼ŒJS æ•£åº¦ä¸å¯å¾®ã€‚åªæœ‰ WS è·ç¦»æ˜¯å¯å¾®çš„ï¼Œè¿™å¯¹äºæ¢¯åº¦ä¸‹é™è€Œè¨€æ˜¯éå¸¸å‹å¥½çš„ã€‚ Use Wasserstein distance as GAN loss functionä½†æ˜¯è¦ç©·å°½ä¸¤ä¸ªè”åˆåˆ†å¸ƒçš„æ‰€æœ‰æƒ…å†µæ¥è®¡ç®— $\\inf_{\\gamma \\sim \\Pi(p_r, p_g)}$ æ˜¯ä¸å¯èƒ½çš„ã€‚WGAN çš„ä½œè€…ç»™å‡ºäº†ä¸€ä¸ªèªæ˜çš„è½¬æ¢ï¼Œå¯ä»¥æŠŠå…¬å¼ (8) å†™æˆï¼š $$W(p_g,p_r)=\\dfrac{1}{K}sup_{|f|_ L \\le K}\\mathbb{E}{x\\sim p_r}[f(x)]-\\mathbb{E}{x\\sim p_g}[f(x)]\\quad\\text{(9)}$$ è¿™é‡Œçš„æ„æ€å°±æ˜¯ç”¨ f å‡½æ•°æ¥è¡¨ç¤ºä¸Šé¢è¯´åˆ°çš„ä»»ä½•å¯èƒ½çš„è”åˆåˆ†å¸ƒã€‚æ‰€ä»¥ $\\mathbb{E}{x\\sim p_r}[f(x)]-\\mathbb{E}{x\\sim p_g}[f(x)]$ ç­‰æ•ˆäº $\\mathbb{E}_{x, y \\sim \\gamma} | x-y |$. ç„¶åæˆ‘ä»¬åˆçŸ¥é“ç¥ç»ç½‘ç»œè¶³å¤Ÿå¼ºå¤§ï¼Œæ‰€ä»¥ç”¨ç¥ç»ç½‘ç»œ D æ¥ä»£æ›¿ fï¼Œæ¥è¡¨ç¤ºä¸Šé¢è¯´åˆ°çš„ä»»ä½•å¯èƒ½çš„è”åˆåˆ†å¸ƒã€‚ä½†æ˜¯ D å¿…é¡»åƒå‰é¢æåˆ°çš„ Wasserstein distance é‚£æ ·è¶³å¤Ÿå…‰æ»‘ã€‚è¿™æ ·ä¸€æ¥ï¼Œ Wasserstein distance å°±è½¬å˜æˆäº†æˆ‘ä»¬æƒ³è¦çš„ loss function. $$V(G,D)=\\max_{D\\sim \\text{1-Lipschitz}}{\\mathbb{E}{x\\sim p_r}[D(x)]-\\mathbb{E}{x\\sim p_g}[D(x)]}\\quad\\text{(10)}$$ é€šè¿‡é‡‡æ ·æ¥è®¡ç®— V(G,D),æˆ‘ä»¬å¸Œæœ› è¿™é‡Œç”¨ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜ä¸ºä»€ä¹ˆ D è¦è¶³å¤Ÿå…‰æ»‘ï¼š æˆ‘ä»¬çœ‹åˆ°è¦è®© V(G,D) åœ¨ real example ä¸Šå¢å¤§ï¼Œåœ¨ fake example ä¸Šå‡å°ï¼Œ D å®Œå…¨å¯ä»¥åšåˆ°åƒä¸Šå›¾ä¸­çº¢è‰²ç®­å¤´é‚£æ ·ã€‚æ‰€ä»¥è¿™æ ·ä¸‹æ¥ï¼ŒD æ— æ³•æ”¶æ•›ã€‚ æ€ä¹ˆä¿è¯ä¸€ä¸ªç¥ç»ç½‘ç»œè¶³å¤Ÿå…‰æ»‘, $D\\sim \\text{1-Lipschitz}$ï¼Œè²Œä¼¼å¬èµ·æ¥å¾ˆéš¾ï¼Œæ¯•ç«Ÿæ¶‰åŠåˆ°é‚£ä¹ˆå¤šçš„å‚æ•°ã€‚ Lipschitz continuity: è¿™é‡Œé¦–å…ˆéœ€è¦ä»‹ç»ä¸€ä¸ªæ¦‚å¿µâ€”â€”Lipschitzè¿ç»­ã€‚å®ƒå…¶å®å°±æ˜¯åœ¨ä¸€ä¸ªè¿ç»­å‡½æ•° f ä¸Šé¢é¢å¤–æ–½åŠ äº†ä¸€ä¸ªé™åˆ¶ï¼Œè¦æ±‚å­˜åœ¨ä¸€ä¸ªå¸¸æ•° $K\\geq 0$ ä½¿å¾—å®šä¹‰åŸŸå†…çš„ä»»æ„ä¸¤ä¸ªå…ƒç´  $x_1$ å’Œ $x_2$ éƒ½æ»¡è¶³ $$|f(x_1) - f(x_2)| \\leq K |x_1 - x_2|$$ æ­¤æ—¶ç§°å‡½æ•° fçš„Lipschitzå¸¸æ•°ä¸ºKã€‚å®é™…ä¸Šå°±æ˜¯ f å‡½æ•°çš„å¯¼å‡½æ•°çš„å€¼ä¸èƒ½è¶…è¿‡ K. Lipschitz è¿ç»­æ¡ä»¶é™åˆ¶äº†ä¸€ä¸ªè¿ç»­å‡½æ•°çš„æœ€å¤§å±€éƒ¨å˜åŠ¨å¹…åº¦ã€‚ åœ¨ WGAN è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…é‡‡ç”¨äº†ä¸€ç§ç‰¹åˆ«ç®€å•çš„æ–¹æ³•ï¼ŒWeight Clipping. å¯¹äºä»»ä½•å‚æ•°ï¼Œä½¿å…¶åœ¨ [-c,c] èŒƒå›´å†…ï¼Œå°±å¯ä»¥ä¿è¯ K ä¸ä¼šç‰¹åˆ«å¤§ã€‚ã€‚ åˆ°æ­¤ï¼Œæˆ‘ä»¬å°±èƒ½æŠŠ Wasserstein distance åº”ç”¨åˆ°äº† GAN ä¸Šï¼Œä¹Ÿå°±æ˜¯ WGAN. ç›¸æ¯”ä¼ ç»Ÿçš„ GANï¼Œå…¶åŒºåˆ«åœ¨äºï¼š åˆ¤åˆ«å™¨ D çš„ä»»åŠ¡ä¸æ˜¯åˆ†ç±»ï¼Œè€Œæ˜¯å›å½’ã€‚æ‰€ä»¥å»æ‰æœ€åä¸€å±‚ sigmoid. ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„ loss ä¸å– logï¼ŒåŸå› æ˜¯ Wasserstein distance å°±æ˜¯è¿™æ ·å‘€~ æ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°c ä¸è¦ç”¨åŸºäºåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬momentumå’ŒAdamï¼‰ï¼Œæ¨èRMSPropï¼ŒSGDä¹Ÿè¡Œ æ€»çš„æµç¨‹å°±æ˜¯ï¼š $n_{\\text{critic}}$ è¡¨ç¤ºåˆ¤åˆ«å™¨çš„è®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼Œç”Ÿæˆå™¨åœ¨ä¸€æ¬¡å®Œæ•´çš„è¿­ä»£ä¸­åªè®­ç»ƒä¸€æ¬¡ã€‚ å¯¹äºåˆ¤åˆ«å™¨çš„losså°±æ˜¯å…¬å¼ï¼ˆ10ï¼‰ $$\\mathbb{E}{x\\sim p_g}[D(x)]-\\mathbb{E}{x\\sim p_r}[D(x)]\\quad(11)$$ ä¸Šå›¾ä¸­ä¸è¿™ä¸ªæ˜¯ç›¸åçš„ï¼Œæ‰€ä»¥ä¸Šè¿°æµç¨‹ä¸­ä½¿ç”¨çš„æ˜¯æ¢¯åº¦ä¸Šå‡ã€‚å¦‚æœç”¨å…¬å¼ï¼ˆ11ï¼‰è¿˜æ˜¯åº”è¯¥æ˜¯æ¢¯åº¦ä¸‹é™ã€‚ ç”Ÿæˆå™¨çš„lossæ˜¯ç¬¬äºŒé¡¹ $$-\\mathbb{E}_{x\\sim p_g}[D(x)]\\quad(12)$$ å…¶ä¸­ $-\\mathbb{E}{x\\sim p_g}[D(x)]+\\mathbb{E}{x\\sim p_r}[D(x)]$ å¯ä»¥æŒ‡ç¤ºè®­ç»ƒè¿›ç¨‹ï¼Œå…¶æ•°å€¼è¶Šå°ï¼Œè¡¨ç¤ºçœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒçš„Wassersteinè·ç¦»è¶Šå°ï¼ŒGANè®­ç»ƒå¾—è¶Šå¥½ã€‚ improved WGANimproved WGAN ä¸»è¦æ˜¯æ”¹è¿› weight clipping è¿™ä¸€ç•¥æ˜¾ç²—ç³™çš„æ–¹å¼ã€‚å–ä»£å®ƒçš„æ˜¯å¢åŠ ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œæ¥çº¦æŸå‚æ•°çš„å˜åŒ–ã€‚ $$|\\nabla_xD(x)|\\le 1$$ ç±»ä¼¼äº SVM lossï¼š $$-\\lambda\\mathbb{E}{x\\sim p{penalty}}[max(0, |\\nabla_xD(x)|- 1)]$$ å…¶ä¸­ $x\\sim p_{penalty}$ è¿™éƒ¨åˆ†è¡¨ç¤ºçš„æ˜¯ $p_rå’Œp_g$ è¿çº¿ä¸Šçš„æ ·æœ¬é‡‡æ ·ã€‚ è¿™é‡Œé¡ºä¾¿æŠŠ $max(0, |\\nabla_xD(x)|- 1)$ æ”¹è¿›æˆäº† $(|\\nabla_xD(x)|- 1)^2$ ä»¥æ­¤æ¥æƒ©ç½šæ¢¯åº¦å¤ªå°çš„é¡¹ã€‚ â€œSimply penalizing overly large gradients also works in theory, but experimentally we found that this approach converged faster and to better optima.â€ Spectrum NormSpectral Normalization â†’ Keep gradient norm smaller than 1 everywhere [Miyato, et al., ICLR, 2018] ä½†å…¶å®å‰é¢è¯´åˆ°çš„ $(|\\nabla_xD(x)|- 1)^2$ è¿™ä¸€æ­£åˆ™æƒ©ç½šé¡¹ä¾ç„¶æ˜¯å­˜åœ¨é—®é¢˜çš„ã€‚å› ä¸ºä»»æ„ sample $p_r å’Œ p_g$ ä¸­çš„ä¸¤ç‚¹ï¼Œç„¶åæ‹‰è¿›ä»–ä»¬ä¿©ï¼Œå®é™…ä¸Šå¹¶ä¸å¤ªåˆç†ï¼Œå› ä¸ºä¸ $p_g$ æœ€æ¥è¿‘çš„ $p_r$ ä¸­çš„ä¸€ç‚¹å¹¶ä¸å°±æ˜¯é‡‡æ ·åˆ°çš„è¿™ä¸ª.","link":"/2019/05/18/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-from-GAN-to-WGAN/"},{"title":"rejectionç³»åˆ—3 OpenMax","text":"paper: Towards Open Set Deep Networks. CVPR Motivationclosed set recognition å¤©ç„¶çš„ç‰¹æ€§ä½¿å¾—å®ƒå¿…é¡»é€‰æ‹©ä¸€ä¸ªç±»åˆ«ä½œä¸ºé¢„æµ‹å¯¹è±¡ã€‚ä½†æ˜¯å®é™…åœºæ™¯ä¸‹ï¼Œ recognition system å¿…é¡»å­¦ä¼š reject unknown/unseen classes åœ¨ testing é˜¶æ®µã€‚ äºæ˜¯ä¹ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„ model layer, OpenMax, èƒ½å¤Ÿä¼°è®¡ä¸€ä¸ªæ ·æœ¬è¾“å…¥æ˜¯æ¥è‡ªäº unknown class çš„æ¦‚ç‡ã€‚ A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. æ‰€ä»¥å…³é”®è¯æ˜¯ meta-recohnition, activation pattern/vector. Introductionå¾ˆå¤šå·¥ä½œæ˜¯åŸºäº threshold æ¥æ‰¾å‡º unknown çš„ï¼Œä»–ä»¬è®¤ä¸º unknwon é€šè¿‡ softmax ä¼šå¾—åˆ° low probability/confidence. ä½†æ˜¯å®é™…ä¸Šå¾ˆå¤š â€œfoolingâ€ â€œrubbishâ€ ä¹Ÿä¼šæ‹¥æœ‰ high probability/confidence scores. æ¯”å¦‚é€šè¿‡å¯¹æŠ—å­¦ä¹ å¾—åˆ°çš„ adversarial images. ä½œè€…åœ¨åé¢ä¹Ÿæåˆ°äº†ï¼Œ threshold å®é™…ä¸Šæ‹’ç»çš„ä¸æ˜¯ unknown, è€Œæ˜¯ uncertain predictions. OpenMax incorporates likelihood of the recognition system failure. This likelihood is used to estimate the probability for a given input belonging to an unknown class. For this estimation, we adapt the concept of Meta-Recognition[22, 32, 9] to deep networks. We use the scores from the penultimate layer of deep networks (the fully connected layer before SoftMax, e.g., FC8) to estimate if the input is â€œfarâ€ from known training data. We call scores in that layer the activation vector(AV). å…³äº OpenMax å¦‚æœå®ç°çš„ç®€å•æ€»ç»“ï¼Œå›è¿‡å¤´åœ¨çœ‹ã€‚ A key insight in our opening deep networks is noting that â€œopen space riskâ€ should be measured in feature space rather than in pixel space. ä¸€ä¸ªé‡è¦çš„è§‚ç‚¹æ˜¯ï¼Œåœ¨ open deep networks é‡Œé¢ï¼Œ open space risk åº”è¯¥æ˜¯ä»ç‰¹å¾ç©ºé—´ feature space çš„è§’åº¦å‡ºå‘çš„ï¼Œ è€Œä¸æ˜¯ pixel space. ä¹Ÿå°±æ˜¯ç¥ç»ç½‘ç»œåˆ¤æ–­æ˜¯ä¸æ˜¯ unknown, åº”è¯¥æ˜¯ä» feature çš„è§’åº¦æ¥çœ‹çš„ã€‚ We show that an extreme-value meta-recognition inspired distance normalization process on the overall activation patterns of the penultimate network layer provides a rejection probability for OpenMax normalization for unknown images, fooling images and even for many adversarial images. Open set deep networks Building on the concepts of open space risk, we seek to choose a layer (feature space) in which we can build a compact abating probability model that can be thresholded to limit open space risk. åŸºäº open space risk çš„æ¦‚å¿µï¼Œæå‡ºäº† compact abating probability model èƒ½é™åˆ¶ open space risk. multi-classes meta-recognitionä½œè€…å…ˆç®€å•ä»‹ç»äº†ä¸€ä¸‹å‰äººçš„å·¥ä½œ: . Prior work on meta-recognition used the final system scores, analyzed their distribution based on Extreme Value Theory (EVT) and found these distributions follow Weibull distribution. æ„Ÿè§‰çœ‹æ‡‚è¿™éƒ¨åˆ†å…ˆè¦ç†è§£æå€¼ç†è®º(Extreme value theory). from wikipedia: It seeks to assess, from a given ordered sample of a given random variable, the probability of events that are more extreme than any previously observed. å®ƒè¯•å›¾ä»ç»™å®šéšæœºå˜é‡çš„ç»™å®šæœ‰åºæ ·æœ¬ä¸­è¯„ä¼°æ¯”å…ˆå‰è§‚å¯Ÿåˆ°çš„ä»»ä½•äº‹ä»¶æ›´æç«¯çš„äº‹ä»¶çš„æ¦‚ç‡. ç„¶åæ˜¯ æå€¼åˆ†å¸ƒ çš„ä¸€ç§ Weibull distribution æ‰€ä»¥ Weibull distribution å°±æ˜¯ä»æ•´ä¸ªåˆ†å¸ƒä¸­å–æœ€æç«¯çš„ä¾‹å­ sampling top-n scoreï¼Œç„¶åçš„åˆ°çš„åˆ†å¸ƒã€‚ å°†æå€¼ç†è®ºè¿ç”¨åˆ°è§†è§‰ç‰¹å¾çš„æå–ä¸­ã€‚å…·ä½“çš„æˆ‘ä¹Ÿä¸å¤ªæ¸…æ¥šäº†ã€‚ã€‚è¿™ä¹Ÿæ˜¯å‰äººçš„ç ”ç©¶ã€‚ä½œè€…ä¹Ÿå¹¶æ²¡æœ‰é‡‡å–è¿™ç§æ–¹æ³•ã€‚ We take the approach that the network values from penultimate layer (hereafter the Activation Vector (AV)), are not an independent per-class score estimate, but rather they provide a distribution of what classes are â€œrelated.â€ ä½œè€…é‡‡ç”¨çš„æ–¹æ³•æ˜¯ å€’æ•°ç¬¬äºŒå±‚ï¼Œä¹Ÿå°±æ˜¯ (Activation Vector) æä¾›ä¸åŒ classes ä¹‹é—´çš„ç›¸å…³æ€§åˆ†å¸ƒï¼Œè€Œä¸æ˜¯æ¯ä¸€ä¸ªç±»å¯¹åº”çš„ç‹¬ç«‹çš„åˆ†å¸ƒã€‚ interpretation of activation vectorOpenMax","link":"/2018/12/11/rejection%E7%B3%BB%E5%88%973-OpenMax/"},{"title":"ä»0å¼€å§‹GAN-2-sequence generation by GAN","text":"paper list Generating Sentences from a Continuous Space GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution Categorical Reparameterization with Gum-bel-Softmax Deep Reinforcement Learning for Dialogue Generation Generative Adversarial Networks æå®æ¯…è€å¸ˆè®²seqGAN Role of RL in Text Generation by GAN(å¼ºåŒ–å­¦ä¹ åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ–‡æœ¬ç”Ÿæˆä¸­æ‰®æ¼”çš„è§’è‰²) å¥½ç©çš„æ–‡æœ¬ç”Ÿæˆ Conditional Generative Adversarial Nets Generative Adversarial Text to Image Synthesis Adversarial Learning for Neural Dialogue Generation How (not) to train your generative model: Scheduled sampling, likelihood, adversary? Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation ä¸ºä»€ä¹ˆGANä¸é€‚åˆæ–‡æœ¬ç”Ÿæˆå‰é¢å­¦è¿‡äº†GANå¾ˆè‡ªç„¶çš„å°±ä¼šæƒ³åˆ°å°†GANå¼•å…¥åˆ°æ–‡æœ¬ç”Ÿæˆä¸­æ¥ï¼Œæ¯”å¦‚å¯¹è¯å¯ä»¥çœ‹ä½œæ˜¯conditional GAN, ä½†å®é™…ä¸Šå´å¹¶ä¸å¦‚æƒ³è±¡ä¸­é‚£æ ·ç®€å•ï¼ŒåŸå› æ˜¯GANåªé€‚ç”¨äºè¿ç»­æ•°æ®çš„ç”Ÿæˆï¼Œå¯¹ç¦»æ•£æ•°æ®æ•ˆæœä¸ä½³ã€‚ Role of RL in Text Generation by GAN(å¼ºåŒ–å­¦ä¹ åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ–‡æœ¬ç”Ÿæˆä¸­æ‰®æ¼”çš„è§’è‰²) è¿™é‡Œé¢ä»ä¸¤æ–¹é¢è®²çš„å¾ˆæ¸…æ¥š: samplingï¼šä»ç”Ÿæˆå¾—åˆ°çš„softmax probabilityåˆ°one-hotå‘é‡ï¼Œä»è€ŒæŸ¥è¯¢å‡ºå¯¹åº”indexçš„è¯ï¼Œè¿™ä¸€æ­¥ç§°ä¸ºâ€œsamplingâ€ï¼Œæ˜¾ç„¶æ˜¯ä¸å¯å¾®çš„ã€‚ å»æ‰sampling,å°†softmax probabilityå’Œone-hot vectorä½œä¸ºdiscriminatorçš„è¾“å…¥ï¼Œå¦‚æœæ˜¯discriminatoræ˜¯ä¸€ä¸ªäºŒåˆ†ç±»å™¨çš„è¯ï¼Œåˆ¤åˆ«å™¨Då¾ˆå®¹æ˜“â€œä½œå¼Šâ€ï¼Œå®ƒæ ¹æœ¬ä¸ç”¨å»åˆ¤æ–­ç”Ÿæˆåˆ†å¸ƒæ˜¯å¦ä¸çœŸå®åˆ†å¸ƒæ›´åŠ æ¥è¿‘ï¼Œå®ƒåªéœ€è¦è¯†åˆ«å‡ºç»™åˆ°çš„åˆ†å¸ƒæ˜¯ä¸æ˜¯é™¤äº†ä¸€é¡¹æ˜¯ 1 ï¼Œå…¶ä½™éƒ½æ˜¯ 0 å°±å¯ä»¥äº†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æƒ³åˆ°ç”¨WGANæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚Improved Training of Wasserstein GANsä¹Ÿç»™å‡ºäº†æ–‡æœ¬ç”Ÿæˆçš„å®éªŒï¼Œæ•ˆæœå½“ç„¶æ˜¯å¥½äº†å¾ˆå¤šï¼Œä¸è‡³äºç›´æ¥å´©äº†ã€‚ ä½†æ˜¯WGANä¸ºä»€ä¹ˆæ²¡é‚£ä¹ˆå¥½å‘¢ï¼Ÿå°†ä¸€ä¸ªsoftmax probabilityå¼ºè¡Œæ‹‰å€’ä¸€ä¸ªone-hot vectorçœŸçš„å¯è¡Œå—ï¼Ÿ Gumbel-softmaxï¼Œæ¨¡æ‹ŸSamplingçš„softmaxRL in text generationreinforcement learningreinforcement learning å’Œç›‘ç£å­¦ä¹ ã€éç›‘ç£å­¦ä¹ ä¸€èµ·æ„æˆæœºå™¨å­¦ä¹ çš„ä¸‰å¤§èŒƒå¼ã€‚ Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) RLæ‰€é€‚ç”¨çš„ç¯å¢ƒæ˜¯ä¸€ä¸ªå…¸å‹çš„é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹(Markov decision process,MDP)ã€‚æ‰€ä»¥å¼ºåŒ–å­¦ä¹ å®é™…ä¸Šä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§åŠ¨æ€è§„åˆ’çš„æ–¹æ³•ã€‚ä¸è¿‡ä¸ä¼ ç»Ÿçš„dynamic programmingæ–¹æ³•ä¸åŒçš„æ˜¯ï¼ŒRLä¸ä¼šå‡è®¾MDPçš„ç²¾ç¡®æ•°å­¦æ¨¡å‹çš„çŸ¥è¯†ã€‚æˆ‘çš„ç†è§£æ˜¯ï¼Œåœ¨å¾ˆå¤šDPé—®é¢˜ä¸­ï¼ŒçŠ¶æ€è½¬ç§»çŸ©é˜µæ˜¯å·²çŸ¥çš„ï¼Œä½†æ˜¯RLæ‰€å¤„ç†çš„é—®é¢˜ï¼Œä»ä¸€ä¸ªçŠ¶æ€åˆ°å¦ä¸€ä¸ªçŠ¶æ€ï¼Œä¸æ˜¯æ ¹æ®å·²æœ‰çš„çŸ¥è¯†ï¼Œè€Œæ˜¯å–å†³äºå½“å‰actionå¸¦æ¥çš„rewardä»¥åŠæœªæ¥çš„reward,æ‰€ä»¥è¿™ä¹Ÿå°±æ¶‰åŠåˆ°äº† exploration å’Œ exploitation çš„å¹³è¡¡é—®é¢˜ã€‚ Markov decision process åŒ…æ‹¬ï¼šGANs-in-NLP/Reinforcement_learning_diagram.png ç¯å¢ƒä»¥åŠagentçŠ¶æ€çš„é›†åˆ S; agentèƒ½é‡‡å–çš„åŠ¨ä½œçš„é›†åˆ $A$ çŠ¶æ€ä¹‹é—´è½¬æ¢çš„è§„åˆ™ $P_a(s,sâ€™)=Pr(s_{t+1}=sâ€™|s_t=s,a_t=a)$ è§„å®šè½¬æ¢ä¹‹åçš„å³æ—¶å¥–åŠ± $R_a(s,sâ€™)$ æè¿°ä¸»ä½“èƒ½å¤Ÿè§‚å¯Ÿåˆ°ä»€ä¹ˆçš„è§„åˆ™(è¿™æ˜¯å•¥ç©æ„ï¼Ÿï¼Ÿ) policyå°†ä»å¤´åˆ°å°¾æ‰€æœ‰çš„åŠ¨ä½œè¿åœ¨ä¸€èµ·å°±ç§°ä¸ºä¸€ä¸ªâ€œç­–ç•¥â€æˆ–â€œç­–ç•¥è·¯å¾„â€ $pi$ ï¼Œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å°±æ˜¯æ‰¾å‡ºèƒ½å¤Ÿè·å¾—æœ€å¤šå¥–åŠ±çš„æœ€ä¼˜ç­–ç•¥. $\\pi: A\\times S \\rightarrow [0,1]$ $\\pi(a,s)=Pr(a_t=a|s_t=s)$ state-value functionçŠ¶æ€-å€¼å‡½æ•° $V_{\\pi}(s)$ å®šä¹‰åœ¨å½“å‰çŠ¶æ€ sä¸‹ï¼ŒæŒ‰ç…§ç­–ç•¥ $\\pi$ æ¥ä¸‹æ¥èƒ½è·å¾—çš„ reward.ä¹Ÿå°±æ˜¯è¯´ï¼Œgiven state sï¼Œå½“å‰ä»¥åŠæœªæ¥çš„rewardæœŸæœ›. $$V_{\\pi}(s)=E[R]=E[\\sum_{t=0}^{\\infty}\\gamma^tr_t|s_0=s]$$ å…¶ä¸­ $\\gamma^t$ æ˜¯æŠ˜æ‰£å› å­ï¼Œå› ä¸ºè¿˜æ˜¯å½“å‰åˆ©ç›Šæœ€é‡è¦å˜›ï¼Œæ‰€ä»¥æœªæ¥çš„rewardè¦æ‰“ä¸ªæŠ˜ã€‚ $$R=\\sum_{t=0}^{\\infty}\\gamma^tr_t$$ value functionvalue funcion å’Œ state-value function çš„åŒºåˆ«æ˜¯åè€…ç»™å®šäº†ä¸€ä¸ª state. è€Œvalue functionæ˜¯è®¡ç®—ç»™å®šä»»æ„åˆå§‹çŠ¶æ€ï¼Œå¾—åˆ°çš„reward. $$V^{\\pi}=E[R|s,\\pi]$$ æ‰€ä»¥æœ€ä¼˜çš„ policy å®é™…ä¸Šå°±æ˜¯ value function çš„æœŸæœ›æœ€å¤§ã€‚$\\rho^{\\pi}=E[V^{\\pi}(S)]$ï¼Œ å…¶ä¸­çŠ¶æ€Sæ˜¯ä»ä¸€ä¸ªåˆ†å¸ƒ $\\mu$ éšæœºé‡‡æ ·å¾—åˆ°çš„ã€‚ å°½ç®¡ state-value è¶³å¤Ÿå®šä¹‰æœ€ä¼˜ policyï¼Œå†å®šä¹‰ä¸€ä¸ª action-value ä¹Ÿæ˜¯å¾ˆæœ‰ç”¨çš„ã€‚ given state s, action a, policy $\\pi$, action-value: $$Q^{\\pi}(s,a)=E[R|s,a,\\pi]$$ ä¸ªäººç†è§£ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œå¾ˆå¤šæ—¶å€™æ˜¯ç”± action æ¥ç¡®å®šä¸‹ä¸€ä¸ª state çš„ã€‚æ‰€ä»¥ action-value è¿™ä¸ªfunctionä¼šæ›´å®ç”¨å§ã€‚æ¯”å¦‚ text generationï¼Œsampleå½“å‰è¯å°±æ˜¯ actionï¼Œç„¶åæ‰æœ‰ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„ state. Monte Carlo methodsTemporal difference methodsRLåº”ç”¨åˆ°å¯¹è¯åœºæ™¯ä¸‹Deep Reinforcement Learning for Dialogue Generation å¯¹è¯ç”Ÿæˆä»»åŠ¡æœ¬èº«éå¸¸ç¬¦åˆå¼ºåŒ–å­¦ä¹ çš„è¿è¡Œæœºç†ï¼ˆè®©äººç±»æ»¡æ„ï¼Œæ‹¿å¥–åŠ±ï¼‰ã€‚ è¾“å…¥å¥å­æ˜¯ h,æ¨¡å‹è¿”å›çš„responseæ˜¯ xï¼Œå…¶ä»äººç±»å¾—åˆ°çš„å¥–åŠ±æ˜¯ $R(h,x)$. åŸºäºRLçš„ç›®æ ‡å‡½æ•°å°±æ˜¯æœ€å¤§åŒ–å¯¹è¯çš„æœŸæœ›å¥–åŠ±ã€‚ä¸Šå›¾ä¸­ $p_{\\theta}(x,h)$ è¡¨ç¤ºåœ¨ $\\theta$ å‚æ•°ä¸‹ï¼Œä¸€ç»„å¯¹è¯ $(x,h)$ å‡ºç°çš„æ¦‚ç‡ã€‚$P(h)$ è¡¨ç¤ºå‡ºç°å¥å­ h çš„æ¦‚ç‡ã€‚ æœ€å¤§åŒ–å¥–åŠ±æœŸæœ›ï¼š $$å…¬å¼(1)$$ ä¸Šå¼ä¸­ $h\\sim P(h)$ å¯ä»¥çœ‹ä½œæ˜¯å‡åŒ€åˆ†å¸ƒï¼Œæ‰€ä»¥ $E_{h\\sim P(h)}\\approx \\dfrac{1}{N}$. å…¶ä¸­ $E_{x\\sim P_{\\theta}(x|h)}$ çš„è®¡ç®—æ— æ³•è€ƒè™‘æ‰€æœ‰çš„å¯¹è¯ï¼Œæ‰€ä»¥é€šè¿‡é‡‡æ · $(h^1,x^1), (h^2,x^2), .., (h^N,x^N)$ æ¥è®¡ç®—ã€‚ ç„¶åé—®é¢˜æ¥äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¼˜åŒ–çš„å‚æ•° $\\theta$ ä¸è§äº†ï¼Œè¿™æ€ä¹ˆå¯¹ $\\theta$ è¿›è¡Œæ±‚å¯¼å‘¢ï¼Ÿå¯ä»¥é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¸­å¸¸ç”¨çš„ policy gradient è¿›è¡Œå˜å½¢ï¼š $$\\dfrac{dlog(f(x))}{dx}=\\dfrac{1}{f(x)}\\dfrac{df(x)}{dx}$$ é€‚å½“å˜å½¢åï¼Œå¯¹ $\\theta$ è¿›è¡Œæ±‚å¯¼ï¼š $$å…¬å¼(2)$$ è¿™æ ·ä¸€æ¥ï¼Œæ¢¯åº¦ä¼˜åŒ–çš„é‡å¿ƒå°±è½¬åŒ–åˆ°äº†ç”Ÿæˆå¯¹è¯çš„æ¦‚ç‡ä¸Šæ¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œé€šè¿‡å¯¹å‚æ•° $\\theta$ è¿›è¡Œæ›´æ–°ï¼Œå¥–åŠ±ä¼šä½¿æ¨¡å‹è¶‹äºå°†ä¼˜è´¨å¯¹è¯çš„å‡ºç°æ¦‚ç‡æé«˜ï¼Œè€Œæƒ©ç½šåˆ™ä¼šè®©æ¨¡å‹è¶‹äºå°†åŠ£è´¨å¯¹è¯çš„å‡ºç°æ¦‚ç‡é™ä½ã€‚ è‡ªAlphaGoä½¿å¾—å¼ºåŒ–å­¦ä¹ çŒ›ç„¶è¿›å…¥å¤§ä¼—è§†é‡ä»¥æ¥ï¼Œå¤§éƒ¨åˆ†å¯¹äºå¼ºåŒ–å­¦ä¹ çš„ç†è®ºç ”ç©¶éƒ½å°†æ¸¸æˆä½œä¸ºä¸»è¦å®éªŒå¹³å°ï¼Œè¿™ä¸€ç‚¹ä¸æ— é“ç†ï¼Œå¼ºåŒ–å­¦ä¹ ç†è®ºä¸Šçš„æ¨å¯¼çœ‹ä¼¼é€»è¾‘é€šé¡ºï¼Œä½†å…¶æœ€å¤§çš„å¼±ç‚¹åœ¨äºï¼ŒåŸºäºäººå·¥è¯„åˆ¤çš„å¥–åŠ± Reward çš„è·å¾—ï¼Œè®©å®éªŒäººå‘˜å®ˆåœ¨ç”µè„‘å‰å¯¹æ¨¡å‹åå‡ºæ¥çš„ç»“æœä¸åœåœ°æ‰“åˆ†çœ‹æ¥æ˜¯ä¸ç°å®çš„ï¼Œæ¸¸æˆç³»ç»Ÿæ°æ°èƒ½ä¼šç»™å‡ºæ­£ç¡®å®¢è§‚çš„æ‰“åˆ†ï¼ˆè¾“/èµ¢ æˆ– æ¸¸æˆScoreï¼‰ã€‚åŸºäºRLçš„å¯¹è¯ç”ŸæˆåŒæ ·ä¼šé¢å¯¹è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨äº†ç±»ä¼¼AlphaGoçš„å®ç°æ–¹å¼ï¼ˆAIæ£‹æ‰‹å¯¹å¼ˆï¼‰â€”â€”åŒæ—¶è¿è¡Œä¸¤ä¸ªæœºå™¨äººï¼Œè®©å®ƒä»¬è‡ªå·±äº’ç›¸å¯¹è¯ï¼ŒåŒæ—¶ï¼Œä½¿ç”¨é¢„è®­ç»ƒï¼ˆpre-trainedï¼‰å¥½çš„â€œæ‰“åˆ†å™¨â€ç»™å‡ºæ¯ç»„å¯¹è¯çš„å¥–åŠ±å¾—åˆ† R(a^i, x^i) ï¼Œå…³äºè¿™ä¸ªé¢„è®­ç»ƒçš„â€œæ‰“åˆ†å™¨â€ R ï¼Œå¯ä»¥æ ¹æ®å®é™…çš„åº”ç”¨å’Œéœ€æ±‚è‡ªå·±DIYã€‚ SeqGANseqGANå¯¹å‰é¢ä»…åŸºäºRLçš„å¯¹è¯ç”Ÿæˆè¿›è¡Œäº†æ”¹è¿›ï¼Œä¹Ÿå°±æ˜¯å‰é¢ç”¨pre-trainedçš„æ‰“åˆ†å™¨ï¼ˆæˆ–è€…æ˜¯äººç±»ï¼‰ï¼Œç”¨GANä¸­çš„åˆ¤åˆ«å™¨è¿›è¡Œäº†ä»£æ›¿ã€‚ è¿™é‡Œé—®é¢˜åœ¨äºç”Ÿæˆå¾—åˆ°çš„response xè¾“å…¥åˆ°åˆ¤åˆ«å™¨æ—¶ï¼Œè¿™ä¸ªè¿‡ç¨‹æ¶‰åŠåˆ°äº†samplingçš„æ“ä½œï¼Œæ‰€ä»¥å›ºå®šdiscriminatoræ¥æ›´æ–°generatoræ—¶ï¼Œæ¢¯åº¦æ— æ³•å›æµã€‚ è¿™å°±éœ€è¦RLçš„å‡ºç°äº†ã€‚ æ€»ç»“ä¸€ä¸‹RLåœ¨è¿™é‡Œé¢çš„ä½œç”¨ï¼šè¿™é‡Œçš„discriminatorå¾—åˆ°çš„æ˜¯rewardã€‚æˆ‘ä»¬fixä½åˆ¤åˆ«å™¨Dæ¥ä¼˜åŒ–ç”Ÿæˆå™¨ $\\theta$ çš„è¿‡ç¨‹å°±å˜æˆäº†ï¼šç”Ÿæˆå™¨ä¸å†æ˜¯åŸæ¥çš„sampleä¸€ä¸ªè¯ï¼Œä½œä¸ºä¸‹ä¸€ä¸ªtime stepçš„è¾“å…¥ï¼Œå› ä¸ºè¿™ä¸å¯å¯¼ã€‚è€Œæ˜¯æŠŠå½“å‰time stepä½œä¸ºä¸€ä¸ªstateï¼Œç„¶åé‡‡å–actionï¼Œè¿™ä¸ªactionå½“ç„¶ä¹Ÿæ˜¯åœ¨è¯è¡¨ä¸­é€‰ä¸€ä¸ªè¯(ç”¨Monte Carlo Search). ä»¥å‰æ˜¯é€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶æ¦‚ç‡ï¼ˆæœ€å°åŒ–äº¤å‰ç†µï¼‰æ¥ä¼˜åŒ–ç”Ÿæˆå™¨ï¼Œç°åœ¨æ˜¯å¯»æ‰¾æœ€ä¼˜çš„ policyï¼ˆæœ€å¤§åŒ–å¥–åŠ±æœŸæœ›ï¼‰æ¥ä¼˜åŒ–ç”Ÿæˆå™¨ã€‚è€Œé‡‡ç”¨policy gradientå¯ä»¥å°†rewardæœŸæœ›å†™æˆ $\\theta$ çš„è¿ç»­å‡½æ•°ï¼Œç„¶åå°±å¯ä»¥æ ¹æ®æœ€å¤§åŒ–rewardæœŸæœ›æ¥ä¼˜åŒ– $\\theta$,ä¹Ÿå°±æ˜¯æ¢¯åº¦ä¸Šå‡ã€‚ æœ‰äº†å‰é¢çš„åŸºç¡€å†é‡æ–°é˜…è¯»seqGANè¿™ç¯‡paper. motivationä¼ ç»Ÿçš„GANåœ¨åºåˆ—ç”Ÿæˆçš„èƒ½åŠ›æœ‰é™ä¸»è¦æ˜¯ä¸¤ä¸ªåŸå› ï¼š æ— æ³•å¤„ç†ç¦»æ•£çš„æ•°æ®ï¼ˆå‰é¢å·²ç»è®²è¿‡äº†ï¼‰ åˆ¤åˆ«å™¨Dåªèƒ½å¯¹å®Œæ•´çš„åºåˆ—è¿›è¡Œè¯„ä»·ï¼ˆåŸå› æ˜¯åˆ¤åˆ«å™¨å°±æ˜¯åŸºäºå®Œæ•´çš„å¥å­æˆ–dialogueè¿›è¡Œè®­ç»ƒçš„ï¼‰ã€‚ä½†æ˜¯åœ¨åºåˆ—ç”Ÿæˆçš„è¿‡ç¨‹ä¸­ï¼Œåœ¨ç”Ÿæˆéƒ¨åˆ†åºåˆ—çš„æ—¶å€™ï¼Œå¯¹å½“å‰éƒ¨åˆ†åºåˆ—çš„è¯„ä»·ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚ ä¼ ç»Ÿçš„åŸºäº RNN/attention çš„åºåˆ—ç”Ÿæˆæ¨¡å‹ä¹Ÿå­˜åœ¨ exposure bias çš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯è®­ç»ƒé˜¶æ®µå’Œinferenceé˜¶æ®µä¸ä¸€è‡´çš„é—®é¢˜ã€‚åœ¨è®­ç»ƒé˜¶æ®µæ˜¯teacher forcingï¼Œè€Œåœ¨inferé˜¶æ®µï¼Œä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ä»…ä»…ä¾èµ–äºå½“å‰çš„éšè—çŠ¶æ€ï¼ˆattention-basedä¼šæœ‰attention vectorï¼‰. Bengio çš„å¼Ÿå¼Ÿï¼Œå¦ä¸€ä¸ª Bengio æå‡ºäº† scheduled sampling çš„æ–¹æ³•ï¼Œä½†è¿™ä¾ç„¶æœªèƒ½å®Œå…¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ ä¸ºæ­¤ï¼Œä½œè€…æå‡ºåŸºäºRLçš„seqGANã€‚å¯¹åºåˆ—ç”Ÿæˆçš„é—®é¢˜è¿›è¡Œå»ºæ¨¡ï¼ŒæŠŠåºåˆ—ç”Ÿæˆé—®é¢˜çœ‹ä½œæ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Data generation as sequential decision making)ï¼Œä»è€Œè½¬æ¢æˆåŸºäºRLçš„å¯»æ‰¾æœ€ä¼˜policyçš„é—®é¢˜ï¼Œæœ‰æ•ˆçš„è§£å†³äº†ä¸Šè¿°ä¸‰ä¸ªé—®é¢˜ã€‚ Sequence Generative Adversarial Netsè¿™é‡Œå…ˆä»‹ç»ä¸€äº›æ•°å­¦ç¬¦å·ï¼š æˆ‘ä»¬çš„ç›®çš„æ˜¯è®­ç»ƒå¾—åˆ°ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ $G_{\\theta}$ï¼Œä½¿å…¶èƒ½ç”Ÿæˆå¾—åˆ°è¿™æ ·çš„ä¸€ä¸ªåºåˆ— $Y_{1:T}=(y_1,â€¦,y_t,â€¦,y_T)$. å…¶ä¸­ $y_t\\sim V$. Væ˜¯å€™é€‰è¯è¡¨ã€‚ç”¨RLæ¥æè¿°åºåˆ—ç”Ÿæˆçš„è¿‡ç¨‹å°±æ˜¯ï¼š å½“å‰æ—¶é—´æ­¥ t çš„çŠ¶æ€ state s: $(y_1,â€¦,y_{t-1})$ action a æ˜¯é€‰æ‹©ä¸‹ä¸€ä¸ª token $y_t$. policyä¹Ÿå°±æ˜¯ç”Ÿæˆæ¨¡å‹ $G_{\\theta}(y_t|Y_{1:t-1})$ çŠ¶æ€çš„è½¬ç§»å–å†³äº action a. æ¯”å¦‚çŠ¶æ€è½¬ç§»çš„æ¦‚ç‡ $\\sigma_{s,sâ€™}^a=1$ï¼Œä¹Ÿå°±æ˜¯åœ¨å½“å‰çŠ¶æ€ $s=Y_{1:t-1}$ æƒ…å†µä¸‹ï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯ $sâ€™$ çš„æ¦‚ç‡ä¸º1ï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯ $sâ€™=Y_{1:t}$,å¯¹åº”çš„actionä¹Ÿå°±æ˜¯ $a=y_t$. é¦–å…ˆæˆ‘ä»¬éœ€è¦è®­ç»ƒä¸€ä¸ªåˆ¤åˆ«æ¨¡å‹ $D_{\\phi}(Y_{1:T})$, é€šè¿‡åˆ¤æ–­è¾“å…¥æ¥è‡ª real or fake è¿›è¡Œè®­ç»ƒã€‚è€Œç”Ÿæˆå™¨çš„è®­ç»ƒéœ€è¦å€ŸåŠ©äºåˆ¤åˆ«å™¨Dçš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯ reward. SeqGAN via Policy Gradientå¦‚æœä¸è€ƒè™‘ä¸­é—´æ¯ä¸€ä¸ªæ—¶é—´æ­¥çš„å¥–åŠ±ï¼Œä¹Ÿå°±æ˜¯åªè€ƒè™‘æ•´ä¸ªsentenceçš„reward, é‚£ä¹ˆåŸºäºç”Ÿæˆæ¨¡å‹ï¼ˆpolicyï¼‰$G_{\\theta}(y_t|Y_{1:t-1})$ çš„æœ€å¤§å¥–åŠ±æœŸæœ›çš„å‡½æ•°æ˜¯: $$J(\\theta)=E[R_T|s_0,\\theta]=\\sum_{y\\sim V}G_{\\theta}(y|s_0)\\cdot Q_{D_{\\phi}}^{G_{\\theta}}(s_0,y)$$ å…¶ä¸­ $R_T$ æ˜¯å¯¹æ•´ä¸ªsentenceçš„å¥–åŠ±, $G_{\\theta}(y|s_0)$ æ˜¯ given $s_0$,ç”Ÿæˆ $y$ çš„æ¦‚ç‡ï¼Œ$Q_{D_{\\phi}}^{G_{\\theta}}(s_0,y )$ æ˜¯ action-value å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯ given $s_0$ å’Œ policy $G_{\\theta}$ åé‡‡å–çš„ action æ˜¯ $y$ æ—¶å¯¹åº”çš„ reward. åœ¨è¿™ç¯‡è®ºæ–‡é‡Œé¢ï¼Œreward å°±æ˜¯åˆ¤åˆ«å™¨åˆ¤æ–­ç”Ÿæˆçš„sentenceä¸ºrealçš„æ¦‚ç‡ã€‚ $$Q_{D_{\\phi}}^{G_{\\theta}}(a=y_T,s=Y_{1:T-1})=D_{\\phi}(Y_{1:T})$$ ä½†æ˜¯å¯¹äºåºåˆ—ç”Ÿæˆé—®é¢˜ï¼Œä¸èƒ½ä»…ä»…è€ƒè™‘å®Œæ•´çš„å¥å­çš„rewardï¼Œè¿˜è¦è€ƒè™‘åˆ°æ¯ä¸€ä¸ª time step. ä½†æ˜¯åœ¨æ¯ä¸€ä¸ªtime stepä¹Ÿä¸èƒ½è´ªå¿ƒçš„åªè€ƒè™‘å½“å‰æœ€å¤§çš„rewardï¼Œè¿˜è¦è€ƒè™‘åˆ°æœªæ¥çš„æƒ…å†µ. ä½œè€…æå‡ºåŸºäº Monte Carlo search çš„æ–¹æ³•ã€‚ Monte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement. Monte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy ${\\displaystyle \\pi }$, the goal is to compute the function values ${\\displaystyle Q^{\\pi }(s,a)}$ (or a good approximation to them) for all state-action pairs ${\\displaystyle (s,a)}$. Assuming (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair ${\\displaystyle (s,a)}$ can be computed by averaging the sampled returns that originated from ${\\displaystyle (s,a)}$ over time. Given sufficient time, this procedure can thus construct a precise estimate ${\\displaystyle Q}$ of the action-value function ${\\displaystyle Q^{\\pi }}$. This finishes the description of the policy evaluation step. policy iterationåˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼Œpolicy evaluationå’Œpolicy improvement.è’™ç‰¹å¡æ´›è¢«ç”¨åœ¨policy evaluation stepä¸­ï¼Œç»™å®šä¸€ä¸ªé™æ€çš„ï¼Œåˆ¤åˆ«å‹çš„policy $\\pi$ï¼Œå…¶ç›®æ ‡æ˜¯è®¡ç®— å…·ä½“æ¥è¯´ï¼Œåœ¨å½“å‰çŠ¶æ€ $s=Y_{1:t}$ ä¸‹ï¼ŒåŸºäºä¸€ä¸ª roll-out policy $G_{\\beta}$ ç”Ÿæˆå‰©ä¸‹çš„ T-t ä¸ªtokensï¼Œè¿™ä¸ªè¿‡ç¨‹é‡å¤ N æ¬¡. $${Y_{1:T}^1,â€¦,Y_{1:T}^N}=MC^{G_{\\beta}}(Y_{1:t;N})$$ å¼å­å·¦è¾¹æ˜¯ N ä¸ªå®Œæ•´çš„sentenceã€‚ å¯¹äº roll-out policy $G_{\\beta}$ ä½œè€…åœ¨è¿™ç¯‡ paper ä¸­é‡‡ç”¨çš„ä¸ç”Ÿæˆæ¨¡å‹ä¸€æ ·çš„ $G_{\\theta}$. å¦‚æœè¿½æ±‚é€Ÿåº¦çš„è¯ï¼Œå¯ä»¥é€‰æ‹©æ›´ç®€å•çš„ç­–ç•¥ã€‚ è¿™æ ·åŸºäº Monte Carlo method å°±èƒ½è®¡ç®—æ¯ä¸€ä¸ª time step çš„èƒ½è€ƒè™‘åˆ° future çš„reward. $$Q_{D_{\\phi}}^{G_{\\theta}}(s=Y_{1:t-1}, a=y_t)= \\begin{cases} \\dfrac{1}{N}\\sum_{n=1}^ND_{\\phi}(Y_{1:T}^n),Y_{1:T}^n \\sim MC^{G_{\\beta}}(Y_{1:t;N}), \\quad \\text{for t &lt; T}\\ D_{\\phi}(Y_{1:t}),\\quad\\text{for t = T} \\end{cases}\\quad (4)$$ å…¬å¼è¿˜æ˜¯æ¯”è¾ƒå¥½ç†è§£çš„ã€‚æ‰€ä»¥äº‹å®ä¸Šåˆ¤åˆ«å™¨ $D_{\\phi}$ ä¾æ—§æ˜¯åªèƒ½åˆ¤æ–­å®Œæ•´çš„sentenceï¼Œä½†æ˜¯åœ¨æ¯ä¸€ä¸ª time step å¯ä»¥å€ŸåŠ©äº roll-out policy æ¥å¾—åˆ°å®Œæ•´çš„sentenceï¼Œè¿›è€Œå¯¹å½“å‰ action è¿›è¡Œè¯„åˆ†ï¼Œè®¡ç®—å¾—åˆ° $a=y_t$ çš„rewardã€‚ çŸ¥é“äº†å¦‚ä½•è®¡ç®—rewardï¼Œå°±å¯ä»¥åˆ©ç”¨æœ€å¤§åŒ–è¿™ä¸ªå¥–åŠ±æœŸæœ›æ¥ä¼˜åŒ–æˆ‘ä»¬çš„ç”Ÿæˆå™¨ï¼ˆpolicy $G_{\\theta}$ï¼‰.å¯¹ $\\theta$ æ±‚å¯¼: $$\\nabla J(\\theta)=\\sum_{t=1}^T\\mathbb{E}{Y{1:t-1}\\sim G_{\\theta}}[\\sum_{y_t\\sim V}\\nabla_{\\theta}G_{\\theta}({y_t|Y_{1:t-1}})\\cdot Q_{D_{\\phi}}^{G_{\\theta}}(Y_{1:t-1},y_t)]\\quad\\text{å…¬å¼(3)}$$ å…¬å¼ï¼ˆ3ï¼‰ä¸å‰é¢æå¼˜æ¯…è€å¸ˆè®²çš„å…¬å¼ï¼ˆ2ï¼‰æ˜¯ä¸€è‡´çš„ï¼Œåªä¸è¿‡è¿™é‡Œè€ƒè™‘çš„ä¸­é—´ reward.ä¸Šå¼ä¸­ $E_{Y_{1:t-1}\\sim G_{\\theta}}[\\cdot]$ ç­‰åŒäºå‰é¢æåˆ°çš„ $E_{x\\sim P_{\\theta}(x|h)}$ éƒ½æ˜¯é€šè¿‡sample æ¥è®¡ç®—çš„ã€‚åŒæ · reward çš„è®¡ç®—å¼ $Q_{D_{\\phi}}^{G_{\\theta}}(Y_{1:t-1},y_t)$ ä¹Ÿæ˜¯ä¸åŒ…å«ç”Ÿæˆå™¨çš„å‚æ•° $\\theta$ çš„ã€‚ ä¸Šè¿°å…¬å¼ä¸­ $\\sum_{y_t\\sim V}\\sim G_{\\theta}(y_t|Y_{1:t-1})$ ç„¶ååŸºäºæ¢¯åº¦ä¸Šå‡æ¥ä¼˜åŒ–å‚æ•° $\\theta$. $$\\theta \\leftarrow \\theta + \\alpha_h\\nabla J(\\theta)\\quad(8)$$ ä½œè€…å»ºè®®ä½¿ç”¨ Adam æˆ– RMSprop ä¼˜åŒ–ç®—æ³•ã€‚ é™¤äº†ç”Ÿæˆå™¨çš„ä¼˜åŒ–ï¼Œè¿™é‡Œçš„åˆ¤åˆ«å™¨Dæ˜¯åŠ¨æ€çš„ã€‚è¿™æ ·ç›¸æ¯”ä¼ ç»ŸåŸºäºpre-trainçš„åˆ¤åˆ«å™¨ä¼šæ›´å¼å§ã€‚ä¼˜åŒ–åˆ¤åˆ«å™¨çš„ç›®æ ‡å‡½æ•°æ˜¯ï¼š $$\\min_{\\phi}-\\mathbb{E}{Y\\sim p{data}}[logD_{\\phi}(Y)]-\\mathbb{E}{Y\\sim G{\\theta}}[log(1-D_{\\phi}(Y))]\\quad(5)$$ å…·ä½“çš„ç®—æ³•æ­¥éª¤æ˜¯ï¼š And to reduce the vari- ability of the estimation, we use different sets of negative samples combined with positive ones, which is similar to bootstrapping (Quinlan 1996) The Generative Model for Sequencesä½œè€…ä½¿ç”¨åŸºäº LSTM çš„ç”Ÿæˆå™¨Gã€‚ $$h_t=g(h_{t-1},x_t)$$ $$p(y_t|x_1,â€¦,x_t)=z(h_t)=softmax(c+Vh_t)$$ The Discriminative Model for Sequencesä½œè€…ä½¿ç”¨åŸºäº CNN çš„åˆ¤åˆ«å™¨ï¼Œç”¨æ¥é¢„æµ‹ä¸€ä¸ªsentenceä¸ºrealçš„æ¦‚ç‡ã€‚ ä¸€äº›ç»†èŠ‚ + ä¸€äº›å»¶ä¼¸åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒåŸºæœ¬ç†è§£äº†seqGANçš„å¤§éƒ¨åˆ†ç»†èŠ‚ï¼Œéœ€è¦çœ‹çœ‹æºç æ¶ˆåŒ–ä¸‹ã€‚ æ¥ä¸‹æ¥ä¼šæœ‰æ›´å¤šçš„ç»†èŠ‚å’Œæ”¹è¿›å¯å…ˆå‚è€ƒï¼šRole of RL in Text Generation by GAN(å¼ºåŒ–å­¦ä¹ åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ–‡æœ¬ç”Ÿæˆä¸­æ‰®æ¼”çš„è§’è‰²) seagan ä»£ç å­¦ä¹ TensorArray å’Œ åŸºäºlstmçš„MDPæ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆè¿™ä¹Ÿæ˜¯seqgançš„æ ¸å¿ƒï¼Œç”¨Monte Carlo searchä»£æ›¿samplingæ¥é€‰æ‹©next token.åœ¨çœ‹å…·ä½“ä»£ç ä¹‹å‰å…ˆäº†è§£ä¸‹ tensorarray. TensorArray Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays This class is meant to be used with dynamic iteration primitives such as while_loop and map_fn. It supports gradient back-propagation via special â€œflowâ€ control flow dependencies. ä¸€ä¸ªå°è£…äº†åŠ¨æ€å¤§å°ã€per-time-step å†™å…¥ä¸€æ¬¡çš„ tensoræ•°ç»„çš„ç±»ã€‚åœ¨åºåˆ—ç”Ÿæˆä¸­ï¼Œåºåˆ—çš„é•¿åº¦é€šå¸¸æ˜¯ä¸å®šçš„ï¼Œæ‰€ä»¥ä¼šéœ€è¦ä½¿ç”¨åŠ¨æ€tensorarray. ç±»åˆå§‹åŒ–12345678910111213141516171819202122232425def __init__(self, dtype, size=None, dynamic_size=None, clear_after_read=None, tensor_array_name=None, handle=None, flow=None, infer_shape=True, element_shape=None, colocate_with_first_write_call=True, name=None): size: int32 scalar Tensor, åŠ¨æ€æ•°ç»„çš„å¤§å° dynamic_size: Python bool, æ˜¯å¦å¯ä»¥å¢é•¿ï¼Œé»˜è®¤false æ–¹æ³• stack 1234567def stack(self, name=None): &quot;&quot;&quot;Return the values in the TensorArray as a stacked `Tensor`. &quot;&quot;&quot; å°†åŠ¨æ€æ•°ç»„ stack èµ·æ¥ï¼Œå¾—åˆ°æœ€ç»ˆçš„ tensor. concat 1234567def concat(self, name=None): &quot;&quot;&quot;Return the values in the TensorArray as a concatenated `Tensor`. &quot;&quot;&quot; å°†åŠ¨æ€æ•°ç»„ concat èµ·æ¥ï¼Œå¾—åˆ°æœ€ç»ˆçš„ tensor. read 123456789def read(self, index, name=None): &quot;&quot;&quot;Read the value at location `index` in the TensorArray. è¯»è¿‡ä¸€æ¬¡ä¹‹åä¼šæ¸…0. ä¸èƒ½è¯»ç¬¬äºŒæ¬¡ã€‚ä½†å¯ä»¥å†æ¬¡å†™å…¥ä¹‹åã€‚ &quot;&quot;&quot; write 1234567891011def write(self, index, value, name=None): &quot;&quot;&quot;Write `value` into index `index` of the TensorArray. &quot;&quot;&quot; - index: int32 scalar with the index to write to. - value: tf.Tensor gather unstack split scatter tf.while_loop1234567891011121314151617181920212223242526272829def while_loop_v2(cond, body, loop_vars, shape_invariants=None, parallel_iterations=10, back_prop=True, swap_memory=False, maximum_iterations=None, name=None):&quot;&quot;&quot;Repeat `body` while the condition `cond` is true.&quot;&quot;&quot;- cond: callable, return boolean scalar tensor. å‚æ•°ä¸ªæ•°å¿…é¡»å’Œ loop_vars ä¸€è‡´ã€‚ - body: vallable. å¾ªç¯æ‰§è¡Œä½“ï¼Œå‚æ•°ä¸ªæ•°å¿…é¡»å’Œ loop_vars ä¸€è‡´.- loop_vars: å¾ªç¯å˜é‡ï¼Œtuple, namedtuple or list of numpy array. example:123456789101112131415161718192021222324252627282930313233343536373839matrix = tf.random.normal(shape=[5, 1], dtype=tf.float32)sequence_length = 5gen_o = tf.TensorArray(dtype=tf.float32, size=sequence_length, dynamic_size=False, infer_shape=True)init_state = (0, gen_o)condition = lambda i, _: i &lt; sequence_lengthbody = lambda i, gen_o : (i+1, gen_o.write(i, matrix[i] * 2))n, gen_o = tf.while_loop(condition, body, init_state)gen_o_stack = gen_o.stack()gen_o_concat = gen_o.concat()ç”¨ LSTM æ¨¡æ‹Ÿé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹print(gen_o) # TensorArray objectprint(gen_o_stack) # tf.Tensor(), [5,]print(gen_o_concat) # tf.Tensor(), [5,1]print(gen_o.read(3)) # -0.22972003, tf.Tensor è¯»è¿‡ä¸€æ¬¡å°±è¢«æ¸…0äº†print(gen_o.write(3, tf.constant([0.22], dtype=tf.float32))) # TensorArray objectprint(gen_o.concat()) # tf.Tensor([-2.568663 0.09471891 1.2042408 0.22 0.2832177 ], shape=(5,), dtype=float32)print(gen_o.read(3)) # tf.Tensor([0.22], shape=(1,), dtype=float32)print(gen_o.read(3)) # Could not read index 3 twice because it was cleared after a previous read ç”¨ LSTM æ¨¡æ‹Ÿé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ current time t state: $(y_1,â€¦,y_t)$. ä½†æ˜¯é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹çš„åŸç†å‘Šè¯‰æˆ‘ä»¬ä¸€æ—¦å½“å‰çŠ¶æ€ç¡®å®šåï¼Œæ‰€æœ‰çš„å†å²ä¿¡æ¯éƒ½å¯ä»¥æ‰”æ‰äº†ã€‚è¿™ä¸ªçŠ¶æ€è¶³å¤Ÿå»é¢„æµ‹ future. æ‰€ä»¥åœ¨LSTMé‡Œé¢å°±æ˜¯éšè—çŠ¶æ€ $h_{t-1}$. ä»¥åŠå½“å‰å¯è§‚æµ‹ä¿¡æ¯ $x_t$. action a: é€‰æ‹© next token $y_t$. policy: $G_{\\theta}(y_t|Y_{1:t-1})$. ä¹Ÿå°±æ˜¯ç”Ÿæˆnext tokençš„ç­–ç•¥ã€‚ä¸‹é¢ä»£ç çš„æ–¹æ³• $o_t \\rightarrow log(softmax(o_t))$. ç„¶ååŸºäºè¿™ä¸ª log-prob çš„åˆ†å¸ƒè¿›è¡Œ sample. é—®é¢˜æ˜¯è¿™ä¸ªè¿‡ç¨‹ä¸å¯å¯¼å‘€ï¼Ÿ generatorè¿™æ˜¯ç”Ÿæˆå™¨ç”Ÿæˆsampleçš„è¿‡ç¨‹ï¼Œåˆå§‹çŠ¶æ€æ˜¯ $h_0$. g_recurrence å°±æ˜¯step-by-stepçš„è¿‡ç¨‹ï¼Œnext_tokenæ˜¯é€šè¿‡tf.multinomialé‡‡æ ·å¾—åˆ°çš„ï¼Œå…¶é‡‡æ ·çš„distributionæ˜¯ log_prob [tf.log(tf.nn.softmax(o_t))]ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131class Generator(tf.keras.Model): ... self.h0 = tf.zeros([self.batch_size, self.hidden_dim]) self.h0 = tf.stack([self.h0, self.h0]) # define variables self.g_embeddings = tf.Variable(self.init_matrix([self.vocab_size, self.emb_dim])) self.g_params.append(self.g_embeddings) self.g_recurrent_unit = self.create_recurrent_unit(self.g_params) # maps h_{t-1} to h_t for generator self.g_output_unit = self.create_output_unit(self.g_params) # maps h_t to o_t (output token logits) def _unsuper_generate(self): &quot;&quot;&quot; unsupervised generate. using in rollout policy. :return: ç”Ÿæˆå¾—åˆ°çš„ token index &quot;&quot;&quot; &quot;&quot;&quot; :param input_x: [batch, seq_len] :param rewards: [batch, seq_len] :return: &quot;&quot;&quot; gen_o = tf.TensorArray(dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True) gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length, dynamic_size=False, infer_shape=True) def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x): h_t = self.g_recurrent_unit(x_t, h_tm1) # hidden_memory_tuple o_t = self.g_output_unit(h_t) # [batch, vocab] , logits not prob log_prob = tf.log(tf.nn.softmax(o_t)) #tf.logging.info(&quot;unsupervised generated log_prob:{}&quot;.format(log_prob[0])) next_token = tf.cast(tf.reshape(tf.multinomial(logits=log_prob, num_samples=1), [self.batch_size]), tf.int32) x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token) # [batch, emb_dim] gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.vocab_size, 1.0, 0.0), tf.nn.softmax(o_t)), 1)) # [batch_size] , prob gen_x = gen_x.write(i, next_token) # indices, batch_size return i + 1, x_tp1, h_t, gen_o, gen_x _, _, _, def _super_generate(self, input_x): &quot;&quot;&quot; supervised generate. :param input_x: :return: ç”Ÿæˆå¾—åˆ°çš„æ˜¯ probability [batch * seq_len, vocab_size] &quot;&quot;&quot; with tf.device(&quot;/cpu:0&quot;): self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, input_x), perm=[1, 0, 2]) # [seq_len, batch_size, emb_dim] # supervised pretraining for generator g_predictions = tf.TensorArray( dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True) ta_emb_x = tf.TensorArray( dtype=tf.float32, size=self.sequence_length) ta_emb_x = ta_emb_x.unstack(self.processed_x) self.gen_o, self.gen_x = tf.while_loop( cond=lambda i, _1, _2, _3, _4: i &lt; self.sequence_length, body=_g_recurrence, loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x)) self.gen_x = self.gen_x.stack() # [seq_length, batch_size] self.gen_x = tf.transpose(self.gen_x, perm=[1, 0]) # [batch_size, seq_length] return self.gen_x æ‰€ä»¥æ˜¯é€šè¿‡monte carloçš„å½¢å¼ç”Ÿæˆfake sampleï¼Œä½œä¸ºdiscriminatorçš„è¾“å…¥å—ï¼Ÿé‚£è¿™ä¸ªè¿‡ç¨‹ä¹Ÿä¸å¯å¯¼å‘€ã€‚å…¶å®ä¸æ˜¯è¿™æ ·çš„ã€‚æˆ‘ä»¬å†çœ‹å¯¹æŠ—å­¦ä¹ ä¸­æ›´æ–°generatorçš„ä»£ç : 123456789101112131415161718192021222324252627282930313233343536373839def gen_reward_train_step(x_batch, rewards): with tf.GradientTape() as tape: g_loss = generator._get_generate_loss(x_batch, rewards) g_gradients, _ = tf.clip_by_global_norm( tape.gradient(g_loss, generator.trainable_variables), clip_norm=5.0) g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables)) return g_losstf.logging.info(&quot;------------------ 6. start Adversarial Training...--------------------------&quot;)for total_batch in range(TOTAL_BATCH): # fix discriminator, and train the generator for one step for it in range(1): samples = generator._unsuper_generate() #tf.logging.info(&quot;unsuper generated samples:{}&quot;.format(samples[0])) rewards = rollout.get_reward(samples, rollout_num=2, discriminator=discriminator) # åŸºäº monte carlo é‡‡æ ·16ï¼Œè®¡ç®—å¹¶ç´¯è®¡ reward. #tf.logging.info(&quot;reward:{}&quot;.format(rewards[0])) gen_reward_train_step(samples, rewards) # update generator. # Update roll-out parameters rollout.update_params() # update roll-out policy. è¿™å„¿é‡‡ç”¨çš„æ˜¯ generator._get_generate_lossï¼Œ æ‰€ä»¥å®ƒå¯¹generatorçš„å‚æ•°éƒ½æ˜¯å¯å¯¼çš„å—ï¼Ÿ æˆ‘ä»¬å†çœ‹è¿™ä¸ªç”Ÿæˆå™¨ä¸­è¿™ä¸ªfunctionçš„ä»£ç ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class Generator(tf.keras.Model): ... def _super_generate(self, input_x): &quot;&quot;&quot; supervised generate. :param input_x: :return: ç”Ÿæˆå¾—åˆ°çš„æ˜¯ probability [batch * seq_len, vocab_size] &quot;&quot;&quot; with tf.device(&quot;/cpu:0&quot;): self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, input_x), perm=[1, 0, 2]) # [seq_len, batch_size, emb_dim] # supervised pretraining for generator g_predictions = tf.TensorArray( dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True) ta_emb_x = tf.TensorArray( dtype=tf.float32, size=self.sequence_length) ta_emb_x = ta_emb_x.unstack(self.processed_x) def _pretrain_recurrence(i, x_t, h_tm1, g_predictions): h_t = self.g_recurrent_unit(x_t, h_tm1) o_t = self.g_output_unit(h_t) g_predictions = g_predictions.write(i, tf.nn.softmax(o_t)) # [batch, vocab_size] x_tp1 = ta_emb_x.read(i) # supervised learning, teaching forcing. return i + 1, x_tp1, h_t, g_predictions _, _, _, self.g_predictions = tf.while_loop( cond=lambda i, _1, _2, _3: i &lt; self.sequence_length, body=_pretrain_recurrence, loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, g_predictions)) self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1, 0, 2]) # [batch_size, seq_length, vocab_size] self.g_predictions = tf.clip_by_value( tf.reshape(self.g_predictions, [-1, self.vocab_size]), 1e-20, 1.0) # [batch_size*seq_length, vocab_size] return self.g_predictions # [batch_size*seq_length, vocab_size] def _get_generate_loss(self, input_x, rewards): &quot;&quot;&quot; :param input_x: [batch, seq_len] :param rewards: [batch, seq_len] :return: &quot;&quot;&quot; self.g_predictions = self._super_generate(input_x) real_target = tf.one_hot( tf.to_int32(tf.reshape(input_x, [-1])), depth=self.vocab_size, on_value=1.0, off_value=0.0) # [batch_size * seq_length, vocab_size] self.pretrain_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_target, logits=self.g_predictions) # [batch * seq_length] self.g_loss = tf.reduce_mean(self.pretrain_loss * tf.reshape(rewards, [-1])) # scalar return self.g_loss æ‰€ä»¥seqgançš„ä½œè€…æ˜¯æ€ä¹ˆåšçš„å‘¢ï¼Œåˆ©ç”¨ generator._unsuper_generateå…ˆç”Ÿæˆfake sampleï¼Œç„¶åå†åˆ©ç”¨ generator._super_generate å¾—åˆ° g_predictions, å°†fake sampleä½œä¸º real_target ä¸ g_predictions åšäº¤å‰ç†µæ±‚å‡º pretrain_lossï¼Œç„¶åä¹˜ä»¥æ¯ä¸€ä¸ªtokenå¯¹åº”çš„rewardså¾—åˆ°æœ€ç»ˆçš„loss. è¿™ä¸ªè¿‡ç¨‹æ˜¯å¯å¯¼çš„ã€‚ é€šå¸¸æƒ…å†µä¸‹Monte carloæ–¹æ³•åœ¨é‡Œé¢çš„ä½œç”¨å…¶å®å°±æ˜¯ collect data. collecting dataçš„è¿‡ç¨‹ç”¨åˆ°äº†policy,ç„¶ååŸºäºrewardå¯¹policyè¿›è¡Œæ±‚å¯¼ã€‚ ä½†æ˜¯seqgançš„ä½œè€…åœ¨ä»£ç ä¸­å‘ˆç°çš„æ˜¯å¦ä¸€ç§trick. å…ˆç”¨generatorç”Ÿæˆfakeæ ·æœ¬ï¼Œç„¶åç”¨rollout policyå¯¹è¯¥æ ·æœ¬è¿›è¡Œæ‰“åˆ†reward.è¿™é‡Œå¹¶ä¸æ˜¯ç›´æ¥å¯¹rewardæ±‚å¯¼ï¼Œè€Œæ˜¯æŠŠfakeæ ·æœ¬ä½œä¸ºtargetè¿›è¡ŒMLEè®­ç»ƒï¼Œå¾—åˆ°pretrain_lossï¼Œrewardä½œä¸ºæƒé‡ä¹˜ä»¥pretrain_lossä½œä¸ºæœ€ç»ˆçš„æŸå¤±å‡½æ•°ã€‚ roll-policyè¿™ä¸ªè¿‡ç¨‹æ¯”è¾ƒå®¹æ˜“ç†è§£ï¼Œå¯¹äºç»™å®šçš„ given_num,å°äº given_num çš„ç›´æ¥ copyï¼Œä½†æ˜¯ $h_t$ çš„è®¡ç®—ä¾æ—§ã€‚å¤§äº given_num çš„tokené‡‡ç”¨ generate._unsuper_generate. ç–‘é—®çœ‹äº†ä»£ç æ€»è§‰å¾—ä»£ç å†™å¾—ä¸è®ºæ–‡æœ‰å‡ºå…¥ã€‚ åŸºäºpolicy gradientæ¥æ›´æ–°policy(generator)ï¼ŒæŒ‰ç…§å…¬å¼åº”è¯¥æ˜¯ç›´æ¥å¯¹rewardsæ±‚å¯¼æ‰å¯¹å§ã€‚åŸºäºMonte carloé‡‡æ ·çš„è¿‡ç¨‹å¯ä»¥çœ‹ä½œæ˜¯sampleä¸åŒçš„æ ·æœ¬ï¼Œæ˜¯ä¸€ç§è¿‘ä¼¼æ¨¡æ‹Ÿ $o_t$ åˆ†å¸ƒçš„æ–¹æ³•ï¼Œæ˜¯ä¸è¦æ±‚å¯å¯¼çš„ã€‚","link":"/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/"},{"title":"Tensorflow RNN API æºç é˜…è¯»","text":"åœ¨ä¸‰æ˜Ÿç ”ç©¶é™¢å®ä¹ ä¸€æ®µæ—¶é—´å‘ç°åœ¨å…¬å¸å†™ä»£ç å’Œåœ¨å­¦æ ¡è¿˜æ˜¯æœ‰å·®åˆ«çš„ã€‚ä¸€æ˜¯åœ¨å…¬å¸è¦è¿½æ±‚æ•ˆç‡ï¼Œä¼šä½¿ç”¨å¾ˆå¤šå®˜æ–¹å°è£…å¥½çš„apiï¼Œè€Œåœ¨å­¦æ ¡çš„æ—¶å€™å› ä¸ºè¦å»ç†è§£å†…éƒ¨åŸç†ï¼Œæ›´å¤šçš„æ˜¯åœ¨é€ è½®å­ï¼Œå¯¼è‡´å¯¹å¾ˆå¤š api ä¸æ˜¯å¾ˆç†Ÿæ‚‰ã€‚ä½†å®é™…ä¸Šå®˜æ–¹apiä¸ä»…åœ¨é€Ÿåº¦ï¼Œä»¥åŠå…¨é¢æ€§ä¸Šéƒ½æ¯”è‡ªå·±å†™çš„è¿˜æ˜¯å¥½å¾ˆå¤šçš„ã€‚äºŒæ˜¯ï¼Œåœ¨å…¬å¸å¯¹ä»£ç çš„å¤ç”¨ç‡è¦æ±‚æ¯”è¾ƒé«˜ï¼Œæ¨¡å‹è·‘åˆ°å“ªä¸€ä¸ªç‰ˆæœ¬äº†ï¼Œå¯¹åº”çš„å‚æ•°éƒ½è¦ç•™ä¸‹æ¥ï¼Œéšæ—¶å¯ä»¥è·‘èµ·æ¥ï¼Œè€Œä¸æ˜¯é‡æ–°è®­ç»ƒï¼Œè¿™å¯¹æ¨¡å‹ã€å‚æ•°çš„ä¿å­˜è¦æ±‚å¾ˆé‡è¦ã€‚ä»¥åŠåœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡éƒ½è¦åœ¨ä»£ç ä¸Šå¾ˆå®Œæ•´ï¼Œè€Œä¸æ˜¯ä»…ä»…çœ‹çœ‹ loss å’Œ accuracy å°±å¯ä»¥çš„ã€‚ è¿™èŠ‚å†…å®¹ä¸»è¦æ˜¯è¯¦ç»†è¿‡ä¸€é tensorflow é‡Œé¢çš„ rnn apiï¼Œæ ¹æ®RNN and Cells (contrib)è¿™é‡Œçš„é¡ºåºé€æ­¥æ·±å…¥ç ”ç©¶ å…ˆå›é¡¾ä¸€ä¸‹ RNN/LSTM/GRU:å‚è€ƒä¹‹å‰ cs224d çš„ç¬”è®° cs224d-lecture9 æœºå™¨ç¿»è¯‘ cs224d-lecture8-RNN å‘ç°æœ‰äº›å°é”™è¯¯ï¼Œä½†ä¸å½±å“è‡ªå·±å¤ä¹ ã€‚ã€‚ basic rnnï¼š $$h_t = \\sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$ å…ˆçœ‹ tf.contrib.rnn.RNNCellhttps://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/ops/rnn_cell_impl.py#L170) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251@tf_export(&quot;nn.rnn_cell.RNNCell&quot;)class RNNCell(base_layer.Layer): &quot;&quot;&quot;Abstract object representing an RNN cell. Every `RNNCell` must have the properties below and implement `call` with the signature `(output, next_state) = call(input, state)`. RNNCell æ˜¯ä¸€ä¸ªæŠ½è±¡çš„çˆ¶ç±»ï¼Œä¹‹åæ›´å¤æ‚çš„ RNN/LSTM/GRU éƒ½æ˜¯é‡æ–°å®ç° call å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯æ›´æ–°éšè—çŠ¶æ€ çš„æ–¹å¼æ”¹å˜äº†ã€‚ The optional third input argument, `scope`, is allowed for backwards compatibility purposes; but should be left off for new subclasses. scope è¿™ä¸ªå‚æ•°ç®¡ç†å˜é‡ï¼Œåœ¨åå‘ä¼ æ’­ä¸­å˜é‡æ˜¯å¦å¯è®­ç»ƒã€‚ This definition of cell differs from the definition used in the literature. In the literature, 'cell' refers to an object with a single scalar output. This definition refers to a horizontal array of such units. è¿™é‡Œçš„ cell çš„æ¦‚å¿µå’Œä¸€äº›è®ºæ–‡ä¸­æ˜¯ä¸ä¸€æ ·çš„ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œcell è¡¨ç¤ºä¸€ä¸ªç¥ç»å…ƒï¼Œä¹Ÿå°±æ˜¯å•ä¸ªå€¼ã€‚è€Œè¿™é‡Œè¡¨ç¤ºçš„æ˜¯ ä¸€ç»„ç¥ç»å…ƒï¼Œæ¯”å¦‚éšè—çŠ¶æ€[batch, num_units]. An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs. This operation results in an output matrix with `self.output_size` columns. If `self.state_size` is an integer, this operation also results in a new state matrix with `self.state_size` columns. If `self.state_size` is a (possibly nested tuple of) TensorShape object(s), then it should return a matching structure of Tensors having shape `[batch_size].concatenate(s)` for each `s` in `self.batch_size`. rnn cell çš„è¾“å…¥æ˜¯ä¸€ä¸ªçŠ¶æ€ state å’Œ input çŸ©é˜µï¼Œå‚æ•°æœ‰ self.output_size å’Œ self.state_size. åˆ†åˆ«è¡¨ç¤ºè¾“å‡ºå±‚å’Œéšè—å±‚çš„ç»´åº¦ã€‚å…¶ä¸­ state_size å¯èƒ½æ˜¯ tupleï¼Œè¿™ä¸ªä¹‹ååœ¨çœ‹ã€‚ &quot;&quot;&quot; def __call__(self, inputs, state, scope=None): &quot;&quot;&quot;Run this RNN cell on inputs, starting from the given state. Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: if `self.state_size` is an integer, this should be a `2-D Tensor` with shape `[batch_size, self.state_size]`. Otherwise, if `self.state_size` is a tuple of integers, this should be a tuple with shapes `[batch_size, s] for s in self.state_size`. scope: VariableScope for the created subgraph; defaults to class name. Returns: A pair containing: - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`. - New state: Either a single `2-D` tensor, or a tuple of tensors matching the arity and shapes of `state`. &quot;&quot;&quot; if scope is not None: with vs.variable_scope(scope, custom_getter=self._rnn_get_variable) as scope: return super(RNNCell, self).__call__(inputs, state, scope=scope) else: scope_attrname = &quot;rnncell_scope&quot; scope = getattr(self, scope_attrname, None) if scope is None: scope = vs.variable_scope(vs.get_variable_scope(), custom_getter=self._rnn_get_variable) setattr(self, scope_attrname, scope) with scope: return super(RNNCell, self).__call__(inputs, state) def _rnn_get_variable(self, getter, *args, **kwargs): variable = getter(*args, **kwargs) if context.executing_eagerly(): trainable = variable._trainable # pylint: disable=protected-access else: trainable = ( variable in tf_variables.trainable_variables() or (isinstance(variable, tf_variables.PartitionedVariable) and list(variable)[0] in tf_variables.trainable_variables())) if trainable and variable not in self._trainable_weights: self._trainable_weights.append(variable) elif not trainable and variable not in self._non_trainable_weights: self._non_trainable_weights.append(variable) return variable @property def state_size(self): &quot;&quot;&quot;size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. &quot;&quot;&quot; raise NotImplementedError(&quot;Abstract method&quot;) @property def output_size(self): &quot;&quot;&quot;Integer or TensorShape: size of outputs produced by this cell.&quot;&quot;&quot; raise NotImplementedError(&quot;Abstract method&quot;) def build(self, _): # This tells the parent Layer object that it's OK to call # self.add_variable() inside the call() method. pass def zero_state(self, batch_size, dtype): &quot;&quot;&quot;Return zero-filled state tensor(s). Args: batch_size: int, float, or unit Tensor representing the batch size. dtype: the data type to use for the state. Returns: If `state_size` is an int or TensorShape, then the return value is a `N-D` tensor of shape `[batch_size, state_size]` filled with zeros. If `state_size` is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of `2-D` tensors with the shapes `[batch_size, s]` for each s in `state_size`. &quot;&quot;&quot; # Try to use the last cached zero_state. This is done to avoid recreating # zeros, especially when eager execution is enabled. state_size = self.state_size is_eager = context.executing_eagerly() if is_eager and hasattr(self, &quot;_last_zero_state&quot;): (last_state_size, last_batch_size, last_dtype, last_output) = getattr(self, &quot;_last_zero_state&quot;) if (last_batch_size == batch_size and last_dtype == dtype and last_state_size == state_size): return last_output with ops.name_scope(type(self).__name__ + &quot;ZeroState&quot;, values=[batch_size]): output = _zero_state_tensors(state_size, batch_size, dtype) if is_eager: self._last_zero_state = (state_size, batch_size, dtype, output) return output ä¸¤ä¸ªå±æ€§ output_size, state_size åˆ†åˆ«è¡¨ç¤ºè¾“å‡ºå±‚çš„ç»´åº¦å’Œéšè—å±‚çš„ç»´åº¦ã€‚call å‡½æ•°ç”¨æ¥è¡¨ç¤ºè®¡ç®—ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€å’Œè¾“å‡ºï¼Œzero_state å‡½æ•°ç”¨æ¥åˆå§‹åŒ–åˆå§‹çŠ¶æ€å…¨ä¸º 0, è¿™é‡Œ state_size æœ‰ä¸¤ç§æƒ…å†µï¼Œä¸€ç§æ˜¯ int æˆ– tensorshape,é‚£ä¹ˆ [batch, state_size]. å¦‚æœæ˜¯å¤šå±‚åµŒå¥— rnn, é‚£ä¹ˆåˆå§‹çŠ¶æ€ [batch, s] for s in state_size 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class LayerRNNCell(RNNCell): &quot;&quot;&quot;Subclass of RNNCells that act like proper `tf.Layer` objects. def __call__(self, inputs, state, scope=None, *args, **kwargs): &quot;&quot;&quot;Run this RNN cell on inputs, starting from the given state. Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: if `self.state_size` is an integer, this should be a `2-D Tensor` with shape `[batch_size, self.state_size]`. Otherwise, if `self.state_size` is a tuple of integers, this should be a tuple with shapes `[batch_size, s] for s in self.state_size`. scope: optional cell scope. *args: Additional positional arguments. **kwargs: Additional keyword arguments. Returns: A pair containing: - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`. - New state: Either a single `2-D` tensor, or a tuple of tensors matching the arity and shapes of `state`. &quot;&quot;&quot; # Bypass RNNCell's variable capturing semantics for LayerRNNCell. # Instead, it is up to subclasses to provide a proper build # method. See the class docstring for more details. return base_layer.Layer.__call__(self, inputs, state, scope=scope, *args, **kwargs) å†çœ‹ Core RNN Cells tf.contrib.rnn.BasicRNNCell tf.contrib.rnn.BasicLSTMCell tf.contrib.rnn.GRUCell tf.contrib.rnn.LSTMCell tf.contrib.rnn.LayerNormBasicLSTMCell tf.contrib.rnn.BasicRNNCellç›´æ¥æ‰’æºç ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129@tf_export(&quot;nn.rnn_cell.BasicRNNCell&quot;)class BasicRNNCell(LayerRNNCell): &quot;&quot;&quot;The most basic RNN cell. Args: num_units: int, The number of units in the RNN cell. activation: Nonlinearity to use. Default: `tanh`. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. name: String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. dtype: Default dtype of the layer (default of `None` means use the type of the first input). Required when `build` is called before `call`. &quot;&quot;&quot; def __init__(self, num_units, activation=None, reuse=None, name=None, dtype=None): super(BasicRNNCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) # Inputs must be 2-dimensional. self.input_spec = base_layer.InputSpec(ndim=2) self._num_units = num_units self._activation = activation or math_ops.tanh @property def state_size(self): return self._num_units @property def output_size(self): return self._num_units def build(self, inputs_shape): if inputs_shape[1].value is None: raise ValueError(&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot; % inputs_shape) input_depth = inputs_shape[1].value self._kernel = self.add_variable( _WEIGHTS_VARIABLE_NAME, shape=[input_depth + self._num_units, self._num_units]) self._bias = self.add_variable( _BIAS_VARIABLE_NAME, shape=[self._num_units], initializer=init_ops.zeros_initializer(dtype=self.dtype)) self.built = True def call(self, inputs, state): &quot;&quot;&quot;Most basic RNN: output = new_state = act(W * input + U * state + B).&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) output = self._activation(gate_inputs) return output, output å¯ä»¥å‘ç° state_size = output_size = num_units, ä»¥åŠè¾“å‡ºå°±æ˜¯ä¸‹ä¸€ä¸ªéšè—çŠ¶æ€ output = new_state = act(W * input + U * state + B) = act(W[input, state] + b) å…¶ä¸­ self._kernel è¡¨ç¤º W, å…¶ç»´åº¦æ˜¯ [input_depth + num_units, num_units] 1234567891011import tensorflow as tfimport tensorflow.contrib.eager as tfetf.enable_eager_execution()print(tfe.executing_eagerly()) True 12345cell = tf.contrib.rnn.BasicRNNCell(num_units=128, activation=None)print(cell.state_size, cell.output_size) 128 128 123456789inputs = tf.random_normal(shape=[32, 100], dtype=tf.float32)h0 = cell.zero_state(batch_size=32, dtype=tf.float32)output, state = cell(inputs=inputs, state=h0)print(output.shape, state.shape) (32, 128) (32, 128) tf.contrib.rnn.BasicLSTMCellå…ˆå›é¡¾ä¸‹ LSTM: è‡ªå·±è¯•ç€æ‰‹æ•²å…¬å¼ï½ çœ‹ç€å›¾è¿˜æ˜¯ç®€å•ï¼Œä¸çœ‹å›¾æ˜¯å¦ä¹Ÿå¯ä»¥å‘¢ï¼Ÿ ä¸‰ä¸ªgateï¼šé—å¿˜é—¨ï¼Œè¾“å…¥/æ›´æ–°é—¨ï¼Œè¾“å‡ºé—¨ $$f_t=\\sigma(W^{f}x_t + U^{f}h_{t-1})$$ $$i_t=\\sigma(W^{i}x_t + U^{i}h_{t-1})$$ $$o_t=\\sigma(W^{o}x_t + U^{o}h_{t-1})$$ new memory cell: $$\\hat c=tanh(W^cx_t + U^ch_{t-1})$$ è¾“å…¥é—¨ä½œç”¨äºæ–°çš„è®°å¿†ç»†èƒ,é—å¿˜é—¨ä½œç”¨äºä¸Šä¸€ä¸ªè®°å¿†ç»†èƒï¼Œå¹¶å¾—åˆ°æœ€ç»ˆçš„è®°å¿†ç»†èƒ: $$c_t=f_t\\circ c_{t-1} + i_t\\circ\\hat c$$ ç”¨æ–°çš„memory cell å’Œè¾“å‡ºé—¨å¾—åˆ°æ–°çš„éšè—çŠ¶æ€ï¼š $$h_t = tanh(o_t\\circ c_t)$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245class BasicLSTMCell(LayerRNNCell): &quot;&quot;&quot;Basic LSTM recurrent network cell. The implementation is based on: http://arxiv.org/abs/1409.2329. We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training. It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline. For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell} that follows. &quot;&quot;&quot; def __init__(self, num_units, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None, dtype=None): &quot;&quot;&quot;Initialize the basic LSTM cell. Args: num_units: int, The number of units in the LSTM cell. forget_bias: float, The bias added to forget gates (see above). Must set to `0.0` manually when restoring from CudnnLSTM-trained checkpoints. state_is_tuple: If True, accepted and returned states are 2-tuples of the `c_state` and `m_state`. If False, they are concatenated along the column axis. The latter behavior will soon be deprecated. activation: Activation function of the inner states. Default: `tanh`. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. name: String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. dtype: Default dtype of the layer (default of `None` means use the type of the first input). Required when `build` is called before `call`. When restoring from CudnnLSTM-trained checkpoints, must use `CudnnCompatibleLSTMCell` instead. &quot;&quot;&quot; super(BasicLSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) if not state_is_tuple: logging.warn(&quot;%s: Using a concatenated state is slower and will soon be &quot; &quot;deprecated. Use state_is_tuple=True.&quot;, self) # Inputs must be 2-dimensional. self.input_spec = base_layer.InputSpec(ndim=2) self._num_units = num_units self._forget_bias = forget_bias self._state_is_tuple = state_is_tuple self._activation = activation or math_ops.tanh @property def state_size(self): return (LSTMStateTuple(self._num_units, self._num_units) if self._state_is_tuple else 2 * self._num_units) @property def output_size(self): return self._num_units def build(self, inputs_shape): if inputs_shape[1].value is None: raise ValueError(&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot; % inputs_shape) input_depth = inputs_shape[1].value h_depth = self._num_units self._kernel = self.add_variable( _WEIGHTS_VARIABLE_NAME, shape=[input_depth + h_depth, 4 * self._num_units]) self._bias = self.add_variable( _BIAS_VARIABLE_NAME, shape=[4 * self._num_units], initializer=init_ops.zeros_initializer(dtype=self.dtype)) self.built = True def call(self, inputs, state): &quot;&quot;&quot;Long short-term memory cell (LSTM). Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: An `LSTMStateTuple` of state tensors, each shaped `[batch_size, num_units]`, if `state_is_tuple` has been set to `True`. Otherwise, a `Tensor` shaped `[batch_size, 2 * num_units]`. Returns: A pair containing the new hidden state, and the new state (either a `LSTMStateTuple` or a concatenated state, depending on `state_is_tuple`). &quot;&quot;&quot; sigmoid = math_ops.sigmoid one = constant_op.constant(1, dtype=dtypes.int32) # Parameters of gates are concatenated into one multiply for efficiency. if self._state_is_tuple: c, h = state else: c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one) gate_inputs = math_ops.matmul( array_ops.concat([inputs, h], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) # i = input_gate, j = new_input, f = forget_gate, o = output_gate i, j, f, o = array_ops.split( value=gate_inputs, num_or_size_splits=4, axis=one) forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype) # Note that using `add` and `multiply` instead of `+` and `*` gives a # performance improvement. So using those at the cost of readability. add = math_ops.add multiply = math_ops.multiply new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))), multiply(sigmoid(i), self._activation(j))) new_h = multiply(self._activation(new_c), sigmoid(o)) if self._state_is_tuple: new_state = LSTMStateTuple(new_c, new_h) else: new_state = array_ops.concat([new_c, new_h], 1) return new_h, new_state é˜…è¯»æºç å¯ä»¥å‘ç°å…·ä½“å®ç°ä¸ä¸Šé¢çš„å…¬å¼è¿˜æ˜¯æœ‰ç‚¹å·®åˆ«çš„ã€‚ å…ˆ concat[input, h], ç„¶å gate_input = matmul(concat[input, h], self._kernel)+self._bias,å¤šäº†åç½®é¡¹,è¿™é‡Œçš„çŸ©é˜µç»´åº¦ [input_depth + h_depth,4*num_units]. ç„¶å i,j,f,o = split(gate_input, 4, axis=1). å…¶ä¸­ j è¡¨ç¤º new memory cell. ç„¶åè®¡ç®— new_cï¼Œå…¶ä¸­ i,f,o å¯¹åº”çš„æ¿€æ´»å‡½æ•°ç¡®å®šæ˜¯ sigmoid,å› ä¸ºå…¶èŒƒå›´åªèƒ½åœ¨(0,1)ä¹‹é—´ã€‚ä½†æ˜¯ j çš„æ¿€æ´»å‡½æ•°self._activation å¯ä»¥é€‰æ‹©ï¼Œé»˜è®¤æ˜¯ tanh. ä¸å…¬å¼çš„å·®åˆ«ä¹‹äºŒåœ¨äº self._forget_bias.é—å¿˜é—¨åœ¨æ¿€æ´»å‡½æ•° $\\sigma$ ä¹‹å‰åŠ äº†åç½®ï¼Œç›®çš„æ˜¯é¿å…åœ¨è®­ç»ƒåˆæœŸä¸¢å¤±å¤ªå¤šä¿¡æ¯ã€‚ è¦æ³¨æ„ state çš„å½¢å¼ï¼Œå–å†³äºå‚æ•° self._state_is_tuple. å…¶ä¸­ c,h=stateï¼Œè¡¨ç¤º $c_{t-1},h_{t-1}$ 123lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=128, forget_bias=1.0, state_is_tuple=True) WARNING:tensorflow:From &lt;ipython-input-9-3f4ca183c5d7&gt;:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version. Instructions for updating: This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell'). æç¤ºè¦æ›´æ–°äº†ï¼Œé‚£å°±æ¢æˆæœ€æ–°çš„å§ 123lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=128, forget_bias=1.0, state_is_tuple=True) 123lstm_cell.output_size, lstm_cell.state_size (128, LSTMStateTuple(c=128, h=128)) 123h0 = lstm_cell.zero_state(batch_size=30, dtype=tf.float32) æˆ‘ä»¬å‘ç° lstm çš„çŠ¶æ€ state æ˜¯ä¸€ä¸ªtuple,åˆ†åˆ«å¯¹åº” c_t å’Œ h_t. 123456789class LSTMStateTuple(_LSTMStateTuple): &quot;&quot;&quot;Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state. Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state and `h` is the output. è¿™é‡Œçš„è§£é‡Šæ„Ÿè§‰æ˜¯æœ‰ç‚¹é—®é¢˜çš„ï¼Œc is the hidden state and h is the output. çœ‹æºç  12345678910111213141516171819new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))), multiply(sigmoid(i), self._activation(j)))new_h = multiply(self._activation(new_c), sigmoid(o))if self._state_is_tuple: new_state = LSTMStateTuple(new_c, new_h)else: new_state = array_ops.concat([new_c, new_h], 1)return new_h, new_state å‘ç° c è¡¨ç¤ºçš„å°±æ˜¯ new memory cell, è€Œ h è¡¨ç¤ºçš„æ˜¯æœ€åçš„éšè—çŠ¶æ€ã€‚ 1234567# è®¡ç®—ä¸‹ä¸€æ­¥çš„ output å’Œ stateinputs = tf.random_normal(shape=[30, 100], dtype=tf.float32)output, state = lstm_cell(inputs, h0) 123output.shape, state[0].shape, state[1].shape (TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(128)])) 123state.c, state.h # c å’Œ h çš„å€¼æ˜¯ä¸ä¸€æ ·çš„ (&lt;tf.Tensor: id=108, shape=(30, 128), dtype=float32, numpy= array([[ 0.08166471, 0.14020835, 0.07970127, ..., -0.1540019 , 0.38848224, -0.0842322 ], [-0.03643086, -0.20558938, 0.1503458 , ..., 0.01846285, 0.15610473, 0.04408235], [-0.0933667 , 0.03454542, -0.09073547, ..., -0.12701994, -0.34669587, 0.09373946], ..., [-0.00752909, 0.22412673, -0.270195 , ..., 0.09341058, -0.20986181, -0.18622127], [ 0.18778914, 0.37687936, -0.24727295, ..., -0.06409463, 0.00218048, 0.5940756 ], [ 0.04073388, -0.08431841, 0.35944715, ..., 0.14135318, 0.08472287, -0.11058106]], dtype=float32)&gt;, &lt;tf.Tensor: id=111, shape=(30, 128), dtype=float32, numpy= array([[ 0.04490132, 0.07412361, 0.03662094, ..., -0.07611651, 0.17290959, -0.0277745 ], [-0.02212535, -0.13554382, 0.08272093, ..., 0.00918258, 0.0861209 , 0.02614526], [-0.05723168, 0.01372226, -0.02919216, ..., -0.06374882, -0.1918035 , 0.03912015], ..., [-0.00377504, 0.15181372, -0.14555399, ..., 0.06073361, -0.09804281, -0.07492835], [ 0.10244624, 0.17440473, -0.09896267, ..., -0.03794969, 0.00123257, 0.21985768], [ 0.01832823, -0.03795732, 0.1654894 , ..., 0.05827027, 0.02769112, -0.05957894]], dtype=float32)&gt;) tf.nn.rnn_cell.GRUCellå…ˆå›é¡¾ä¸‹ GRU. æ‰‹æ•² GRU å…¬å¼ï¼š $$r_t=\\sigma(W^rx_t + U^rh_{t-1})$$ $$z_t=\\sigma(W^zx_t + U^zh_{t-1})$$ $$\\tilde h_t = tanh(Wx_t + r_t\\circ h_{t-1})$$ $$h_t=(1-z_t)\\circ\\tilde h_t + z_t\\circ h_{t-1}$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197@tf_export(&quot;nn.rnn_cell.GRUCell&quot;)class GRUCell(LayerRNNCell): &quot;&quot;&quot;Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078). Args: num_units: int, The number of units in the GRU cell. activation: Nonlinearity to use. Default: `tanh`. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. kernel_initializer: (optional) The initializer to use for the weight and projection matrices. bias_initializer: (optional) The initializer to use for the bias. name: String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. dtype: Default dtype of the layer (default of `None` means use the type of the first input). Required when `build` is called before `call`. &quot;&quot;&quot; def __init__(self, num_units, activation=None, reuse=None, kernel_initializer=None, bias_initializer=None, name=None, dtype=None): super(GRUCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) # Inputs must be 2-dimensional. self.input_spec = base_layer.InputSpec(ndim=2) self._num_units = num_units self._activation = activation or math_ops.tanh self._kernel_initializer = kernel_initializer self._bias_initializer = bias_initializer @property def state_size(self): return self._num_units @property def output_size(self): return self._num_units def build(self, inputs_shape): if inputs_shape[1].value is None: raise ValueError(&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot; % inputs_shape) input_depth = inputs_shape[1].value self._gate_kernel = self.add_variable( &quot;gates/%s&quot; % _WEIGHTS_VARIABLE_NAME, shape=[input_depth + self._num_units, 2 * self._num_units], initializer=self._kernel_initializer) self._gate_bias = self.add_variable( &quot;gates/%s&quot; % _BIAS_VARIABLE_NAME, shape=[2 * self._num_units], initializer=( self._bias_initializer if self._bias_initializer is not None else init_ops.constant_initializer(1.0, dtype=self.dtype))) self._candidate_kernel = self.add_variable( &quot;candidate/%s&quot; % _WEIGHTS_VARIABLE_NAME, shape=[input_depth + self._num_units, self._num_units], initializer=self._kernel_initializer) self._candidate_bias = self.add_variable( &quot;candidate/%s&quot; % _BIAS_VARIABLE_NAME, shape=[self._num_units], initializer=( self._bias_initializer if self._bias_initializer is not None else init_ops.zeros_initializer(dtype=self.dtype))) self.built = True def call(self, inputs, state): &quot;&quot;&quot;Gated recurrent unit (GRU) with nunits cells.&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._gate_kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias) value = math_ops.sigmoid(gate_inputs) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state candidate = math_ops.matmul( array_ops.concat([inputs, r_state], 1), self._candidate_kernel) candidate = nn_ops.bias_add(candidate, self._candidate_bias) c = self._activation(candidate) new_h = u * state + (1 - u) * c return new_h, new_h_LSTMStateTuple = collections.namedtuple(&quot;LSTMStateTuple&quot;, (&quot;c&quot;, &quot;h&quot;)) ä»”ç»†é˜…è¯»æºç å‘ç°åœ¨ $sigma$ è®¡ç®— gateï¼Œä»¥åŠ tanh è®¡ç®— candidate ä¹‹å‰éƒ½æœ‰åç½®é¡¹ï¼Œä¸è¿‡å…¬å¼ä¸­éƒ½æ²¡å†™å‡ºæ¥ã€‚è€Œä¸”åœ¨ä¸è®¾ç½® bias çš„åˆå§‹å€¼æ—¶ï¼Œé»˜è®¤çš„ GRU ä¸­ gate_bias çš„åˆå§‹å€¼æ˜¯ 1, è€Œ LSTM ä¸­ gate_bias çš„åˆå§‹å€¼æ˜¯ 0. 123gru_cell = tf.nn.rnn_cell.GRUCell(num_units=128) 123gru_cell.state_size, gru_cell.output_size (128, 128) 123h0 = gru_cell.zero_state(batch_size=30, dtype=tf.float32) 1234567inputs = tf.random_normal(shape=[30, 100], dtype=tf.float32)output, state = gru_cell(inputs, h0)output.shape, state.shape (TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(128)])) å‡ºç°ä¸ªå¾ˆç¥å¥‡çš„ç°è±¡ï¼Œå¦‚æœæˆ‘å†™æˆè¿™æ ·ï¼š 12345output, state = gru_cell.call(inputs, h0) # ä¼šæŠ¥é”™çš„ï¼Œgru_cell æ²¡æœ‰ self._gate_kernel è¿™ä¸ªå±æ€§ï¼Œå¾ˆç¥å¥‡ï¼Œ # ä¸è¿‡è¿™é‡Œå…ˆè¿è¡Œä¸Šé¢é‚£è¡Œä»£ç ï¼Œæ‰€ä»¥æ²¡æœ‰å‡ºç°æŠ¥é”™ tf.nn.rnn_cell.LSTMCell, tf.contrib.rnn.LSTMCell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@tf_export(&quot;nn.rnn_cell.LSTMCell&quot;)class LSTMCell(LayerRNNCell): &quot;&quot;&quot;Long short-term memory unit (LSTM) recurrent network cell. The default non-peephole implementation is based on: http://www.bioinf.jku.at/publications/older/2604.pdf S. Hochreiter and J. Schmidhuber. &quot;Long Short-Term Memory&quot;. Neural Computation, 9(8):1735-1780, 1997. The peephole implementation is based on: https://research.google.com/pubs/archive/43905.pdf Hasim Sak, Andrew Senior, and Francoise Beaufays. &quot;Long short-term memory recurrent neural network architectures for large scale acoustic modeling.&quot; INTERSPEECH, 2014. The class uses optional peep-hole connections, optional cell clipping, and an optional projection layer. &quot;&quot;&quot; def __init__(self, num_units, use_peepholes=False, cell_clip=None, initializer=None, num_proj=None, proj_clip=None, num_unit_shards=None, num_proj_shards=None, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None, dtype=None): &quot;&quot;&quot;Initialize the parameters for an LSTM cell. &quot;&quot;&quot; super(LSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) ç›¸æ¯” BasicLSTMCell å¤šäº†è¿™å››ä¸ªå‚æ•°ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071Args: use_peepholes: bool, set True to enable diagonal/peephole connections. cell_clip: (optional) A float value, if provided the cell state is clipped by this value prior to the cell output activation. num_proj: (optional) int, The output dimensionality for the projection matrices. If None, no projection is performed. proj_clip: (optional) A float value. If `num_proj &gt; 0` and `proj_clip` is provided, then the projected values are clipped elementwise to within `[-proj_clip, proj_clip]`.````å…¶ä¸­ cell_clip å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯é™åˆ¶éšè—çŠ¶æ€çš„å¤§å°ï¼Œä¹Ÿå°±æ˜¯ output å’Œ state çš„å¤§å°ã€‚ è€Œ num_proj å‘¢ï¼Ÿ```python if num_proj: self._state_size = ( LSTMStateTuple(num_units, num_proj) if state_is_tuple else num_units + num_proj) self._output_size = num_proj else: self._state_size = ( LSTMStateTuple(num_units, num_units) if state_is_tuple else 2 * num_units) self._output_size = num_units ````é€šè¿‡æºç å¯ä»¥å‘ç°ï¼Œå¦‚æœæœ‰ num_proj é‚£ä¹ˆ state è¿˜è¦åŠ ä¸€ä¸ªå…¨é“¾æ¥ï¼Œ state_size = num_units + num_proj. è€Œ proj_clip æ˜¯é™åˆ¶è¿™ä¸ªå…¨é“¾æ¥çš„è¾“å‡ºçš„ã€‚BacisLSTMCell å’Œ LSTMCell åŒºåˆ«è¿˜åœ¨äºåè€…å¢åŠ äº† peephole å’Œ cell_clip![](https://img-blog.csdn.net/20171201095120010?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWJjbGhxMjAwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)è¾“å…¥è¾“å‡ºçš„shapeï¼Œä»¥åŠçŠ¶æ€éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªä¸è¿‡å†…éƒ¨è®¡ç®—æ–¹å¼æ›´åŠ å¤æ‚äº†ã€‚å¯¹äº peephole æ˜¯å€¼å¾—è®¡ç®— gate æ—¶ï¼Œè€ƒè™‘åˆ°äº† $c_{t-1}$ å’Œ $c_t$.```pythoncell = tf.nn.rnn_cell.LSTMCell(num_units=64,cell_clip=0.000000001, num_proj=128, proj_clip=0.001) 123cell.state_size, cell.output_size (LSTMStateTuple(c=64, h=128), 128) å‘ç° state_size ä¸­çš„ h ç»´åº¦å‘ç”Ÿäº†å˜åŒ–ï¼Œç›¸å½“äºåœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥å¾—åˆ°çš„ state.h ä¹‹åå†æ·»åŠ ä¸€ä¸ªå…¨é“¾æ¥ã€‚ åœ¨ decoder ä¸­å¯ä»¥å°† num_proj è®¾ç½®ä¸ºè¯è¡¨çš„å¤§å°ï¼Œé‚£ä¹ˆè¾“å‡ºå°±æ˜¯å¯¹åº”æ—¶é—´æ­¥çš„è¯è¡¨çš„åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šåœ¨ softmax å°±å¯ä»¥å§ï¼Ÿ ä½†æ˜¯ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥çš„éšè—çŠ¶æ€ $h_{t-1}$ å²‚ä¸æ˜¯ç»´åº¦ä¸ºè¯è¡¨å¤§å°ã€‚ã€‚ã€‚æ„Ÿè§‰æœ€å¥½è¿˜æ˜¯ä¸ç”¨è¿™ä¸ªå‚æ•°å§ 12345h0 = cell.zero_state(batch_size=30, dtype=tf.float32)h0.c.shape, h0.h.shape (TensorShape([Dimension(30), Dimension(64)]), TensorShape([Dimension(30), Dimension(128)])) 123inputs = tf.ones(shape=[30,50]) 123output, state = cell(inputs=inputs, state=h0) 123output.shape, state.c.shape, state.h.shape (TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(64)]), TensorShape([Dimension(30), Dimension(128)])) å°è£…äº† RNN çš„å…¶ä»–ç»„ä»¶Core RNN Cell wrappers (RNNCells that wrap other RNNCells) tf.contrib.rnn.MultiRNNCell tf.contrib.rnn.LSTMBlockWrapper tf.contrib.rnn.DropoutWrapper tf.contrib.rnn.EmbeddingWrapper tf.contrib.rnn.InputProjectionWrapper tf.contrib.rnn.OutputProjectionWrapper tf.contrib.rnn.DeviceWrapper tf.contrib.rnn.ResidualWrapper ä¸»è¦çœ‹ tf.contrib.rnn.MultiRNNCell å’Œ tf.contrib.rnn.DropoutWrapperå§ï¼Œå…¶ä»–çš„å°è£…çš„å¤ªå¥½äº†ä¹Ÿä¸å¥½ï¼Œç”¨çš„å…¶å®ä¹Ÿå°‘ã€‚ tf.contrib.rnn.MultiRNNCell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165class MultiRNNCell(RNNCell): &quot;&quot;&quot; RNN cell composed sequentially of multiple simple cells. &quot;&quot;&quot; def __init__(self, cells, state_is_tuple=True): &quot;&quot;&quot;Create a RNN cell composed sequentially of a number of RNNCells. Args: cells: list of RNNCells that will be composed in this order. state_is_tuple: If True, accepted and returned states are n-tuples, where `n = len(cells)`. If False, the states are all concatenated along the column axis. This latter behavior will soon be deprecated. Raises: ValueError: if cells is empty (not allowed), or at least one of the cells returns a state tuple but the flag `state_is_tuple` is `False`. &quot;&quot;&quot; super(MultiRNNCell, self).__init__() if not cells: raise ValueError(&quot;Must specify at least one cell for MultiRNNCell.&quot;) if not nest.is_sequence(cells): raise TypeError( &quot;cells must be a list or tuple, but saw: %s.&quot; % cells) self._cells = cells for cell_number, cell in enumerate(self._cells): # Add Checkpointable dependencies on these cells so their variables get # saved with this object when using object-based saving. if isinstance(cell, checkpointable.CheckpointableBase): # TODO(allenl): Track down non-Checkpointable callers. self._track_checkpointable(cell, name=&quot;cell-%d&quot; % (cell_number,)) self._state_is_tuple = state_is_tuple if not state_is_tuple: if any(nest.is_sequence(c.state_size) for c in self._cells): raise ValueError(&quot;Some cells return tuples of states, but the flag &quot; &quot;state_is_tuple is not set. State sizes are: %s&quot; % str([c.state_size for c in self._cells])) @property def state_size(self): if self._state_is_tuple: return tuple(cell.state_size for cell in self._cells) else: return sum([cell.state_size for cell in self._cells]) @property def output_size(self): return self._cells[-1].output_size def zero_state(self, batch_size, dtype): with ops.name_scope(type(self).__name__ + &quot;ZeroState&quot;, values=[batch_size]): if self._state_is_tuple: return tuple(cell.zero_state(batch_size, dtype) for cell in self._cells) else: # We know here that state_size of each cell is not a tuple and # presumably does not contain TensorArrays or anything else fancy return super(MultiRNNCell, self).zero_state(batch_size, dtype) def call(self, inputs, state): &quot;&quot;&quot;Run this multi-layer cell on inputs, starting from state.&quot;&quot;&quot; cur_state_pos = 0 cur_inp = inputs new_states = [] for i, cell in enumerate(self._cells): with vs.variable_scope(&quot;cell_%d&quot; % i): if self._state_is_tuple: if not nest.is_sequence(state): raise ValueError( &quot;Expected state to be a tuple of length %d, but received: %s&quot; % (len(self.state_size), state)) cur_state = state[i] else: cur_state = array_ops.slice(state, [0, cur_state_pos], [-1, cell.state_size]) cur_state_pos += cell.state_size cur_inp, new_state = cell(cur_inp, cur_state) new_states.append(new_state) new_states = (tuple(new_states) if self._state_is_tuple else array_ops.concat(new_states, 1)) return cur_inp, new_states å‚æ•° cell æ˜¯å…ƒç´ ä¸º RNNCellå¯¹è±¡ çš„ list æˆ– tuple. å…¶å®è¿˜æ˜¯åªæ˜¯è®¡ç®—ä¸€ä¸ªæ—¶é—´æ­¥çš„ state è¿™é‡Œå…ˆä¸è€ƒè™‘åŒå‘ï¼Œåªè€ƒè™‘ deep. ä¹Ÿå°±æ˜¯ä½¿ç”¨ MultiRNNCell 1234567num_units = [64, 128]stack_rnns = [tf.nn.rnn_cell.BasicLSTMCell(num_units=i) for i in num_units]stack_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(stack_rnns) 123h0 = [cell.zero_state(batch_size=32, dtype=tf.float32) for cell in stack_rnn] 12345inputs = tf.random_normal(shape=[32, 100], dtype=tf.float32)output, state = stack_rnn_cell(inputs=inputs, state=h0) 123output.shape TensorShape([Dimension(32), Dimension(128)]) 123state[0].c.shape, state[0].h.shape (TensorShape([Dimension(32), Dimension(64)]), TensorShape([Dimension(32), Dimension(64)])) 123state[1].c.shape, state[1].h.shape (TensorShape([Dimension(32), Dimension(128)]), TensorShape([Dimension(32), Dimension(128)])) æºç ä¸­çš„ä¸€éƒ¨åˆ†ï¼š 12345cur_inp, new_state = cell(cur_inp, cur_state)new_states.append(new_state) å…¶ä¸­ä»æºç ä¸­ä¹Ÿå¯ä»¥å‘ç°æŠŠæ¯ä¸€å±‚çš„ state éƒ½å‚¨å­˜èµ·æ¥äº†ï¼Œè€Œ output è¦ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œæœ€åå¾—åˆ°çš„ output æ˜¯æœ€åä¸€å±‚çš„è¾“å‡ºã€‚ tf.contrib.rnn.DropoutWrapperå‚è€ƒpaper: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141@tf_export(&quot;nn.rnn_cell.DropoutWrapper&quot;)class DropoutWrapper(RNNCell): &quot;&quot;&quot;Operator adding dropout to inputs and outputs of the given cell.&quot;&quot;&quot; def __init__(self, cell, input_keep_prob=1.0, output_keep_prob=1.0, state_keep_prob=1.0, variational_recurrent=False, input_size=None, dtype=None, seed=None, dropout_state_filter_visitor=None): &quot;&quot;&quot;Create a cell with added input, state, and/or output dropout. If `variational_recurrent` is set to `True` (**NOT** the default behavior), then the same dropout mask is applied at every step, as described in: Y. Gal, Z Ghahramani. &quot;A Theoretically Grounded Application of Dropout in Recurrent Neural Networks&quot;. https://arxiv.org/abs/1512.05287 å¦‚æœå‚æ•° variational_recurrent è®¾ç½®ä¸º Trueï¼Œé‚£ä¹ˆ dropout åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½ä¼šæ‰§è¡Œ dropoutï¼Œ Otherwise a different dropout mask is applied at every time step. Note, by default (unless a custom `dropout_state_filter` is provided), the memory state (`c` component of any `LSTMStateTuple`) passing through a `DropoutWrapper` is never modified. This behavior is described in the above article. Args: cell: an RNNCell, a projection to output_size is added to it. input_keep_prob: unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input dropout will be added. output_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. state_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the outgoing states of the cell. **Note** the state components to which dropout is applied when `state_keep_prob` is in `(0, 1)` are also determined by the argument `dropout_state_filter_visitor` (e.g. by default dropout is never applied to the `c` component of an `LSTMStateTuple`). ä¸Šé¢ä¸‰ä¸ªå‚æ•°åˆ†åˆ«è¡¨ç¤º inputï¼Œoutputï¼Œstate æ˜¯å¦ dropoutï¼Œä»¥åŠ dropout ç‡ã€‚ variational_recurrent: Python bool. If `True`, then the same dropout pattern is applied across all time steps per run call. If this parameter is set, `input_size` **must** be provided. è¿™ä¸ªå‚æ•°å¦‚æœä¸º Trueï¼Œé‚£ä¹ˆæ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½éœ€è¦ dropout. input_size: (optional) (possibly nested tuple of) `TensorShape` objects containing the depth(s) of the input tensors expected to be passed in to the `DropoutWrapper`. Required and used **iff** `variational_recurrent = True` and `input_keep_prob &lt; 1`. dtype: (optional) The `dtype` of the input, state, and output tensors. Required and used **iff** `variational_recurrent = True`. seed: (optional) integer, the randomness seed. dropout_state_filter_visitor: (optional), default: (see below). Function that takes any hierarchical level of the state and returns a scalar or depth=1 structure of Python booleans describing which terms in the state should be dropped out. In addition, if the function returns `True`, dropout is applied across this sublevel. If the function returns `False`, dropout is not applied across this entire sublevel. Default behavior: perform dropout on all terms except the memory (`c`) state of `LSTMCellState` objects, and don't try to apply dropout to `TensorArray` objects: Raises: TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided but not `callable`. ValueError: if any of the keep_probs are not between 0 and 1. &quot;&quot;&quot; 123456789cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.LSTMCell(num_units=128), input_keep_prob=1.0, output_keep_prob=1.0, state_keep_prob=1.0) 123cell.state_size, cell.output_size (LSTMStateTuple(c=128, h=128), 128) 123456789# å¤šå±‚ rnnfrom tensorflow.nn.rnn_cell import *NUM_UNITS = [32,64, 128]rnn = MultiRNNCell([DropoutWrapper(LSTMCell(num_units=n), output_keep_prob=0.8) for n in NUM_UNITS]) 123rnn.output_size, rnn.state_size (128, (LSTMStateTuple(c=32, h=32), LSTMStateTuple(c=64, h=64), LSTMStateTuple(c=128, h=128))) tf.nn.dynamic_rnnæœ€åå‰é¢è¯´äº†è¿™ä¹ˆå¤š classï¼Œä»–ä»¬éƒ½åªæ˜¯ä¸€ç§è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„ output å’Œ state çš„æ–¹å¼ï¼Œä½†æ˜¯ rnn å¤„ç†çš„éƒ½æ˜¯åºåˆ—ï¼Œæ‰€ä»¥æ€ä¹ˆå°†è¿™äº› cell å¯¹è±¡å°è£…åˆ°åºåˆ— rnn ä¸­ 12345678910111213def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None): &quot;&quot;&quot;Creates a recurrent neural network specified by RNNCell `cell`. Performs fully dynamic unrolling of `inputs`. &quot;&quot;&quot; 12345rnn_layers = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=n)) for n in [32, 64]]cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers) 123inputs = tf.random_normal(shape=[30, 10, 100]) 123initial_state = cell.zero_state(batch_size=30, dtype=tf.float32) 123output, state = tf.nn.dynamic_rnn(cell,inputs=inputs, initial_state=initial_state, dtype=tf.float32) 123output.shape TensorShape([Dimension(30), Dimension(10), Dimension(64)]) 123cell.state_size (LSTMStateTuple(c=32, h=32), LSTMStateTuple(c=64, h=64)) æ‰€ä»¥ç›®å‰ä¸ºæ­¢ï¼Œæš‚æ—¶ojbkäº†ï½ï½ æ¥ä¸‹æ¥å°±æ˜¯åœ¨ attention å°è£… rnn äº†","link":"/2018/09/01/tensorflow-rnn-api-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"title":"ä»0å¼€å§‹GAN-4-ScratchGAN","text":"Training Language GANs from Scratchå‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œç›®å‰çœ‹åˆ°language gansçš„ç›¸å…³paperå¤§éƒ¨åˆ†æ˜¯Googleï¼ŒDeepMindçš„paper. æ„Ÿè§‰æ˜¯ä¸ªæ·±ä¸è§åº•çš„å‘ï¼Œå¼±æ¸£å“­äº†ã€‚ã€‚ã€‚ Motivationæˆ‘ä»¬çŸ¥é“language GANéå¸¸éš¾è®­ç»ƒï¼Œä¸»è¦æ˜¯å› ä¸ºgradient estimation, optimization instability, and mode collapseç­‰åŸå› ï¼Œè¿™å¯¼è‡´å¾ˆå¤šNLPeré€‰æ‹©å…ˆåŸºäºmaximum likelihoodå¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç”¨language GANè¿›è¡Œfine-tune.ä½œè€…è®¤ä¸ºè¿™ç§ fine-tune ç»™æ¨¡å‹å¸¦æ¥çš„benefitå¹¶ä¸clearï¼Œç”šè‡³ä¼šå¸¦æ¥ä¸å¥½çš„æ•ˆæœã€‚ å…³äºmode collapseï¼Œæå®æ¯…è€å¸ˆè®²è¿‡ï¼Œåœ¨å¯¹è¯ç”Ÿæˆæ—¶ï¼Œæ¨¡å‹æ€»æ˜¯å€¾å‘äºç”Ÿæˆâ€œæˆ‘ä¸çŸ¥é“â€ï¼Œâ€æˆ‘çŸ¥é“äº†â€è¿™æ ·é€šç”¨çš„æ²¡æœ‰å¤ªå¤šsenseçš„å›å¤ï¼Œå…¶å®å°±æ˜¯å±äºmode collapse. ç±»ä¼¼äºå›¾åƒé¢†åŸŸï¼Œæ—¢è¦ç”Ÿæˆé¼»å­ï¼Œåˆè¦ç”Ÿæˆå˜´å·´ï¼Œä½†æ˜¯æ¨¡å‹ä¼šå€¾å‘äºç”Ÿæˆä¸€ä¸ªå±…ä¸­çš„distributionæ¥æ¨¡æ‹Ÿè¿™ä¸¤ä¸ªdistributionã€‚ å…³äºgradient estimatorï¼Œæ˜¯å› ä¸ºå¯¹äºç¦»æ•£çš„æ•°æ®ï¼Œå…¶gradientsçš„æ–¹å·®ä¼šå¾ˆå¤§ã€‚ [13-16]å°±æ˜¯å…ˆä½¿ç”¨MLé¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šadversarial fine-tune.[17-18]åˆ™è¯´æ˜äº† â€œthat the best-performing GANs tend to stay close to the solution given by maximum-likelihood trainingâ€. æ‰€ä»¥ä½œè€…ä¸ºäº†è¯æ˜language GANçœŸçš„èƒ½workï¼Œå°±from scratchè®­ç»ƒäº†ä¸€ä¸ªlanguage GAN, å¯¹ï¼Œæ²¡æœ‰é¢„è®­ç»ƒã€‚ä½œè€…è®¤ä¸ºä»å¤´è®­ç»ƒå¥½language GANçš„æ ¸å¿ƒæŠ€æœ¯æ˜¯ large batch sizes, dense rewards and discriminator regularization. æœ¬æ–‡çš„è´¡çŒ®ï¼š ä»å¤´è®­ç»ƒä¸€ä¸ªlanguage GANèƒ½è¾¾åˆ°åŸºäºMLæ–¹æ³•çš„unconditional text generation. è¯æ˜ large batch sizes, dense rewards and discriminator regularization å¯¹äºè®­ç»ƒlanguage GANçš„é‡è¦æ€§ã€‚ ä½œè€…å¯¹æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„evaluationæå‡ºäº†ä¸€äº›æ€§çš„æ‹“å±•ï¼Œèƒ½å……åˆ†æŒ–æ˜ç”Ÿæˆçš„languageæ›´å¤šçš„ç‰¹æ€§ã€‚æ¯”å¦‚ï¼š BLEU and Self-BLEU [19] capture basic local consistency. The Frechet Distance metric [17] captures global consistency and semantic information. Language and Reverse Language model scores [18] across various softmax temperatures to capture the diversity-quality trade-off. Nearest neighbor analysis in embedding and data space provide evidence that our model is not trivially overfitting. Generative Models of Textç”Ÿæˆæ¨¡å‹çš„æœ¬è´¨å°±æ˜¯å¯¹unknown data distributionè¿›è¡Œå»ºæ¨¡ï¼Œä¹Ÿå°±æ˜¯å­¦ä¹ æ¨¡å‹ p(x|y) çš„å‚æ•°ã€‚åœ¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ é‡Œé¢ï¼Œæˆ‘ä»¬è®¤ä¸ºæ¨¡å‹ p(x|y) çš„åˆ†å¸ƒå°±æ˜¯å¤šç»´é«˜æ–¯æ­£æ€åˆ†å¸ƒï¼Œç„¶åç”¨EMç®—æ³•å»å­¦ä¹ å¾—åˆ°å‚æ•°ã€‚åœ¨åŸºäºneural networkçš„è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¯¹äº $x=[x_1,..,x_T]$ï¼Œ $p_{\\theta}(x_t|x_1,â€¦,x_{t-1})$ ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯å­¦ä¹ è¿™æ ·ä¸€ä¸ªdistributionï¼Œåªä¸è¿‡æ¨¡å‹çš„å‚æ•°ä¸æ˜¯é«˜æ–¯æ­£æ€åˆ†å¸ƒè¿™ä¹ˆç®€å•ï¼Œè€Œæ˜¯åŸºäºnetworkæ¥æ¨¡æ‹Ÿçš„ã€‚åŒæ ·åºåˆ—ç‰¹æ€§ä½¿å¾—å…¶éå¸¸é€‚åˆä½¿ç”¨è‡ªå›å½’æ¨¡å‹è¿›è¡Œå»ºæ¨¡: $$p_{\\theta}=\\prod_{t=1}^Tp_{\\theta}(x_t|x_1,â€¦,x_{t-1})$$ Maximum Likelihoodä¸€æ—¦æ¨¡å‹å»ºç«‹å¥½äº†ï¼Œæ¥ä¸‹æ¥å°±æ˜¯è®­ç»ƒæ¨¡å‹ã€‚æœ€å¸¸ç”¨çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ maximum likelihood estimation(MLE). $$\\argmax_{\\theta}\\mathbb{E}{p^* (x)}logp{\\theta}(x)$$ å…³äº maximum likelihood æ˜¯å¦æ˜¯æœ€ä¼˜è§£ï¼Œè¿™ç¯‡paperæœ‰è®¨è®º[9]ã€‚ Generative Adversarial Networks å‰é¢seqganä¹Ÿè¯´è¿‡è‡ªå›å½’æ¨¡å‹ä¸­ $p_{\\theta}=\\prod_{t=1}^Tp_{\\theta}(x_t|x_1,â€¦,x_{t-1})$çš„è¿‡ç¨‹æœ‰ä¸ªsampleçš„æ“ä½œï¼Œè¿™æ˜¯ä¸å¯å¯¼çš„ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œæœ‰ä¸‰ç§è§£å†³æ–¹æ³•ï¼š é«˜æ–¹å·®ï¼Œæ— åä¼°è®¡çš„ reinforce[28]. åŸºäºå¤§æ•°å®šå¾‹çš„æ¡ä»¶ä¸‹ï¼Œå»sampleæ›´å¤šçš„exampleï¼Œæ¥æ¨¡æ‹Ÿ $p(y_t|s_t)$ çš„åˆ†å¸ƒï¼Œç„¶ååŸºäºpolicy gradientå»ä¼˜åŒ–è¿™ä¸ªdistributionï¼Œè¿™ä½¿å¾—é€Ÿåº¦å¾ˆæ…¢ã€‚ ä½æ–¹å·®ï¼Œæœ‰åä¼°è®¡çš„ gumbel-softmax trick[29-30]. other continuous relaxations[11]. Learning Signalså¯¹äºgeneratorçš„è®­ç»ƒï¼Œä½œè€…é‡‡ç”¨äº†åŸºäº REINFORCE çš„æ–¹æ³•: å…¶ä¸­åŒ MaliGAN[15] ä¸€æ ·ï¼Œè®¾ç½® $R(x)=\\dfrac{p^* (x)}{p_{\\theta}(x)}$, è¿™æ ·ç­‰æ•ˆäº MLE ä¼°è®¡ã€‚ åŸºäºMLE eatimatorçš„æ¢¯åº¦æ›´æ–°å¯ä»¥çœ‹ä½œæ˜¯reinforceçš„ä¸€ä¸ªspacial case.åŒºåˆ«åœ¨äºlanguage gansçš„rewardæ˜¯å¯ä»¥å­¦ä¹ çš„ï¼Œä¹Ÿå°±æ˜¯discriminatoræ˜¯ä¸æ–­æ›´æ–°çš„ã€‚å¯å­¦ä¹ çš„discriminatorçš„æ•ˆæœå·²ç»è¢«è¯æ˜è¿‡äº†[34]. å¦‚æœlearned rewardèƒ½å¤Ÿæä¾›ç›¸æ¯”MLE lossæ›´å…‰æ»‘çš„ä¿¡å·ï¼Œé‚£ä¹ˆdiscriminatorå°±èƒ½æä¾›æ›´å¤šæœ‰æ„ä¹‰çš„signalï¼Œç”šè‡³training dataæ²¡æœ‰coverçš„distribution. åŒæ—¶ï¼Œdiscriminatoræ˜¯å¯ä»¥ensembleçš„ï¼Œä½¿ç”¨æ›´å¤šçš„domain knowledge.è¿™æ ·èƒ½å­¦ä¹ åˆ°æ›´å¤šçš„ä¿¡æ¯ã€‚ Training Language GANs from Scratchä½œè€…é€šè¿‡å®éªŒéªŒè¯ï¼Œè¦è®­å¥½ä¸€ä¸ªlanguage gansï¼Œæ‰€éœ€è¦çš„æ˜¯ï¼š a recurrent discriminator used to provide dense rewards at each time step large batches for variance reduction discriminator regularization dense rewardsåˆ¤åˆ«å™¨èƒ½å¤Ÿåˆ¤åˆ«generated sentenceå’Œreal sentenceï¼Œä½†æ˜¯å¯¹äºä¸å®Œæ•´çš„å¥å­ï¼Œå°±æ²¡åŠæ³•å»åˆ¤æ–­ã€‚è¿™å°±é€ æˆï¼Œå¦‚æœgenerated sentenceå¾ˆå®¹æ˜“å°±è¢«åˆ¤æ–­ä¸ºfakeï¼Œé‚£ä¹ˆåœ¨fix discriminatorè®­ç»ƒgeneratoræ—¶ï¼Œç”Ÿæˆå™¨æ— æ³•è·å¾—æœ‰æ„ä¹‰çš„ä¿¡å·ï¼Œä¹Ÿå°±æ˜¯æ¢¯åº¦ä¸º0å§ã€‚ ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œä½œè€…é‡‡ç”¨äº† MaskGAN[32] çš„æ–¹æ³•: maskGAN maskGANæ˜¯ä¸€ç§ actor-critic æ–¹æ³•ï¼Œåˆ©ç”¨ç±»ä¼¼äºå®Œå½¢å¡«ç©ºçš„å½¢å¼ï¼Œåªéœ€è¦ç”Ÿæˆè¢«æŒ–å»çš„è¯ï¼Œå°±èƒ½å¯¹æ•´ä¸ªsentenceè¿›è¡Œåˆ¤åˆ«ï¼Œå¹¶è®¡ç®—rewardï¼Œè¿™æ ·å¾—åˆ°çš„rewardç›¸æ¯”sentenceä¸­çš„æ¯ä¸€ä¸ªè¯éƒ½æ˜¯ç”Ÿæˆçš„ï¼Œå…¶varianceä¼šå°å¾ˆå¤šã€‚ å…·ä½“åšæ³•æ˜¯ï¼š ç”Ÿæˆå™¨æ˜¯ seq2seq çš„å½¢å¼ï¼Œè¾“å…¥sequence $x=(x_1,â€¦,x_T)$. é€šè¿‡ binary mask $m=(m_1,â€¦,m_T)$ å¾—åˆ° $m(x)$. æ ¹æ® m(x) æ¥ç”Ÿæˆå¾—åˆ°å®Œæ•´çš„ generated examples $\\hat x=(\\hat x_1, \\hat x_2,â€¦,\\hat x_T)$. è¿™é‡Œç”Ÿæˆçš„æ—¶å€™å‚è€ƒä¸Šå›¾ï¼Œå¦‚æœå½“å‰time-stepè¢«maskäº†ï¼Œåˆ™éœ€è¦ç”¨åˆ°ä¸Šä¸€ä¸ªtime-stepç”Ÿæˆçš„è¯ï¼Œå¦‚æœæ²¡æœ‰è¢«maskï¼Œå°±ç›´æ¥ä½¿ç”¨å½“å‰è¯ï¼Œç±»ä¼¼äºteacher-forcing. åˆ¤åˆ«å™¨å°±æ˜¯è®¡ç®—æ¯ä¸€ä¸ªè¯ä¸ºçœŸçš„æ¦‚ç‡ï¼Œæ³¨æ„è¿™é‡Œåˆ¤åˆ«å™¨çš„è¾“å…¥ä¹Ÿæœ‰ m(x)ï¼Œå…¶åŸå› æ˜¯è®©æ¨¡å‹æ›´å¥½çš„è¯†åˆ«ç”Ÿæˆçš„sentenceä¸­ï¼Œå“ªä¸€ä¸ªæ˜¯ä¹‹å‰è¢«maskäº†çš„ã€‚ $$D_{\\phi}(\\tilde x_t|\\tilde x_{0:T}, m(x)) = P(\\tilde x_t=x_t^{real}|\\tilde x_{0:T}, m(x))$$ reward çš„è®¡ç®—ï¼š $$r_t=logD_{\\phi}(\\tilde x_t|\\tilde x_{0:T}, m(x))$$ Large Batch Sizes for Variance Reductionreference: [9] How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv [12-16] [17-18] [32] Maskgan: Better text generation via filling in the ____ [34]","link":"/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/"},{"title":"ä»0å¼€å§‹GAN-5-NAT Decoding","text":"non-autoregressive decode ç›¸å…³çš„paperï¼š Non-autoregressive neural machine translation. Gu et al. 2018 ICLR End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinemen paper1Constant-Time Machine Translation with Conditional Masked Language Models MotivationæŠŠauto-regressiveè½¬æ¢æˆnon-autoregressive. å…¶åšæ³•æ˜¯å…ˆç¡®å®šä¸€ä¸ªtarget senteceçš„é•¿åº¦ï¼Œç„¶åå¯ä»¥çœ‹ä½œæ˜¯æ¯ä¸€ä¸ªtime-stepçš„åˆ†ç±»ä»»åŠ¡äº†ã€‚è¿™æ ·decoderå°±æ˜¯å¯å¹¶è¡Œçš„äº†ã€‚ architectureæ¨¡å‹æ¶æ„é‡‡ç”¨transformerçš„æ¶æ„ã€‚ åŸç”Ÿçš„ Transformer: source-language encoder: self-attention, åŒ…æ‹¬padding mask. translation model decoder self-attention, åŒ…æ‹¬padding maskå’Œ look ahead maskï¼Œç”¨ä»¥maskæ‰future information. interaction attention with enc_out, åŒ…æ‹¬ padding mask. è¿™ç¯‡paperä¸­çš„ conditional mask language model(CMLM) ä¸transormerçš„åŒºåˆ«åœ¨äº decoder éƒ¨åˆ†çš„self-attentionå»æ‰äº† look ahead mask. æ‰€ä»¥å¯ä»¥ç±»ä¼¼äº BERT é‚£æ ·åŸºäºä¸Šä¸‹æ–‡æ¥é¢„æµ‹è¢« mask çš„è¯ï¼Œdecoder æ˜¯ bi-directional. Decoding with Mask-Predictdecoder çš„å…·ä½“æ“ä½œæ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ã€‚ | src | Der Abzug der franzsischen Kampftruppen wurde am 20. November abgeschlossen . | | :â€”â€“ | :â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”- | |t=0|The withdrawal of French combat troops was completed on November 20th .| |t=1|The departure of the French combat completed completed on 20 November .| |t=2|The departure of the French combat completed completed on 20 November .| è¡¨ä¸­åŠ ç²—çš„éƒ¨åˆ†æ˜¯è¢« mask çš„ã€‚å¯ä»¥çœ‹åˆ°éšç€è¿­ä»£è¿›è¡Œï¼Œmaskçš„è¯è¶Šæ¥è¶Šå°‘ã€‚ å¦‚ä½•é€‰æ‹©maskçš„è¯ï¼š maskè¯çš„æ•°é‡n: åŸºäºä¸€ä¸ªé€’å‡çš„å…¬å¼, $n=N\\dfrac{T-t}{T}$. t æ˜¯è¿­ä»£æ¬¡æ•°ã€‚ maskå“ªäº›è¯å‘¢ï¼š$Y^{(t)}_{mask}=argmin_i(p_i,n)$ $p_i$ è¡¨ç¤ºä¸Šä¸€æ¬¡predictionå¾—åˆ°çš„æ¯ä¸€ä¸ªè¯çš„ç½®ä¿¡åº¦ï¼Œé€‰æ‹©æ¦‚ç‡æœ€ä½çš„ n ä¸ªè¯ã€‚ åŸºäº encoder_src, $Y_{obs}$ å¯¹ mask token è¿›è¡Œé¢„æµ‹: target sequence lengthè¿™ä¸­ non-Autoregressive å­˜åœ¨çš„ä¸€ä¸ªå¤§é—®é¢˜å°±æ˜¯å¦‚ä½•ç¡®å®štarget sentence çš„é•¿åº¦ã€‚åœ¨ auto-egressive é‡Œé¢æ˜¯æ ¹æ® ${}$ æ¥ç¡®å®šå¥å­é•¿åº¦çš„ã€‚ é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…é‡‡ç”¨äº†ç±»ä¼¼äº BERT ä¸­ CLS çš„åšæ³•ã€‚ä½¿ç”¨äº† $LENGTH$ æ¥é¢„æµ‹sentenceçš„é•¿åº¦ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œè¿™ä¸ª LENGTH å¯¹åº”çš„è¯è¡¨åº”è¯¥å°±æ˜¯é•¿åº¦~ ä½œè€…é€‰å– top b lengthï¼Œç±»ä¼¼äº beam search. ç„¶åé€‰æ‹© candidated b sentence ä¸­æ¦‚ç‡æœ€å¤§çš„. $$\\dfrac{1}{N}\\sum logp_i^{(T)}$$ code readingpaper2Non-Autoregressive Neural Machine Translation Motivation:ç°æœ‰çš„æœºå™¨ç¿»è¯‘æ¨¡å‹åœ¨inferenceæ—¶ï¼Œéœ€è¦åœ¨ç”Ÿæˆå‰ä¸€ä¸ªå•è¯çš„åŸºç¡€ä¸Šç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªå•è¯ï¼Œè¿™ç§è‡ªå›å½’çš„ç‰¹æ€§ä¸¥é‡å½±å“äº†æ¨ç†çš„é€Ÿåº¦ã€‚ å¹¶ä¸”ä¸è®­ç»ƒé˜¶æ®µçš„ä¸ä¸€è‡´å¯¼è‡´å­˜åœ¨exposure biasã€‚ä½œè€…æå‡ºä¸€ä¸ªéè‡ªå›å½’çš„æ–¹æ³•ï¼Œåœ¨inferé˜¶æ®µå¹¶è¡Œè¾“å‡ºã€‚ Exposure bias: training é˜¶æ®µä¸Šä¸€ä¸ªtokenæ˜¯ground truth infer é˜¶æ®µä¸Šä¸€ä¸ªtokenæ˜¯ç”Ÿæˆå¾—åˆ°çš„ï¼Œè¿™æ ·è‡ªå›å½’ç”Ÿæˆæ•´ä¸ªå¥å­å­˜åœ¨è¯¯å·®ç´¯ç§¯ ä¸¤ä¸ªé˜¶æ®µç”Ÿæˆtargetçš„æ–¹å¼ä¸ä¸€æ ·ï¼Œä¹Ÿå°±æ˜¯ exposure bias. Model Architecture: å‰ä¸€é¡¹è¡¨ç¤ºåŸºäºç›‘ç£å­¦ä¹ æ¥é¢„æµ‹targets å¥å­çš„é•¿åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ä½œè€…ä½¿ç”¨äº†è¿™ä¸ªè¯ fertilities(ç”Ÿè‚²èƒ½åŠ›) æ¥è¡¨ç¤ºé€šè¿‡sourceå¥å­é€šè¿‡encodeä¹‹åæ‰€åŒ…å«çš„çŸ¥è¯†. åä¸€é¡¹ä¾æ—§æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œä¹Ÿå°±æ˜¯ independent cross-entropy losses on each output distributionã€‚ ä½†ä¸åŒçš„æ˜¯ï¼Œåœ¨inferenceé˜¶æ®µä¹Ÿæ˜¯å¯ä»¥å¹¶è¡Œçš„ã€‚ è¿™é‡Œæœ‰ä¸ªç–‘é—®ï¼Œåœ¨è®­ç»ƒé˜¶æ®µä¼šé¢„æµ‹å¾—åˆ°ä¸€ä¸ªé•¿åº¦Tï¼Œä½†æ˜¯è®­ç»ƒé˜¶æ®µæ—¶groud truthé•¿åº¦çš„ï¼Œè¿™ä¸ªæ€ä¹ˆè§£å†³ï¼Ÿ è¿™é‡Œåœ¨è®­ç»ƒé˜¶æ®µæ˜¾ç„¶éœ€è¦é•¿åº¦ä¸ ground truth çš„target sentenceé•¿åº¦ä¸€è‡´ï¼Œæ‰èƒ½è®¡ç®— word-wise corss entropy loss. Decoder Stack1.decoder input é¦–å…ˆå…³äº decoder çš„åˆå§‹è¾“å…¥ï¼Œåœ¨å·²æœ‰çš„æ¨¡å‹ä¸­ï¼Œè®­ç»ƒé˜¶æ®µ decoder çš„è¾“å…¥æ˜¯ time-shifted target outputsï¼Œæ¨ç†é˜¶æ®µæ˜¯å‰é¢æ—¶é—´æ­¥é¢„æµ‹çš„è¾“å‡ºã€‚ å¯¹äºNATæ¨¡å‹ï¼Œéœ€è¦æå‰ç¡®å®š target output çš„é•¿åº¦ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼š Copy source inputs uniformly Copy source inputs using fertilitiesï¼Œ å¦‚ä¸Šå›¾ä¸­è¾“å…¥çš„æ¯ä¸ªæ—¶é—´æ­¥éƒ½æœ‰å…¶å¯¹åº”çš„ fertility. ç„¶åæŠŠsource inputæŒ‰ç…§å…¶å¯¹åº”çš„æ¬¡æ•°copyåˆ°decoderçš„è¾“å…¥ã€‚ 2.Non-causal self-attention å› ä¸ºä¸æ˜¯è‡ªå›å½’ï¼Œä¹Ÿå°±æ˜¯ä¸‹ä¸€ä¸ªè¯çš„ç”Ÿæˆå¹¶ä¸ä¾èµ–äºpreviousçš„tokensï¼Œæ‰€ä»¥å¯ä»¥å»æ‰transformerä¸­decoderéƒ¨åˆ†çš„cause-mask,ä¹Ÿå°±æ˜¯å¯ä»¥ç»“åˆä¸Šä¸‹æ–‡çš„è¯ï¼Œè€Œä¸ä»…ä»…åªæ˜¯ä¸Šæ–‡ã€‚ 3.position attention We also include an additional positional attention module in each decoder layer, which is a multi-head attention module with the same general attention mechanism used in other parts of the Transformer network. ä¸ºäº†å¼ºè°ƒä½ç½®ä¿¡æ¯ã€‚ Modeling fertility to tackle the multimodality problem $P_F(f_{tâ€™}|x_{1:Tâ€™}; \\theta)$ è¡¨ç¤º fertility åœ¨ tâ€™ æ—¶é—´æ­¥çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶æ˜¯é€šè¿‡encoderé¡¶å±‚çš„ mlp + softmax å¾—åˆ°çš„ã€‚","link":"/2019/06/29/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-5-decoding/"},{"title":"ä»0å¼€å§‹GAN-7-IRGAN","text":"MotivationIRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models ä¿¡æ¯æ£€ç´¢çš„æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªæµæ´¾ï¼Œç”Ÿæˆå¼æ£€sç´¢æ¨¡å‹(generative retrieval model)å’Œåˆ¤åˆ«å¼æ£€ç´¢æ¨¡å‹(discriminative retrieval model)ã€‚ ç”Ÿæˆå¼æ£€ç´¢æ¨¡å‹ ($q \\rightarrow d$)ï¼šè®¤ä¸ºqueryå’Œæ£€ç´¢æ‰€éœ€è¦çš„documentä¹‹é—´æœ‰ä¸€ä¸ªæ½œåœ¨çš„éšæœºç”Ÿæˆçš„è¿‡ç¨‹ã€‚ä¹Ÿå°±æ˜¯ç»™å®šä¸€ä¸ª queryï¼Œç„¶åç”Ÿæˆç›¸åº”çš„ document. åˆ¤åˆ«å¼æ£€ç´¢æ¨¡å‹ ($q + d \\rightarrow r$)ï¼šæŠŠqueryå’Œdocumentä½œä¸ºè”åˆfeatureï¼Œè®¡ç®—å…¶ç›¸å…³æ€§relevancy. ç„¶ååŸºäº relevancy å¯¹ document è¿›è¡Œæ’åºã€‚å…¶ä¸­å…³äº ranking a list of documents æœ‰ä¸‰ç§èŒƒå¼ï¼špointwise, pairwise, listwise. ä½œè€…å°†ä¸Šè¿°ä¸¤ç§æ¨¡å‹ä¸GANç›¸ç»“åˆï¼Œåˆ©ç”¨GANçš„å¯¹æŠ—æ€§çš„æ€æƒ³å»æå‡ä¸¤ç±»æ¨¡å‹ã€‚ åˆ¤åˆ«å¼æ£€ç´¢æ¨¡å‹ $p_{\\phi}(r|q,d)$ ä½œä¸ºåˆ¤åˆ«å™¨ï¼Œmaximize æ¥è‡ªçœŸå® labeled çš„æ•°æ®ã€‚å®ƒæä¾›ä¿¡æ¯æ¥æŒ‡å¯¼ç”Ÿæˆå™¨çš„è®­ç»ƒï¼Œè¿™ç§ä¿¡æ¯ä¸åŒäºä¼ ç»Ÿçš„ log-likelihood. åˆ¤åˆ«å¼æ£€ç´¢æ¨¡å‹ $p_{\\theta}(d|q,r)$ æ˜¯ç”Ÿæˆå™¨ï¼Œç”Ÿæˆgenerated sampleæ¥è¿·æƒ‘åˆ¤åˆ«å™¨ï¼Œminimize å¯¹åº”çš„ç›®æ ‡å‡½æ•°ã€‚ Model ArchitectureA Minimax Retrieval Frameworka set of queries ${q_1,â€¦,q_N}$, a set of documents ${d_1,â€¦,d_M}$. å…¶ä¸­ ç»™å®šä¸€ä¸ª query éƒ½æœ‰å¯¹åº”çš„ç›¸å…³åº¦è¾ƒé«˜çš„ document ä¹Ÿå°±æ˜¯çœŸå®çš„æ•°æ® $true_{(q,d)}$ï¼Œå…¶æ•°æ®é‡æ˜¯è¿œå°äºæ€»çš„documentæ•°é‡ M çš„. The underlying true relevance distribution can be expressed as conditional probability $p_{true} (d|q, r)$, which depicts the (userâ€™s) relevance preference distribution over the candidate documents with respect to her submitted query. è¿™æ ·çœŸå®çš„ç›¸å…³æ€§ (q,d) å­˜åœ¨æ½œåœ¨çš„ç›¸å…³æ€§æ¡ä»¶åˆ†å¸ƒ $p_{true}(d|q,r)$. Generative retrieval model $p_{\\theta}(d|q,r)$: ç”Ÿæˆå™¨çš„ç›®çš„å°±æ˜¯å»å°½å¯èƒ½çš„æ¨¡æ‹ŸçœŸå®çš„ç›¸å…³æ€§åˆ†å¸ƒ $p_{ture}(d|q,r)$, ä»è€Œå°½å¯èƒ½ç”Ÿæˆç›¸ä¼¼åº¦é«˜çš„ document. Discriminative retrieval model $f_{\\phi}(q,d)$ï¼šæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»åˆ†ç±»å™¨ã€‚ å…¶ä¸­åˆ¤åˆ«å™¨å…·ä½“çš„è®¡ç®— $f_{\\phi}(d,q)$ ä¸IR taskæœ‰å…³ã€‚åç»­ä¼šè¯¦ç»†ä»‹ç»ã€‚ Overall Objective ç›®æ ‡å‡½æ•°ï¼š æœ€å°åŒ–æ¥è‡ªç”Ÿæˆå™¨ $p_{\\theta}$ çš„ sample çš„æ¦‚ç‡ï¼Œæœ€å¤§åŒ–æ¥è‡ªtrue data $p_{true}$ çš„ sample çš„æ¦‚ç‡. Optimising Discriminative Retrievalä¼˜åŒ–åˆ¤åˆ«å™¨ï¼š Optimising Generative Retrievalè¿™ç¯‡paperä¸­ç”Ÿæˆå™¨ä¸æ˜¯token by tokençš„ç”Ÿæˆæ–°çš„ducomentï¼Œè€Œæ˜¯ä»given documentsä¸­é€‰æ‹©æœ€ç›¸å…³çš„document. å¯¹äºç”Ÿæˆå™¨çš„ä¼˜åŒ–ï¼Œæœ€å°åŒ–ç›®æ ‡å‡½æ•°ï¼ˆ1ï¼‰ï¼š ä¸Šè¿°å…¬å¼ä»ç¬¬ä¸€æ­¥åˆ°ç¬¬äºŒæ­¥æœ‰ç‚¹å°å˜åŒ–ï¼Œç®€å•æ¨å¯¼ä¸‹å³å¯ã€‚ è¿™é‡Œsampleå¾—åˆ°dçš„è¿‡ç¨‹æ˜¯ç¦»æ•£çš„ã€‚æ€ä¹ˆç†è§£å‘¢ï¼Œå¯ä»¥ç±»æ¯”æ–‡æœ¬çš„ç”Ÿæˆï¼ˆå°½ç®¡æ­¤è¡¨çš„åˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œä½†æ˜¯ä»ä¸­é€‰ä¸€ä¸ªtokenï¼Œç„¶åä½œä¸ºåˆ¤åˆ«å™¨çš„è¾“å…¥ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯ä¸å¯å¯¼çš„ï¼‰ã€‚åŒæ ·ï¼Œè¿™é‡Œæ˜¯ä»ä¸€ç³»åˆ—documentsä¸­sampleä¸€ä¸ªä½œä¸ºåˆ¤åˆ«å™¨çš„è¾“å…¥ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯ç¦»æ•£çš„ï¼Œä¸”ä¸å¯å¯¼ã€‚æ‰€ä»¥ä½œè€…é‡‡ç”¨äº†policy gradientçš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ å…¬å¼(4)ä¸­å¯¹ç”Ÿæˆå™¨çš„ä¼˜åŒ–å¯ä»¥çœ‹ä½œæ˜¯ maximize $J^G(q_n)$. ä½¿ç”¨policy gradientä¼˜åŒ–çš„æ¨å¯¼å¦‚ä¸‹ï¼š è¿™é‡Œçš„policyæ˜¯ $p_{\\theta}(d|q_n,r)$ å°±æ˜¯æˆ‘ä»¬éœ€è¦ä¼˜åŒ–çš„ç”Ÿæˆå¼æ£€ç´¢æ¨¡å‹ï¼Œå¯¹åº”çš„actionæ˜¯ç»™å®šenvironment qçš„æƒ…å†µä¸‹sampleå¾—åˆ° document. åˆ¤åˆ«å™¨å¾—åˆ°çš„log-probå°±æ˜¯reward. ä¸ºäº†å‡å°REINFORCEæ–¹æ³•ä¸­varianceï¼Œä½œè€…é‡‡ç”¨äº†advantage-functionï¼Œä¹Ÿå°±æ˜¯å‡å»baselineï¼Œå…¶ä¸­baselineæ˜¯å‡å€¼ï¼š æ•´ä¸ªIRGANçš„è®­ç»ƒè¿‡ç¨‹çš„ä¼ªä»£ç ï¼š ä¸Šå›¾ä¸­çš„å…¬å¼(22)å°±æ˜¯å…¬å¼(5). æ•´ä¸ªè¿‡ç¨‹ç†è§£èµ·æ¥è¿˜æ˜¯è›®ç®€å•çš„ã€‚ è¿˜æœ‰ä¸ªé—®é¢˜ä¸ºè§£å†³çš„æ˜¯ï¼Œå‰é¢æåˆ°å¯¹äºä¸åŒçš„ IR ä»»åŠ¡ï¼Œåˆ¤åˆ«å™¨ $f_{\\phi}(q,d)$ çš„æ–¹å¼æ˜¯ä¸ä¸€æ ·çš„ã€‚ pairwise case Furthermore, ifwe use graded relevance scales (indicating a varying degree of match between each document and the corresponding query) rather than binary relevance, the training data could also be represented naturally as ordered document pairs. æ­¤å¤–ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨åˆ†çº§ç›¸å…³æ€§æ¯”ä¾‹ï¼ˆæŒ‡ç¤ºæ¯ä¸ªæ–‡æ¡£ä¸ç›¸åº”æŸ¥è¯¢ä¹‹é—´çš„ä¸åŒåŒ¹é…ç¨‹åº¦ï¼‰è€Œä¸æ˜¯äºŒå…ƒç›¸å…³æ€§ï¼Œåˆ™è®­ç»ƒæ•°æ®ä¹Ÿå¯ä»¥è‡ªç„¶åœ°è¡¨ç¤ºä¸ºæœ‰åºæ–‡æ¡£å¯¹. ä¹Ÿå°±æ˜¯ä¸ä»…ä»…æ ¹æ®queryå’Œdocumentä¹‹é—´æ˜¯å¦ç›¸ä¼¼è¿™æ ·çš„äºŒå…ƒæ–‡æ¡£å¯¹ï¼Œè€Œæ˜¯åˆ©ç”¨æœ‰åºæ–‡æ¡£å¯¹ï¼ˆè¿™åœ¨IRä¸­å…¶å®æ›´ä¸ºå¸¸è§ï¼‰ï¼Œä½œä¸ºåˆ¤åˆ«å™¨çš„è¾“å…¥ï¼Œè¿™æ ·èƒ½è·å–æ›´å¤šçš„ä¿¡æ¯ã€‚ è¿™ä¸ªæ—¶å€™çš„labeled documentæ˜¯ $R_n={&lt;d_i,d_j&gt;|d_i &gt; d_j}$, å…¶ä¸­ $d_i &gt; d_j$ æ„å‘³ç€ $d_i$ æ¯” $d_j$ çš„ç›¸å…³æ€§æ›´é«˜ã€‚ ä½¿ç”¨pairwise discriminatorå¯¹åº”çš„ç›®æ ‡å‡½æ•°ï¼š å…¶ä¸­ $o = &lt;d_u,d_v&gt;, oâ€™=&lt;d_uâ€™,d_vâ€™&gt;$. åœ¨å®é™…çš„æ“ä½œä¸­ï¼Œé€‰æ‹©ä¸€å¯¹document $&lt;d_i,d_j&gt;$. ç„¶åé€‰æ‹©ç›¸ä¼¼åº¦è¾ƒä½çš„ $d_j$ ä¸ç”Ÿæˆå™¨å¾—åˆ°çš„ $d_k$ ç»„æˆæ–°çš„pairs $&lt;d_k, d_j&gt;$ï¼Œä½œä¸ºåˆ¤åˆ«å™¨çš„è¾“å…¥ã€‚è¿™æ ·çš„ç›®çš„å°±æ˜¯è®¤ä¸º $d_k$ çš„ç›¸ä¼¼åº¦é«˜äº $d_j$ çš„æƒ…å†µä¸‹ï¼Œè®© $d_k$ å°½å¯èƒ½çš„å»ä¸ $d_i$ ç›¸ä¼¼ã€‚ åœ¨å‰é¢ä»‹ç»äº†ç”Ÿæˆå™¨ $p_{\\theta}(d|q,r)$ å®é™…ä¸Šå°±æ˜¯ softmaxï¼Œçœ‹å…¬å¼(2). å¯¹äºpairwiseçš„å½¢å¼,$d_j$ ä¹Ÿä½œä¸ºç”Ÿæˆå™¨çš„è¾“å…¥ä¹‹ä¸€ï¼Œå¯¹åº”çš„ç”Ÿæˆå™¨æ˜¯å¦ä¸€ç§ softmax: å…¶ä¸­ $g_{\\theta}(q,d)$ is a task-specific real-valued function reflecting the chance of d being generated from q.","link":"/2019/08/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-7-IRGAN/"},{"title":"ä»0å¼€å§‹GAN-3-æ–‡æœ¬ç”Ÿæˆplanning","text":"å†™è¿™ç¯‡åšå®¢æºäºåœ¨çŸ¥ä¹ä¸Šçœ‹åˆ°å¤§ä½¬Towseråœ¨ â€œBERTæ¨¡å‹åœ¨NLPä¸­ç›®å‰å–å¾—å¦‚æ­¤å¥½çš„æ•ˆæœï¼Œé‚£ä¸‹ä¸€æ­¥NLPè¯¥ä½•å»ä½•ä»ï¼Ÿâ€ è¿™ä¸ªé—®é¢˜ä¸‹çš„å›ç­”ï¼Œå¯¹äºæ–‡æœ¬ç”Ÿæˆçš„æ€»ç»“è§‰å¾—å¤ªèµäº†ã€‚æ‰€ä»¥åŸºäºå¤§ä½¬çš„å›ç­”ï¼Œç”»äº†ä¸€ä¸ªè„‘å›¾(http://www.xmind.net/m/AcA3bE)ï¼Œæ¥ä¸‹æ¥ä¸€ä¸¤ä¸ªæœˆçš„æ—¶é—´ä¹Ÿå†³å®šæŒ‰ç…§è¿™ä¸ªè·¯çº¿è¿›è¡Œå­¦ä¹ ã€‚ Reward Augmented Maximum Likelihood for Neural Structured PredictionMotivationMaximum likilihood based methodå¯¹äºNMTæˆ–è€…å…¶ä»–çš„ conditional generationï¼Œæœ€å¸¸ç”¨çš„seq2seqæ¨¡å‹æ˜¯åŸºäºmaximum likilihood(ML)æ¥æœ€å°åŒ–ä¸‹é¢è¿™ä¸ªç›®æ ‡å‡½æ•°çš„ï¼š $$L_{ML}=\\sum_{(x,y^* )\\in D}-logp_{\\theta}(y^* |x)$$ ä½†æ˜¯è¿™ç§æ–¹å¼å­˜åœ¨å‡ ä¸ªé—®é¢˜: Minimizing this objective increases the conditional probability of the target outputs, $logp_{\\theta}p(y^* |x)$, while decreasing the conditional probability of alternative incorrect outputs. According to this objective, all negative outputs are equally wrong, and none is preferred over the others. åœ¨æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°ï¼Œæ„å‘³ç€å¢åŠ  ground truth outputçš„æ¦‚ç‡ $logp_{\\theta}p(y^* |x)$ï¼Œå‡å°‘å…¶ä»–é”™è¯¯è¾“å‡ºçš„æ¦‚ç‡ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¯¹äºé”™è¯¯çš„outputï¼Œæ¨¡å‹è®¤ä¸ºæ‰€æœ‰çš„negative outputéƒ½æ˜¯åŒç­‰çš„ï¼Œè¿™å…¶å®æ˜¯ä¸å¤ªæ­£ç¡®çš„ã€‚ Generating Sentences from a Continuous Space:However, by breaking the model structure down into a series of next-step predictions, the rnnlm does not expose an interpretable representation of global features like topic or of high-level syntactic properties. å°†ç»“æ„åŒ–è¾“å‡ºçš„é¢„æµ‹é—®é¢˜ï¼Œåˆ†è§£æˆä¸€ç³»åˆ—word prediction(seq2seqåœ¨è®­ç»ƒé˜¶æ®µï¼Œå…¶ç›®æ ‡å‡½æ•°çš„lossæ˜¯å°†æ‰€æœ‰çš„wordå¯¹åº”çš„cross entropyåŠ èµ·æ¥ï¼Œå¹¶æ²¡æœ‰å°†sentenceä½œä¸ºä¸€ä¸ªæ•´ä½“æ¥è¿›è¡Œä¼˜åŒ–)ï¼Œæ‰€ä»¥ä½¿å¾—æ¨¡å‹å¾ˆéš¾å­¦åˆ°global featureï¼Œç±»ä¼¼äºtopicæˆ–è€…high-levelçš„å¥æ³•ç‰¹æ€§ã€‚ exposure biasçš„é—®é¢˜ã€‚Quantifying Exposure Bias for Neural Language Generation RL based method$$L_{RL}(\\theta;\\tau,D)=\\sum_{(x,y^* )\\in D}{-\\tau\\mathbb{H}(p_{\\theta}(y^* |x))-\\sum_{y\\in \\mathbb{Y}}p_{\\theta}(y|x)r(y,y^* )}\\quad{(1)}$$ D è¡¨ç¤º training parallel data. $\\mathbb{H}(p)$ è¡¨ç¤ºæ¦‚ç‡åˆ†å¸ƒ $p_{\\theta}$ å¯¹åº”çš„äº¤å‰ç†µ, $H(p(y))=\\sum_{y\\in \\mathbb{Y}p(y)logp(y)}$. $\\tau$ è¡¨ç¤º temperature parameterï¼Œæ˜¯ä¸€ä¸ªè¶…å‚ã€‚è¿™ä¸ªå…¬å¼çš„ç†è§£å¯ä»¥ä¸ä¸Šä¸€ç¯‡blogä¸­seqgançš„å…¬å¼å¯¹åº”èµ·æ¥ï¼š $$J(\\theta)=E[R_T|s_0,\\theta]=\\sum_{y_1\\in V}G_{\\theta}(y_1|s_0)\\cdot Q_{D_{\\phi}}^{G_{\\theta}}(s_0,y_1)\\quad{(2)}$$ ï¼ˆ1ï¼‰å¼ä¸­çš„ç¬¬2é¡¹å°±æ˜¯ï¼ˆ2ï¼‰å¼ã€‚é‚£ä¹ˆï¼ˆ1ï¼‰å¼ä¸­çš„ç¬¬ä¸€é¡¹è¡¨ç¤ºçš„æ˜¯Maximum likilihoodçš„äº¤å‰ç†µï¼Ÿ ä½¿ç”¨RL basedçš„æ–¹æ³•å­˜åœ¨è¿™æ ·ä¸¤ä¸ªé—®é¢˜ï¼š ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™SGDæ¥ä¼˜åŒ– $L_{RL}(\\theta;\\tau)$ éå¸¸å›°éš¾ï¼Œå› ä¸ºrewardå¯¹åº”çš„gradientsçš„æ–¹å·®å¾ˆå¤§(large variance). æ²¡èƒ½æœ‰æ•ˆåˆ©ç”¨åˆ°ç›‘ç£ä¿¡æ¯ã€‚ ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½ç»“åˆMLå’ŒRLçš„ä¼˜åŠ¿ã€‚ RAMIä½œè€…åœ¨output spaceå®šä¹‰äº†ä¸€ä¸ª exponentiated payoffdistribution, è¡¨ç¤ºMLå’ŒRLçš„central distributionï¼š å…¶ä¸­ $Z(y^* ,\\tau)=\\sum_{y\\in \\mathbb{Y}}exp{r(y, y^* )/\\tau}$. ç®€å•ç‚¹ç†è§£å°±æ˜¯åŸºäº $r(y,y^* )$ è®¡ç®—å¾—åˆ°çš„reward rï¼Œç„¶åsoftmaxå¾—åˆ°çš„åˆ†å¸ƒã€‚ $$q(y|y^* ;\\tau)=\\dfrac{1}{Z(y^* ,\\tau)}exp{r(y, y^* )/\\tau}\\quad(3)$$ æ˜¾ç„¶ï¼Œè¿™ä¸ª $r(y,y^* )$ çš„è®¡ç®—æ˜¯åŸºäº BLEU æ¥è®¡ç®—çš„ã€‚è¿™æ ·ä¸€æ¥ï¼Œæ—¢è€ƒè™‘åˆ°äº†ä¸åŒçš„ y ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œä¹Ÿå°† BLEU çš„è®¡ç®—è½¬æ¢æˆäº† distribution. ç„¶åä½œè€…æ¨å¯¼äº†å„ç§å…¬å¼è¯æ˜äº†ä»MLçš„è§’åº¦æ¥ä¼˜åŒ– $q(y|y^* ;\\tau)$ å’Œ $p_{\\theta}(y|x)$ çš„KLæ•£åº¦ç­‰æ•ˆäºä¼˜åŒ– $L_{RL}$. æ‰€ä»¥ reward-augmented maximum likelihood (RAML) çš„loss functionå¯ä»¥å†™æˆ: Note that the temperature parameter, $\\tau \\ge 0$, serves as a hyper-parameter that controls the smoothness of the optimal distribution around correct targets by taking into account the reward function in the output space. optimizationå¯¹äº $L_{RAMI}(\\theta;\\tau)$ çš„ä¼˜åŒ–å¾ˆç®€å•ï¼Œå°±æ˜¯ç›´æ¥é€šè¿‡ $q(y|y^* ;\\tau)$ æ¥samplingå‡º unbiased samples y. å¦‚æœè¶…å‚æ•° $\\tau=0$,é‚£ä¹ˆå°±åªèƒ½sampleå‡º $y^* $. å¯¹å…¬ç¤ºï¼ˆï¼—ï¼‰æ±‚å¯¼ï¼Œå¯ä»¥å¾—åˆ°ï¼š è¿™é‡Œ $q(y|y^* ;\\tau)$ æ˜¯é€šè¿‡ $y^* $ æ¥sample y,ä¸åŒ…å«éœ€è¦è®­ç»ƒçš„å‚æ•°çš„ã€‚æ‰€ä»¥ RAMI ä¹Ÿå°±æ˜¯ä¼˜åŒ–log-likelihood,ä¸è¿‡è¿™é‡Œçš„ y ä¸æ˜¯ground truthï¼Œè€Œæ˜¯åŸºäº ground truthå’Œè¯„ä¼°æŒ‡æ ‡metricæ¥sampleå¾—åˆ°çš„y. å¯¹æ¯”åŸºäº RL çš„ä¼˜åŒ–ï¼Œä½œè€…è¿›è¡Œäº†åæ§½ï¼š RLä¸­sampleå¾—åˆ°çš„æ ·æœ¬ y æ˜¯é€šè¿‡ç”Ÿæˆæ¨¡å‹å¾—åˆ°çš„ï¼Œè€Œä¸”è¿™ä¸ªmodelè¿˜æ˜¯ä¸æ–­è¿›åŒ–çš„ã€‚è¿™ä½¿å¾—è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢ï¼Œæ¯”å¦‚ seqgan ä¸­çš„roll-out policy. reward åœ¨é«˜ç»´outputç©ºé—´éå¸¸ç¨€ç–ï¼Œè¿™ä½¿å¾—ä¼˜åŒ–å¾ˆå›°éš¾ã€‚ actor-critique methods. Sampling from the exponentiated payoff distributionåœ¨é€šè¿‡å…¬å¼ï¼ˆ9ï¼‰è¿›è¡Œä¼˜åŒ–ä¹‹å‰ï¼Œéœ€è¦å…ˆé€šè¿‡ exponentiated payoff distribution $q(y|y^* ;\\tau)$ æ¥sampleå¾—åˆ° y. This sampling is the price that we have to pay to learn with rewards. è¿™ä¸ªsampleè¿‡ç¨‹ä¸RLç›¸æ¯”æ˜¯æ²¡æœ‰å‚æ•°çš„ï¼Œç¬é—´ç®€å•äº†å¾ˆå¤šå•Šã€‚ã€‚ é‚£ä¹ˆå…·ä½“æ˜¯æ€ä¹ˆsampleçš„å‘¢ï¼Œä½œè€…ä½¿ç”¨çš„åŸºäºedit distanceçš„æ–¹æ³•ã€‚ ç»™å®šçš„ground truth $y^* $ é•¿åº¦æ˜¯m åŸºäºedit distance $y^* $ sampleå‡ºä¸ $y^* $ è·ç¦»åœ¨ e èŒƒå›´å†…çš„sentences y, å…¶ä¸­ $e\\in {0,â€¦,2m}$. çŸ¥ä¹ä¸Šæœ‰å¤§ä½¬å¯¹è¿™ç¯‡paperåšäº†ä¸€ä¸ªç®€å•çš„æ€»ç»“, NLPå…«å¦æ¯æ—¥è°ˆ 2. RL: x â€“&gt; é€šè¿‡decoder sampleä¸€ä¸ªå¥å­yâ€™ â€“&gt; å’Œyè®¡ç®—metric â€“&gt; æŠŠmetricä½œä¸ºrewardï¼Œç®—policy gradient RAML: y â€“&gt; é€šè¿‡å’Œmetricå¯¹åº”çš„ä¸€ä¸ªdistribution sampleä¸€ä¸ªå¥å­y* â€“&gt; æŠŠy* ä½œä¸ºGTè¿›è¡ŒMLè®­ç»ƒ è¿™æ ·åšçš„å¥½å¤„æ˜¯RLçš„sampleæ˜¯æ ¹æ®decoder sampleï¼Œè€Œdecoderæœ‰å‚æ•°ï¼Œæ‰€ä»¥éœ€è¦policy gradientã€‚è€ŒRAMLï¼Œæ˜¯æ ¹æ®yï¼ˆtarget sentenceï¼‰æ¥sampleå¥å­ã€‚è¿™æ ·å°±æ²¡æœ‰å‚æ•°çš„é—®é¢˜ï¼Œä¹Ÿå°±ä¸éœ€è¦policy gradientäº†ã€‚ RAMLçœ‹èµ·æ¥å‡ ä¹å®Œç¾ï¼Œä¸å­˜åœ¨ä»»ä½•ä¼˜åŒ–é—®é¢˜ã€‚å¯å¤©ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤ã€‚RAMLçš„éš¾ç‚¹åœ¨äºå¦‚ä½•å°†Metricè½¬åŒ–æˆå¯¹åº”çš„distributionã€‚RAMLåªæä¾›äº†å°†è¯¸å¦‚edit distanceç­‰metricè½¬åŒ–æˆdistçš„æ–¹æ³•ï¼Œä½†å¯¹äºBLEUç­‰å´æ— èƒ½ä¸ºåŠ›ã€‚ æ‰€ä»¥ç›®å‰ä¸ºæ­¢ï¼ŒRAMLçš„ä¸»è¦è´¡çŒ®åœ¨äºè®©æˆ‘ä»¬ç†è§£RL language generationåˆ°åº•trainäº†ä¸ªå•¥ã€‚ç®€å•æ¥è¯´å°±æ˜¯ä¸å­¦ground truth distributionï¼Œè€Œå­¦ä¹ ä¸€ä¸ªè·Ÿmetricç›¸å…³çš„dense distributionã€‚è¿™ä¹ˆåšçš„å¥½å¤„æ˜¯yçš„distributionæ›´å¤§ï¼Œç›¸å¯¹æ¥è¯´æ›´å®¹æ˜“å­¦ä¹  å…³äºç»“æ„åŒ–é¢„æµ‹related work(a) supervised learning approaches that ignore task reward and use supervision; (b) reinforcement learning approaches that use only task reward and ignore supervision; (c) hybrid approaches that attempt to exploit both supervision and task reward. Generating Sentences from a Continuous Space Samuelè¿™æ˜¯éå¸¸æ—©æœŸçš„ä¸€ç¯‡åŸºäºå˜åˆ†è‡ªç¼–ç åšæ–‡æœ¬ç”Ÿæˆçš„è®ºæ–‡ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“VAEå’ŒGANæ˜¯éå¸¸ç±»ä¼¼çš„ã€‚æ‰€ä»¥åœ¨çœ‹ GAN text generationç›¸å…³çš„paperä¹‹å‰å…ˆå­¦ä¹ ä¸‹å¦‚ä½•ç”¨VAEåšæ–‡æœ¬ç”Ÿæˆã€‚ å…³äº VAE æœ‰ä¸¤ç¯‡éå¸¸ä¸é”™çš„blog: è‹å‰‘æ—å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆä¸€ï¼‰ï¼šåŸæ¥æ˜¯è¿™ä¹ˆä¸€å›äº‹ Variational Autoencoders Explained ä½•ä¸º VAEMotivationä¼ ç»Ÿ RNNLM åœ¨åštextç”Ÿæˆçš„æ—¶å€™ï¼Œå…¶ç»“æ„æ˜¯æŠŠä¸€ä¸ªåºåˆ—breakingæˆä¸€ä¸ªä¸ªnext wordçš„prediction. è¿™ä½¿å¾—æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾ç¤ºçš„è·å–æ–‡æœ¬çš„å…¨å±€ä¿¡æ¯ï¼Œä¹Ÿæ²¡æœ‰è·å–ç±»ä¼¼äºtopicå’Œå¥æ³•ç›¸å…³çš„é«˜çº§ç‰¹å¾ã€‚ äºæ˜¯ä¹ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºvatiational encoderçš„æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆè·å–global featureï¼Œå¹¶ä¸”èƒ½é¿å… MLM å¸¦æ¥çš„å‡ ä¹ä¸å¯èƒ½å®Œæˆçš„è®¡ç®—ã€‚åŒæ—¶ï¼Œä½œè€…è®¤ä¸ºåŸºäºä¼ ç»Ÿçš„language modelçš„éªŒè¯æ–¹æ³•å¹¶ä¸èƒ½æœ‰æ•ˆå±•ç¤ºå‡ºglobal featureçš„å­˜åœ¨ï¼Œäºæ˜¯æå‡ºäº†ä¸€ç§æ–°çš„ evaluation strategy. For this task, we introduce a novel evaluation strategy using an adversarial classifier, sidestepping the issue of intractable likelihood computations by drawing inspiration from work on non-parametric two-sample tests and adversarial training.","link":"/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-3-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90planning/"},{"title":"ä»0å¼€å§‹GAN-6-pretraining for NLG","text":"related papers BERT BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model GPT/GPT-2.0 MASS: Masked Sequence to Sequence Pre-training for Language Generation Unified Language Model Pre-training for Natural Language Understanding and Generation Pretraining for Conditional Generation with Pseudo Self Attention Transformer-XL XLNet Defending Against Neural Fake News ERNIE WWM SpanBERT cross-lingual word embeddingA survey of cross-lingual word embedding models, Ruder et al.2017 Word translation without parallel data. Conneau et al.2017 Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Artetxe et al.2018 contextual word embeddingELMo Word2vec Glove GPT ULMFT: Universal language model fine-tuning for text classificatio Cross-lingual language model pretraining Polyglot contextual representations im- prove crosslingual transfer pre-trained for NMTTowards Making the Most of BERT in Neural Machine Translation Unsupervised Pretraining for Sequence to Sequence Learning When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation? Cross-lingual Language Model Pretraining XLMpaper: Cross-lingual Language Model Pretraining ä½œè€…æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥å­¦ä¹  cross-lingual è¯­è¨€æ¨¡å‹ã€‚å…¶ä¸­ä¸€ç§æ˜¯ä»…åŸºäº monolingual data, å¦ä¸€ç§æ˜¯åŸºäºå¹³è¡Œè¯­æ–™ã€‚åœ¨ cross-lingal ç›¸å…³çš„ä»»åŠ¡ä¸Šéƒ½æœ‰å¾ˆå¤§çš„æå‡ã€‚æ¯”å¦‚ XNLIï¼Œunsupervised machine translation, ä»¥åŠ supervised machine tranlsation. ç°æœ‰çš„åœ¨NLPé¢†åŸŸçš„å‘å±•ä¸»è¦æ˜¯å›´ç»•è‹±æ–‡è¿›è¡Œçš„ï¼Œä¸€äº›start-of-the-artæˆ–è€…NLPä»»åŠ¡çš„benchmarkséƒ½æ˜¯ä»¥è‹±æ–‡ä¸ºåŸºç¡€çš„ã€‚å…¶ä»–çš„ä¸€äº›è¯­è¨€å—é™äºè¯­æ–™çš„é—®é¢˜ï¼Œå‘å±•ç›¸å¯¹ç¼“æ…¢ã€‚è¿‘æœŸéšç€cross-lingual sentence representationçš„å‘å±•ï¼Œæ¶ˆé™¤English-centric bias,å¹¶ä¸”æ„å»ºä¸€ä¸ªé€šç”¨çš„cross-lingual encoderæ¥è®²ä»»ä½•è¯­è¨€çš„sentenceç¼–ç åˆ°å…±äº«çš„embeddingç©ºé—´æˆä¸ºå¯èƒ½ã€‚ Shared sub-word vocabulary ä½¿ç”¨ bpe,å¹¶ä¸”ä¸åŒçš„languageå…±äº«è¯è¡¨. this greatly improves the alignment of embedding spaces across languages that share either the same alphabet or anchor tokens such as digits (Smith et al., 2017) or proper nouns. å…±äº«è¯è¡¨èƒ½æ˜¾è‘—æå‡é‚£äº›å…·æœ‰ç›¸åŒå­—æ¯è¡¨æˆ–è€…anchor token(æ•°å­—æˆ–ä¸“æœ‰åè¯)çš„è¯­è¨€ä¹‹é—´çš„å‘é‡ç©ºé—´çš„å¯¹é½ã€‚ ä½œè€…å…ˆä»ä¸åŒè¯­è¨€çš„monolingual dataä¸­ç­›é€‰å‡ºéƒ¨åˆ†dataï¼Œç„¶åå­¦ä¹ bpe splits. Sentences are sampled according to a multinomial distribution with probabilities ${q_i}_{i=1â€¦N}$, where: å…¶ä¸­ $n_i$ è¡¨ç¤ºç¬¬ i ä¸­è¯­è¨€ä¸­sentenceçš„æ€»æ•°ã€‚ $\\sum_{k=1}^nn_k$ è¡¨ç¤ºNç§è¯­è¨€æ‰€æœ‰çš„sentenceçš„æ€»æ•°ã€‚$p_i$ åˆ™è¡¨ç¤ºç¬¬ i ä¸­è¯­è¨€sampleçš„æ¦‚ç‡ã€‚è®¾å®š $\\alpha=0.5$ï¼Œè¿™æ ·èƒ½å¢åŠ  low-resource çš„æ¯”ä¾‹ï¼Œä»è€Œå‡è½» bias to high-resource language. ä½œè€…æ€»å…±æå‡ºäº†ä¸‰ç§ language model. æ¥ä¸‹æ¥ä¸€ä¸€ä»‹ç»ï¼š Causal Language Modeling (CLM) $p(w_t|w_1,â€¦,w_{t-1},\\theta)$ ä¹Ÿå°±æ˜¯æ™®é€šçš„ aotu-regressive è¯­è¨€æ¨¡å‹ã€‚ Character-Level Language Modeling with Deeper Self-Attention è¿™ç¯‡paperä½¿ç”¨çš„self-attention, æˆ‘ä»¬çŸ¥é“self-attention ä¸åƒrnné‚£æ ·å…·æœ‰hidden stateçš„æ¦‚ç‡ï¼Œè¿™ç¯‡paperæŠŠä¸Šä¸€ä¸ªbatchä½œä¸ºä¸‹ä¸€ä¸ªbatchçš„contextï¼Œæœ‰ç‚¹ç±»ä¼¼äº transformer-XL,ä½†æ˜¯è¿™å¯¹äºcross-lingualä¸å¤ªé€‚åˆï¼Œæ‰€ä»¥è¿™é‡Œçš„ CLM ä¸ä¼ ç»Ÿçš„language modelå®Œå…¨ä¸€è‡´ã€‚ Masked Language Modeling (MLM) ä¸ BERT ä¸­MLMçš„åŒºåˆ«ï¼š Differences between our approach and the MLM of Devlin et al. (2018) include the use of text streams of an arbitrary number of sentences (truncated at 256 tokens) instead of pairs of sentences. æ–‡æœ¬ stream æ˜¯ä»»æ„æ•°é‡çš„sentencesï¼Œè€Œä¸æ˜¯pairs.ï¼ˆè¿™é‡Œçš„pairs in BERTåº”è¯¥æŒ‡çš„æ˜¯ next sentence prediction.ï¼‰ åœ¨ç­›é€‰ç”¨æ¥maskçš„è¯æ—¶ï¼Œä¸ºäº†å¤„ç† rare word å’Œ frequent word(punctuation or stop words) çš„ä¸å‡è¡¡é—®é¢˜: tokens in a text stream are sampled according to a multinomial distribution, whose weights are proportional to the square root of their invert frequencies. Translation Language Modeling (TLM) åœ¨é¢„æµ‹ä¸€ä¸ª masked english word çš„åŒæ—¶ï¼Œä¸ä»…å¯ä»¥attend english contextï¼Œä¹Ÿå¯ä»¥ attend franch translation. Cross-lingual Language Models å¦‚ä½•ä½¿ç”¨è¿™ä¸‰ç§è¯­è¨€æ¨¡å‹ï¼ŒCLM å’Œ MLM åœ¨å•è¯­ä¸Šè¿›è¡Œè®­ç»ƒã€‚ TLM åœ¨å¹³è¡Œè¯­æ–™ä¸Šè®­ç»ƒã€‚TLM åœ¨ä½¿ç”¨æ—¶æ˜¯è”åˆ MLM ä¸€èµ·è®­ç»ƒçš„ï¼Œè¿­ä»£ä¼˜åŒ–ä¸¤ä¸ªç›®æ ‡å‡½æ•°ã€‚ In this work, we consider cross-lingual language model pretraining with either CLM, MLM, or MLM used in combination with TLM. For the CLM and MLM objectives, we train the model with batches of 64 streams of continuous sentences composed of 256 tokens. At each iteration, a batch is composed of sentences coming from the same language, which is sampled from the distribution ${q_i}_{i=1â€¦N}$ above, with Î± = 0.7. When TLM is used in combination with MLM, we alternate between these two objectives, and sample the language pairs with a similar approach. CTNMTpaper: [Towards Making the Most of BERT in Neural Machine Translation Jiacheng](https://arxiv.org/abs/1908.05672) ByteDance çš„ä¸€ç¯‡paper. å‰äººçš„ç ”ç©¶ä¸­æˆ‘ä»¬å‘ç°ï¼ŒBERT pretrian å¯¹NMTå‡ ä¹æ²¡æœ‰æå‡ã€‚ä½œè€…è®¤ä¸ºå…¶åŸå› æ˜¯ï¼ŒNMT ç›¸å¯¹å…¶ä»–linguisticçš„ä»»åŠ¡ï¼Œè®­ç»ƒçš„stepsä¼šå¤šå¾ˆå¤šï¼Œæ¯”å¦‚NMTä¸€èˆ¬æ˜¯10ä¸‡stepï¼Œè€Œ POS taggingåªéœ€è¦å‡ ç™¾æ­¥ã€‚è¿™ä½¿å¾—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‚æ•°çš„æ›´æ–°å¤ªå¤šå¯¼è‡´ catastrophic forgetting problem. ä¹Ÿå°±æ˜¯ BERT è®­ç»ƒå¾—åˆ°çš„knowledgeå¹¶ä¸èƒ½ç»™NMTä»£æ¥æå‡ã€‚ äºæ˜¯ä¹ï¼Œä½œè€…è®¤ä¸ºåªæ˜¯å¤§å®¶æ²¡æœ‰å¥½å¥½åˆ©ç”¨BERTè€Œå·²ï¼Œåƒæˆ‘ä»¬è¿™æ ·æ, BERTè¿˜æ˜¯èƒ½å¯¹NMTæœ‰æå‡çš„ã€‚ç„¶åæå‡ºäº†ä¸‰ç§techniques: Asymptotic Distillation æ¸è¿‘è’¸é¦ï¼Œä¸»è¦æ˜¯ç”¨æ¥è§£å†³ catastrophic forgetting è¿™ä¸€é—®é¢˜çš„ï¼Œå’Œè¿™ç¯‡paper â€œOvercoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translationâ€œ ç›¸ä¼¼ï¼Œä¹Ÿæ˜¯é‡‡ç”¨çš„ Elastic Weight Consolidation(EWC) çš„æ–¹æ³•ï¼Œå¯¹weighté‡‡ç”¨MSEçš„çº¦æŸã€‚ $$L_{kd}=-||\\hat h^{lm}-h_l||^2_2$$ $$L=\\alpha\\cdot L_{nmt}+(1-\\alpha)\\cdot L_{kd}$$ å…¶ä¸­ $L_{kd}$ æ˜¯æ­£åˆ™åŒ–é¡¹ï¼Œ$\\hat h^{lm}$ æ˜¯ç»è¿‡BERTç¼–ç ä¹‹åçš„ sentence embedding. $h_l$ åˆ™æ˜¯NMTçš„encodedä¹‹åçš„ sentence embedding. ä½œè€…éƒ½ä½¿ç”¨çš„æœ€åä¸€å±‚çš„è¡¨ç¤ºã€‚åœ¨åç»­å®éªŒä¸­ï¼Œä½œè€…ä¹Ÿæµ‹è¯•äº†ä¸åŒå±‚çš„è¡¨ç¤ºè¿›è¡Œçº¦æŸã€‚ Dynamic Switch åŠ¨æ€å¼€å…³ã€‚ å°±æ˜¯GRUä¸­çš„gateæœºåˆ¶ã€‚ $$g = \\sigma(Wh^{lm} + Uh^{nmt} + b)$$ $$h=g\\odot h^{lm}+(1-g)\\odot h^{nmt}$$ Rate-scheduled learning slanted triangular learning, æ–œä¸‰è§’å­¦ä¹ ç‡ã€‚æœ€å¼€å§‹æå‡ºæ˜¯åœ¨ ULMFT: Universal language model fine-tuning for text classificatio è¿™ç¯‡è®ºæ–‡ä¸­ã€‚ $$\\theta_t=\\theta_{t-1}-\\eta\\nabla_{\\theta}L(\\theta)$$ å¯¹ NMT å’Œ LM å¯¹åº”çš„å‚æ•°ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼Œä½†æ˜¯éƒ½é‡‡ç”¨è¿™ç§scheduledå­¦ä¹ ç‡. Result","link":"/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/"},{"title":"ä»0å¼€å§‹GAN-9-metric for NLG","text":"related papers: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Why We Need New Evaluation Metrics for NLG Beyond BLEU: Training Neural Machine Translation with Semantic Similarity Better Rewards Yield Better Summaries: Learning to Summarise Without References RUSE: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation ROUGE: A Package for Automatic Evaluation of Summaries Chin-Yew MoverScoreMotivationè¯„ä»·æŒ‡æ ‡å¯¹äºæ¨¡å‹çš„è®­ç»ƒæˆ–é€‰æ‹©è‡³å…³é‡è¦ï¼Œç°é˜¶æ®µå¯¹äºæ–‡æœ¬ç”Ÿæˆçš„æ¨¡å‹ï¼ˆæœºå™¨ç¿»è¯‘ï¼Œæ‘˜è¦ç”Ÿäº§ï¼Œå›¾åƒæ ‡é¢˜ç”Ÿæˆï¼‰å¤§éƒ½é‡‡ç”¨çš„hard matchçš„æ–¹å¼ï¼Œæ¯”å¦‚ BLEU, ROUGE. è¿™äº›éƒ½æ˜¯ç®€å•çš„åŸºäºå…±ç°è¯çš„ç»Ÿè®¡ï¼Œè¿™ç§metricä»…ä»…åªè€ƒè™‘äº†è¡¨é¢çš„å½¢å¼ï¼Œæ— æ³•è¦†ç›–åŒæ„å´ä¸åŒè¯çš„è¡¨è¾¾ï¼Œæ‰€ä»¥ä»–ä»¬å¹¶ä¸å¤ªå¥½ï¼ˆover correctionï¼‰ï¼Œä¸å…·å¤‡è¯„ä»·æ–‡æœ¬ç›¸ä¼¼æ€§çš„èƒ½åŠ›ã€‚ MoverScore It is particularly important for a metric to not only capture the amount of shared content between two texts, i.e., intersect(A,B), as is the case with many semantic textual similarity measures æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦æ¥è®¡ç®—è·ç¦»ã€‚ è®¡ç®— MoverScore çš„ä¸€äº›ç¬¦å·ï¼š sentenceï¼š$x=(x_1,x_2,â€¦,x_m)$ $x^n$ è¡¨ç¤º x ä¸­çš„ n-gram. $f_{x^n}$ è¡¨ç¤ºxä¸­æ¯ä¸€ä¸ª n-gram çš„æƒé‡ã€‚å¦‚æœ n=(size of sentence), é‚£ä¹ˆ $f_{x^n}=1$ n-gram ä¹‹é—´çš„è·ç¦»ï¼š $$C_{ij}=d(x_i^n,y_j^n)$$ è¡¨ç¤º x ä¸­ç¬¬ $i^{th}$ ä¸ª n-gram ä¸ y ä¸­ç¬¬ $j^{th}$ ä¸ª n-gram çš„è·ç¦»ã€‚ é‚£ä¹ˆä¸¤ä¸ªå¥å­ä¸­æ‰€æœ‰ n-gram çš„è·ç¦» Word Moverâ€™s Distance (WMD)ï¼š $$WMD(x^n,y^n):=min_{F\\in R^{|x^n|\\times |y^n|}}&lt;C,F&gt;$$ $&lt;&gt;$ è¡¨ç¤ºåŠ æƒæ±‚å’Œã€‚è®¡ç®—å‡ºä¸¤ä¸ª n-gram åºåˆ—çš„æ¨åœŸæœºè·ç¦»ä¸ä¼ ç»Ÿçš„æ¨åœŸæœºè·ç¦»ä¸å¤ªä¸€æ ·çš„åœ°æ–¹æ˜¯ï¼Œè¿™é‡Œæ¯ä¸ª n-gram è¿˜æœ‰æƒé‡ã€‚ é‚£ä¹ˆå¦‚ä½•è®¡ç®—ä¸¤ä¸ª n-gram çš„è·ç¦» $d(x_i^n,y_j^n)$ å‘¢, ä½œè€…é‡‡ç”¨çš„æ˜¯ Euclidean distanceï¼š $$d(x_i^n,y_j^n)=||E(x_i^n)-E(y^n_j)||_ {2}$$ $E$ æ˜¯n-gram çš„å‘é‡è¡¨ç¤ºï¼Œæ¯”å¦‚ $x_i^n=(x_i,..,x_{i+n-1})$ æ˜¯ x ä¸­ç¬¬ i ä¸ª n-gram. $$E{(x_i^n)}=\\sum_{k=i}^{i+n-1}idf{(x_k)}\\cdot E{(x_k)}$$ n-gram çš„æƒé‡è®¡ç®—ï¼š $$f_{x_i^n}=\\dfrac{1}{Z}\\sum_{k=i}^{i+n-1}idf{(x_k)}$$ Z æ˜¯å½’ä¸€åŒ–å¸¸æ•°ï¼Œä¹Ÿå°±æ˜¯æ€»å’Œå§ã€‚ å½“ n&gt;(size of sentence) æ—¶ï¼Œ$WMD(x^n,y^n)$ å˜æˆè®¡ç®—ä¸¤ä¸ªå®Œæ•´çš„å¥å­çš„è·ç¦»ï¼š $$SMD(x^n,y^n)=||E(x_1^{l_x})-E(y_1^{l_y})||$$ å…¶ä¸­ $l_x,l_y$ è¡¨ç¤ºä¸¤ä¸ªsentence çš„é•¿åº¦ã€‚ Contextualized Representationså¦‚ä½•å¾—åˆ°ä¸€ä¸ª word/n-gram çš„å‘é‡è¡¨ç¤ºï¼ŒåŸºäºé¢„è®­ç»ƒçš„æ¨¡å‹æ¥å¾—åˆ° contextualized è¡¨ç¤ºæ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„é—®é¢˜ï¼ŒElmoå’ŒBERTéƒ½æ˜¯å¤šå±‚ç»“æ„ï¼Œä¸åŒçš„layeråŒ…å«äº†ä¸åŒçš„å«ä¹‰ã€‚ä½œè€…è¿™é‡Œæåˆ°äº†ä¸¤ç§æ–¹æ³•ï¼Œå¹¶æœ€ç»ˆé‡‡ç”¨äº†å‰è€…ï¼š the concatenation of power means a routing mechanism for aggregation power means: $$h_i(p)=(\\dfrac{z_{i,1}^p+â€¦+z_{i,L}^p}{L})^{1/p}$$ L è¡¨ç¤ºé¢„è®­ç»ƒæ¨¡å‹çš„å±‚æ•°ï¼Œp=1æ˜¯æ•°å€¼å¹³å‡ï¼Œp=0æ—¶æ˜¯è°ƒå’Œå¹³å‡ã€‚ $$E(x_i)=h_i^{p_1}\\oplus â€¦. \\oplus h_i^{p_k}$$ $\\oplus$ è¡¨ç¤º concatenation. ä½œè€…è®¾ç½® p=1,K=3. ä¹Ÿå°±æ˜¯ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºç”±ä¸‰ä¸ªå‘é‡è¡¨ç¤º $h$ æ‹¼æ¥è€Œæˆ,è€Œæ¯ä¸ªhåˆæ˜¯ä¸åŒå±‚çš„æ•°å€¼å¹³å‡ã€‚ resultå¯¹äºè¿™ç§æå‡ºæ–°æŒ‡æ ‡çš„é—®é¢˜ï¼Œä¸€ç›´å¾ˆç–‘æƒ‘æ€ä¹ˆå» evaluationã€‚å¥½åƒåªèƒ½é€šè¿‡äººå·¥å»è¯„ä»·äº†å¯¹å§ï¼Ÿ è¿™æ˜¯æœºå™¨ç¿»è¯‘çš„ç»“æœã€‚ WMD-1/2+BERT+MNLI+PMeansï¼šè¡¨ç¤º 1-gram çš„word mover distences + åœ¨NMLIè¯­æ–™ä¸Šè®­ç»ƒçš„BERT + PMeans çš„èåˆæ–¹å¼ã€‚ æ ¹æ® NMT system å¾—åˆ° translationsï¼Œç„¶åä¸ references è®¡ç®—å¯¹åº”çš„æŒ‡æ ‡ã€‚ç„¶åæ ¹æ®æŒ‡æ ‡ä¸human evalationç›¸ä¼¼åº¦è¿›è¡Œå¯¹æ¯”ï¼Œè¶Šæ¥è¿‘äººç±»è¯„ä»·çš„ï¼Œè¿™ä¸ªæŒ‡æ ‡å°±è¶Šå¥½ã€‚","link":"/2019/10/31/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-9-metric-for-NLG/"},{"title":"ä»0å¼€å§‹GAN-8-RL in NMT","text":"related papers: A Study of Reinforcement Learning for Neural Machine Translation Bilingual-GAN: A Step Towards Parallel Text Generation [On the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://arxiv.org/pdf/1907.01752.pdf) æš‚æ—¶æœ‰ä¸ªideaï¼Œæ ¹æ®é‚£ç¯‡paper, beyond bleu æå‡ºçš„metricæ¥ä½œä¸ºä¼˜åŒ–æŒ‡æ ‡ã€‚å¥½å¤„æ˜¯ï¼Œå¯¹äºæ¯ä¸€ä¸ªtokenç”Ÿæˆçš„æ—¶å€™ï¼Œä¸éœ€è¦æ¥ç€ç”Ÿæˆå®Œæ•´çš„å¥å­å°±èƒ½å¾—åˆ°æœ‰æ•ˆçš„rewardï¼ˆè¿™ç‚¹éœ€è¦ç”¨å®éªŒæ¥éªŒè¯ï¼‰ã€‚è¿™æ ·å¯¹äºæ¯ä¸ªå¥å­ä¸­çš„tokenéƒ½ä¼šæœ‰å¯¹åº”çš„rewards,æœ€å¥½å¯ä»¥ç»™æ¯ä¸ªrewardsä¸€ä¸ªæŠ˜æ‰£å› å­ï¼Œè¶Šé å‰çš„ç³»æ•°è¶Šå°ï¼Œè¶Šé åçš„ç³»æ•°è¶Šå¤§ã€‚ RL in NMTpaper: [On the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://arxiv.org/pdf/1907.01752.pdf) Motivation","link":"/2019/10/11/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-8-RL-in-NMT/"},{"title":"æ–‡æœ¬åˆ†ç±»ç³»åˆ—1-fasttext","text":"åœ¨Facebook fasttext githubä¸»é¡µä¸­ï¼Œå…³äºfasttextçš„ä½¿ç”¨åŒ…æ‹¬ä¸¤ä¸ªæ–¹é¢ï¼Œè¯å‘é‡è¡¨ç¤ºå­¦ä¹ ä»¥åŠæ–‡æœ¬åˆ†ç±»ã€‚ è¯å‘é‡è¡¨ç¤ºå­¦ä¹ ï¼šEnriching Word Vectors with Subword Information æ–‡æœ¬åˆ†ç±»ï¼šBag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov Paper Reading1è¿™ç¯‡æ–‡ç« æ˜¯ç”¨æ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„: Bag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov è¿™ä¸ªæ¨¡å‹è·Ÿword2vec ä¸­çš„CBOwæ¨¡å‹æå…¶ç›¸ä¼¼ï¼ŒåŒºåˆ«åœ¨äºå°†ä¸­å¿ƒè¯æ¢æˆæ–‡æœ¬æ ‡ç­¾ã€‚é‚£ä¹ˆè¾“å…¥å±‚æ˜¯æ–‡æœ¬ä¸­å•è¯ç»è¿‡åµŒå…¥æ›¾ä¹‹åçš„è¯å‘é‡æ„æˆçš„çš„n-gramï¼Œç„¶åæ±‚å¹³å‡æ“ä½œå¾—åˆ°ä¸€ä¸ªæ–‡æœ¬sentenceçš„å‘é‡ï¼Œä¹Ÿå°±æ˜¯éšè—å±‚hï¼Œç„¶åå†ç»è¿‡ä¸€ä¸ªè¾“å‡ºå±‚æ˜ å°„åˆ°æ‰€æœ‰ç±»åˆ«ä¸­ï¼Œè®ºæ–‡é‡Œé¢è¿˜è¯¦ç»†è®ºè¿°äº†å¦‚ä½•ä½¿ç”¨n-gram featureè€ƒè™‘å•è¯çš„é¡ºåºå…³ç³»ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨Hierarchical softmaxæœºåˆ¶åŠ é€Ÿsoftmaxå‡½æ•°çš„è®¡ç®—é€Ÿåº¦ã€‚æ¨¡å‹çš„åŸç†å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š ç›®æ ‡å‡½æ•°ï¼š $$\\dfrac{1}{N}\\sum_{n=1}^Ny_nlog(f(BAx_n))$$ Nè¡¨ç¤ºæ–‡æœ¬æ•°é‡ï¼Œè®­ç»ƒæ—¶å°±æ˜¯Batch sizeå§ï¼Ÿ $x_n$ è¡¨ç¤ºç¬¬nä¸ªæ–‡æœ¬çš„ normalized bag of features $y_n$ è¡¨ç¤ºç¬¬nä¸ªæ–‡æœ¬çš„ç±»æ ‡ç­¾ A is the look up table over n-gram. ç±»ä¼¼äºattentionä¸­çš„æƒé‡å§ B is the weight matrix éšè—å±‚åˆ°è¾“å‡ºå±‚çš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(hk)$. hæ˜¯éšè—å±‚çš„ç»´åº¦ï¼Œkæ˜¯æ€»çš„ç±»åˆ«æ•°ã€‚ç»è¿‡hierarchical softmaxå¤„ç†åï¼Œå¤æ‚åº¦ä¸º $O(hlog_2k)$ è¿™ç§æ¨¡å‹çš„ä¼˜ç‚¹åœ¨äºç®€å•ï¼Œæ— è®ºè®­ç»ƒè¿˜æ˜¯é¢„æµ‹çš„é€Ÿåº¦éƒ½å¾ˆå¿«ï¼Œæ¯”å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹é«˜äº†å‡ ä¸ªé‡çº§ ç¼ºç‚¹æ˜¯æ¨¡å‹è¿‡äºç®€å•ï¼Œå‡†ç¡®åº¦è¾ƒä½ã€‚ paper reading2è¿™ç¯‡æ–‡ç« æ˜¯åœ¨word2vecçš„åŸºç¡€ä¸Šæ‹“å±•äº†ï¼Œç”¨æ¥å­¦ä¹ è¯å‘é‡è¡¨ç¤º Enriching Word Vectors with Subword Information AbstractPopular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. ä¹‹å‰çš„æ¨¡å‹åœ¨ç”¨ç¦»æ•£çš„å‘é‡è¡¨ç¤ºå•è¯æ—¶éƒ½å¿½ç•¥äº†å•è¯çš„å½¢æ€ã€‚ In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªskipgramæ¨¡å‹,å…¶ä¸­æ¯ä¸€ä¸ªå•è¯è¡¨ç¤ºä¸ºç»„æˆè¿™ä¸ªå•è¯çš„å­—è¢‹æ¨¡å‹ a bag of character n-grams. ä¸€ä¸ªå•è¯çš„è¯å‘é‡è¡¨ç¤ºä¸ºè¿™äº› n-gramsè¡¨ç¤ºçš„æ€»å’Œã€‚ Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information. We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach. è¿™ç¯‡æ–‡ç« å¯ä»¥çœ‹ä½œæ˜¯word2vecçš„æ‹“å±•ï¼Œä¸»è¦æ˜¯é’ˆå¯¹ä¸€äº›å½¢æ€ç‰¹åˆ«å¤æ‚çš„è¯­è¨€ã€‚ word2vecåœ¨è¯æ±‡å»ºæ¨¡æ–¹é¢äº§ç”Ÿäº†å·¨å¤§çš„è´¡çŒ®ï¼Œç„¶è€Œå…¶ä¾èµ–äºå¤§é‡çš„æ–‡æœ¬æ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œå¦‚æœä¸€ä¸ªwordå‡ºç°æ¬¡æ•°è¾ƒå°‘é‚£ä¹ˆå­¦åˆ°çš„vectorè´¨é‡ä¹Ÿä¸ç†æƒ³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ä½œè€…æå‡ºä½¿ç”¨subwordä¿¡æ¯æ¥å¼¥è¡¥è¿™ä¸€é—®é¢˜ï¼Œç®€å•æ¥è¯´å°±æ˜¯é€šè¿‡è¯ç¼€çš„vectoræ¥è¡¨ç¤ºè¯ã€‚æ¯”å¦‚unofficialæ˜¯ä¸ªä½é¢‘è¯ï¼Œå…¶æ•°æ®é‡ä¸è¶³ä»¥è®­ç»ƒå‡ºé«˜è´¨é‡çš„vectorï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡un+officialè¿™ä¸¤ä¸ªé«˜é¢‘çš„è¯ç¼€å­¦ä¹ åˆ°ä¸é”™çš„vectorã€‚æ–¹æ³•ä¸Šï¼Œæœ¬æ–‡æ²¿ç”¨äº†word2vecçš„skip-gramæ¨¡å‹ï¼Œä¸»è¦åŒºåˆ«ä½“ç°åœ¨ç‰¹å¾ä¸Šã€‚word2vecä½¿ç”¨wordä½œä¸ºæœ€åŸºæœ¬çš„å•ä½ï¼Œå³é€šè¿‡ä¸­å¿ƒè¯é¢„æµ‹å…¶ä¸Šä¸‹æ–‡ä¸­çš„å…¶ä»–è¯æ±‡ã€‚è€Œsubword modelä½¿ç”¨å­—æ¯n-gramä½œä¸ºå•ä½ï¼Œæœ¬æ–‡nå–å€¼ä¸º3~6ã€‚è¿™æ ·æ¯ä¸ªè¯æ±‡å°±å¯ä»¥è¡¨ç¤ºæˆä¸€ä¸²å­—æ¯n-gramï¼Œä¸€ä¸ªè¯çš„embeddingè¡¨ç¤ºä¸ºå…¶æ‰€æœ‰n-gramçš„å’Œã€‚è¿™æ ·æˆ‘ä»¬è®­ç»ƒä¹Ÿä»ç”¨ä¸­å¿ƒè¯çš„embeddingé¢„æµ‹ç›®æ ‡è¯ï¼Œè½¬å˜æˆç”¨ä¸­å¿ƒè¯çš„n-gram embeddingé¢„æµ‹ç›®æ ‡è¯ã€‚ Related workMorphological word representationsé’ˆå¯¹å½¢æ€è¯è¡¨ç¤ºå·²æœ‰çš„å·¥ä½œï¼š ä¼ ç»Ÿçš„ç”¨å•è¯çš„å½¢æ€ç‰¹å¾æ¥è¡¨ç¤ºå•è¯ï¼š [Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored neural language models. In Proc. NAACL] introduced factored neural language models. å› å¼åˆ†è§£æ¨¡å‹ words are represented as sets of features. These features might include morphological information SchÃ¼tze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing the four-grams representations. è¿™ç¯‡æ–‡æ­£çš„å·¥ä½œè·Ÿæœ¬æ–‡çš„æ–¹æ³•æ˜¯æ¯”è¾ƒæ¥è¿‘çš„ã€‚ Character level features for NLP NLPçš„å­—ç¬¦ç‰¹å¾å­—ç¬¦çº§åˆ«çš„ç ”ç©¶å·¥ä½œæœ€è¿‘å¾ˆå¤šäº†ã€‚ä¸€ç±»æ˜¯åŸºäºRNNçš„ï¼Œå¦ä¸€ç±»æ˜¯åŸºäºCNNçš„ã€‚ General Modelè¿™ä¸€éƒ¨åˆ†æ˜¯å¯¹word2vecä¸­è·³å­—æ¨¡å‹çš„å›é¡¾ã€‚Skip-gram predicts the distribution (probability) of context words from a center word. giving a sequence of words $w_1, w_2,â€¦,w_T$ é‚£ä¹ˆskipgram æ¨¡å‹å°±æ˜¯æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼š $$\\sum_{t=1}^T\\sum_{c\\in C_t}logp(w_c|w_t)$$ we are given a scoring function s which maps pairs of (word, context) to scores in R. $$p(w_c|w_t)=\\dfrac{e^{s(w_t,w_c)}}{\\sum_{j=1}^We^{s(w_t,j)}}$$ The problem of predicting context words can instead be framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. For the word at position t we consider all context words as positive examples and sample negatives at random from the dictionary. è¿™ç¯‡æ–‡ç« é‡‡ç”¨è´Ÿé‡‡æ ·çš„æ–¹æ³•ã€‚ä¸åŸæœ¬çš„softmaxæˆ–è€…æ˜¯hierarchical softmaxä¸ä¸€æ ·çš„æ˜¯ï¼Œè´Ÿé‡‡æ ·ä¸­é¢„æµ‹ä¸€ä¸ªä¸Šä¸‹æ–‡çš„å•è¯context $w_c$ æ˜¯æŠŠå®ƒçœ‹åšä¸€ä¸ªç‹¬ç«‹çš„äºŒåˆ†ç±»ï¼Œå­˜åœ¨æˆ–è€…æ˜¯ä¸å­˜åœ¨ã€‚ å› æ­¤é€‰æ‹©ä¸€ä¸ªä¸Šä¸‹æ–‡ position c, using binary logistic loss we obtain the following negative log-likelihood:: $$log(1+e^{-s(w_t,w_c)})+\\sum_{n\\in N_{t,c}}log(1+e^{s(w_t,n)})$$ å…¶å®è·Ÿword2vecä¸­æ˜¯ä¸€æ ·çš„å°±æ˜¯ $-log\\sigma(s(w_t,w_c))=-log\\dfrac{1}{1+e^{-s(w_t,w_c)}}=log(1+e^{-s(w_t,w_c)})$ é‚£ä¹ˆå¯¹äºæ•´ä¸ªSequenceï¼Œè®¾å®š $l: x\\rightarrow log(1+e^{-x})$ é‚£ä¹ˆï¼š $$\\sum_{t=1}^T[\\sum_{c\\in C_t}l(s(w_t,w_c))+\\sum_{n\\in N_{t,c}}l(-s(w_t,n))]$$ $N_{t,c}$ is a set of negative examples sampled from the vocabulary. æ€ä¹ˆé€‰è´Ÿé‡‡æ ·å‘¢ï¼Ÿ æ¯ä¸ªå•è¯éƒ½è¢«ç»™äºˆä¸€ä¸ªç­‰äºå®ƒé¢‘ç‡çš„æƒé‡ï¼ˆå•è¯å‡ºç°çš„æ•°ç›®ï¼‰çš„3/4æ¬¡æ–¹ã€‚é€‰æ‹©æŸä¸ªå•è¯çš„æ¦‚ç‡å°±æ˜¯å®ƒçš„æƒé‡é™¤ä»¥æ‰€æœ‰å•è¯æƒé‡ä¹‹å’Œã€‚ $$p(w_i)=\\dfrac{f(w_i)^{3/4}}{\\sum_{j=0}^W(f(w_j)^{3/4})}$$ Then the score can be computed as the scalar product between word and context vectors as: $$s(w_t,w_c) = u_{w_t}^Tv_{w_v}$$ Subword modelBy using a distinct vector representation for each word, the skipgram model ignores the internal structure of words. In this section, we propose a different scoring function s, in order to take into account this information. å•è¯çš„ç¦»æ•£è¯å‘é‡è¡¨ç¤ºæ˜¯å¿½ç•¥äº†å•è¯å†…éƒ¨çš„ç»“æ„ä¿¡æ¯çš„ï¼Œä¹Ÿå°±æ˜¯å…¶å­—æ¯ç»„æˆã€‚ ç»™æ¯ä¸ªå•è¯å·¦å³åŠ ä¸Š &lt; å’Œ &gt;ï¼Œç”¨æ¥åŒºåˆ†å‰ç¼€å’Œåç¼€ã€‚å¯¹äºå•è¯ where æ¥è¯´ï¼Œç”¨ character trigram è¡¨ç¤ºï¼š ç”¨ $z_g$ è¡¨ç¤ºn-gram g çš„å‘é‡è¡¨ç¤ºã€‚é‚£ä¹ˆ scoring function: $$s(w,c)=\\sum_{g\\in G_w}z_g^Tv_c$$ å¦‚æœè¯è¡¨å¾ˆå¤§çš„è¯ï¼Œå…¶å¯¹åº”çš„ n-gram ä¹Ÿä¼šéå¸¸å¤šå§ï¼Œä¸ºäº†é™åˆ¶å ç”¨çš„å†…å­˜ï¼Œwe use a hashing function that maps n-grams to integers in 1 to K. We hash character sequences using the Fowler-Noll-Vo hashing function (specifically the FNV-1a variant).1 We set $K = 2.10^6$ below. ä»£ç å®ç°éœ€è¦æ³¨æ„çš„é—®é¢˜ ä»£ç å®ç°ä¸­å¯¹äºsentenceçš„å‘é‡è¡¨ç¤ºï¼Œæ˜¯unigramçš„å¹³å‡å€¼ï¼Œå¦‚æœè¦è®©æ•ˆæœæ›´å¥½ï¼Œå¯ä»¥æ·»åŠ bigram, trigramç­‰ã€‚ tf.train.exponential_decay tf.nn.nce_loss","link":"/2018/05/23/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%971-fasttext/"},{"title":"æ–‡æœ¬åˆ†ç±»ç³»åˆ—5-Hierarchical Attention Networks","text":"Hierarchical Attention Networks for Document Classification paper readingä¸»è¦åŸç†ï¼š the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since **documents have a hierarchical structure (words form sentences, sentences form a document)**, we likewise construct a document representation by first building representations of sentences and then aggregating those into a document representation. Second, it is observed that different words and sentences in a documents are differentially informative. å¯¹äºä¸€ä¸ªdocumentå«æœ‰è¿™æ ·çš„å±‚æ¬¡ç»“æ„ï¼Œdocumentç”±sentencesç»„æˆï¼Œsentenceç”±wordsç»„æˆã€‚ the importance of words and sentences are highly context dependent, i.e. the same word or sentence may be differentially important in different context (x3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015) â€” one at the word level and one at the sentence level â€” that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document. wordså’Œsentenceséƒ½æ˜¯é«˜åº¦ä¸Šä¸‹æ–‡ä¾èµ–çš„ï¼ŒåŒä¸€ä¸ªè¯æˆ–sentenceåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå…¶è¡¨ç°çš„é‡è¦æ€§ä¼šæœ‰å·®åˆ«ã€‚å› æ­¤ï¼Œè¿™ç¯‡è®ºæ–‡ä¸­ä½¿ç”¨äº†ä¸¤ä¸ªattentionæœºåˆ¶ï¼Œæ¥è¡¨ç¤ºç»“åˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¯æˆ–å¥å­çš„é‡è¦ç¨‹åº¦ã€‚ï¼ˆè¿™é‡Œç»“åˆçš„ä¸Šä¸‹æ–‡çš„è¯æˆ–å¥å­ï¼Œå°±æ˜¯ç»è¿‡RNNå¤„ç†åçš„éšè—çŠ¶æ€ï¼‰ã€‚ Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014) attentionä¸ä»…æœ‰å¥½çš„æ•ˆæœï¼Œè€Œä¸”èƒ½å¤Ÿå¯è§†åŒ–çš„çœ‹è§å“ªäº›è¯æˆ–å¥å­å¯¹å“ªä¸€ç±»documentçš„åˆ†ç±»å½±å“å¤§ã€‚ æœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œè€ƒè™‘äº†ducumentä¸­sentenceè¿™ä¸€å±‚æ¬¡ç»“æ„ï¼Œå› ä¸ºå¯¹äºä¸€ä¸ªdocumentçš„åˆ†ç±»ï¼Œå¯èƒ½å‰é¢å‡ å¥è¯éƒ½æ˜¯åºŸè¯ï¼Œè€Œæœ€åä¸€å¥è¯æ¥äº†ä¸€ä¸ªè½¬æŠ˜ï¼Œå¯¹documentçš„åˆ†ç±»èµ·å†³å®šæ€§ä½œç”¨ã€‚è€Œä¹‹å‰çš„ç ”ç©¶ï¼Œåªè€ƒè™‘äº†documentä¸­çš„è¯ã€‚ Model Architecture GRU-based sequence encoderreset gate: controls how much the past state contributes to the candidate state. $$r_t=\\sigma(W_rx_t+U_rh_{t-1}+b_r)$$ candidate state: $$\\tilde h_t=tanh(W_hx_t+r_t\\circ (U_hh_{t-1})+b_h)$$ update gate: decides how much past information is kept and how much new information is added. $$z_t=\\sigma(W_zx_t+U_zh_{t-1}+b_z)$$ new state: a linear interpolation between the previous state $h_{tâˆ’1}$ and the current new state $\\tilde h_t$ computed with new sequence information. $$h_t=(1-z_t)\\circ h_{t-1}+z_t\\circ \\tilde h_t$$ Hierarchical AttentionWord Encoder$$x_{it}=W_ew_{it}, t\\in [1, T]$$ $$\\overrightarrow h_{it}=\\overrightarrow {GRU}(x_{it}),t\\in[1,T]$$ $$\\overleftarrow h_{it}=\\overleftarrow {GRU}(x_{it}),t\\in [T,1]$$ $$h_{it} = [\\overrightarrow h_{it},\\overleftarrow h_{it}]$$ i means the $i^{th}$ sentence in the document, and t means the $t^{th}$ word in the sentence. Word AttentionNot all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Attentionæœºåˆ¶è¯´åˆ°åº•å°±æ˜¯ç»™äºˆsentenceä¸­æ¯ä¸ªç»“åˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¯ä¸€ä¸ªæƒé‡ã€‚å…³é”®åœ¨äºè¿™ä¸ªæƒé‡æ€ä¹ˆç¡®å®šï¼Ÿ $$u_{it}=tanh(W_wh_{it}+b_w)$$ $$\\alpha_{it}=\\dfrac{exp(u_{it}^Tu_w)}{\\sum_t^Texp(u_{it}^Tu_w)}$$ $$s_i=\\sum_t^T\\alpha_{it}h_{it}$$ è¿™é‡Œé¦–å…ˆæ˜¯å°† $h_{it}$ é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚å¾—åˆ° hidden representation $u_{it}$,ç„¶åè®¡ç®— $u_{it}$ ä¸ $u_w$ çš„ç›¸ä¼¼æ€§ã€‚å¹¶é€šè¿‡softmaxå½’ä¸€åŒ–å¾—åˆ°æ¯ä¸ªè¯ä¸ $u_w$ ç›¸ä¼¼çš„æ¦‚ç‡ã€‚è¶Šç›¸ä¼¼çš„è¯ï¼Œè¿™ä¸ªè¯æ‰€å æ¯”é‡è¶Šå¤§ï¼Œå¯¹æ•´ä¸ªsentenceçš„å‘é‡è¡¨ç¤ºå½±å“è¶Šå¤§ã€‚ é‚£ä¹ˆå…³é”®æ˜¯è¿™ä¸ª $u_w$ æ€ä¹ˆè¡¨ç¤ºï¼Ÿ The context vector $u_w$ can be seen as a high level representation of a fixed query â€œwhat is the informative wordâ€ over the words like that used in memory networks (Sukhbaatar et al., 2015, End-to-end memory networks.; Kumar et al., 2015, Ask me anything: Dynamic memory networks for natural language processing.). The word context vector $u_w$ is randomly initialized and jointly learned during the training process. Sentence Encoder$$\\overrightarrow h_{i}=\\overrightarrow {GRU}(s_{i}),t\\in[1,L]$$ $$\\overleftarrow h_{i}=\\overleftarrow {GRU}(s_{i}),t\\in [L,1]$$ $$H_i=[\\overrightarrow h_{i}, \\overleftarrow h_{i}]$$ hi summarizes the neighbor sentences around sentence i but still focus on sentence i. Sentence Attention$$u_i=tanh(W_sH_i+b_s)$$ $$\\alpha_i=\\dfrac{exp(u_i^Tu_s)}{\\sum_i^Lexp(u_i^Tu_s)}$$ $$v = \\sum_i^L\\alpha_ih_i$$ åŒæ ·çš„ $u_s$ è¡¨ç¤ºï¼š a sentence level context vector $u_s$ Document ClassificationThe document vector v is a high level representation of the document and can be used as features for document classification: $$p=softmax(W_cv+b_c)$$ ä»£ç å®ç°éœ€è¦æ³¨æ„çš„é—®é¢˜ å¦‚æœä½¿ç”¨tensorboardå¯è§†åŒ– å˜é‡èŒƒå›´çš„é—®é¢˜ Context dependent attention weightsVisualization of attention","link":"/2018/06/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks/"},{"title":"ä»£ç å®ç°é«˜æ–¯æ··åˆæ¨¡å‹","text":"sklearnæºç é˜…è¯»ï¼Œç”¨emç®—æ³•è®¡ç®—é«˜æ–¯æ··åˆæ¨¡å‹GMM ä»£ç å®ç°é«˜æ–¯æ··åˆæ¨¡å‹å‚è€ƒè¿™ç¯‡åšå®¢Regularized Gaussian Covariance Estimationéå¸¸å€¼å¾—ä¸€è¯»ï¼ŒåŒäº‹è¿™ç¯‡åšå®¢å¾ˆæ·±å…¥çš„è®²äº†åæ–¹å·®æ€ä¹ˆæ±‚çš„é—®é¢˜ï¼Œåœ¨å‰æ–‡ä¸­æˆ‘ä¹Ÿæœ‰æåˆ°ï½ä½†æˆ‘è§£é‡Šçš„å¾ˆlowã€‚ã€‚ ä»£ç ç›´æ¥å°±çœ‹sklearné‡Œé¢çš„æºç å§ï½ç½‘ä¸Šå¾ˆå¤šä¸é è°±ã€‚ã€‚ã€‚ githubæºç  ç±»åˆå§‹åŒ–123456789101112131415161718192021222324252627282930313233343536373839404142434445class GaussianMixture(BaseMixture): &quot;&quot;&quot; Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a Gaussian mixture distribution. å¯¹æ··åˆçš„é«˜æ–¯åˆ†å¸ƒè¿›è¡Œå‚æ•°ä¼°è®¡ï½ &quot;&quot;&quot; def __init__(self, n_components=1, covariance_type='full', tol=1e-3, reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10): super(GaussianMixture, self).__init__( n_components=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval) # ä¸»è¦æ˜¯é’ˆå¯¹3ä¸ªè¦å­¦ä¹ çš„å‚æ•°çš„åˆå§‹åŒ– self.covariance_type = covariance_type # åæ–¹å·®çŸ©é˜µå½¢å¼ self.weights_init = weights_init # å¤šé¡¹å¼åˆ†å¸ƒï¼Œæ¯ä¸€ç±»çš„æ¦‚ç‡ self.means_init = means_init # å‡å€¼ (n_components, n_features) self.precisions_init = precisions_init # åæ–¹å·® å…ˆçœ‹åˆå§‹åŒ–æ„é€ å‡½æ•°ï¼Œå‚æ•°æ˜¯çœŸçš„å¤šã€‚ã€‚ã€‚ n_components=1: The number of mixture components.è¡¨ç¤ºæ··åˆç±»åˆ«çš„ä¸ªæ•°ï¼Œä¹Ÿå°±æ˜¯æ··åˆé«˜æ–¯åˆ†å¸ƒçš„ä¸ªæ•° covariance_type=â€™fullâ€™: åæ–¹å·®çŸ©é˜µçš„ç±»å‹ã€‚{â€˜fullâ€™, â€˜tiedâ€™, â€˜diagâ€™, â€˜sphericalâ€™} åˆ†åˆ«å¯¹åº”å®Œå…¨åæ–¹å·®çŸ©é˜µï¼ˆå…ƒç´ éƒ½ä¸ä¸ºé›¶ï¼‰ï¼Œç›¸åŒçš„å®Œå…¨åæ–¹å·®çŸ©é˜µï¼ˆHMMä¼šç”¨åˆ°ï¼‰ï¼Œå¯¹è§’åæ–¹å·®çŸ©é˜µï¼ˆéå¯¹è§’ä¸ºé›¶ï¼Œå¯¹è§’ä¸ä¸ºé›¶ï¼‰ï¼Œçƒé¢åæ–¹å·®çŸ©é˜µï¼ˆéå¯¹è§’ä¸ºé›¶ï¼Œå¯¹è§’å®Œå…¨ç›¸åŒï¼Œçƒé¢ç‰¹æ€§ï¼‰ï¼Œé»˜è®¤â€˜fullâ€™ å®Œå…¨åæ–¹å·®çŸ©é˜µ tol=1e-3 æ”¶æ•›é˜ˆå€¼ï¼ŒEM iterations will stop when the lower bound average gain is below this threshold.ä¹Ÿå°±æ˜¯å½“ä¸‹ç•Œçš„å¹³å‡å¢ç›Šå°äºé˜ˆå€¼æ—¶ï¼Œemè¿­ä»£å°±åœæ­¢ã€‚è¿™é‡Œçš„ä¸‹ç•ŒæŒ‡çš„æ˜¯å…¬å¼ ï¼ˆ3ï¼‰ä¸­çš„ä¸‹ç•Œå‡¸å‡½æ•°ã€‚æˆ‘ä»¬çŸ¥é“emç®—æ³•åˆ†ä¸¤æ­¥ï¼Œe stepæ˜¯æœŸæœ›ï¼Œä¹Ÿå°±æ˜¯ä¸ç­‰å¼ç›¸ç­‰ï¼Œm setpæ˜¯æœ€å¤§åŒ–ï¼Œ ä¹Ÿå°±æ˜¯ä¸‹ç•Œå‡¸å‡½æ•°æœ€å¤§åŒ–ã€‚è¿™é‡Œçš„é˜ˆå€¼å¹³å‡å¢ç›Šå°±æ˜¯æŒ‡å‡¸å‡½æ•°çš„æœ€å¤§åŒ–è¿‡ç¨‹ä¸­çš„å¢ç›Šã€‚ reg_covar=1e-6ï¼š Non-negative regularization added to the diagonal of covariance.Allows to assure that the covariance matrices are all positive. éè´Ÿæ­£åˆ™åŒ–æ·»åŠ åˆ°åæ–¹å·®çŸ©é˜µå¯¹è§’çº¿ä¸Šï¼Œä¿è¯åæ–¹å·®çŸ©é˜µéƒ½æ˜¯æ­£å®šçš„ã€‚ max_iter=100: emç®—æ³•çš„æœ€å¤§è¿­ä»£æ¬¡æ•° n_init: int, defaults to 1.åˆå§‹åŒ–çš„æ¬¡æ•° init_params: {â€˜kmeansâ€™, â€˜randomâ€™}, defaults to â€˜kmeansâ€™. The method used to initialize the weights, the means and the precisionsself. Must be one of:: - 'kmeans' : responsibilities are initialized using kmeans. - 'random' : responsibilities are initialized randomly. - è¿™é‡Œå¯¹åº”çš„åˆå§‹åŒ–ï¼Œæ˜¯æŒ‡çš„éšè—å˜é‡zçš„åˆ†ç±»æ‰€å æ¯”ä¾‹ï¼Œä¹Ÿå°±æ˜¯weight_initï¼Œkmeansè¡¨ç¤ºâ€œhardâ€guessï¼Œ {0, 1} or {1, . . . , k}) randomåº”è¯¥å°±æ˜¯â€softâ€guesså§ã€‚ weights_init : shape (n_components, ), optional The user-provided initial weights, defaults to None. If it None, weights are initialized using the init_params method. å…ˆéªŒæƒé‡åˆå§‹åŒ–ï¼Œå¯¹åº”çš„å°±æ˜¯éšè—å˜é‡æœ‰n_componentsç±»ï¼Œè€Œæ¯ä¸€ç±»æ‰€å çš„æ¯”ä¾‹ï¼Œä¹Ÿå°±æ˜¯å¤šé¡¹å¼åˆ†å¸ƒçš„åˆå§‹åŒ–ï½å¯¹åº”$\\phi_i$ means_init : array-like, shape (n_components, n_features), optional. The user-provided initial means, defaults to None, If it None, means are initialized using the init_params method.æ··åˆé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼åˆå§‹åŒ–ï¼Œæ³¨æ„shape=(n_components, n_features),æœ‰n_componentsè¿™æ ·çš„å¤šç»´é«˜æ–¯åˆ†å¸ƒï¼Œæ¯ä¸ªé«˜æ–¯åˆ†å¸ƒæœ‰n_featuresç»´åº¦ precisions_init : The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the â€˜init_paramsâ€™ method.The shape depends on â€˜covariance_typeâ€™:: (n_components,) if â€˜sphericalâ€™, (n_features, n_features) if â€˜tiedâ€™, (n_components, n_features) if â€˜diagâ€™, (n_components, n_features, n_features) if â€˜fullâ€™ ç”¨æ¥åˆå§‹åŒ–é«˜æ–¯åˆ†å¸ƒä¸­çš„åæ–¹å·®çŸ©é˜µï¼Œåæ–¹å·®çŸ©é˜µä»£è¡¨çš„æ˜¯n_featuresç»´å‘é‡ä¸­æ¯ä¸€ç»´ç‰¹å¾ä¸å…¶ä»–ç»´åº¦ç‰¹å¾çš„å…³ç³»ï¼Œå¯¹äºä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒæ¥è¯´æ˜¯n_featuresn_featuresï¼Œn_componentsä¸ªæ··åˆä¹Ÿå°±æ˜¯â€™fullâ€™ã€‚å…¶ä¸­è¦å­¦ä¹ çš„å‚æ•°ä¸ªæ•°æ˜¯(n_features+1) n_features/2.å…·ä½“å…³äºåæ–¹å·®çŸ©é˜µå‚è€ƒå‰é¢é‚£ç¯‡åšå®¢ random_state : int, RandomState instance or None, optional (default=None) éšæœºæ•°ç”Ÿæˆå™¨ warm_start : bool, default to False.If â€˜warm_startâ€™ is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. è‹¥ä¸ºTrueï¼Œåˆ™fitï¼ˆï¼‰è°ƒç”¨ä¼šä»¥ä¸Šä¸€æ¬¡fitï¼ˆï¼‰çš„ç»“æœä½œä¸ºåˆå§‹åŒ–å‚æ•°ï¼Œé€‚åˆç›¸åŒé—®é¢˜å¤šæ¬¡fitçš„æƒ…å†µï¼Œèƒ½åŠ é€Ÿæ”¶æ•›ï¼Œé»˜è®¤ä¸ºFalseã€‚ verbose : int, default to 0. Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step. ä½¿èƒ½è¿­ä»£ä¿¡æ¯æ˜¾ç¤ºï¼Œé»˜è®¤ä¸º0ï¼Œå¯ä»¥ä¸º1æˆ–è€…å¤§äº1ï¼ˆæ˜¾ç¤ºçš„ä¿¡æ¯ä¸åŒï¼‰ verbose_interval: ä¸13æŒ‚é’©ï¼Œè‹¥ä½¿èƒ½è¿­ä»£ä¿¡æ¯æ˜¾ç¤ºï¼Œè®¾ç½®å¤šå°‘æ¬¡è¿­ä»£åæ˜¾ç¤ºä¿¡æ¯ï¼Œé»˜è®¤10æ¬¡ã€‚ E stepå°±æ˜¯æ±‚$w_j^i$ 12345678910111213141516171819202122232425def _e_step(self, X): &quot;&quot;&quot;E step. Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- log_prob_norm : log_responsibility : åéªŒæ¦‚ç‡ï¼Œæ ·æœ¬iæ˜¯jç±»çš„æ¦‚ç‡w_j^{i} &quot;&quot;&quot; log_prob_norm, log_resp = self._estimate_log_prob_resp(X) return np.mean(log_prob_norm), log_resp é‚£ä¹ˆå¦‚ä½•æ±‚$w_j^{i}$å‘¢ï¼Ÿ $$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)=\\dfrac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}{\\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\\mu,\\Sigma)p(z^{(i)}=l;\\phi)}$$ è¦æ³¨æ„çš„æ˜¯ï¼Œä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œæœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š å› ä¸ºåˆ†å­åˆ†æ¯ä¸­è®¾è®¡åˆ°æ­£æ€åˆ†å¸ƒï¼Œå³æŒ‡æ•°å½¢å¼ï¼Œæ•…å…ˆè®¡ç®—å…¶logå½¢å¼ã€‚ç„¶åå¸¦å…¥åˆ°M stepä¸­å–å›æŒ‡æ•°å½¢å¼å³å¯ã€‚ å¯¹äºåæ–¹å·®çŸ©é˜µï¼Œå¦‚æœn_featureså¾ˆå¤§çš„è¯ï¼Œè®¡ç®—å…¶é€†çŸ©é˜µå’Œè¡Œåˆ—å¼å°±å¾ˆå¤æ‚ï¼Œå› æ­¤å¯ä»¥å…ˆè®¡ç®—å…¶precisionçŸ©é˜µï¼Œç„¶åè¿›è¡Œcholeskyåˆ†è§£ï¼Œä»¥ä¾¿ä¼˜åŒ–è®¡ç®—ã€‚ å…ˆè®¡ç®—åˆ†å­å¯¹æ•°å½¢å¼ï¼Œä¸¤ä¸ªå¯¹æ•°ç›¸åŠ ï¼š$$logp(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)+logp(z^{(i)}=j;\\phi)$$ 1234567891011121314151617181920212223# è®¡ç®—P(x|z)p(z)çš„å¯¹æ•°å½¢å¼def _estimate_weighted_log_prob(self, X): &quot;&quot;&quot;Estimate the weighted log-probabilities, log P(X | Z) + log weights. Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- weighted_log_prob : array, shape (n_samples, n_component) &quot;&quot;&quot; return self._estimate_log_prob(X) + self._estimate_log_weights() å…¶ä¸­å‰è€…æ˜¯é«˜æ–¯åˆ†å¸ƒæ¦‚ç‡çš„å¯¹æ•°,æ ¹æ®å‡å€¼ï¼Œåæ–¹å·®çŸ©é˜µçš„choleskyåˆ†è§£å¯æ±‚å¾—ã€‚ 1234567def _estimate_log_prob(self, X): return _estimate_log_gaussian_prob( X, self.means_, self.precisions_cholesky_, self.covariance_type) è¿™ä¸ªå‡½æ•°ï¼Œ_estimate_log_gaussian_probæ ¹æ®é«˜æ–¯åˆ†å¸ƒçš„å‚æ•°è®¡ç®—æ¦‚ç‡ï¼Œæ¶‰åŠåˆ°åæ–¹å·®çŸ©é˜µï¼Œè¦ä¼˜åŒ–è®¡ç®—ï¼Œå¾ˆå¤æ‚ï¼Œæ”¾åœ¨æœ€åè¯´ã€‚å…ˆæŠŠæ•´ä¸ªæµç¨‹èµ°å®Œã€‚ åè€…æ˜¯æ¯ä¸€ç±»é«˜æ–¯åˆ†å¸ƒæ‰€å çš„æƒé‡ï¼Œä¹Ÿå°±æ˜¯$\\phi_j$ 1234567def _estimate_log_weights(self): # åˆšå¼€å§‹æ˜¯åˆå§‹å€¼ï¼Œåé¢éšç€m stepè€Œæ›´æ–° return np.log(self.weights_) å†è®¡ç®—$w_j^i$1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def _estimate_log_prob_resp(self, X): &quot;&quot;&quot;Estimate log probabilities and responsibilities for each sample. Compute the log probabilities, weighted log probabilities per component and responsibilities for each sample in X with respect to the current state of the model. Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- log_prob_norm : array, shape (n_samples,) log p(X) log_responsibilities : array, shape (n_samples, n_components) logarithm of the responsibilities &quot;&quot;&quot; # è®¡ç®—åˆ†å­log P(X | Z) + log weights. weighted_log_prob = self._estimate_weighted_log_prob(X) # è®¡ç®—åˆ†æ¯log P(x) log_prob_norm = logsumexp(weighted_log_prob, axis=1) with np.errstate(under='ignore'): # å¿½ç•¥ä¸‹æº¢ï¼Œè®¡ç®—log(w_J^j)ï¼Œä¹Ÿå°±æ˜¯ä¸¤ä¸ªå¯¹æ•°ç›¸å‡ log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis] return log_prob_norm, log_resp M setp123456789101112131415161718192021222324252627282930313233343536373839def _m_step(self, X, log_resp): &quot;&quot;&quot;M step. Parameters ---------- X : array-like, shape (n_samples, n_features) log_resp : array-like, shape (n_samples, n_components) Logarithm of the posterior probabilities (or responsibilities) of the point of each sample in X. &quot;&quot;&quot; n_samples, _ = X.shape # æ ¹æ®E stepä¸­æ±‚å¾—çš„log_resp,æ›´æ–°æƒé‡ï¼Œå‡å€¼å’Œåæ–¹å·® self.weights_, self.means_, self.covariances_ = ( _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar, self.covariance_type)) # æ›´æ–°ç±»åˆ«æƒé‡phi_j self.weights_ /= n_samples # æ›´æ–°åæ–¹å·®çŸ©é˜µçš„ç²¾åº¦çŸ©é˜µ self.precisions_cholesky_ = _compute_precision_cholesky( self.covariances_, self.covariance_type) å…·ä½“æ€ä¹ˆæ±‚ï¼Œå°±æ˜¯æ ¹æ®å‰é¢æ¨å¯¼çš„å…¬å¼äº†ã€‚æ ¹æ®å‰é¢çš„å…¬å¼åˆ†åˆ«æ±‚å¯¹åº”çš„ä¼°è®¡å‚æ•°ï¼š $$\\Sigma_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T}{\\sum_{i=1}^mw_j^{(i)}}$$ åæ–¹å·®çŸ©é˜µï¼šä»¥â€˜fullâ€™ä¸ºä¾‹1234567891011121314151617181920212223242526272829def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar): &quot;&quot;&quot;Estimate the full covariance matrices. resp:è¡¨ç¤ºE stepä¸­çŒœæµ‹çš„w_j^{i} &quot;&quot;&quot; n_components, n_features = means.shape # åæ–¹å·®çŸ©é˜µ covariances = np.empty((n_components, n_features, n_features)) for k in range(n_components): diff = X - means[k] covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k] # æ­£åˆ™åŒ–ï¼Œflatè¡¨ç¤ºå±•å¼€æˆä¸€ç»´ï¼Œç„¶åæ¯éš”n_featureså–ä¸€ä¸ªå…ƒç´ ï¼Œå•ä¸ªåæ–¹å·®çŸ©é˜µshapeæ˜¯ # [n_features, n_features],æ‰€ä»¥å°±æ˜¯å¯¹è§’çº¿å…ƒç´ åŠ ä¸Šreg_covar covariances[k].flat[::n_features + 1] += reg_covar return covariances ç„¶åæ˜¯æ­£æ€åˆ†å¸ƒçš„å‚æ•°ä¼°è®¡$u_j, \\phi_j$$$\\phi_j:=\\frac{1}{m}\\sum_{i=1}^mw_j^{(i)}$$ $$\\mu_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\\sum_{i=1}^mw_j^{(i)}}$$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type): &quot;&quot;&quot;Estimate the Gaussian distribution parameters. Parameters ---------- X : æ ·æœ¬æ•°æ® (n_samples, n_features) resp : EstepçŒœæµ‹çš„æ ·æœ¬iæ˜¯jç±»çš„æ¦‚ç‡w_i^{j}, shape (n_samples, n_components) reg_covar : å¯¹è§’çº¿æ­£åˆ™åŒ–é¡¹ covariance_type : {'full', 'tied', 'diag', 'spherical'} Returns ------- nk : å½“å‰ç±»åˆ«ä¸‹çš„æ ·æœ¬å’Œ (n_components,) ä¹Ÿå°±æ˜¯\\sum_i^{m}(w_j^{i}) means : kä¸ªnç»´æ­£æ€åˆ†å¸ƒçš„å‡å€¼, shape (n_components, n_features) covariances : åæ–¹å·®çŸ©é˜µ &quot;&quot;&quot; # å› ä¸ºè¦åšåˆ†æ¯ï¼Œé¿å…ä¸º0 nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps means = np.dot(resp.T, X) / nk[:, np.newaxis] covariances = {&quot;full&quot;: _estimate_gaussian_covariances_full, &quot;tied&quot;: _estimate_gaussian_covariances_tied, &quot;diag&quot;: _estimate_gaussian_covariances_diag, &quot;spherical&quot;: _estimate_gaussian_covariances_spherical }[covariance_type](resp, X, nk, means, reg_covar) return nk, means, covariances è¿­ä»£æ”¶æ•›ï¼Œé‡å¤ä»¥ä¸Šè¿‡ç¨‹123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161def fit(self, X, y=None): &quot;&quot;&quot;Estimate model parameters with the EM algorithm. The method fit the model `n_init` times and set the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for `max_iter` times until the change of likelihood or lower bound is less than `tol`, otherwise, a `ConvergenceWarning` is raised. è¿­ä»£ç»ˆæ­¢æ¡ä»¶ï¼š è¿­ä»£æ¬¡æ•°ï½_initï¼Œæå¤§ä¼¼ç„¶å‡½æ•°æˆ–ä¸‹ç•Œå‡½æ•°çš„å¢ç›Šå°äº`tol` Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- self &quot;&quot;&quot; X = _check_X(X, self.n_components) self._check_initial_parameters(X) # if we enable warm_start, we will have a unique initialisation do_init = not(self.warm_start and hasattr(self, 'converged_')) n_init = self.n_init if do_init else 1 max_lower_bound = -np.infty self.converged_ = False random_state = check_random_state(self.random_state) n_samples, _ = X.shape # åˆå§‹åŒ–æ¬¡æ•° for init in range(n_init): self._print_verbose_msg_init_beg(init) # å…ˆåˆå§‹åŒ–å‚æ•° if do_init: self._initialize_parameters(X, random_state) self.lower_bound_ = -np.infty # è¿­ä»£æ¬¡æ•° for n_iter in range(self.max_iter): prev_lower_bound = self.lower_bound_ # E stepæ±‚å‡ºåéªŒæ¦‚ç‡w_j^iæˆ–æ˜¯Qåˆ†å¸ƒ log_prob_norm, log_resp = self._e_step(X) # ï¼­ stepæ›´æ–°å‚æ•° self._m_step(X, log_resp) # æ±‚å‡ºä¸‹ç•Œå‡½æ•°çš„æœ€å¤§å€¼ self.lower_bound_ = self._compute_lower_bound( log_resp, log_prob_norm) # ä¸‹ç•Œå‡½æ•°çš„å¢ç›Š change = self.lower_bound_ - prev_lower_bound self._print_verbose_msg_iter_end(n_iter, change) # æ¯”è¾ƒä¸‹ç•Œå‡½æ•°å¢ç›Šä¸ï½”ï½ï½Œ if abs(change) &lt; self.tol: self.converged_ = True break self._print_verbose_msg_init_end(self.lower_bound_) # if self.lower_bound_ &gt; max_lower_bound: max_lower_bound = self.lower_bound_ best_params = self._get_parameters() best_n_iter = n_iter if not self.converged_: warnings.warn('Initialization %d did not converge. ' 'Try different init parameters, ' 'or increase max_iter, tol ' 'or check for degenerate data.' % (init + 1), ConvergenceWarning) self._set_parameters(best_params) self.n_iter_ = best_n_iter return self E stepä¸­p(x|z=j)æ ¹æ®é«˜æ–¯åˆ†å¸ƒçš„å‚æ•°è®¡ç®—æ¦‚ç‡,ä¼˜åŒ–çš„è®¡ç®—æ–¹æ³•ã€‚ å…ˆè®¡ç®—åæ–¹å·®çŸ©é˜µçš„precisionçŸ©é˜µï¼Œå¹¶è¿›è¡Œcholeskyåˆ†è§£ Precision matrix åæ–¹å·®çŸ©é˜µçš„é€†çŸ©é˜µï¼šhttps://www.statlect.com/glossary/precision-matrix ç„¶åæ ¹æ®ç²¾åº¦çŸ©é˜µçš„choleskyåˆ†è§£å½¢å¼,è¿™æ ·å¯ä»¥ä¼˜åŒ–çŸ©é˜µè¿ç®— 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def _compute_precision_cholesky(covariances, covariance_type): &quot;&quot;&quot;Compute the Cholesky decomposition of the precisions. Parameters ---------- covariances : array-like The covariance matrix of the current components. The shape depends of the covariance_type. covariance_type : {'full', 'tied', 'diag', 'spherical'} The type of precision matrices. Returns ------- precisions_cholesky : array-like The cholesky decomposition of sample precisions of the current components. The shape depends of the covariance_type. &quot;&quot;&quot; if covariance_type in 'full': n_components, n_features, _ = covariances.shape precisions_chol = np.empty((n_components, n_features, n_features)) for k, covariance in enumerate(covariances): try: cov_chol = linalg.cholesky(covariance, lower=True) except linalg.LinAlgError: raise ValueError(estimate_precision_error_message) precisions_chol[k] = linalg.solve_triangular(cov_chol, np.eye(n_features), lower=True).T elif covariance_type == 'tied': _, n_features = covariances.shape try: cov_chol = linalg.cholesky(covariances, lower=True) except linalg.LinAlgError: raise ValueError(estimate_precision_error_message) precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features), lower=True).T else: if np.any(np.less_equal(covariances, 0.0)): raise ValueError(estimate_precision_error_message) precisions_chol = 1. / np.sqrt(covariances) return precisions_chol è®¡ç®—choleskyåˆ†è§£çš„è¡Œåˆ—å¼123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Gaussian mixture probability estimators# æ ¹æ®choleskyåˆ†è§£è®¡ç®—è¡Œåˆ—å¼çš„logdef _compute_log_det_cholesky(matrix_chol, covariance_type, n_features): &quot;&quot;&quot;Compute the log-det of the cholesky decomposition of matrices. Parameters ---------- matrix_chol : åæ–¹å·®çŸ©é˜µçš„choleskyåˆ†è§£ covariance_type : {'full', 'tied', 'diag', 'spherical'} n_features : int Number of features. Returns ------- log_det_precision_chol : array-like, shape (n_components,) The determinant of the precision matrix for each component. &quot;&quot;&quot; if covariance_type == 'full': n_components, _, _ = matrix_chol.shape log_det_chol = (np.sum(np.log( matrix_chol.reshape( n_components, -1)[:, ::n_features + 1]), 1)) elif covariance_type == 'tied': log_det_chol = (np.sum(np.log(np.diag(matrix_chol)))) elif covariance_type == 'diag': log_det_chol = (np.sum(np.log(matrix_chol), axis=1)) else: log_det_chol = n_features * (np.log(matrix_chol)) return log_det_chol è®¡ç®—åˆ†å­: $logp(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)$123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type): &quot;&quot;&quot;Estimate the log Gaussian probability. Parameters ---------- X : æ ·æœ¬æ•°æ®(n_samples, n_features) means : kä¸ªnç»´æ­£æ€åˆ†å¸ƒçš„å‡å€¼(n_components, n_features) precisions_chol : ç²¾åº¦çŸ©é˜µçš„Choleskyåˆ†è§£ covariance_type : {'full', 'tied', 'diag', 'spherical'} Returns ------- log_prob : (n_samples, n_components) &quot;&quot;&quot; n_samples, n_features = X.shape n_components, _ = means.shape # det(precision_chol) is half of det(precision) log_det = _compute_log_det_cholesky( precisions_chol, covariance_type, n_features) if covariance_type == 'full': log_prob = np.empty((n_samples, n_components)) for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)): y = np.dot(X, prec_chol) - np.dot(mu, prec_chol) log_prob[:, k] = np.sum(np.square(y), axis=1) elif covariance_type == 'tied': pass elif covariance_type == 'diag': pass elif covariance_type == 'spherical': pass return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det sklearnä¸­å®ä¾‹Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid. We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data. On the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only the first two dimensions are shown here, and thus some points are separated in other dimensions. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231import matplotlib as mplimport matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasetsfrom sklearn.mixture import GaussianMixturefrom sklearn.model_selection import StratifiedKFoldprint(__doc__)colors = ['navy', 'turquoise', 'darkorange']iris = datasets.load_iris() # data, target# Break up the dataset into non-overlapping training (75%) and testing# (25%) sets.skf = StratifiedKFold(n_splits=4)# Only take the first fold.train_index, test_index = next(iter(skf.split(iris.data, iris.target)))X_train = iris.data[train_index] # (111, 4)y_train = iris.target[train_index] # (111,)X_test = iris.data[test_index] # (39, 4)y_test = iris.target[test_index] # (39,)n_classes = len(np.unique(y_train))# Try GMMs using different types of covariances. æ ¹æ®åæ–¹å·®çŸ©é˜µï¼Œæœ‰4ä¸­ä¸åŒçš„GMMæ¨¡å‹estimators = dict((cov_type, GaussianMixture(n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0)) for cov_type in ['spherical', 'diag', 'tied', 'full'])n_estimators = len(estimators) # 4# figsizeè¡¨ç¤ºå›¾åƒçš„å°ºå¯¸ï¼ˆwidth, height in inchesï¼‰plt.figure(figsize=(3 * 5 // 2, 6))# å›¾åƒä¹‹é—´çš„é—´è·plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05, left=.01, right=.99)# æ¤­åœ†def make_ellipses(gmm, ax): for n, color in enumerate(colors): if gmm.covariance_type == 'full': covariances = gmm.covariances_[n][:2, :2] elif gmm.covariance_type == 'tied': covariances = gmm.covariances_[:2, :2] elif gmm.covariance_type == 'diag': covariances = np.diag(gmm.covariances_[n][:2]) elif gmm.covariance_type == 'spherical': covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n] # å‚æ•°ä¼°è®¡å¾—åˆ°çš„æ··åˆäºŒç»´é«˜æ–¯åˆ†å¸ƒï¼Œå°†å…¶ç”¨æ¤­åœ†è¡¨ç¤ºå‡ºæ¥ï½ v, w = np.linalg.eigh(covariances) # è¿”å›åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼å’Œåˆ—å‘é‡ç”±ç‰¹å¾çŸ©é˜µæ„æˆçš„çŸ©é˜µ u = w[0] / np.linalg.norm(w[0]) # order=None è¡¨ç¤º Frobenius normï¼Œ2-norm angle = np.arctan2(u[1], u[0]) angle = 180 * angle / np.pi # è½¬æ¢ä¸ºè§’åº¦ v = 2. * np.sqrt(2.) * np.sqrt(v) ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1], 180 + angle, color=color) ell.set_clip_box(ax.bbox) ell.set_alpha(0.5) ax.add_artist(ell) # å¢åŠ æ–‡å­—for index, (name, estimator) in enumerate(estimators.items()): # Since we have class labels for the training data, we can # initialize the GMM parameters in a supervised manner. # è¿™é‡Œå› ä¸ºæœ‰ç±»æ ‡ç­¾ï¼Œæ‰€ä»¥ç›´æ¥ç”¨çœŸå®å‡å€¼æ¥åˆå§‹åŒ–GMMçš„å‡å€¼ã€‚åœ¨æ— æ ‡ç­¾æˆ–è€…æ ‡ç­¾è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œåˆ™éœ€è¦éšæœºåˆå§‹åŒ– estimator.means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)]) # Train the other parameters using the EM algorithm. # ç”¨emç®—æ³•æ¥ä¼°è®¡å…¶ä»–å‚æ•° estimator.fit(X_train) # ç”»æ¤­åœ† h = plt.subplot(2, n_estimators // 2, index + 1) make_ellipses(estimator, h) for n, color in enumerate(colors): data = iris.data[iris.target == n] # ä¸åŒçš„ç§ç±»æ•°æ®ç”¨ä¸åŒçš„ç‚¹è¡¨ç¤º plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]) # ç”¨xxè¡¨ç¤ºæµ‹è¯•é›† for n, color in enumerate(colors): data = X_test[y_test == n] plt.scatter(data[:, 0], data[:, 1], marker='x', color=color) # è®­ç»ƒé›†çš„å‡†ç¡®ç‡ y_train_pred = estimator.predict(X_train) # é¢„æµ‹æ˜¯é€‰å–æ¦‚ç‡æœ€å¤§çš„ä¸€ç±» # å½“æ— æ ‡ç­¾æ—¶æ˜¯æ²¡æœ‰åŠæ³•è®¡ç®—å‡†ç¡®ç‡çš„ã€‚ä½†æ˜¯è¿™é‡Œæœ‰æ ‡ç­¾ï¼Œ # y_train_predè¿”å›çš„æ˜¯æ¦‚ç‡æœ€å¤§çš„ç´¢å¼•ï¼Œ y_trainçš„å…ƒç´ æ˜¯[0,1,2,3]ä¸­çš„ä¸€ä¸ª # å› æ­¤å¯ä»¥æ±‚å¾—å‡†ç¡®ç‡ print(y_train_pred[:5]) print(y_train[:5]) train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100 plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy, transform=h.transAxes) y_test_pred = estimator.predict(X_test) test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100 plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy, transform=h.transAxes) plt.xticks(()) plt.yticks(()) plt.title(name)plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))plt.show()","link":"/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"title":"æ–‡æœ¬åˆ†ç±»ç³»åˆ—0ï¼šNLTKå­¦ä¹ å’Œç‰¹å¾å·¥ç¨‹","text":"è®¡ç®—è¯­è¨€ï¼šç®€å•çš„ç»Ÿè®¡123from nltk.book import * *** Introductory Examples for the NLTK Book *** Loading text1, ..., text9 and sent1, ..., sent9 Type the name of the text or sentence to view it. Type: 'texts()' or 'sents()' to list the materials. text1: Moby Dick by Herman Melville 1851 text2: Sense and Sensibility by Jane Austen 1811 text3: The Book of Genesis text4: Inaugural Address Corpus text5: Chat Corpus text6: Monty Python and the Holy Grail text7: Wall Street Journal text8: Personals Corpus text9: The Man Who Was Thursday by G . K . Chesterton 1908 æ‰¾å‡ºtext1,ã€Šç™½é²¸è®°ã€‹ä¸­çš„è¯monstrousï¼Œä»¥åŠå…¶ä¸Šä¸‹æ–‡ 123text1.concordance(&quot;monstrous&quot;, width=40, lines=10) Displaying 10 of 11 matches: was of a most monstrous size . ... Thi Touching that monstrous bulk of the wh enish array of monstrous clubs and spea wondered what monstrous cannibal and s e flood ; most monstrous and most mount Moby Dick as a monstrous fable , or sti PTER 55 Of the Monstrous Pictures of Wh exion with the monstrous pictures of wh ose still more monstrous stories of the ed out of this monstrous cabinet there æ‰¾å‡ºtext1ä¸­ä¸monstrouså…·æœ‰ç›¸åŒè¯­å¢ƒçš„è¯ã€‚æ¯”å¦‚monstrousçš„ä¸Šä¸‹æ–‡ the __ pictures, the __ size. åŒæ ·åœ¨text1ä¸­ä¸monstrousç±»ä¼¼çš„ä¸Šä¸‹æ–‡çš„è¯ã€‚å¾ˆå¥½å¥‡è¿™ä¸ªæ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ 1234567891011def similar(self, word, num=20): &quot;&quot;&quot; Distributional similarity: find other words which appear in the same contexts as the specified word; list most similar words first. &quot;&quot;&quot; 123text1.similar(&quot;monstrous&quot;) true contemptible christian abundant few part mean careful puzzled mystifying passing curious loving wise doleful gamesome singular delightfully perilous fearless å…±ç”¨ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šè¯æ±‡çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚monstrouså’Œvery 123text2.common_contexts([&quot;monstrous&quot;, &quot;very&quot;]) a_pretty am_glad a_lucky is_pretty be_glad è‡ªåŠ¨æ£€æµ‹å‡ºç°åœ¨æ–‡æœ¬ä¸­çš„ç‰¹å®šè¯ï¼Œå¹¶æ˜¾ç¤ºåŒä¸€ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„å…¶ä»–è¯ã€‚text4æ˜¯ã€Šå°±èŒæ¼”è¯´è¯­æ–™ã€‹ï¼Œ 12345if __name__ == &quot;__main__&quot;: text4.dispersion_plot([&quot;citizens&quot;, &quot;liberty&quot;, &quot;freedom&quot;]) &lt;matplotlib.figure.Figure at 0x7f3794818588&gt; å¦‚æœä¸ä½¿ç”¨ if name==â€mainâ€œ çš„è¯ä¼šæŠ¥é”™``` â€˜NoneTypeâ€™ object has no attribute â€˜showâ€™ 12345678910111213```pythonfdist1 = FreqDist(text1)vocabulary1 = list(fdist1.keys()) # keys() è¿”å›keyå€¼ç»„æˆçš„listprint(vocabulary1[:10]) ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.'] éœ€è¦åŠ listï¼Œä¸ç„¶å›æŠ¥é”™ï¼Œâ€œTypeError: 'dict_keys' object is not subscriptableâ€ dict.keys() returns an iteratable but not indexable object. The most simple (but not so efficient) solution would be: 1234567# åŒæ ·çš„é“ç†è¿™é‡Œä¹Ÿéœ€è¦åŠ listï¼Œå› ä¸ºç”Ÿæˆçš„&lt;class 'dict_items'&gt;zåœ¨python3ä¸­æ˜¯è¿­ä»£å™¨print(type(fdist1.items()))print(list(fdist1.items())[:10]) &lt;class 'dict_items'&gt; [('[', 3), ('Moby', 84), ('Dick', 84), ('by', 1137), ('Herman', 1), ('Melville', 1), ('1851', 3), (']', 1), ('ETYMOLOGY', 1), ('.', 6862)] 123456789# dict.items() å®é™…ä¸Šæ˜¯å°†dictè½¬æ¢ä¸ºå¯è¿­ä»£å¯¹è±¡listï¼Œlistçš„å¯¹è±¡æ˜¯ ('[', 3), ('Moby', 84), ('Dick', 84), ('by', 1137)è¿™æ ·çš„# è¿™ä¸‹æ€»èƒ½è®°ä½dictæŒ‰ç…§valueæ’åºäº†å§ã€‚ã€‚ã€‚å°´å°¬ï¼Œä»¥å‰å±…ç„¶æ²¡å¼„æ‡‚ï¼Ÿï¼Ÿfdist_sorted = sorted(fdist1.items(), key=lambda item:item[1], reverse=True)print(fdist_sorted[:10]) [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)] 1234567# è¿™ä¸ªå°±æ˜¯æŒ‰ç…§keyæ’åºã€‚fdist_sorted2 = sorted(fdist1.keys(), reverse=True)print(fdist_sorted2[:10]) ['zoology', 'zones', 'zoned', 'zone', 'zodiac', 'zig', 'zephyr', 'zeal', 'zay', 'zag'] 123fdist1.plot(20, cumulative=True) å¯ä»¥çœ‹åˆ°é«˜é¢‘è¯å¤§éƒ½æ˜¯æ— ç”¨çš„åœç”¨è¯ 12345678910111213# ä½é¢‘è¯ fdist.hapaxes() å‡ºç°æ¬¡æ•°ä¸º1çš„è¯print(len(fdist1.hapaxes()))for i in fdist1.hapaxes(): if fdist1[i] is not 1: print(&quot;hh&quot;) 9002 å¯ä»¥çœ‹åˆ°ä½é¢‘è¯ä¹Ÿå¾ˆå¤šï¼Œè€Œä¸”å¤§éƒ½ä¹Ÿæ˜¯å¾ˆæ— ç”¨çš„è¯ã€‚ è¯è¯­æ­é…123list(bigrams(['more', 'is', 'sad', 'than', 'done'])) [('more', 'is'), ('is', 'sad'), ('sad', 'than'), ('than', 'done')] 123text4.collocations(window_size=4) United States; fellow citizens; four years; years ago; men women; Federal Government; General Government; self government; Vice President; American people; every citizen; within limits; Old World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice; one another; Declaration Independence; protect defend æ–‡æœ¬4æ˜¯å°±èŒæ¼”è¯´è¯­æ–™ï¼Œå¯ä»¥çœ‹åˆ°n-gramsèƒ½å¤Ÿå¾ˆå¥½çš„å±•ç°å‡ºæ–‡æœ¬çš„ç‰¹æ€§ï¼Œè¯´æ˜n-gramsæ˜¯ä¸é”™çš„ç‰¹å¾ã€‚ collections()æºç  12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def collocations(self, num=20, window_size=2): &quot;&quot;&quot; Print collocations derived from the text, ignoring stopwords. :seealso: find_collocations :param num: The maximum number of collocations to print. :type num: int :param window_size: The number of tokens spanned by a collocation (default=2) :type window_size: int &quot;&quot;&quot; if not ('_collocations' in self.__dict__ and self._num == num and self._window_size == window_size): self._num = num self._window_size = window_size #print(&quot;Building collocations list&quot;) from nltk.corpus import stopwords ignored_words = stopwords.words('english') finder = BigramCollocationFinder.from_words(self.tokens, window_size) finder.apply_freq_filter(2) finder.apply_word_filter(lambda w: len(w) &lt; 3 or w.lower() in ignored_words) bigram_measures = BigramAssocMeasures() self._collocations = finder.nbest(bigram_measures.likelihood_ratio, num) colloc_strings = [w1+' '+w2 for w1, w2 in self._collocations] print(tokenwrap(colloc_strings, separator=&quot;; &quot;)) è‡ªåŠ¨ç†è§£è‡ªç„¶è¯­è¨€ è¯ä¹‰æ¶ˆæ­§ Ambiguity å…³äºè¯ä¹‰æ¶ˆæ­§çš„ç†è§£å¯ä»¥çœ‹ä¹‹å‰çš„ç¬”è®°chapter12-å¥æ³•åˆ†æ æŒ‡ä»£æ¶ˆè§£ anaphora resolution è‡ªåŠ¨é—®ç­” æœºå™¨ç¿»è¯‘ äººæœºå¯¹è¯ç³»ç»Ÿ è·å¾—æ–‡æœ¬è¯­æ–™å’Œè¯æ±‡èµ„æºå¸ƒæœ—è¯­æ–™åº“123from nltk.corpus import brown æœ‰ä»¥ä¸‹è¿™äº›ç±»åˆ«çš„æ–‡æœ¬ 123print(brown.categories()) ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction'] 123456789import nltknews_text = brown.words(categories=&quot;news&quot;)fdist_news = nltk.FreqDist([w.lower() for w in news_text])print(len(fdist_news)) 13112 æ ‡æ³¨æ–‡æœ¬è¯­æ–™åº“ç»è¿‡äº†æ ‡æ³¨çš„è¯­æ–™åº“ï¼Œæœ‰è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“ã€å¥æ³•ç»“æ„ã€è¯­ä¹‰è§’è‰²ç­‰ã€‚ åˆ†ç±»å’Œæ ‡æ³¨è¯æ±‡1234567text = nltk.word_tokenize(&quot;and now for something completely differences!&quot;)print(text)print(nltk.pos_tag(text)) ['and', 'now', 'for', 'something', 'completely', 'differences', '!'] [('and', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('differences', 'VBZ'), ('!', '.')] è¯æ€§æ ‡æ³¨NLTKä¸­é‡‡ç”¨çš„æ–¹æ³•å¯å‚è€ƒï¼šA Good Part-of-Speech Tagger in about 200 Lines of Python å¯¹äºä¸€äº›åŒå½¢åŒéŸ³å¼‚ä¹‰è¯ï¼Œé€šè¿‡è¯æ€§æ ‡æ³¨èƒ½æ¶ˆé™¤æ­§ä¹‰.å¾ˆå¤šæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿé€šå¸¸éœ€è¦è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼Œå› ä¸ºä¸åŒæ„æ€å‘éŸ³ä¼šä¸å¤ªä¸€æ ·ã€‚ 123456789text1 = nltk.word_tokenize(&quot;They refuse to permit us tpo obtain the refuse permit&quot;)print(nltk.pos_tag(text1))text2 = nltk.word_tokenize(&quot;They refuse to permit us to obtain the refuse permit&quot;)print(nltk.pos_tag(text2)) [('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('tpo', 'VB'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')] [('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')] è·å–å·²ç»æ ‡æ³¨å¥½çš„è¯­æ–™åº“123print(nltk.corpus.brown.tagged_words()) [('The', 'AT'), ('Fulton', 'NP-TL'), ...] 12345print(nltk.corpus.treebank.tagged_words())print(nltk.corpus.treebank.tagged_sents()[0]) [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...] [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')] æŸ¥çœ‹brownè¯­æ–™åº“ä¸­æ–°é—»ç±»æœ€å¸¸è§çš„è¯æ€§ 1234567brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)tag_fd.keys() dict_keys(['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']) æ–‡æœ¬åˆ†ç±»æœ´ç´ è´å¶æ–¯åˆ†ç±»é€‰å–ç‰¹å¾ï¼Œå°†åå­—çš„æœ€åä¸€ä¸ªå­—æ¯ä½œä¸ºç‰¹å¾. è¿”å›çš„å­—å…¸ç§°ä¸ºç‰¹å¾é›† 1234567def gender_features(word): return {'last_letter':word[-1]}gender_features('Shrek') {'last_letter': 'k'} å®šä¹‰ä¸€ä¸ªç‰¹å¾æå–å™¨ 123456789from nltk.corpus import namesimport randomnames = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]) 12345print(nltk.corpus.names.words('male.txt')[:10])print(names[:10]) ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim'] [('Aamir', 'male'), ('Aaron', 'male'), ('Abbey', 'male'), ('Abbie', 'male'), ('Abbot', 'male'), ('Abbott', 'male'), ('Abby', 'male'), ('Abdel', 'male'), ('Abdul', 'male'), ('Abdulkarim', 'male')] ä½¿ç”¨ç‰¹å¾æå–å™¨å¤„ç†namesæ•°æ®ï¼Œå¹¶æŠŠæ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† 12345# äºŒåˆ†ç±»features = [(gender_features(n), g) for (n, g) in names] 123train_set, test_set = features[500:], features[:500] 123print(train_set[:10]) [({'last_letter': 'n'}, 'male'), ({'last_letter': 'e'}, 'male'), ({'last_letter': 'e'}, 'male'), ({'last_letter': 'b'}, 'male'), ({'last_letter': 'b'}, 'male'), ({'last_letter': 'e'}, 'male'), ({'last_letter': 'y'}, 'male'), ({'last_letter': 'y'}, 'male'), ({'last_letter': 't'}, 'male'), ({'last_letter': 'e'}, 'male')] 123classifier = nltk.NaiveBayesClassifier.train(train_set) 12345# é¢„æµ‹ä¸€ä¸ªæœªå‡ºç°çš„åå­—classifier.classify(gender_features('Pan')) 'male' 12345# æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡print(nltk.classify.accuracy(classifier, test_set)) 0.602 123classifier.show_most_informative_features(5) Most Informative Features last_letter = 'a' female : male = 35.5 : 1.0 last_letter = 'k' male : female = 34.1 : 1.0 last_letter = 'f' male : female = 15.9 : 1.0 last_letter = 'p' male : female = 13.5 : 1.0 last_letter = 'v' male : female = 12.7 : 1.0 æ„å»ºåŒ…å«æ‰€æœ‰å®ä¾‹ç‰¹å¾çš„å•ç‹¬listä¼šå ç”¨å¤§é‡å†…å­˜ï¼Œæ‰€æœ‰åº”è¯¥æŠŠè¿™äº›ç‰¹å¾é›†æˆèµ·æ¥ã€‚ å®šä¹‰ä¸€ä¸ªç‰¹å¾æå–å™¨åŒ…å«å¤šä¸ªç‰¹å¾12345678910111213141516171819# æ·»åŠ å¤šä¸ªç‰¹å¾from nltk.classify import apply_featuresdef gender_features2(word): features = {} features['firstletter'] = word[0].lower() features['lastletter'] = word[-1].lower() for letter in 'abcdefghijklmnopqrstuvwxyz': features[&quot;count(%s)&quot;%letter] = word.lower().count(letter) return features 12345print(gender_features2('xiepan'))print(len(gender_features2('xiepan'))) # æœ‰28ä¸ªç‰¹å¾ï¼Œ 2+26=28 {'firstletter': 'x', 'lastletter': 'n', 'count(a)': 1, 'count(b)': 0, 'count(c)': 0, 'count(d)': 0, 'count(e)': 1, 'count(f)': 0, 'count(g)': 0, 'count(h)': 0, 'count(i)': 1, 'count(j)': 0, 'count(k)': 0, 'count(l)': 0, 'count(m)': 0, 'count(n)': 1, 'count(o)': 0, 'count(p)': 1, 'count(q)': 0, 'count(r)': 0, 'count(s)': 0, 'count(t)': 0, 'count(u)': 0, 'count(v)': 0, 'count(w)': 0, 'count(x)': 1, 'count(y)': 0, 'count(z)': 0} 28 1234567# å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œç‰¹å¾å¤„ç†features = [(gender_features(n), g) for (n,g) in names]print(len(features)) 7944 123456789# è®­ç»ƒé›†ï¼Œå¼€å‘é›†å’Œæµ‹è¯•é›†train_set = features[1500:]dev_set = apply_features(gender_features2, names[500:1500])test_set = apply_features(gender_features2, names[:500]) 123456789classifier = nltk.NaiveBayesClassifier.train(train_set)print(nltk.classify.accuracy(classifier, dev_set))print(nltk.classify.accuracy(classifier, test_set))print(nltk.classify.accuracy(classifier, train_set)) ## æ˜æ˜¾è¿‡æ‹Ÿåˆäº†ï½ 0.007 0.008 0.883302296710118 æ–‡æ¡£åˆ†ç±»1234567from nltk.corpus import movie_reviewsdocuments = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category) ] 123movie_reviews.categories() ['neg', 'pos'] 123456789neg_docu = movie_reviews.fileids('neg')print(len(neg_docu)) # negç±»åˆ«çš„æ–‡æ¡£æ•° 1000print(len(documents)) # æ€»çš„æ–‡æ¡£æ•° 1000len(movie_reviews.words(neg_docu[0])) # ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸­å•è¯æ•° 879 1000 2000 879 123random.shuffle(documents) æ–‡æ¡£åˆ†ç±»çš„ç‰¹å¾æå–å™¨æ‰€è°“ç‰¹å¾æå–å™¨å®é™…ä¸Šå°±æ˜¯å°†æ–‡æ¡£åŸæœ¬çš„å†…å®¹ç”¨è®¤ä¸ºé€‰å®šçš„ç‰¹å¾æ¥è¡¨ç¤ºã€‚ç„¶åç”¨åˆ†ç±»å™¨æ‰¾å‡ºè¿™äº›ç‰¹å¾å’Œå¯¹åº”ç±»æ ‡ç­¾çš„æ˜ å°„å…³ç³»ã€‚ é‚£ä¹ˆä»€ä¹ˆæ ·çš„ç‰¹å¾æ‰æ˜¯å¥½çš„ç‰¹å¾ï¼Œè¿™å°±æ˜¯ç‰¹å¾å·¥ç¨‹äº†å§ã€‚ æ–‡æœ¬åˆ†ç±»æ¦‚è¿°æ–‡æœ¬åˆ†ç±»ï¼Œé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯æ ¹æ®æ–‡æœ¬å†…å®¹æœ¬èº«å°†æ–‡æœ¬å½’ä¸ºä¸åŒçš„ç±»åˆ«ï¼Œé€šå¸¸æ˜¯æœ‰ç›‘ç£å­¦ä¹ çš„ä»»åŠ¡ã€‚æ ¹æ®æ–‡æœ¬å†…å®¹çš„é•¿çŸ­ï¼Œæœ‰åšå¥å­ã€æ®µè½æˆ–è€…æ–‡ç« çš„åˆ†ç±»ï¼›æ–‡æœ¬çš„é•¿çŸ­ä¸åŒå¯èƒ½ä¼šå¯¼è‡´æ–‡æœ¬å¯æŠ½å–çš„ç‰¹å¾ä¸Šçš„ç•¥å¾®å·®å¼‚ï¼Œä½†æ˜¯æ€»ä½“ä¸Šæ¥è¯´ï¼Œæ–‡æœ¬åˆ†ç±»çš„æ ¸å¿ƒéƒ½æ˜¯å¦‚ä½•ä»æ–‡æœ¬ä¸­æŠ½å–å‡ºèƒ½å¤Ÿä½“ç°æ–‡æœ¬ç‰¹ç‚¹çš„å…³é”®ç‰¹å¾ï¼ŒæŠ“å–ç‰¹å¾åˆ°ç±»åˆ«ä¹‹é—´çš„æ˜ å°„ã€‚ æ‰€ä»¥ï¼Œç‰¹å¾å·¥ç¨‹å°±æ˜¾å¾—éå¸¸é‡è¦ï¼Œç‰¹å¾æ‰¾çš„å¥½ï¼Œåˆ†ç±»æ•ˆæœä¹Ÿä¼šå¤§å¹…æé«˜ï¼ˆå½“ç„¶å‰ææ˜¯æ ‡æ³¨æ•°æ®è´¨é‡å’Œæ•°é‡ä¹Ÿè¦åˆé€‚ï¼Œæ•°æ®çš„å¥½åå†³å®šæ•ˆæœçš„ä¸‹é™ï¼Œç‰¹å¾å·¥ç¨‹å†³å®šæ•ˆæœçš„ä¸Šé™ï¼‰ã€‚ ä¹Ÿè®¸ä¼šæœ‰äººé—®æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯èƒ½å¤Ÿé¿å…æˆ‘ä»¬æ„é€ ç‰¹å¾è¿™ä»¶äº‹ï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦ç‰¹å¾å·¥ç¨‹ï¼Ÿæ·±åº¦å­¦ä¹ å¹¶ä¸æ˜¯ä¸‡èƒ½çš„ï¼Œåœ¨NLPé¢†åŸŸæ·±åº¦å­¦ä¹ æŠ€æœ¯å–å¾—çš„æ•ˆæœæœ‰é™ï¼ˆæ¯•ç«Ÿè¯­è¨€æ˜¯é«˜é˜¶æŠ½è±¡çš„ä¿¡æ¯ï¼Œæ·±åº¦å­¦ä¹ åœ¨å›¾åƒã€è¯­éŸ³è¿™äº›ä½é˜¶å…·ä½“çš„ä¿¡æ¯å¤„ç†ä¸Šæ›´é€‚åˆï¼Œå› ä¸ºåœ¨ä½é˜¶å…·ä½“çš„ä¿¡æ¯ä¸Šæ„é€ ç‰¹å¾æ˜¯ä¸€ä»¶è´¹åŠ›çš„äº‹æƒ…ï¼‰ï¼Œå¹¶ä¸æ˜¯å¦è®¤æ·±åº¦å­¦ä¹ åœ¨NLPé¢†åŸŸä¸Šå–å¾—çš„æˆç»©ï¼Œå·¥ä¸šç•Œç°åœ¨é€šç”¨çš„åšæ³•éƒ½æ˜¯ä¼šæŠŠæ·±åº¦å­¦ä¹ æ¨¡å‹ä½œä¸ºç³»ç»Ÿçš„ä¸€ä¸ªå­æ¨¡å—ï¼ˆä¹Ÿæ˜¯ä¸€ç»´ç‰¹å¾ï¼‰ï¼Œå’Œä¸€äº›ä¼ ç»Ÿçš„åŸºäºç»Ÿè®¡çš„è‡ªç„¶è¯­è¨€æŠ€æœ¯çš„ç‰¹å¾ï¼Œè¿˜æœ‰ä¸€äº›é’ˆå¯¹å…·ä½“ä»»åŠ¡æœ¬èº«ä¸“é—¨è®¾è®¡çš„ç‰¹å¾ï¼Œä¸€èµ·ä½œä¸ºä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹ï¼ˆä¹Ÿç§°Ensembleï¼Œå³æ¨¡å‹é›†æˆï¼‰çš„è¾“å…¥ï¼Œæœ€ç»ˆæ„æˆä¸€ä¸ªæ–‡æœ¬å¤„ç†ç³»ç»Ÿã€‚ ç‰¹å¾å·¥ç¨‹é‚£ä¹ˆï¼Œå¯¹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡è€Œè¨€ï¼Œå·¥ä¸šç•Œå¸¸ç”¨åˆ°çš„ç‰¹å¾æœ‰å“ªäº›å‘¢ï¼Ÿä¸‹é¢ç”¨ä¸€å¼ å›¾ä»¥æ¦‚æ‹¬ï¼š æˆ‘ä¸»è¦å°†è¿™äº›ç‰¹å¾åˆ†ä¸ºå››ä¸ªå±‚æ¬¡ï¼Œç”±ä¸‹å¾€ä¸Šï¼Œç‰¹å¾ç”±æŠ½è±¡åˆ°å…·ä½“ï¼Œç²’åº¦ä»ç»†åˆ°ç²—ã€‚æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿä»ä¸åŒçš„è§’åº¦å’Œçº¬åº¦æ¥è®¾è®¡ç‰¹å¾ï¼Œä»¥æ•æ‰è¿™äº›ç‰¹å¾å’Œç±»åˆ«ä¹‹é—´çš„å…³ç³»ã€‚ä¸‹é¢è¯¦ç»†ä»‹ç»è¿™å››ä¸ªå±‚æ¬¡ä¸Šå¸¸ç”¨åˆ°çš„ç‰¹å¾è¡¨ç¤ºã€‚","link":"/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"title":"æ–‡æœ¬åˆ†ç±»ç³»åˆ—4-textRCNN","text":"paper readingpaper: Recurrent Convolutional Neural Networks for Text Classification Introductionå…ˆå¯¹ä¹‹å‰çš„ç ”ç©¶è¿›è¡Œä¸€ç•ªæ‰¹åˆ¤(0.0). ä¼ ç»Ÿçš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•éƒ½æ˜¯åŸºäºç‰¹å¾å·¥ç¨‹ feature representationï¼Œä¸»è¦åŒ…æ‹¬ï¼š è¯è¢‹æ¨¡å‹ bag-of-words(BOW)modelï¼Œç”¨äºæå–unigram, bigram, n-gramsçš„ç‰¹å¾ã€‚ å¸¸è§çš„ç‰¹å¾é€‰æ‹©çš„æ–¹æ³•ï¼šfrequency, MI (Cover and Thomas 2012), pLSA (Cai and Hofmann 2003), LDA (Hingmire et al. 2013)ï¼Œç”¨äºé€‰æ‹©å…·æœ‰æ›´å¥½çš„åˆ¤åˆ«æ•ˆæœçš„ç‰¹å¾ã€‚ å…¶åŸç†å°±æ˜¯å»å™ªå£°æ¥æé«˜åˆ†ç±»æ•ˆæœã€‚æ¯”å¦‚å»æ‰åœç”¨è¯ï¼Œä½¿ç”¨ä¿¡æ¯å¢ç›Šï¼Œäº’ä¿¡æ¯ï¼Œæˆ–è€…L1æ­£åˆ™åŒ–æ¥è·å–æœ‰ç”¨çš„ç‰¹å¾ã€‚ä½†ä¼ ç»Ÿçš„ç‰¹å¾è¡¨ç¤ºçš„æ–¹æ³•é€šå¸¸å¿½è§†äº†ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œè¯åºä¿¡æ¯ã€‚ Richard Socher æå‡ºçš„ Recursive Neural Network RecusiveNN é€šè¿‡è¯­è¨€çš„treeç»“æ„æ¥è·å–å¥å­çš„è¯­ä¹‰ä¿¡æ¯ã€‚ä½†æ˜¯åˆ†ç±»çš„å‡†ç¡®ç‡å¤ªä¾èµ–æ–‡æœ¬çš„æ ‘ç»“æ„ã€‚åœ¨æ–‡æœ¬åˆ†ç±»ä¹‹å‰å»ºç«‹ä¸€ä¸ªæ ‘ç»“æ„éœ€è¦çš„è®¡ç®—å¤æ‚åº¦å°±æ˜¯ $O(n^2)$ ï¼ˆnæ˜¯å¥å­çš„é•¿åº¦ï¼‰ã€‚æ‰€ä»¥å¯¹äºå¾ˆé•¿çš„å¥å­å¹¶ä¸é€‚ç”¨ã€‚ å¾ªç¯ç¥ç»ç½‘ç»œ Recurrent Neural Network è®¡ç®—å¤æ‚åº¦æ˜¯ $O(n)$ï¼Œä¼˜ç‚¹æ˜¯èƒ½å¤Ÿå¾ˆå¥½çš„æ•è·é•¿æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½†æ˜¯åœ¨rnnæ¨¡å‹ä¸­ï¼Œlater words are more dominatant than earlier words. ä½†æ˜¯å¦‚æœå¯¹ä¸æŸä¸€ä¸ªæ–‡æœ¬çš„åˆ†ç±»ï¼Œå‡ºç°åœ¨ä¹‹å‰çš„wordå½±å“æ›´å¤§çš„è¯ï¼ŒRNNçš„è¡¨ç°å°±ä¸ä¼šå¾ˆå¥½ã€‚ ä¸ºè§£å†³RNNè¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥å°†CNNè¿™ä¸ªæ²¡æœ‰åè§çš„æ¨¡å‹å¼•å…¥åˆ°NLPçš„å·¥ä½œä¸­æ¥ï¼ŒCNNèƒ½å…¬å¹³çš„å¯¹å¾…å¥å­ä¸­çš„æ¯ä¸€ä¸ªçŸ­è¯­ã€‚ To tackle the bias problem, the Convolutional Neural Network (CNN), an unbiased model is introduced to NLP tasks, which can fairly determine discriminative phrases in a text with a max-pooling layer. ä½†æ˜¯å‘¢ï¼Œé€šè¿‡å‰é¢çš„å­¦ä¹ æˆ‘ä»¬çŸ¥é“CNNçš„filteræ˜¯å›ºå®šå°ºå¯¸çš„ï¼ˆfixed windowï¼‰ï¼Œå¦‚æœå°ºå¯¸å¤ªçŸ­ï¼Œä¼šä¸¢å¤±å¾ˆå¤šä¿¡æ¯ï¼Œå¦‚æœå°ºå¯¸è¿‡é•¿ï¼Œè®¡ç®—å¤æ‚åº¦åˆå¤ªå¤§ã€‚æ‰€ä»¥ä½œè€…æå‡ºä¸ªé—®é¢˜ï¼šèƒ½ä¸èƒ½é€šè¿‡åŸºäºçª—å£çš„ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å­¦åˆ°æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ›´å¥½çš„è¡¨ç¤ºæ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯å‘¢ï¼Ÿ Therefore, it raises a question: can we learn more contextual information than conventional window-based neural networks and represent the semantic of texts more precisely for text classification. äºæ˜¯ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºäº† Recurrent Concolution Neural Network(RCNN). Model $$c_l{(w_i)} = f(W^{(l)}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))$$ $$c_r{(w_i)} = f(W^{(r)}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))$$ è¿™ä¸¤ä¸ªå…¬å¼ç±»ä¼¼äºåŒå‘RNNï¼Œå°† $c_l(w_i)$ çœ‹ä½œå‰ä¸€ä¸ªæ—¶åˆ»çš„éšè—çŠ¶æ€ $h_{t-1}$, $c_l(w_{i-1})$ å°±æ˜¯ t-2 æ—¶åˆ»çš„éšè—çŠ¶æ€ $h_{t-2}$â€¦ æ‰€ä»¥è¿™å°±æ˜¯ä¸ªåŒå‘RNNâ€¦. ç„¶åæ¯”è¾ƒæœ‰åˆ›æ–°çš„æ˜¯ï¼Œä½œè€…å°†éšè—çŠ¶æ€ $h_{t-1}$ å’Œ $\\tilde h_{t+1}$ ($\\tilde h$ è¡¨ç¤ºåå‘), ä»¥åŠå½“å‰wordçš„è¯å‘é‡å †åœ¨ä¸€èµ·ï¼Œä½œä¸ºå½“å‰è¯ä»¥åŠè·å–äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å‘é‡è¡¨ç¤ºã€‚ $$x_i = [c_l(w_i);e(w_i);c_r(w_i)]$$ ç„¶åæ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œè¿™ä¸ªå¯ä»¥çœ‹åštextCNNä¸­çš„å·ç§¯å±‚,åªæ˜¯filter_size=1ï¼š $$y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})$$ æ¥ç€æ˜¯æœ€å¤§æ± åŒ–å±‚ï¼š $$y^{(3)} = max_{i=1}^ny_i^{(2)}$$ ç„¶åæ˜¯å…¨è¿æ¥å±‚+softmaxï¼š $$y^{(4)} = W^{(4)}y^{(3)}+b^{(4)}$$ $$p_i=\\dfrac{exp(y_i^{(4)})}{\\sum_{k=1}^nexp(y_k^{(4)})}$$ æ„Ÿè§‰å°±æ˜¯åŒå‘rnnå‘€ï¼Œåªä¸è¿‡ä¹‹å‰çš„æ–¹æ³•æ˜¯ç”¨æœ€åä¸€ä¸ªéšè—å±‚çš„è¾“å‡ºä½œä¸ºæ•´ä¸ªsentenceçš„å‘é‡è¡¨ç¤ºï¼Œä½†è¿™ç¯‡è®ºæ–‡æ˜¯ç”¨æ¯ä¸€ä¸ªæ—¶åˆ»çš„å‘é‡è¡¨ç¤º(å åŠ äº†ä¸Šä¸‹æ—¶åˆ»çš„éšè—çŠ¶æ€)ï¼Œé€šè¿‡å·ç§¯å±‚ã€maxpoolåå¾—åˆ°çš„å‘é‡æ¥è¡¨ç¤ºæ•´ä¸ªsentence. ç¡®å®æ˜¯è§£å†³äº†RNNè¿‡äºé‡è§†å¥å­ä¸­é åçš„è¯çš„é—®é¢˜ï¼Œä½†æ˜¯RNNè®­ç»ƒæ…¢çš„é—®é¢˜è¿˜æ˜¯æ²¡æœ‰è§£å†³å‘€ã€‚ä½†æ˜¯åœ¨è¿™é‡Œ brightmart/text_classification ä¸­textCNN å’Œ RCNNçš„è®­ç»ƒæ—¶é—´å±…ç„¶æ˜¯ä¸€æ ·çš„ã€‚whyï¼Ÿ Results and Discussionä»£ç å®ç°éœ€è¦æ³¨æ„çš„é—®é¢˜ï¼š tf.nn.rnn_cell.DropoutWrapper tf.nn.bidirectional_dynamic_rnn tf.einsum æŸå¤±å‡½æ•°çš„å¯¹æ¯” tf.nn.softmax_cross_entropy_with_logits è¯å‘é‡æ˜¯å¦éœ€è¦æ­£åˆ™åŒ– tensorflow.contrib.layers.python.layers import optimize_loss å’Œ tf.train.AdamOptimizer(learning_rate).minimize(self.loss, self.global_steps) çš„åŒºåˆ«","link":"/2018/06/01/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%974-textRCNN/"},{"title":"æ–‡æœ¬åˆ†ç±»ç³»åˆ—2-textCNN","text":"paper readingä¸»è¦æ¡†æ¶å’Œä½¿ç”¨CNNè¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„æ„å›¾å‚è€ƒpaper: Convolutional Neural Networks for Sentence Classification å¯å‚è€ƒcs224dä¸­çš„è¯¾å ‚ç¬”è®°ï¼Œè¿™å ‚è¯¾å°±æ˜¯è®²çš„è¿™ç¯‡paperï¼š cs224d-lecture13 å·ç§¯ç¥ç»ç½‘ç»œ TextCNNè¯¦ç»†è¿‡ç¨‹ï¼š ç¬¬ä¸€å±‚æ˜¯å›¾ä¸­æœ€å·¦è¾¹çš„7ä¹˜5çš„å¥å­çŸ©é˜µï¼Œæ¯è¡Œæ˜¯è¯å‘é‡ï¼Œç»´åº¦=5ï¼Œè¿™ä¸ªå¯ä»¥ç±»æ¯”ä¸ºå›¾åƒä¸­çš„åŸå§‹åƒç´ ç‚¹äº†,ç„¶ååœ¨å›¾åƒä¸­å›¾åƒçš„è¡¨ç¤ºæ˜¯[length, width, channel],è¿™é‡Œå°†æ–‡æœ¬çš„è¡¨ç¤º[sequence_len, embed_size, 1]ã€‚å¯ä»¥çœ‹åˆ°ä¸‹é¢ä»£ç ä¸­ï¼š 1234567embeded_words = tf.nn.embedding_lookup(self.embedding, self.input_x) # [None, sentence_len, embed_size]# three channels similar to the image. using the tf.nn.conv2dself.sentence_embedding_expanded = tf.expand_dims(embeded_words, axis=-1) # [None, sentence_len, embed_size, 1] ç„¶åç»è¿‡æœ‰ filter_size=(2,3,4) çš„ä¸€ç»´å·ç§¯å±‚ï¼Œæ¯ä¸ªfilter_size æœ‰ä¸¤ä¸ªè¾“å‡º channelã€‚ä¸Šå›¾ä¸­çš„filteræœ‰3ä¸ªï¼Œåˆ†åˆ«ä¸ºï¼š filter:[2, 5, 2] ==&gt; feature map:[6,1,2] filter:[3, 5, 2] ==&gt; feature map:[5,1,2] filter:[4, 5, 2] ==&gt; feature map:[4,1,2] ç¬¬ä¸‰ç»´è¡¨ç¤ºchannelsï¼Œå·ç§¯åå¾—åˆ°ä¸¤ä¸ªfeature maps. ç¬¬ä¸‰å±‚æ˜¯ä¸€ä¸ª1-max poolingå±‚ï¼Œè¿™æ ·ä¸åŒé•¿åº¦å¥å­ç»è¿‡poolingå±‚ä¹‹åéƒ½èƒ½å˜æˆscaleï¼Œè¿™é‡Œæ¯ä¸ªfiter_sizeçš„channelä¸º2ï¼Œæ‰€ä»¥è¾“å…¥çš„pooling.shape=[batch_size,1,1,2], ç„¶åconcatä¸ºä¸€ä¸ªflattenå‘é‡ã€‚ æœ€åæ¥ä¸€å±‚å…¨è¿æ¥çš„ softmax å±‚ï¼Œè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚ ç‰¹å¾ï¼šè¿™é‡Œçš„ç‰¹å¾å°±æ˜¯è¯å‘é‡ï¼Œæœ‰é™æ€ï¼ˆstaticï¼‰å’Œéé™æ€ï¼ˆnon-staticï¼‰æ–¹å¼ã€‚staticæ–¹å¼é‡‡ç”¨æ¯”å¦‚word2vecé¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸æ›´æ–°è¯å‘é‡ï¼Œå®è´¨ä¸Šå±äºè¿ç§»å­¦ä¹ äº†ï¼Œç‰¹åˆ«æ˜¯æ•°æ®é‡æ¯”è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œé‡‡ç”¨é™æ€çš„è¯å‘é‡å¾€å¾€æ•ˆæœä¸é”™ã€‚non-staticåˆ™æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°è¯å‘é‡ã€‚æ¨èçš„æ–¹å¼æ˜¯ non-static ä¸­çš„ fine-tunningæ–¹å¼ï¼Œå®ƒæ˜¯ä»¥é¢„è®­ç»ƒï¼ˆpre-trainï¼‰çš„word2vecå‘é‡åˆå§‹åŒ–è¯å‘é‡ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´è¯å‘é‡ï¼Œèƒ½åŠ é€Ÿæ”¶æ•›ï¼Œå½“ç„¶å¦‚æœæœ‰å……è¶³çš„è®­ç»ƒæ•°æ®å’Œèµ„æºï¼Œç›´æ¥éšæœºåˆå§‹åŒ–è¯å‘é‡æ•ˆæœä¹Ÿæ˜¯å¯ä»¥çš„ã€‚ é€šé“ï¼ˆChannelsï¼‰ï¼šå›¾åƒä¸­å¯ä»¥åˆ©ç”¨ (R, G, B) ä½œä¸ºä¸åŒchannelï¼Œè€Œæ–‡æœ¬çš„è¾“å…¥çš„channelé€šå¸¸æ˜¯ä¸åŒæ–¹å¼çš„embeddingæ–¹å¼ï¼ˆæ¯”å¦‚ word2vecæˆ–Gloveï¼‰ï¼Œå®è·µä¸­ä¹Ÿæœ‰åˆ©ç”¨é™æ€è¯å‘é‡å’Œfine-tunningè¯å‘é‡ä½œä¸ºä¸åŒchannelçš„åšæ³•ã€‚ä¸‹é¢ä»£ç ä¸­çš„é€šé“ä¸º1. ä¸€ç»´å·ç§¯ï¼ˆconv-1dï¼‰ï¼šå›¾åƒæ˜¯äºŒç»´æ•°æ®ï¼Œç»è¿‡è¯å‘é‡è¡¨è¾¾çš„æ–‡æœ¬ä¸ºä¸€ç»´æ•°æ®ï¼Œå› æ­¤åœ¨TextCNNå·ç§¯ç”¨çš„æ˜¯ä¸€ç»´å·ç§¯ã€‚ä¸€ç»´å·ç§¯å¸¦æ¥çš„é—®é¢˜æ˜¯éœ€è¦è®¾è®¡é€šè¿‡ä¸åŒ filter_size çš„ filter è·å–ä¸åŒå®½åº¦çš„è§†é‡ã€‚ Poolingå±‚ï¼šåˆ©ç”¨CNNè§£å†³æ–‡æœ¬åˆ†ç±»é—®é¢˜çš„æ–‡ç« è¿˜æ˜¯å¾ˆå¤šçš„ï¼Œæ¯”å¦‚è¿™ç¯‡ A Convolutional Neural Network for Modelling Sentences æœ€æœ‰æ„æ€çš„è¾“å…¥æ˜¯åœ¨ pooling æ”¹æˆ (dynamic) k-max pooling ï¼Œpoolingé˜¶æ®µä¿ç•™ k ä¸ªæœ€å¤§çš„ä¿¡æ¯ï¼Œä¿ç•™äº†å…¨å±€çš„åºåˆ—ä¿¡æ¯ã€‚æ¯”å¦‚åœ¨æƒ…æ„Ÿåˆ†æåœºæ™¯ï¼Œä¸¾ä¸ªä¾‹å­ï¼š â€œ æˆ‘è§‰å¾—è¿™ä¸ªåœ°æ–¹æ™¯è‰²è¿˜ä¸é”™ï¼Œä½†æ˜¯äººä¹Ÿå®åœ¨å¤ªå¤šäº† â€ è™½ç„¶å‰åŠéƒ¨åˆ†ä½“ç°æƒ…æ„Ÿæ˜¯æ­£å‘çš„ï¼Œå…¨å±€æ–‡æœ¬è¡¨è¾¾çš„æ˜¯åè´Ÿé¢çš„æƒ…æ„Ÿï¼Œåˆ©ç”¨ k-max poolingèƒ½å¤Ÿå¾ˆå¥½æ•æ‰è¿™ç±»ä¿¡æ¯ã€‚ ä»£ç å®ç°å‚æ•°è®¾ç½®å’Œæ¨¡å‹å…·ä½“å®ç°å‚è€ƒpaper: A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification è®¾è®¡ä¸€ä¸ªæ¨¡å‹éœ€è¦è€ƒè™‘çš„ï¼š input word vector representations è¾“å…¥çš„è¯å‘é‡è¡¨ç¤º filter region size(s); å·ç§¯æ ¸çš„å¤§å° the number of feature maps; ç‰¹å¾å›¾çš„é€šé“æ•° the activation function æ¿€æ´»å‡½æ•° the pooling strategy æ± åŒ–çš„æ–¹å¼ regularization terms (dropout/l2) æ­£åˆ™åŒ–é¡¹ï¼ˆdropout/l2ï¼‰ éœ€è¦æ³¨æ„çš„é—®é¢˜ä¸€ä¸ªå•æœ¬å¯¹åº”å•ä¸ªæ ‡ç­¾å’Œå¤šä¸ªæ ‡ç­¾çš„åŒºåˆ«ï¼Ÿå…³äºå¤šæ ‡ç­¾åˆ†ç±»ï¼Œåº”è¯¥çœ‹çœ‹å‘¨å¿—åè€å¸ˆçš„è¿™ç¯‡æ–‡ç« A Review on Multi-Label Learning Algorithms, çŸ¥ä¹ä¸Šè¿˜æœ‰å…¶ä»–èµ„æ–™å¤šæ ‡ç­¾ï¼ˆmulti-labelï¼‰æ•°æ®çš„å­¦ä¹ é—®é¢˜ï¼Œå¸¸ç”¨çš„åˆ†ç±»å™¨æˆ–è€…åˆ†ç±»ç­–ç•¥æœ‰å“ªäº›ï¼Ÿ æœ¬æ–‡ä»£ç ä¸­çš„æ–¹æ³•ï¼š çœŸå®å€¼labelsçš„è¾“å…¥ï¼šå•ä¸ªæ ‡ç­¾çš„çœŸå®å€¼æ˜¯ input_y.shape=[batch_size], å¤šä¸ªæ ‡ç­¾çš„çœŸå®å€¼æ˜¯ input_y_multilabels.shape=[batch_size, label_size] 12345self.input_y = tf.placeholder(dtype=tf.int32, shape=[None], name='input_y')self.input_y_multilabels = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name=&quot;input_y_multilabels&quot;) æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼š è¯„ä»·æŒ‡æ ‡çš„åŒºåˆ«ï¼š","link":"/2018/05/30/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%972-textCNN/"},{"title":"æœºå™¨å­¦ä¹ -ä¸å‡è¡¡æ•°æ®é—®é¢˜","text":"å¯¹äºæœºå™¨å­¦ä¹ ä¸­ä¸å‡è¡¡æ•°æ®çš„é—®é¢˜æ˜¯å¾ˆå¸¸è§çš„ï¼Œä¹‹å‰åœ¨ä¸‰æ˜Ÿå®ä¹ ä¹Ÿé‡åˆ°è¿‡ï¼Œä½†å½“æ—¶å¯¹æŒ‡æ ‡æ€§èƒ½è¦æ±‚å¹¶ä¸é«˜ï¼Œå·®ä¸å¤šä¹Ÿå°±è¡Œäº†ã€‚ã€‚ä½†è¿™æ¬¡å‚åŠ  kaggle æ¯”èµ›ï¼Œ0.1% å°±æœ‰å¾ˆå¤§å·®è·äº†ï¼Œæ‰€ä»¥è¿˜æ˜¯è¦å¥½å¥½ææ imbalanced data problem. å¾ˆå¤šæ—¶å€™ï¼Œæ•°æ®ä¸å‡è¡¡ä¸ä»…ä»…æ•°æ®æ”¶é›†å’Œæ•´ç†çš„é—®é¢˜ï¼Œè€Œæ˜¯æ•°æ®æœ¬èº«é¢„æœŸçš„å°±æ˜¯è¿™æ ·ï¼Œæ¯”å¦‚è¡¨å¾æ¬ºè¯ˆæ€§äº¤æ˜“ï¼ˆcharacterize fraudulent transactionsï¼‰ï¼Œå¤§å¤šæ•°äº¤æ˜“éƒ½æ˜¯ä¸å…·æœ‰æ¬ºè¯ˆæ€§çš„,åªæœ‰æå°‘æ•°çš„ å…·æœ‰æ¬ºè¯ˆæ€§çš„ï¼ˆFraudï¼‰, æˆ–è€… kaggle æ¯”èµ›ä¸­çš„ insincere-questions, å¤§å¤šæ•°é—®é¢˜éƒ½æ˜¯æ­£å¸¸çš„ï¼Œåªæœ‰æå°‘æ•°çš„ insincere-questions. More dataæ˜¾ç„¶ï¼Œè¿™æ˜¯æœ€ç›´æ¥çš„ã€‚åœ¨å®é™…ä¸­ï¼Œä¹Ÿæ˜¯æœ€å¯è¡Œçš„ã€‚ä½†æ‰“æ¯”èµ›æ—¶ï¼Œæ•°æ®æ˜¯ç»™å®šçš„ï¼Œå°±éœ€è¦ç”¨åˆ° resampling. Metricsæ”¹å˜è¯„ä»·æŒ‡æ ‡æ¥é€‰æ‹©æ›´ä¼˜çš„æ¨¡å‹ã€‚ accurayc æ˜¾ç„¶è¿™ä¸ªå¯¹ä¸å‡è¡¡æ•°æ®æ˜¯ä¸åˆé€‚çš„ Precision: (tp/(tp+fp)) Recall: (tp/(tp+fn)) F1 Score (or F-score): precision å’Œ recall çš„ä¸€ç§æƒè¡¡ Kappa: Cohenâ€™s kappa ROC æ›²çº¿ï¼šè¿™äº›åœ¨ä¹‹å‰çš„ç¬”è®°ä¸­éƒ½æœ‰ä»‹ç» æœºå™¨å­¦ä¹ -å¸¸ç”¨æŒ‡æ ‡æ€»ç»“ PRAUC æŸå¤±å‡½æ•°ï¼Ÿ ai challenger æ¯”èµ›ä¸­çœ‹åˆ°çš„ Resampling è¿‡é‡‡æ · over-sampling æ¬ é‡‡æ · under-sampling å®è·µä¸­å¯ä»¥ä¸¤ç§éƒ½å°è¯•ï¼Œå¹¶ä¸”èƒ½åšé›†æˆå¢å¼ºã€‚ ä¸€äº›é‡‡æ ·ç­–ç•¥ï¼š æ•°æ®é‡å¾ˆå¤§ï¼ˆä¸Šä¸‡æˆ–è€…åä¸‡ï¼Ÿï¼‰ï¼Œå»ºè®®æ¬ é‡‡æ ·ï¼Œæ•°æ®é‡è¾ƒå°ï¼Œå»ºè®®è¿‡é‡‡æ · å¯ä»¥å°è¯•éšæœºé‡‡æ ·ï¼Œä¹Ÿå¯ä»¥å°è¯•åˆ†å±‚é‡‡æ ·ï¼ˆåˆ’åˆ†å¥½ï¼‰ å°è¯•ä¸åŒçš„æ¯”ä¾‹ï¼Œå¹¶ä¸ä¸€å®šè¦æœ€åæ˜¯ 1:1 Generate Synthetic SamplesSMOTE(Synthetic Minority Over-sampling Technique): ä¸æ˜¯ç®€å•çš„ copy, è€Œæ˜¯é€‰å–ä¸¤ä¸ªæˆ–æ›´å¤šçš„ç›¸ä¼¼çš„æ ·æœ¬ï¼ˆæ ¹æ®è·ç¦»ï¼‰ï¼Œç„¶åéšæœºæ‰°åŠ¨ä¸€ä¸ªæ ·æœ¬çš„å±æ€§ï¼ˆæŸä¸€ç»´ç‰¹å¾å§ï¼‰ï¼Œæ‰°åŠ¨å€¼åœ¨å®ƒä¸ç›¸é‚»æ ·æœ¬çš„å·®å¼‚ä¹‹é—´ã€‚ paper: SMOTE: Synthetic Minority Over-sampling Technique python tool: UnbalancedDataset Try Penalized Modelsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç»™ç±»åˆ«è¾ƒå°‘çš„ä¸€ç±»å¢åŠ æƒ©ç½šé¡¹ï¼Œé€šå¸¸æ˜¯æ­£åˆ™åŒ–ï¼è¿™æœ‰åˆ©äºæ¨¡å‹èƒ½æ³¨é‡ minority class. å¯¹ç±»å’Œæƒé‡çš„æƒ©ç½šå¯¹ä¸åŒçš„ç®—æ³•ä¸å¤ªä¸€æ ·ï¼Œæ‰€æœ‰æœ‰ä¸“é—¨çš„ç‰ˆæœ¬çš„ç®—æ³•ï¼špenalized-SVM å’Œ penalized-LDA. ä¹Ÿæœ‰é€šç”¨çš„æƒ©ç½šæ¨¡å‹ï¼Œé€‚ç”¨äºä¸åŒçš„åˆ†ç±»å™¨ CostSensitiveClassifier æƒ©ç½šæ¨¡å‹æä¾›äº†å¦å¤–ä¸€ç§æ–¹æ³•æ¥ â€œbalanceâ€ æ¨¡å‹ï¼Œä½†è®¾ç½®æƒ©ç½šçŸ©é˜µæ˜¯å¾ˆå¤æ‚çš„ï¼Œéœ€è¦å¾ˆå¤šå°è¯•ã€‚ Try a Different Perspectiveå¯¹äºä¹‹å‰è¯´è¿‡çš„æ¬ºè¯ˆæ€§æ£€æµ‹å’Œkaggle insincere é—®é¢˜çš„å‘ç°ï¼Œä»å¦å¤–ä¸€ä¸ªè§’åº¦çœ‹ï¼Œä¹Ÿèƒ½çœ‹åšæ˜¯å¼‚å¸¸æ£€æµ‹ï¼ˆanomaly detectionï¼‰å’Œ å˜å¼‚æ£€æµ‹ï¼ˆchange detectionï¼‰. ç±»ä¼¼äº open set recognition æˆ– out-of-distribution é—®é¢˜ï¼Œæ ·æœ¬æ•°æå°‘çš„é‚£ä¸ªç±»åˆ« minior class å¯ä»¥çœ‹åšæ˜¯ outliers class. Anomaly detection is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by itâ€™s sequence of system calls. The events are rare and when compared to normal operation. Change detection is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.","link":"/2018/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%8D%E5%9D%87%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98/"},{"title":"æœºå™¨å­¦ä¹ -ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†","text":"ä¸­æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ç‰¹ç‚¹å‚è€ƒï¼šhttps://www.cnblogs.com/pinard/p/6744056.html é¦–å…ˆæˆ‘ä»¬çœ‹çœ‹ä¸­æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†å’Œè‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ç›¸æ¯”çš„ä¸€äº›ç‰¹æ®Šç‚¹ã€‚ é¦–å…ˆï¼Œä¸­æ–‡æ–‡æœ¬æ˜¯æ²¡æœ‰åƒè‹±æ–‡çš„å•è¯ç©ºæ ¼é‚£æ ·éš”å¼€çš„ï¼Œå› æ­¤ä¸èƒ½ç›´æ¥åƒè‹±æ–‡ä¸€æ ·å¯ä»¥ç›´æ¥ç”¨æœ€ç®€å•çš„ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·å®Œæˆåˆ†è¯ã€‚æ‰€ä»¥ä¸€èˆ¬æˆ‘ä»¬éœ€è¦ç”¨åˆ†è¯ç®—æ³•æ¥å®Œæˆåˆ†è¯ï¼Œåœ¨æ–‡æœ¬æŒ–æ˜çš„åˆ†è¯åŸç†ä¸­ï¼Œæˆ‘ä»¬å·²ç»è®²åˆ°äº†ä¸­æ–‡çš„åˆ†è¯åŸç†ï¼Œè¿™é‡Œå°±ä¸å¤šè¯´ã€‚ ç¬¬äºŒï¼Œä¸­æ–‡çš„ç¼–ç ä¸æ˜¯utf8ï¼Œè€Œæ˜¯unicodeã€‚è¿™æ ·ä¼šå¯¼è‡´åœ¨åˆ†è¯çš„æ—¶å€™ï¼Œå’Œè‹±æ–‡ç›¸æ¯”ï¼Œæˆ‘ä»¬è¦å¤„ç†ç¼–ç çš„é—®é¢˜ã€‚ è¿™ä¸¤ç‚¹æ„æˆäº†ä¸­æ–‡åˆ†è¯ç›¸æ¯”è‹±æ–‡åˆ†è¯çš„ä¸€äº›ä¸åŒç‚¹ï¼Œåé¢æˆ‘ä»¬ä¹Ÿä¼šé‡ç‚¹è®²è¿°è¿™éƒ¨åˆ†çš„å¤„ç†ã€‚å½“ç„¶ï¼Œè‹±æ–‡åˆ†è¯ä¹Ÿæœ‰è‡ªå·±çš„çƒ¦æ¼ï¼Œè¿™ä¸ªæˆ‘ä»¬åœ¨ä»¥åå†è®²ã€‚äº†è§£äº†ä¸­æ–‡é¢„å¤„ç†çš„ä¸€äº›ç‰¹ç‚¹åï¼Œæˆ‘ä»¬å°±è¨€å½’æ­£ä¼ ï¼Œé€šè¿‡å®è·µæ€»ç»“ä¸‹ä¸­æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†æµç¨‹ã€‚ æ•°æ®é›†æ”¶é›†åœ¨æ–‡æœ¬æŒ–æ˜ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¾—åˆ°æ–‡æœ¬æ•°æ®ï¼Œæ–‡æœ¬æ•°æ®çš„è·å–æ–¹æ³•ä¸€èˆ¬æœ‰ä¸¤ç§ï¼šä½¿ç”¨åˆ«äººåšå¥½çš„è¯­æ–™åº“å’Œè‡ªå·±ç”¨çˆ¬è™«å»åœ¨ç½‘ä¸Šå»çˆ¬è‡ªå·±çš„è¯­æ–™æ•°æ®ã€‚ å¯¹äºç¬¬ä¸€ç§æ–¹æ³•ï¼Œå¸¸ç”¨çš„æ–‡æœ¬è¯­æ–™åº“åœ¨ç½‘ä¸Šæœ‰å¾ˆå¤šï¼Œå¦‚æœå¤§å®¶åªæ˜¯å­¦ä¹ ï¼Œåˆ™å¯ä»¥ç›´æ¥ä¸‹è½½ä¸‹æ¥ä½¿ç”¨ï¼Œä½†å¦‚æœæ˜¯æŸäº›ç‰¹æ®Šä¸»é¢˜çš„è¯­æ–™åº“ï¼Œæ¯”å¦‚â€œæœºå™¨å­¦ä¹ â€ç›¸å…³çš„è¯­æ–™åº“ï¼Œåˆ™è¿™ç§æ–¹æ³•è¡Œä¸é€šï¼Œéœ€è¦æˆ‘ä»¬è‡ªå·±ç”¨ç¬¬äºŒç§æ–¹æ³•å»è·å–ã€‚ å¯¹äºç¬¬äºŒç§ä½¿ç”¨çˆ¬è™«çš„æ–¹æ³•ï¼Œå¼€æºå·¥å…·æœ‰å¾ˆå¤šï¼Œé€šç”¨çš„çˆ¬è™«æˆ‘ä¸€èˆ¬ä½¿ç”¨beautifulsoupã€‚ä½†æ˜¯æˆ‘ä»¬æˆ‘ä»¬éœ€è¦æŸäº›ç‰¹æ®Šçš„è¯­æ–™æ•°æ®ï¼Œæ¯”å¦‚ä¸Šé¢æåˆ°çš„â€œæœºå™¨å­¦ä¹ â€ç›¸å…³çš„è¯­æ–™åº“ï¼Œåˆ™éœ€è¦ç”¨ä¸»é¢˜çˆ¬è™«ï¼ˆä¹Ÿå«èšç„¦çˆ¬è™«ï¼‰æ¥å®Œæˆã€‚è¿™ä¸ªæˆ‘ä¸€èˆ¬ä½¿ç”¨acheã€‚ acheå…è®¸æˆ‘ä»¬ç”¨å…³é”®å­—æˆ–è€…ä¸€ä¸ªåˆ†ç±»ç®—æ³•æ¥è¿‡æ»¤å‡ºæˆ‘ä»¬éœ€è¦çš„ä¸»é¢˜è¯­æ–™ï¼Œæ¯”è¾ƒå¼ºå¤§ã€‚ é™¤å»æ•°æ®ä¸­éæ–‡æœ¬éƒ¨åˆ†è¿™ä¸€æ­¥ä¸»è¦æ˜¯é’ˆå¯¹æˆ‘ä»¬ç”¨çˆ¬è™«æ”¶é›†çš„è¯­æ–™æ•°æ®ï¼Œç”±äºçˆ¬ä¸‹æ¥çš„å†…å®¹ä¸­æœ‰å¾ˆå¤šhtmlçš„ä¸€äº›æ ‡ç­¾ï¼Œéœ€è¦å»æ‰ã€‚å°‘é‡çš„éæ–‡æœ¬å†…å®¹çš„å¯ä»¥ç›´æ¥ç”¨Pythonçš„æ­£åˆ™è¡¨è¾¾å¼(re)åˆ é™¤, å¤æ‚çš„åˆ™å¯ä»¥ç”¨beautifulsoupæ¥å»é™¤ã€‚å»é™¤æ‰è¿™äº›éæ–‡æœ¬çš„å†…å®¹åï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡ŒçœŸæ­£çš„æ–‡æœ¬é¢„å¤„ç†äº†ã€‚ å¤„ç†ä¸­æ–‡ç¼–ç é—®é¢˜ç”±äºPython2ä¸æ”¯æŒunicodeçš„å¤„ç†ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨Python2åšä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†æ—¶éœ€è¦éµå¾ªçš„åŸåˆ™æ˜¯ï¼Œå­˜å‚¨æ•°æ®éƒ½ç”¨utf8ï¼Œè¯»å‡ºæ¥è¿›è¡Œä¸­æ–‡ç›¸å…³å¤„ç†æ—¶ï¼Œä½¿ç”¨GBKä¹‹ç±»çš„ä¸­æ–‡ç¼–ç ï¼Œåœ¨ä¸‹é¢ä¸€èŠ‚çš„åˆ†è¯æ—¶ï¼Œæˆ‘ä»¬å†ç”¨ä¾‹å­è¯´æ˜è¿™ä¸ªé—®é¢˜ã€‚ ä¸­æ–‡åˆ†è¯å¸¸ç”¨çš„ä¸­æ–‡åˆ†è¯è½¯ä»¶æœ‰å¾ˆå¤šï¼Œä¸ªäººæ¯”è¾ƒæ¨èç»“å·´åˆ†è¯ã€‚å®‰è£…ä¹Ÿå¾ˆç®€å•ï¼Œæ¯”å¦‚åŸºäºPythonçš„ï¼Œç”¨â€pip install jiebaâ€å°±å¯ä»¥å®Œæˆã€‚ä¸‹é¢æˆ‘ä»¬å°±ç”¨ä¾‹å­æ¥çœ‹çœ‹å¦‚ä½•ä¸­æ–‡åˆ†è¯ã€‚ é¦–å…ˆæˆ‘ä»¬å‡†å¤‡äº†ä¸¤æ®µæ–‡æœ¬ï¼Œè¿™ä¸¤æ®µæ–‡æœ¬åœ¨ä¸¤ä¸ªæ–‡ä»¶ä¸­ã€‚ä¸¤æ®µæ–‡æœ¬çš„å†…å®¹åˆ†åˆ«æ˜¯nlp_test0.txtå’Œnlp_test2.txtï¼š 1234567891011import jiebawith open(&quot;./nlp_test1.txt&quot;) as f: document = f.read() # å¦‚æœæ˜¯python2ï¼Œåˆ™éœ€è¦ç”¨ decode(&quot;GBK&quot;) document_cut = jieba.cut(document)document_cut &lt;generator object Tokenizer.cut at 0x7f6a84cf09e8&gt; 12345result = &quot; &quot;.join(document_cut)result Building prefix dict from the default dictionary ... Loading model from cache /tmp/jieba.cache Loading model cost 0.438 seconds. Prefix dict has been built succesfully. ' æ²™ ç‘é‡‘ èµå¹ æ˜“ å­¦ä¹  çš„ èƒ¸æ€€ ï¼Œ æ˜¯ é‡‘å±± çš„ ç™¾å§“ æœ‰ç¦ ï¼Œ å¯æ˜¯ è¿™ä»¶ äº‹å¯¹ æè¾¾åº· çš„ è§¦åŠ¨ å¾ˆå¤§ ã€‚ æ˜“ å­¦ä¹  åˆ å›å¿†èµ· ä»–ä»¬ ä¸‰äºº åˆ†å¼€ çš„ å‰ä¸€æ™š ï¼Œ å¤§å®¶ ä¸€èµ· å–é…’ è¯åˆ« ï¼Œ æ˜“ å­¦ä¹  è¢« é™èŒ åˆ° é“å£ å¿å½“ å¿é•¿ ï¼Œ ç‹ å¤§è·¯ ä¸‹æµ·ç»å•† ï¼Œ æè¾¾åº· è¿è¿ èµ”ç¤¼é“æ­‰ ï¼Œ è§‰å¾— å¯¹ä¸èµ· å¤§å®¶ ï¼Œ ä»– æœ€ å¯¹ä¸èµ· çš„ æ˜¯ ç‹ å¤§è·¯ ï¼Œ å°± å’Œ æ˜“ å­¦ä¹  ä¸€èµ· ç»™ ç‹ å¤§è·¯ å‡‘ äº† 5 ä¸‡å— é’± ï¼Œ ç‹ å¤§è·¯ è‡ªå·± ä¸œæŒªè¥¿æ’® äº† 5 ä¸‡å— ï¼Œ å¼€å§‹ ä¸‹æµ·ç»å•† ã€‚ æ²¡æƒ³åˆ° åæ¥ ç‹ å¤§è·¯ ç«Ÿç„¶ åš å¾— é£ç”Ÿæ°´ èµ· ã€‚ æ²™ ç‘é‡‘ è§‰å¾— ä»–ä»¬ ä¸‰äºº ï¼Œ åœ¨ å›°éš¾ æ—¶æœŸ è¿˜ èƒ½ ä»¥æ²« ç›¸åŠ© ï¼Œ å¾ˆ ä¸ å®¹æ˜“ ã€‚ \\n \\n æ²™ ç‘é‡‘ å‘ æ¯›å¨… æ‰“å¬ ä»–ä»¬ å®¶ åœ¨ äº¬å· çš„ åˆ«å¢… ï¼Œ æ¯›å¨… ç¬‘ ç€ è¯´ ï¼Œ ç‹ å¤§è·¯ äº‹ä¸šæœ‰æˆ ä¹‹å ï¼Œ è¦ ç»™ æ¬§é˜³ è å’Œ å¥¹ å…¬å¸ çš„ è‚¡æƒ ï¼Œ å¥¹ä»¬ æ²¡æœ‰ è¦ ï¼Œ ç‹ å¤§è·¯ å°± åœ¨ äº¬å·å¸ è±ªå›­ ä¹° äº† ä¸‰å¥— åˆ«å¢… ï¼Œ å¯æ˜¯ æè¾¾ åº·å’Œæ˜“ å­¦ä¹  éƒ½ ä¸è¦ ï¼Œ è¿™äº› æˆ¿å­ éƒ½ åœ¨ ç‹ å¤§è·¯ çš„ åä¸‹ ï¼Œ æ¬§é˜³ è å¥½åƒ å» ä½ è¿‡ ï¼Œ æ¯›å¨… ä¸æƒ³ å» ï¼Œ å¥¹ è§‰å¾— æˆ¿å­ å¤ªå¤§ å¾ˆ æµªè´¹ ï¼Œ è‡ªå·± å®¶ä½ å¾— å°± å¾ˆ è¸å® ã€‚' 12345with open(&quot;./nlp_test2.txt&quot;, &quot;w&quot;) as f2: f2.write(result) å¯ä»¥å‘ç°å¯¹äºä¸€äº›äººåå’Œåœ°åï¼Œjiebaå¤„ç†çš„ä¸å¥½ï¼Œä¸è¿‡æˆ‘ä»¬å¯ä»¥å¸®jiebaåŠ å…¥è¯æ±‡å¦‚ä¸‹ï¼š 123456789jieba.suggest_freq('æ²™ç‘é‡‘', True)jieba.suggest_freq('æ˜“å­¦ä¹ ', True)jieba.suggest_freq('ç‹å¤§è·¯', True)jieba.suggest_freq('äº¬å·', True) 3 æ‰€ä»¥åœ¨å¾ˆå¤š NLP ä»»åŠ¡ä¸­å…ˆåšå‘½ä»¤å®ä½“è¯†åˆ«çš„æ„ä¹‰å°±åœ¨è¿™é‡Œå¯¹å§? 123456789101112131415with open(&quot;./nlp_test1.txt&quot;, &quot;r&quot;) as f1: text = f1.read() text_cut = jieba.cut(text) # list result = &quot; &quot;.join(text_cut) print(result) with open(&quot;./nlp_test2.txt&quot;, &quot;w&quot;) as f2: f2.write(result) æ²™ç‘é‡‘ èµå¹ æ˜“å­¦ä¹  çš„ èƒ¸æ€€ ï¼Œ æ˜¯ é‡‘å±± çš„ ç™¾å§“ æœ‰ç¦ ï¼Œ å¯æ˜¯ è¿™ä»¶ äº‹å¯¹ æè¾¾åº· çš„ è§¦åŠ¨ å¾ˆå¤§ ã€‚ æ˜“å­¦ä¹  åˆ å›å¿†èµ· ä»–ä»¬ ä¸‰äºº åˆ†å¼€ çš„ å‰ä¸€æ™š ï¼Œ å¤§å®¶ ä¸€èµ· å–é…’ è¯åˆ« ï¼Œ æ˜“å­¦ä¹  è¢« é™èŒ åˆ° é“å£ å¿å½“ å¿é•¿ ï¼Œ ç‹å¤§è·¯ ä¸‹æµ·ç»å•† ï¼Œ æè¾¾åº· è¿è¿ èµ”ç¤¼é“æ­‰ ï¼Œ è§‰å¾— å¯¹ä¸èµ· å¤§å®¶ ï¼Œ ä»– æœ€ å¯¹ä¸èµ· çš„ æ˜¯ ç‹å¤§è·¯ ï¼Œ å°± å’Œ æ˜“å­¦ä¹  ä¸€èµ· ç»™ ç‹å¤§è·¯ å‡‘ äº† 5 ä¸‡å— é’± ï¼Œ ç‹å¤§è·¯ è‡ªå·± ä¸œæŒªè¥¿æ’® äº† 5 ä¸‡å— ï¼Œ å¼€å§‹ ä¸‹æµ·ç»å•† ã€‚ æ²¡æƒ³åˆ° åæ¥ ç‹å¤§è·¯ ç«Ÿç„¶ åš å¾— é£ç”Ÿæ°´ èµ· ã€‚ æ²™ç‘é‡‘ è§‰å¾— ä»–ä»¬ ä¸‰äºº ï¼Œ åœ¨ å›°éš¾ æ—¶æœŸ è¿˜ èƒ½ ä»¥æ²« ç›¸åŠ© ï¼Œ å¾ˆ ä¸ å®¹æ˜“ ã€‚ æ²™ç‘é‡‘ å‘ æ¯›å¨… æ‰“å¬ ä»–ä»¬ å®¶ åœ¨ äº¬å· çš„ åˆ«å¢… ï¼Œ æ¯›å¨… ç¬‘ ç€ è¯´ ï¼Œ ç‹å¤§è·¯ äº‹ä¸šæœ‰æˆ ä¹‹å ï¼Œ è¦ ç»™ æ¬§é˜³ è å’Œ å¥¹ å…¬å¸ çš„ è‚¡æƒ ï¼Œ å¥¹ä»¬ æ²¡æœ‰ è¦ ï¼Œ ç‹å¤§è·¯ å°± åœ¨ äº¬å· å¸è±ªå›­ ä¹° äº† ä¸‰å¥— åˆ«å¢… ï¼Œ å¯æ˜¯ æè¾¾åº· å’Œ æ˜“å­¦ä¹  éƒ½ ä¸è¦ ï¼Œ è¿™äº› æˆ¿å­ éƒ½ åœ¨ ç‹å¤§è·¯ çš„ åä¸‹ ï¼Œ æ¬§é˜³ è å¥½åƒ å» ä½ è¿‡ ï¼Œ æ¯›å¨… ä¸æƒ³ å» ï¼Œ å¥¹ è§‰å¾— æˆ¿å­ å¤ªå¤§ å¾ˆ æµªè´¹ ï¼Œ è‡ªå·± å®¶ä½ å¾— å°± å¾ˆ è¸å® ã€‚ å¼•å…¥åœç”¨è¯åœ¨ä¸Šé¢æˆ‘ä»¬è§£æçš„æ–‡æœ¬ä¸­æœ‰å¾ˆå¤šæ— æ•ˆçš„è¯ï¼Œæ¯”å¦‚â€œç€â€ï¼Œâ€œå’Œâ€ï¼Œè¿˜æœ‰ä¸€äº›æ ‡ç‚¹ç¬¦å·ï¼Œè¿™äº›æˆ‘ä»¬ä¸æƒ³åœ¨æ–‡æœ¬åˆ†æçš„æ—¶å€™å¼•å…¥ï¼Œå› æ­¤éœ€è¦å»æ‰ï¼Œè¿™äº›è¯å°±æ˜¯åœç”¨è¯ã€‚å¸¸ç”¨çš„ä¸­æ–‡åœç”¨è¯è¡¨æ˜¯1208ä¸ªï¼Œä¸‹è½½åœ°å€åœ¨è¿™ã€‚å½“ç„¶ä¹Ÿæœ‰å…¶ä»–ç‰ˆæœ¬çš„åœç”¨è¯è¡¨ï¼Œä¸è¿‡è¿™ä¸ª1208è¯ç‰ˆæ˜¯æˆ‘å¸¸ç”¨çš„ã€‚ åœ¨æˆ‘ä»¬ç”¨scikit-learnåšç‰¹å¾å¤„ç†çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡å‚æ•°stop_wordsæ¥å¼•å…¥ä¸€ä¸ªæ•°ç»„ä½œä¸ºåœç”¨è¯è¡¨ã€‚ 123456789stpword_path = &quot;stop_words.txt&quot;with open(stpword_path, encoding=&quot;gbk&quot;) as f: stpword_content = f.read() stpword_list = stpword_content.splitlines() 123print(stpword_list[:100]) [',', '?', 'ã€', 'ã€‚', 'â€œ', 'â€', 'ã€Š', 'ã€‹', 'ï¼', 'ï¼Œ', 'ï¼š', 'ï¼›', 'ï¼Ÿ', 'äººæ°‘', 'æœ«##æœ«', 'å•Š', 'é˜¿', 'å“', 'å“å‘€', 'å“å“Ÿ', 'å”‰', 'ä¿º', 'ä¿ºä»¬', 'æŒ‰', 'æŒ‰ç…§', 'å§', 'å§å“’', 'æŠŠ', 'ç½¢äº†', 'è¢«', 'æœ¬', 'æœ¬ç€', 'æ¯”', 'æ¯”æ–¹', 'æ¯”å¦‚', 'é„™äºº', 'å½¼', 'å½¼æ­¤', 'è¾¹', 'åˆ«', 'åˆ«çš„', 'åˆ«è¯´', 'å¹¶', 'å¹¶ä¸”', 'ä¸æ¯”', 'ä¸æˆ', 'ä¸å•', 'ä¸ä½†', 'ä¸ç‹¬', 'ä¸ç®¡', 'ä¸å…‰', 'ä¸è¿‡', 'ä¸ä»…', 'ä¸æ‹˜', 'ä¸è®º', 'ä¸æ€•', 'ä¸ç„¶', 'ä¸å¦‚', 'ä¸ç‰¹', 'ä¸æƒŸ', 'ä¸é—®', 'ä¸åª', 'æœ', 'æœç€', 'è¶', 'è¶ç€', 'ä¹˜', 'å†²', 'é™¤', 'é™¤æ­¤ä¹‹å¤–', 'é™¤é', 'é™¤äº†', 'æ­¤', 'æ­¤é—´', 'æ­¤å¤–', 'ä»', 'ä»è€Œ', 'æ‰“', 'å¾…', 'ä½†', 'ä½†æ˜¯', 'å½“', 'å½“ç€', 'åˆ°', 'å¾—', 'çš„', 'çš„è¯', 'ç­‰', 'ç­‰ç­‰', 'åœ°', 'ç¬¬', 'å®å’š', 'å¯¹', 'å¯¹äº', 'å¤š', 'å¤šå°‘', 'è€Œ', 'è€Œå†µ', 'è€Œä¸”', 'è€Œæ˜¯'] ç‰¹å¾å¤„ç†ç°åœ¨æˆ‘ä»¬å°±å¯ä»¥ç”¨scikit-learnæ¥å¯¹æˆ‘ä»¬çš„æ–‡æœ¬ç‰¹å¾è¿›è¡Œå¤„ç†äº†ï¼Œåœ¨æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¹‹å‘é‡åŒ–ä¸Hash Trickä¸­ï¼Œæˆ‘ä»¬è®²åˆ°äº†ä¸¤ç§ç‰¹å¾å¤„ç†çš„æ–¹æ³•ï¼Œå‘é‡åŒ–ä¸Hash Trickã€‚è€Œå‘é‡åŒ–æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯ä»¥æ¥ç€è¿›è¡ŒTF-IDFçš„ç‰¹å¾å¤„ç†ã€‚åœ¨æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¹‹TF-IDFä¸­ï¼Œæˆ‘ä»¬ä¹Ÿè®²åˆ°äº†TF-IDFç‰¹å¾å¤„ç†çš„æ–¹æ³•ã€‚è¿™é‡Œæˆ‘ä»¬å°±ç”¨scikit-learnçš„TfidfVectorizerç±»æ¥è¿›è¡ŒTF-IDFç‰¹å¾å¤„ç†ã€‚ å‘é‡åŒ–ä¸ Hash Trickè¯è¢‹æ¨¡å‹åœ¨è®²å‘é‡åŒ–ä¸Hash Trickä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè¯´è¯´è¯è¢‹æ¨¡å‹(Bag of Words,ç®€ç§°BoW)ã€‚è¯è¢‹æ¨¡å‹å‡è®¾æˆ‘ä»¬ä¸è€ƒè™‘æ–‡æœ¬ä¸­è¯ä¸è¯ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼Œä»…ä»…åªè€ƒè™‘æ‰€æœ‰è¯çš„æƒé‡ã€‚è€Œæƒé‡ä¸è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„é¢‘ç‡æœ‰å…³ã€‚ è¯è¢‹æ¨¡å‹é¦–å…ˆä¼šè¿›è¡Œåˆ†è¯ï¼Œåœ¨åˆ†è¯ä¹‹åï¼Œé€šè¿‡ç»Ÿè®¡æ¯ä¸ªè¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°è¯¥æ–‡æœ¬åŸºäºè¯çš„ç‰¹å¾ï¼Œå¦‚æœå°†å„ä¸ªæ–‡æœ¬æ ·æœ¬çš„è¿™äº›è¯ä¸å¯¹åº”çš„è¯é¢‘æ”¾åœ¨ä¸€èµ·ï¼Œå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„å‘é‡åŒ–ã€‚å‘é‡åŒ–å®Œæ¯•åä¸€èˆ¬ä¹Ÿä¼šä½¿ç”¨TF-IDFè¿›è¡Œç‰¹å¾çš„æƒé‡ä¿®æ­£ï¼Œå†å°†ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ã€‚ å†è¿›è¡Œä¸€äº›å…¶ä»–çš„ç‰¹å¾å·¥ç¨‹åï¼Œå°±å¯ä»¥å°†æ•°æ®å¸¦å…¥æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œåˆ†ç±»èšç±»äº†ã€‚ æ€»ç»“ä¸‹è¯è¢‹æ¨¡å‹çš„ä¸‰éƒ¨æ›²ï¼šåˆ†è¯ï¼ˆtokenizingï¼‰ï¼Œç»Ÿè®¡ä¿®è®¢è¯ç‰¹å¾å€¼ï¼ˆcountingï¼‰ä¸æ ‡å‡†åŒ–ï¼ˆnormalizingï¼‰ã€‚ è¯è¢‹æ¨¡å‹æœ‰å¾ˆå¤§çš„å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»…ä»…è€ƒè™‘äº†è¯é¢‘ï¼Œæ²¡æœ‰è€ƒè™‘ä¸Šä¸‹æ–‡çš„å…³ç³»ï¼Œå› æ­¤ä¼šä¸¢å¤±ä¸€éƒ¨åˆ†æ–‡æœ¬çš„è¯­ä¹‰ã€‚ä½†æ˜¯å¤§å¤šæ•°æ—¶å€™ï¼Œå¦‚æœæˆ‘ä»¬çš„ç›®çš„æ˜¯åˆ†ç±»èšç±»ï¼Œåˆ™è¯è¢‹æ¨¡å‹è¡¨ç°çš„å¾ˆå¥½ã€‚ è¯è¢‹æ¨¡å‹ä¹‹å‘é‡åŒ–åœ¨è¯è¢‹æ¨¡å‹çš„ç»Ÿè®¡è¯é¢‘è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°è¯¥æ–‡æœ¬ä¸­æ‰€æœ‰è¯çš„è¯é¢‘ï¼Œæœ‰äº†è¯é¢‘ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¯å‘é‡è¡¨ç¤ºè¿™ä¸ªæ–‡æœ¬ã€‚è¿™é‡Œæˆ‘ä»¬ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œä¾‹å­ç›´æ¥ç”¨scikit-learnçš„CountVectorizerç±»æ¥å®Œæˆï¼Œè¿™ä¸ªç±»å¯ä»¥å¸®æˆ‘ä»¬å®Œæˆæ–‡æœ¬çš„è¯é¢‘ç»Ÿè®¡ä¸å‘é‡åŒ–ï¼Œä»£ç å¦‚ä¸‹ï¼š 12345from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer() 1234567891011corpus=[&quot;I come to China to travel&quot;, &quot;This is a car polupar in China&quot;, &quot;I love tea and Apple &quot;, &quot;The work is to write some papers in science&quot;]print(vectorizer.fit_transform(corpus)) (0, 16) 1 (0, 3) 1 (0, 15) 2 (0, 4) 1 (1, 5) 1 (1, 9) 1 (1, 2) 1 (1, 6) 1 (1, 14) 1 (1, 3) 1 (2, 1) 1 (2, 0) 1 (2, 12) 1 (2, 7) 1 (3, 10) 1 (3, 8) 1 (3, 11) 1 (3, 18) 1 (3, 17) 1 (3, 13) 1 (3, 5) 1 (3, 6) 1 (3, 15) 1 å¯ä»¥çœ‹å‡º4ä¸ªæ–‡æœ¬çš„è¯é¢‘å·²ç»ç»Ÿè®¡å‡ºï¼Œåœ¨è¾“å‡ºä¸­ï¼Œå·¦è¾¹çš„æ‹¬å·ä¸­çš„ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯æ–‡æœ¬çš„åºå·ï¼Œç¬¬2ä¸ªæ•°å­—æ˜¯è¯çš„åºå·ï¼Œæ³¨æ„è¯çš„åºå·æ˜¯åŸºäºæ‰€æœ‰çš„æ–‡æ¡£çš„ã€‚ç¬¬ä¸‰ä¸ªæ•°å­—å°±æ˜¯æˆ‘ä»¬çš„è¯é¢‘ã€‚ æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥çœ‹çœ‹æ¯ä¸ªæ–‡æœ¬çš„è¯å‘é‡ç‰¹å¾å’Œå„ä¸ªç‰¹å¾ä»£è¡¨çš„è¯ï¼Œä»£ç å¦‚ä¸‹ï¼š 123print(vectorizer.fit_transform(corpus).toarray()) [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] 123print(vectorizer.get_feature_names()) ['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write'] ä¹Ÿå°±æ˜¯å…ˆç»Ÿè®¡æ•´ä¸ªæ–‡æœ¬corpus, å»æ‰åœç”¨è¯ï¼Œå‰©ä¸‹çš„è¯å°±æ˜¯å‘é‡çš„ç»´åº¦ã€‚ç„¶åç»Ÿè®¡æ¯ä¸€è¡Œæ–‡å­—å‡ºç°çš„è¯é¢‘ï¼Œå¾—åˆ°ç›¸åº”çš„å‘é‡ã€‚æ˜¾ç„¶è¯è¡¨æ˜¯æŒ‰ç…§å­—æ¯é¡ºåºæ’åºçš„ã€‚ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬ä¸€å…±æœ‰19ä¸ªè¯ï¼Œæ‰€ä»¥4ä¸ªæ–‡æœ¬éƒ½æ˜¯19ç»´çš„ç‰¹å¾å‘é‡ã€‚è€Œæ¯ä¸€ç»´çš„å‘é‡ä¾æ¬¡å¯¹åº”äº†ä¸‹é¢çš„19ä¸ªè¯ã€‚å¦å¤–ç”±äºè¯â€Iâ€åœ¨è‹±æ–‡ä¸­æ˜¯åœç”¨è¯ï¼Œä¸å‚åŠ è¯é¢‘çš„ç»Ÿè®¡ã€‚ ç”±äºå¤§éƒ¨åˆ†çš„æ–‡æœ¬éƒ½åªä¼šä½¿ç”¨è¯æ±‡è¡¨ä¸­çš„å¾ˆå°‘ä¸€éƒ¨åˆ†çš„è¯ï¼Œå› æ­¤æˆ‘ä»¬çš„è¯å‘é‡ä¸­ä¼šæœ‰å¤§é‡çš„0ã€‚ä¹Ÿå°±æ˜¯è¯´è¯å‘é‡æ˜¯ç¨€ç–çš„ã€‚åœ¨å®é™…åº”ç”¨ä¸­ä¸€èˆ¬ä½¿ç”¨ç¨€ç–çŸ©é˜µæ¥å­˜å‚¨ã€‚ è¿™é‡Œæœ‰ä¸ªç–‘é—®ï¼Ÿ å‘é‡åŒ–ä¹‹åçš„ç»´åº¦æ˜¯æ ¹æ®è‡ªå·±çš„æ•°æ®é›†æ¥å®šï¼Œä¸ºä»€ä¹ˆä¸å°±æ˜¯è¯è¡¨å¤§å°å‘¢ã€‚è¿™é‡Œæ˜¯æ ¹æ®è‡ªå·±çš„æ•°æ®é›†æ¥çš„ï¼Œä½†æˆ‘ä»¬å¯¹æµ‹è¯•é›†åˆ†ç±»æ—¶ï¼Œä¼šå‡ºç° UNK è¯å§ï¼Œä½†æ˜¯è¿™ä¸ªè¯å…¶å®åœ¨è¯è¡¨ä¸­æ˜¯æœ‰çš„ã€‚é‚£ä¹ˆåœ¨è®­ç»ƒé›†ä¸­å¦‚æœåŠ ä¸Šè¿™ä¸ªç»´åº¦ï¼Œå…¶å®ä¹Ÿæ²¡æœ‰å¤ªå¤§æ„ä¹‰ï¼Œå› ä¸ºåœ¨è®­ç»ƒé›†ä¸­è¿™ä¸ªç»´åº¦ä¸Šæ‰€æœ‰çš„å€¼éƒ½ä¸º0. å°†æ–‡æœ¬åšäº†è¯é¢‘ç»Ÿè®¡åï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šé€šè¿‡TF-IDFè¿›è¡Œè¯ç‰¹å¾å€¼ä¿®è®¢ï¼Œè¿™éƒ¨åˆ†æˆ‘ä»¬åé¢å†è®²ã€‚ å‘é‡åŒ–çš„æ–¹æ³•å¾ˆå¥½ç”¨ï¼Œä¹Ÿå¾ˆç›´æ¥ï¼Œä½†æ˜¯åœ¨æœ‰äº›åœºæ™¯ä¸‹å¾ˆéš¾ä½¿ç”¨ï¼Œæ¯”å¦‚åˆ†è¯åçš„è¯æ±‡è¡¨éå¸¸å¤§ï¼Œè¾¾åˆ°100ä¸‡+ï¼Œæ­¤æ—¶å¦‚æœæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å‘é‡åŒ–çš„æ–¹æ³•ï¼Œå°†å¯¹åº”çš„æ ·æœ¬å¯¹åº”ç‰¹å¾çŸ©é˜µè½½å…¥å†…å­˜ï¼Œæœ‰å¯èƒ½å°†å†…å­˜æ’‘çˆ†ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æ€ä¹ˆåŠå‘¢ï¼Ÿç¬¬ä¸€ååº”æ˜¯æˆ‘ä»¬è¦è¿›è¡Œç‰¹å¾çš„é™ç»´ï¼Œè¯´çš„æ²¡é”™ï¼è€ŒHash Trickå°±æ˜¯éå¸¸å¸¸ç”¨çš„æ–‡æœ¬ç‰¹å¾é™ç»´æ–¹æ³•ã€‚ Hash Trickåœ¨å¤§è§„æ¨¡çš„æ–‡æœ¬å¤„ç†ä¸­ï¼Œç”±äºç‰¹å¾çš„ç»´åº¦å¯¹åº”åˆ†è¯è¯æ±‡è¡¨çš„å¤§å°ï¼Œæ‰€ä»¥ç»´åº¦å¯èƒ½éå¸¸ææ€–ï¼Œæ­¤æ—¶éœ€è¦è¿›è¡Œé™ç»´ï¼Œä¸èƒ½ç›´æ¥ç”¨æˆ‘ä»¬ä¸Šä¸€èŠ‚çš„å‘é‡åŒ–æ–¹æ³•ã€‚è€Œæœ€å¸¸ç”¨çš„æ–‡æœ¬é™ç»´æ–¹æ³•æ˜¯Hash Trickã€‚è¯´åˆ°Hashï¼Œä¸€ç‚¹ä¹Ÿä¸ç¥ç§˜ï¼Œå­¦è¿‡æ•°æ®ç»“æ„çš„åŒå­¦éƒ½çŸ¥é“ã€‚è¿™é‡Œçš„Hashæ„ä¹‰ä¹Ÿç±»ä¼¼ã€‚ åœ¨Hash Trické‡Œï¼Œæˆ‘ä»¬ä¼šå®šä¹‰ä¸€ä¸ªç‰¹å¾Hashåå¯¹åº”çš„å“ˆå¸Œè¡¨çš„å¤§å°ï¼Œè¿™ä¸ªå“ˆå¸Œè¡¨çš„ç»´åº¦ä¼šè¿œè¿œå°äºæˆ‘ä»¬çš„è¯æ±‡è¡¨çš„ç‰¹å¾ç»´åº¦ï¼Œå› æ­¤å¯ä»¥çœ‹æˆæ˜¯é™ç»´ã€‚å…·ä½“çš„æ–¹æ³•æ˜¯ï¼Œå¯¹åº”ä»»æ„ä¸€ä¸ªç‰¹å¾åï¼Œæˆ‘ä»¬ä¼šç”¨Hashå‡½æ•°æ‰¾åˆ°å¯¹åº”å“ˆå¸Œè¡¨çš„ä½ç½®ï¼Œç„¶åå°†è¯¥ç‰¹å¾åå¯¹åº”çš„è¯é¢‘ç»Ÿè®¡å€¼ç´¯åŠ åˆ°è¯¥å“ˆå¸Œè¡¨ä½ç½®ã€‚å¦‚æœç”¨æ•°å­¦è¯­è¨€è¡¨ç¤º,å‡å¦‚å“ˆå¸Œå‡½æ•°hä½¿ç¬¬iä¸ªç‰¹å¾å“ˆå¸Œåˆ°ä½ç½®j,å³ $h(i)=j$,åˆ™ç¬¬iä¸ªåŸå§‹ç‰¹å¾çš„è¯é¢‘æ•°å€¼ $\\phi(i)$ å°†ç´¯åŠ åˆ°å“ˆå¸Œåçš„ç¬¬jä¸ªç‰¹å¾çš„è¯é¢‘æ•°å€¼ $\\hat \\phi(i)$ä¸Šï¼Œå³ï¼š $$\\hat \\phi(i)=\\sum_{i\\in J;h(i)=j}\\phi(i)$$ å…¶ä¸­ J æ˜¯åŸå§‹ç‰¹å¾çš„ç»´åº¦ã€‚ ä½†æ˜¯ä¸Šé¢çš„æ–¹æ³•æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œæœ‰å¯èƒ½ä¸¤ä¸ªåŸå§‹ç‰¹å¾çš„å“ˆå¸Œåä½ç½®åœ¨ä¸€èµ·å¯¼è‡´è¯é¢‘ç´¯åŠ ç‰¹å¾å€¼çªç„¶å˜å¤§ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå‡ºç°äº†hash Trickçš„å˜ç§signed hash trick,æ­¤æ—¶é™¤äº†å“ˆå¸Œå‡½æ•°h,æˆ‘ä»¬å¤šäº†ä¸€ä¸ªä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼š $$\\xi:N\\rightarrow \\pm1$$ æ­¤æ—¶æˆ‘ä»¬æœ‰ $$\\hat \\phi(j)=\\sum_{i\\in J;h(i)=j}\\phi(i)\\xi(i)$$ è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œå“ˆå¸Œåçš„ç‰¹å¾ä»ç„¶æ˜¯ä¸€ä¸ªæ— åçš„ä¼°è®¡ï¼Œä¸ä¼šå¯¼è‡´æŸäº›å“ˆå¸Œä½ç½®çš„å€¼è¿‡å¤§ã€‚ å½“ç„¶ï¼Œå¤§å®¶ä¼šæœ‰ç–‘æƒ‘ï¼Œè¿™ç§æ–¹æ³•æ¥å¤„ç†ç‰¹å¾ï¼Œå“ˆå¸Œåçš„ç‰¹å¾æ˜¯å¦èƒ½å¤Ÿå¾ˆå¥½çš„ä»£è¡¨å“ˆå¸Œå‰çš„ç‰¹å¾å‘¢ï¼Ÿä»å®é™…åº”ç”¨ä¸­è¯´ï¼Œç”±äºæ–‡æœ¬ç‰¹å¾çš„é«˜ç¨€ç–æ€§ï¼Œè¿™ä¹ˆåšæ˜¯å¯è¡Œçš„ã€‚å¦‚æœå¤§å®¶å¯¹ç†è®ºä¸Šä¸ºä½•è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œå»ºè®®å‚è€ƒè®ºæ–‡ï¼šFeature hashing for large scale multitask learning.è¿™é‡Œå°±ä¸å¤šè¯´äº†ã€‚ åœ¨scikit-learnçš„HashingVectorizerç±»ä¸­ï¼Œå®ç°äº†åŸºäºsigned hash trickçš„ç®—æ³•ï¼Œè¿™é‡Œæˆ‘ä»¬å°±ç”¨HashingVectorizeræ¥å®è·µä¸€ä¸‹Hash Trickï¼Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸Šé¢çš„19ç»´è¯æ±‡è¡¨ï¼Œå¹¶å“ˆå¸Œé™ç»´åˆ°6ç»´ã€‚å½“ç„¶åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ19ç»´çš„æ•°æ®æ ¹æœ¬ä¸éœ€è¦Hash Trickï¼Œè¿™é‡Œåªæ˜¯åšä¸€ä¸ªæ¼”ç¤ºï¼Œä»£ç å¦‚ä¸‹ï¼š 1234567from sklearn.feature_extraction.text import HashingVectorizervectorizer2 = HashingVectorizer(n_features=6, norm=None)print(vectorizer2.fit_transform(corpus)) (0, 1) 2.0 (0, 2) -1.0 (0, 4) 1.0 (0, 5) -1.0 (1, 0) 1.0 (1, 1) 1.0 (1, 2) -1.0 (1, 5) -1.0 (2, 0) 2.0 (2, 5) -2.0 (3, 0) 0.0 (3, 1) 4.0 (3, 2) -1.0 (3, 3) 1.0 (3, 5) -1.0 å¤§å®¶å¯ä»¥çœ‹åˆ°ç»“æœé‡Œé¢æœ‰è´Ÿæ•°ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬çš„å“ˆå¸Œå‡½æ•°Î¾å¯ä»¥å“ˆå¸Œåˆ°1æˆ–è€…-1å¯¼è‡´çš„ã€‚ å’ŒPCAç±»ä¼¼ï¼ŒHash Trické™ç»´åçš„ç‰¹å¾æˆ‘ä»¬å·²ç»ä¸çŸ¥é“å®ƒä»£è¡¨çš„ç‰¹å¾åå­—å’Œæ„ä¹‰ã€‚æ­¤æ—¶æˆ‘ä»¬ä¸èƒ½åƒä¸Šä¸€èŠ‚å‘é‡åŒ–æ—¶å€™å¯ä»¥çŸ¥é“æ¯ä¸€åˆ—çš„æ„ä¹‰ï¼Œæ‰€ä»¥Hash Trickçš„è§£é‡Šæ€§ä¸å¼ºã€‚ å‘é‡åŒ–ä¸ Hash Track å°ç»“è¿™é‡Œæˆ‘ä»¬å¯¹å‘é‡åŒ–ä¸å®ƒçš„ç‰¹ä¾‹Hash Trickåšä¸€ä¸ªæ€»ç»“ã€‚åœ¨ç‰¹å¾é¢„å¤„ç†çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä»€ä¹ˆæ—¶å€™ç”¨ä¸€èˆ¬æ„ä¹‰çš„å‘é‡åŒ–ï¼Œä»€ä¹ˆæ—¶å€™ç”¨Hash Trickå‘¢ï¼Ÿæ ‡å‡†ä¹Ÿå¾ˆç®€å•ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œåªè¦è¯æ±‡è¡¨çš„ç‰¹å¾ä¸è‡³äºå¤ªå¤§ï¼Œå¤§åˆ°å†…å­˜ä¸å¤Ÿç”¨ï¼Œè‚¯å®šæ˜¯ä½¿ç”¨ä¸€èˆ¬æ„ä¹‰çš„å‘é‡åŒ–æ¯”è¾ƒå¥½ã€‚å› ä¸ºå‘é‡åŒ–çš„æ–¹æ³•è§£é‡Šæ€§å¾ˆå¼ºï¼Œæˆ‘ä»¬çŸ¥é“æ¯ä¸€ç»´ç‰¹å¾å¯¹åº”å“ªä¸€ä¸ªè¯ï¼Œè¿›è€Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨TF-IDFå¯¹å„ä¸ªè¯ç‰¹å¾çš„æƒé‡ä¿®æ”¹ï¼Œè¿›ä¸€æ­¥å®Œå–„ç‰¹å¾çš„è¡¨ç¤ºã€‚ è€ŒHash Trickç”¨å¤§è§„æ¨¡æœºå™¨å­¦ä¹ ä¸Šï¼Œæ­¤æ—¶æˆ‘ä»¬çš„è¯æ±‡é‡æå¤§ï¼Œä½¿ç”¨å‘é‡åŒ–æ–¹æ³•å†…å­˜ä¸å¤Ÿç”¨ï¼Œè€Œä½¿ç”¨Hash Trické™ç»´é€Ÿåº¦å¾ˆå¿«ï¼Œé™ç»´åçš„ç‰¹å¾ä»ç„¶å¯ä»¥å¸®æˆ‘ä»¬å®Œæˆåç»­çš„åˆ†ç±»å’Œèšç±»å·¥ä½œã€‚å½“ç„¶ç”±äºåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶çš„å­˜åœ¨ï¼Œå…¶å®ä¸€èˆ¬æˆ‘ä»¬ä¸ä¼šå‡ºç°å†…å­˜ä¸å¤Ÿçš„æƒ…å†µã€‚å› æ­¤ï¼Œå®é™…å·¥ä½œä¸­æˆ‘ä½¿ç”¨çš„éƒ½æ˜¯ç‰¹å¾å‘é‡åŒ–ã€‚ å‘é‡åŒ–ä¸Hash Trickå°±ä»‹ç»åˆ°è¿™é‡Œï¼Œä¸‹ä¸€ç¯‡æˆ‘ä»¬è®¨è®ºTF-IDFã€‚ æ–‡æœ¬å‘é‡åŒ–ç‰¹å¾çš„ä¸è¶³åœ¨å°†æ–‡æœ¬åˆ†è¯å¹¶å‘é‡åŒ–åï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°è¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯åœ¨å„ä¸ªæ–‡æœ¬ä¸­å½¢æˆçš„è¯å‘é‡ï¼Œæ¯”å¦‚åœ¨æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¹‹å‘é‡åŒ–ä¸Hash Trickè¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸‹é¢4ä¸ªçŸ­æ–‡æœ¬åšäº†è¯é¢‘ç»Ÿè®¡ï¼š 123456789corpus=[&quot;I come to China to travel&quot;, &quot;This is a car polupar in China&quot;, &quot;I love tea and Apple &quot;, &quot;The work is to write some papers in science&quot;] ä¸è€ƒè™‘åœç”¨è¯ï¼Œå¤„ç†åå¾—åˆ°çš„è¯å‘é‡å¦‚ä¸‹ï¼š 123456789[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] å¦‚æœæˆ‘ä»¬ç›´æ¥å°†ç»Ÿè®¡è¯é¢‘åçš„19ç»´ç‰¹å¾åšä¸ºæ–‡æœ¬åˆ†ç±»çš„è¾“å…¥ï¼Œä¼šå‘ç°æœ‰ä¸€äº›é—®é¢˜ã€‚æ¯”å¦‚ç¬¬ä¸€ä¸ªæ–‡æœ¬ï¼Œæˆ‘ä»¬å‘ç°â€comeâ€,â€Chinaâ€å’Œâ€œTravelâ€å„å‡ºç°1æ¬¡ï¼Œè€Œâ€œtoâ€œå‡ºç°äº†ä¸¤æ¬¡ã€‚ä¼¼ä¹çœ‹èµ·æ¥è¿™ä¸ªæ–‡æœ¬ä¸â€toâ€œè¿™ä¸ªç‰¹å¾æ›´å…³ç³»ç´§å¯†ã€‚ä½†æ˜¯å®é™…ä¸Šâ€toâ€œæ˜¯ä¸€ä¸ªéå¸¸æ™®éçš„è¯ï¼Œå‡ ä¹æ‰€æœ‰çš„æ–‡æœ¬éƒ½ä¼šç”¨åˆ°ï¼Œå› æ­¤è™½ç„¶å®ƒçš„è¯é¢‘ä¸º2ï¼Œä½†æ˜¯é‡è¦æ€§å´æ¯”è¯é¢‘ä¸º1çš„â€Chinaâ€å’Œâ€œTravelâ€è¦ä½çš„å¤šã€‚å¦‚æœæˆ‘ä»¬çš„å‘é‡åŒ–ç‰¹å¾ä»…ä»…ç”¨è¯é¢‘è¡¨ç¤ºå°±æ— æ³•ååº”è¿™ä¸€ç‚¹ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥çš„é¢„å¤„ç†æ¥ååº”æ–‡æœ¬çš„è¿™ä¸ªç‰¹å¾ï¼Œè€Œè¿™ä¸ªé¢„å¤„ç†å°±æ˜¯TF-IDFã€‚ TF-IDFæ¦‚è¿°TF-IDFæ˜¯Term Frequency - Inverse Document Frequencyçš„ç¼©å†™ï¼Œå³â€œè¯é¢‘-é€†æ–‡æœ¬é¢‘ç‡â€ã€‚å®ƒç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼ŒTFå’ŒIDFã€‚ å‰é¢çš„TFä¹Ÿå°±æ˜¯æˆ‘ä»¬å‰é¢è¯´åˆ°çš„è¯é¢‘ï¼Œæˆ‘ä»¬ä¹‹å‰åšçš„å‘é‡åŒ–ä¹Ÿå°±æ˜¯åšäº†æ–‡æœ¬ä¸­å„ä¸ªè¯çš„å‡ºç°é¢‘ç‡ç»Ÿè®¡ï¼Œå¹¶ä½œä¸ºæ–‡æœ¬ç‰¹å¾ï¼Œè¿™ä¸ªå¾ˆå¥½ç†è§£ã€‚å…³é”®æ˜¯åé¢çš„è¿™ä¸ªIDFï¼Œå³â€œé€†æ–‡æœ¬é¢‘ç‡â€å¦‚ä½•ç†è§£ã€‚åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®²åˆ°å‡ ä¹æ‰€æœ‰æ–‡æœ¬éƒ½ä¼šå‡ºç°çš„â€toâ€å…¶è¯é¢‘è™½ç„¶é«˜ï¼Œä½†æ˜¯é‡è¦æ€§å´åº”è¯¥æ¯”è¯é¢‘ä½çš„â€Chinaâ€å’Œâ€œTravelâ€è¦ä½ã€‚æˆ‘ä»¬çš„IDFå°±æ˜¯æ¥å¸®åŠ©æˆ‘ä»¬æ¥ååº”è¿™ä¸ªè¯çš„é‡è¦æ€§çš„ï¼Œè¿›è€Œä¿®æ­£ä»…ä»…ç”¨è¯é¢‘è¡¨ç¤ºçš„è¯ç‰¹å¾å€¼ã€‚ æ¦‚æ‹¬æ¥è®²ï¼Œ IDFååº”äº†ä¸€ä¸ªè¯åœ¨æ‰€æœ‰æ–‡æœ¬ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œå¦‚æœä¸€ä¸ªè¯åœ¨å¾ˆå¤šçš„æ–‡æœ¬ä¸­å‡ºç°ï¼Œé‚£ä¹ˆå®ƒçš„IDFå€¼åº”è¯¥ä½ï¼Œæ¯”å¦‚ä¸Šæ–‡ä¸­çš„â€œtoâ€ã€‚è€Œåè¿‡æ¥å¦‚æœä¸€ä¸ªè¯åœ¨æ¯”è¾ƒå°‘çš„æ–‡æœ¬ä¸­å‡ºç°ï¼Œé‚£ä¹ˆå®ƒçš„IDFå€¼åº”è¯¥é«˜ã€‚æ¯”å¦‚ä¸€äº›ä¸“ä¸šçš„åè¯å¦‚â€œMachine Learningâ€ã€‚è¿™æ ·çš„è¯IDFå€¼åº”è¯¥é«˜ã€‚ä¸€ä¸ªæç«¯çš„æƒ…å†µï¼Œå¦‚æœä¸€ä¸ªè¯åœ¨æ‰€æœ‰çš„æ–‡æœ¬ä¸­éƒ½å‡ºç°ï¼Œé‚£ä¹ˆå®ƒçš„IDFå€¼åº”è¯¥ä¸º0ã€‚ ä¸Šé¢æ˜¯ä»å®šæ€§ä¸Šè¯´æ˜çš„IDFçš„ä½œç”¨ï¼Œé‚£ä¹ˆå¦‚ä½•å¯¹ä¸€ä¸ªè¯çš„IDFè¿›è¡Œå®šé‡åˆ†æå‘¢ï¼Ÿè¿™é‡Œç›´æ¥ç»™å‡ºä¸€ä¸ªè¯xçš„IDFçš„åŸºæœ¬å…¬å¼å¦‚ä¸‹ï¼š $$IDF(x)=\\dfrac{N}{N(x)}$$ å…¶ä¸­ï¼ŒNä»£è¡¨è¯­æ–™åº“ä¸­æ–‡æœ¬çš„æ€»æ•°ï¼Œè€Œ $N(x)$ ä»£è¡¨è¯­æ–™åº“ä¸­åŒ…å«è¯xçš„æ–‡æœ¬æ€»æ•°ã€‚ä¸ºä»€ä¹ˆIDFçš„åŸºæœ¬å…¬å¼åº”è¯¥æ˜¯æ˜¯ä¸Šé¢è¿™æ ·çš„è€Œä¸æ˜¯åƒ $N/N(x)$ è¿™æ ·çš„å½¢å¼å‘¢ï¼Ÿè¿™å°±æ¶‰åŠåˆ°ä¿¡æ¯è®ºç›¸å…³çš„ä¸€äº›çŸ¥è¯†äº†ã€‚æ„Ÿå…´è¶£çš„æœ‹å‹å»ºè®®é˜…è¯»å´å†›åšå£«çš„ã€Šæ•°å­¦ä¹‹ç¾ã€‹ç¬¬11ç« ã€‚ ä¸Šé¢çš„IDFå…¬å¼å·²ç»å¯ä»¥ä½¿ç”¨äº†ï¼Œä½†æ˜¯åœ¨ä¸€äº›ç‰¹æ®Šçš„æƒ…å†µä¼šæœ‰ä¸€äº›å°é—®é¢˜ï¼Œæ¯”å¦‚æŸä¸€ä¸ªç”Ÿåƒ»è¯åœ¨è¯­æ–™åº“ä¸­æ²¡æœ‰ï¼Œè¿™æ ·æˆ‘ä»¬çš„åˆ†æ¯ä¸º0ï¼Œ IDFæ²¡æœ‰æ„ä¹‰äº†ã€‚æ‰€ä»¥å¸¸ç”¨çš„IDFæˆ‘ä»¬éœ€è¦åšä¸€äº›å¹³æ»‘ï¼Œä½¿è¯­æ–™åº“ä¸­æ²¡æœ‰å‡ºç°çš„è¯ä¹Ÿå¯ä»¥å¾—åˆ°ä¸€ä¸ªåˆé€‚çš„IDFå€¼ã€‚å¹³æ»‘çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼Œæœ€å¸¸è§çš„IDFå¹³æ»‘åçš„å…¬å¼ä¹‹ä¸€ä¸ºï¼š $$IDF(x)=log\\dfrac{N+1}{N(x)+1}+1$$ æœ‰äº†IDFçš„å®šä¹‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—æŸä¸€ä¸ªè¯çš„TF-IDFå€¼äº†ï¼š $$\\text{TF-IDF(x)}=TF(x)*IDF(x)$$ å…¶ä¸­TF(x)æŒ‡è¯xåœ¨å½“å‰æ–‡æœ¬ä¸­çš„è¯é¢‘ã€‚ ç”¨scikit-learnè¿›è¡ŒTF-IDFé¢„å¤„ç†åœ¨scikit-learnä¸­ï¼Œæœ‰ä¸¤ç§æ–¹æ³•è¿›è¡ŒTF-IDFçš„é¢„å¤„ç†ã€‚ ç¬¬ä¸€ç§æ–¹æ³•æ˜¯åœ¨ç”¨CountVectorizerç±»å‘é‡åŒ–ä¹‹åå†è°ƒç”¨TfidfTransformerç±»è¿›è¡Œé¢„å¤„ç†ã€‚ç¬¬äºŒç§æ–¹æ³•æ˜¯ç›´æ¥ç”¨TfidfVectorizerå®Œæˆå‘é‡åŒ–ä¸TF-IDFé¢„å¤„ç†ã€‚ é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ç¬¬ä¸€ç§æ–¹æ³•ï¼ŒCountVectorizer+TfidfTransformerçš„ç»„åˆï¼Œä»£ç å¦‚ä¸‹ï¼š 123456789101112131415161718192021222324252627from sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizercorpus = [&quot;I come to China to travel&quot;, &quot;This is a car polupar in China&quot;, &quot;I love tea and Apple &quot;, &quot;The work is to write some papers in science&quot;]vectorizer = CountVectorizer()transformer = TfidfTransformer()tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))print(tfidf) (0, 4) 0.4424621378947393 (0, 15) 0.697684463383976 (0, 3) 0.348842231691988 (0, 16) 0.4424621378947393 (1, 3) 0.3574550433419527 (1, 14) 0.45338639737285463 (1, 6) 0.3574550433419527 (1, 2) 0.45338639737285463 (1, 9) 0.45338639737285463 (1, 5) 0.3574550433419527 (2, 7) 0.5 (2, 12) 0.5 (2, 0) 0.5 (2, 1) 0.5 (3, 15) 0.2811316284405006 (3, 6) 0.2811316284405006 (3, 5) 0.2811316284405006 (3, 13) 0.3565798233381452 (3, 17) 0.3565798233381452 (3, 18) 0.3565798233381452 (3, 11) 0.3565798233381452 (3, 8) 0.3565798233381452 (3, 10) 0.3565798233381452 123456789from sklearn.feature_extraction.text import TfidfVectorizertfidf2 = TfidfVectorizer()re = tfidf2.fit_transform(corpus)print(re) (0, 4) 0.4424621378947393 (0, 15) 0.697684463383976 (0, 3) 0.348842231691988 (0, 16) 0.4424621378947393 (1, 3) 0.3574550433419527 (1, 14) 0.45338639737285463 (1, 6) 0.3574550433419527 (1, 2) 0.45338639737285463 (1, 9) 0.45338639737285463 (1, 5) 0.3574550433419527 (2, 7) 0.5 (2, 12) 0.5 (2, 0) 0.5 (2, 1) 0.5 (3, 15) 0.2811316284405006 (3, 6) 0.2811316284405006 (3, 5) 0.2811316284405006 (3, 13) 0.3565798233381452 (3, 17) 0.3565798233381452 (3, 18) 0.3565798233381452 (3, 11) 0.3565798233381452 (3, 8) 0.3565798233381452 (3, 10) 0.3565798233381452 è¾“å‡ºçš„å„ä¸ªæ–‡æœ¬å„ä¸ªè¯çš„TF-IDFå€¼å’Œç¬¬ä¸€ç§çš„è¾“å‡ºå®Œå…¨ç›¸åŒã€‚å¤§å®¶å¯ä»¥è‡ªå·±å»éªŒè¯ä¸€ä¸‹ã€‚ ç”±äºç¬¬äºŒç§æ–¹æ³•æ¯”è¾ƒçš„ç®€æ´ï¼Œå› æ­¤åœ¨å®é™…åº”ç”¨ä¸­æ¨èä½¿ç”¨ï¼Œä¸€æ­¥åˆ°ä½å®Œæˆå‘é‡åŒ–ï¼ŒTF-IDFä¸æ ‡å‡†åŒ–ã€‚ TF-IDFæ˜¯éå¸¸å¸¸ç”¨çš„æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†åŸºæœ¬æ­¥éª¤ï¼Œä½†æ˜¯å¦‚æœé¢„å¤„ç†ä¸­ä½¿ç”¨äº†Hash Trickï¼Œåˆ™ä¸€èˆ¬å°±æ— æ³•ä½¿ç”¨TF-IDFäº†ï¼Œå› ä¸ºHash Trickåæˆ‘ä»¬å·²ç»æ— æ³•å¾—åˆ°å“ˆå¸Œåçš„å„ç‰¹å¾çš„IDFçš„å€¼ã€‚ä½¿ç”¨äº†IF-IDFå¹¶æ ‡å‡†åŒ–ä»¥åï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨å„ä¸ªæ–‡æœ¬çš„è¯ç‰¹å¾å‘é‡ä½œä¸ºæ–‡æœ¬çš„ç‰¹å¾ï¼Œè¿›è¡Œåˆ†ç±»æˆ–è€…èšç±»åˆ†æã€‚ å½“ç„¶TF-IDFä¸å…‰å¯ä»¥ç”¨äºæ–‡æœ¬æŒ–æ˜ï¼Œåœ¨ä¿¡æ¯æ£€ç´¢ç­‰å¾ˆå¤šé¢†åŸŸéƒ½æœ‰ä½¿ç”¨ã€‚å› æ­¤å€¼å¾—å¥½å¥½çš„ç†è§£è¿™ä¸ªæ–¹æ³•çš„æ€æƒ³ è¿˜çš„å¥½å¥½ç†è§£ä¸‹ TF-IDF æ˜¯æ€ä¹ˆå®ç°çš„ï¼ å»ºç«‹åˆ†ææ¨¡å‹æœ‰äº†æ¯æ®µæ–‡æœ¬çš„TF-IDFçš„ç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨è¿™äº›æ•°æ®å»ºç«‹åˆ†ç±»æ¨¡å‹ï¼Œæˆ–è€…èšç±»æ¨¡å‹äº†ï¼Œæˆ–è€…è¿›è¡Œä¸»é¢˜æ¨¡å‹çš„åˆ†æã€‚æ¯”å¦‚æˆ‘ä»¬ä¸Šé¢çš„ä¸¤æ®µæ–‡æœ¬ï¼Œå°±å¯ä»¥æ˜¯ä¸¤ä¸ªè®­ç»ƒæ ·æœ¬äº†ã€‚æ­¤æ—¶çš„åˆ†ç±»èšç±»æ¨¡å‹å’Œä¹‹å‰è®²çš„éè‡ªç„¶è¯­è¨€å¤„ç†çš„æ•°æ®åˆ†ææ²¡æœ‰ä»€ä¹ˆä¸¤æ ·ã€‚å› æ­¤å¯¹åº”çš„ç®—æ³•éƒ½å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚è€Œ ä¸»é¢˜æ¨¡å‹ æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æ¯”è¾ƒç‰¹æ®Šçš„ä¸€å—ï¼Œè¿™ä¸ªæˆ‘ä»¬åé¢å†å•ç‹¬è®²ã€‚ ä¸­æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†æ€»ç»“ä¸Šé¢æˆ‘ä»¬å¯¹ä¸­æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†çš„è¿‡ç¨‹åšäº†ä¸€ä¸ªæ€»ç»“ï¼Œå¸Œæœ›å¯ä»¥å¸®åŠ©åˆ°å¤§å®¶ã€‚éœ€è¦æ³¨æ„çš„æ˜¯è¿™ä¸ªæµç¨‹ä¸»è¦é’ˆå¯¹ä¸€äº›å¸¸ç”¨çš„æ–‡æœ¬æŒ–æ˜ï¼Œå¹¶ä½¿ç”¨äº†è¯è¢‹æ¨¡å‹ï¼Œå¯¹äºæŸä¸€äº›è‡ªç„¶è¯­è¨€å¤„ç†çš„éœ€æ±‚åˆ™æµç¨‹éœ€è¦ä¿®æ”¹ã€‚æ¯”å¦‚æˆ‘ä»¬æ¶‰åŠåˆ°è¯ä¸Šä¸‹æ–‡å…³ç³»çš„ä¸€äº›éœ€æ±‚ï¼Œæ­¤æ—¶ä¸èƒ½ä½¿ç”¨è¯è¢‹æ¨¡å‹ã€‚è€Œæœ‰æ—¶å€™æˆ‘ä»¬å¯¹äºç‰¹å¾çš„å¤„ç†æœ‰è‡ªå·±çš„ç‰¹æ®Šéœ€æ±‚ï¼Œå› æ­¤è¿™ä¸ªæµç¨‹ä»…ä¾›è‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨è€…å‚è€ƒã€‚","link":"/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"},{"title":"æœºå™¨å­¦ä¹ -å¸¸ç”¨æŒ‡æ ‡æ€»ç»“","text":"å‚è€ƒé“¾æ¥ http://www.cnblogs.com/maybe2030/p/5375175.html#_label0 http://alexkong.net/2013/06/introduction-to-auc-and-roc/ ç²¾ç¡®ç‡ Precision, å¬å›ç‡ Recall å’Œ F1 å€¼ å¯¹äºæ•°æ®ä¸å‡è¡¡æ—¶ï¼Œä½¿ç”¨accuracyæ˜¯ä¸å‡†ç¡®çš„ã€‚ ä¸¾ä¸ªæ —å­ï¼š a million tweet: 999,900æ¡ä¸æ˜¯å…³äºpieçš„ï¼Œåªæœ‰100æ¡æ˜¯å…³äºpieçš„ å¯¹äºä¸€ä¸ªstupidåˆ†ç±»å™¨ï¼Œä»–è®¤ä¸ºæ‰€æœ‰çš„tweetéƒ½æ˜¯è·Ÿpieæ— å…³çš„ï¼Œé‚£ä¹ˆå®ƒçš„å‡†ç¡®ç‡æ˜¯99.99%ï¼ä½†è¿™ä¸ªåˆ†ç±»å™¨æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œå› è€Œaccuracyä¸æ˜¯ä¸€ä¸ªå¥½çš„metricï¼Œå½“å®ƒç›®æ ‡æ˜¯rareï¼Œæˆ–æ˜¯complete unbalanced. å¼•å…¥å¦å¤–ä¸¤ä¸ªæŒ‡æ ‡ï¼š ç²¾åº¦ precision: æ˜¯æ£€ç´¢å‡ºæ¥çš„æ¡ç›®ï¼ˆæ¯”å¦‚ï¼šæ–‡æ¡£ã€ç½‘é¡µç­‰ï¼‰æœ‰å¤šå°‘æ˜¯å‡†ç¡®çš„ã€‚ç²¾åº¦æ˜¯æ£€ç´¢å‡ºç›¸å…³æ–‡æ¡£æ•°ä¸æ£€ç´¢å‡ºçš„æ–‡æ¡£æ€»æ•°çš„æ¯”ç‡ï¼Œè¡¡é‡çš„æ˜¯æ£€ç´¢ç³»ç»Ÿçš„æŸ¥å‡†ç‡ã€‚ å¬å› callï¼š å¬å›ç‡æ˜¯æŒ‡æ£€ç´¢å‡ºçš„ç›¸å…³æ–‡æ¡£æ•°å’Œæ–‡æ¡£åº“ä¸­æ‰€æœ‰çš„ç›¸å…³æ–‡æ¡£æ•°çš„æ¯”ç‡ï¼Œè¡¡é‡çš„æ˜¯æ£€ç´¢ç³»ç»Ÿçš„æŸ¥å…¨ç‡. éœ€è¦å…ˆç¡®å®šä¸€ä¸ªæ­£åˆ†ç±»ï¼š è¿™é‡Œéœ€è¦æ£€ç´¢å‡ºæ¥çš„æ˜¯ä¸ pie æ— å…³çš„ï¼Œä¹Ÿå°±æ˜¯ä¸ pie æ— å…³çš„æ˜¯æ­£åˆ†ç±»ã€‚é‚£ä¹ˆç²¾åº¦å°±æ˜¯åˆ†ç±»å™¨æ£€æµ‹å‡ºæ¥çš„æ­£åˆ†ç±»ä¸­ gold pos æ‰€å çš„æ¯”ä¾‹ï¼Œé‚£ä¹ˆ precision = 0/(0+0) è¿™é‡Œéœ€è¦æ£€ç´¢çš„æ˜¯ä¸ pie æ— å…³çš„ï¼Œæ£€ç´¢å‡ºæ¥çš„ true pos å æ ·æœ¬ä¸­ gold pos çš„æ¯”ä¾‹ï¼Œé‚£ä¹ˆ recall = 0/(100+0) = 0. æ€»ç»“ä¸‹æ¥ï¼Œprecision å’Œ recall éƒ½æ˜¯ä»¥ true pos ä½œä¸ºåˆ†å­ï¼Œprecision æ˜¯ä»¥åˆ†ç±»å™¨é¢„æµ‹å‡ºæ¥çš„ pos(true pos + false neg) ä½œä¸ºåˆ†æ¯ï¼Œæ‰€ä»¥æ˜¯å·®å‡†ç‡. recall åˆ™æ˜¯ä»¥æ€»çš„ gold pos(true pos + false neg) ä½œä¸ºåˆ†æ¯ï¼Œæ‰€ä»¥æ˜¯æŸ¥å…¨ç‡ã€‚ å½“ç„¶å¸Œæœ›æ£€ç´¢ç»“æœPrecisionè¶Šé«˜è¶Šå¥½ï¼ŒåŒæ—¶Recallä¹Ÿè¶Šé«˜è¶Šå¥½ï¼Œä½†äº‹å®ä¸Šè¿™ä¸¤è€…åœ¨æŸäº›æƒ…å†µä¸‹æœ‰çŸ›ç›¾çš„ã€‚æ¯”å¦‚æç«¯æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæœç´¢å‡ºäº†ä¸€ä¸ªç»“æœï¼Œä¸”æ˜¯å‡†ç¡®çš„ï¼Œé‚£ä¹ˆPrecisionå°±æ˜¯100%ï¼Œä½†æ˜¯Recallå°±å¾ˆä½ï¼›è€Œå¦‚æœæˆ‘ä»¬æŠŠæ‰€æœ‰ç»“æœéƒ½è¿”å›ï¼Œé‚£ä¹ˆæ¯”å¦‚Recallæ˜¯100%ï¼Œä½†æ˜¯Precisionå°±ä¼šå¾ˆä½ã€‚å› æ­¤åœ¨ä¸åŒçš„åœºåˆä¸­éœ€è¦è‡ªå·±åˆ¤æ–­å¸Œæœ›Precisionæ¯”è¾ƒé«˜æˆ–æ˜¯Recallæ¯”è¾ƒé«˜ã€‚å¦‚æœæ˜¯åšå®éªŒç ”ç©¶ï¼Œå¯ä»¥ç»˜åˆ¶Precision-Recallæ›²çº¿æ¥å¸®åŠ©åˆ†æã€‚ ï¼¦-measure $$F_{\\beta}=\\dfrac{(\\beta^2+1)PR}{\\beta^2P+R}$$ å½“ $\\beta&gt;1$æ—¶ï¼ŒRecallçš„æ¯”é‡æ›´å¤§;å½“ $\\beta&lt;1$æ—¶ï¼Œprecisionçš„æ¯”é‡æ›´å¤§ã€‚ä½¿ç”¨æœ€å¤šçš„æ˜¯ $\\beta=1$,ä¹Ÿå°±æ˜¯ $F_{\\beta=1}, F_1$. $\\beta$ çš„çš„å–å€¼å–å†³äºå®é™…åº”ç”¨ã€‚ $$F_1 = \\dfrac{2PR}{P+R}$$ ï¼¦-measure æ˜¯ precision å’Œ recall çš„ **åŠ æƒè°ƒå’Œå¹³å‡å€¼(weighted harmonic mean)**ã€‚ è°ƒå’Œå¹³å‡å€¼æ˜¯å€’æ•°çš„ç®—æœ¯å¹³å‡å€¼çš„å€’æ•°ã€‚ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è°ƒå’Œå¹³å‡å€¼å‘¢ï¼Ÿå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªæ›´ ä¿å®ˆçš„åº¦é‡(conservative metric). ç›¸æ¯”ç›´æ¥è®¡ç®— P å’Œ R çš„å¹³å‡å€¼ï¼Œ F-measureçš„å€¼æ›´çœ‹é‡ä¸¤è€…ä¸­çš„è¾ƒå°å€¼ã€‚ ROCæ›²çº¿å’ŒAUCè€ƒè™‘ä¸€ä¸ªäºŒåˆ†é—®é¢˜ï¼Œå³å°†å®ä¾‹åˆ†æˆæ­£ç±»ï¼ˆpositiveï¼‰æˆ–è´Ÿç±»ï¼ˆnegativeï¼‰ã€‚ TPR, çœŸæ­£ç±»ç‡(true positive rate ,TPR),ï¼š å¦‚æœä¸€ä¸ªå®ä¾‹æ˜¯æ­£ç±»å¹¶ä¸”ä¹Ÿè¢« é¢„æµ‹æˆæ­£ç±»ï¼Œå³ä¸ºçœŸæ­£ç±»ï¼ˆTrue positiveï¼‰ï¼ŒçœŸæ­£ç±»ç‡æ˜¯æŒ‡åˆ†ç±»å™¨æ‰€è¯†åˆ«å‡ºæ¥çš„ æ­£å®ä¾‹å æ‰€æœ‰æ­£å®ä¾‹çš„æ¯”ä¾‹ã€‚å°±æ˜¯ Recall å§ï½ TPR = TP / (TP + FN) FPR, è´Ÿæ­£ç±»ç‡ï¼š åˆ†ç±»å™¨é”™è®¤ä¸ºæ­£ç±»çš„è´Ÿå®ä¾‹å æ‰€æœ‰è´Ÿå®ä¾‹çš„æ¯”ä¾‹ï¼ŒFPR = FP / (FP + TN) TNRï¼Œ çœŸè´Ÿç±»ç‡ï¼š åˆ†ç±»å™¨è®¤ä¸ºè´Ÿç±»çš„è´Ÿå®ä¾‹å æ‰€æœ‰è´Ÿå®ä¾‹çš„æ¯”ä¾‹ï¼Œä¹Ÿå°±æ˜¯è´Ÿç±»çš„ Recall å§ï½ TNR = TN /(FP + TN) = 1 - FPR ä¸ºä»€ä¹ˆè¦å¼•å…¥ ROC æ›²çº¿ Motivation1ï¼šåœ¨ä¸€ä¸ªäºŒåˆ†ç±»æ¨¡å‹ä¸­ï¼Œå¯¹äºæ‰€å¾—åˆ°çš„è¿ç»­ç»“æœï¼Œå‡è®¾å·²ç¡®å®šä¸€ä¸ªé˜€å€¼ï¼Œæ¯”å¦‚è¯´ 0.6ï¼Œå¤§äºè¿™ä¸ªå€¼çš„å®ä¾‹åˆ’å½’ä¸ºæ­£ç±»ï¼Œå°äºè¿™ä¸ªå€¼åˆ™åˆ’åˆ°è´Ÿç±»ä¸­ã€‚å¦‚æœå‡å°é˜€å€¼ï¼Œå‡åˆ°0.5ï¼Œå›ºç„¶èƒ½è¯†åˆ«å‡ºæ›´å¤šçš„æ­£ç±»ï¼Œä¹Ÿå°±æ˜¯æé«˜äº†è¯†åˆ«å‡ºçš„æ­£ä¾‹å æ‰€æœ‰æ­£ä¾‹ çš„æ¯”ç±»ï¼Œå³TPR,ä½†åŒæ—¶ä¹Ÿå°†æ›´å¤šçš„è´Ÿå®ä¾‹å½“ä½œäº†æ­£å®ä¾‹ï¼Œå³æé«˜äº†FPRã€‚ä¸ºäº†å½¢è±¡åŒ–è¿™ä¸€å˜åŒ–ï¼Œå¼•å…¥ROCï¼ŒROCæ›²çº¿å¯ä»¥ç”¨äºè¯„ä»·ä¸€ä¸ªåˆ†ç±»å™¨ã€‚ Motivation2ï¼šåœ¨ç±»ä¸å¹³è¡¡çš„æƒ…å†µä¸‹,å¦‚æ­£æ ·æœ¬90ä¸ª,è´Ÿæ ·æœ¬10ä¸ª,ç›´æ¥æŠŠæ‰€æœ‰æ ·æœ¬åˆ†ç±»ä¸ºæ­£æ ·æœ¬,å¾—åˆ°è¯†åˆ«ç‡ä¸º90%ã€‚ä½†è¿™æ˜¾ç„¶æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚å•çº¯æ ¹æ®Precisionå’ŒRecallæ¥è¡¡é‡ç®—æ³•çš„ä¼˜åŠ£å·²ç»ä¸èƒ½è¡¨å¾è¿™ç§ç—…æ€é—®é¢˜ã€‚ ROC æ›²çº¿å‰é¢å·²ç»è¯´é“ï¼Œå¯¹äºæ•°æ®ä¸å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œprecision å’Œ recall ä¸è¶³ä»¥è¡¨å¾è¿™ç±»é—®é¢˜ï¼Œå°±æ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ä¸­ï¼ŒæŠŠæ‰¾å‡ºä¸å…³äº pie çš„ tweetçœ‹ä½œæ˜¯æ­£ç±»ï¼Œé‚£ä¹ˆå®ƒçš„ ç²¾åº¦å’Œå¬å›ç‡ éƒ½å¾ˆé«˜ã€‚æ‰€ä»¥å¼•å…¥ ROC å› ä¸ºæˆ‘ä»¬è¦å…³æ³¨çš„æ˜¯æ­£ç±»ï¼Œæ‰€ä»¥å…³æ³¨æŒ‡æ ‡æ˜¯ çœŸæ­£ç±»ç‡ TPR å’Œ è´Ÿæ­£ç±»ç‡ FPRï¼ŒçœŸæ­£ç±»ç‡è¶Šé«˜è¶Šå¥½ï¼Œè´Ÿæ­£ç±»ç‡è¶Šä½è¶Šå¥½ã€‚ä½†æ˜¾ç„¶è¿™ä¸¤è€…ä¹‹é—´æ˜¯çŸ›ç›¾çš„ï¼Œä¸åˆ†ç±»å™¨çš„é˜ˆå€¼æœ‰å…³ï¼ŒROC å°±æ˜¯ç”¨æ¥è¡¨å¾é˜ˆå€¼ä¸ TPR å’Œ FPR ä¹‹é—´çš„å…³ç³»æ›²çº¿ã€‚ ROCï¼ˆReceiver Operating Characteristicï¼‰ç¿»è¯‘ä¸ºâ€œæ¥å—è€…æ“ä½œæ›²çº¿â€ã€‚æ›²çº¿ç”±ä¸¤ä¸ªå˜é‡1-specificity å’Œ Sensitivityç»˜åˆ¶. 1-specificity=FPRï¼Œå³è´Ÿæ­£ç±»ç‡ã€‚Sensitivityå³æ˜¯çœŸæ­£ç±»ç‡ï¼ŒTPR(True positive rate),åæ˜ äº†æ­£ç±»è¦†ç›–ç¨‹åº¦ã€‚ ä¸ºäº†æ›´å¥½åœ°ç†è§£ROCæ›²çº¿ï¼Œæˆ‘ä»¬ä½¿ç”¨å…·ä½“çš„å®ä¾‹æ¥è¯´æ˜ï¼š å¦‚åœ¨åŒ»å­¦è¯Šæ–­ä¸­,åˆ¤æ–­æœ‰ç—…çš„æ ·æœ¬ã€‚é‚£ä¹ˆå°½é‡æŠŠæœ‰ç—…çš„æªå‡ºæ¥æ˜¯ä¸»è¦ä»»åŠ¡,ä¹Ÿå°±æ˜¯ç¬¬ä¸€ä¸ªæŒ‡æ ‡TPR,è¦è¶Šé«˜è¶Šå¥½ã€‚è€ŒæŠŠæ²¡ç—…çš„æ ·æœ¬è¯¯è¯Šä¸ºæœ‰ç—…çš„,ä¹Ÿå°±æ˜¯ç¬¬äºŒä¸ªæŒ‡æ ‡FPR,è¦è¶Šä½è¶Šå¥½ã€‚ ä¸éš¾å‘ç°,è¿™ä¸¤ä¸ªæŒ‡æ ‡ä¹‹é—´æ˜¯ç›¸äº’åˆ¶çº¦çš„ã€‚å¦‚æœæŸä¸ªåŒ»ç”Ÿå¯¹äºæœ‰ç—…çš„ç—‡çŠ¶æ¯”è¾ƒæ•æ„Ÿ,ç¨å¾®çš„å°ç—‡çŠ¶éƒ½åˆ¤æ–­ä¸ºæœ‰ç—…,é‚£ä¹ˆä»–çš„ç¬¬ä¸€ä¸ªæŒ‡æ ‡åº”è¯¥ä¼šå¾ˆé«˜,ä½†æ˜¯ç¬¬äºŒä¸ªæŒ‡æ ‡ä¹Ÿå°±ç›¸åº”åœ°å˜é«˜ã€‚æœ€æç«¯çš„æƒ…å†µä¸‹,ä»–æŠŠæ‰€æœ‰çš„æ ·æœ¬éƒ½çœ‹åšæœ‰ç—…,é‚£ä¹ˆç¬¬ä¸€ä¸ªæŒ‡æ ‡è¾¾åˆ°1,ç¬¬äºŒä¸ªæŒ‡æ ‡ä¹Ÿä¸º1ã€‚ æˆ‘ä»¬ä»¥FPRä¸ºæ¨ªè½´,TPRä¸ºçºµè½´,å¾—åˆ°å¦‚ä¸‹ROCç©ºé—´ã€‚ å‚è€ƒé“¾æ¥ä¸­ ROC æ›²çº¿çš„è§£é‡Š: http://www.cnblogs.com/maybe2030/p/5375175.html#_label0 ä¸Šé¢çš„å›¾è¡¨ç¤ºï¼Œåˆ†ç±»å™¨å¯¹äº pos å’Œ neg çš„åˆ†åˆ«æ˜¯æœ‰ä¸ªé˜ˆå€¼çš„ï¼Œé˜ˆå€¼å¾ˆé«˜ï¼Œä¹Ÿå°±æ˜¯ A å¤„ï¼Œæ˜¾ç„¶å®ƒçš„ TPR ä¸ä¼šå¾ˆé«˜ï¼Œé˜ˆå€¼è¶Šä½ï¼ŒTPR è¶Šé«˜ã€‚ ä½†æ˜¯é˜ˆå€¼å¾ˆä½çš„è¯ï¼Œé¢„æµ‹ä¸ºæ­£ç±»çš„è´Ÿå®ä¾‹ä¹Ÿå°±è¶Šå¤šï¼Œ FPR ä¹Ÿä¼šè¶Šé«˜ã€‚ æ›²çº¿è·ç¦»å·¦ä¸Šè§’è¶Šè¿‘,è¯æ˜åˆ†ç±»å™¨æ•ˆæœè¶Šå¥½ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªæ ‡é‡ AUC æ¥é‡åŒ–è¿™ä¸ªåˆ†ç±»æ•ˆæœã€‚ æ€ä¹ˆå¾—åˆ° ROC æ›²çº¿æˆ‘ä»¬çŸ¥é“å¯¹ä¸€ä¸ªäºŒå€¼åˆ†ç±»å™¨ï¼Œå…¶é¢„æµ‹å‡ºæ¥çš„æ­£ç±»çš„ score æ˜¯ä¸€ä¸ªæ¦‚ç‡ã€‚å½“ä¸€ä¸ªæ ·æœ¬ä¸ºæ­£ç±»çš„æ¦‚ç‡å¤§äº thresholdæ—¶ï¼Œæˆ‘ä»¬åˆ¤åˆ«å®ƒä¸ºæ­£ç±»ã€‚æ‰€ä»¥ä¸åŒçš„ thresholdï¼Œå…¶å¯¹åº”çš„ TPR å’Œ FPR çš„å€¼ä¹Ÿå°±ä¸ä¸€æ ·ï¼Œè¿™æ ·å°±å¾—åˆ°äº† ROC æ›²çº¿ã€‚ è¯¦ç»†å¯å‚è€ƒï¼šROCå’ŒAUCä»‹ç»ä»¥åŠå¦‚ä½•è®¡ç®—AUC éå¸¸æ¸…æ¥šï¼ï¼ï¼ AUCAUCå€¼ä¸ºROCæ›²çº¿æ‰€è¦†ç›–çš„åŒºåŸŸé¢ç§¯,æ˜¾ç„¶,AUCè¶Šå¤§,åˆ†ç±»å™¨åˆ†ç±»æ•ˆæœè¶Šå¥½ã€‚ AUC = 1ï¼Œæ˜¯å®Œç¾åˆ†ç±»å™¨ï¼Œé‡‡ç”¨è¿™ä¸ªé¢„æµ‹æ¨¡å‹æ—¶ï¼Œä¸ç®¡è®¾å®šä»€ä¹ˆé˜ˆå€¼éƒ½èƒ½å¾—å‡ºå®Œç¾é¢„æµ‹ã€‚ç»å¤§å¤šæ•°é¢„æµ‹çš„åœºåˆï¼Œä¸å­˜åœ¨å®Œç¾åˆ†ç±»å™¨ã€‚ 0.5 &lt; AUC &lt; 1ï¼Œä¼˜äºéšæœºçŒœæµ‹ã€‚è¿™ä¸ªåˆ†ç±»å™¨ï¼ˆæ¨¡å‹ï¼‰å¦¥å–„è®¾å®šé˜ˆå€¼çš„è¯ï¼Œèƒ½æœ‰é¢„æµ‹ä»·å€¼ã€‚ AUC = 0.5ï¼Œè·ŸéšæœºçŒœæµ‹ä¸€æ ·ï¼ˆä¾‹ï¼šä¸¢é“œæ¿ï¼‰ï¼Œæ¨¡å‹æ²¡æœ‰é¢„æµ‹ä»·å€¼ã€‚ AUC &lt; 0.5ï¼Œæ¯”éšæœºçŒœæµ‹è¿˜å·®ï¼›ä½†åªè¦æ€»æ˜¯åé¢„æµ‹è€Œè¡Œï¼Œå°±ä¼˜äºéšæœºçŒœæµ‹ã€‚ AUCçš„ç‰©ç†æ„ä¹‰ï¼šå‡è®¾åˆ†ç±»å™¨çš„è¾“å‡ºæ˜¯æ ·æœ¬å±äºæ­£ç±»çš„socreï¼ˆç½®ä¿¡åº¦ï¼‰ï¼Œåˆ™AUCçš„ç‰©ç†æ„ä¹‰ä¸ºï¼Œä»»å–ä¸€å¯¹ï¼ˆæ­£ã€è´Ÿï¼‰æ ·æœ¬ï¼Œæ­£æ ·æœ¬çš„scoreå¤§äºè´Ÿæ ·æœ¬çš„scoreçš„æ¦‚ç‡ã€‚ æ€ä¹ˆè®¡ç®— AUCAUCï¼ˆArea Under Curveï¼‰è¢«å®šä¹‰ä¸ºROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œæ˜¾ç„¶è¿™ä¸ªé¢ç§¯çš„æ•°å€¼ä¸ä¼šå¤§äº1ã€‚åˆç”±äºROCæ›²çº¿ä¸€èˆ¬éƒ½å¤„äºy=xè¿™æ¡ç›´çº¿çš„ä¸Šæ–¹ï¼Œæ‰€ä»¥AUCçš„å–å€¼èŒƒå›´åœ¨0.5å’Œ1ä¹‹é—´ã€‚ä½¿ç”¨AUCå€¼ä½œä¸ºè¯„ä»·æ ‡å‡†æ˜¯å› ä¸ºå¾ˆå¤šæ—¶å€™ROCæ›²çº¿å¹¶ä¸èƒ½æ¸…æ™°çš„è¯´æ˜å“ªä¸ªåˆ†ç±»å™¨çš„æ•ˆæœæ›´å¥½ï¼Œè€Œä½œä¸ºä¸€ä¸ªæ•°å€¼ï¼Œå¯¹åº”AUCæ›´å¤§çš„åˆ†ç±»å™¨æ•ˆæœæ›´å¥½ã€‚ ä»£ç å®ç°ï¼š sklearn.metrics.roc_auc_score ç½®ä¿¡åº¦å’Œç½®ä¿¡åŒºé—´è¿™é‡Œå¥½åƒè·Ÿç½®ä¿¡åº¦å’Œç½®ä¿¡åŒºé—´æ²¡å•¥å…³ç³»ã€‚ã€‚ã€‚ä½†æ˜¯äº†è§£ä¸‹ä¹Ÿæ²¡äº‹ ç½®ä¿¡åŒºé—´å†æ¥ç†è§£ç½®ä¿¡åº¦ï¼Œé¦–å…ˆè¦ç†è§£ 95%ç½®ä¿¡åŒºé—´ å’Œ ç½®ä¿¡åº¦ã€‚ çŸ¥ä¹ï¼š95%ç½®ä¿¡åŒºé—´ æˆ‘çš„ç†è§£å°±æ˜¯ï¼Œé¦–å…ˆæ€»ä½“å‡å€¼æ˜¯å›ºå®šçš„ï¼Œä½†åªæœ‰ä¸Šå¸çŸ¥é“ã€‚æˆ‘ä»¬å°±è¦ç”¨æ ·æœ¬å»ä¼°è®¡æ€»ä½“å‡å€¼ã€‚ä¸€æ¬¡æ¬¡æŠ½æ ·ä¼šå¾—åˆ°å¾ˆå¤šæ ·æœ¬å‡å€¼ï¼Œä½†æ˜¯æˆ‘ä»¬æ— æ³•åˆ¤æ–­å“ªä¸ªå‡å€¼æœ€å¥½ï¼Œæœ€æ¥è¿‘æ€»ä½“å‡å€¼ã€‚äºæ˜¯ï¼Œæˆ‘ä»¬æ„é€ åŒºé—´æ¥çœ‹è¿™ä¸ªåŒºé—´æ˜¯å¦å«æœ‰æ€»ä½“å‡å€¼ã€‚ æ€ä¹ˆæ„é€ åŒºé—´ï¼š é€šè¿‡ä¸€æ¬¡æ¬¡æŠ½æ ·å¾—åˆ°çš„æ ·æœ¬å‡å€¼ï¼š $$M=\\dfrac{X_1 + X_2+â€¦+X_n}{n}$$ æ ¹æ®å¤§æ•°å®šå¾‹ï¼š $$M\\sim N(\\mu,\\dfrac{\\sigma^2}{n})$$ é€šè¿‡æŸ¥è¡¨ æ ‡å‡†æ­£å¤ªåˆ†å¸ƒè¡¨å¯çŸ¥ï¼Œè¿™æ ·å¯ä»¥è®¡ç®—å‡ºç½®ä¿¡åŒºé—´ï¼Œï¼ˆå¦‚æœæ–¹å·®ä¸ºæ­¢ï¼Œåˆ™ç”¨æ ·æœ¬æ–¹å·®ä»£æ›¿ï¼‰ æˆ‘ä»¬ä»¥ $1.96\\dfrac{\\sigma }{\\sqrt{n}}$ ä¸ºåŠå¾„åšåŒºé—´ï¼Œå°±æ„é€ å‡ºäº† $95%$ ç½®ä¿¡åŒºé—´ã€‚æŒ‰è¿™æ ·å»æ„é€ çš„100ä¸ªåŒºé—´ï¼Œå…¶ä¸­å¤§çº¦ä¼šæœ‰95ä¸ªä¼šåŒ…å« $\\mu$ ï¼š é‚£ä¹ˆï¼Œåªæœ‰ä¸€ä¸ªé—®é¢˜äº†ï¼Œæˆ‘ä»¬ä¸çŸ¥é“ã€å¹¶ä¸”æ°¸è¿œéƒ½ä¸ä¼šçŸ¥é“çœŸå®çš„ $\\mu$ æ˜¯å¤šå°‘: æˆ‘ä»¬å°±åªæœ‰ç”¨ $\\hat{\\mu }$ æ¥ä»£æ›¿ $\\mu$ ï¼š $$P(\\hat \\mu-1.96\\dfrac{\\sigma}{n}\\le M\\le\\hat \\mu+1.96\\dfrac{\\sigma}{n}) = 0.95$$ è¿™æ ·å¯ä»¥å¾—åˆ°ç½®ä¿¡åŒºé—´äº†ã€‚å¦‚æœæŠ½æ ·100æ¬¡ï¼Œå¯¹åº”ä¹Ÿå°±æœ‰100ä¸ªç½®ä¿¡åŒºé—´ï¼Œé‚£ä¹ˆå…¶ä¸­å«æœ‰æ€»ä½“å‡å€¼çš„æ¦‚ç‡çº¦ä¸º 95%. ç½®ä¿¡åº¦æ ·æœ¬æ•°ç›®ä¸å˜çš„æƒ…å†µä¸‹ï¼Œåšä¸€ç™¾æ¬¡è¯•éªŒï¼Œæœ‰95ä¸ªç½®ä¿¡åŒºé—´åŒ…å«äº†æ€»ä½“çœŸå€¼ã€‚ç½®ä¿¡åº¦ä¸º95%","link":"/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/"},{"title":"æœºå™¨å­¦ä¹ -è‹±æ–‡æ–‡æœ¬é¢„å¤„ç†","text":"è½¬è½½è‡ªï¼šhttp://www.cnblogs.com/pinard/p/6756534.htmlè‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ç‰¹ç‚¹è‹±æ–‡æ–‡æœ¬çš„é¢„å¤„ç†æ–¹æ³•å’Œä¸­æ–‡çš„æœ‰éƒ¨åˆ†åŒºåˆ«ã€‚é¦–å…ˆï¼Œè‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¸€èˆ¬å¯ä»¥ä¸åšåˆ†è¯ï¼ˆç‰¹æ®Šéœ€æ±‚é™¤å¤–ï¼‰ï¼Œè€Œä¸­æ–‡é¢„å¤„ç†åˆ†è¯æ˜¯å¿…ä¸å¯å°‘çš„ä¸€æ­¥ã€‚ç¬¬äºŒç‚¹ï¼Œå¤§éƒ¨åˆ†è‹±æ–‡æ–‡æœ¬éƒ½æ˜¯uft-8çš„ç¼–ç ï¼Œè¿™æ ·åœ¨å¤§å¤šæ•°æ—¶å€™å¤„ç†çš„æ—¶å€™ä¸ç”¨è€ƒè™‘ç¼–ç è½¬æ¢çš„é—®é¢˜ï¼Œè€Œä¸­æ–‡æ–‡æœ¬å¤„ç†å¿…é¡»è¦å¤„ç†unicodeçš„ç¼–ç é—®é¢˜ã€‚ è€Œè‹±æ–‡æ–‡æœ¬çš„é¢„å¤„ç†ä¹Ÿæœ‰è‡ªå·±ç‰¹æ®Šçš„åœ°æ–¹ï¼Œç¬¬ä¸‰ç‚¹å°±æ˜¯æ‹¼å†™é—®é¢˜ï¼Œå¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬çš„é¢„å¤„ç†è¦åŒ…æ‹¬æ‹¼å†™æ£€æŸ¥ï¼Œæ¯”å¦‚â€œHelo Worldâ€è¿™æ ·çš„é”™è¯¯ï¼Œæˆ‘ä»¬ä¸èƒ½åœ¨åˆ†æçš„æ—¶å€™è®²é”™çº é”™ã€‚æ‰€ä»¥éœ€è¦åœ¨é¢„å¤„ç†å‰åŠ ä»¥çº æ­£ã€‚ç¬¬å››ç‚¹å°±æ˜¯è¯å¹²æå–(stemming)å’Œè¯å½¢è¿˜åŸ(lemmatization)ã€‚è¿™ä¸ªä¸œè¥¿ä¸»è¦æ˜¯è‹±æ–‡æœ‰å•æ•°ï¼Œå¤æ•°å’Œå„ç§æ—¶æ€ï¼Œå¯¼è‡´ä¸€ä¸ªè¯ä¼šæœ‰ä¸åŒçš„å½¢å¼ã€‚æ¯”å¦‚â€œcountriesâ€å’Œâ€countryâ€ï¼Œâ€wolfâ€å’Œâ€wolvesâ€ï¼Œæˆ‘ä»¬æœŸæœ›æ˜¯æœ‰ä¸€ä¸ªè¯ã€‚ è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¸€ï¼šæ•°æ®æ”¶é›†è¿™éƒ¨åˆ†è‹±æ–‡å’Œä¸­æ–‡ç±»ä¼¼ã€‚è·å–æ–¹æ³•ä¸€èˆ¬æœ‰ä¸¤ç§ï¼šä½¿ç”¨åˆ«äººåšå¥½çš„è¯­æ–™åº“å’Œè‡ªå·±ç”¨çˆ¬è™«å»åœ¨ç½‘ä¸Šå»çˆ¬è‡ªå·±çš„è¯­æ–™æ•°æ®ã€‚ å¯¹äºç¬¬ä¸€ç§æ–¹æ³•ï¼Œå¸¸ç”¨çš„æ–‡æœ¬è¯­æ–™åº“åœ¨ç½‘ä¸Šæœ‰å¾ˆå¤šï¼Œå¦‚æœå¤§å®¶åªæ˜¯å­¦ä¹ ï¼Œåˆ™å¯ä»¥ç›´æ¥ä¸‹è½½ä¸‹æ¥ä½¿ç”¨ï¼Œä½†å¦‚æœæ˜¯æŸäº›ç‰¹æ®Šä¸»é¢˜çš„è¯­æ–™åº“ï¼Œæ¯”å¦‚â€œdeep learningâ€ç›¸å…³çš„è¯­æ–™åº“ï¼Œåˆ™è¿™ç§æ–¹æ³•è¡Œä¸é€šï¼Œéœ€è¦æˆ‘ä»¬è‡ªå·±ç”¨ç¬¬äºŒç§æ–¹æ³•å»è·å–ã€‚ å¯¹äºç¬¬äºŒç§ä½¿ç”¨çˆ¬è™«çš„æ–¹æ³•ï¼Œå¼€æºå·¥å…·æœ‰å¾ˆå¤šï¼Œé€šç”¨çš„çˆ¬è™«æˆ‘ä¸€èˆ¬ä½¿ç”¨beautifulsoupã€‚ä½†æ˜¯æˆ‘ä»¬æˆ‘ä»¬éœ€è¦æŸäº›ç‰¹æ®Šçš„è¯­æ–™æ•°æ®ï¼Œæ¯”å¦‚ä¸Šé¢æåˆ°çš„â€œdeep learningâ€ç›¸å…³çš„è¯­æ–™åº“ï¼Œåˆ™éœ€è¦ç”¨ä¸»é¢˜çˆ¬è™«ï¼ˆä¹Ÿå«èšç„¦çˆ¬è™«ï¼‰æ¥å®Œæˆã€‚è¿™ä¸ªæˆ‘ä¸€èˆ¬ä½¿ç”¨acheã€‚ acheå…è®¸æˆ‘ä»¬ç”¨å…³é”®å­—æˆ–è€…ä¸€ä¸ªåˆ†ç±»ç®—æ³•æ¨¡å‹æ¥è¿‡æ»¤å‡ºæˆ‘ä»¬éœ€è¦çš„ä¸»é¢˜è¯­æ–™ï¼Œæ¯”è¾ƒå¼ºå¤§ã€‚ è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†äºŒï¼šé™¤å»æ•°æ®ä¸­éæ–‡æœ¬éƒ¨åˆ†è¿™ä¸€æ­¥ä¸»è¦æ˜¯é’ˆå¯¹æˆ‘ä»¬ç”¨çˆ¬è™«æ”¶é›†çš„è¯­æ–™æ•°æ®ï¼Œç”±äºçˆ¬ä¸‹æ¥çš„å†…å®¹ä¸­æœ‰å¾ˆå¤šhtmlçš„ä¸€äº›æ ‡ç­¾ï¼Œéœ€è¦å»æ‰ã€‚å°‘é‡çš„éæ–‡æœ¬å†…å®¹çš„å¯ä»¥ç›´æ¥ç”¨Pythonçš„æ­£åˆ™è¡¨è¾¾å¼(re)åˆ é™¤, å¤æ‚çš„åˆ™å¯ä»¥ç”¨beautifulsoupæ¥å»é™¤ã€‚å¦å¤–è¿˜æœ‰ä¸€äº›ç‰¹æ®Šçš„éè‹±æ–‡å­—ç¬¦(non-alpha),ä¹Ÿå¯ä»¥ç”¨Pythonçš„æ­£åˆ™è¡¨è¾¾å¼(re)åˆ é™¤ã€‚ re æ¨¡å—å‚è€ƒ blog æ­£åˆ™è¡¨è¾¾å¼ï¼ˆRegular Expressionï¼‰æ˜¯å­—ç¬¦ä¸²å¤„ç†çš„å¸¸ç”¨å·¥å…·ï¼Œé€šå¸¸è¢«ç”¨æ¥æ£€ç´¢ã€æ›¿æ¢é‚£äº›ç¬¦åˆæŸä¸ªæ¨¡å¼ï¼ˆPatternï¼‰çš„æ–‡æœ¬ã€‚å¾ˆå¤šç¨‹åºè®¾è®¡è¯­è¨€éƒ½æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ï¼ŒåƒPerlã€Javaã€C/C++ã€‚åœ¨ Python ä¸­æ˜¯é€šè¿‡æ ‡å‡†åº“ä¸­çš„ re æ¨¡å— æä¾›å¯¹æ­£åˆ™çš„æ”¯æŒã€‚ å…³äºæ­£åˆ™è¡¨è¾¾å¼çš„è¯­æ³•å¯ä»¥çœ‹ speech and language processing chapter2 æ­£åˆ™è¡¨è¾¾å¼ä¸­çš„*ï¼Œ+ï¼Œï¼Ÿä»¥åŠ\\wå’Œ\\Wçš„åŒºåˆ«ç­‰å¸¸è§é—®é¢˜çš„æ€»ç»“ ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼re æ¨¡å—æä¾›äº† re.compile() å‡½æ•°å°†ä¸€ä¸ªå­—ç¬¦ä¸²ç¼–è¯‘æˆ pattern objectï¼Œç”¨äºåŒ¹é…æˆ–æœç´¢ã€‚å‡½æ•°åŸå‹å¦‚ä¸‹ï¼š 123re.compile(pattern, flags=0) re.compile() è¿˜æ¥å—ä¸€ä¸ªå¯é€‰çš„å‚æ•° flagï¼Œç”¨äºæŒ‡å®šæ­£åˆ™åŒ¹é…çš„æ¨¡å¼ã€‚å…³äºåŒ¹é…æ¨¡å¼ï¼Œåé¢å°†ä¼šè®²åˆ°ã€‚ åæ–œæ çš„å›°æ‰°åœ¨ python çš„å­—ç¬¦ä¸²ä¸­ï¼Œ\\ æ˜¯è¢«å½“åšè½¬ä¹‰å­—ç¬¦çš„ã€‚åœ¨æ­£åˆ™è¡¨è¾¾å¼ä¸­ï¼Œ\\ ä¹Ÿæ˜¯è¢«å½“åšè½¬ä¹‰å­—ç¬¦ã€‚è¿™å°±å¯¼è‡´äº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœä½ è¦åŒ¹é… \\ å­—ç¬¦ä¸²ï¼Œé‚£ä¹ˆä¼ é€’ç»™ re.compile() çš„å­—ç¬¦ä¸²å¿…é¡»æ˜¯ â€\\\\\\\\â€œã€‚ ç”±äºå­—ç¬¦ä¸²çš„è½¬ä¹‰ï¼Œæ‰€ä»¥å®é™…ä¼ é€’ç»™ re.compile() çš„æ˜¯ â€\\\\â€œï¼Œç„¶åå†é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼çš„è½¬ä¹‰ï¼Œâ€\\\\â€œ ä¼šåŒ¹é…åˆ°å­—ç¬¦â€\\â€œã€‚è¿™æ ·è™½ç„¶å¯ä»¥æ­£ç¡®åŒ¹é…åˆ°å­—ç¬¦ \\ï¼Œä½†æ˜¯å¾ˆéº»çƒ¦ï¼Œè€Œä¸”å®¹æ˜“æ¼å†™åæ–œæ è€Œå¯¼è‡´ Bugã€‚é‚£ä¹ˆæœ‰ä»€ä¹ˆå¥½çš„è§£å†³æ–¹æ¡ˆå‘¢ï¼Ÿ åŸå§‹å­—ç¬¦ä¸²å¾ˆå¥½çš„è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡åœ¨å­—ç¬¦ä¸²å‰é¢æ·»åŠ ä¸€ä¸ªrï¼Œè¡¨ç¤ºåŸå§‹å­—ç¬¦ä¸²ï¼Œä¸è®©å­—ç¬¦ä¸²çš„åæ–œæ å‘ç”Ÿè½¬ä¹‰ã€‚é‚£ä¹ˆå°±å¯ä»¥ä½¿ç”¨r&quot;\\\\\\\\&quot;æ¥åŒ¹é…å­—ç¬¦ \\äº†ã€‚ patern object æ‰§è¡ŒåŒ¹é…ä¸€æ—¦ä½ ç¼–è¯‘å¾—åˆ°äº†ä¸€ä¸ª pattern objectï¼Œä½ å°±å¯ä»¥ä½¿ç”¨ pattern object çš„æ–¹æ³•æˆ–å±æ€§è¿›è¡ŒåŒ¹é…äº†ï¼Œä¸‹é¢åˆ—ä¸¾å‡ ä¸ªå¸¸ç”¨çš„æ–¹æ³•ï¼Œæ›´å¤šè¯·çœ‹è¿™é‡Œã€‚ Pattern.match(string[, pos[, endpos]]) åŒ¹é…ä» pos åˆ° endpos çš„å­—ç¬¦å­ä¸²çš„å¼€å¤´ã€‚åŒ¹é…æˆåŠŸè¿”å›ä¸€ä¸ª match objectï¼Œä¸åŒ¹é…è¿”å› Noneã€‚ pos çš„é»˜è®¤å€¼æ˜¯0ï¼Œendpos çš„é»˜è®¤å€¼æ˜¯ len(string)ï¼Œæ‰€ä»¥é»˜è®¤æƒ…å†µä¸‹æ˜¯åŒ¹é…æ•´ä¸ªå­—ç¬¦ä¸²çš„å¼€å¤´ã€‚ 1234567891011pattern = re.compile(&quot;d&quot;)print(pattern.match('dog')) # åœ¨å­—ä¸²å¼€å¤´ï¼ŒåŒ¹é…æˆåŠŸprint(pattern.match('god')) # ä¸å†å­ä¸²å¼€å¤´ï¼ŒåŒ¹é…ä¸æˆåŠŸprint(pattern.match('ddaa', 1,5)) # åœ¨å­ä¸²å¼€å¤´,åŒ¹é…æˆåŠŸprint(pattern.match('monday', 3)) 123456789&lt;_sre.SRE_Match object; span=(0, 1), match='d'&gt;&lt;_sre.SRE_Match object; span=(0, 1), match='g'&gt;&lt;_sre.SRE_Match object; span=(1, 2), match='d'&gt;&lt;_sre.SRE_Match object; span=(3, 4), match='d'&gt; regex.search(string[, pos[, endpos]]) æ‰«ææ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›å®ƒæ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªåŒ¹é… å’Œ regex.match() ä¸€æ ·ï¼Œå¯ä»¥é€šè¿‡ pos å’Œ endpos æŒ‡å®šèŒƒå›´ 1234567891011pattern = re.compile(&quot;ar{1}&quot;)match = pattern.search(&quot;marray&quot;)print(match)&lt;_sre.SRE_Match object; span=(1, 3), match='ar'&gt; regex.findall(string[, pos[, endpos]]) æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„å­ä¸²ï¼Œå¹¶è¿”å›ä¸€ä¸ª list å¯é€‰å‚æ•° pos å’Œ endpos å’Œä¸Šé¢ä¸€æ · 1234567891011pattern = re.compile(r&quot;\\d+&quot;) # åŒ¹é…å­—ç¬¦ä¸²ä¸­çš„æ•°å­—lst = pattern.findall(&quot;abc1def2rst3xyz&quot;)print(lst)['1', '2', '3'] regex.finditer(string[, pos[, endpos]]) æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„å­ä¸²ï¼Œå¹¶è¿”å›ç”±è¿™äº›åŒ¹é…ç»“æœï¼ˆmatch objectï¼‰ç»„æˆçš„è¿­ä»£å™¨ å¯é€‰å‚æ•° pos å’Œ endpos å’Œä¸Šé¢ä¸€æ ·ã€‚ 1234567891011121314151617pattern = re.compile(r&quot;\\d+&quot;)p = pattern.finditer(&quot;abc1def2rst3xyz&quot;)for i in p: print(i)&lt;_sre.SRE_Match object; span=(3, 4), match='1'&gt;&lt;_sre.SRE_Match object; span=(7, 8), match='2'&gt;&lt;_sre.SRE_Match object; span=(11, 12), match='3'&gt; match object è·å–ç»“æœåœ¨ä¸Šé¢è®²åˆ°ï¼Œé€šè¿‡ pattern object çš„æ–¹æ³•ï¼ˆé™¤ findall å¤–ï¼‰è¿›è¡ŒåŒ¹é…å¾—åˆ°çš„è¿”å›ç»“æœéƒ½æ˜¯ match objectã€‚æ¯ä¸€ä¸ª match object éƒ½åŒ…å«äº†åŒ¹é…åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼Œæ¯”å¦‚ï¼Œèµ·å§‹ä½ç½®ã€åŒ¹é…åˆ°çš„å­ä¸²ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•ä» match object ä¸­æå–è¿™äº›ä¿¡æ¯å‘¢ï¼Ÿ match.group([group1, ...])ï¼š è¿”å› match object ä¸­çš„å­—ç¬¦ä¸²ã€‚ æ¯ä¸€ä¸ª ( ) éƒ½æ˜¯ä¸€ä¸ªåˆ†ç»„ï¼Œåˆ†ç»„ç¼–å·ä»1å¼€å§‹ï¼Œä»å·¦å¾€å³ï¼Œæ¯é‡åˆ°ä¸€ä¸ªå·¦æ‹¬å·ï¼Œåˆ†ç»„ç¼–å·+1ã€‚ ç»„ 0 æ€»æ˜¯å­˜åœ¨çš„ï¼Œå®ƒå°±æ˜¯æ•´ä¸ªè¡¨è¾¾å¼ æ²¡æœ‰å‚æ•°æ—¶ï¼Œgroup1é»˜è®¤ä¸º0ï¼Œè¿™æ—¶è¿”å›æ•´ä¸ªåŒ¹é…åˆ°çš„å­—ç¬¦ä¸²ã€‚ æŒ‡å®šä¸€ä¸ªå‚æ•°ï¼ˆæ•´æ•°ï¼‰æ—¶ï¼Œè¿”å›è¯¥åˆ†ç»„åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²ã€‚ æŒ‡å®šå¤šä¸ªå‚æ•°æ—¶ï¼Œè¿”å›ç”±é‚£å‡ ä¸ªåˆ†ç»„åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²ç»„æˆçš„ tupleã€‚ 1234567891011121314151617181920212223242526272829pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;) # \\w åŒ¹é…ä»»æ„å­—æ¯ï¼Œæ•°å­—ï¼Œä¸‹åˆ’çº¿m = pattern.match(&quot;He _ Kobe Bryant, Lakers player&quot;)print(m)print(m.group())print(m.group(1))print(m.group(2))print(m.group(1,2))&lt;_sre.SRE_Match object; span=(0, 4), match='He _'&gt;He _He_('He', '_') match.groups() è¿”å›ç”±æ‰€æœ‰åˆ†ç»„åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²ç»„æˆçš„ tupleã€‚ 1234567891011m = re.match(r&quot;(\\d+)\\.(\\d+)&quot;, '24.163')m.groups()('24', '163') match.start([group]) æ²¡æœ‰å‚æ•°æ—¶ï¼Œè¿”å›åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²çš„èµ·å§‹ä½ç½®ã€‚ æŒ‡å®šå‚æ•°ï¼ˆæ•´æ•°ï¼‰æ—¶ï¼Œè¿”å›è¯¥åˆ†ç»„åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²çš„èµ·å§‹ä½ç½®ã€‚ 123456789pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m.start()) # 0print(m.start(2)) # 5 match.end([group])ï¼š æ²¡æœ‰å‚æ•°æ—¶ï¼Œè¿”å›åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²çš„ç»“æŸä½ç½®ã€‚ æŒ‡å®šå‚æ•°ï¼ˆæ•´æ•°ï¼‰æ—¶ï¼Œè¿”å›è¯¥åˆ†ç»„åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²çš„ç»“æŸä½ç½®ã€‚ 123456789pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m.end()) # 11print(m.end(1)) # 4 match.span([group])ï¼š è¿”å›ä¸€ä¸ªäºŒå…ƒ tuple è¡¨ç¤ºåŒ¹é…åˆ°çš„å­—ç¬¦ä¸²çš„èŒƒå›´ï¼Œå³ (start, end)ã€‚ æŒ‡å®šå‚æ•°æ—¶ï¼Œè¿”å›è¯¥åˆ†ç»„åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²çš„ (start, end)ã€‚ 123456789pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m.span()) # (0, 11)print(m.span(2)) # (5, 11) æ¨¡å—çº§åˆ«çš„å‡½æ•°ä¸Šé¢è®²åˆ°çš„å‡½æ•°éƒ½æ˜¯å¯¹è±¡çš„æ–¹æ³•ï¼Œè¦ä½¿ç”¨å®ƒä»¬å¿…é¡»å…ˆå¾—åˆ°ç›¸åº”çš„å¯¹è±¡ã€‚æœ¬èŠ‚å°†ä»‹ç»ä¸€äº›Module-Level Functionsï¼Œæ¯”å¦‚ match()ï¼Œsearch()ï¼Œfindall() ç­‰ç­‰ã€‚ä½ ä¸éœ€è¦åˆ›å»ºä¸€ä¸ª pattern object å°±å¯ä»¥ç›´æ¥è°ƒç”¨è¿™äº›å‡½æ•°ã€‚ re.match(pattern, string, flags=0) 1234567891011121314151617181920212223pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m)# ç›¸å½“äºm = re.match(r&quot;(\\w+) (\\w+)&quot;,&quot;Kobe Bryant, Lakers&quot;)print(m)&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt;&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt; 1234567891011121314151617181920212223pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.search(&quot;Kobe Bryant, Lakers&quot;)print(m)# ç›¸å½“äºm = re.search(r&quot;(\\w+) (\\w+)&quot;,&quot;Kobe Bryant, Lakers&quot;)print(m)&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt;&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt; re.findall(pattern, string, flags=0):ä¸ä¸Šé¢ç±»ä¼¼ã€‚ re.finditer(pattern, string, flags=0):ä¸ä¸Šé¢ç±»ä¼¼ ç¼–è¯‘æ ‡å¿—ï¼ˆåŒ¹é…æ¨¡å¼ï¼‰ re.IGNORECASEï¼šå¿½ç•¥å¤§å°å†™ï¼ŒåŒ re.Iã€‚ re.MULTILINEï¼šå¤šè¡Œæ¨¡å¼ï¼Œæ”¹å˜^å’Œ$çš„è¡Œä¸ºï¼ŒåŒ re.Mã€‚ re.DOTALLï¼šç‚¹ä»»æ„åŒ¹é…æ¨¡å¼ï¼Œè®©â€™.â€™å¯ä»¥åŒ¹é…åŒ…æ‹¬â€™\\nâ€™åœ¨å†…çš„ä»»æ„å­—ç¬¦ï¼ŒåŒ re.Sã€‚ re.LOCALEï¼šä½¿é¢„å®šå­—ç¬¦ç±» \\w \\W \\b \\B \\s \\S å–å†³äºå½“å‰åŒºåŸŸè®¾å®šï¼Œ åŒ re.Lã€‚ re.ASCIIï¼šä½¿ \\w \\W \\b \\B \\s \\S åªåŒ¹é… ASCII å­—ç¬¦ï¼Œè€Œä¸æ˜¯ Unicode å­—ç¬¦ï¼ŒåŒ re.Aã€‚ re.VERBOSEï¼šè¯¦ç»†æ¨¡å¼ã€‚è¿™ä¸ªæ¨¡å¼ä¸‹æ­£åˆ™è¡¨è¾¾å¼å¯ä»¥æ˜¯å¤šè¡Œï¼Œå¿½ç•¥ç©ºç™½å­—ç¬¦ï¼Œå¹¶å¯ä»¥åŠ å…¥æ³¨é‡Šã€‚ä¸»è¦æ˜¯ä¸ºäº†è®©æ­£åˆ™è¡¨è¾¾å¼æ›´æ˜“è¯»ï¼ŒåŒ re.Xã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹ä¸¤ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ˜¯ç­‰ä»·çš„ï¼š 12345678910111213a = re.compile(r&quot;\\d + \\. \\d *#re.X&quot;) the integral partb = re.compile(r&quot;\\d+\\.\\d*&quot;)print(b.match(&quot;123.45&quot;))&lt;_sre.SRE_Match object; span=(0, 6), match='123.45'&gt; ä¿®æ”¹å­—ç¬¦ä¸²ç¬¬äºŒéƒ¨åˆ†è®²çš„æ˜¯å­—ç¬¦ä¸²çš„åŒ¹é…å’Œæœç´¢ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰æ”¹å˜å­—ç¬¦ä¸²ã€‚ä¸‹é¢å°±è®²ä¸€ä¸‹å¯ä»¥æ”¹å˜å­—ç¬¦ä¸²çš„æ“ä½œã€‚ åˆ†å‰²å­—ç¬¦ä¸²split()å‡½æ•°åœ¨åŒ¹é…çš„åœ°æ–¹å°†å­—ç¬¦ä¸²åˆ†å‰²ï¼Œå¹¶è¿”å›ä¸€ä¸ª listã€‚åŒæ ·çš„ï¼Œre æ¨¡å—æä¾›äº†ä¸¤ç§ split å‡½æ•°ï¼Œä¸€ä¸ªæ˜¯ pattern object çš„æ–¹æ³•ï¼Œä¸€ä¸ªæ˜¯æ¨¡å—çº§çš„å‡½æ•°ã€‚ regex.split(string, maxsplit=0)ï¼š maxsplitç”¨äºæŒ‡å®šæœ€å¤§åˆ†å‰²æ¬¡æ•°ï¼Œä¸æŒ‡å®šå°†å…¨éƒ¨åˆ†å‰²ã€‚ 1234567891011pattern = re.compile(r&quot;[A-Z]+&quot;)m = pattern.split(&quot;abcDefgHijkLmnoPqrs&quot;)print(m)['abc', 'efg', 'ijk', 'mno', 'qrs'] re.split(pattern, string, maxsplit=0, flags=0)ï¼š æ¨¡å—çº§å‡½æ•°ï¼ŒåŠŸèƒ½ä¸ regex.split() ç›¸åŒã€‚ flagsç”¨äºæŒ‡å®šåŒ¹é…æ¨¡å¼ã€‚ 1234567891011m = re.split(r&quot;[A-Z]+&quot;,&quot;abcDefgHijkLmnoPqrs&quot;)print(m)# è¾“å‡ºç»“æœï¼š['abc', 'efg', 'ijk', 'mno', 'qrs'] æœç´¢ä¸æ›¿æ¢å¦ä¸€ä¸ªå¸¸ç”¨çš„åŠŸèƒ½æ˜¯æ‰¾åˆ°æ‰€æœ‰çš„åŒ¹é…ï¼Œå¹¶æŠŠå®ƒä»¬ç”¨ä¸åŒçš„å­—ç¬¦ä¸²æ›¿æ¢ã€‚re æ¨¡å—æä¾›äº†sub()å’Œsubn()æ¥å®ç°æ›¿æ¢çš„åŠŸèƒ½ï¼Œè€Œå®ƒä»¬ä¹Ÿåˆ†åˆ«æœ‰è‡ªå·±ä¸¤ä¸ªä¸åŒç‰ˆæœ¬çš„å‡½æ•°ã€‚ regex.sub(repl, string, count=0)ï¼š ä½¿ç”¨ repl æ›¿æ¢ string ä¸­æ¯ä¸€ä¸ªåŒ¹é…çš„å­ä¸²ï¼Œè¿”å›æ›¿æ¢åçš„å­—ç¬¦ä¸²ã€‚è‹¥æ‰¾ä¸åˆ°åŒ¹é…ï¼Œåˆ™è¿”å›åŸå­—ç¬¦ä¸²ã€‚ repl å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°ã€‚ å½“replæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æ—¶ï¼Œä»»ä½•åœ¨å…¶ä¸­çš„åæ–œæ éƒ½ä¼šè¢«å¤„ç†ã€‚ å½“replæ˜¯ä¸€ä¸ªå‡½æ•°æ—¶ï¼Œè¿™ä¸ªå‡½æ•°åº”å½“åªæ¥å—ä¸€ä¸ªå‚æ•°ï¼ˆpatternå¯¹è±¡ï¼‰ï¼Œå¯¹åŒ¹é…åˆ°çš„å¯¹è±¡è¿›è¡Œå¤„ç†ï¼Œç„¶åè¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ç”¨äºæ›¿æ¢ã€‚ count ç”¨äºæŒ‡å®šæœ€å¤šæ›¿æ¢æ¬¡æ•°ï¼Œä¸æŒ‡å®šæ—¶å…¨éƒ¨æ›¿æ¢ã€‚ 12345678910111213141516171819pattern = re.compile(r&quot;like&quot;, re.I)s1 = pattern.sub(r&quot;love&quot;, &quot;I like you, do you like me?&quot;)s2 = pattern.sub(lambda m:m.group().upper(), &quot;I like you, do you like me?&quot;) # repl æ˜¯å‡½æ•°ï¼Œå…¶å‚æ•°æ˜¯ patternprint(s1)print(s2)I love you, do you love me?I LIKE you, do you LIKE me? re.sub(pattern, repl, string, count=0, flags=0)ï¼š æ¨¡å—çº§å‡½æ•°ï¼Œä¸ regex.sub() å‡½æ•°åŠŸèƒ½ç›¸åŒã€‚ flags ç”¨äºæŒ‡å®šåŒ¹é…æ¨¡å¼ã€‚ 123456789s1 = re.sub(r&quot;(\\w)'s\\b&quot;, r&quot;\\1 is&quot;, &quot;She's Xie Pan&quot;)print(s1)She is Xie Pan regex.subn(repl, string, count=0) åŒ sub()ï¼Œåªä¸è¿‡è¿”å›å€¼æ˜¯ä¸€ä¸ªäºŒå…ƒ tupleï¼Œå³(subå‡½æ•°è¿”å›å€¼, æ›¿æ¢æ¬¡æ•°)ã€‚ 1234567891011121314151617pattern = re.compile(r&quot;like&quot;, re.I)s1 = pattern.subn(r&quot;love&quot;, &quot;I like you, do you like me?&quot;)s2 = pattern.subn(lambda m:m.group().upper(), &quot;I like you, do you like me?&quot;) # repl æ˜¯å‡½æ•°ï¼Œå…¶å‚æ•°æ˜¯ patternprint(s1)print(s2)('I love you, do you love me?', 2)('I LIKE you, do you LIKE me?', 2) re.subn(pattern, repl, string, count=0, flags=0)ï¼š åŒä¸Š è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¸‰ï¼šæ‹¼å†™æ£€æŸ¥ç”±äºè‹±æ–‡æ–‡æœ¬ä¸­å¯èƒ½æœ‰æ‹¼å†™é”™è¯¯ï¼Œå› æ­¤ä¸€èˆ¬éœ€è¦è¿›è¡Œæ‹¼å†™æ£€æŸ¥ã€‚å¦‚æœç¡®ä¿¡æˆ‘ä»¬åˆ†æçš„æ–‡æœ¬æ²¡æœ‰æ‹¼å†™é—®é¢˜ï¼Œå¯ä»¥ç•¥å»æ­¤æ­¥ã€‚ æ‹¼å†™æ£€æŸ¥ï¼Œæˆ‘ä»¬ä¸€èˆ¬ç”¨pyenchantç±»åº“å®Œæˆã€‚pyenchantçš„å®‰è£…å¾ˆç®€å•ï¼šâ€pip install pyenchantâ€å³å¯ã€‚ å¯¹äºä¸€æ®µæ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„æ–¹å¼å»æ‰¾å‡ºæ‹¼å†™é”™è¯¯ï¼š 1234567# å‘ç°è¿™æ ·å®‰è£…å¹¶ä¸æ˜¯åœ¨è™šæ‹Ÿç¯å¢ƒä¸‹ï¼Œéœ€è¦å»ç»ˆç«¯å¯¹åº”çš„è™šæ‹Ÿç¯å¢ƒä¸‹å®‰è£…# source avtivate NLP!pip install pyenchant /bin/sh: 1: source: not found Requirement already satisfied: pyenchant in /home/panxie/anaconda3/lib/python3.6/site-packages (2.0.0) \u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m \u001b[33mYou are using pip version 10.0.1, however version 18.0 is available. You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m 1234567891011121314151617from enchant.checker import SpellCheckerchkr = SpellChecker('en_US')chkr.set_text(&quot;Many peopel like too watch In the Name of people&quot;)for err in chkr: print(&quot;ERROR:&quot;, err.word)ERROR: peopel å‘ç°åªèƒ½æ‰¾å•è¯æ‹¼å†™é”™è¯¯çš„ï¼Œä½† too è¿™æ ·çš„æ˜¯æ²¡åŠæ³•æ‰¾å‡ºçš„ã€‚æ‰¾å‡ºé”™è¯¯åï¼Œæˆ‘ä»¬å¯ä»¥è‡ªå·±æ¥å†³å®šæ˜¯å¦è¦æ”¹æ­£ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨pyenchantä¸­çš„wxSpellCheckerDialogç±»æ¥ç”¨å¯¹è¯æ¡†çš„å½¢å¼æ¥äº¤äº’å†³å®šæ˜¯å¿½ç•¥ï¼Œæ”¹æ­£è¿˜æ˜¯å…¨éƒ¨æ”¹æ­£æ–‡æœ¬ä¸­çš„é”™è¯¯æ‹¼å†™ã€‚ æ›´å¤šæ“ä½œå¯å‚è€ƒï¼š https://www.jianshu.com/p/96c01666aeeb https://pythonhosted.org/pyenchant/tutorial.html è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†å››ï¼šè¯å¹²æå–(stemming)å’Œè¯å½¢è¿˜åŸ(lemmatization)è¯å¹²æå–(stemming)å’Œè¯å‹è¿˜åŸ(lemmatization)æ˜¯è‹±æ–‡æ–‡æœ¬é¢„å¤„ç†çš„ç‰¹è‰²ã€‚ä¸¤è€…å…¶å®æœ‰å…±åŒç‚¹ï¼Œå³éƒ½æ˜¯è¦æ‰¾åˆ°è¯çš„åŸå§‹å½¢å¼ã€‚åªä¸è¿‡è¯å¹²æå–(stemming)ä¼šæ›´åŠ æ¿€è¿›ä¸€ç‚¹ï¼Œå®ƒåœ¨å¯»æ‰¾è¯å¹²çš„æ—¶å€™å¯ä»¥ä¼šå¾—åˆ°ä¸æ˜¯è¯çš„è¯å¹²ã€‚æ¯”å¦‚â€imagingâ€çš„è¯å¹²å¯èƒ½å¾—åˆ°çš„æ˜¯â€imagâ€, å¹¶ä¸æ˜¯ä¸€ä¸ªè¯ã€‚è€Œè¯å½¢è¿˜åŸåˆ™ä¿å®ˆä¸€äº›ï¼Œå®ƒä¸€èˆ¬åªå¯¹èƒ½å¤Ÿè¿˜åŸæˆä¸€ä¸ªæ­£ç¡®çš„è¯çš„è¯è¿›è¡Œå¤„ç†ã€‚ä¸ªäººæ¯”è¾ƒå–œæ¬¢ä½¿ç”¨è¯å‹è¿˜åŸè€Œä¸æ˜¯è¯å¹²æå–ã€‚ åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸€èˆ¬ä½¿ç”¨nltkæ¥è¿›è¡Œè¯å¹²æå–å’Œè¯å‹è¿˜åŸã€‚å®‰è£…nltkä¹Ÿå¾ˆç®€å•ï¼Œâ€pip install nltkâ€å³å¯ã€‚åªä¸è¿‡æˆ‘ä»¬ä¸€èˆ¬éœ€è¦ä¸‹è½½nltkçš„è¯­æ–™åº“ï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„ä»£ç å®Œæˆï¼Œnltkä¼šå¼¹å‡ºå¯¹è¯æ¡†é€‰æ‹©è¦ä¸‹è½½çš„å†…å®¹ã€‚é€‰æ‹©ä¸‹è½½è¯­æ–™åº“å°±å¯ä»¥äº†ã€‚ 1234567891011121314151617import nltknltk.download('wordnet')[nltk_data] Downloading package wordnet to /home/panxie/nltk_data...[nltk_data] Unzipping corpora/wordnet.zip.True åœ¨nltkä¸­ï¼Œåšè¯å¹²æå–çš„æ–¹æ³•æœ‰PorterStemmerï¼ŒLancasterStemmerå’ŒSnowballStemmerã€‚ä¸ªäººæ¨èä½¿ç”¨SnowballStemmerã€‚è¿™ä¸ªç±»å¯ä»¥å¤„ç†å¾ˆå¤šç§è¯­è¨€ï¼Œå½“ç„¶ï¼Œé™¤äº†ä¸­æ–‡ã€‚ 1234567891011from nltk.stem import SnowballStemmerstemmer = SnowballStemmer(&quot;english&quot;)stemmer.stem(&quot;countries&quot;)'countri' è¾“å‡ºæ˜¯â€countriâ€,è¿™ä¸ªè¯å¹²å¹¶ä¸æ˜¯ä¸€ä¸ªè¯ã€‚ è€Œå¦‚æœæ˜¯åšè¯å‹è¿˜åŸï¼Œåˆ™ä¸€èˆ¬å¯ä»¥ä½¿ç”¨WordNetLemmatizerç±»ï¼Œå³wordnetè¯å½¢è¿˜åŸæ–¹æ³•ã€‚ 12345678910111213from nltk.stem import WordNetLemmatizerwnl = WordNetLemmatizer()print(wnl.lemmatize('countries'))country è¾“å‡ºæ˜¯â€countryâ€,æ¯”è¾ƒç¬¦åˆéœ€æ±‚ã€‚ åœ¨å®é™…çš„è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†çš„æ—¶å€™ï¼Œå»ºè®®ä½¿ç”¨åŸºäºwordnetçš„è¯å½¢è¿˜åŸå°±å¯ä»¥äº†ã€‚ åœ¨è¿™é‡Œæœ‰ä¸ªè¯å¹²æå–å’Œè¯å‹è¿˜åŸçš„demoï¼Œå¦‚æœæ˜¯è¿™å—çš„æ–°æ‰‹å¯ä»¥å»çœ‹çœ‹ï¼Œä¸Šæ‰‹å¾ˆåˆé€‚ã€‚ è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†äº”ï¼šè½¬åŒ–ä¸ºå°å†™123456789text = 'XiePan'print(text.lower())xiepan è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†å…­ï¼šå¼•å…¥åœç”¨è¯åœ¨è‹±æ–‡æ–‡æœ¬ä¸­æœ‰å¾ˆå¤šæ— æ•ˆçš„è¯ï¼Œæ¯”å¦‚â€œaâ€ï¼Œâ€œtoâ€ï¼Œä¸€äº›çŸ­è¯ï¼Œè¿˜æœ‰ä¸€äº›æ ‡ç‚¹ç¬¦å·ï¼Œè¿™äº›æˆ‘ä»¬ä¸æƒ³åœ¨æ–‡æœ¬åˆ†æçš„æ—¶å€™å¼•å…¥ï¼Œå› æ­¤éœ€è¦å»æ‰ï¼Œè¿™äº›è¯å°±æ˜¯åœç”¨è¯ã€‚ä¸ªäººå¸¸ç”¨çš„è‹±æ–‡åœç”¨è¯è¡¨ä¸‹è½½åœ°å€åœ¨è¿™ã€‚å½“ç„¶ä¹Ÿæœ‰å…¶ä»–ç‰ˆæœ¬çš„åœç”¨è¯è¡¨ï¼Œä¸è¿‡è¿™ä¸ªç‰ˆæœ¬æ˜¯æˆ‘å¸¸ç”¨çš„ã€‚ åœ¨æˆ‘ä»¬ç”¨scikit-learnåšç‰¹å¾å¤„ç†çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡å‚æ•°stop_wordsæ¥å¼•å…¥ä¸€ä¸ªæ•°ç»„ä½œä¸ºåœç”¨è¯è¡¨ã€‚è¿™ä¸ªæ–¹æ³•å’Œå‰æ–‡è®²ä¸­æ–‡åœç”¨è¯çš„æ–¹æ³•ç›¸åŒï¼Œè¿™é‡Œå°±ä¸å†™å‡ºä»£ç ï¼Œå¤§å®¶å‚è€ƒå‰æ–‡å³å¯ã€‚ 123456789101112131415from nltk import word_tokenizefrom nltk.corpus import stopwordsstop = set(stopwords.words('english')) # åœç”¨è¯stop.add(&quot;foo&quot;) # å¢åŠ ä¸€ä¸ªè¯stop.remove(&quot;is&quot;) # å»æ‰ä¸€ä¸ªè¯sentence = &quot;this is a foo bar sentence&quot;[i for i in word_tokenize(sentence.lower()) if i not in stop] ['is', 'bar', 'sentence'] è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¸ƒï¼šç‰¹å¾å¤„ç†ç°åœ¨æˆ‘ä»¬å°±å¯ä»¥ç”¨scikit-learnæ¥å¯¹æˆ‘ä»¬çš„æ–‡æœ¬ç‰¹å¾è¿›è¡Œå¤„ç†äº†ï¼Œåœ¨æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¹‹å‘é‡åŒ–ä¸Hash Trickä¸­ï¼Œæˆ‘ä»¬è®²åˆ°äº†ä¸¤ç§ç‰¹å¾å¤„ç†çš„æ–¹æ³•ï¼Œå‘é‡åŒ–ä¸Hash Trickã€‚è€Œå‘é‡åŒ–æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯ä»¥æ¥ç€è¿›è¡ŒTF-IDFçš„ç‰¹å¾å¤„ç†ã€‚åœ¨æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¹‹TF-IDFä¸­ï¼Œæˆ‘ä»¬ä¹Ÿè®²åˆ°äº†TF-IDFç‰¹å¾å¤„ç†çš„æ–¹æ³•ã€‚ TfidfVectorizerç±»å¯ä»¥å¸®åŠ©æˆ‘ä»¬å®Œæˆå‘é‡åŒ–ï¼ŒTF-IDFå’Œæ ‡å‡†åŒ–ä¸‰æ­¥ã€‚å½“ç„¶ï¼Œè¿˜å¯ä»¥å¸®æˆ‘ä»¬å¤„ç†åœç”¨è¯ã€‚è¿™éƒ¨åˆ†å·¥ä½œå’Œä¸­æ–‡çš„ç‰¹å¾å¤„ç†ä¹Ÿæ˜¯å®Œå…¨ç›¸åŒçš„ï¼Œå¤§å®¶å‚è€ƒå‰æ–‡å³å¯ã€‚ è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†å…«ï¼šå»ºç«‹åˆ†ææ¨¡å‹æœ‰äº†æ¯æ®µæ–‡æœ¬çš„TF-IDFçš„ç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨è¿™äº›æ•°æ®å»ºç«‹åˆ†ç±»æ¨¡å‹ï¼Œæˆ–è€…èšç±»æ¨¡å‹äº†ï¼Œæˆ–è€…è¿›è¡Œä¸»é¢˜æ¨¡å‹çš„åˆ†æã€‚æ­¤æ—¶çš„åˆ†ç±»èšç±»æ¨¡å‹å’Œä¹‹å‰è®²çš„éè‡ªç„¶è¯­è¨€å¤„ç†çš„æ•°æ®åˆ†ææ²¡æœ‰ä»€ä¹ˆä¸¤æ ·ã€‚å› æ­¤å¯¹åº”çš„ç®—æ³•éƒ½å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚è€Œä¸»é¢˜æ¨¡å‹æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æ¯”è¾ƒç‰¹æ®Šçš„ä¸€å—ï¼Œè¿™ä¸ªæˆ‘ä»¬åé¢å†å•ç‹¬è®²ã€‚ è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†æ€»ç»“ä¸Šé¢æˆ‘ä»¬å¯¹è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†çš„è¿‡ç¨‹åšäº†ä¸€ä¸ªæ€»ç»“ï¼Œå¸Œæœ›å¯ä»¥å¸®åŠ©åˆ°å¤§å®¶ã€‚éœ€è¦æ³¨æ„çš„æ˜¯è¿™ä¸ªæµç¨‹ä¸»è¦é’ˆå¯¹ä¸€äº›å¸¸ç”¨çš„æ–‡æœ¬æŒ–æ˜ï¼Œå¹¶ä½¿ç”¨äº†è¯è¢‹æ¨¡å‹ï¼Œå¯¹äºæŸä¸€äº›è‡ªç„¶è¯­è¨€å¤„ç†çš„éœ€æ±‚åˆ™æµç¨‹éœ€è¦ä¿®æ”¹ã€‚æ¯”å¦‚æœ‰æ—¶å€™éœ€è¦åšè¯æ€§æ ‡æ³¨ï¼Œè€Œæœ‰æ—¶å€™æˆ‘ä»¬ä¹Ÿéœ€è¦è‹±æ–‡åˆ†è¯ï¼Œæ¯”å¦‚å¾—åˆ°â€New Yorkâ€è€Œä¸æ˜¯â€œNewâ€å’Œâ€œYorkâ€ï¼Œå› æ­¤è¿™ä¸ªæµç¨‹ä»…ä¾›è‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨è€…å‚è€ƒï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®æˆ‘ä»¬çš„æ•°æ®åˆ†æç›®çš„é€‰æ‹©åˆé€‚çš„é¢„å¤„ç†æ–¹æ³•ã€‚","link":"/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%8B%B1%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"},{"title":"æ·±åº¦å­¦ä¹ -Batch Normalization","text":"Paper Reading paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Motivation Training Deep Neural Networks is complicated by the fact that the distribution of each layerâ€™s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift. ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­å‚æ•°ä¸æ–­æ”¹å˜å¯¼è‡´åç»­æ¯ä¸€å±‚è¾“å…¥çš„åˆ†å¸ƒä¹Ÿå‘ç”Ÿå˜åŒ–ï¼Œè€Œå­¦ä¹ çš„è¿‡ç¨‹åˆè¦ä½¿æ¯ä¸€å±‚é€‚åº”è¾“å…¥çš„åˆ†å¸ƒï¼Œè¿™ä½¿å¾—ä¸å¾—ä¸é™ä½å­¦ä¹ ç‡ã€å°å¿ƒåœ°åˆå§‹åŒ–ï¼Œå¹¶ä¸”ä½¿å¾—é‚£äº›å…·æœ‰æ˜“é¥±å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°çš„ç½‘ç»œè®­ç»ƒè‡­åæ˜­è‘—ã€‚ä½œè€…å°†åˆ†å¸ƒå‘ç”Ÿå˜åŒ–ç§°ä¹‹ä¸º internal covariate shiftã€‚ stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate and the initial parameter values. The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers â€“ so that small changes to the network parameters amplify as the network becomes deeper. åœ¨æ·±åº¦å­¦ä¹ ä¸­æˆ‘ä»¬é‡‡ç”¨SGDå–å¾—äº†éå¸¸å¥½çš„æ•ˆæœï¼ŒSGDç®€å•æœ‰æ•ˆï¼Œä½†æ˜¯å®ƒå¯¹è¶…å‚æ•°éå¸¸æ•æ„Ÿï¼Œå°¤å…¶æ˜¯å­¦ä¹ ç‡å’Œåˆå§‹åŒ–å‚æ•°ã€‚ The change in the distributions of layersâ€™ inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience covariate shift (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. å› ä¸ºå­¦ä¹ çš„è¿‡ç¨‹ä¸­æ¯ä¸€å±‚éœ€è¦å»è¿ç»­çš„é€‚åº”æ¯ä¸€å±‚è¾“å…¥çš„åˆ†å¸ƒï¼Œæ‰€ä»¥è¾“å…¥åˆ†å¸ƒå‘ç”Ÿå˜åŒ–æ—¶ï¼Œä¼šäº§ç”Ÿä¸€äº›é—®é¢˜ã€‚è¿™é‡Œä½œè€…å¼•ç”¨äº† covariate shift å’Œ domain adaptation è¿™ä¸¤ä¸ªæ¦‚å¿µã€‚ Therefore, the input distribution properties that aid the network generalization â€“ such as having the same distribution between the training and test data â€“ apply to training the sub-network as well.As such it is advantageous for the distribution of x to remain fixed over time. æœ‰åŠ©äºç½‘ç»œæ³›åŒ–çš„è¾“å…¥åˆ†å¸ƒå±æ€§ï¼šä¾‹å¦‚åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¹‹é—´å…·æœ‰ç›¸åŒçš„åˆ†å¸ƒï¼Œä¹Ÿé€‚ç”¨äºè®­ç»ƒå­ç½‘ç»œ Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the subnetwork, as well. å›ºå®šè¾“å…¥åˆ†å¸ƒå¯¹è¯¥å­ç½‘ç»œå…¶ä»–éƒ¨åˆ†çš„ç½‘ç»œçš„è®­ç»ƒä¼šäº§ç”Ÿç§¯æçš„å½±å“ã€‚ æ€»ç»“ä¸‹ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ BNï¼š åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œå› ä¸ºå‰ä¸€å±‚çš„å‚æ•°æ”¹å˜ï¼Œå°†ä¼šå¯¼è‡´åä¸€å±‚çš„è¾“å…¥çš„åˆ†å¸ƒä¸æ–­åœ°å‘ç”Ÿæ”¹å˜ï¼Œè¿™å°±éœ€è¦é™ä½å­¦ä¹ é€Ÿç‡åŒæ—¶è¦æ³¨æ„å‚æ•°çš„åˆå§‹åŒ–ï¼Œä¹Ÿä½¿å…·æœ‰é¥±å’Œéçº¿æ€§ï¼ˆsaturating nonlinearityï¼‰ç»“æ„çš„æ¨¡å‹éå¸¸éš¾è®­ç»ƒï¼ˆæ‰€è°“çš„é¥±å’Œå°±æ˜¯æŒ‡å‡½æ•°çš„å€¼åŸŸæ˜¯ä¸ªæœ‰é™å€¼ï¼Œå³å½“å‡½æ•°è‡ªå˜é‡è¶‹å‘æ— ç©·æ—¶ï¼Œå‡½æ•°å€¼ä¸è¶‹å‘æ— ç©·ï¼‰ã€‚æ·±åº¦ç¥ç»ç½‘ç»œä¹‹æ‰€ä»¥å¤æ‚æ˜¯å› ä¸ºå®ƒæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½ä¼šå—åˆ°ä¹‹å‰å±‚çš„å½±å“ï¼Œå› æ­¤ä¸€ä¸ªå°å°çš„å‚æ•°æ”¹å˜éƒ½ä¼šå¯¹ç½‘ç»œäº§ç”Ÿå·¨å¤§çš„æ”¹å˜ã€‚ä½œè€…å°†è¿™ç§ç°è±¡ç§°ä¸ºinternal covariate shiftï¼Œæå‡ºäº†å¯¹æ¯ä¸ªè¾“å…¥å±‚è¿›è¡Œè§„èŒƒåŒ–æ¥è§£å†³ã€‚åœ¨æ–‡ä¸­ï¼Œä½œè€…æåˆ°ä½¿ç”¨BNå¯ä»¥åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ä½¿ç”¨è¾ƒé«˜çš„å­¦ä¹ é€Ÿç‡ï¼Œå¯ä»¥æ¯”è¾ƒéšæ„çš„å¯¹å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼ŒåŒæ—¶BNä¹Ÿèµ·åˆ°äº†ä¸€ç§æ­£åˆ™åŒ–çš„ä½œç”¨ï¼Œåœ¨æŸç§ç¨‹åº¦ä¸Šå¯ä»¥å–ä»£dropoutçš„ä½œç”¨ã€‚ è€ƒè™‘ä¸€ä¸ªä»¥sigmoidä¸ºæ¿€æ´»å‡½æ•°çš„ç¥ç»å±‚ï¼š $z=g(Wu+b)$ å…¶ä¸­ u æ˜¯è¾“å…¥ï¼Œ g æ˜¯ sigmoid æ¿€æ´»å‡½æ•° $g(x)=\\dfrac{1}{1+exp(x)}$ï¼Œå½“ |x| å¢åŠ æ—¶ï¼Œ$gâ€™(x)$ è¶‹è¿‘äº0, è¿™æ„å‘³ç€ $x=Wu+b$ çš„æ‰€æœ‰ç»´åº¦ï¼Œé™¤äº†ç»å¯¹å€¼è¾ƒå°çš„ç»´åº¦ï¼Œå…¶ä»–çš„æµå‘è¾“å…¥ u çš„æ¢¯åº¦éƒ½ä¼šæ¶ˆå¤±,ä¹Ÿå°±æ˜¯è¿›å…¥éçº¿æ€§çš„é¥±å’ŒåŒºåŸŸï¼Œè¿™ä¼šé™ä½æ¨¡å‹è®­ç»ƒé€Ÿåº¦ã€‚ åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯¹äºéçº¿æ€§é¥±å’Œçš„æƒ…å†µï¼Œå·²ç»æœ‰å¾ˆæœ‰å¯¹åº”ç­–ç•¥ï¼š ReLU åˆå§‹åŒ– Xavier initialization. ç”¨ä¸€ä¸ªè¾ƒå°çš„å­¦ä¹ é€Ÿç‡è¿›è¡Œå­¦ä¹  If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate. å¦‚æœä¿è¯éçº¿æ€§è¾“å…¥çš„åˆ†å¸ƒç¨³å®šï¼Œä¼˜åŒ–å™¨ä¹Ÿå°±ä¸ä¼šé™·äºé¥±å’ŒåŒºåŸŸäº†ï¼Œè®­ç»ƒä¹Ÿä¼šåŠ é€Ÿã€‚ We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as Internal Covariate Shift. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. ä½œè€…æŠŠè¿™ç§è¾“å…¥åˆ†å¸ƒçš„å˜åŒ–å«åšå†…éƒ¨åæ–¹å·®åç§»ã€‚å¹¶æå‡ºäº† Batch Normalization,é€šè¿‡å›ºå®šè¾“å…¥çš„å‡å€¼å’Œæ–¹å·®ã€‚ Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes. BN é™¤äº†èƒ½è§£å†³ internal covariate shift çš„é—®é¢˜ï¼Œè¿˜èƒ½å¤Ÿé™ä½æ¢¯åº¦å¯¹å­¦ä¹ ç‡ï¼Œåˆå§‹åŒ–å‚æ•°è®¾ç½®çš„ä¾èµ–ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼Œæ­£åˆ™åŒ–æ¨¡å‹ï¼Œé™ä½å¯¹ dropout çš„éœ€æ±‚ï¼Œæœ€åè¿˜ä¿è¯ç½‘ç»œèƒ½å¤Ÿä½¿ç”¨å…·æœ‰é¥±å’Œæ€§çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚ Towards Reducing Internal Covariate Shiftwhitening ç™½åŒ–æ“ä½œ It has been long known (LeCun et al., 1998b; Wiesler &amp; Ney, 2011) that the network training converges faster if its inputs are whitened â€“ i.e., linearly transformed to have zero means and unit variances, and decorrelated. ä½¿ç”¨ç™½åŒ– whitening æœ‰åŠ©äºæ¨¡å‹æ”¶æ•›ï¼Œç™½åŒ–æ˜¯çº¿æ€§å˜åŒ–ï¼Œè½¬åŒ–ä¸ºå‡å€¼ä¸º0,æ–¹å·®ä¸º1,å¹¶ä¸”å»ç›¸å…³æ€§ã€‚ However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step. å¦‚æœå°†ç™½åŒ–ä¸åŸºäºæ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–æ··åˆåœ¨ä¸€èµ·ï¼Œé‚£ä¹ˆåœ¨æ‰§è¡Œæ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä¸­ä¼šå—åˆ°æ ‡å‡†åŒ–çš„å‚æ•°æ›´æ–°çš„å½±å“ï¼Œè¿™æ ·ä¼šå‡å¼±ç”šè‡³æŠµæ¶ˆæ¢¯åº¦ä¸‹é™çš„äº§ç”Ÿçš„å½±å“ã€‚ ä½œè€…ä¸¾äº†è¿™æ ·ä¸€ä¸ªä¾‹å­ï¼š è€ƒè™‘ä¸€ä¸ªè¾“å…¥ u å’Œä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•° b ç›¸åŠ ä½œä¸ºä¸€ä¸ª layer. é€šè¿‡å‡å»å‡å€¼è¿›è¡Œæ ‡å‡†åŒ– $\\hat x=x-E[x]$, å…¶ä¸­ x=u+b. åˆ™å‰å‘ä¼ æ’­çš„è¿‡ç¨‹ï¼š $x=u+b \\rightarrow \\hat x = x-E[x] \\rightarrow loss$ åå‘ä¼ æ’­å¯¹å‚æ•° b æ±‚å¯¼ï¼ˆä¸è€ƒè™‘ b å’Œ E[x] çš„ç›¸å…³æ€§ï¼‰ï¼š $\\dfrac{\\partial l}{\\partial b}=\\dfrac{\\partial l}{\\partial \\hat x}\\dfrac{\\partial \\hat x}{\\partial b} = \\dfrac{\\partial l}{\\partial \\hat x}$ é‚£ä¹ˆ $\\Delta b = -\\dfrac{\\partial l}{\\partial \\hat x}$, åˆ™å¯¹äºå‚æ•° b çš„æ›´æ–°ï¼š $b \\leftarrow \\Delta b + b$. é‚£ä¹ˆç»è¿‡äº†æ ‡å‡†åŒ–ã€æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ä¹‹åï¼š $u+(b+\\Delta b)-E[u+(b+\\Delta b)]=u+b-E[u+b]$ è¿™æ„å‘³ç€è¿™ä¸ª layer çš„è¾“å‡ºæ²¡æœ‰å˜åŒ–ï¼ŒæŸå¤± $\\dfrac{\\partial l}{\\partial \\hat x}ä¹Ÿæ²¡æœ‰å˜åŒ–$, é‚£ä¹ˆéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œ**bä¼šæ— é™çš„å¢é•¿???**ï¼Œè€Œlossä¸å˜ã€‚ This problem can get worse if the normalization not only centers but also scales the activations. We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step. å¦‚æœè§„èŒƒåŒ–ä¸ä»…ä¸­å¿ƒå¤„ç†(å³å‡å»å‡å€¼)ï¼Œè€Œä¸”è¿˜å¯¹æ¿€æ´»å€¼è¿›è¡Œç¼©æ”¾ï¼Œé—®é¢˜ä¼šå˜å¾—æ›´ä¸¥é‡ã€‚é€šè¿‡å®éªŒå‘ç°ï¼Œ å½“å½’ä¸€åŒ–å‚æ•°åœ¨æ¢¯åº¦ä¸‹é™æ­¥éª¤ä¹‹å¤–è¿›è¡Œï¼Œæ¨¡å‹ä¼šçˆ†ç‚¸ã€‚ è¿›è¡Œç™½åŒ–æ“ä½œï¼Œå¹¶ä¸”åœ¨ä¼˜åŒ–æ—¶è€ƒè™‘æ ‡å‡†åŒ–çš„é—®é¢˜ The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. To address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution.Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters Î˜. ä¹‹æ‰€ä»¥ä¼šäº§ç”Ÿä»¥ä¸Šçš„é—®é¢˜ï¼Œä¸»è¦æ˜¯æ¢¯åº¦ä¼˜åŒ–çš„è¿‡ç¨‹ä¸­æ²¡æœ‰è€ƒè™‘åˆ°æ ‡å‡†åŒ–æ“ä½œçš„è¿›è¡Œ(ä¸å¥½å®ç°)ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºæˆ‘ä»¬éœ€è¦ä¿è¯ç½‘ç»œäº§ç”Ÿçš„æ¿€æ´»æ€»æ˜¯æœ‰ç›¸åŒçš„åˆ†å¸ƒã€‚è¿™æ ·åšå…è®¸æŸå¤±å€¼å…³äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦è€ƒè™‘åˆ°æ ‡å‡†åŒ–ã€‚ å†ä¸€æ¬¡è€ƒè™‘ x æ˜¯ä¸€ä¸ª layer çš„è¾“å…¥ï¼Œçœ‹ä½œä¸€ä¸ªå‘é‡ï¼Œ$\\chi$ æ˜¯æ•´ä¸ªè®­ç»ƒé›†ï¼Œåˆ™æ ‡å‡†åŒ–ï¼š $\\hat x = Norm(x, \\chi)$ è¿™æ—¶æ ‡å‡†åŒ–çš„å‚æ•°ä¸ä»…å–å†³äºå½“å‰çš„è¾“å…¥xï¼Œè¿˜å’Œæ•´ä¸ªè®­ç»ƒé›† $\\chi$ æœ‰å…³ï¼Œå½“xæ¥è‡ªå…¶å®ƒå±‚çš„è¾“å‡ºæ—¶ï¼Œé‚£ä¹ˆä¸Šå¼å°±ä¼šå’Œå‰é¢å±‚çš„ç½‘ç»œå‚æ•° $\\theta$ æœ‰å…³ï¼Œåå‘ä¼ æ’­æ—¶éœ€è¦è®¡ç®—: $$\\frac{\\partial{Norm(x,\\chi)}}{\\partial{x}}\\text{ and }\\frac{\\partial{Norm(x,\\chi)}}{\\partial{\\chi}}$$ å¦‚æœå¿½ç•¥ä¸Šè¾¹ç¬¬äºŒé¡¹å°±ä¼šå‡ºç°ä¹‹å‰è¯´åˆ°çš„é—®é¢˜ã€‚ä½†æ˜¯ç›´æ¥åœ¨è¿™ä¸€æ¶æ„ä¸‹è¿›è¡Œç™½è¯æ“ä½œå¾ˆéå¸¸çš„è´¹æ—¶ï¼Œä»£ä»·å¾ˆå¤§ã€‚ä¸»è¦æ˜¯éœ€è¦è®¡ç®—åæ–¹å·®çŸ©é˜µï¼Œè¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥åŠåå‘ä¼ æ’­æ—¶ä¹Ÿéœ€è¦è¿›è¡Œç›¸å…³çš„è®¡ç®—ã€‚å› æ­¤è¿™å°±éœ€è¦å¯»æ‰¾ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæ—¢å¯ä»¥è¾¾åˆ°ç±»ä¼¼çš„æ•ˆæœï¼Œåˆä¸éœ€è¦åœ¨æ¯ä¸ªå‚æ•°æ›´æ–°ååˆ†ææ•´ä¸ªè®­ç»ƒé›†ã€‚ Normalization via Mini-Batch Statisticså¯¹æ¯”äºç™½åŒ–çš„ä¸¤ä¸ªç®€åŒ– Since the full whitening of each layerâ€™s inputs is costly, we make two necessary simplifications. The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have zero mean and unit variance. æ—¢ç„¶ç™½åŒ–æ“ä½œè¿™ä¹ˆè´¹æ—¶è´¹åŠ›ï¼Œä½œè€…è€ƒè™‘ä¸¤ç‚¹å¿…è¦çš„ç®€åŒ–ã€‚ç¬¬ä¸€ç‚¹ï¼Œå¯¹è¾“å…¥ç‰¹å¾çš„æ¯ä¸€ç»´ $x=(x^{(1)},â€¦,x^{(d)})$ è¿›è¡Œå»å‡å€¼å’Œå•ä½æ–¹å·®çš„å¤„ç†ã€‚ $$\\hat x^{(k)} = \\dfrac{x^{(k)}-E[x^{(k)}]}{\\sqrt {Var[x^{(k)}]}}$$ where the expectation and variance are computed over the training data set. å…¶ä¸­å‡å€¼å’Œæ–¹å·®æ˜¯åŸºäºæ•´ä¸ªè®­ç»ƒé›†è®¡ç®—å¾—åˆ°çš„ã€‚ Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. ä½†æ˜¯å¦‚æœä»…æ˜¯ç®€å•çš„å¯¹æ¯ä¸€å±‚çš„è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–å¯èƒ½ä¼šå¯¹è¯¥å±‚çš„è¡¨è¾¾é€ æˆèƒ½åŠ›æ”¹å˜ã€‚æ¯”å¦‚å¯¹ä¸€ä¸ªsigmoidæ¿€æ´»å‡½æ•°çš„è¾“å…¥æ ‡å‡†åŒ–ä¼šå°†è¾“å…¥å›ºå®šåœ¨çº¿æ€§åŒºåŸŸã€‚ ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†è¿™æ ·çš„æ”¹å˜,å¼•å…¥ä¸€å¯¹å‚æ•° $\\gamma^{(k)}$, $\\beta^{(k)}$ æ¥å¯¹å½’ä¸€åŒ–ä¹‹åçš„å€¼è¿›è¡Œç¼©æ”¾å’Œå¹³ç§»ã€‚ $$y^{(k)} = \\gamma^{(k)}\\hat x^{(k)} + \\beta^{(k)}$$ $\\gamma^{(k)}$, $\\beta^{(k)}$ æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œç”¨æ¥å›å¤ç»è¿‡æ ‡å‡†åŒ–ä¹‹åçš„ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚å¦‚æœ $\\gamma^{(k)}=\\sqrt {Var[x^{(k)}]}$, $\\beta^{(k)}=E[x^{(k)}]$ In the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations. However, this is impractical when using stochastic optimization. Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation. åœ¨batchä¸­ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†çš„å‡å€¼å’Œæ–¹å·®æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œå› æ­¤ï¼Œä½œè€…æå‡ºäº† ç¬¬äºŒä¸ªç®€åŒ–ï¼Œç”¨ mini-batch æ¥ä¼°è®¡å‡å€¼å’Œæ–¹å·®ã€‚ Note that the use of mini-batches is enabled by computation of per-dimension variances rather than joint covariances; in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices. æ³¨æ„åˆ° mini-batches æ˜¯è®¡ç®—æ¯ä¸€ç»´çš„æ–¹å·®ï¼Œè€Œä¸æ˜¯è”åˆåæ–¹å·®ã€‚ä½¿ç”¨åæ–¹å·®å°±éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œmini-batches çš„å¤§å°å¾€å¾€å°äºéœ€è¦ç™½åŒ–çš„æ¿€æ´»å€¼çš„æ•°é‡,ä¼šå¾—åˆ° å¥‡å¼‚åæ–¹å·®çŸ©é˜µ(singular vorariance matrices)???. BN æ ¸å¿ƒæµç¨‹batch size m, æˆ‘ä»¬å…³æ³¨å…¶ä¸­æŸä¸€ä¸ªç»´åº¦ $x^{k}$, k è¡¨ç¤ºç¬¬kç»´ç‰¹å¾ã€‚é‚£ä¹ˆå¯¹äº batch ä¸­è¯¥ç»´ç‰¹å¾çš„ m ä¸ªå€¼ï¼š $$B={x_{1,â€¦,m}}$$ ç»è¿‡çº¿æ€§è½¬æ¢ï¼š $$BN_{\\gamma, \\beta}:x_{1,..,m}\\rightarrow y_{1,..,m}$$ å¯¹äºè¾“å…¥çš„ mini-batch çš„ä¸€ä¸ªç»´åº¦ï¼Œè®¡ç®—å‡å€¼å’Œæ–¹å·® æ ‡å‡†åŒ–ï¼ˆæ³¨æ„ epsilon é¿å…0é”™è¯¯ï¼‰ ä½¿ç”¨ä¸¤ä¸ªå‚æ•°è¿›è¡Œå¹³ç§»å’Œç¼©æ”¾ è¿™é‡Œæœ‰ç‚¹ç–‘æƒ‘ï¼šä¸ºä»€ä¹ˆåœ¨ç¬¬ä¸‰æ­¥å·²ç»å®Œæˆæ ‡å‡†åŒ–çš„æƒ…å†µä¸‹è¿˜è¦è¿›è¡Œ4æ“ä½œï¼Œåæ¥å‘ç°å…¶å®ä½œè€…åœ¨å‰æ–‡å·²ç»è¯´äº†ã€‚é¦–å…ˆ $\\hat x$ æ˜¯æ ‡å‡†åŒ–åçš„è¾“å‡ºï¼Œä½†æ˜¯å¦‚æœä»…ä»¥æ­¤ä¸ºè¾“å‡ºï¼Œå…¶è¾“å‡ºå°±è¢«é™å®šä¸ºäº†æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œè¿™æ ·å¾ˆå¯èƒ½ä¼šé™åˆ¶åŸå§‹ç½‘ç»œèƒ½è¡¨è¾¾çš„ä¿¡æ¯ï¼Œå‰æ–‡å·²ç”¨sigmoidå‡½æ•°è¿›è¡Œäº†ä¸¾ä¾‹è¯´æ˜ã€‚å› ä¸º $\\gamma, \\beta$ è¿™ä¸¤ä¸ªå‚æ•°æ˜¯å¯ä»¥å­¦ä¹ çš„ï¼Œæ‰€ä»¥çš„æ ‡å‡†åŒ–åçš„â€æ¢å¤â€ç¨‹åº¦å°†åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ç”±ç½‘ç»œè‡ªä¸»å†³å®šã€‚ åˆ©ç”¨é“¾å¼æ³•åˆ™ï¼Œæ±‚æŸå¤±å‡½æ•°å¯¹å‚æ•° $\\gamma, \\beta$ æ±‚å¯¼ï¼š Thus, BN transform is a differentiable transformation that introduces normalized activations into the network. This ensures that as the model is training, layers can continue learning on input distributions that exhibit less internal covariate shift, thus accelerating the training. BN æ˜¯å¯å¾®çš„ï¼Œä¿è¯æ¨¡å‹å¯è®­ç»ƒï¼Œç½‘ç»œå¯ä»¥å­¦ä¹ å¾—åˆ°è¾“å…¥çš„åˆ†å¸ƒï¼Œæ¥å‡å° internal covarite shift, ä»è€ŒåŠ é€Ÿè®­ç»ƒã€‚ Training and Inference with Batch-Normalized Networks The normalization of activations that depends on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterministically. For this, once the network has been trained, we use the normalization $\\hat x = \\dfrac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}$ using the population, rather than mini-batch, statistics. åœ¨è®­ç»ƒé˜¶æ®µå’Œæ¨ç†(inference)é˜¶æ®µä¸ä¸€æ ·ï¼Œè¿™é‡Œçš„æ¨ç†é˜¶æ®µæŒ‡çš„å°±æ˜¯æµ‹è¯•é˜¶æ®µï¼Œåœ¨æµ‹è¯•é˜¶æ®µä½¿ç”¨æ€»ä½“çš„å‡å€¼ï¼Œè€Œä¸æ˜¯ mini-batch çš„å‡å€¼ã€‚ Using moving averages instead, we can track the accuracy of a model as it trains. Since the means and variances are fixed during inference, the normalization is simply a linear transform applied to each activation. Batch-Normalized Convolutional Networks ç¬¬1-5æ­¥æ˜¯ç®—æ³•1çš„æµç¨‹ï¼Œå¯¹æ¯ä¸€ç»´æ ‡å‡†åŒ–ï¼Œå¾—åˆ° $N_{BN}^{tr}$ 6-7æ­¥ä¼˜åŒ–è®­ç»ƒå‚æ•° $\\theta \\bigcup {\\gamma^{k}, \\beta^{k}}$ï¼Œåœ¨æµ‹è¯•é˜¶æ®µå‚æ•°æ˜¯å›ºå®šçš„ 8-12æ­¥éª¤æ˜¯å°†è®­ç»ƒé˜¶æ®µçš„ç»Ÿè®¡ä¿¡æ¯è½¬åŒ–ä¸ºè®­ç»ƒé›†æ•´ä½“çš„ç»Ÿè®¡ä¿¡æ¯ã€‚å› ä¸ºå®Œæˆè®­ç»ƒååœ¨é¢„æµ‹é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ¨¡å‹å­˜å‚¨çš„æ•´ä½“çš„ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™é‡Œæ¶‰åŠåˆ°é€šè¿‡æ ·æœ¬å‡å€¼å’Œæ–¹å·®ä¼°è®¡æ€»ä½“çš„å‡å€¼å’Œæ–¹å·®çš„æ— åä¼°è®¡ï¼Œæ ·æœ¬å‡å€¼æ˜¯ç­‰äºæ€»ä½“å‡å€¼çš„æ— åä¼°è®¡çš„ï¼Œè€Œæ ·æœ¬å‡å€¼ä¸ç­‰äºæ€»ä½“å‡å€¼çš„æ— åä¼°è®¡ã€‚å…·ä½“å¯çœ‹çŸ¥ä¹ä¸Šçš„è§£ç­” https://www.zhihu.com/question/20099757 Batch Normalization enables higher learning rates In traditional deep networks, too high a learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. å­¦ä¹ ç‡è¿‡å¤§å®¹æ˜“å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ï¼Œä»è€Œé™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚ By normalizing activations throughout the network, it prevents small changes in layer parameters from amplifying as the data propagates through a deep network. é€šè¿‡è§„èŒƒåŒ–æ•´ä¸ªç½‘ç»œä¸­çš„æ¿€æ´»ï¼Œå¯ä»¥é˜²æ­¢å±‚å‚æ•°çš„å¾®å°å˜åŒ–åœ¨æ•°æ®é€šè¿‡æ·±å±‚ç½‘ç»œä¼ æ’­æ—¶æ”¾å¤§ã€‚ Batch Normalization also makes training more resilient to the parameter scale. Normally, large learning rates may increase the scale of layer parameters, which then amplify the gradient during backpropagation and lead to the model explosion. However, with Batch Normalization, backpropagation through a layer is unaffected by the scale of its parameters. BN èƒ½è®©è®­ç»ƒæ—¶çš„å‚æ•°æ›´æœ‰å¼¹æ€§ã€‚é€šå¸¸ï¼Œå­¦ä¹ ç‡è¿‡å¤§ä¼šå¢å¤§ç½‘ç»œå‚æ•°ï¼Œåœ¨åå‘ä¼ æ’­ä¸­å¯¼è‡´æ¢¯åº¦è¿‡å¤§è€Œå‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸ã€‚è€Œ BN ä½¿å¾—ç½‘ç»œä¸å—å‚æ•°çš„å¤§å°çš„å½±å“ã€‚ æ­£åˆ™åŒ–é™¤äº†å¯ä»¥æ›´å¿«åœ°è®­ç»ƒç½‘ç»œï¼ŒBNå±‚è¿˜æœ‰å¯¹æ¨¡å‹èµ·åˆ°æ­£åˆ™åŒ–çš„ä½œç”¨ã€‚å› ä¸ºå½“è®­ç»ƒä¸€ä¸ªBNç½‘ç»œçš„æ—¶å€™ï¼Œå¯¹äºä¸€ä¸ªç»™å®šçš„æ ·æœ¬ï¼Œå®ƒè¿˜å¯ä»¥â€çœ‹åˆ°â€ä¸€ä¸ªbatchä¸­å…¶ä»–çš„æƒ…å†µï¼Œè¿™æ ·ç½‘ç»œå¯¹äºä¸€ä¸ªç»™å®šçš„æ ·æœ¬è¾“å…¥æ¯æ¬¡å°±å¯ä»¥äº§ç”Ÿä¸€ä¸ªä¸ç¡®å®šçš„è¾“å‡º(å› ä¸ºæ ‡å‡†åŒ–çš„è¿‡ç¨‹å’Œbatchä¸­å…¶ä»–çš„æ ·æœ¬å‡æœ‰å…³è”)ï¼Œä½œè€…é€šè¿‡å®éªŒè¯æ˜è¿™å¯¹å‡å°‘æ¨¡å‹çš„è¿‡æ‹Ÿåˆå…·æœ‰ä½œç”¨ã€‚ ä»£ç å®ç°tensorflow å·²ç»å°è£…å¥½äº† BN å±‚ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡ tf.contrib.layers.batch_norm() è°ƒç”¨ï¼Œå¦‚æœä½ æƒ³çŸ¥é“å‡½æ•°èƒŒåçš„å…·ä½“å®ç°æ–¹æ³•ï¼ŒåŠ æ·±å¯¹BNå±‚çš„ç†è§£ï¼Œå¯ä»¥å‚è€ƒè¿™ç¯‡æ–‡ç« Implementing Batch Normalization in Tensorflowã€‚ reference: paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) å…³äºBatch Normalizationçš„ä¸€äº›é˜…è¯»ç†è§£","link":"/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/"},{"title":"æ·±åº¦å­¦ä¹ -Dropout","text":"dropoutçš„æ•°å­¦åŸç†ã€‚ Dropoutéšæœºå¤±æ´»ï¼ˆDropoutï¼‰æ˜¯ä¸€ä¸ªç®€å•åˆæå…¶æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç”±Srivastavaåœ¨è®ºæ–‡Dropout: A Simple Way to Prevent Neural Networks from Overfittingä¸­æå‡ºçš„ï¼Œä¸L1æ­£åˆ™åŒ–ï¼ŒL2æ­£åˆ™åŒ–å’Œæœ€å¤§èŒƒå¼çº¦æŸç­‰æ–¹æ³•äº’ä¸ºè¡¥å……ã€‚åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œéšæœºå¤±æ´»çš„å®ç°æ–¹æ³•æ˜¯è®©ç¥ç»å…ƒä»¥è¶…å‚æ•°pçš„æ¦‚ç‡è¢«æ¿€æ´»æˆ–è€…è¢«è®¾ç½®ä¸º0ã€‚ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºå¤±æ´»å¯ä»¥è¢«è®¤ä¸ºæ˜¯å¯¹å®Œæ•´çš„ç¥ç»ç½‘ç»œæŠ½æ ·å‡ºä¸€äº›å­é›†ï¼Œæ¯æ¬¡åŸºäºè¾“å…¥æ•°æ®åªæ›´æ–°å­ç½‘ç»œçš„å‚æ•°ï¼ˆç„¶è€Œï¼Œæ•°é‡å·¨å¤§çš„å­ç½‘ç»œä»¬å¹¶ä¸æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› ä¸ºå®ƒä»¬éƒ½å…±äº«å‚æ•°ï¼‰ã€‚åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ä¸ä½¿ç”¨éšæœºå¤±æ´»ï¼Œå¯ä»¥ç†è§£ä¸ºæ˜¯å¯¹æ•°é‡å·¨å¤§çš„å­ç½‘ç»œä»¬åšäº†æ¨¡å‹é›†æˆï¼ˆmodel ensembleï¼‰ï¼Œä»¥æ­¤æ¥è®¡ç®—å‡ºä¸€ä¸ªå¹³å‡çš„é¢„æµ‹ã€‚ å…³äºdropoutçš„ç†è§£:çŸ¥ä¹ä¸Šçš„å›ç­” pythonä»£ç : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&quot;&quot;&quot; æ™®é€šç‰ˆéšæœºå¤±æ´»: ä¸æ¨èå®ç° (çœ‹ä¸‹é¢ç¬”è®°) &quot;&quot;&quot;p = 0.5 # æ¿€æ´»ç¥ç»å…ƒçš„æ¦‚ç‡. på€¼æ›´é«˜ = éšæœºå¤±æ´»æ›´å¼±def train_step(X): &quot;&quot;&quot; Xä¸­æ˜¯è¾“å…¥æ•°æ® &quot;&quot;&quot; # 3å±‚neural networkçš„å‰å‘ä¼ æ’­ H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &lt; p # ç¬¬ä¸€ä¸ªéšæœºå¤±æ´»é®ç½©,rand() [0,1)çš„éšæœºæ•° H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &lt; p # ç¬¬äºŒä¸ªéšæœºå¤±æ´»é®ç½© H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # åå‘ä¼ æ’­:è®¡ç®—æ¢¯åº¦... (ç•¥) # è¿›è¡Œå‚æ•°æ›´æ–°... (ç•¥)def predict(X): # å‰å‘ä¼ æ’­æ—¶æ¨¡å‹é›†æˆ H1 = np.maximum(0, np.dot(W1, X) + b1) * p # æ³¨æ„ï¼šæ¿€æ´»æ•°æ®è¦ä¹˜ä»¥p H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # æ³¨æ„ï¼šæ¿€æ´»æ•°æ®è¦ä¹˜ä»¥p out = np.dot(W3, H2) + b3 åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œtrain_stepå‡½æ•°åœ¨ç¬¬ä¸€ä¸ªéšå±‚å’Œç¬¬äºŒä¸ªéšå±‚ä¸Šè¿›è¡Œäº†ä¸¤æ¬¡éšæœºå¤±æ´»ã€‚åœ¨è¾“å…¥å±‚ä¸Šé¢è¿›è¡Œéšæœºå¤±æ´»ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œä¸ºæ­¤éœ€è¦ä¸ºè¾“å…¥æ•°æ®Xåˆ›å»ºä¸€ä¸ªäºŒå€¼çš„é®ç½©ã€‚åå‘ä¼ æ’­ä¿æŒä¸å˜ï¼Œä½†æ˜¯è‚¯å®šéœ€è¦å°†é®ç½©U1å’ŒU2åŠ å…¥è¿›å»ã€‚ æ³¨æ„ï¼šåœ¨predictå‡½æ•°ä¸­ä¸è¿›è¡Œéšæœºå¤±æ´»ï¼Œä½†æ˜¯å¯¹äºä¸¤ä¸ªéšå±‚çš„è¾“å‡ºéƒ½è¦ä¹˜ä»¥pï¼Œè°ƒæ•´å…¶æ•°å€¼èŒƒå›´ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºåœ¨æµ‹è¯•æ—¶æ‰€æœ‰çš„ç¥ç»å…ƒéƒ½èƒ½çœ‹è§å®ƒä»¬çš„è¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬æƒ³è¦ç¥ç»å…ƒçš„è¾“å‡ºä¸è®­ç»ƒæ—¶çš„é¢„æœŸè¾“å‡ºæ˜¯ä¸€è‡´çš„ã€‚ä»¥p=0.5ä¸ºä¾‹ï¼Œåœ¨æµ‹è¯•æ—¶ç¥ç»å…ƒå¿…é¡»æŠŠå®ƒä»¬çš„è¾“å‡ºå‡åŠï¼Œè¿™æ˜¯å› ä¸ºåœ¨è®­ç»ƒçš„æ—¶å€™å®ƒä»¬çš„è¾“å‡ºåªæœ‰ä¸€åŠã€‚ä¸ºäº†ç†è§£è¿™ç‚¹ï¼Œå…ˆå‡è®¾æœ‰ä¸€ä¸ªç¥ç»å…ƒxçš„è¾“å‡ºï¼Œé‚£ä¹ˆè¿›è¡Œéšæœºå¤±æ´»çš„æ—¶å€™ï¼Œè¯¥ç¥ç»å…ƒçš„è¾“å‡ºå°±æ˜¯px+(1-p)0ï¼Œè¿™æ˜¯æœ‰1-pçš„æ¦‚ç‡ç¥ç»å…ƒçš„è¾“å‡ºä¸º0ã€‚åœ¨æµ‹è¯•æ—¶ç¥ç»å…ƒæ€»æ˜¯æ¿€æ´»çš„ï¼Œå°±å¿…é¡»è°ƒæ•´x\\to pxæ¥ä¿æŒåŒæ ·çš„é¢„æœŸè¾“å‡ºã€‚åœ¨æµ‹è¯•æ—¶ä¼šåœ¨æ‰€æœ‰å¯èƒ½çš„äºŒå€¼é®ç½©ï¼ˆä¹Ÿå°±æ˜¯æ•°é‡åºå¤§çš„æ‰€æœ‰å­ç½‘ç»œï¼‰ä¸­è¿­ä»£å¹¶è®¡ç®—å®ƒä»¬çš„åä½œé¢„æµ‹ï¼Œè¿›è¡Œè¿™ç§å‡å¼±çš„æ“ä½œä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯ä¸ä¹‹ç›¸å…³çš„ã€‚ åå‘éšæœºå¤±æ´»å®ƒæ˜¯åœ¨è®­ç»ƒæ—¶å°±è¿›è¡Œæ•°å€¼èŒƒå›´è°ƒæ•´ï¼Œä»è€Œè®©å‰å‘ä¼ æ’­åœ¨æµ‹è¯•æ—¶ä¿æŒä¸å˜ã€‚è¿™æ ·åšè¿˜æœ‰ä¸€ä¸ªå¥½å¤„ï¼Œæ— è®ºä½ å†³å®šæ˜¯å¦ä½¿ç”¨éšæœºå¤±æ´»ï¼Œé¢„æµ‹æ–¹æ³•çš„ä»£ç å¯ä»¥ä¿æŒä¸å˜ã€‚åå‘éšæœºå¤±æ´»çš„ä»£ç å¦‚ä¸‹ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;&quot;&quot;åå‘éšæœºå¤±æ´»: æ¨èå®ç°æ–¹å¼.åœ¨è®­ç»ƒçš„æ—¶å€™dropå’Œè°ƒæ•´æ•°å€¼èŒƒå›´ï¼Œæµ‹è¯•æ—¶ä¸åšä»»ä½•äº‹.&quot;&quot;&quot;p = 0.5 # æ¿€æ´»ç¥ç»å…ƒçš„æ¦‚ç‡. på€¼æ›´é«˜ = éšæœºå¤±æ´»æ›´å¼±def train_step(X): # 3å±‚neural networkçš„å‰å‘ä¼ æ’­ H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # ç¬¬ä¸€ä¸ªéšæœºå¤±æ´»é®ç½©. æ³¨æ„/p! H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = (np.random.rand(*H2.shape) &lt; p) / p # ç¬¬äºŒä¸ªéšæœºå¤±æ´»é®ç½©. æ³¨æ„/p! H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # åå‘ä¼ æ’­:è®¡ç®—æ¢¯åº¦... (ç•¥) # è¿›è¡Œå‚æ•°æ›´æ–°... (ç•¥)def predict(X): # å‰å‘ä¼ æ’­æ—¶æ¨¡å‹é›†æˆ H1 = np.maximum(0, np.dot(W1, X) + b1) # ä¸ç”¨æ•°å€¼èŒƒå›´è°ƒæ•´äº† H2 = np.maximum(0, np.dot(W2, H1) + b2) out = np.dot(W3, H2) + b3 åœ¨éšæœºå¤±æ´»å‘å¸ƒåï¼Œå¾ˆå¿«æœ‰å¤§é‡ç ”ç©¶ä¸ºä»€ä¹ˆå®ƒçš„å®è·µæ•ˆæœå¦‚æ­¤ä¹‹å¥½ï¼Œä»¥åŠå®ƒå’Œå…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ä¹‹é—´çš„å…³ç³»ã€‚å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå¯ä»¥çœ‹çœ‹è¿™äº›æ–‡çŒ®ï¼š Dropout paper by Srivastava et al. 2014. Dropout Training as Adaptive Regularizationï¼šâ€œæˆ‘ä»¬è®¤ä¸ºï¼šåœ¨ä½¿ç”¨è´¹å¸Œå°”ä¿¡æ¯çŸ©é˜µï¼ˆfisher information matrixï¼‰çš„å¯¹è§’é€†çŸ©é˜µçš„æœŸæœ›å¯¹ç‰¹å¾è¿›è¡Œæ•°å€¼èŒƒå›´è°ƒæ•´åï¼Œå†è¿›è¡ŒL2æ­£åˆ™åŒ–è¿™ä¸€æ“ä½œï¼Œä¸éšæœºå¤±æ´»æ­£åˆ™åŒ–æ˜¯ä¸€é˜¶ç›¸ç­‰çš„ã€‚â€","link":"/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Dropout/"},{"title":"æœºå™¨å­¦ä¹ -è¿‡æ‹Ÿåˆ","text":"è¿‡æ‹Ÿåˆçš„åŸç†ä»¥åŠè§£å†³æ–¹æ³•ã€‚ Overfittingè¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰æ˜¯æŒ‡åœ¨æ¨¡å‹å‚æ•°æ‹Ÿåˆè¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œç”±äºè®­ç»ƒæ•°æ®åŒ…å«æŠ½æ ·è¯¯å·®ï¼Œè®­ç»ƒæ—¶ï¼Œå¤æ‚çš„æ¨¡å‹å°†æŠ½æ ·è¯¯å·®ä¹Ÿè€ƒè™‘åœ¨å†…ï¼Œå°†æŠ½æ ·è¯¯å·®ä¹Ÿè¿›è¡Œäº†å¾ˆå¥½çš„æ‹Ÿåˆã€‚ å…·ä½“è¡¨ç°å°±æ˜¯æœ€ç»ˆæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šæ•ˆæœå¥½ï¼›åœ¨æµ‹è¯•é›†ä¸Šæ•ˆæœå·®ã€‚æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼±ã€‚ ä¸ºä»€ä¹ˆè¦è§£å†³è¿‡æ‹Ÿåˆä¸ºä»€ä¹ˆè¦è§£å†³è¿‡æ‹Ÿåˆç°è±¡ï¼Ÿè¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ‹Ÿåˆçš„æ¨¡å‹ä¸€èˆ¬æ˜¯ç”¨æ¥é¢„æµ‹æœªçŸ¥çš„ç»“æœï¼ˆä¸åœ¨è®­ç»ƒé›†å†…ï¼‰ï¼Œè¿‡æ‹Ÿåˆè™½ç„¶åœ¨è®­ç»ƒé›†ä¸Šæ•ˆæœå¥½ï¼Œä½†æ˜¯åœ¨å®é™…ä½¿ç”¨æ—¶ï¼ˆæµ‹è¯•é›†ï¼‰æ•ˆæœå·®ã€‚åŒæ—¶ï¼Œåœ¨å¾ˆå¤šé—®é¢˜ä¸Šï¼Œæˆ‘ä»¬æ— æ³•ç©·å°½æ‰€æœ‰çŠ¶æ€ï¼Œä¸å¯èƒ½å°†æ‰€æœ‰æƒ…å†µéƒ½åŒ…å«åœ¨è®­ç»ƒé›†ä¸Šã€‚æ‰€ä»¥ï¼Œå¿…é¡»è¦è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ ä¸ºä»€ä¹ˆåœ¨æœºå™¨å­¦ä¹ ä¸­æ¯”è¾ƒå¸¸è§ï¼Ÿè¿™æ˜¯å› ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•ä¸ºäº†æ»¡è¶³å°½å¯èƒ½å¤æ‚çš„ä»»åŠ¡ï¼Œå…¶æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ä¸€èˆ¬è¿œè¿œé«˜äºé—®é¢˜å¤æ‚åº¦ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•æœ‰ã€Œæ‹Ÿåˆå‡ºæ­£ç¡®è§„åˆ™çš„å‰æä¸‹ï¼Œè¿›ä¸€æ­¥æ‹Ÿåˆå™ªå£°ã€çš„èƒ½åŠ›ã€‚ è€Œä¼ ç»Ÿçš„å‡½æ•°æ‹Ÿåˆé—®é¢˜ï¼ˆå¦‚æœºå™¨äººç³»ç»Ÿè¾¨è¯†ï¼‰ï¼Œä¸€èˆ¬éƒ½æ˜¯é€šè¿‡ç»éªŒã€ç‰©ç†ã€æ•°å­¦ç­‰æ¨å¯¼å‡ºä¸€ä¸ªå«å‚æ¨¡å‹ï¼Œæ¨¡å‹å¤æ‚åº¦ç¡®å®šäº†ï¼Œåªéœ€è¦è°ƒæ•´ä¸ªåˆ«å‚æ•°å³å¯ã€‚æ¨¡å‹ã€Œæ— å¤šä½™èƒ½åŠ›ã€æ‹Ÿåˆå™ªå£°ã€‚ è§£å†³æ–¹æ³•è·å–æ›´å¤šæ•°æ®è¿™æ˜¯è§£å†³è¿‡æ‹Ÿåˆæœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåªè¦ç»™è¶³å¤Ÿå¤šçš„æ•°æ®ï¼Œè®©æ¨¡å‹ã€Œçœ‹è§ã€å°½å¯èƒ½å¤šçš„ã€Œä¾‹å¤–æƒ…å†µã€ï¼Œå®ƒå°±ä¼šä¸æ–­ä¿®æ­£è‡ªå·±ï¼Œä»è€Œå¾—åˆ°æ›´å¥½çš„ç»“æœï¼š å¦‚ä½•è·å–æ›´å¤šæ•°æ®ï¼Œå¯ä»¥æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹æ³•ï¼š ä»æ•°æ®æºå¤´è·å–æ›´å¤šæ•°æ®ï¼šè¿™ä¸ªæ˜¯å®¹æ˜“æƒ³åˆ°çš„ï¼Œä¾‹å¦‚ç‰©ä½“åˆ†ç±»ï¼Œæˆ‘å°±å†å¤šæ‹å‡ å¼ ç…§ç‰‡å¥½äº†ï¼›ä½†æ˜¯ï¼Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œå¤§å¹…å¢åŠ æ•°æ®æœ¬èº«å°±ä¸å®¹æ˜“ï¼›å¦å¤–ï¼Œæˆ‘ä»¬ä¸æ¸…æ¥šè·å–å¤šå°‘æ•°æ®æ‰ç®—å¤Ÿï¼› æ ¹æ®å½“å‰æ•°æ®é›†ä¼°è®¡æ•°æ®åˆ†å¸ƒå‚æ•°ï¼Œä½¿ç”¨è¯¥åˆ†å¸ƒäº§ç”Ÿæ›´å¤šæ•°æ®ï¼šè¿™ä¸ªä¸€èˆ¬ä¸ç”¨ï¼Œå› ä¸ºä¼°è®¡åˆ†å¸ƒå‚æ•°çš„è¿‡ç¨‹ä¹Ÿä¼šä»£å…¥æŠ½æ ·è¯¯å·®ã€‚ æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰ï¼šé€šè¿‡ä¸€å®šè§„åˆ™æ‰©å……æ•°æ®ã€‚å¦‚åœ¨ç‰©ä½“åˆ†ç±»é—®é¢˜é‡Œï¼Œç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®ã€å§¿æ€ã€å°ºåº¦ï¼Œæ•´ä½“å›¾ç‰‡æ˜æš—åº¦ç­‰éƒ½ä¸ä¼šå½±å“åˆ†ç±»ç»“æœã€‚æˆ‘ä»¬å°±å¯ä»¥é€šè¿‡å›¾åƒå¹³ç§»ã€ç¿»è½¬ã€ç¼©æ”¾ã€åˆ‡å‰²ç­‰æ‰‹æ®µå°†æ•°æ®åº“æˆå€æ‰©å……ï¼› ä½¿ç”¨åˆé€‚çš„æ¨¡å‹å‰é¢è¯´äº†ï¼Œè¿‡æ‹Ÿåˆä¸»è¦æ˜¯æœ‰ä¸¤ä¸ªåŸå› é€ æˆçš„ï¼šæ•°æ®å¤ªå°‘+æ¨¡å‹å¤ªå¤æ‚ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨åˆé€‚å¤æ‚åº¦çš„æ¨¡å‹æ¥é˜²æ­¢è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œè®©å…¶è¶³å¤Ÿæ‹ŸåˆçœŸæ­£çš„è§„åˆ™ï¼ŒåŒæ—¶åˆä¸è‡³äºæ‹Ÿåˆå¤ªå¤šæŠ½æ ·è¯¯å·®ã€‚ ï¼ˆPSï¼šå¦‚æœèƒ½é€šè¿‡ç‰©ç†ã€æ•°å­¦å»ºæ¨¡ï¼Œç¡®å®šæ¨¡å‹å¤æ‚åº¦ï¼Œè¿™æ˜¯æœ€å¥½çš„æ–¹æ³•ï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ è¿™ä¹ˆç«çš„ç°åœ¨ï¼Œæˆ‘è¿˜åšæŒè¯´åˆå­¦è€…è¦å­¦æŒæ¡ä¼ ç»Ÿçš„å»ºæ¨¡æ–¹æ³•ã€‚ï¼‰ å¯¹äºç¥ç»ç½‘ç»œè€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹å››ä¸ªæ–¹é¢æ¥é™åˆ¶ç½‘ç»œèƒ½åŠ›ï¼š ç½‘ç»œç»“æ„ Architectureè¿™ä¸ªå¾ˆå¥½ç†è§£ï¼Œå‡å°‘ç½‘ç»œçš„å±‚æ•°ã€ç¥ç»å…ƒä¸ªæ•°ç­‰å‡å¯ä»¥é™åˆ¶ç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›ï¼› è®­ç»ƒæ—¶é—´ Early stoppingå¯¹äºæ¯ä¸ªç¥ç»å…ƒè€Œè¨€ï¼Œå…¶æ¿€æ´»å‡½æ•°åœ¨ä¸åŒåŒºé—´çš„æ€§èƒ½æ˜¯ä¸åŒçš„ï¼š å½“ç½‘ç»œæƒå€¼è¾ƒå°æ—¶ï¼Œç¥ç»å…ƒçš„æ¿€æ´»å‡½æ•°å·¥ä½œåœ¨çº¿æ€§åŒºï¼Œæ­¤æ—¶ç¥ç»å…ƒçš„æ‹Ÿåˆèƒ½åŠ›è¾ƒå¼±ï¼ˆç±»ä¼¼çº¿æ€§ç¥ç»å…ƒï¼‰ã€‚ æœ‰äº†ä¸Šè¿°å…±è¯†ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥è§£é‡Šä¸ºä»€ä¹ˆé™åˆ¶è®­ç»ƒæ—¶é—´ï¼ˆearly stoppingï¼‰æœ‰ç”¨ï¼šå› ä¸ºæˆ‘ä»¬åœ¨åˆå§‹åŒ–ç½‘ç»œçš„æ—¶å€™ä¸€èˆ¬éƒ½æ˜¯åˆå§‹ä¸ºè¾ƒå°çš„æƒå€¼ã€‚è®­ç»ƒæ—¶é—´è¶Šé•¿ï¼Œéƒ¨åˆ†ç½‘ç»œæƒå€¼å¯èƒ½è¶Šå¤§ã€‚å¦‚æœæˆ‘ä»¬åœ¨åˆé€‚æ—¶é—´åœæ­¢è®­ç»ƒï¼Œå°±å¯ä»¥å°†ç½‘ç»œçš„èƒ½åŠ›é™åˆ¶åœ¨ä¸€å®šèŒƒå›´å†…ã€‚ é™åˆ¶æƒå€¼ Weight-decayï¼Œä¹Ÿå«æ­£åˆ™åŒ–ï¼ˆregularizationï¼‰åŸç†åŒä¸Šï¼Œä½†æ˜¯è¿™ç±»æ–¹æ³•ç›´æ¥å°†æƒå€¼çš„å¤§å°åŠ å…¥åˆ° Cost é‡Œï¼Œåœ¨è®­ç»ƒçš„æ—¶å€™é™åˆ¶æƒå€¼å˜å¤§ã€‚ä»¥ L2 regularizationä¸ºä¾‹ï¼š è®­ç»ƒè¿‡ç¨‹éœ€è¦é™ä½æ•´ä½“çš„ Costï¼Œè¿™æ—¶å€™ï¼Œä¸€æ–¹é¢èƒ½é™ä½å®é™…è¾“å‡ºä¸æ ·æœ¬ä¹‹é—´çš„è¯¯å·® ï¼Œä¹Ÿèƒ½é™ä½æƒå€¼å¤§å°ã€‚ å¢åŠ å™ªå£° Noiseç»™ç½‘ç»œåŠ å™ªå£°ä¹Ÿæœ‰å¾ˆå¤šæ–¹æ³•ï¼š åœ¨è¾“å…¥ä¸­åŠ å™ªå£°ï¼šå™ªå£°ä¼šéšç€ç½‘ç»œä¼ æ’­ï¼ŒæŒ‰ç…§æƒå€¼çš„å¹³æ–¹æ”¾å¤§ï¼Œå¹¶ä¼ æ’­åˆ°è¾“å‡ºå±‚ï¼Œå¯¹è¯¯å·® Cost äº§ç”Ÿå½±å“ã€‚æ¨å¯¼ç›´æ¥çœ‹ Hinton çš„ PPT å§ï¼š åœ¨è¾“å…¥ä¸­åŠ é«˜æ–¯å™ªå£°ï¼Œä¼šåœ¨è¾“å‡ºä¸­ç”Ÿæˆ çš„å¹²æ‰°é¡¹ã€‚è®­ç»ƒæ—¶ï¼Œå‡å°è¯¯å·®ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹å™ªå£°äº§ç”Ÿçš„å¹²æ‰°é¡¹è¿›è¡Œæƒ©ç½šï¼Œè¾¾åˆ°å‡å°æƒå€¼çš„å¹³æ–¹çš„ç›®çš„ï¼Œè¾¾åˆ°ä¸ L2 regularization ç±»ä¼¼çš„æ•ˆæœï¼ˆå¯¹æ¯”å…¬å¼ï¼‰ã€‚ åœ¨æƒå€¼ä¸ŠåŠ å™ªå£°åœ¨åˆå§‹åŒ–ç½‘ç»œçš„æ—¶å€™ï¼Œç”¨0å‡å€¼çš„é«˜æ–¯åˆ†å¸ƒä½œä¸ºåˆå§‹åŒ–ã€‚Alex Graves çš„æ‰‹å†™è¯†åˆ« RNN å°±æ˜¯ç”¨äº†è¿™ä¸ªæ–¹æ³• Graves, Alex, et al. â€œA novel connectionist system for unconstrained handwriting recognition.â€ IEEE transactions on pattern analysis and machine intelligence 31.5 (2009): 855-868. It may work better, especially in recurrent networks (Hinton) å¯¹ç½‘ç»œçš„å“åº”åŠ å™ªå£°å¦‚åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè®©é»˜å†™ç¥ç»å…ƒçš„è¾“å‡ºå˜ä¸º binary æˆ– randomã€‚æ˜¾ç„¶ï¼Œè¿™ç§æœ‰ç‚¹ä¹±æ¥çš„åšæ³•ä¼šæ‰“ä¹±ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ï¼Œè®©è®­ç»ƒæ›´æ…¢ï¼Œä½†æ® Hinton è¯´ï¼Œåœ¨æµ‹è¯•é›†ä¸Šæ•ˆæœä¼šæœ‰æ˜¾è‘—æå‡ ï¼ˆBut it does significantly better on the test set!ï¼‰ã€‚ ç»“åˆå¤šç§æ¨¡å‹ç®€è€Œè¨€ä¹‹ï¼Œè®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œä»¥æ¯ä¸ªæ¨¡å‹çš„å¹³å‡è¾“å‡ºä½œä¸ºç»“æœã€‚ ä» N ä¸ªæ¨¡å‹é‡Œéšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºè¾“å‡ºçš„æœŸæœ›è¯¯å·® ï¼Œä¼šæ¯”æ‰€æœ‰æ¨¡å‹çš„å¹³å‡è¾“å‡ºçš„è¯¯å·® å¤§ï¼ˆæˆ‘ä¸çŸ¥é“å…¬å¼é‡Œçš„åœ†æ‹¬å·ä¸ºä»€ä¹ˆæ˜¾ç¤ºä¸äº†ï¼‰ï¼š å¤§æ¦‚åŸºäºè¿™ä¸ªåŸç†ï¼Œå°±å¯ä»¥æœ‰å¾ˆå¤šæ–¹æ³•äº†ï¼š Baggingç®€å•ç†è§£ï¼Œå°±æ˜¯åˆ†æ®µå‡½æ•°çš„æ¦‚å¿µï¼šç”¨ä¸åŒçš„æ¨¡å‹æ‹Ÿåˆä¸åŒéƒ¨åˆ†çš„è®­ç»ƒé›†ã€‚ä»¥éšæœºæ£®æ—ï¼ˆRand Forestsï¼‰ä¸ºä¾‹ï¼Œå°±æ˜¯è®­ç»ƒäº†ä¸€å †äº’ä¸å…³è”çš„å†³ç­–æ ‘ã€‚ä½†ç”±äºè®­ç»ƒç¥ç»ç½‘ç»œæœ¬èº«å°±éœ€è¦è€—è´¹è¾ƒå¤šè‡ªç”±ï¼Œæ‰€ä»¥ä¸€èˆ¬ä¸å•ç‹¬ä½¿ç”¨ç¥ç»ç½‘ç»œåšBaggingã€‚ Boostingæ—¢ç„¶è®­ç»ƒå¤æ‚ç¥ç»ç½‘ç»œæ¯”è¾ƒæ…¢ï¼Œé‚£æˆ‘ä»¬å°±å¯ä»¥åªä½¿ç”¨ç®€å•çš„ç¥ç»ç½‘ç»œï¼ˆå±‚æ•°ã€ç¥ç»å…ƒæ•°é™åˆ¶ç­‰ï¼‰ã€‚é€šè¿‡è®­ç»ƒä¸€ç³»åˆ—ç®€å•çš„ç¥ç»ç½‘ç»œï¼ŒåŠ æƒå¹³å‡å…¶è¾“å‡ºã€‚ Dropout åœ¨è®­ç»ƒæ—¶ï¼Œæ¯æ¬¡éšæœºï¼ˆå¦‚50%æ¦‚ç‡ï¼‰å¿½ç•¥éšå±‚çš„æŸäº›èŠ‚ç‚¹ï¼›è¿™æ ·ï¼Œæˆ‘ä»¬ç›¸å½“äºéšæœºä»2^Hä¸ªæ¨¡å‹ä¸­é‡‡æ ·é€‰æ‹©æ¨¡å‹ï¼›åŒæ—¶ï¼Œç”±äºæ¯ä¸ªç½‘ç»œåªè§è¿‡ä¸€ä¸ªè®­ç»ƒæ•°æ®ï¼ˆæ¯æ¬¡éƒ½æ˜¯éšæœºçš„æ–°ç½‘ç»œï¼‰ï¼Œæ‰€ä»¥ç±»ä¼¼ bagging çš„åšæ³•ï¼Œè¿™å°±æ˜¯æˆ‘ä¸ºä»€ä¹ˆå°†å®ƒåˆ†ç±»åˆ°ã€Œç»“åˆå¤šç§æ¨¡å‹ã€ä¸­ï¼› æ­¤å¤–ï¼Œè€Œä¸åŒæ¨¡å‹ä¹‹é—´æƒå€¼å…±äº«ï¼ˆå…±åŒä½¿ç”¨è¿™ H ä¸ªç¥ç»å…ƒçš„è¿æ¥æƒå€¼ï¼‰ï¼Œç›¸å½“äºä¸€ç§æƒå€¼æ­£åˆ™æ–¹æ³•ï¼Œå®é™…æ•ˆæœæ¯” L2 regularization æ›´å¥½ã€‚ è´å¶æ–¯æ–¹æ³•æ€»ç»“","link":"/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/"},{"title":"æ–‡æœ¬åˆ†ç±»ç³»åˆ—3-TextRNN","text":"paper readingå°½ç®¡TextCNNèƒ½å¤Ÿåœ¨å¾ˆå¤šä»»åŠ¡é‡Œé¢èƒ½æœ‰ä¸é”™çš„è¡¨ç°ï¼Œä½†CNNæœ‰ä¸ªæœ€å¤§é—®é¢˜æ˜¯å›ºå®š filter_size çš„è§†é‡ï¼Œä¸€æ–¹é¢æ— æ³•å»ºæ¨¡æ›´é•¿çš„åºåˆ—ä¿¡æ¯ï¼Œå¦ä¸€æ–¹é¢ filter_size çš„è¶…å‚è°ƒèŠ‚ä¹Ÿå¾ˆç¹çã€‚CNNæœ¬è´¨æ˜¯åšæ–‡æœ¬çš„ç‰¹å¾è¡¨è¾¾å·¥ä½œï¼Œè€Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ›´å¸¸ç”¨çš„æ˜¯é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNN, Recurrent Neural Networkï¼‰ï¼Œèƒ½å¤Ÿæ›´å¥½çš„è¡¨è¾¾ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒBi-directional RNNï¼ˆå®é™…ä½¿ç”¨çš„æ˜¯åŒå‘LSTMï¼‰ä»æŸç§æ„ä¹‰ä¸Šå¯ä»¥ç†è§£ä¸ºå¯ä»¥æ•è·å˜é•¿ä¸”åŒå‘çš„çš„ â€œn-gramâ€ ä¿¡æ¯ã€‚ RNNç®—æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸéå¸¸ä¸€ä¸ªæ ‡é…ç½‘ç»œäº†ï¼Œåœ¨åºåˆ—æ ‡æ³¨/å‘½åä½“è¯†åˆ«/seq2seqæ¨¡å‹ç­‰å¾ˆå¤šåœºæ™¯éƒ½æœ‰åº”ç”¨ï¼ŒRecurrent Neural Network for Text Classification with Multi-Task Learningæ–‡ä¸­ä»‹ç»äº†RNNç”¨äºåˆ†ç±»é—®é¢˜çš„è®¾è®¡ï¼Œä¸‹å›¾LSTMç”¨äºç½‘ç»œç»“æ„åŸç†ç¤ºæ„å›¾ï¼Œç¤ºä¾‹ä¸­çš„æ˜¯åˆ©ç”¨æœ€åä¸€ä¸ªè¯çš„ç»“æœç›´æ¥æ¥å…¨è¿æ¥å±‚softmaxè¾“å‡ºäº†ã€‚ å…³äºè§£å†³RNNæ— æ³•å¹¶è¡ŒåŒ–ï¼Œè®¡ç®—æ•ˆç‡ä½çš„é—®é¢˜Factorization tricks for LSTM networks We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is â€œmatrix factorization by designâ€ of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.","link":"/2018/05/31/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%973-TextRNN/"},{"title":"æœºå™¨å­¦ä¹ -ç”Ÿæˆæ¨¡å‹åˆ°é«˜æ–¯åˆ¤åˆ«åˆ†æå†åˆ°GMMå’ŒEMç®—æ³•","text":"ç”Ÿæˆæ¨¡å‹-é«˜æ–¯åˆ¤åˆ«åˆ†æGDA- é«˜æ–¯æ··åˆæ¨¡å‹GMM- EMç®—æ³• åœ¨å­¦ä¹ ç”Ÿæˆæ¨¡å‹ä¹‹å‰ï¼Œå…ˆå­¦ä¹ äº†è§£ä¸‹å¯†åº¦ä¼°è®¡å’Œé«˜æ–¯æ··åˆæ¨¡å‹ã€‚ ç”Ÿæˆå­¦ä¹ ç®—æ³•(cs229,Ng)ç”Ÿæˆç®—æ³•å’Œåˆ¤åˆ«ç®—æ³•çš„åŒºåˆ«ä¸¾ä¸ªæ —å­ï¼š æˆ‘ä»¬è¦åŒºåˆ†elephants(y=1)å’Œdogs(y=0) å¯¹åˆ¤åˆ«æ¨¡å‹ï¼ˆdiscriminativeï¼‰ï¼Œä»¥logisticå›å½’ä¸ºä¾‹ï¼š logisticå›å½’æ¨¡å‹ï¼š$p(y|x;\\theta)ï¼Œh_{\\theta}=g(\\theta^Tx)$,å¯¹åº”çš„æ¨¡å‹å…¶ä¸­gæ˜¯sigmoidå‡½æ•°ã€‚é€šè¿‡logisticå›å½’ï¼Œæˆ‘ä»¬æ‰¾åˆ°ä¸€æ¡å†³ç­–è¾¹ç•Œdecision boundaryï¼Œèƒ½å¤ŸåŒºåˆ†elephantså’Œdogs. è¿™ä¸ªå­¦ä¹ çš„è¿‡ç¨‹å°±æ˜¯æ‰¾åˆ°è¡¨å¾è¿™ä¸ªå†³ç­–è¿‡ç¨‹çš„å‚æ•° $\\theta$. ç”Ÿæˆæ¨¡å‹ï¼ˆgenerativeï¼‰ï¼š åŒæ ·çš„æˆ‘ä»¬ä¹Ÿæ˜¯è¦é€šè¿‡ç»™å®šçš„ç‰¹å¾xæ¥åˆ¤åˆ«å…¶å¯¹åº”çš„ç±»åˆ«yã€‚ä½†æˆ‘ä»¬æ¢ä¸ªæ€è·¯ï¼Œå°±æ˜¯å…ˆæ±‚p(x|y),ä¹Ÿå°±æ˜¯é€šè¿‡yæ¥åˆ†æå¯¹åº”xæ»¡è¶³çš„ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹p(x|y)ã€‚ç„¶ååœ¨åè¿‡æ¥çœ‹ç‰¹å¾xï¼Œä»¥äºŒåˆ†ç±»ä¸ºä¾‹ï¼Œp(x|y=0)å’Œp(x|y=1)å“ªä¸ªæ¦‚ç‡å¤§ï¼Œé‚£ä¹ˆxå°±å±äºå“ªä¸€ç±»ã€‚ æ¨¡å‹ï¼šp(x|y)ï¼Œåœ¨ç»™å®šäº†æ ·æœ¬æ‰€å±çš„ç±»çš„æ¡ä»¶ä¸‹ï¼Œå¯¹æ ·æœ¬ç‰¹å¾å»ºç«‹æ¦‚ç‡æ¨¡å‹ã€‚ p(x|y=1)æ˜¯elephantsçš„åˆ†ç±»ç‰¹å¾æ¨¡å‹ p(x|y=0)æ˜¯dogsçš„åˆ†ç±»ç‰¹å¾æ¨¡å‹ ç„¶åé€šè¿‡p(x|y)æ¥åˆ¤æ–­ç‰¹å¾xæ‰€å±çš„ç±»åˆ«ï¼Œæ ¹æ®è´å¶æ–¯å…¬å¼ï¼š $$p(y=1|x) = \\dfrac{p(x|y=1)p(x)}{p(x)}$$ åœ¨ç»™å®šäº†xçš„æƒ…å†µä¸‹p(x)æ˜¯ä¸ªå®šå€¼ï¼Œp(y)æ˜¯å…ˆéªŒåˆ†å¸ƒï¼Œé‚£ä¹ˆè®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š $$arg\\max_yp(y|x) = arg\\max_{y}\\dfrac{p(x|y)p(y)}{p(x)}= arg\\max_{y}p(x|y)p(y)$$ æ€»ç»“ä¸‹å°±æ˜¯ï¼š ç”Ÿæˆæ¨¡å‹ï¼šä¸€èˆ¬æ˜¯å­¦ä¹ ä¸€ä¸ªä»£è¡¨ç›®æ ‡çš„æ¨¡å‹ï¼Œç„¶åé€šè¿‡å®ƒå»æœç´¢å›¾åƒåŒºåŸŸï¼Œç„¶åæœ€å°åŒ–é‡æ„è¯¯å·®ã€‚ç±»ä¼¼äºç”Ÿæˆæ¨¡å‹æè¿°ä¸€ä¸ªç›®æ ‡ï¼Œç„¶åå°±æ˜¯æ¨¡å¼åŒ¹é…äº†ï¼Œåœ¨å›¾åƒä¸­æ‰¾åˆ°å’Œè¿™ä¸ªæ¨¡å‹æœ€åŒ¹é…çš„åŒºåŸŸï¼Œå°±æ˜¯ç›®æ ‡äº†ã€‚ åˆ¤åˆ«æ¨¡å‹ï¼šä»¥åˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œç„¶åæ‰¾åˆ°ç›®æ ‡å’ŒèƒŒæ™¯çš„å†³ç­–è¾¹ç•Œã€‚å®ƒä¸ç®¡ç›®æ ‡æ˜¯æ€ä¹ˆæè¿°çš„ï¼Œé‚£åªè¦çŸ¥é“ç›®æ ‡å’ŒèƒŒæ™¯çš„å·®åˆ«åœ¨å“ªï¼Œç„¶åä½ ç»™ä¸€ä¸ªå›¾åƒï¼Œå®ƒçœ‹å®ƒå¤„äºè¾¹ç•Œçš„é‚£ä¸€è¾¹ï¼Œå°±å½’ä¸ºå“ªä¸€ç±»ã€‚ ç”±ç”Ÿæˆæ¨¡å‹å¯ä»¥å¾—åˆ°åˆ¤åˆ«æ¨¡å‹ï¼Œä½†ç”±åˆ¤åˆ«æ¨¡å‹å¾—ä¸åˆ°ç”Ÿæˆæ¨¡å‹ã€‚ ç„¶é¹…ï¼Œç”Ÿæˆæ¨¡å‹p(x|y)æ€ä¹ˆå¾—åˆ°å‘¢ï¼Ÿä¸æ…Œï¼Œæˆ‘ä»¬å…ˆäº†è§£ä¸‹å¤šç»´æ­£æ€åˆ†å¸ƒï½ å¤šç»´æ­£æ€åˆ†å¸ƒ(the multivariate nirmal distribution) å…³äºä¸€ç»´æ­£æ€åˆ†å¸ƒæ€ä¹ˆæ¨å¯¼å‡ºå¤šç»´æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œå¯å‚è€ƒçŸ¥ä¹:å¤šç»´é«˜æ–¯åˆ†å¸ƒæ˜¯å¦‚ä½•ç”±ä¸€ç»´å‘å±•è€Œæ¥çš„ï¼Ÿ é¦–å…ˆä¸€ç»´æ­£æ€åˆ†å¸ƒ: $p(x) = \\dfrac{1}{\\sqrt{2\\pi}}exp(\\dfrac{-x^2}{2})$ äºŒç»´æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå°±æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„ä¸€ç»´æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºå˜é‡çš„è”åˆåˆ†å¸ƒï¼š $p(x,y) = p(x)p(y)=\\dfrac{1}{2\\pi}exp(-\\dfrac{x^2+y^2}{2})$ æŠŠä¸¤ä¸ªéšæœºå˜é‡ç»„åˆæˆä¸€ä¸ªéšæœºå‘é‡ï¼š$v=[x\\quad y]^T$ $p(v)=\\dfrac{1}{2\\pi}exp(-\\dfrac{1}{2}v^Tv)\\quad$ æ˜¾ç„¶x,yç›¸äº’ç‹¬ç«‹çš„è¯ï¼Œå°±æ˜¯ä¸Šé¢çš„äºŒç»´æ ‡å‡†æ­£æ€åˆ†å¸ƒå…¬å¼ï½ ç„¶åä»æ ‡å‡†æ­£æ€åˆ†å¸ƒæ¨å¹¿åˆ°ä¸€èˆ¬æ­£æ€åˆ†å¸ƒï¼Œé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜åŒ–ï¼š$v=A(x-\\mu)$ $p(x)=\\dfrac{|A|}{2\\pi}exp[-\\dfrac{1}{2}(x-\\mu)^TA^TA(x-\\mu)]$ æ³¨æ„å‰é¢çš„ç³»æ•°å¤šäº†ä¸€ä¸ª|A|ï¼ˆAçš„è¡Œåˆ—å¼ï¼‰ã€‚ å¯ä»¥è¯æ˜è¿™ä¸ªåˆ†å¸ƒçš„å‡å€¼ä¸º$\\mu$ï¼Œåæ–¹å·®ä¸º$(A^TA)^{-1}$ã€‚è®°$\\Sigma = (A^TA)^{-1}$ï¼Œé‚£å°±æœ‰ $$p(\\mathbf{x}) = \\frac{1}{2\\pi|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (\\mathbf{x} - \\mu) ^T \\Sigma^{-1} (\\mathbf{x} - \\mu) \\right]$$ æ¨å¹¿åˆ°nç»´ï¼š $$p(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (\\mathbf{x} - \\mu) ^T \\Sigma^{-1} (\\mathbf{x} - \\mu) \\right]$$ éœ€è¦æ³¨æ„çš„æ˜¯ï¼šè¿™é‡Œçš„äºŒç»´ã€nç»´åˆ°åº•æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿ ä»¥é£æœºæ£€æµ‹çš„æ•°æ®ç‚¹ä¸ºä¾‹ï¼Œå‡è®¾å®ƒç”±heatå’Œtimeå†³å®šï¼Œé‚£ä¹ˆè¿™å°±æ˜¯ä¸ªäºŒç»´æ­£æ€åˆ†å¸ƒï¼Œæ•°æ®ç‚¹çš„ç”Ÿæˆæ‰€å¤„çš„ä½ç½®ç”±å…¶æ¦‚ç‡å†³å®šï¼Œä¹Ÿå°±æ˜¯$p(\\mathbf{x})$ å¦‚æœè¿™ä¸ªæ•°æ®æœ‰nä¸ªç‰¹å¾ï¼Œé‚£ä¹ˆå…¶åˆ†å¸ƒå°±æ˜¯nç»´æ­£æ€åˆ†å¸ƒã€‚ ä¹‹å‰ä¸€ç›´ç†è§£çš„æ˜¯ï¼Œnç»´æ­£æ€åˆ†å¸ƒæ˜¯ä¸¤ä¸ªå‘é‡å·´æ‹‰å·´æ‹‰ã€‚ã€‚å¥½åƒä¸€ç›´æ²¡ææ‡‚ã€‚ã€‚ å†é¡ºä¾¿äº†è§£ä¸‹åæ–¹å·®çŸ©é˜µå§ï½ å…³äºåæ–¹å·®çŸ©é˜µï¼Œå‚è€ƒblogå¯¹å¤šç»´éšæœºå˜é‡$X=[X_1,X_2,â€¦,X_n]^T$ï¼Œæˆ‘ä»¬å¾€å¾€éœ€è¦è®¡ç®—å„ç»´åº¦ä¹‹é—´çš„åæ–¹å·®ï¼Œè¿™æ ·åæ–¹å·®å°±ç»„æˆäº†ä¸€ä¸ªnÃ—nçš„çŸ©é˜µï¼Œç§°ä¸ºåæ–¹å·®çŸ©é˜µã€‚åæ–¹å·®çŸ©é˜µæ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šçš„å…ƒç´ æ˜¯å„ç»´åº¦ä¸Šéšæœºå˜é‡çš„æ–¹å·®,éå¯¹è§’çº¿å…ƒç´ æ˜¯ç»´åº¦ä¹‹é—´çš„åæ–¹å·®ã€‚ æˆ‘ä»¬å®šä¹‰åæ–¹å·®ä¸º$\\Sigma$, çŸ©é˜µå†…çš„å…ƒç´ $\\Sigma_{ij}$ä¸º: $$\\Sigma_{ij} = cov(X_i,X_j) = E[(X_i - E(X_i)) (X_j - E(X_j))]$$ åˆ™åæ–¹å·®çŸ©é˜µä¸º: $$\\Sigma = E[(X-E(X)) (X-E(X))^T] = \\left[ \\begin{array}{cccc} cov(X_1,X_1) &amp; cov(X_1,X_2) &amp; \\cdots &amp; cov(X_1,X_n) \\ cov(X_2,X_1) &amp; cov(X_2,X_2) &amp; \\cdots &amp;cov(X_2,X_n) \\ \\vdots &amp; \\vdots&amp; \\vdots &amp; \\vdots \\ cov(X_n,X_1) &amp; cov(X_n,X_2,)&amp;\\cdots&amp; cov(X_n,X_n) \\end{array} \\right]$$å¦‚æœX~$N(\\mu,\\Sigma)$,åˆ™$Cov(X)=\\Sigma$ å¯ä»¥è¿™ä¹ˆç†è§£åæ–¹å·®ï¼Œå¯¹äºnç»´éšæœºå˜é‡Xï¼Œç¬¬ä¸€ç»´æ˜¯ä½“é‡$X_1$ï¼Œç¬¬äºŒç»´æ˜¯é¢œå€¼$X_2$ï¼Œæ˜¾ç„¶è¿™ä¸¤ä¸ªç»´åº¦æ˜¯æœ‰ä¸€å®šè”ç³»çš„ï¼Œå°±ç”¨$cov(X_1,X_2)$æ¥è¡¨å¾ï¼Œè¿™ä¸ªå€¼è¶Šå°ï¼Œä»£è¡¨ä»–ä»¬è¶Šç›¸ä¼¼ã€‚åæ–¹å·®æ€ä¹ˆæ±‚ï¼Œå‡è®¾æœ‰mä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆæ‰€æœ‰çš„æ ·æœ¬çš„ç¬¬ä¸€ç»´å°±æ„æˆ$X_1$â€¦ä¸è¦æŠŠ$X_1$å’Œæ ·æœ¬ææ··æ·†äº†ã€‚ äº†è§£äº†å¤šç»´æ­£æ€åˆ†å¸ƒå’Œåæ–¹å·®ï¼Œæˆ‘ä»¬å†å›åˆ°ç”Ÿæˆæ¨¡å‹p(x|y)ã€‚ã€‚å…¶å®æˆ‘ä»¬å°±æ˜¯å‡è®¾å¯¹äºnç»´ç‰¹å¾ï¼Œp(x|y)æ˜¯nç»´æ­£æ€åˆ†å¸ƒï½æ€ä¹ˆç†è§£å‘¢ï¼Œä¸‹é¢å°±è¯´ï¼ é«˜æ–¯åˆ¤åˆ«åˆ†ææ¨¡å‹The Gaussian Discriminant Analysis modelé«˜æ–¯åˆ¤åˆ«æ¨¡å‹å°±æ˜¯ï¼šå‡è®¾p(x|y)æ˜¯ä¸€ä¸ªå¤šç»´æ­£æ€åˆ†å¸ƒï¼Œä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆå‡è®¾å‘¢ï¼Ÿå› ä¸ºå¯¹äºç»™å®šyçš„æ¡ä»¶ä¸‹å¯¹åº”çš„ç‰¹å¾xéƒ½æ˜¯ç”¨æ¥æè¿°è¿™ä¸€ç±»yçš„ï¼Œæ¯”å¦‚ç‰¹å¾æ˜¯nç»´çš„ï¼Œç¬¬ä¸€ç»´æè¿°èº«é«˜ï¼Œä¸€èˆ¬éƒ½æ˜¯æ»¡è¶³æ­£æ€åˆ†å¸ƒçš„å§ï¼Œç¬¬äºŒç»´æè¿°ä½“é‡ï¼Œä¹Ÿå¯è®¤ä¸ºæ˜¯æ­£æ€åˆ†å¸ƒå§ï½ åˆ™ç”Ÿæˆæ¨¡å‹ï¼š y ~ Bernoulli($\\phi)$ ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œåˆç§°ä¸¤ç‚¹åˆ†å¸ƒï¼Œ0-1åˆ†å¸ƒ x|y=0 ~ $N(u_0,\\Sigma)$ x|y=1 ~ $N(u_1,\\Sigma)$ è¿™é‡Œå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ï¼Œy=0å’Œy=1,å¯ä»¥çœ‹ä½œæ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œåˆ™$p(y)=\\phi^y(1-\\phi)^{1-y}$ï¼Œè¦å­¦çš„å‚æ•°ä¹‹ä¸€: $\\phi=p(y=1)$ï¼Œè¯•æƒ³å¦‚æœæ˜¯å¤šåˆ†ç±»å‘¢ï¼Œé‚£ä¹ˆè¦å­¦ä¹ çš„å‚æ•°å°±æœ‰$\\phi_1,\\phi_2,â€¦.\\phi_k$ å…¶ä¸­ç±»åˆ«å¯¹åº”çš„ç‰¹å¾x|y=0,x|y=1æœä»æ­£æ€åˆ†å¸ƒã€‚æ€ä¹ˆç†è§£å‘¢ï¼Ÿå°±æ˜¯æ—¢ç„¶ä½ ä»¬éƒ½æ˜¯ä¸€ç±»äººï¼Œé‚£ä¹ˆä½ ä»¬çš„èº«é«˜å•Šï¼Œä½“é‡å•Šç­‰ç­‰åº”è¯¥æ»¡è¶³æ­£æ€åˆ†å¸ƒã€‚ã€‚æœ‰å‡ ç»´ç‰¹å¾å°±æ»¡è¶³å‡ ç»´æ­£æ€åˆ†å¸ƒ è¿™é‡Œxæ˜¯nç»´ç‰¹å¾ï¼Œèº«é«˜ï¼Œä½“é‡ï¼Œé¢œå€¼â€¦balabalaï¼Œæ‰€ä»¥x|y=0æ»¡è¶³nç»´æ­£æ€åˆ†å¸ƒï½x|y=1ä¹Ÿæ˜¯å•¦ï¼Œåªä¸è¿‡å¯¹äºä¸åŒçš„ç±»ï¼Œå¯¹åº”nç»´ç‰¹å¾çš„å‡å€¼ä¸ä¸€æ ·ï¼Œå¥‡æ€ªä¸ºä»€ä¹ˆåæ–¹å·®çŸ©é˜µæ˜¯ä¸€æ ·çš„ï¼Ÿï¼Ÿè¿™é‡Œæ˜¯å°†å®ƒç‰¹æ®ŠåŒ–äº†ï¼Œåé¢ä¼šè®²çš„ä¸€èˆ¬æ€§çš„emç®—æ³•å°±ä¸æ˜¯è¿™æ ·çš„äº† æ¯ä¸ªåˆ†ç±»å¯¹åº”çš„nç»´ç‰¹å¾çš„åˆ†å¸ƒæ˜¾ç„¶ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œæ¯”å¦‚ä½“é‡å’Œé¢œå€¼è¿˜æ˜¯æœ‰å…³ç³»çš„å§ï½ä»–ä»¬çš„åæ–¹å·®ï¼Œæ–¹å·®å°±ç»Ÿç»Ÿéƒ½åœ¨$\\Sigma$åæ–¹å·®çŸ©é˜µé‡Œé¢äº† $$p(x|y=0) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_0) ^T \\Sigma^{-1} (x - \\mu_0) \\right]$$ $$p(x|y=1) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_1) ^T \\Sigma^{-1} (x - \\mu_1) \\right]$$ è¿™æ ·ï¼Œæ¨¡å‹ä¸­æˆ‘ä»¬è¦å­¦ä¹ çš„å‚æ•°æœ‰$\\phi,\\Sigma, \\mu_0,\\mu_1$ï¼Œå¯¹äºè®­ç»ƒæ•°æ®ï¼Œå°±æ˜¯è§‚æµ‹åˆ°çš„æ•°æ®x,yï¼Œæ—¢ç„¶ä»–ä»¬å‡ºç°äº†ï¼Œé‚£ä¹ˆä»–ä»¬çš„è”åˆæ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯ä¼¼ç„¶å‡½æ•°$\\prod_{i=1}^mp(x,y)$å°±è¦æœ€å¤§ï½å…¶å¯¹æ•°ä¼¼ç„¶log-likelihoodï¼š $$\\begin{equation}\\begin{aligned} L(\\phi,\\Sigma, \\mu_0,\\mu_1) &amp;= log\\prod_{i=1}^mp(x^{(i)},y^{(i)};\\phi,\\Sigma, \\mu_0,\\mu_1$) \\ &amp;= log\\prod_{i=1}^mp(x^{(i)}|y^{(i)};\\phi,\\Sigma, \\mu_0,\\mu_1$) p(y^{(i)};\\phi)\\ \\end{aligned}\\end{equation}\\label{eq2}$$ å…¶ä¸­$p(y^{(i)};\\phi)$æ˜¯å·²çŸ¥çš„ï¼Œä¹Ÿå°±æ˜¯å…ˆéªŒæ¦‚ç‡(class priors)ï¼Œ$p(x^{(i)}|y^{(i)})$å°±æ˜¯ä¸Šé¢æ¨å¯¼çš„ï½ä»£å…¥åï¼Œåˆ†åˆ«å¯¹å‚æ•°æ±‚å¯¼å³å¯ï¼š åœ¨å›è¿‡å¤´æ¥çœ‹è¿™äº›å…¬å¼ï¼Œ $\\phi$å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æ ·æœ¬ä¸­æ­£åˆ†ç±»çš„æ¦‚ç‡ã€‚ $\\mu_0$å°±æ˜¯è´Ÿåˆ†ç±»ä¸­xå¯¹åº”çš„å‡å€¼ $\\mu_1$å°±æ˜¯æ­£åˆ†ç±»ä¸­xå¯¹åº”çš„å‡å€¼ $\\Sigma$å°±æ˜¯$(x-\\mu_1)$å’Œ$x-\\mu_2$çš„åæ–¹å·®çŸ©é˜µ ç„¶åé€šè¿‡p(x|y=0),p(x|y=1)å³å¯å¯¹éœ€è¦é¢„æµ‹çš„xæ±‚å‡ºå¯¹åº”çš„æ¦‚ç‡ï¼Œç„¶ååšå‡ºåˆ¤åˆ«äº†ã€‚è¿™æ ·çœ‹æ¥ï¼Œå¦‚æœç›´æ¥å¯¹x|y=1,å’Œx|y=0åšå‡ºäº†æ­£æ€åˆ†å¸ƒçš„çŒœæµ‹ï¼Œå°±å¯ä»¥ç›´æ¥å†™å‡ºæ¥äº†ã€‚åªä¸è¿‡ï¼Œæˆ‘ä»¬ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡é‡æ–°æ¨å¯¼äº†ä¸€éã€‚ é«˜æ–¯æ··åˆæ¨¡å‹GMMGMMå‰é¢GDAæ˜¯æœ‰æ ‡ç­¾çš„ï¼Œä¹Ÿç®—æ˜¯æœ‰ç›‘ç£å­¦ä¹ ã€‚è€Œåœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹å‘¢ï¼Œå°±æ˜¯æ— ç›‘ç£å­¦ä¹ äº†ï¼Œè™½ç„¶æˆ‘ä»¬æ— æ³•ç»™å‡ºxæ‰€å±çš„ç±»å«å•¥ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥åˆ¤æ–­å‡ºå“ªäº›xæ˜¯åŒä¸€ç±»ï¼Œä»¥åŠæ ·æœ¬ä¸­æ€»å…±æœ‰å¤šå°‘ç±»ï¼ˆè™½ç„¶è¿™ä¸ªç±»æ•°å˜›ã€‚ã€‚ç±»ä¼¼äºk-meansçš„ç±»æ•°ï¼Œå¯æ ¹æ®äº¤å‰éªŒè¯é€‰æ‹©ï¼‰ã€‚ å…¶å®å’ŒGDAéå¸¸ç›¸ä¼¼ï¼Œä¸è¿‡è¿™é‡Œæ²¡æœ‰äº†ç±»æ ‡ç­¾ï¼Œåªæœ‰ä¸€å †æ ·æœ¬ç‰¹å¾ï¼Œ${x^{(1)},x^{(2)},â€¦,x^{(m)}}$, æˆ‘ä»¬ä¸çŸ¥é“è¿™äº›æ ·æœ¬å±äºå‡ ä¸ªç±»åˆ«ï¼Œä¹Ÿä¸çŸ¥é“æœ‰å“ªäº›ç±»äº†ã€‚ä½†è™½ç„¶ä¸çŸ¥é“ï¼Œæˆ‘ä»¬ç¡®å®šä»–ä»¬æ˜¯å­˜åœ¨çš„ï¼Œåªæ˜¯çœ‹ä¸è§è€Œå·²ã€‚æˆ‘ä»¬å¯ä»¥å‡è®¾å­˜åœ¨kç±»ï¼Œ${z^{(1)},z^{(2)},â€¦,z^{(k)}}$,çœ‹ä¸è§çš„ï¼Œæˆ‘ä»¬å°±å«å®ƒä»¬éšè—éšæœºå˜é‡(latent random variable)ï¼Œ è¿™æ ·ä¸€æ¥ï¼Œå°±è®­ç»ƒæ ·æœ¬å°±å¯ä»¥ç”¨è¿™æ ·çš„è”åˆåˆ†æ¦‚ç‡æ¨¡å‹è¡¨ç¤ºäº†ï¼Œ$p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})$ åŒGDAä¸ä¸€æ ·çš„æ˜¯ï¼Œè¿™é‡Œæ˜¯å¤šåˆ†ç±»ï¼Œå¯å‡å®š$z^{(i)}\\sim Multinomial(\\phi)$ï¼Œå¤šé¡¹å¼åˆ†å¸ƒï¼ˆäºŒé¡¹åˆ†å¸ƒçš„æ‹“å±•ï½ï¼‰ï¼Œé‚£ä¹ˆ$p(z^{(i)})=\\phi_j$ åŒGDAç›¸åŒçš„æ˜¯ï¼Œå¯¹äºæ¯ä¸€ä¸ªç±»åˆ«ï¼Œå…¶å¯¹åº”çš„æ ·æœ¬æ»¡è¶³nç»´æ­£æ€åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯ï¼š$x^{(i)}|z^{(i)}=j\\sim N(\\mu_j,\\Sigma_j)$,ä½†æ³¨æ„å“¦ï¼Œè¿™é‡Œæ¯ä¸ªé«˜æ–¯åˆ†å¸ƒä½¿ç”¨äº†ä¸åŒçš„åæ–¹å·®çŸ©é˜µ$\\Sigma_j$ $$p(x|z^{(1)}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_0|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_0) ^T \\Sigma_0^{-1} (x - \\mu_0) \\right]$$ $$p(x|z^{(2)}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_1|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_1) ^T \\Sigma_1^{-1} (x - \\mu_1) \\right]$$ $$â€¦.$$ $$p(x|z^{(k)}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_k|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_k) ^T \\Sigma_k^{-1} (x - \\mu_k) \\right]$$ ç„¶åå¸¦å…¥åˆ°è®­ç»ƒæ ·æœ¬çš„å¯¹æ•°ä¼¼ç„¶ï¼ˆlog-likelihoodï¼‰: $$L(\\phi,\\mu,\\Sigma)=\\sum_{i=1}^{m}logp(x^{(i)};\\phi,\\mu,\\Sigma)$$ $$L(\\phi,\\mu,\\Sigma)=\\sum_{i=1}^{m}log\\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)};\\mu,\\Sigma) p(z^{(i)};\\phi)\\$$ è¿™é‡Œéœ€è¦æ³¨æ„ä¸‹æ ‡ï¼šå¯¹äºç±»åˆ«æœ‰kç±»ï¼Œç¬¬ä¸€ä¸ªæ±‚å’Œç¬¦å·æ˜¯å¯¹ç¬¬iä¸ªæ ·æœ¬åœ¨kä¸ªç±»åˆ«ä¸Šçš„è”åˆæ¦‚ç‡ï¼Œç¬¬äºŒä¸ªæ±‚å’Œç¬¦å·æ˜¯mä¸ªæ ·æœ¬çš„è”åˆæ¦‚ç‡ã€‚ æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“$z^{(i)}$,é‚£ä¹ˆè¿™ä¸ªä¼¼ç„¶å‡½æ•°æ±‚æå¤§å€¼å°±å¾ˆå®¹æ˜“äº†ï¼Œç±»ä¼¼äºé«˜æ–¯åˆ¤åˆ«åˆ†æï¼Œè¿™é‡Œçš„$z^{(i)}$ç›¸å½“äºæ ‡ç­¾ï¼Œåˆ†åˆ«å¯¹å‚æ•°æ±‚å¯¼å¯å¾—ï¼š å…¶ä¸­çš„å‚æ•°: $1{z^{(i)}=j}$è¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬ä¸ºjç±»æ—¶ï¼Œè¿™ä¸ªå€¼å°±ä¸ºï¼‘ï¼Œé‚£ä¹ˆ$\\phi_j=\\frac{1}{m}\\sum_{i=1}^m1{z^{(i)}=j}$è¡¨ç¤ºæ ·æœ¬ä¸­ç±»åˆ«ä¸ºjçš„æ¦‚ç‡ å…¶ä¸­$p(z^{(i)};\\phi)$æ˜¯æ ¹æ®ä¼¯åŠªåˆ©åˆ†å¸ƒå¾—åˆ°çš„ï¼Œåœ¨GDAä¸­$p(y|\\phi)$æ˜¯å·²çŸ¥çš„é¢‘ç‡æ¦‚ç‡ã€‚ So $z^{(i)}$ åˆ°åº•æœ‰å¤šå°‘ä¸ªåˆ†ç±»ï¼Ÿæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿè­¬å¦‚ä¸Šå¼ä¸­ $\\sum_{i=1}^{m}1{z^{(i)}=j}$ è¿™ä¸ªæ²¡æ³•æ±‚å¯¹å§ï½å®ƒæ˜¯éšè—å˜é‡ï¼æ‰€ä»¥è¿˜æ˜¯æŒ‰ç…§è¿™ä¸ªæ–¹æ³•æ˜¯æ±‚ä¸å‡ºæ¥çš„ï½ è¿™ä¸ªæ—¶å€™EMç®—æ³•å°±ç™»åœºäº†ï½ï½ï½ ç”¨EMç®—æ³•æ±‚è§£GMMæ¨¡å‹ä¸Šé¢ä¹Ÿæåˆ°äº†ï¼Œå¦‚æœ$z^({i})$æ˜¯å·²çŸ¥çš„è¯ï¼Œé‚£ä¹ˆ$\\phi_j=\\frac{1}{m}\\sum_{i=1}^m1{z^{(i)}=j}$è¡¨ç¤ºç±»åˆ«jçš„æ¦‚ç‡$p(z^{(i)}=j)$ä¹Ÿå°±å·²çŸ¥äº†ï¼Œä½†æ˜¯å‘¢ï¼Ÿæˆ‘ä»¬ä¸çŸ¥é“ã€‚ã€‚æ‰€ä»¥æˆ‘ä»¬è¦çŒœæµ‹$p(z^{(i)}=j)$è¿™ä¸ªå€¼ï¼Œä¹Ÿå°±æ˜¯EMç®—æ³•çš„ç¬¬ä¸€æ­¥ï¼š Repeat until convergence è¿­ä»£ç›´åˆ°æ”¶æ•›:{ (E-step):for each i,j,set: $w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)$ $w_j^{(i)}$ä»€ä¹ˆæ„æ€å‘¢?å°±æ˜¯å¯¹äºiæ ·æœ¬,å®ƒæ˜¯jç±»çš„åéªŒæ¦‚ç‡ã€‚åœ¨GDAé‡Œé¢ï¼Œx_içš„ç±»åˆ«æ˜¯ç¡®å®šçš„ï¼Œåœ¨GMMé‡Œé¢å‘¢ï¼Ÿä¸çŸ¥é“å®ƒçš„ç±»åˆ«ï¼Œæ‰€ä»¥åªèƒ½å‡è®¾kç±»éƒ½æœ‰å¯èƒ½ï¼Œå®ƒæ˜¯jç±»åˆ«çš„æ¦‚ç‡å°±æ˜¯$w_j^{(i)}$ï¼Œå®ƒä»…ä»…å–å†³äº$\\phi_j$,è€Œåœ¨GMMé‡Œé¢ï¼Œå®ƒå–å†³äº$\\phi_j,\\mu_j,\\Sigma_j$ï¼Œå®é™…ä¸Š$w_j^{(i)}$çš„å€¼ï¼Œå°±åŒ…å«äº†ä¸¤ä¸ªæˆ‘ä»¬åœ¨GMMæ‰€åšçš„å‡è®¾ï¼Œå¤šé¡¹å¼åˆ†å¸ƒå’Œæ­£æ€åˆ†å¸ƒã€‚ The values $w_j$ calculated in the E-step represent our â€œsoftâ€ guesses for the values of $z^{(i)}$ . The term â€œsoftâ€ refers to our guesses being probabilities and taking values in [0, 1]; in contrast, a â€œhardâ€ guess is one that represents a single best guess (such as taking values in {0, 1} or {1, . . . , k}). ç¡¬çŒœæµ‹æ˜¯kå‡å€¼èšç±»ï¼ŒGMMæ˜¯è½¯çŒœæµ‹ã€‚ è¿™æ ·ä¸€æ¥ï¼Œå‚æ•°æ›´æ–°å°±å¯ä»¥è¿™æ ·å†™äº†ï¼Œä¹Ÿå°±æ˜¯EMç®—æ³•çš„ç¬¬äºŒæ­¥ï¼š (M-step) Updata the parameters: ç„¶åå¯¹ä¼¼ç„¶å‡½æ•°æ±‚å¯¼ï¼Œåé¢ä¼šè¯¦ç»†ä»‹ç» $$\\phi_j:=\\frac{1}{m}\\sum_{i=1}^mw_j^{(i)}$$ $$\\mu_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\\sum_{i=1}^mw_j^{(i)}}$$ $$\\Sigma_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T}{\\sum_{i=1}^mw_j^{(i)}}$$ ï½ è®­ç»ƒè¿‡ç¨‹çš„ç†è§£å¯å‚è€ƒblog $w_j^{(i)}è¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬ä¸ºï½Šç±»åˆ«çš„æ¦‚ç‡ï¼Œè€Œ\\phi_j$è¡¨ç¤ºmä¸ªæ ·æœ¬ä¸­jç±»åˆ«çš„æ¦‚ç‡ï¼Œ$\\mu_j,\\Sigma_j$åˆ†åˆ«è¡¨ç¤ºjç±»åˆ«å¯¹åº”çš„nç»´é«˜æ–¯åˆ†å¸ƒçš„æœŸæœ›å’Œåæ–¹å·®çŸ©é˜µ æ‰€ä»¥ï¼Œæ±‚å‡º$w_j^{(i)}$ï¼Œä¸€åˆ‡å°±éƒ½è§£å†³äº†å§ï¼Ÿå¯¹äºåéªŒæ¦‚ç‡$p(z^{(i)}=j|x^{(i)})$å¯ä»¥æ ¹æ®Bayeså…¬å¼ï¼š $$p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)=\\dfrac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}{\\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\\mu,\\Sigma)p(z^{(i)}=l;\\phi)}$$ å…¶ä¸­å…ˆéªŒæ¦‚ç‡$p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)$å¯ä»¥æ ¹æ®é«˜æ–¯åˆ†å¸ƒçš„å¯†åº¦å‡½æ•°æ¥æ±‚ï¼Œ$z^{(i)}=j\\sim N(\\mu_j,\\Sigma_j)$ ç±»å…ˆéªŒ(class priors)$p(z^{(i)}=j;\\phi)$å¯ä»¥å–å†³äºå¤šé¡¹å¼åˆ†å¸ƒä¸­jç±»çš„æ¦‚ç‡$\\phi_j$ The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the â€œhardâ€ cluster assignments $c^(i)$, we instead have the â€œsoftâ€ assignments $w_j$ . Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea. EMç®—æ³•ä½¿æˆ‘ä»¬è”æƒ³èµ·äº†k-means,åŒºåˆ«åœ¨äºk-meansçš„èšç±»æ˜¯é€šè¿‡æ¬§æ°è·ç¦»c(i)æ¥å®šä¹‰çš„ï¼Œè€ŒEMæ˜¯é€šè¿‡$w_j$probabilitiesæ¥åˆ†ç±»çš„ã€‚åŒk-meansä¸€æ ·ï¼Œè¿™é‡Œçš„EMç®—æ³•ä¹Ÿæ˜¯å±€éƒ¨ä¼˜åŒ–ï¼Œå› æ­¤æœ€å¥½é‡‡ç”¨ä¸åŒçš„æ–¹å¼åˆå§‹åŒ–ï½ convergence?æˆ‘ä»¬çŸ¥é“k-meansä¸€å®šæ˜¯æ”¶æ•›çš„ï¼Œè™½ç„¶ç»“æœä¸ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜è§£ï¼Œä½†å®ƒæ€»èƒ½è¾¾åˆ°ä¸€ä¸ªæœ€ä¼˜è§£ã€‚ä½†æ˜¯EMç®—æ³•å‘¢ï¼Œä¹Ÿæ˜¯æ”¶æ•›çš„ã€‚ The EM algorithmå‰é¢æˆ‘ä»¬è®²çš„æ˜¯åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹çš„EMç®—æ³•ï¼Œä½†ä¸€å®šæ‰€æœ‰çš„ç±»åˆ«éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒå—ï¼Ÿè¿˜æœ‰å¡æ–¹åˆ†å¸ƒï¼Œæ³Šæ¾åˆ†å¸ƒç­‰ç­‰å‘¢ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°±å°†è®¨è®ºEMç®—æ³•çš„ä¸€èˆ¬æ€§ã€‚ åœ¨å­¦ä¹ ä¸€èˆ¬æ€§çš„EMç®—æ³•å‰ï¼Œå…ˆäº†è§£ä¸‹Jensenâ€™s inequality Jensenâ€™s inequalityå¦‚æœå‡½æ•°$f$ï¼Œå…¶äºŒé˜¶å¯¼æ’å¤§ä¸ç­‰äº0 $(f^{â€˜â€™}\\ge 0)$ï¼Œåˆ™å®ƒæ˜¯å‡¸å‡½æ•°f(convec function)ã€‚ å¦‚æœå‡¸å‡½æ•°çš„è¾“å…¥æ˜¯å‘é‡vector-valued inputsï¼Œé‚£ä¹ˆå®ƒçš„æµ·æ£®çŸ©é˜µ(hessian)Hæ˜¯åŠæ­£å®šçš„ã€‚Jensenâ€™s ä¸ç­‰å¼ï¼š Let f be a convex function, and let X be a random variable. Then: $$E[f (X)] â‰¥ f (EX).$$ Moreover, if f is strictly convex, then $E[f (X)] = f (EX)$ holds true if and only if $X = E[X]$ with probability 1 (i.e., if X is a constant). ä¸¾ä¸ªæ —å­æ¥è§£é‡Šjensenä¸ç­‰å¼ï¼š å‡è®¾è¾“å…¥éšæœºå˜é‡Xæ˜¯ä¸€ç»´çš„å“ˆï¼Œç„¶åï¼¸å–a,bçš„æ¦‚ç‡éƒ½æ˜¯0.5,é‚£ä¹ˆ $$EX=(a+b)/2,f(EX)=f(\\dfrac{a+b}{2})$$,$$E[f(X)]=\\dfrac{f(a)+f(b)}{2}$$ å› ä¸ºæ˜¯å‡¸å‡½æ•°ï¼Œæ‰€ä»¥ $f(EX)\\le E[f(X)]$ åŒç†ï¼Œå¦‚æœæ˜¯å‡¹å‡½æ•°(concave function),é‚£ä¹ˆä¸ç­‰å¼æ–¹å‘ç›¸å$f(EX)\\ge E[f(X)]$ã€‚åé¢EMç®—æ³•é‡Œé¢å°±è¦ç”¨åˆ°log(X)ï¼Œlog(x)å°±æ˜¯ä¸ªå…¸å‹çš„å‡¹å‡½æ•°ï½ The EM algorithmé¦–å…ˆï¼Œé—®é¢˜æ˜¯ï¼šæˆ‘ä»¬è¦åŸºäºç»™å®šçš„mä¸ªè®­ç»ƒæ ·æœ¬${x^{(1)},x^{(2)},â€¦,x^{(m)}}$æ¥è¿›è¡Œå¯†åº¦ä¼°è®¡ï½ åƒå‰é¢ä¸€æ ·ï¼Œåˆ›å»ºä¸€ä¸ªå‚æ•°æ¨¡å‹p(x,z)æ¥æœ€å¤§åŒ–è®­ç»ƒæ ·æœ¬çš„å¯¹æ•°ä¼¼ç„¶ï¼š $$L(\\theta)=\\sum_{i=1}^mlogp(x;\\theta)$$ $$L(\\theta)=\\sum_{i=1}^mlog\\sum_zp(x,z;\\theta)$$ ä¸€èˆ¬æ€§å°±æ˜¯æŠŠå‰é¢ç‰¹æ®ŠåŒ–çš„å‡è®¾å»æ‰ï¼Œæ²¡æœ‰äº†æ­£æ€åˆ†å¸ƒå’Œå¤šé¡¹å¼åˆ†å¸ƒã€‚ å¯ä»¥çœ‹åˆ°ï¼Œ$z^{(i)}$æ˜¯éšè—çš„éšæœºå˜é‡(latent random variable),å…³äºå‚æ•°$\\theta$çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡å°±å¾ˆéš¾è®¡ç®—äº†ã€‚ è§£é‡Šä¸‹å…¬å¼ä¸­çš„æ¨å¯¼ï¼š è¿™é‡Œæ˜¯é’ˆå¯¹æ ·æœ¬iæ¥è¯´ï¼Œå¯¹äºæ ·æœ¬iï¼Œå®ƒå¯èƒ½æ˜¯$z^1,z^2,â€¦,z^k$éƒ½æœ‰å¯èƒ½ï¼Œä½†ä»–ä»¬çš„probabilityä¹‹å’Œä¸ºï¼‘ï¼Œä¹Ÿå°±æ˜¯ $\\sum_zQ_i(z)=1$ (2)åˆ°(3)çš„æ¨å¯¼ï¼šå¯ä»¥å°† $\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}$ çœ‹åšéšæœºå˜é‡ï¼¸,é‚£ä¹ˆï¼ˆï¼’ï¼‰å¼ä¸­çš„ååŠéƒ¨åˆ† $log\\sum_{z^{(i)})}[\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}]$å°±æ˜¯log(EX)äº†ï¼Œ$logx$æ˜¯ä¸€ä¸ªå‡¹å‡½æ•°ï¼Œåˆ™å…¶å¤§äº$E[log(x)]$ EMè¿­ä»£è¿‡ç¨‹(é‡ç‚¹): ï¼ˆ1ï¼‰æ ¹æ®ä¸Šå¼å¯ä»¥çœ‹åš$L(\\theta)\\ge J(Q,\\theta)$.ä¸¤è¾¹éƒ½æ˜¯å…³äº$\\theta$çš„å‡½æ•°ï¼Œé‚£ä¹ˆå°†$\\theta$å›ºå®šï¼Œè°ƒæ•´Qåœ¨ä¸€å®šæ¡ä»¶ä¸‹èƒ½ä½¿ç­‰å¼æˆç«‹ã€‚ ï¼ˆ2ï¼‰ç„¶åå›ºå®šQ,è°ƒæ•´$\\theta^t$åˆ°$\\theta^{t+1}$æ‰¾åˆ°ä¸‹ç•Œå‡½æ•°çš„æœ€å¤§å€¼$J(Q,\\theta^{t+1})$.æ˜¾ç„¶åœ¨å½“å‰Qçš„æ¡ä»¶ä¸‹ï¼Œ$L(\\theta^{t+1})\\ne J(Q,\\theta^{t+1})$,é‚£ä¹ˆæ ¹æ®Jensenä¸ç­‰å¼ï¼Œ$L(\\theta_{t+1})&gt;J(Q,\\theta^{t+1})=L(\\theta^{t})$,ä¹Ÿå°±æ˜¯è¯´æ‰¾åˆ°äº†ä½¿å¾—å¯¹æ•°ä¼¼ç„¶Læ›´å¤§çš„$\\theta$.è¿™ä¸å°±æ˜¯æˆ‘ä»¬çš„ç›®çš„å—ï¼Ÿï¼ ç„¶åè¿­ä»£å¾ªç¯(1)(2)æ­¥éª¤ï¼Œç›´åˆ°åœ¨è°ƒæ•´$\\theta$æ—¶ï¼Œä¸‹ç•Œå‡½æ•°$J(Q,\\theta)$ä¸åœ¨å¢åŠ ï¼Œå³å°äºæŸä¸ªé˜ˆå€¼ã€‚ çœ‹ä¸‹Ngç”»çš„å›¾ï¼š ä»»æ„åˆå§‹åŒ–$\\theta$å’ŒQ,ç„¶åæ‰¾ä¸‹ç•Œå‡½æ•°å’Œ$l(\\theta)$äº¤æ¥çš„ç‚¹ï¼Œè¿™å°±æ˜¯EMç®—æ³•çš„ç¬¬ä¸€æ­¥ï¼š æˆ‘ä»¬è¦è®©ä¸ç­‰å¼ç›¸ç­‰,å³Jensenâ€™s inequalityä¸­çš„éšæœºå˜é‡å–å€¼æ˜¯ä¸€ä¸ªå¸¸é‡ï¼Œçœ‹(2)å¼ï¼š $$\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c$$ å¯¹å·¦è¾¹åˆ†å­åˆ†æ¯åŒæ—¶å¯¹zæ±‚ç±»å’Œï¼š $$\\dfrac{\\sum_zp(x^{(i)},z^{(i)};\\theta)}{\\sum_zQ_i(z^{(i)})}=c$$ æ ¹æ®$\\sum_zQ_i(z)=1$ï¼š $$\\sum_zp(x^{(i)},z^{(i)};\\theta)=c$$ å¸¦å›å»å¯å¾—ï¼š $$Q_i(z^{(i)})=\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{\\sum_zp(x^{(i)},z^{(i)};\\theta)}$$ $$Q_i(z^{(i)})=\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{p(x^{(i)};\\theta)}$$ $$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\\theta)$$ EMæ€»ç»“ä¸‹æ¥ï¼š Repeat until convergence { (E-step) For each i,æ‰¾åˆ°ä¸‹ç•Œå‡½æ•°, set: $$Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\\theta)$$ (M-step)æ‰¾åˆ°ä¸‹ç•Œå‡¹å‡½æ•°çš„æœ€å¤§å€¼,ä¹Ÿå°±æ˜¯(3)å¼ Set: $$\\theta:=arg\\max_{\\theta}\\sum_i^m\\sum_{z^{(i)}}^kQ_i(z^{(i)})log\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}$$ } è¦ç†è§£çš„æ˜¯ï¼š EMç®—æ³•åªæ˜¯ä¸€ç§è®¡ç®—æ–¹å¼ï¼Œå¯¹äºä¸Šå¼ä¸­çš„$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\\theta)$æˆ‘ä»¬è¿˜æ˜¯è¦æ ¹æ®å‡è®¾æ¥æ±‚å¾—ï¼Œæ¯”å¦‚GMMä¸­çš„å¤šç±»é«˜æ–¯åˆ†å¸ƒã€‚ç„¶åå¸¦å›åˆ°å¯¹æ•°ä¼¼ç„¶ä¸­ï¼Œé€šè¿‡æ±‚å¯¼å¾—åˆ°å‚æ•°ä¼°è®¡ã€‚æˆ‘ä»¬è´¹å°½å¿ƒæœºè¯æ˜EMç®—æ³•æ”¶æ•›ï¼Œåªæ˜¯ä¸ºäº†å»è¯æ˜è¿™æ ·å»æ±‚ä¼¼ç„¶å‡½æ•°çš„æå¤§å€¼æ˜¯å¯è¡Œçš„ï¼Œç„¶ååº”ç”¨åˆ°ç±»ä¼¼äºGMMï¼ŒHMMä¸­ã€‚ training and will converge?é¦–å…ˆè¯´æ˜¯å¦æ”¶æ•›ï¼Œç­”æ¡ˆæ˜¯è‚¯å®šæ”¶æ•›çš„ã€‚ã€‚æ‡’å¾—è¾“å…¬å¼äº†ã€‚ã€‚ç›´æ¥è´´å›¾å§ï¼Œè¿™ä¸ªæ¯”è¾ƒå¥½ç†è§£ï¼š ä¸Šé¢å†™è¿™ä¹ˆå¤šï¼Œå…¶å®å°±æ˜¯è¯æ˜$L(\\theta_{t+1})&gt;L(\\theta_t)$. Mixture of Gaussians revisitedæˆ‘ä»¬çŸ¥é“äº†emç®—æ³•æ˜¯ä¸€ç§è®¡ç®—æ–¹å¼ï¼Œç”¨æ¥è§£å†³å«æœ‰éšå˜é‡ä¼¼ç„¶å¯¹æ•°å¾ˆéš¾æ±‚çš„é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬æŠŠå®ƒè¿ç”¨åˆ°GMMä¸­ã€‚ E step: $w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)$ $$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)=\\dfrac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}{\\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\\mu,\\Sigma)p(z^{(i)}=l;\\phi)}$$ å…¶ä¸­å…ˆéªŒæ¦‚ç‡$p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)$å¯ä»¥æ ¹æ®é«˜æ–¯åˆ†å¸ƒçš„å¯†åº¦å‡½æ•°æ¥æ±‚ï¼Œ$z^{(i)}=j\\sim N(\\mu_j,\\Sigma_j)$ ç±»å…ˆéªŒ(class priors)$p(z^{(i)}=j;\\phi)$å¯ä»¥å–å†³äºå¤šé¡¹å¼åˆ†å¸ƒä¸­jç±»çš„æ¦‚ç‡$\\phi_j$ è¿™æ ·æˆ‘ä»¬å°±å®Œæˆäº†å¯¹$w_j^{(i)}$çš„soft â€˜guessâ€™ï¼Œä¹Ÿå°±æ˜¯E step. M step: ç„¶åå¯¹å‚æ•°æ±‚å¯¼ï¼š è¯¦ç»†æ¨å¯¼è¿‡ç¨‹ï¼Œå‚è€ƒcs229-notes8 æˆ‘ä»¬åœ¨æ•´ä½“å›é¡¾ä¸€ä¸‹æ•´ä¸ªè¿‡ç¨‹ï¼Œæ‰€è°“çš„E stepå°±æ˜¯æ‰¾åˆ°$Q_i(z^{j}),w_i^j$ï¼ˆåœ¨ä¸€å®šå‡è®¾ä¸‹æ˜¯å¯ä»¥é€šè¿‡bayeså…¬å¼æ±‚å¾—çš„ï¼‰ï¼Œä½¿å¾—ä¸‹ç•Œå‡½æ•°ä¸logå‡½æ•°ç›¸ç­‰ï¼Œä¹Ÿå°±æ˜¯Jensenå–ç­‰å·æ—¶ã€‚ç„¶åæ˜¯M stepå°±æ˜¯åœ¨Qçš„æ¡ä»¶ä¸‹æ‰¾åˆ°ä¸‹ç•Œå‡½æ•°æœ€å¤§å€¼ï¼Œä¹Ÿå°±æ˜¯å¯¹å‚æ•°æ±‚å¯¼ï¼Œå¯¼æ•°ä¸º0çš„åœ°æ–¹ã€‚ ç„¶ååœ¨æ ¹æ®æ±‚å¾—çš„å‚æ•°ï¼Œå†æ±‚Qï¼Œå†å¸¦å…¥æ±‚å¯¼ã€‚ã€‚ã€‚è¿­ä»£ç›´åˆ°æ”¶æ•›ã€‚","link":"/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/"},{"title":"cs224d-lecture2-è¯å‘é‡çš„é«˜çº§è¡¨ç¤º","text":"what do word2vec capture? window based coocurrence Matrix GloVe Intrinsic evaluation What do word2vec capture? Go through each word of the whole corpus predict surrounding words of each word word2vec captures coocurrence of words one at a time. Why not capture coocurrence counts directly? word2vecå°†çª—å£è§†ä½œè®­ç»ƒå•ä½ï¼Œæ¯ä¸ªçª—å£æˆ–è€…å‡ ä¸ªçª—å£éƒ½è¦è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚è¦çŸ¥é“ï¼Œå¾ˆå¤šè¯ä¸²å‡ºç°çš„é¢‘æ¬¡æ˜¯å¾ˆé«˜çš„ã€‚èƒ½ä¸èƒ½éå†ä¸€éè¯­æ–™ï¼Œè¿…é€Ÿå¾—åˆ°ç»“æœå‘¢ï¼Ÿ æ—©åœ¨word2vecä¹‹å‰ï¼Œå°±å·²ç»å‡ºç°äº†å¾ˆå¤šå¾—åˆ°è¯å‘é‡çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ˜¯åŸºäºç»Ÿè®¡å…±ç°çŸ©é˜µçš„æ–¹æ³•ã€‚å¦‚æœåœ¨çª—å£çº§åˆ«ä¸Šç»Ÿè®¡è¯æ€§å’Œè¯­ä¹‰å…±ç°ï¼Œå¯ä»¥å¾—åˆ°ç›¸ä¼¼çš„è¯ã€‚å¦‚æœåœ¨æ–‡æ¡£çº§åˆ«ä¸Šç»Ÿè®¡ï¼Œåˆ™ä¼šå¾—åˆ°ç›¸ä¼¼çš„æ–‡æ¡£ï¼ˆæ½œåœ¨è¯­ä¹‰åˆ†æLSA,Latent Semantic Analysisï¼‰ã€‚ Window based Co-occurrence Matrix åŸºäºçª—å£çš„å…±ç°çŸ©é˜µ Solution: Low dimensional vectors SVDçš„é—®é¢˜ï¼š è®¡ç®—å¤æ‚åº¦é«˜ï¼šå¯¹nÃ—mçš„çŸ©é˜µæ˜¯O(mn2) ä¸æ–¹ä¾¿å¤„ç†æ–°è¯æˆ–æ–°æ–‡æ¡£ ä¸å…¶ä»–DLæ¨¡å‹è®­ç»ƒå¥—è·¯ä¸åŒ Count based VS direct prediction è¿™äº›åŸºäºè®¡æ•°çš„æ–¹æ³•åœ¨ä¸­å°è§„æ¨¡è¯­æ–™è®­ç»ƒå¾ˆå¿«ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†ç»Ÿè®¡ä¿¡æ¯ã€‚ä½†ç”¨é€”å—é™äºæ•æ‰è¯è¯­ç›¸ä¼¼åº¦ï¼Œä¹Ÿæ— æ³•æ‹“å±•åˆ°å¤§è§„æ¨¡è¯­æ–™ã€‚ è€ŒNNLM, HLBL, RNN, Skip-gram/CBOWè¿™ç±»è¿›è¡Œé¢„æµ‹çš„æ¨¡å‹å¿…é¡»éå†æ‰€æœ‰çš„çª—å£è®­ç»ƒï¼Œä¹Ÿæ— æ³•æœ‰æ•ˆåˆ©ç”¨å•è¯çš„å…¨å±€ç»Ÿè®¡ä¿¡æ¯ã€‚ä½†å®ƒä»¬æ˜¾è‘—åœ°æé«˜äº†ä¸Šçº§NLPä»»åŠ¡ï¼Œå…¶æ•æ‰çš„ä¸ä»…é™äºè¯è¯­ç›¸ä¼¼åº¦ã€‚ 2. Combining the beat of both words: GloveGloVe: Global Vectors for Word Representation Gloveçš„åŸç†ï¼š Using global statistics to predict the probability of word j appearing in the context of word i with a least squares objective. å³åˆ©ç”¨äº†è¯é¢‘ç»Ÿè®¡çš„ä½œç”¨ï¼Œåˆåˆ©ç”¨äº†word2vecä¸­å‡ºç°åœ¨åŒä¸€ä¸ªçª—å£çš„ä¸¤ä¸ªè¯çš„æ¦‚ç‡ï¼Œç”¨è¯å‘é‡åšå†…ç§¯æ¥è¡¨ç¤ºã€‚ åœ¨word2vecä¸­ $$P(u_c|\\hat v) = \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{* }$$ å…±ç°çŸ©é˜µ Co-occurrence MatrixåŸºäºé¢‘ç‡ç»Ÿè®¡æ¦‚ç‡ï½ word $w_j$ å‡ºç°åœ¨ä¸­å¿ƒè¯ $w_i$ çš„ä¸Šä¸‹æ–‡ä¸­çš„æ¦‚ç‡ï¼š $$P_{ij}=P(w_j|w_i)=\\dfrac{X_{ij}}{X_i}=\\dfrac{count(w_i,w_j)}{count(w_i)}$$ Least Squares Objectiveä¾æ®word2vec skip-gramä¸­çš„åŸç†ï¼Œ$w_j$ å‡ºç°åœ¨ $w_i$ çš„ä¸Šä¸‹æ–‡ä¸­çš„æ¦‚ç‡ï¼š $$Q_{ij}=P(w_j|w_i) = \\dfrac{exp(u_j^Tv_i)}{\\sum_{j=1}^{|V|}exp(u_j^Tv_i)}$$ æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡çš„è´Ÿæ•°å½¢å¼ï¼š $$J=-\\sum_{i\\in corpus}\\sum_{j\\in context(i)}Q_{ij}$$ å¯¹äºå…±ç°è¯ $w_i, w_j$ åŒæ—¶å‡ºç°å¤šæ¬¡ï¼Œå› æ­¤å¯ä»¥å°†ä¸Šå¼ç®€åŒ–ä¸ºï¼š $$J=-\\sum_{i=1}^V\\sum_{j=1}^VX_{ij}Q_{ij}$$ æˆ‘ä»¬çŸ¥é“ $Q_{ij}$ æ˜¯è¡¨ç¤ºç»è¿‡softmaxå½’ä¸€åŒ–ä¹‹åçš„æ¦‚ç‡ï¼Œéœ€è¦éå†æ•´ä¸ªcorpusï¼Œè¿™ä¼šå¯¼è‡´å¾ˆå¤§çš„è®¡ç®—é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç”¨æœ€å°äºŒä¹˜æ³•æ¥ä½œä¸ºç›®æ ‡å‡½æ•°ï¼š $$\\hat J = \\sum_{i=1}^V\\sum_{j=1}^VX_i(\\hat P_{ij}-\\hat Q_{ij})$$ å…¶ä¸­ $\\hat P_{ij}=X_{ij}$ å’Œ $\\hat Q_{ij}=exp(u_j^Tv_i)$ éƒ½è¡¨ç¤ºæœªå½’ä¸€åŒ–åˆ†å¸ƒã€‚è¿™é‡Œå…¶å®åšäº†ä¸€ä¸ªè¿‘ä¼¼ï½ï½ï½å°†softmaxä¸­çš„å¾ˆéš¾è®¡ç®—çš„é‡ $\\sum_{j=1}^Vexp(u_j^Tv_i)$ è¿‘ä¼¼æˆäº† $X_i$ é€šå¸¸è¯­æ–™åº“è¾ƒå¤§çš„æƒ…å†µä¸‹ $X_{ij}$ éƒ½ä¼šå¾ˆå¤§ï¼Œè¿™ä¼šä½¿å¾—ä¼˜åŒ–å˜å¾—å›°éš¾ã€‚ä¸€ä¸ªæœ‰æ•ˆçš„æ–¹æ³•æ˜¯æœ€å°åŒ–å®ƒä»¬ä¿©çš„å¯¹æ•°å½¢å¼ï¼š $$\\begin{align} \\hat J&amp;=\\sum_{i=1}^V\\sum_{j=1}^V(log(\\hat P)_ {ij}-log(\\hat Q)_ {ij})\\ &amp;=\\sum_{i=1}^V\\sum_{j=1}^VX_i(u_j^Tv_i-logX_{ij})^2 \\end{align}$$ Gloveçš„ä¼˜ç‚¹ï¼š è®­ç»ƒè¿…é€Ÿï¼šä¹Ÿéœ€è¦éå†æ•´ä¸ªè¯­æ–™åº“ï¼Œä½†æ˜¯è®¡ç®—æ¯ä¸€ä¸ªè¯çš„æ¦‚ç‡æ—¶å¹¶ä¸éœ€è¦åƒword2vecé‚£æ ·æ¶ˆè€—softmaxé‚£ä¹ˆå¤§çš„è®¡ç®—é‡ scalable to huge corpora å¯æ‹“å±•æ€§ å¯¹äºè¾ƒå°çš„è¯­æ–™åº“å’Œå‘é‡ä¹Ÿæœ‰å¾ˆå¥½çš„æ€§èƒ½ 3. Intrinsic evaluation3.1 Word Vector Analogies è¯­ä¹‰ semantic informationï¼š å¥æ³•ç»“æ„ syntactic structure: 3.2 Intrinsic Evaluation Tuning Example: Analogy Evaluationséœ€è¦è°ƒèŠ‚çš„è¶…å‚æ•°ï¼š è¯å‘é‡çš„ç»´åº¦ dimension of word vectors è¯­æ–™åº“çš„å¤§å° corpus size è¯­æ–™åº“çš„ç§ç±» corpus source/type ä¸Šä¸‹æ–‡çª—å£å¤§å° context window size ä¸Šä¸‹æ–‡å¯¹ç§°æ€§ context symmetry 3.4 Further Reading: Dealing With AmbiguityImproving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al, 2012) 4. Extrinsic Taskssentiment, named-entity recognition(NER), given a context and a central word, than classify the central word to be one of many classes. 4.1 retraining word VectorsIf we retrain word vectors using the extrinsic task, we need to ensure that the training set is large enough to cover most words from the vocabulary. presentationLinear algebraic structure of word senses with applications to polysemy reference: CS224nç¬”è®°3 é«˜çº§è¯å‘é‡è¡¨ç¤º","link":"/2018/04/30/cs224d-lecture2-%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E9%AB%98%E7%BA%A7%E8%A1%A8%E7%A4%BA/"},{"title":"è®ºæ–‡ç¬”è®°-Autoencoders Are Vision Learners","text":"DALL-E: Zero-Shot Text-to-Image Generation BEIT: BERT Pre-Training of Image Transformers Discrete representations strengthen vision transformer robustness IBOT: Image BERT Pre-training with online tokenizer Masked Autoencoders Are Scalable Vision Learners VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition","link":"/2021/11/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Autoencoders-Are-Vision-Learners/"},{"title":"è®ºæ–‡ç¬”è®°-CNNä¸è‡ªç„¶è¯­è¨€å¤„ç†","text":"æœ€è¿‘åœ¨å‚åŠ  AI challenge è§‚ç‚¹å‹é˜…è¯»ç†è§£çš„æ¯”èµ›ã€‚æ•°æ®é›†å½¢å¼å¦‚ä¸‹ï¼š æœ€å¼€å§‹å°è¯•çš„æ¨¡å‹ä¸»è¦åŒ…æ‹¬å‡ ä¸ªéƒ¨åˆ†ï¼š Embedding: ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡è¯å‘é‡ã€‚ Encoder: åŸºäº Bi-GRU å¯¹ passage,query å’Œ alternatives è¿›è¡Œç¼–ç å¤„ç†ã€‚ Attention: ç”¨ trilinear çš„æ–¹å¼ï¼Œå¹¶ mask ä¹‹åå¾—åˆ°ç›¸ä¼¼çŸ©é˜µï¼Œç„¶åé‡‡ç”¨ç±»ä¼¼äº BiDAF ä¸­çš„å½¢å¼ bi-attention flow å¾—åˆ° attened passage. contextual: ç”¨ Bi-GRU å¯¹ attened passage è¿›è¡Œç¼–ç ï¼Œå¾—åˆ° fusion. match ä½¿ç”¨ attention pooling çš„æ–¹å¼å°† fusion å’Œ enc_answer è½¬æ¢ä¸ºå•ä¸ª vector. ç„¶åä½¿ç”¨ cosin è¿›è¡ŒåŒ¹é…è®¡ç®—å¾—åˆ°æœ€ç›¸ä¼¼çš„ç­”æ¡ˆã€‚ ç›®å‰èƒ½å¾—åˆ°çš„å‡†ç¡®ç‡æ˜¯ 0.687. è·ç¦»ç¬¬ä¸€åå·®äº† 0.1â€¦å…¶å®é™¤äº†æ¢æ¨¡å‹ï¼Œèƒ½æå‡å’Œæ”¹è¿›çš„åœ°æ–¹æ˜¯æŒºå¤šçš„ã€‚ å¯ä»¥ç”¨ ELMO æˆ– wordvec å…ˆå¯¹è®­ç»ƒé›†è¿›è¡Œé¢„è®­ç»ƒå¾—åˆ°è‡ªå·±çš„è¯å‘é‡ã€‚ attention å±‚å¯ä»¥ä½¿ç”¨æ›´ä¸°å¯Œçš„æ–¹å¼ï¼Œå¾ˆå¤špaper ä¸­ä¹Ÿæœ‰æåˆ°ã€‚ç”šè‡³å¯ä»¥åŠ ä¸Šäººå·¥æå–çš„ç‰¹å¾ã€‚æ¯”å¦‚è‹å‰‘æ— blog ä¸­æåˆ°çš„ã€‚ è¿˜æœ‰ä¸ªå¾ˆé‡è¦çš„å°±æ˜¯ match éƒ¨åˆ†ï¼Œ attention pooling æ˜¯å¦å¯ä»¥æ¢æˆå…¶ä»–æ›´å¥½çš„æ–¹å¼ï¼Ÿ ä½†æ˜¯ï¼Œä¸æ–­å°è¯•å„ç§æ¨¡å‹çš„å‰æä¹Ÿè¦è€ƒè™‘é€Ÿåº¦å§ã€‚ã€‚rnn å®åœ¨æ˜¯å¤ªæ…¢äº†ï¼Œæ‰€ä»¥å†³å®šè¯•è¯• CNN çš„æ–¹å¼æ¥å¤„ç† NLP çš„ä»»åŠ¡ã€‚ å…³äºä½¿ç”¨ CNN æ¥å¤„ç†é˜…è¯»ç†è§£çš„ä»»åŠ¡çš„å¤§ä½œè¿˜æ˜¯æŒºå¤šçš„ï¼Œè¿™é‡Œä¸»è¦ä»‹ç»è¿™ä¸¤ç¯‡ï¼š Facebook: Convolutional Sequence to Sequence Learning Fast Reading Comprehension with ConvNets ConvS2Spaper: Convolutional Sequence to Sequence Learning è¿™ç¯‡ paper å¯¹åº”çš„ NLP ä»»åŠ¡æ˜¯æœºå™¨ç¿»è¯‘ï¼Œé™¤äº†ç”¨ CNN å¯¹ sentence è¿›è¡Œç¼–ç ä¹‹å¤–ï¼Œå…¶æ ¸å¿ƒæ˜¯åœ¨ decoder çš„æ—¶å€™ä¹Ÿä½¿ç”¨ CNN. å¯¹äºé˜…è¯»ç†è§£æ¥è¯´ï¼Œèƒ½å¤Ÿå€Ÿç”¨çš„æ˜¯å…¶ç¼–ç  sentence çš„æ–¹å¼ã€‚ä½†è¿™é‡Œä½œä¸ºå­¦ä¹ ï¼Œä¹Ÿå¤šäº†è§£ä¸€ä¸‹ decoder å§ï½ å¯¹æ–‡æœ¬æ¥è¯´ï¼Œçœ‹åˆ° CNN æˆ‘ä»¬é¦–å…ˆæƒ³åˆ°çš„æ˜¯ cnn èƒ½æœ‰æ•ˆåˆ©ç”¨å±€éƒ¨ä¿¡æ¯ï¼Œæå–å‡ºå±€éƒ¨ç‰¹å¾ï¼Œæ‰€ä»¥é€‚åˆåšæ–‡æœ¬åˆ†ç±»ã€‚ä½†æ˜¯å¯¹äº æœºå™¨ç¿»è¯‘ã€é˜…è¯»ç†è§£è¿™æ ·çš„éœ€è¦è€ƒè™‘å…¨å±€ä¿¡æ¯çš„ä»»åŠ¡ï¼ŒCNN ä¼¼ä¹çœ‹èµ·æ¥å¹¶ä¸é‚£ä¹ˆæœ‰æ•ˆã€‚è€Œä¸”åœ¨ decoder çš„æ—¶å€™ï¼Œè¯çš„ç”Ÿæˆæ˜¯ one by one çš„ï¼Œä¸‹ä¸€ä¸ªè¯çš„ç”Ÿæˆæ˜¯ä¾èµ–äºä¸Šä¸€ä¸ªè¯çš„ã€‚æ‰€ä»¥åœ¨ decoder ä¸­ä½¿ç”¨ RNN ä¹Ÿæ˜¯å¾ˆè‡ªç„¶è€Œç„¶çš„ã€‚ Facebook çš„è¿™ç¯‡ paper å°±æ”¹å˜äº†è¿™äº›ä¼ ç»Ÿçš„æ€ç»´ï¼Œä¸ä»…ç”¨ CNN ç¼–ç å…¨å±€ä¿¡æ¯ï¼Œè€Œä¸”è¿˜èƒ½ decoder. Motivation Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher layers. å¤šå±‚ CNN å…·æœ‰å±‚çº§è¡¨ç¤ºç»“æ„ï¼Œç›¸é‚»çš„è¯ä¹‹é—´åœ¨è¾ƒä½å±‚çš„ layer äº¤äº’ï¼Œè·ç¦»è¾ƒè¿œçš„è¯åœ¨è¾ƒé«˜å±‚çš„ layer äº¤äº’ï¼ˆäº¤äº’çš„ç›®çš„å°±æ˜¯è¯­ä¹‰æ¶ˆæ­§ï¼‰ã€‚ Hierarchical structure provides a shorter path to capture long-range dependencies compared to the chain structure modeled by recurrent networks, e.g. we can obtain a feature representation capturing relationships within a window of n words by applying only O(n/k) convolutional operations for kernels of width k, compared to a linear number O(n) for recurrent neural networks. å±‚çº§ç»“æ„æä¾›äº†ä¸€ä¸ªæ›´çŸ­çš„è·¯å¾„æ¥è·å–é•¿æœŸä¾èµ–ã€‚æ¯”å¦‚ç›¸è·ä¸º n çš„ä¸¤ä¸ªè¯ï¼Œåœ¨ rnn ä¸­äº¤äº’éœ€è¦çš„æ­¥æ•°æ˜¯ O(n),åœ¨å±‚çº§ CNN ä¸­éœ€è¦ O(n/k).è¿™æ ·å‡å°‘äº†éçº¿æ€§çš„æ“ä½œï¼Œé™ä½äº†æ¢¯åº¦æ¶ˆå¤±çš„æƒ…å†µã€‚æ‰€ä»¥è¿™ä¸¤ä¸ªè¯çš„äº¤äº’æ•ˆæœä¼šæ›´å¥½ï½ Inputs to a convolutional network are fed through a constant number of kernels and non-linearities, whereas recurrent networks apply up to n operations and non-linearities to the first word and only a single set of operations to the last word. Fixing the number of nonlinearities applied to the inputs also eases learning. è¾“å…¥åˆ° CNN ä¸­æ¯ä¸ªè¯éƒ½ä¼šç»å†å›ºå®šçš„ kernel å’Œ éçº¿æ€§æ“ä½œã€‚è€Œè¾“å…¥åˆ° RNN çš„ï¼Œç¬¬ä¸€ä¸ªè¯éœ€è¦ç»è¿‡ n ä¸ª operationsï¼Œæœ€åä¸€ä¸ªè¯åªç»å†äº†ä¸€ä¸ª operations. ä½œè€…è®¤ä¸ºå›ºå®šçš„æ“ä½œæ›´å®¹æ˜“å­¦ä¹ ã€‚ è¿™ä¸€ç‚¹æˆ‘ä¸ªäººè®¤ä¸ºå¹¶ä¸ä¸€å®šå°±æ˜¯åˆç†çš„ï¼Œæœ¬æ¥ä¸€ä¸ªå¥å­ä¸­ä¸åŒè¯çš„é‡è¦æ€§å°±æ˜¯ä¸ä¸€æ ·çš„ã€‚ ï¼­odel Architecture æ¨¡å‹åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š position embedding convolution block structure Multi-step attention position encoding è¿™éƒ¨åˆ†åœ¨å¾ˆå¤šåœ°æ–¹éƒ½å‡ºç°è¿‡äº†ï¼Œåœ¨æ²¡æœ‰ rnn çš„æƒ…å†µä¸‹ï¼Œéƒ½ä¼šç”¨ PE æ¥ç¼–ç ä½ç½®ä¿¡æ¯ã€‚ä½†æ˜¯åœ¨è¿™ç¯‡ paper ä¸­ï¼Œä½œè€…é€šè¿‡å®éªŒå‘ç°ï¼ŒPE ä½œç”¨ä¼¼ä¹å¹¶ä¸æ˜¯å¾ˆé‡è¦ã€‚ convolution blocksä½œè€…ä½¿ç”¨çš„æ˜¯é—¨æ¿€æ´»æœºåˆ¶ï¼Œ GLU, gate linear units. æ¥è‡ªäº paper: Language modeling with gated convolutional networks åœ¨è¿™ç¯‡ paper ä¸­ï¼Œä½œè€…ç”¨æ— ç›‘ç£çš„æ–¹å¼ï¼Œæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå°† CNN å¾—åˆ°çš„è¯­è¨€æ¨¡å‹ä¸ LSTM è¿›è¡Œå¯¹æ¯”ã€‚ ä¹Ÿå°±æ˜¯: $$h_l=(XW+b)\\otimes \\sigma(XV+c)$$ The output of each layer is a linear projection X âˆ— W + b modulated by the gates Ïƒ(X âˆ— V + c). Similar to LSTMs, these gates multiply each element of the matrix X âˆ—W+b and control the information passed on in the hierarchy. å¦‚æœæ˜¯ LSTM-styleï¼Œåº”è¯¥æ˜¯ GTUï¼š $$h_i^l=tanh(XW+b)\\otimes \\sigma(XV+c)$$ ä½œè€…å°†ä¸¤è€…è¿›è¡Œäº†å¯¹æ¯”ï¼Œå‘ç° GLU æ•ˆæœæ›´å¥½ã€‚ residual connection: ä¸ºäº†å¾—åˆ°æ›´ deep çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œä½œè€…å¢åŠ äº†æ®‹å·®é“¾æ¥ã€‚ $$h_i^l=v(W^l[h_{i-k/2}^{l-1},â€¦,h_{i+k/2}^{l-1}]+b_w^l)+h_i^{l-1}$$ å·ç§¯çš„æ•´ä¸ªè¿‡ç¨‹ï¼š è®ºæ–‡ä¸­ä¸¾äº†è¿™æ ·ä¸€ä¸ªä¾‹å­ï¼š For instance, stacking 6 blocks with k = 5 results in an input field of 25 elements, i.e. each output depends on 25 inputs. Non-linearities allow the networks to exploit the full input field, or to focus on fewer elements if needed. è¿™ä¸ªæ€ä¹ˆç®—çš„å‘¢ï¼Ÿçœ‹ä¸‹å›¾ï¼š ä»ä¸Šå›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œå½“ k=3 æ—¶ï¼Œ3 ä¸ª blocksï¼Œç¬¬ä¸‰å±‚ä¸­çš„æ¯ä¸€ä¸ªè¾“å…¥éƒ½ä¸è¾“å…¥ä¸­çš„ 7 åˆ—æœ‰å…³ã€‚æ‰€ä»¥è®¡ç®—æ–¹å¼æ˜¯ k + (k-1)* (blocks-1). ä¸€ç»´å·ç§¯å’ŒäºŒç»´å·ç§¯çš„åŒºåˆ«ï¼š ConvS2S æ˜¯ 1D å·ç§¯ï¼Œkernel åªæ˜¯åœ¨æ—¶é—´ç»´åº¦ä¸Šå¹³ç§»ï¼Œä¸” stride çš„å›ºå®š size ä¸º1,è¿™æ˜¯å› ä¸ºè¯­è¨€ä¸å…·å¤‡å›¾åƒçš„å¯ä¼¸ç¼©æ€§ï¼Œå›¾åƒåœ¨å‡åŒ€çš„è¿›è¡Œé™é‡‡æ ·åä¸æ”¹å˜å›¾åƒçš„ç‰¹å¾ï¼Œè€Œä¸€ä¸ªå¥å­é—´éš”ç€å–è¯ï¼Œæ„æ€å°±ä¼šæ”¹å˜å¾ˆå¤šäº†ã€‚ åœ¨å›¾åƒä¸­ä¸€ä¸ªå·ç§¯å±‚å¾€å¾€æœ‰å¤šä¸ª filterï¼Œä»¥è·å–å›¾åƒä¸åŒçš„ patternï¼Œä½†æ˜¯åœ¨ ConvS2S ä¸­ï¼Œæ¯ä¸€å±‚åªæœ‰ä¸€ä¸ª filterã€‚ä¸€ä¸ªå¥å­è¿›å…¥ filter çš„æ•°æ®å½¢å¼æ˜¯ [1, n, d]. å…¶ä¸­ n ä¸ºå¥å­é•¿åº¦ï¼Œ filter å¯¹æ•°æ®è¿›è¡Œ n æ–¹å‘ä¸Šå·ç§¯ï¼Œè€Œ d æ˜¯è¯çš„å‘é‡ç»´åº¦ï¼Œå¯ä»¥ç†è§£ä¸º channelï¼Œä¸å½©è‰²å›¾ç‰‡ä¸­çš„ rgb ä¸‰ä¸ª channel ç±»ä¼¼ã€‚ Facebook åœ¨è®¾è®¡æ—¶ï¼Œå¹¶æ²¡æœ‰åƒå›¾åƒä¸­å¸¸åšçš„é‚£æ ·ï¼Œæ¯ä¸€å±‚åªè®¾ç½®ä¸€ä¸ª filterã€‚è¿™æ ·åšçš„åŸå› ï¼Œä¸€æ˜¯ä¸ºäº†ç®€åŒ–æ¨¡å‹ï¼ŒåŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼ŒäºŒæ˜¯ä»–ä»¬è®¤ä¸ºä¸€ä¸ªå¥å­çš„ pattern è¦è¾ƒå›¾åƒç®€å•å¾ˆå¤šï¼Œé€šè¿‡æ¯å±‚è®¾ç½®ä¸€ä¸ª filterï¼Œé€å±‚å †å åä¾¿èƒ½æŠ“åˆ°æ‰€æœ‰çš„ pattern. æ›´æœ‰å¯èƒ½çš„åŸå› æ˜¯å‰è€…ã€‚å› ä¸ºåœ¨ transorfmer ä¸­ï¼Œmulti-head attention å¤šå¤´èšç„¦å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œè¯´æ˜ä¸€ä¸ªå¥å­çš„ pattern æ˜¯æœ‰å¤šä¸ªçš„. è¿™æ®µè¯æ˜¯æœ‰é—®é¢˜çš„å§ï¼Ÿ filter çš„ä¸ªæ•°éš¾é“ä¸æ˜¯ 2då—ï¼Ÿ åªä¸è¿‡è¿™é‡Œè¯´çš„ transorfmer çš„å¤šå¤´èšç„¦æ˜¯å€¼å¾—èšç„¦åˆ°ä¸€ä¸ªè¯å‘é‡ä¸­çš„éƒ¨åˆ†ç»´åº¦ã€‚è®°å¾—åœ¨ cs224d ä¸­ manning æ›¾ç»è®²è¿‡ä¸€ä¸ªä¾‹å­ï¼Œç»è¿‡è®­ç»ƒæˆ–è¯ä¸è¯ä¹‹é—´çš„äº¤äº’åï¼Œè¯å‘é‡ä¸­çš„éƒ¨åˆ†ç»´åº¦å‘ç”Ÿäº†å˜åŒ–ã€‚ åœ¨ paper ä¸­ï¼Œå·ç§¯æ ¸çš„å°ºå¯¸å¤§å°æ˜¯ $W\\in R^{2d\\times kd}$. For encoder networks we ensure that the output of the convolutional layers matches the input length by padding the input at each layer. However, for decoder networks we have to take care that no future information is available to the decoder (Oord et al., 2016a). Specifically, we pad the input by k âˆ’ 1 elements on both the left and right side by zero vectors, and then remove k elements from the end of the convolution output. åœ¨ encoder å’Œ decoder ç½‘ç»œä¸­ï¼Œpadding çš„æ–¹å¼æ˜¯ä¸ä¸€æ ·çš„ã€‚å› ä¸ºåœ¨ decoder çš„æ—¶å€™ä¸èƒ½è€ƒè™‘æœªæ¥ä¿¡æ¯. åœ¨ encoder æ—¶ï¼Œå°† (k-1) pad åˆ°å·¦å³ä¸¤è¾¹ï¼Œä¿è¯å·ç§¯å±‚çš„é•¿åº¦ä¸å˜ã€‚ åœ¨ decoder ä¸­ï¼Œå°† (k-1) pad åˆ°å¥å­çš„å·¦è¾¹ã€‚å› æ­¤ç”Ÿæˆçš„è¯ä¾æ—§æ˜¯ one by one. Multi-step Attention $$d_i^l=W_d^lh_i^l+b_d^l+g_i$$ $$a_{ij}^l=\\dfrac{exp(d_i^l\\cdot z_j^u)}{\\sum_{t=1}^mexp(d_i^l\\cdot z_j^u)}$$ $$c_i^l=\\sum_{j=1}^ma_{ij}^l(z_j^u+e_j)$$ ä¸Šå¼ä¸­ï¼Œl è¡¨ç¤º decoder ä¸­å·ç§¯å±‚çš„å±‚æ•°ï¼Œi è¡¨ç¤ºæ—¶é—´æ­¥ã€‚ å®é™…ä¸Šè·Ÿ rnn çš„ decoder è¿˜æ˜¯æ¯”è¾ƒæ¥è¿‘çš„ã€‚ åœ¨è®­ç»ƒé˜¶æ®µæ˜¯ teacher forcing, å·ç§¯æ ¸ $W_d^l$ åœ¨ target sentence $h^l$ ä¸Šç§»åŠ¨åšå·ç§¯å¾—åˆ° $(W_d^lh_i^l + b_d^l)$ï¼Œç±»ä¼¼ä¸ rnn-decoder ä¸­çš„éšè—çŠ¶æ€ã€‚ç„¶ååŠ ä¸Šä¸Šä¸€ä¸ªè¯çš„ embedding $g_i$,å¾—åˆ° $d_i^l$. ä¸ encdoer å¾—åˆ°çš„ source sentence åšäº¤äº’ï¼Œé€šè¿‡ softmax å¾—åˆ° attention weights $a_{ij}^l$. å¾—åˆ° attention vector è·Ÿ rnn-decoder æœ‰æ‰€ä¸åŒï¼Œè¿™é‡ŒåŠ ä¸Šäº† input element embedding $e_j$. è‡³äºè¿™é‡Œä¸ºä»€ä¹ˆè¦åŠ  $e_j$? We found adding e_j to be beneficial and it resembles key-value memory networks where the keys are the z_j^u and the values are the z^u_j + e_j (Miller et al., 2016). Encoder outputs z_j^u represent potentially large input contexts and e_j provides point information about a specific input element that is useful when making a prediction. Once c^l_i has been computed, it is simply added to the output of the corresponding decoder layer h^l_i. $z_j^u$ è¡¨ç¤ºæ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œè€Œ $e_j$ èƒ½å¤Ÿèƒ½å…·ä½“çš„æŒ‡å‡ºè¾“å…¥ä¸­å¯¹é¢„æµ‹æœ‰ç”¨çš„ä¿¡æ¯ã€‚è¿˜æ˜¯è°ç”¨è°çŸ¥é“å§ã€‚ã€‚ å…³äº multi-hop attention: This can be seen as attention with multiple â€™hopsâ€™ (Sukhbaatar et al., 2015) compared to single step attention (Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016). In particular, the attention of the first layer determines a useful source context which is then fed to the second layer that takes this information into account when computing attention etc. The decoder also has immediate access to the attention history of the k âˆ’ 1 previous time steps because the conditional inputs $c^{l-1}_{iâˆ’k}, . . . , c^{l-1}i$ are part of $h^{l-1}{i-k}, . . . , h^{l-1}_i$ which are input to $h^l_i$. This makes it easier for the model to take into account which previous inputs have been attended to already compared to recurrent nets where this information is in the recurrent state and needs to survive several non-linearities. Overall, our attention mechanism considers which words we previously attended to (Yang et al., 2016) and performs multiple attention â€™hopsâ€™ per time step. In Appendix Â§C, we plot attention scores for a deep decoder and show that at different layers, different portions of the source are attended to. è¿™ä¸ªè·Ÿ memory networks ä¸­çš„ multi-hop æ˜¯æœ‰ç‚¹ç±»ä¼¼ã€‚ FAST READING COMPREHENSION WITH CONVNETSGated Linear Dilated Residual Network (GLDR): a combination of residual networks (He et al., 2016), dilated convolutions (Yu &amp; Koltun, 2016) and gated linear units (Dauphin et al., 2017). text understanding with dilated convolution kernel:$k=[k_{-l},k_{-l+1},â€¦,k_l]$, size=$2l+1$ input: $x=[x_1,x_2,â€¦,x_n]$ dilation: d å·ç§¯å¯ä»¥è¡¨ç¤ºä¸ºï¼š $$(k*x)_ t=\\sum_{i=-l}^lk_i\\cdot x_{t + d\\cdot i}$$ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è†¨èƒ€å·ç§¯å‘¢ï¼Ÿ Why Dilated convolution? Repeated dilated convolution (Yu &amp; Koltun, 2016) increases the receptive region of ConvNet outputs exponentially with respect to the network depth, which results in drastically shortened computation paths. èƒ½å¤Ÿæ˜¾è‘—ç¼©çŸ­ä¸¤ä¸ªè¯ä¹‹é—´çš„è®¡ç®—è·¯å¾„ã€‚ ä½œè€…å°† GLDR å’Œ self-attention,ä»¥åŠ RNN è¿›è¡Œäº†å¯¹æ¯”ï¼Œinput sequence length n, network width w, kernel size k, and network depth D. model Architectureä½œè€…ä¸ BiDAF å’Œ DrQA è¿›è¡Œäº†å¯¹æ¯”ï¼Œå°† BiDAF å’Œ DrQA ä¸­çš„ BiLSTM éƒ¨åˆ†æ›¿æ¢æˆ GLDR Convolution. The receptive field of this convolutional network grows exponentially with depth and soon encompasses a long sequence, essentially enabling it to capture similar long-term dependencies as an actual sequential model. æ„Ÿå—é‡çš„å°ºå¯¸å¤§å°æŒ‡æ•°å¢åŠ ï¼Œèƒ½å¤Ÿè¿…é€Ÿå‹ç¼© long sentence,å¹¶ capture é•¿æœŸä¾èµ–ã€‚ Convolutional BiDAF. In our convolutional version of BiDAF, we replaced all bidirectional LSTMs with GLDRs . We have two 5-layer GLDRs in the contextual layer whose weights are un-tied. In the modeling layer, a 17-layer GLDR with dilation 1, 2, 4, 8, 16 in the first 5 residual blocks is used, which results in a reception region of 65 words. A 3-layer GLDR replaces the bidirectional LSTM in the output layer. For simplicity, we use same-padding and kernel size 3 for all convolutions unless specified. The hidden size of all GLDRs is 100 which is the same as the LSTMs in BiDAF. å…·ä½“ç½‘ç»œç»“æ„ï¼Œå®é™…å‚æ•°å¯ä»¥çœ‹ paper å®éªŒéƒ¨åˆ†ã€‚","link":"/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"title":"è®ºæ–‡ç¬”è®° - Attention ç»¼è¿°","text":"Attention and Augmented Recurrent Neural Networks å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„æ ¹åŸºä¹‹ä¸€ï¼Œå¥¹å…è®¸ç¥ç»ç½‘ç»œå¯¹åºåˆ—æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæ¯”å¦‚æ–‡æœ¬ï¼Œè¯­éŸ³å’Œè§†é¢‘ã€‚ è¿™å¥è¯å½¢å®¹ç‰¹åˆ«è´´åˆ‡ï¼Œæ„æ€å°±æ˜¯rnnèƒ½å°†åºåˆ—è½¬æ¢æˆåŒ…å«äº†ç†è§£ï¼Œè¯­ä¹‰çš„è¡¨ç¤ºã€‚ They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch! åŸºæœ¬çš„ rnn åœ¨è§£å†³ long sequence æ—¶éå¸¸æŒ£æ‰ï¼Œå…¶å˜ä½“ LSTM æœ‰æ•ˆçš„è§£å†³è¿™ä¸€é—®é¢˜ï¼ˆä½†é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼‰ã€‚ éšç€æ—¶é—´çš„å‘å±•ï¼Œå‡ºç°äº†å„ç§ augument RNNs. å…¶ä¸­ä»¥ä¸‹4ç§ stand out exciting. è¿™äº›å˜ä½“éƒ½æ˜¯ RNN æœ‰åŠ›çš„æ‹“å±•ï¼Œæ›´ä»¤äººæƒŠå¥‡çš„æ˜¯ï¼Œä»–ä»¬è¿˜èƒ½æœ‰æ•ˆçš„ç»“åˆåœ¨ä¸€èµ·ï¼Œä¼¼ä¹åªæ˜¯å¹¿é˜”ç©ºé—´ä¸­çš„ä¸€ç‚¹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä»–ä»¬éƒ½ä¾èµ–äºåŒæ ·çš„ underlying trick â€” attention. Neural Turing MachinesNeural Turing Machines ç¥ç»å›¾çµæœºå°† RNNs å’Œå¤–éƒ¨ memory bank ç»“åˆåœ¨ä¸€èµ·ã€‚å‘é‡ç”¨æ¥è¡¨ç¤ºç¥ç»ç½‘ç»œä¸­çš„è‡ªç„¶è¯­è¨€ï¼Œmemory æ˜¯å‘é‡çš„æ•°ç»„ã€‚ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œèƒ½å¤Ÿçœ‹å‡ºå…¶åŸç†æ˜¯å°†æ¯ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºå­˜å‚¨åœ¨ memory ä¸­ã€‚é‚£ä¹ˆä» memory ä¸­è¯»ã€ä»¥åŠå†™å…¥ memory æ˜¯æ€ä¹ˆæ“ä½œçš„å‘¢ï¼Ÿ æœ€å¤§çš„éš¾ç‚¹åœ¨äºè®©æ•´ä¸ªè¿‡ç¨‹å¯å¾®(differentiable).ç‰¹åˆ«çš„æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è¯»å‡ºå’Œå†™å…¥çš„ä½ç½®å…·æœ‰å·®å¼‚æ€§ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥çŸ¥é“ä»å“ªå„¿è¯»å’Œå†™ã€‚è¿™å¾ˆæ£˜æ‰‹(tricky)ï¼Œå› ä¸º memory åœ°å€åŸºæœ¬ä¸Šæ˜¯ç¦»æ•£çš„ã€‚ NMTs é‡‡å–äº†ä¸€ä¸ªéå¸¸èªæ˜çš„æ–¹æ³•ï¼Œæ¯ä¸€æ­¥è¯»å’Œå†™éƒ½åŒ…æ‹¬ memory ä¸­çš„æ‰€æœ‰ä½ç½®ï¼Œåªæ˜¯å¯¹äºä¸åŒçš„ä½ç½®ï¼Œè¯»å’Œå†™çš„ç¨‹åº¦ä¸åŒã€‚ ä¸¾ä¸ªä¾‹å­ï¼Œè¿™é‡Œæˆ‘ä»¬åªå…³æ³¨ reading. RNN è¾“å‡ºä¸€ä¸ª â€œattention distributionâ€ï¼Œè¡¨ç¤ºå¯¹äº memory ä¸åŒä½ç½®çš„è¯»å–ç¨‹åº¦ã€‚è¿™æ ·ï¼Œè¯»å…¥çš„æ“ä½œå¯ä»¥çœ‹ä½œåŠ æƒæ±‚å’Œã€‚ $$r \\leftarrow \\sum_ia_iM_i$$ ç±»ä¼¼çš„ï¼Œåœ¨è¯»å…¥çš„è¿‡ç¨‹ä¸­ä¹Ÿæ˜¯å¯¹ memory ä¸­æ‰€æœ‰ä½ç½®ã€‚å†ä¸€æ¬¡ï¼Œè¾“å‡ºä¸€ä¸ª â€œattention distributionâ€ ç”¨æ¥è¡¨ç¤ºå¯¹æ¯ä¸ªä½ç½®çš„å†™å…¥ç¨‹åº¦ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ª memory ä½ç½®è·å¾—æ–°çš„ value æ˜¯æ—§çš„ memory å’Œå†™å…¥å€¼çš„ å‡¸ç»“åˆ(convex combination). $$M_i \\leftarrow a_iw+(1-a_i)M_i$$ å…¶ä¸­ $w$ æ˜¯å†™å…¥å€¼(write value)ã€‚ ä½†æ˜¯ NTMs å¦‚ä½•ç¡®å®šå»æ³¨æ„ memory ä¸­çš„é‚£ä¸€ä¸ªä½ç½®å‘¢ï¼Ÿ ä»–ä»¬ä½¿ç”¨äº†ä¸¤ç§ä¸åŒçš„æ–¹æ³•çš„ç»“åˆï¼š content-based attention å’Œ location-based attention. å‰è€…å…è®¸ NMTs é€šè¿‡åŒ¹é… memory ä¸­çš„æ¯ä¸€ä¸ªä½ç½®ï¼Œå¹¶ focus on æœ€ match çš„ä½ç½®ã€‚åè€…å…è®¸ memory ä¸­çš„ç›¸å¯¹ç§»åŠ¨ï¼Œä¿è¯ NMT èƒ½å¾ªç¯ã€‚ è¿™ç§è¯»å’Œå†™çš„èƒ½åŠ›å…è®¸ NTM æ‰§è¡Œè®¸å¤šç®€å•çš„ç®—æ³•ï¼Œè€Œè¿™äº›ç®—æ³•ä»¥å‰è¶…è¶Šäº†ç¥ç»ç½‘ç»œã€‚ ä¾‹å¦‚ï¼Œä»–ä»¬å¯ä»¥åœ¨ memoryä¸­å­˜å‚¨ä¸€ä¸ªé•¿åºåˆ—ï¼Œç„¶åéå†å®ƒï¼Œåå‘é‡å¤å®ƒã€‚ å½“ä»–ä»¬è¿™æ ·åšæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹ä»–ä»¬è¯»å’Œå†™çš„åœ°æ–¹ï¼Œä»¥æ›´å¥½åœ°ç†è§£ä»–ä»¬åœ¨åšä»€ä¹ˆï¼š å…³äº repeat copy çš„å®éªŒï¼Œå¯ä»¥å‚è€ƒè¿™ç¯‡è®ºæ–‡Show, attend and tell: Neural image caption generation with visual attention ä»–ä»¬èƒ½å¤Ÿæ¨¡ä»¿ä¸€ä¸ª lookup table,ç”šè‡³å¯ä»¥ sort numbers(althought they kind of cheat). ä½†å¦ä¸€æ–¹é¢ï¼Œä»–ä»¬ä»ç„¶ä¸èƒ½åšå¾ˆå¤šåŸºæœ¬çš„äº‹å„¿ï¼Œæ¯”å¦‚ add or multiply numbers. è‡ªä» NTM è¿™ç¯‡è®ºæ–‡ä¹‹åï¼Œåˆå‡ºç°äº†å¾ˆå¤šç›¸åŒæ–¹å‘çš„ä»¤äºº exciting çš„æ–‡ç« . The Neural GPU 4 overcomes the NTMâ€™s inability to add and multiply numbers. Zaremba &amp; Sutskever 5 train NTMs using reinforcement learning instead of the differentiable read/writes used by the original. Neural Random Access Machines 6 work based on pointers. Some papers have explored differentiable data structures, like stacks and queues [7, 8]. memory networks [9, 10] are another approach to attacking similar problems. Code Neural Turing Machine Taehoon Kimâ€™s Neural GPU publication TensorFlow Models repository Memory Networks, Taehoon Kimâ€™s å…³äºè¿™ç¯‡ paper çœŸçš„å¾ˆéš¾çœ‹æ‡‚ï¼Œ Attention Interfaceså½“æˆ‘ç¿»è¯‘ä¸€ä¸ªå¥å­æ—¶ï¼Œæˆ‘ç‰¹åˆ«æ³¨æ„æˆ‘æ­£åœ¨ç¿»è¯‘çš„å•è¯ã€‚ å½“æˆ‘å½•åˆ¶å½•éŸ³æ—¶ï¼Œæˆ‘ä¼šä»”ç»†è†å¬æˆ‘æ­£åœ¨ç§¯æå†™ä¸‹çš„ç‰‡æ®µã€‚ å¦‚æœä½ è¦æ±‚æˆ‘æè¿°æˆ‘æ­£åœ¨åçš„æˆ¿é—´ï¼Œé‚£ä¹ˆæˆ‘ä¼šæµè§ˆæˆ‘æ­£åœ¨æè¿°çš„ç‰©ä½“ã€‚ ç¥ç»ç½‘ç»œèƒ½å¤Ÿé€šè¿‡ attention æœºåˆ¶å®Œæˆä¸Šè¿°è¡Œä¸ºï¼Œä¹Ÿå°±æ˜¯ focusing on ç»™å‡ºä¿¡æ¯çš„éƒ¨åˆ†å†…å®¹ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œä¸€ä¸ª RNN èƒ½å¤Ÿ attend over å¦ä¸€ä¸ª RNN çš„è¾“å‡ºï¼Œå¹¶åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œ focus on å¦ä¸€ä¸ª RNN çš„ä¸åŒçš„ä½ç½®ã€‚ ä¸ºäº†è®© attention å¯å¾®ï¼Œæˆ‘ä»¬é‡‡ç”¨è·Ÿ NTM åŒæ ·çš„æ–¹æ³•ï¼Œfocus æ‰€æœ‰çš„ä½ç½®ï¼Œæ¯ä¸ªä½ç½®çš„ç¨‹åº¦ä¸åŒã€‚ ä¸Šå›¾ä¸­é¢œè‰²çš„æ·±æµ…è¡¨ç¤ºæ³¨æ„çš„ç¨‹åº¦ã€‚ å…¶ä¸­ attention distribution æ˜¯ç”± content-based attention ç”Ÿæˆçš„ã€‚å…·ä½“è¿‡ç¨‹è¿˜æ˜¯çœ‹è‹±æ–‡æè¿°å§ï¼š The attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution. æœ€ç»å…¸çš„ä½¿ç”¨ RNNs ä¹‹é—´çš„ attention å°±æ˜¯æœºå™¨ç¿»è¯‘äº†ï¼ŒNeural machine translation by jointly learning to align and translate. ä¼ ç»Ÿçš„ seq2seq æ¨¡å‹é€šè¿‡ RNN å°†æ•´ä¸ªè¾“å…¥åºåˆ—è½¬åŒ–ä¸ºå•ä¸ªå‘é‡(ä¹Ÿå°±æ˜¯æœ€åä¸€å±‚çš„éšè—çŠ¶æ€)ï¼Œç„¶åå°†å…¶å±•å¼€å¾—åˆ°è¾“å‡º(word by word). æ³¨æ„åŠ›æœºåˆ¶é¿å…äº†è¿™ä¸€ç‚¹ï¼Œå®ƒå…è®¸ RNN åœ¨ç”Ÿæˆè¾“å‡ºæ—¶ï¼Œèƒ½çœ‹åˆ°æ‰€æœ‰è¾“å…¥ä¸­çš„æ¯ä¸€ä¸ªè¯ï¼Œå¹¶ä¸”æ ¹æ®ä»–ä»¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæ¥é€‰æ‹©æ€§æ³¨æ„éƒ¨åˆ†è¯ã€‚ åŸå›¾æ˜¯å¯ä»¥æ“ä½œçš„ï¼Œå¤§ç‰›ä»¬çš„æŠ€æœ¯çœŸæ˜¯å‰å®³ã€‚ã€‚ã€‚å¯è§†åŒ–ä¹Ÿå¾ˆ6ã€‚ã€‚ è¿™ç§ç±»å‹çš„ RNN è¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„åº”ç”¨ï¼Œæ¯”å¦‚åœ¨ è¯­éŸ³è¯†åˆ«ä¸Šçš„ä½¿ç”¨Listen, Attend and Spell, ä½¿ç”¨ä¸€ä¸ª RNN æ¥å¤„ç†éŸ³é¢‘ï¼Œç„¶åç”¨å¦ä¸€ä¸ª RNN æ¥éå†å®ƒï¼Œå¹¶åœ¨ç”Ÿæˆä¸€ä¸ªæŠ„æœ¬(transcript)æ—¶é‡ç‚¹å…³æ³¨ç›¸å…³çš„éƒ¨åˆ†ã€‚ è¿™ä¸ªå›¾çœ‹ä¸å‡ºæ¥ï¼Œä¸èƒ½æˆªå‡ºé‚£ç§æ•ˆæœã€‚ å…¶å®å¯ä»¥çœ‹å‡ºå¯¹è¯­éŸ³çš„è¯†åˆ«ï¼Œåœ¨ç”Ÿæˆå¯¹åº”çš„è¯æ—¶æ›´å…³æ³¨çš„æ˜¯å¯¹åº”çš„éŸ³é¢‘ï¼Œå¹¶ä¸åƒæ–‡æœ¬é‚£ç§éœ€è¦é•¿æ—¶é—´ä¾èµ–ã€‚ è¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„åº”ç”¨ï¼š parsing tree:Grammar as a foreign language conversational modeling:A Neural Conversational Model image captioning: attention interface ä¹Ÿå¯ä»¥æ˜¯ CNN å’Œ RNN ä¹‹é—´çš„ï¼Œå®ƒå…è®¸ RNN åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ç”Ÿæˆæ–‡æœ¬æ—¶èƒ½å…³æ³¨å›¾åƒä¸­çš„ä¸åŒçš„ä½ç½®ã€‚ Then an RNN runs, generating a description of the image. As it generates each word in the description, the RNN focuses on the conv netâ€™s interpretation of the relevant parts of the image. We can explicitly visualize this: å›¾ç‰‡æ¥è‡ªäºShow, attend and tell: Neural image caption generation with visual attention More broadly, attentional interfaces can be used whenever one wants to interface with a neural network that has a repeating structure in its output. Adaptive Computation time Standard RNNs do the same amount of computation for each time step. This seems unintuitive. Surely, one should think more when things are hard? It also limits RNNs to doing O(n) operations for a list of length n. æ ‡å‡†çš„ RNN åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½åšç€ç›¸åŒçš„å¤§é‡çš„è®¡ç®—ã€‚å®ƒè²Œä¼¼æ˜¯å¾ˆåˆç†çš„ï¼Œä½†å½“å¤„ç†çš„ä¿¡æ¯å¾ˆå¤æ‚æ—¶ï¼Œæ˜¯å¦å¯ä»¥åœ¨ä¸€ä¸ªæ—¶é—´æ­¥è€ƒè™‘æ›´å¤šï¼Ÿè¿™æ ·åŒæ ·çš„è®¡ç®—é‡çš„æ–¹å¼é™åˆ¶äº† RNN åœ¨å¤„ç†é•¿åº¦ä¸º n çš„åºåˆ—æ—¶ï¼Œå…¶å¤æ‚åº¦ä¹Ÿä¸º O(n). Adaptive Computation Time [15] is a way for RNNs to do different amounts of computation each step. The big picture idea is simple: allow the RNN to do multiple steps of computation for each time step. è‡ªé€‚åº”è®¡ç®—æ—¶é—´æ­¥(ACT) èƒ½è®© RNN åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥åšä¸åŒçš„è®¡ç®—é‡. å®ƒçš„å¤§è‡´åŸç†å¾ˆç®€å•ï¼šå…è®¸ RNN åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥åšå¤šæ­¥è¿ç®—ã€‚ In order for the network to learn how many steps to do, we want the number of steps to be differentiable. We achieve this with the same trick we used before: instead of deciding to run for a discrete number of steps, we have an attention distribution over the number of steps to run. The output is a weighted combination of the outputs of each step. ä¸ºäº†è®©ç½‘ç»œå­¦ä¹ å¾—åˆ°æ¯ä¸€ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—æ­¥æ•°ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸ªè®¡ç®—æ­¥æ•°æ˜¯å¯å¾®çš„ï¼ˆä¹Ÿå°±æ˜¯æŠŠå…¶æ­¥æ•°å½“ä½œå­¦ä¹ å¾—åˆ°çš„å‚æ•°ï¼Œç„¶åé€šè¿‡åå‘ä¼ æ’­æ¥å­¦ä¹ è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¸ºäº†å®ç°è¿™ä¸ª trickï¼Œæˆ‘ä»¬é‡‡ç”¨ç±»ä¼¼ä¹‹å‰ NTM çš„æ–¹å¼ï¼Œç›¸æ¯”æ¯æ¬¡è®¡ç®—ä¸€ä¸ªç¦»æ•£çš„æ­¥æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨æ³¨æ„åŠ›åˆ†å¸ƒçš„æœºåˆ¶æ¥é€‰æ‹©æ­¥æ•°ï¼Œè¾“å‡ºçš„ç»“æœæ˜¯æ‰€æœ‰æ­¥çš„åŠ æƒæ±‚å’Œã€‚ There are a few more details, which were left out in the previous diagram. Hereâ€™s a complete diagram of a time step with three computation steps. ä¸Šå›¾ä¸­è¿˜æœ‰æ›´å¤šçš„ç»†èŠ‚ã€‚ä¸‹å›¾æ˜¯ RNN ä¸­çš„åœ¨ä¸€ä¸ªæ—¶é—´æ­¥ä¸­æœ‰3ä¸ªæ­¥æ•°çš„è®¡ç®—é‡ã€‚ Thatâ€™s a bit complicated, so letâ€™s work through it step by step. At a high-level, weâ€™re still running the RNN and outputting a weighted combination of the states: è¿™ä¸ªå›¾çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œæˆ‘ä»¬ä¼šä¸€æ­¥ä¸€æ­¥çš„è§£é‡Šå®ƒã€‚åœ¨æ›´é«˜çš„å±‚æ¬¡ï¼Œæˆ‘ä»¬ä¾ç„¶è¿è¡Œ RNN å¹¶è¾“å‡ºä¸€ä¸ªçŠ¶æ€çš„åŠ æƒä¹‹å’Œã€‚ S è¡¨ç¤º RNN ä¸­çš„éšè—çŠ¶æ€ã€‚ The weight for each step is determined by a â€œhalting neuron.â€ Itâ€™s a sigmoid neuron that looks at the RNN state and gives a halting weight, which we can think of as the probability that we should stop at that step. æ¯ä¸€æ­¥çš„æƒé‡ç”±ä¸€ä¸ªâ€œåœæ­¢ç¥ç»å…ƒâ€ç¡®å®šã€‚è¿™æ˜¯ä¸€ä¸ªsigmoidç¥ç»å…ƒï¼Œç”¨æ¥å…³æ³¨å½“å‰çš„ RNN çš„çŠ¶æ€ä»¥åŠèµ‹äºˆå®ƒä¸€ä¸ªæƒé‡ã€‚æˆ‘ä»¬å¯ä»¥çœ‹ä½œæ˜¯è¿™ä¸€æ­¥æ˜¯å¦åº”è¯¥åœæ­¢çš„æ¦‚ç‡ã€‚ We have a total budget for the halting weights of 1, so we track that budget along the top. When it gets to less than epsilon, we stop æˆ‘ä»¬è®¾æ€»çš„åœæ­¢æƒé‡ä¸º 1,ä¾æ¬¡å‡å»æ¯æ­¥è¾“å‡ºçš„ halt å€¼ï¼Œç›´åˆ°å‰©ä½™çš„ halt å€¼å°äº epsilon åå°±åœæ­¢ã€‚ When we stop, might have some left over halting budget because we stop when it gets to less than epsilon. What should we do with it? Technically, itâ€™s being given to future steps but we donâ€™t want to compute those, so we attribute it to the last step. å½“æˆ‘ä»¬ç»“æŸåï¼Œåº”è¯¥è¿˜ä¼šé—ç•™ä¸€äº›åœæ­¢å€¼ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨åœæ­¢å€¼å°äºepsilonæ—¶åœæ­¢çš„ã€‚æˆ‘ä»¬è¯¥æ€ä¹ˆå¤„ç†è¿™äº›å‰©ä½™çš„åœæ­¢å€¼ï¼Ÿä»æŠ€æœ¯ä¸Šè®²ï¼Œå®ƒä»¬åº”è¯¥ä¼ é€’åˆ°åé¢çš„è®¡ç®—æ­¥éª¤ä¸­å»ï¼Œä½†æˆ‘ä»¬ä¸æƒ³è®¡ç®—è¿™äº›å€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±æŠŠè¿™äº›å‰©ä½™çš„åœæ­¢å€¼å½’æ€»åˆ°æœ€åä¸€æ­¥ã€‚ When training Adaptive Computation Time models, one adds a â€œponder costâ€ term to the cost function. This penalizes the model for the amount of computation it uses. The bigger you make this term, the more it will trade-off performance for lowering compute time. å½“è®­ç»ƒ ACT æ¨¡å‹æ—¶ï¼Œéœ€è¦ç»™æŸå¤±å‡½æ•°åŠ ä¸Šä¸€ä¸ªæƒ©ç½šé¡¹ã€‚ç”¨æ¥æƒ©ç½šæ•´ä¸ªæ¨¡å‹ä¸­çš„æ€»çš„è®¡ç®—é‡ã€‚è¿™ä¸€é¡¹è¶Šå¤§æ—¶ï¼Œå®ƒä¼šåœ¨æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—é‡çš„æŠ˜è¡·ä¸­å€¾å‘ä¸å‡å°è®¡ç®—é‡ã€‚ Codeï¼š The only open source implementation of Adaptive Computation Time at the moment seems to be Mark Neumannâ€™s (TensorFlow). reference: Attention and Augmented Recurrent Neural Networks","link":"/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/"},{"title":"è®ºæ–‡ç¬”è®°, Attention Is All You Need","text":"Attention Is All You Need 1. paper reading1.1 IntroductionRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state htâˆ’1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. RNNæ¨¡å‹æœ‰ä¸¤ä¸ªå¾ˆè‡´å‘½çš„ç¼ºç‚¹ï¼š $$y_t=f(y_{t-1},x_t)$$ ä¸€æ˜¯æ— æ³•è§£å†³é•¿å¥å­çš„é•¿æœŸä¾èµ–çš„é—®é¢˜ï¼ˆè¿™æ˜¯å› ä¸ºåå‘ä¼ æ’­ï¼Œlossçš„æ¢¯åº¦å¾ˆéš¾ä¼ é€’åˆ°æ¯”è¾ƒé å‰çš„ä½ç½®ï¼Œé€ æˆå‰é¢çš„è¯å¯¹æ•´ä½“çš„å½±å“åå°ï¼Œ[Gradient flow in recurrent nets: the difficulty of learning long-term dependencies](http://www.bioinf.jku.at/publications/older/ch7.pdf)ï¼‰ï¼› äºŒæ˜¯è®¡ç®—æ— æ³•å¹¶è¡ŒåŒ–çš„é—®é¢˜ï¼ˆåä¸€ä¸ªæ—¶åˆ»çš„è®¡ç®—ä¾èµ–äºå‰ä¸€ä¸ªæ—¶åˆ»çš„è®¡ç®—ï¼‰ï¼Œå¯¼è‡´è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢ã€‚ Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network. Attentionæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³RNNæ— æ³•é•¿æ—¶é—´ä¾èµ–çš„é—®é¢˜ï¼Œä½†æ˜¯å¯¹äºæ— æ³•å¹¶è¡ŒåŒ–è®¡ç®—çš„é—®é¢˜ä¾æ—§å­˜åœ¨ã€‚ 2. Background2.1 Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]2.2 Self-attentionSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. A structured self-attentive sentence embedding 2.3 End-to-end memory networksEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks. End-to-end memory networks 2.4 TransformerTransformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. æœ¬æ–‡ä¸»è¦å’Œä»¥ä¸‹ä¸‰ç¯‡æ–‡ç« å¯¹æ¯”ï¼š Neural GPUs learn algorithms Neural machine translation in linear time Convolutional sequence to sequence learning Model Architectureåœ¨ä»¥å¾€çš„encoder-decoderæ¨¡å‹ä¸­ï¼š the encoder maps an input sequence of symbol representations $(x_1,â€¦,x_n)$ to a sequence of continuous representations $z = (z_1,â€¦,z_n)$. Given z, the decoder then generates an output sequence $(y_1,â€¦,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. ä»¥å¾€çš„attentionè™½ç„¶ä¹Ÿèƒ½è§£å†³long dependecyçš„é—®é¢˜ï¼Œä½†æ˜¯å—åˆ¶äºRNNçš„åŸå› ï¼Œæ¯ä¸€æ­¥çš„è®¡ç®—å¿…é¡»åœ¨ä¸Šä¸€æ—¶é—´æ­¥å®Œæˆåè¿›è¡Œã€‚å› æ­¤æ— æ³•å¹¶è¡Œè®¡ç®—ã€‚ Transformer ä¹Ÿæ˜¯ç”± encoder å’Œ decoder ç»„æˆã€‚ Encoderå…¶ä¸­ Encoder ç”±6ä¸ªå®Œå…¨ç›¸åŒçš„layerå †å ï¼ˆstackï¼‰è€Œæˆã€‚æ¯ä¸€å±‚layerç”±ä¸¤ä¸ª sub-layer ç»„æˆï¼Œåˆ†åˆ«æ˜¯ multi-head self-attention mechanism å’Œ point-wise fully connected feed-forward network. æ¯ä¸€ä¸ª sub-layer åº”ç”¨ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼ˆresidual connectionï¼‰,ç„¶åå†è¿æ¥ä¸€ä¸ª normalization å±‚ã€‚ Decoderè·Ÿ encoder éå¸¸ç±»ä¼¼ï¼ŒåŒæ ·ç”±6ç”±6ä¸ªå®Œå…¨ç›¸åŒçš„layerå †å è€Œæˆï¼Œä½†æ˜¯æ¯ä¸€å±‚æœ‰3ä¸ª sub-layerï¼Œ å¢åŠ äº†ä¸€ä¸ª multi-head attention. åŒæ ·çš„ä¹Ÿæœ‰æ®‹å·®é“¾æ¥å’Œnormalizationå±‚ã€‚ å¯¹ self-attention è¿›è¡Œäº†ä¿®æ”¹ï¼Œmasking: We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. ç›®çš„åº”è¯¥å°±æ˜¯è®©ä¸‹ä¸€ä¸ªtæ—¶åˆ»çš„ç”Ÿæˆè¯åªä¾èµ–äºtæ—¶åˆ»ä¹‹å‰çš„è¯ã€‚ AttentionReally love this short description of attention: An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Scaled Dot-Product Attention queries: $Q\\in R^{n\\times d_k}$ keys: $K\\in R^{n\\times d_k}$ values: $V\\in R^{n\\times d_v}$ è®¡ç®—å‘é‡å…§ç§¯ä½œä¸ºç›¸ä¼¼åº¦ï¼Œå¹¶ä½¿ç”¨softmaxè®¡ç®—æƒé‡ï¼Œç„¶ååŠ æƒæ±‚å’Œã€‚ è¿™ç§ attention å…¶å®ä¹Ÿå¾ˆå¸¸è§äº†ï¼Œè¿™é‡Œ google ç®—æ˜¯ç»™è¿™ç§ç»“æ„ä¸€ä¸ªå®˜æ–¹çš„åå­—å§ã€‚ è®ºæ–‡ä¸­ä½œè€…è¿˜å¯¹æ¯”äº†æ¯”è¾ƒå¸¸ç”¨çš„å¦ä¸€ç§attentionæœºåˆ¶ï¼Œ additive attention (Neural Machine Translation by Jointly Learning to Align and Translate è¿™ç¯‡éå¸¸ç»å…¸çš„æ–‡ç« ä¸­æå‡ºçš„)ï¼Œadditive æ˜¯ä½¿ç”¨çš„å‰é¦ˆç¥ç»ç½‘ç»œæ¥è®¡ç®— (å…·ä½“å…¬å¼å¯ä»¥çœ‹è¿™é‡Œcs224d-lecture10 æœºå™¨ç¿»è¯‘å’Œæ³¨æ„åŠ›æœºåˆ¶). è™½ç„¶ä»è®¡ç®—å¤æ‚åº¦ä¸Šæ¥è®²ï¼Œä¸¤è€…æ˜¯å·®ä¸å¤šçš„ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ dot-product æ›´å¿«ï¼Œè€Œä¸”ç©ºé—´å¤æ‚åº¦æ›´ä½ï¼Œå› ä¸ºå¯ä»¥é€šè¿‡çŸ©é˜µä¼˜åŒ–è®¡ç®—ã€‚å…³äºattentionæœºåˆ¶çš„å¯¹æ¯”å¯å‚è€ƒMassive Exploration of Neural Machine Translation Architectures æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œä½¿ç”¨äº†å½’ä¸€åŒ–ï¼Œä¹Ÿå°±æ˜¯ Scaled. å½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œ additive attention çš„æ•ˆæœè¦ä¼˜äº dot-product attentionï¼Œ ä½œè€…æ€€ç–‘(suspect)æ˜¯å½“ $d_k$ å¤ªå¤§æ—¶ï¼Œé€šè¿‡ softmax è®¡ç®—å¾—åˆ°çš„æƒé‡éƒ½ä¼šå¾ˆæ¥è¿‘0æˆ–1,å¯¼è‡´æ¢¯åº¦å¾ˆå°ã€‚ $q\\cdot k=\\sum_{i=1}^{d_k}q_ik_i$ å½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œ$q\\cdot k$ çš„æ–¹å·®ä¹Ÿä¼šå¾ˆå¤§ã€‚ Multi-Head Attention æ¥ä¸‹æ¥æŒ‰ç…§æ•´ä¸ªæ¨¡å‹çš„æ•°æ®æµè¿‡ç¨‹æ¥ä»‹ç»æ¨¡å‹ä¸­çš„æ¯ä¸€ä¸ªæ¨¡å—ã€‚ Components and Trainingå‰é¢ä¸‰éƒ¨åˆ† Encoder, Decoder, Attention ç»„æˆäº† Transformer æ¨¡å‹çš„åŸºæœ¬æ¶æ„ã€‚å…¶ä¸­å…·ä½“ç»†èŠ‚ï¼Œä»¥åŠ Training å®ç°è¿‡ç¨‹å°†é€šè¿‡ä»£ç å®ç°ã€‚ EncoderStage1Training data and batchingWMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. ä½œè€…ä½¿ç”¨äº†ï¼š byte-pair è¿™ç¯‡è®ºæ–‡ä¸­æœ‰ä»‹ç»ï¼šMassive exploration of neural machine translation architectures word-piece Googleâ€™s neural machine translation system: Bridging the gap between human and machine translation è¿™é‡Œæˆ‘å°†ä½¿ç”¨åˆ›æ–°å·¥å‚ä¸¾åŠçš„ challenge.ai çš„ä¸­è‹±æ–‡æ¯”èµ›æ•°æ®ï¼šhttps://challenger.ai/datasets/translation è¿™é‡Œæš‚æ—¶å…ˆä¸ç®¡æ•°æ®é¢„å¤„ç†ï¼Œåœ¨æ¨¡å‹ä¸­ä½¿ç”¨å ä½ç¬¦ placeholder. 12345678910111213# add placeholdersself.input_x = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len])self.input_y = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len])# define decoder inputsself.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:1])*2, self.input_y[:,:-1]],axis=-1) # 2:&lt;S&gt; è¿™é‡Œçš„sentence_len æŒ‡çš„æ˜¯æºè¯­è¨€å¥å­çš„æœ€å¤§é•¿åº¦å’Œç›®æ ‡è¯­è¨€å¥å­çš„æœ€å¤§é•¿åº¦ã€‚é•¿åº¦ä¸è¶³çš„éœ€è¦zero padding. decoder ä¸­self-attentionçš„ query, keys, values éƒ½æ˜¯ç›¸åŒçš„ï¼Œåˆå§‹å€¼æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œshape ä¸ self.input_y ä¸€è‡´å³å¯ã€‚ Embeddingwe use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. In our model, we share the same weight matrix between the two embedding layers. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def embedding(inputs, vocab_size, num_units, zero_pad=True, scale=True, reuse=None): &quot;&quot;&quot; :param inputs: A `Tensor` with type `int32` or `int64` containing the ids to be looked up in `lookup table`. shape is [batch, sentence_len] :param vocab_size: vocabulary size :param num_units: Number of embedding hidden units. in the paper, it is called d_model :param zero_pad: If True, all the values of the fist row (id 0) should be constant zeros. :param scale: If True. the outputs is multiplied by sqrt num_units. :param reuse: Boolean, whether to reuse the weights of a previous layer by the same name. :return: A `Tensor` with one more rank than inputs's. The last dimensionality should be `num_units`. &quot;&quot;&quot; with tf.variable_scope(&quot;embedding-layer&quot;, reuse=reuse): embedding = tf.get_variable(&quot;embedding&quot;, [vocab_size, num_units], initializer=xavier_initializer()) if zero_pad: embedding = tf.concat([tf.zeros([1, num_units]), embedding[1:, :]], axis=0) # index=0 for nil word output = tf.nn.embedding_lookup(embedding, inputs) # [batch, sentence_len, num_units] if scale: output = output * np.sqrt(num_units) return output é€šå¸¸embeddingæˆ‘ä»¬åœ¨å†™çš„å‚æ•°è¾“å…¥ vocab_size å’Œ num_units(ä¹Ÿå°±æ˜¯ embed_size)ï¼Œä½†æœºå™¨ç¿»è¯‘ä¸­è®¾è®¡åˆ°ä¸¤ç§è¯­è¨€ï¼Œç›´æ¥å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå¹¶å°†inputä½œä¸ºè¾“å…¥ä¼šè®©ç¨‹åºæ›´ç®€æ´å§ã€‚ã€‚ è¿™é‡Œå°†vocabulary ä¸­index=0çš„è®¾ç½®ä¸º constant 0, ä¹Ÿå°±æ˜¯ä½œä¸º input ä¸­çš„ zero padding çš„è¯å‘é‡ã€‚ å½’ä¸€åŒ–ï¼Œé™¤ä»¥ np.sqrt(num_units). ä¸æ‡‚ä¸ºä½•è¦è¿™ä¹ˆåšï¼Ÿæœ‰è®ºæ–‡ç ”ç©¶è¿‡å—ï¼Ÿ position encoding pos æ˜¯wordåœ¨å¥å­ä¸­çš„ä½ç½®ï¼Œ i æ˜¯å¯¹åº” $d_{model}$ è¯å‘é‡ä¸­çš„ç¬¬ i ç»´ã€‚ That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2Ï€ to 10000 Â· 2Ï€. ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ç½®ç¼–ç çš„æ¯ä¸ªç»´åº¦å¯¹åº”äºæ­£å¼¦æ›²çº¿ã€‚æ³¢é•¿å½¢æˆä»2Ï€åˆ°10000Â·2Ï€çš„å‡ ä½•çº§æ•°ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127def position_encoding_mine(n_position, d_model): &quot;&quot;&quot; Init the sinusoid position encoding table. :param n_position: the lenght of sentence :param d_model: the same with embedding :return: &quot;&quot;&quot; # keep dim -1 for padding token position encoding zero vector # pos=-1 ç”¨äº padded zero vector encoding = np.zeros([n_position, d_model], np.float32) for pos in range(1, n_position): for i in range(0, d_model): encoding[pos, i] = pos /np.power(10000, 2.*i/d_model) encoding[1:-2, 0::2] = np.sin(encoding[1:-2, 0::2]) # dim 2i encoding[1:-2, 1::2] = np.cos(encoding[1:-2, 1::2]) # dim 2i+1 return encodingdef positional_encoding(inputs, num_units, zero_pad=True, scale=True, scope=&quot;positional_encoding&quot;, reuse=None): '''Sinusoidal Positional_Encoding. Args: inputs: A 2d Tensor with shape of (N, T). num_units: Output dimensionality zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper) scope: Optional scope for `variable_scope`. reuse: Boolean, whether to reuse the weights of a previous layer by the same name. Returns: A 'Tensor' with one more rank than inputs's, with the dimensionality should be 'num_units' ''' N, T = inputs.get_shape().as_list() # N means batch_size, T means the sentence length. with tf.variable_scope(scope, reuse=reuse): position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # [N, T] # First part of the PE function: sin and cos argument position_enc = np.array([ [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)] for pos in range(T)]) # [T, num_units] # Second part, apply the cosine to even columns and sin to odds. position_enc[:, 0::2] = np.sin(position_enc[:, 0::2]) # dim 2i position_enc[:, 1::2] = np.cos(position_enc[:, 1::2]) # dim 2i+1 # Convert to a tensor lookup_table = tf.convert_to_tensor(position_enc, dtype=tf.float32) if zero_pad: lookup_table = tf.concat((tf.zeros(shape=[1, num_units]), lookup_table[1:, :]), axis=0) outputs = tf.nn.embedding_lookup(lookup_table, position_ind) # [N, T, num_units] if scale: outputs = outputs * num_units**0.5 return outputs å…³äº position encoding çš„è§£é‡Šï¼Œå¯ä»¥å‚è€ƒè¿™ç¯‡blog The Transformer â€“ Attention is all you need. In RNN (LSTM), the notion of time step is encoded in the sequence as inputs/outputs flow one at a time. In FNN, the positional encoding must be preserved to represent the time in some way to preserve the positional encoding. In case of the Transformer authors propose to encode time as sine wave, as an added extra input. Such signal is added to inputs and outputs to represent time passing. In general, adding positional encodings to the input embeddings is a quite interesting topic. One way is to embed the absolute position of input elements (as in ConvS2S). However, authors use â€œsine and cosine functions of different frequenciesâ€. The â€œsinusoidalâ€ version is quite complicated, while giving similar performance to the absolute position version. The crux is however, that it may allow the model to produce better translation on longer sentences at test time (at least longer than the sentences in the training data). This way sinusoidal method allows the model to extrapolate to longer sequence lengths. è¯´çœŸçš„ï¼Œè¿˜æ˜¯ä¸å¤ªç†è§£ã€‚ã€‚ã€‚ å¯è§†åŒ– encoding çŸ©é˜µ: 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport matplotlib.pyplot as pltnum_units = 100sentence_len = 10i = np.tile(np.expand_dims(range(num_units), 0), [sentence_len, 1]) # (100,)-&gt; (1, 100) -&gt;(10, 100)pos = np.tile(np.expand_dims(range(sentence_len), 1), [1, num_units]) #(10,)-&gt; (10, 1) -&gt; (10, 100)pos = np.multiply(pos, 1/10000.0)i = np.multiply(i, 2.0/num_units)matrix = np.power(pos, i)matrix[:, 1::2] = np.sin(matrix[:, 1::2])matrix[:, ::2] = np.cos(matrix[:, ::2])im = plt.imshow(matrix, aspect='auto')plt.show() Stage2scaled dot-product attention$$Attention(Q,K,V)=softmax\\dfrac{QK^T}{\\sqrt d_k}V$$ Multi-head attentionTransformer reduces the number of operations required to relate (especially distant) positions in input and output sequence to a O(1). However, this comes at cost of reduced effective resolution because of averaging attention-weighted positions. h = 8 attention layers (aka â€œheadsâ€): that represent linear projection (for the purpose of dimension reduction) of key K and query Q into $d_k$-dimension and value V into $d_v$-dimension: $$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\\dots,h$$ å…¶ä¸­ï¼š $$W^Q_i, W^K_i\\in\\mathbb{R}^{d_{model}\\times d_k}, W^V_i\\in\\mathbb{R}^{d_{model}\\times d_v}, for\\ d_k=d_v=d_{model}/h = 64$$ scaled-dot attention applied in parallel on each layer (different linear projections of k,q,v) results in $d_v$-dimensional output. concatenate outputs of each layer (different linear projection; also referred as â€headâ€): Concat$(head_1,â€¦,head_h)$ linearly project the concatenation result form the previous step: $$MultiHeadAttention(Q,K,V) = Concat(head_1,\\dots,head_h) W^O$$ where $W^0\\in\\mathbb{R}^{d_{hd_v}\\times d_{model}}$ å…³äº attention åœ¨æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œæœ‰ä¸‰ç§æƒ…å†µ 1.In â€œencoder-decoder attentionâ€ layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoderã€‚ 2.The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. 3.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’1) all values in the input of the softmax which correspond to illegal connections. æ€»ç»“ä¸‹å°±æ˜¯ï¼š Transformer ä¸­çš„attentionæœºåˆ¶æ€»å…±æœ‰ä¸‰ç§æƒ…å†µï¼š 1.encoderæ¨¡å—ä¸­çš„ self-attentionï¼Œå…¶ä¸­ queries, keys, values éƒ½æ˜¯æ¥è‡ª input_x, ä¹Ÿå°±æ˜¯æºè¯­è¨€çš„è¯è¡¨ç¤ºã€‚é€šè¿‡å¤šå±‚ multi-head attention, FFN, å¾—åˆ°æœ€åçš„ input sentence çš„å‘é‡è¡¨ç¤ºï¼Œåœ¨æ²¡æœ‰ä½¿ç”¨RNNï¼ŒCNNçš„æƒ…å†µä¸‹ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯éƒ½åŒ…å«äº†å…¶ä»–æ‰€æœ‰è¯çš„ä¿¡æ¯ï¼Œè€Œä¸”æ•ˆæœæ¯” RNNï¼ŒCNN å¾—åˆ°çš„å‘é‡è¡¨ç¤ºè¦å¥½ã€‚ 2.encoder-encoderæ¨¡å—ä¸­çš„ attention. å…¶ä¸­ queries æ¥è‡ªä¸Šä¸€ä¸ªsub-layer, ä¹Ÿå°±æ˜¯ decoder ä¸­ masked multi-head attention çš„è¾“å‡ºï¼Œkeys-values æ¥è‡ª encoder çš„è¾“å‡ºã€‚ 3.decoderæ¨¡å—ä¸­çš„ self-attentionï¼Œå…¶ä¸­ queries, keys, values éƒ½æ˜¯æ¥è‡ªäºä¸Šä¸€ä¸ª decoder çš„è¾“å‡ºã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193def multiheadattention(q, k, v, d_model, heads, keys_mask=None, causality=None, dropout_keep_prob=0.1, is_training=True): &quot;&quot;&quot; multi scaled dot product attention :param q: A 3d tensor with shape of [batch, length_q, d_k]. :param k: A 3d tensor with shape of [batch, lenght_kv, d_k]. :param v: :param heads:An int. Number of heads. :param dropout_keep_prob: :param causality: If true, units that reference the future are masked. :return: &quot;&quot;&quot; # 1. Linear projections with tf.variable_scope('linear-projection-multiheads'): q_proj = tf.layers.dense(q, d_model) # [batch, lenght_q, d_model] k_proj = tf.layers.dense(k, d_model) # [batch, lenght_kv, d_model] v_proj = tf.layers.dense(v, d_model) # [batch, lenght_kv, d_model] with tf.variable_scope(&quot;multihead-attention&quot;): # d_k = d_v = d_model/heads if d_model % heads != 0: raise ValueError(&quot;Key\\values\\query depth (%d) must be divisible by&quot; &quot;the number of attention heads (%d)&quot; %(d_model, heads)) # 2. split and concat q_ = tf.concat(tf.split(q_proj, heads, axis=2), axis=0) # [batch*heads, length_q, d_k] k_ = tf.concat(tf.split(k_proj, heads, axis=2), axis=0) # [batch*heads, length_kv, d_k] v_ = tf.concat(tf.split(v_proj, heads, axis=2), axis=0) # [batch*heads, length_kv, d_v] # 3. attention score # outputs.shape=[batch*heads, length_q, length_kv] # è¦ç†è§£è¿™ä¸ªçŸ©é˜µè¿ç®—ï¼Œå¯¹ä¸€ä¸ªkeysçš„å¥å­é•¿åº¦ä¸ºlength_kv,éœ€è¦è®¡ç®—çš„å…¶ä¸­çš„æ¯ä¸€ä¸ªè¯ä¸queryä¸­æ¯ä¸€ä¸ªè¯çš„å…§ç§¯ã€‚æ‰€ä»¥æœ€åçš„scoreæ˜¯[length_q, lenght_kv] scalar = tf.rsqrt(d_model/heads) # 1/sqrt(d_k) outputs = tf.matmul(q_*scalar, k_, transpose_b=True) # [batch*heads, length_q, lenght_kv] # 4. mask if keys_mask is not None: # `y = sign(x) = -1` if `x &lt; 0`; 0 if `x == 0` or `tf.is_nan(x)`; 1 if `x &gt; 0`. key_masks = tf.sign(tf.abs(tf.reduce_sum(k, axis=-1))) # (batch, length_kv) key_masks = tf.tile(key_masks, [heads, 1]) # (batch*heads, length_kv) key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, q.get_shape()[1], 1]) # (batch*heads, length_q, length_kv) # def where(condition, x=None, y=None, name=None) # The `condition` tensor acts as a mask that chooses, based on the value at each # element, whether the corresponding element / row in the output should be taken # from `x` (if true) or `y` (if false). paddings = tf.ones_like(outputs) * (-2 ** 32 + 1) outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # [batch*heads, length_q, lenght_kv] # Causality = Future blinding # causalityå‚æ•°å‘ŠçŸ¥æˆ‘ä»¬æ˜¯å¦å±è”½æœªæ¥åºåˆ—çš„ä¿¡æ¯ï¼ˆè§£ç å™¨self attentionçš„æ—¶å€™ä¸èƒ½çœ‹åˆ°è‡ªå·±ä¹‹åçš„é‚£äº›ä¿¡æ¯ï¼‰ï¼Œ # è¿™é‡Œå³causalityä¸ºTrueæ—¶çš„å±è”½æ“ä½œã€‚ if causality: diag_vals = tf.ones_like(outputs[0, :, :]) # [length_q, lenght_kv] tril = LinearOperatorLowerTriangular(diag_vals).to_dense() # [length_q, lenght_kv] å¾—åˆ°ä¸€ä¸ªä¸‰è§’é˜µï¼Œä¸‹æ ‡indexå¤§äºå½“å‰è¡Œçš„å€¼éƒ½å˜ä¸º0 masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # [batch*heads, length_q, lenght_kv] paddings = tf.ones_like(masks) * (-2 ** 32 + 1) outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # [batch*heads, length_q, lenght_kv] # å°†socreè½¬æ¢ä¸ºæ¦‚ç‡ outpts = tf.nn.softmax(outputs) # Query Masking query_mask = tf.sign(tf.abs(tf.reduce_sum(q, axis=-1, keepdims=False))) # [batch, lenght_q] query_mask = tf.tile(query_mask, [heads, 1]) # [batch*heads, length_q] # ç›®çš„æ˜¯ä¸ºäº†è®©queryå’Œoutputsä¿æŒå½¢çŠ¶ä¸€è‡´ query_mask = tf.tile(tf.expand_dims(query_mask, axis=-1), [1, 1, tf.shape(k)[-1]]) # [batch*heads, length_q, length_kv] paddings = tf.ones_like(outputs) * (-2 ** 32 + 1) outputs = tf.where(tf.equal(query_mask, 0), paddings, outputs) # [batch*heads, length_q, length_kv] # Dropout if is_training: outputs = tf.layers.dropout(outputs, dropout_keep_prob, ) # weights sum outputs = tf.matmul(outputs, v_) # [batch*heads, length_q, k_v] # restore shape outputs = tf.concat(tf.split(outputs, heads, axis=0), axis=-1) #[batch,length_q, k_v*heads] = [batch, lenght_q, d_model] # Residual connection outputs += q # [batch, lenght_q, d_model] # Normalize outputs = Normalize(outputs) return outputs # [batch, length_q, d_model] å…³äºä»£ç çš„è¯¦ç»†è§£æï¼Œå¯ä»¥çœ‹è¿™ç¯‡blog æœºå™¨ç¿»è¯‘æ¨¡å‹Transformerä»£ç è¯¦ç»†è§£æ. Stage3: Position-wise Feed-Forward Networks$$FFN(x) = MAX(0, xW_1+b_1)W_2+b_2$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109def position_wise_feed_forward(inputs, num_units1=2048, num_units2=512, reuse=None): &quot;&quot;&quot; Point-wise feed forward net. :param inputs: A 3D tensor with shape of [batch, length_q, d_model] :param num_units1: A integers. :param num_units2: A integers :param reuse: Boolean, whether to reuse the weights of a previous layer by the same name. :return: A 3d tensor with the same shape and dtype as inputs &quot;&quot;&quot; with tf.variable_scope(&quot;feed-forward-networks&quot;): # inner layers params1 = {&quot;inputs&quot;:inputs, &quot;filters&quot;:num_units1, &quot;kernel_size&quot;:1, &quot;activation&quot;:tf.nn.relu, &quot;use_bias&quot;:True, &quot;strides&quot;:1} outputs = tf.layers.conv1d(**params1) # readout layer params2 = {&quot;inputs&quot;:outputs, &quot;filters&quot;:num_units2, &quot;kernel_size&quot;:1, &quot;activation&quot;:None, &quot;use_bias&quot;:True, &quot;strides&quot;:1} outputs = tf.layers.conv1d(**params2) # residual connection outputs += inputs # Normalize outputs = Normalize(outputs) return outputsdef position_wise_feed_forward_mine(inputs, num_units1=2048, num_units2=512, reuse=None): with tf.variable_scope(&quot;feed-forward-networks&quot;): W1 = tf.get_variable(&quot;weight1&quot;, [inputs.get_shape()[-1], num_units1],initializer=xavier_initializer()) b1 = tf.get_variable('bias1', [num_units1], initializer=tf.constant_initializer(0.1)) outputs = tf.einsum('aij,jk-&gt;aik', inputs, W1) + b1 # [batch, length_q, num_units1] W2 = tf.get_variable(&quot;weight1&quot;, [outputs.get_shape()[-1], num_units2], initializer=xavier_initializer()) b2 = tf.get_variable('bias1', [num_units2], initializer=tf.constant_initializer(0.1)) outputs = tf.einsum('aij,jk-&gt;aik', inputs, W2) + b2 # [batch, length_q, num_units1] # residual connection outputs += inputs # Normalize outputs = Normalize(outputs) return outputs encoder å„æ¨¡å—ç»„åˆåœ¨ä¸€èµ·1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677def _encoder(self): with tf.variable_scope(&quot;encoder&quot;): # 1. embedding with tf.variable_scope(&quot;embedding-layer&quot;): self.enc = embedding(inputs=self.input_x, vocab_size=self.vocab_size_cn, num_units=self.d_model, scale=True) # [batch, sentence_len, d_model] # 2. position encoding with tf.variable_scope(&quot;position_encoding&quot;): encoding = position_encoding_mine(self.enc.get_shape()[1], self.d_model) self.enc *= encoding # 3.dropout self.enc = tf.layers.dropout(self.enc, rate=self.dropout_keep_prob, training=self.is_training) # 4. Blocks for i in range(self.num_layers): with tf.variable_scope(&quot;num_layer_{}&quot;.format(i)): # multihead attention # encoder: self-attention self.enc = multiheadattention(q=self.enc, k=self.enc, v=self.enc, d_model=self.d_model, heads=self.heads, causality=False, dropout_keep_prob=self.dropout_keep_prob, is_training=True) # Feed Froward self.enc = position_wise_feed_forward(self.enc, num_units1= 4*self.d_model, num_units2= self.d_model, reuse=False) return self.enc Decoderdecoder æ¨¡å—ä¸­ self-attention çš„åˆå§‹è¾“å…¥ï¼š 12345# define decoder inputsself.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:1])*2, self.input_y[:,:-1]],axis=-1) # 2:&lt;S&gt; ä¸encoder ä¸åŒçš„æ˜¯ï¼Œåˆ†ä¸º encoder-decoder attention å’Œ self-attention. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def _decoder(self): with tf.variable_scope(&quot;decoder&quot;): # embedding self.dec = embedding(self.decoder_inputs, vocab_size=self.vocab_size_en, num_units=self.d_model) # [batch, sentence_len, d_model] # position decoding encoding = position_encoding_mine(self.dec.get_shape()[1], self.d_model) self.dec *= encoding # blocks for i in range(self.num_layers): with tf.variable_scope(&quot;num_layers_{}&quot;.format(i)): # self-attention with tf.variable_scope(&quot;self.attention&quot;): self.dec = multiheadattention(q=self.dec, k=self.dec, v=self.dec, d_model=self.d_model, heads=self.heads, keys_mask=True, causality=True) # encoder-decoder-attention with tf.variable_scope(&quot;encoder-decoder-attention&quot;): self.dec = multiheadattention(q=self.dec, k=self.enc, v=self.enc, d_model=self.d_model, heads=self.heads, keys_mask=True, causality=True) self.dec = position_wise_feed_forward(self.dec, num_units1= 4*self.d_model, num_units2= self.d_model) # [batch, sentence_len, d_model] return self.dec Optimizer 123456789def add_train_op(self): self.optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.9, beta2=0.98, epsilon=1e-9) self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step) return self.train_op RegularizationResidual Dropoutlabel smoothingDuring training, we employed label smoothing of value \u000fls = 0:1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 12345678910111213141516171819def label_smoothing(inputs, epsilon=0.1): &quot;&quot;&quot; Applies label smoothing. See https://arxiv.org/abs/1512.00567 :param inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary. :param epsilon: Smoothing rate. For example, &quot;&quot;&quot; K = inputs.get_shape().as_list()[-1] # number of channels return ((1-epsilon) * inputs) + (epsilon/K) Reference: Attention and Augmented Recurrent Neural Networks The Transformer â€“ Attention is all you need. æœºå™¨ç¿»è¯‘æ¨¡å‹Transformerä»£ç è¯¦ç»†è§£æ","link":"/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/"},{"title":"Dynamic Routing Between Capsules","text":"paper: Dynamic Routing Between Capsules blog: Understanding Hintonâ€™s Capsule Networks. æµ…æç¬¬ä¸€ç¯‡Capsuleï¼šDynamic Routing Between Capsules Part 1, IntutionCNN æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ æµ…å±‚çš„å·ç§¯å±‚ä¼šæ£€æµ‹ä¸€äº›ç®€å•çš„ç‰¹å¾ï¼Œä¾‹å¦‚è¾¹ç¼˜å’Œé¢œè‰²æ¸å˜ã€‚ æ›´é«˜å±‚çš„ layer ä½¿ç”¨å·ç§¯æ“ä½œå°†ç®€å•çš„ç‰¹å¾åŠ æƒåˆå¹¶åˆ°æ›´å¤æ‚çš„ç‰¹å¾ä¸­ã€‚ æœ€åï¼Œç½‘ç»œé¡¶éƒ¨çš„ç½‘ç»œå±‚ä¼šç»“åˆè¿™äº›éå¸¸é«˜çº§çš„ç‰¹å¾å»åšåˆ†ç±»é¢„æµ‹ã€‚ åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œéœ€è¦ç†è§£çš„æ˜¯ï¼ŒCNN æ˜¯é€šè¿‡åŠ æƒæ±‚å’Œçš„æ–¹å¼å°†ä½å±‚æ¬¡çš„ç‰¹å¾ç»„åˆåˆ°é«˜å±‚æ¬¡çš„ç‰¹å¾çš„(weighted, added, nonlinear)ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œç®€å•çš„ç‰¹å¾åœ¨ç»„åˆæˆæ›´å¤æ‚çš„ç‰¹å¾ä¹‹å‰ï¼Œä»–ä»¬ä¹‹é—´æ˜¯å­˜åœ¨ä½ç½®å…³ç³»çš„(pose translation and ratational)ã€‚ å¯¹äºè¿™ä¸ªä½ç½®å…³ç³»çš„é—®é¢˜ï¼ŒCNN æ˜¯é€šè¿‡ max pooling/successive convolution layer æ¥è§£å†³è¿™ä¸ªé—®é¢˜çš„ã€‚é€šè¿‡å‡å°å›¾åƒçš„å°ºå¯¸ï¼Œå¢åŠ é«˜å±‚ç¥ç»å…ƒçš„æ„Ÿå—é‡ï¼ˆfield of viewï¼‰ï¼Œè¿™ä½¿å¾—ä»–ä»¬èƒ½åœ¨æ›´å¤§çš„åŒºåŸŸå†…æ£€æµ‹å‡ºæ›´é«˜é˜¶çš„ç‰¹å¾ã€‚ CNN å­˜åœ¨çš„é—®é¢˜ä½†æ˜¯ä¸è¦è¢« CNN çš„è¡¨ç°å¥½æ‰€è¿·æƒ‘ï¼Œå°½ç®¡ CNN çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚Hintonè®¤ä¸ºï¼šmax poolingå·¥ä½œå¾—è¿™ä¹ˆå¥½å…¶å®æ˜¯ä¸€ä¸ªå¤§ç¾éš¾ã€‚ Hinton: â€œThe pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.â€ å°±ç®—ä½ ä¸ä½¿ç”¨max poolingï¼Œä¼ ç»ŸCNNä¾ç„¶å­˜åœ¨è¿™æ ·çš„å…³é”®é—®é¢˜ï¼Œå­¦ä¹ ä¸åˆ°ç®€å•ä¸å¤æ‚å¯¹è±¡ä¹‹é—´çš„é‡è¦çš„ç©ºé—´å±‚æ¬¡ç‰¹å¾ã€‚ Internal data representation of a convolutional neural network does not take into account important spatial hierarchies between simple and complex objects. CNNåªå…³æ³¨è¦æ£€æµ‹çš„ç›®æ ‡æ˜¯å¦å­˜åœ¨ï¼Œè€Œä¸å…³æ³¨è¿™äº›ç»„ä»¶ä¹‹é—´çš„ä½ç½®å’Œç›¸å¯¹çš„ç©ºé—´å…³ç³»ã€‚å¦‚ä¸‹ä¾‹ï¼ŒCNNåˆ¤æ–­äººè„¸åªéœ€è¦æ£€æµ‹å‡ºå®ƒæ˜¯å¦å­˜åœ¨ä¸¤ä¸ªçœ¼ç›ï¼Œä¸¤ä¸ªçœ‰æ¯›ï¼Œä¸€ä¸ªé¼»å­å’Œä¸€ä¸ªå˜´å”‡ï¼Œç°åœ¨æˆ‘ä»¬æŠŠå³çœ¼å’Œå˜´å·´æ¢ä¸ªä½ç½®ï¼ŒCNNä¾ç„¶è®¤ä¸ºè¿™æ˜¯ä¸ªäººã€‚ ä¸ªäººç†è§£ï¼šç¥ç»ç½‘ç»œèƒ½å¦å…·å¤‡æ¨¡ç³Šè¯†åˆ«çš„èƒ½åŠ›ï¼Ÿæ¯”å¦‚ä»…ä»…æ˜¯é¼»å­å’Œçœ¼ç›è°ƒæ¢äº†ä½ç½®ï¼Œä½†æ˜¯å¤§è‡´çœ‹èµ·æ¥ä¾ç„¶æ˜¯ä¸€å¼ äººè„¸ã€‚ç¥ç»ç½‘ç»œèƒ½å¦åƒæˆ‘ä»¬äººç±»ä¸€æ ·ï¼ŒçŸ¥é“è¿™å¼ å›¾ç‰‡æƒ³å»è¡¨è¾¾ä¸€å¼ äººè„¸ï¼Œåªä¸è¿‡æœ‰äº›åœ°æ–¹é™¤äº†é—®é¢˜ï¼Œé‚£ä¹ˆå®ƒæ˜¯å¦çŸ¥é“é—®é¢˜å‡ºåœ¨å“ªå„¿äº†å‘¢ï¼Ÿè€Œä¸ä»…ä»…æ˜¯è¯†åˆ«çš„é—®é¢˜å¯¹å§ï¼Ÿ å†æ¯”å¦‚åœ¨æ–‡æœ¬é¢†åŸŸï¼Œä¸€ä¸¤ä¸ªå­—é¢ å€’é¡ºåºå¹¶ä¸å½±å“æˆ‘ä»¬äººç±»é˜…è¯»ï¼Œç¥ç»ç½‘ç»œä¹Ÿèƒ½ç†è§£å®ƒçš„æ„æ€ï¼Œä½†æ˜¯èƒ½å¦å‡†ç¡®çš„çŸ¥é“è¿™ä¸¤ä¸ªå­—é¡ºåºæ˜¯é¢ å€’çš„å‘¢ï¼Ÿ è¿™èƒ½å¦ä½œä¸ºä¸€ä¸ªè¯¾é¢˜ã€‚ã€‚ç¥ç»ç½‘ç»œçº é”™ã€‚ã€‚å½“ç„¶ï¼Œå¦‚æœç»™å‡ºäº†é”™è¯¯çš„æ•°æ®é›†ï¼Œé‚£ä¹Ÿå°±æ˜¯ç›‘ç£å­¦ä¹ ä»¥åŠåˆ†ç±»çš„é—®é¢˜ï¼Œèƒ½ä¸èƒ½åœ¨æ²¡æœ‰ç»™å‡ºè¿™ä¸ªæœ‰éƒ¨åˆ†é”™è¯¯çš„æ•°æ®æƒ…å†µä¸‹ï¼Œä¾æ—§è¯†åˆ«å‡ºæ¥å‘¢ï¼Ÿ CNN å¯¹æ—‹è½¬ä¸å…·å¤‡ä¸å˜æ€§ï¼Œå­¦ä¸åˆ° 3D ç©ºé—´ä¿¡æ¯ã€‚ä¾‹å¦‚ä¸‹é¢çš„è‡ªç”±å¥³ç¥ï¼Œæˆ‘ä»¬åªçœ‹è‡ªç”±å¥³ç¥çš„ä¸€å¼ ç…§ç‰‡ï¼Œæˆ‘ä»¬å¯èƒ½æ²¡æœ‰çœ‹è¿‡å…¶å®ƒè§’åº¦çš„ç…§ç‰‡ï¼Œè¿˜æ˜¯èƒ½åˆ†è¾¨å‡ºè¿™äº›æ—‹è½¬åçš„ç…§ç‰‡éƒ½æ˜¯è‡ªç”±å¥³ç¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå›¾åƒçš„å†…éƒ¨è¡¨å¾ä¸å–å†³äºæˆ‘ä»¬çš„è§†è§’ã€‚ä½†æ˜¯CNNåšè¿™ä¸ªäº‹æƒ…å¾ˆå›°éš¾ï¼Œå› ä¸ºå®ƒæ— æ³•ç†è§£3Dç©ºé—´ã€‚ å¦å¤–ï¼Œç¥ç»ç½‘ç»œä¸€èˆ¬éœ€è¦å­¦ä¹ æˆåƒä¸Šä¸‡ä¸ªä¾‹å­ã€‚äººç±»å­¦ä¹ æ•°å­—ï¼Œå¯èƒ½åªéœ€è¦çœ‹å‡ åä¸ªä¸ªä¾‹å­ï¼Œæœ€å¤šå‡ ç™¾ä¸ªï¼Œå°±èƒ½åŒºåˆ«æ•°å­—ã€‚ä½†æ˜¯CNNéœ€è¦æˆåƒä¸Šä¸‡ä¸ªä¾‹å­æ‰èƒ½è¾¾åˆ°æ¯”è¾ƒå¥½çš„æ•ˆæœï¼Œå¼ºåˆ¶å»å­¦ä¹ ã€‚å¹¶ä¸”å…³äºå‰é¢çš„æ—‹è½¬ä¸å˜æ€§ï¼ŒCNNå¯ä»¥é€šè¿‡å¢å¼ºæ•°æ®çš„æ‰‹æ®µå»æ”¹å–„ï¼Œä½†æ˜¯è¿™ä¹Ÿå°±éœ€è¦ç”¨åˆ°å¤§é‡çš„æ•°æ®é›†ã€‚ Hardcoding 3D World into a Neural Net: Inverse Graphics ApproachCapsules å°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ã€‚å…¶çµæ„Ÿæ¥æºäºè®¡ç®—æœºå›¾å½¢å­¦ä¸­çš„æ¸²æŸ“æŠ€æœ¯ã€‚ Computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data. åœ¨è®¡ç®—æœºå›¾å½¢é¢†åŸŸï¼Œæ˜¯é€šè¿‡å‡ ä½•æ•°æ®çš„å†…éƒ¨å±‚æ¬¡è¡¨ç¤ºæ¥é‡æ„ä¸€ä¸ªè§†è§‰å›¾åƒçš„ã€‚è¿™é¡¹æŠ€æœ¯å« â€œæ¸²æŸ“ (rendering)â€. Inspired by this idea, Hinton argues that brains, in fact, do the opposite of rendering. He calls it inverse graphics: from visual information received by eyes, they deconstruct a hierarchical representation of the world around us and try to match it with already learned patterns and relationships stored in the brain. This is how recognition happens. And the key idea is that representation of objects in the brain does not depend on view angle. Hiton è®¤ä¸ºå¤§è„‘æ˜¯åå‘æ¸²æŸ“çš„ä¸€ä¸ªè¿‡ç¨‹ã€‚æ¥å—è§†è§‰ä¿¡æ¯ï¼Œç„¶åå¯¹è¿™æ ·ä¸€ä¸ªå…·æœ‰å±‚æ¬¡çš„ä¿¡æ¯è¡¨ç¤ºè¿›è¡Œè§£æ„ï¼Œå¹¶ä¸æˆ‘ä»¬å·²çŸ¥çš„æ¨¡å¼è¿›è¡ŒåŒ¹é…ã€‚è¿™é‡Œçš„å…³é”®æ˜¯å¯¹è±¡ä¿¡æ¯çš„è¡¨ç¤ºåœ¨å¤§è„‘ä¸­æ˜¯ä¸ä¾èµ–äºæŸä¸€ä¸ªè§†è§’çš„ã€‚ So at this point the question is: how do we model these hierarchical relationships inside of a neural network? é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•é€šè¿‡ç¥ç»ç½‘ç»œå¯¹ä¸€ä¸ªå±‚æ¬¡ä¿¡æ¯è¿›è¡Œå»ºæ¨¡å‘¢ï¼Ÿ Capsules Capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network. Capsuleå¯ä»¥å­¦ä¹ åˆ°ç‰©ä½“ä¹‹é—´çš„ä½ç½®å…³ç³»ï¼Œä¾‹å¦‚å®ƒå¯ä»¥å­¦ä¹ åˆ°çœ‰æ¯›ä¸‹é¢æ˜¯çœ¼ç›ï¼Œé¼»å­ä¸‹é¢æ˜¯å˜´å”‡ï¼Œå¯ä»¥å‡è½»å‰é¢çš„ç›®æ ‡ç»„ä»¶ä¹±åºé—®é¢˜ã€‚ Capsuleå¯ä»¥å¯¹3Dç©ºé—´çš„å…³ç³»è¿›è¡Œæ˜ç¡®å»ºæ¨¡ï¼Œcapsuleå¯ä»¥å­¦ä¹ åˆ°ä¸Šé¢å’Œä¸‹é¢çš„å›¾ç‰‡æ˜¯åŒä¸€ä¸ªç±»åˆ«ï¼Œåªæ˜¯è§†å›¾çš„è§’åº¦ä¸ä¸€æ ·ã€‚Capsuleå¯ä»¥æ›´å¥½åœ°åœ¨ç¥ç»ç½‘ç»œçš„å†…éƒ¨çŸ¥è¯†è¡¨è¾¾ä¸­å»ºç«‹å±‚æ¬¡å…³ç³» Capsuleåªä½¿ç”¨CNNæ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†ï¼Œå°±èƒ½è¾¾åˆ°å¾ˆå¥½çš„ç»“æœï¼Œæ›´åŠ æ¥è¿‘äººè„‘çš„æ€è€ƒæ–¹å¼ï¼Œé«˜æ•ˆå­¦ä¹ ï¼Œå¹¶ä¸”èƒ½å­¦åˆ°æ›´å¥½çš„ç‰©ä½“è¡¨å¾ã€‚ å°ç»“ï¼šè¿™ä¸€èŠ‚å¼•ç”¨äº†capsuleçš„æ¦‚å¿µï¼Œå¯ä»¥ç”¨äºæ·±åº¦å­¦ä¹ ï¼Œæ›´å¥½åœ°åœ¨ç¥ç»ç½‘ç»œçš„å†…éƒ¨çŸ¥è¯†è¡¨è¾¾ä¸­å»ºç«‹å±‚æ¬¡å…³ç³»ï¼Œå¹¶ç”¨ä¸åŒçš„æ–¹æ³•å»è®­ç»ƒè¿™æ ·ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚ Part2, How Capsules WorkWhat is a Capsule?CNN åœ¨è§£å†³è§†è§’çš„ä¸å˜æ€§æ—¶æ˜¯é€šè¿‡ max pooling è§£å†³çš„ã€‚é€‰æ‹©ä¸€å—åŒºåŸŸçš„æœ€å¤§å€¼ï¼Œè¿™æ ·æˆ‘ä»¬å°±èƒ½å¾—åˆ°æ¿€æ´»çš„ä¸å˜æ€§(invariance of activities). ä¸å˜æ€§æ„å‘³ç€ï¼Œç¨å¾®æ”¹å˜è¾“å…¥çš„ä¸€å°éƒ¨åˆ†ï¼Œè¾“å‡ºä¾æ—§ä¸å˜ã€‚å¹¶ä¸”ï¼Œåœ¨å›¾åƒä¸­ç§»åŠ¨æˆ‘ä»¬è¯†åˆ«çš„ç›®æ ‡ï¼Œæˆ‘ä»¬ä¾ç„¶èƒ½æ£€æµ‹å‡ºè¿™ä¸ªç›®æ ‡æ¥ã€‚ ä½†æ˜¯ max pooling ä¼šä¸¢å¤±å¾ˆå¤šä¿¡æ¯ï¼Œå¹¶ä¸” CNN ä¸èƒ½ç¼–ç ç‰¹å¾ä¹‹é—´çš„ç›¸å¯¹ç©ºé—´å…³ç³»ã€‚æ‰€ä»¥é‡‡ç”¨ capsules. Capsules encapsulate all important information about the state of the feature they are detecting in vector form. Capsule æ˜¯ä¸€ä¸ªç¥ç»å…ƒå‘é‡ï¼ˆactivity vectorï¼‰ è¿™ä¸ªå‘é‡çš„æ¨¡é•¿è¡¨ç¤ºæŸä¸ªentityå­˜åœ¨çš„æ¦‚ç‡ï¼Œentityå¯ä»¥ç†è§£ä¸ºæ¯”å¦‚é¼»å­ï¼Œçœ¼ç›ï¼Œæˆ–è€…æŸä¸ªç±»åˆ«ï¼Œå› ä¸ºç”¨vectorçš„æ¨¡é•¿å»è¡¨ç¤ºæ¦‚ç‡ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦å¯¹vectorè¿›è¡Œå‹ç¼©ï¼ŒæŠŠvectorçš„æ¨¡é•¿å‹ç¼©åˆ°å°äº1ï¼Œå¹¶ä¸”ä¸æ”¹å˜orientationï¼Œä¿è¯å±æ€§ä¸å˜åŒ–ã€‚ è¿™ä¸ªå‘é‡çš„æ–¹å‘è¡¨ç¤ºentiryçš„å±æ€§ï¼ˆorientationï¼‰ï¼Œæˆ–è€…ç†è§£ä¸ºè¿™ä¸ªvectoré™¤äº†é•¿åº¦ä»¥å¤–çš„ä¸åŒå½¢æ€çš„instantiation parameterï¼Œæ¯”å¦‚ä½ç½®ï¼Œå¤§å°ï¼Œè§’åº¦ï¼Œå½¢æ€ï¼Œé€Ÿåº¦ï¼Œåå…‰åº¦ï¼Œé¢œè‰²ï¼Œè¡¨é¢çš„è´¨æ„Ÿç­‰ç­‰ã€‚ How does a capsule work? ä¼ ç»Ÿç¥ç»å…ƒæ˜¯ä¸€ä¸ª scaler, ç¥ç»å…ƒä¸ç¥ç»å…ƒä¹‹é—´é€šè¿‡åŠ æƒæ±‚å’Œçš„æ–¹å¼è¿æ¥ã€‚å…¶è®¡ç®—æ–¹å¼ï¼š è®¡ç®—è¾“å…¥æ ‡é‡ $x_i$ çš„æƒé‡ $w_i$ å¯¹è¾“å…¥æ ‡é‡ $x_i$ è¿›è¡ŒåŠ æƒæ±‚å’Œ é€šè¿‡éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œè¿›è¡Œæ ‡é‡ä¸æ ‡é‡ä¹‹é—´çš„å˜æ¢ï¼Œå¾—åˆ°æ–°æ ‡é‡ $h_j$ capsule çš„å‰å‘è½¬æ¢çš„è®¡ç®—æ–¹å¼ï¼š matrix multiplication of input vectorsï¼ˆè¾“å…¥å‘é‡ $u_i$ çš„çŸ©é˜µ W ä¹˜æ³•ï¼‰ scalar weighting of input vectorsï¼ˆè¾“å…¥å‘é‡ $\\hat u_i$ çš„æ ‡é‡åŠ æƒ $c_i$ï¼‰ sum of weighted input vectorsï¼ˆè¾“å…¥å‘é‡çš„åŠ æƒæ±‚å’Œï¼‰ vector-to-vector nonlinearityï¼ˆå‘é‡åˆ°å‘é‡çš„éçº¿æ€§å˜æ¢ï¼‰ è¾“å…¥å‘é‡ $u_i$ çš„çŸ©é˜µ W ä¹˜æ³•Affine transform: $$\\hat u_{j|i}=W_{ij}u_i$$ å‘é‡çš„é•¿åº¦è¡¨ç¤ºä½ç»´çš„èƒ¶å›Šèƒ½æ£€æµ‹å‡ºå¯¹åº”å®ä½“çš„æ¦‚ç‡ã€‚å‘é‡çš„æ–¹å‘åˆ™ç¼–ç äº†æ£€æµ‹å¯¹è±¡çš„ä¸­é—´çŠ¶æ€è¡¨ç¤ºã€‚æˆ‘ä»¬å¯ä»¥å‡è®¾ä½ç»´èƒ¶å›Š $u_i$ åˆ†åˆ«è¡¨ç¤ºçœ¼ç›ï¼Œå˜´å·´ï¼Œé¼»å­è¿™ä¸‰ä¸ªä½å±‚æ¬¡çš„ç‰¹å¾ï¼Œé«˜ç»´èƒ¶å›Š $u_j$ æ£€æµ‹è„¸éƒ¨é«˜å±‚æ¬¡ç‰¹å¾ã€‚ çŸ©é˜µ W ç¼–ç äº†ä½å±‚æ¬¡ç‰¹å¾ä¹‹é—´æˆ–ä½å±‚æ¬¡ç‰¹å¾ä¸é«˜å±‚æ¬¡ç‰¹å¾ä¹‹é—´çš„é‡è¦çš„ç©ºé—´æˆ–å…¶ä»–å…³ç³»ã€‚ $u_i$ ä¹˜ä»¥ç›¸åº”çš„æƒé‡çŸ©é˜µ W å¾—åˆ°prediction vectorï¼ˆæ³¨æ„è¿™ä¸ªå›¾é‡Œåªç”»äº†ä¸€ä¸ª prediction vectorï¼Œä¹Ÿå°±æ˜¯ $\\hat u_i$ï¼Œå› ä¸ºè¿™é‡Œåªå¯¹åº”äº†ä¸€ä¸ª capsule è¾“å‡ºï¼Œå¦‚æœä¸‹ä¸€å±‚æœ‰ j ä¸ª capuslesï¼Œ$u_i$ å°±ä¼šç”Ÿæˆ j ä¸ª prediction vectorsï¼‰ ä¾‹å¦‚ï¼ŒçŸ©é˜µ $W_{2j}$ å¯ä»¥å¯¹é¼»å­å’Œé¢éƒ¨çš„å…³ç³»è¿›è¡Œç¼–ç :é¢éƒ¨ä»¥é¼»å­ä¸ºä¸­å¿ƒï¼Œå…¶å¤§å°æ˜¯é¼»å­çš„10å€ï¼Œè€Œåœ¨ç©ºé—´ä¸Šçš„æ–¹å‘ä¸é¼»å­çš„æ–¹å‘ä¸€è‡´ã€‚çŸ©é˜µ $W_{1j}$ å’Œ $W_{3j}$ ä¹Ÿæ˜¯ç±»ä¼¼çš„ã€‚ç»è¿‡çŸ©é˜µç›¸ä¹˜ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ›´é«˜å±‚æ¬¡ç‰¹å¾çš„é¢„æµ‹ä½ç½®ã€‚æ¯”å¦‚ï¼Œ$\\hat u_1$ æ ¹æ®çœ¼ç›é¢„æµ‹äººè„¸ä½ç½®ï¼Œ$\\hat u_2$ æ ¹æ®å˜´å·´é¢„æµ‹äººè„¸ä½ç½®ï¼Œ$\\hat u_3$ æ ¹æ®é¼»å­é¢„æµ‹äººè„¸ä½ç½®ã€‚æœ€åå¦‚æœè¿™3ä¸ªä½å±‚æ¬¡ç‰¹å¾çš„é¢„æµ‹æŒ‡å‘é¢éƒ¨çš„åŒä¸€ä¸ªä½ç½®å’ŒçŠ¶æ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬åˆ¤æ–­è¿™æ˜¯ä¸€å¼ è„¸ è¾“å…¥å‘é‡ $\\hat u_i$ çš„æ ‡é‡åŠ æƒ $c_i$weighting: $$s_{j|i} = c_{ij}\\hat u_{j|i}$$ $$s_{k|i} = c_{ik}\\hat u_{k|i}$$ å‰é¢æåˆ°çš„ä¼ ç»Ÿç¥ç»å…ƒåœ¨è¿™ä¸€æ­¥å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒï¼Œè¿™äº›æƒé‡æ˜¯åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¾—åˆ°çš„ï¼Œä½† Capsule æ˜¯é€šè¿‡ dynamic routing çš„æ–¹å¼è¿›è¡Œäº¤æµçš„ã€‚åœ¨è¿™ä¸€æ­¥ï¼Œä½å±‚æ¬¡ capsule éœ€è¦å†³å®šæ€ä¹ˆæŠŠè‡ªå·±çš„è¾“å‡ºåˆ†é…ç»™é«˜å±‚æ¬¡capsuleã€‚ $c_{ij}$ æ˜¯è€¦åˆç³»æ•° (coupling coefficients), é€šè¿‡è¿­ä»£åŠ¨æ€è·¯ç”±è¿‡ç¨‹æ¥å†³å®šã€‚æ¯”å¦‚ä¸Šå›¾ä¸­çš„ï¼Œé«˜å±‚æ¬¡æœ‰ä¸¤ä¸ªèƒ¶å›Š capsule J å’Œ capsule K. é‚£ä¹ˆå¯¹äº capsule i é€šè¿‡ä¸Šä¸€æ­¥çš„çŸ©é˜µ W å°±å¯ä»¥å¾—åˆ° $\\hat u_{j|i}, \\hat u_{k|i}$, ä»–ä»¬å¯¹åº”çš„æƒé‡åˆ†åˆ«æ˜¯ $c_{ij}, c_{ik}$, å¹¶ä¸”æœ‰ $c_{ij} + c_{ik} = 1$. åœ¨åŠ¨æ€è·¯ç”±çš„è¿‡ç¨‹ä¸­æ˜¯å¦‚ä½•ç¡®å®šæƒé‡çš„å‘¢ï¼ŒRouting by agreementï¼š åœ¨è¿™å¼ å›¾ç‰‡ä¸­ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªä½å±‚æ¬¡çš„å·²ç»è¢«æ¿€æ´»çš„capsuleï¼Œå®ƒå¯¹ä¸‹å±‚æ¯ä¸ªcapsuleä¼šç”Ÿæˆä¸€ä¸ªprediction vectorï¼Œæ‰€ä»¥è¿™ä¸ªä½å±‚æ¬¡capsuleç°åœ¨æœ‰ä¸¤ä¸ªprediction vectorï¼Œå¯¹è¿™ä¸¤ä¸ªprediction vectorsåˆ†é…æƒé‡åˆ†åˆ«è¾“å…¥åˆ°é«˜å±‚æ¬¡capsule Jå’ŒKä¸­ã€‚ ç°åœ¨ï¼Œæ›´é«˜å±‚æ¬¡çš„capsuleå·²ç»ä»å…¶ä»–ä½å±‚æ¬¡çš„capsuleä¸­è·å¾—äº†è®¸å¤šè¾“å…¥å‘é‡ï¼Œä¹Ÿå°±æ˜¯å›¾ç‰‡ä¸­çš„ç‚¹ï¼Œæ ‡çº¢çš„éƒ¨åˆ†æ˜¯èšé›†çš„ç‚¹ï¼Œå½“è¿™äº›èšé›†ç‚¹æ„å‘³ç€è¾ƒä½å±‚æ¬¡çš„capsuleçš„é¢„æµ‹æ˜¯ç›¸äº’æ¥è¿‘çš„ã€‚ ä½å±‚æ¬¡capsuleå¸Œæœ›æ‰¾åˆ°é«˜ä¸€å±‚ä¸­åˆé€‚çš„capsuleï¼ŒæŠŠè‡ªå·±æ›´å¤šçš„vectoræ‰˜ä»˜ç»™å®ƒã€‚ä½å±‚æ¬¡capsuleé€šè¿‡dynamic routingçš„æ–¹å¼å»è°ƒæ•´æƒé‡cã€‚ ä¾‹å¦‚ï¼Œå¦‚æœä½å±‚æ¬¡capsuleçš„prediction vectorè¿œç¦»capsule Jä¸­â€œcorrectâ€é¢„æµ‹çš„çº¢è‰²é›†ç¾¤ï¼Œå¹¶ä¸”æ¥è¿‘capsule K ä¸­çš„â€œtrueâ€é¢„æµ‹çš„çº¢è‰²é›†ç¾¤ï¼Œé‚£ä¹ˆä½å±‚æ¬¡capsuleå°±ä¼šè°ƒé«˜capsule Kå¯¹åº”çš„æƒé‡ï¼Œé™ä½capsule Jå¯¹åº”çš„æƒé‡ã€‚æœ€åï¼Œå¦‚æœé«˜å±‚æ¬¡capsuleæ¥æ”¶åˆ°çš„è¿™äº›predictionéƒ½agreeè¿™ä¸ªcapsuleï¼Œé‚£ä¹ˆè¿™ä¸ªcapsuleå°±ä¼šè¢«æ¿€æ´»ï¼Œå¤„äºactiveçŠ¶æ€ï¼Œè¿™å°±å«Routing by agreementã€‚ åŠ æƒè¾“å…¥å‘é‡çš„æ€»å’Œsum: $$s_j = \\sum_i c_{ij}\\hat u_{j|i}$$ é«˜å±‚æ¬¡capsuleæ ¹æ®å‰é¢è®¡ç®—çš„æƒé‡cï¼Œå¯¹æ‰€æœ‰ä½å±‚æ¬¡capsuleçš„prediction vectorsè¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°ä¸€ä¸ªå‘é‡ã€‚ å‘é‡åˆ°å‘é‡çš„éçº¿æ€§å˜æ¢Squashingæ˜¯ä¸€ç§æ–°çš„æ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬å¯¹å‰é¢è®¡ç®—å¾—åˆ°çš„å‘é‡æ–½åŠ è¿™ä¸ªæ¿€æ´»å‡½æ•°ï¼ŒæŠŠè¿™ä¸ªå‘é‡çš„æ¨¡é•¿å‹ç¼©åˆ°1ä»¥ä¸‹ï¼ŒåŒæ—¶ä¸æ”¹å˜å‘é‡æ–¹å‘ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨æ¨¡é•¿å»é¢„æµ‹æ¦‚ç‡ï¼Œå¹¶ä¸”ä¿è¯å±æ€§ä¸å˜åŒ–ã€‚ è¿™ä¸ªå…¬å¼çš„å·¦è¾¹æ˜¯å˜æ¢å°ºåº¦ï¼Œå³è¾¹æ˜¯å•ä½å‘é‡ã€‚ å°ç»“ï¼šè¿™ä¸€èŠ‚ä»‹ç»äº†capsuleçš„åŸºæœ¬æ¦‚å¿µï¼ŒcapsuleæŠŠå•ä¸ªç¥ç»å…ƒæ‰©å±•åˆ°ç¥ç»å…ƒå‘é‡ï¼Œæœ‰æ›´å¼ºå¤§çš„ç‰¹å¾è¡¨å¾èƒ½åŠ›ï¼Œå¹¶ä¸”å¼•å…¥çŸ©é˜µæƒé‡æ¥å¯¹ä¸åŒlayerç‰¹æ€§ä¹‹é—´çš„é‡è¦å±‚æ¬¡å…³ç³»è¿›è¡Œç¼–ç ï¼Œç»“æœè¯´æ˜äº†ä»¥ä¸‹ä¸¤ç§æ€§è´¨ï¼š Invariance ä¸å˜æ€§ï¼šç‰©ä½“è¡¨ç¤ºä¸éšå˜æ¢å˜åŒ–ï¼Œä¾‹å¦‚ç©ºé—´çš„ Invarianceï¼Œæ˜¯å¯¹ç‰©ä½“å¹³ç§»ä¹‹ç±»ä¸æ•æ„Ÿï¼ˆç‰©ä½“ä¸åŒçš„ä½ç½®ä¸å½±å“å®ƒçš„è¯†åˆ«) EquivarianceåŒå˜æ€§ï¼šç”¨å˜æ¢çŸ©é˜µè¿›è¡Œè½¬æ¢åï¼Œç‰©ä½“è¡¨ç¤ºä¾æ—§ä¸å˜ï¼Œè¿™æ˜¯å¯¹ç‰©ä½“å†…å®¹çš„ä¸€ç§å˜æ¢ Part 3, Dynamic Routing Between Capsulesç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼š Lower level capsule will send its input to the higher level capsule that â€œagreesâ€ with its input. This is the essence of the dynamic routing algorithm. åŠ¨æ€è·¯ç”±ç®—æ³•çš„è®¾è®¡æ˜¯ä¸ºäº†åŠ¨æ€è°ƒæ•´ $c_{ij}$ çš„å€¼ï¼š $c_{ij}$ ä¸ºéè´Ÿ scalar å¯¹äºæ¯ä¸€ä¸ªçš„ low level capsule iï¼Œæ‰€æœ‰çš„æƒé‡ä¹‹å’Œ $c_{ij}$ ä¸º 1ï¼Œj è¡¨ç¤º high-level capsule j. å¯¹äºæ¯ä¸€ä¸ª low-level capsule, å¯¹åº”çš„æƒé‡æ•°é‡ç­‰äº high-level çš„æ•°é‡ æƒé‡ç”±åŠ¨æ€è·¯ç”±ç®—æ³•ç¡®å®š ä¸‹é¢æˆ‘ä»¬é€è¡Œè§£é‡Šä¼ªä»£ç ï¼š è¿™ä¸ªè¿‡ç¨‹çš„è¾“å…¥æ˜¯ ç¬¬ $l$ å±‚çš„ capsule ç»è¿‡çŸ©é˜µå˜æ¢ä¹‹åçš„ prediction vector $\\hat u$. è¿­ä»£æ­¥æ•° r åˆå§‹åŒ– $b_{ij}$ ä¸º 0ï¼Œ $b_{ij}$ æ˜¯ç”¨æ¥è®¡ç®— $c_{ij}$ çš„ å¯¹æ¥ä¸‹æ¥4-6è¡Œä»£ç è¿­ä»£ræ¬¡ï¼Œè®¡ç®—ç¬¬ $l+1$ å±‚ capsule j çš„ output åœ¨ç¬¬ $l$ å±‚ï¼Œæ¯ä¸ª capsule i å¯¹ $b_{ij}$ åš softmax å¾—åˆ° $c_{ij}$ï¼Œ$c_{ij}$ æ˜¯ç¬¬ $l$ å±‚ capsule i ç»™ç¬¬ $l+1$ å±‚ capsule j åˆ†é…çš„æƒé‡ã€‚åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œæ‰€æœ‰çš„ $b_{ij}$ éƒ½åˆå§‹åŒ–ä¸º 0ï¼Œæ‰€ä»¥è¿™é‡Œå¾—åˆ°çš„ $c_{ij}$ éƒ½æ˜¯ç›¸ç­‰çš„æ¦‚ç‡ï¼Œå½“ç„¶åé¢è¿­ä»£å¤šæ¬¡åï¼Œ$c_{ij}$ çš„å€¼ä¼šæœ‰æ›´æ–°ã€‚ åœ¨ç¬¬ $l+1$ å±‚ï¼Œæ¯ä¸ª capsule j åˆ©ç”¨ $c_ij$ å¯¹ $\\hat u_j|i$ è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°è¾“å‡ºå‘é‡ $s_j$ åœ¨ç¬¬ $l+1$ å±‚ï¼Œä½¿ç”¨squashæ¿€æ´»å‡½æ•°å¯¹ $s_j$ åšå°ºåº¦çš„å˜æ¢ï¼Œå‹ç¼©å…¶æ¨¡é•¿ä¸å¤§äº1 æˆ‘ä»¬åœ¨è¿™ä¸€æ­¥æ›´æ–°å‚æ•°ï¼Œå¯¹æ‰€æœ‰çš„ $l$ å±‚ capsule iå’Œ $l+1$ å±‚ capsule jï¼Œè¿­ä»£æ›´æ–° $b_{ij}$ï¼Œæ›´æ–° $b_{ij}$ ä¸ºæ—§ $b_{ij}$ + capsule j çš„è¾“å…¥å’Œè¾“å‡ºçš„ç‚¹ä¹˜ï¼Œè¿™ä¸ªç‚¹ä¹˜æ˜¯ä¸ºäº†è¡¡é‡è¿™ä¸ª capsule çš„è¾“å…¥å’Œè¾“å‡ºçš„ç›¸ä¼¼åº¦ï¼Œä½å±‚ capsule ä¼šæŠŠè‡ªå·±çš„è¾“å‡ºåˆ†ç»™è·Ÿå®ƒç›¸ä¼¼çš„é«˜å±‚ capsuleã€‚ ç»è¿‡ræ¬¡å¾ªç¯ï¼Œé«˜å±‚capsuleå¯ä»¥ç¡®å®šä½å±‚åˆ†é…çš„æƒé‡ä»¥åŠè®¡ç®—å…¶è¾“å‡ºï¼Œå‰å‘ä¼ æ’­å¯ä»¥ç»§ç»­è¿æ¥åˆ°ä¸‹ä¸€ä¸ªç½‘ç»œå±‚ã€‚è¿™ä¸ªrä¸€èˆ¬æ¨èè®¾ç½®ä¸º3ï¼Œæ¬¡æ•°å¤ªå¤šäº†å¯èƒ½ä¼šè¿‡æ‹Ÿåˆã€‚ susht å¤§ä½¬ç”»çš„å›¾ï¼Œä¸è¿‡æ²¡æœ‰æŠŠ $b_{ij}$ è¡¨ç¤ºå‡ºæ¥ã€‚ $b_{ij}$ çš„å‚æ•°é‡å’Œ $c_{ij}$ ä»¥åŠ $u_{ij}$ çš„ä¸ªæ•°æ˜¯ä¸€è‡´çš„ã€‚ å‡è®¾æœ‰ä¸¤ä¸ªé«˜å±‚capsuleï¼Œç´«è‰²å‘é‡v1å’Œv2åˆ†åˆ«æ˜¯è¿™ä¸¤ä¸ªcapsuleçš„è¾“å‡ºï¼Œæ©™è‰²å‘é‡æ˜¯æ¥è‡ªä½å±‚ä¸­æŸä¸ªcapsuleçš„è¾“å…¥ï¼Œé»‘è‰²å‘é‡æ˜¯ä½å±‚å…¶å®ƒcapsuleçš„è¾“å…¥ã€‚ å·¦è¾¹çš„capsuleï¼Œæ©™è‰²u_hatè·Ÿv1æ–¹å‘ç›¸åï¼Œä¹Ÿå°±æ˜¯è¿™ä¸¤ä¸ªå‘é‡ä¸ç›¸ä¼¼ï¼Œå®ƒä»¬çš„ç‚¹ä¹˜å°±ä¼šä¸ºè´Ÿï¼Œæ›´æ–°çš„æ—¶å€™ä¼šå‡å°å¯¹åº”çš„c_11æ•°å€¼ã€‚å³è¾¹çš„capsuleï¼Œæ©™è‰²u_hatè·Ÿv2æ–¹å‘ç›¸è¿‘ï¼Œæ›´æ–°çš„æ—¶å€™å°±ä¼šå¢å¤§å¯¹åº”çš„c_12æ•°å€¼ã€‚ é‚£ä¹ˆç»è¿‡å¤šæ¬¡è¿­ä»£æ›´æ–°ï¼Œæ‰€æœ‰çš„routingæƒé‡céƒ½ä¼šæ›´æ–°åˆ°è¿™æ ·ä¸€ç§ç¨‹åº¦ï¼šå¯¹ä½å±‚capsuleçš„è¾“å‡ºä¸é«˜å±‚capsuleçš„è¾“å‡ºéƒ½è¿›è¡Œäº†æœ€ä½³åŒ¹é…ã€‚ å°ç»“ï¼šè¿™èŠ‚ä»‹ç»äº† dynamic routing algorithm by agreement çš„æ–¹å¼å»è®­ç»ƒ capsNetï¼Œä¸»è¦ idea æ˜¯é€šè¿‡ç‚¹ä¹˜å»è¡¡é‡ä¸¤ä¸ª capsule è¾“å‡ºçš„ç›¸ä¼¼åº¦ï¼Œå¹¶ä¸”æ›´æ–°routingçš„æƒé‡å‚æ•°ã€‚ CapsNet Architectureè®ºæ–‡ç»™å‡ºäº†ä¸€ä¸ªç®€å•çš„CapsNetæ¨¡å‹ï¼Œç¬¬ä¸€å±‚æ˜¯ä¸ªæ™®é€šçš„convå±‚ï¼Œç¬¬ä¸¤å±‚ä¹Ÿç»“åˆäº†convæ“ä½œå»æ„å»ºåˆå§‹çš„capsuleï¼Œå†é€šè¿‡routingçš„æ–¹å¼å’Œç¬¬ä¸‰å±‚çš„DigitCapsäº¤æµï¼Œæœ€ååˆ©ç”¨DigitCapsçš„capsuleå»åšåˆ†ç±»ã€‚ ReLU Conv1: è¿™æ˜¯ä¸€ä¸ªæ™®é€šçš„å·ç§¯å±‚ï¼Œå‚æ•°é‡æ˜¯ $9\\times 9\\times 256$, å‡è®¾è¾“å…¥æ˜¯ $28\\times 28\\times 1$, å¾—åˆ°çš„è¾“å‡ºæ˜¯ $20\\times 20\\times 256$. PrimaryCaps: è¿™é‡Œæ„å»ºäº† 32 ä¸ª channels çš„ capsules, æ¯ä¸ª capsule çš„å‘é‡ç»´åº¦æ˜¯ 8. ä¾æ—§æ˜¯é‡‡ç”¨å·ç§¯çš„æ–¹æ³•ï¼Œæ¯ä¸€ä¸ª channels ä½¿ç”¨ 8 ä¸ªå·ç§¯æ ¸ $9\\times 9$. æ‰€ä»¥æ€»çš„å‚æ•°é‡æ˜¯ $9\\times 9\\times 256 \\times 32\\times 8 + 32 \\times 8= 5308672$ã€‚ ä¸€ä¸ª channels å¯¹åº”ä¸€ä¸ª feature mapï¼Œåœ¨è¿™é‡Œæ˜¯ $6\\times 6$ ä¸ª 8 ç»´çš„ capsules. æ‰€ä»¥æœ€åæ˜¯ $6\\times 6\\times 32=1152$ ä¸ª capsulesã€‚ DigitCaps: å¯¹å‰é¢1152ä¸ªcapulesè¿›è¡Œä¼ æ’­ä¸routingæ›´æ–°ï¼Œè¾“å…¥æ˜¯1152ä¸ªcapsulesï¼Œè¾“å‡ºæ˜¯10ä¸ªcapulesï¼Œæ¯ä¸ª capsule çš„ç»´åº¦ç”± 8 å˜æˆäº† 16ï¼Œè¡¨ç¤º10ä¸ªæ•°å­—ç±»åˆ«ï¼Œæœ€åæˆ‘ä»¬ç”¨è¿™10ä¸ªcapuleså»åšåˆ†ç±». æ€»çš„å‚æ•°é‡æ˜¯ $1152\\times 8\\times 16+1152+1152=149760$. åé¢ä¸¤ä¸ª 1152 åˆ†åˆ«è¡¨ç¤º $b_{ij}, c_{ij}$. loss functionåœ¨ dynamic routing è¿‡ç¨‹ä¸­æˆ‘ä»¬æ›´æ–°äº† $c_{ij}$ å‚æ•°ï¼Œå…¶ä½™å‚æ•°æ˜¯é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œæ›´æ–°ï¼Œè¿™é‡Œå¯¹äºæ¯ä¸€ä¸ªcapsule kæˆ‘ä»¬éƒ½è®¡ç®—å®ƒçš„ Margin lossæŸå¤±å‡½æ•° $L_k$ è·Ÿ SVM çš„ loss function éå¸¸ç±»ä¼¼ã€‚ $$L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$ ä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œè®¾å®šäº† æ­£ç±»çš„æ¦‚ç‡ è¶Šå°äº $m^+=0.9$ å…¶è´¡çŒ®çš„ loss è¶Šå°ï¼Œè´Ÿç±»çš„æ¦‚ç‡è¶Šå¤§äº $m^-=0.1$ å…¶è´¡çŒ®çš„ loss è¶Šå¤§ã€‚ $\\lambda$ å‚æ•°è®¾ç½®ä¸º0.5ï¼Œè¿™æ˜¯ä¸ºäº†é˜²æ­¢è´Ÿä¾‹å‡å°äº†æ‰€æœ‰capsuleå‘é‡çš„æ¨¡é•¿ã€‚ Decoder: reconstruction as a regularization method è¿™é‡Œä¼šmaskæ‰ä¸€äº›capsuleï¼Œç”¨æœ‰ä»£è¡¨æ€§çš„capsuleå»é‡æ„å›¾åƒï¼Œé€šè¿‡å‡ å±‚å…¨è¿æ¥å±‚å¾—åˆ°784ç»´çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯åŸå§‹å›¾åƒçš„åƒç´ ç‚¹ä¸ªæ•°ï¼Œä»¥è¿™ä¸ªè¾“å‡ºè·ŸåŸå§‹åƒç´ ç‚¹çš„æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºé‡æ„æŸå¤±å‡½æ•°ã€‚ CapsNetä¸CNNçš„è”ç³»ç›¸åŒä¹‹å¤„ï¼šCNNå¯ä»¥ä¸Šä¸‹å·¦å³å¹³ç§»åœ¨å›¾åƒä¸Šè¿›è¡Œæ‰«æï¼Œä¹Ÿå°±æ˜¯å®ƒåœ¨è¿™ä¸ªåœ°æ–¹å­¦åˆ°çš„ä¸œè¥¿å¯ä»¥ç”¨åˆ°ä¸‹ä¸€ä¸ªåœ°æ–¹ï¼Œå¯ä»¥æœ‰æ•ˆè¯†åˆ«å›¾ç‰‡çš„å†…å®¹ï¼Œæ‰€ä»¥capsuleä¹Ÿé‡‡ç”¨äº†convolutionçš„ç‰¹ç‚¹ï¼Œæœ€åä¸€å±‚åªç”¨capsule. ä¸åŒä¹‹å¤„ï¼šcapsuleä¸é‡‡ç”¨max poolingçš„åšæ³•ï¼Œå› ä¸ºMax poolingåªä¿ç•™äº†åŒºåŸŸå†…æœ€å¥½çš„ç‰¹å¾ï¼Œè€Œå¿½è§†äº†å…¶å®ƒçš„ç‰¹å¾ï¼Œroutingå¹¶ä¸ä¼šä¸¢å¤±è¿™äº›ä¿¡æ¯ï¼Œå®ƒå¯ä»¥é«˜æ•ˆå¤„ç†é«˜åº¦å±‚å çš„ä¾‹å­ã€‚æµ…å±‚capsuleä¼šæŠŠä½ç½®ä¿¡æ¯é€šè¿‡place-codedçš„æ–¹å¼è®°å½•åœ¨æ¿€æ´»çš„capsuleä¸­ï¼Œé«˜å±‚çš„capsuleä¼šé€šè¿‡rate-codedçš„æ–¹å¼è®°å½•åœ¨capsule vectorçš„å€¼å½“ä¸­ã€‚ Capsulesåšåˆ†ç±»çš„ä¼˜ç‚¹ é€‚åˆè¾“å‡ºæœ‰å¤šä¸ªç±»åˆ«çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œsoftmaxåªé€‚åˆåªæœ‰ä¸€ä¸ªè¾“å‡ºçš„å¤šåˆ†ç±»é—®é¢˜ï¼Œå› ä¸ºsoftmaxä¼šæé«˜æŸä¸€ç±»çš„æ¦‚ç‡ï¼Œè€Œå‹ä½å…¶ä½™æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡ã€‚ å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨k sigmoidå»ä»£æ›¿softmaxï¼ŒæŠŠå¤šåˆ†ç±»ä»»åŠ¡è½¬æ¢æˆKä¸ªäºŒåˆ†ç±»ä»»åŠ¡.","link":"/2019/01/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Capsules-Network/"},{"title":"æ·±åº¦å­¦ä¹ -ä¼˜åŒ–ç®—æ³•","text":"paper: An overview of gradient descent optimization algorithms Gradient descent variantsBatch gradient descentcomputes the gradient of the cost function to the parameters $\\theta$ for the entire training dataset. $$\\theta= \\theta - \\delta_{\\theta}J(\\theta)$$ 1234567for i in range ( nb_epochs ): params_grad = evaluate_gradient ( loss_function , data , params ) params = params - learning_rate * params_grad Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces. Stochastic gradient descentStochastic gradient descent (SGD) in contrast performs a parameter update for each training example x(i) and label y(i): $$\\theta= \\theta - \\delta_{\\theta}J(\\theta; x^{(i)}; y^{(i)})$$ 1234567891011for i in range ( nb_epochs ): np. random . shuffle ( data ) for example in data : params_grad = evaluate_gradient ( loss_function , example , params ) params = params - learning_rate * params_grad Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. æ‰¹æ¢¯åº¦ä¸‹é™çš„è®¡ç®—è¿‡äºå†—ä½™ï¼Œå®ƒåœ¨æ¯ä¸€æ¬¡å‚æ•°æ›´æ–°ä¹‹å‰çš„è®¡ç®—è¿‡ç¨‹ä¸­ä¼šè®¡ç®—å¾ˆå¤šç›¸ä¼¼çš„æ ·æœ¬ã€‚éšæœºæ¢¯åº¦ä¸‹é™åˆ™æ˜¯æ¯ä¸€æ¬¡å‚æ•°æ›´æ–°è®¡ç®—ä¸€ä¸ªæ ·æœ¬ï¼Œå› æ­¤æ›´æ–°é€Ÿåº¦ä¼šå¾ˆå¿«ï¼Œå¹¶ä¸”å¯ä»¥åœ¨çº¿å­¦ä¹ ã€‚ä½†æ˜¯ç”¨äºæ›´æ–°çš„æ¢¯åº¦çš„æ–¹å·®ä¼šå¾ˆå¤§ï¼Œå¯¼è‡´ loss æ›²çº¿æ³¢åŠ¨å¾ˆå¤§ã€‚ While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGDâ€™s fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. æ‰¹æ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°çš„æœ€å°å€¼ä¸ç›¸åº”çš„å‚æ•°å…³ç³»å¾ˆå¤§ï¼ˆä¹Ÿå°±æ˜¯è¯´è·Ÿæƒé‡çš„åˆå§‹åŒ–ä¼šæœ‰å¾ˆå¤§å½±å“ï¼‰ã€‚è€Œ SGD ç”±äºlossæ³¢åŠ¨å¾ˆå¤§ï¼Œæ›´æœ‰æ•ˆçš„è·³å‡ºå±€éƒ¨æœ€ä¼˜åŒºåŸŸï¼Œä»è€Œè·å¾—æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜å€¼ã€‚ä½†å¦ä¸€æ–¹é¢ï¼Œè¿™ä¹Ÿä¼šä½¿å¾— SGD éš¾ä»¥æ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼Œç¼“æ…¢çš„é™ä½å­¦ä¹ ç‡ï¼Œ SGD å’Œ BatchGD èƒ½è·å¾—åŒæ ·çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚ Mini-batch gradient descentMini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples. $$\\theta= \\theta - \\delta_{\\theta}J(\\theta; x^{(i+n)}; y^{(i+n)})$$ 1234567891011for i in range ( nb_epochs ): np. random . shuffle ( data ) for batch in get_batches (data , batch_size =50): params_grad = evaluate_gradient ( loss_function , batch , params ) params = params - learning_rate * params_grad reduces the variance of the parameter updates, which can lead to more stable convergence; å‡å°å‚æ•°æ›´æ–°çš„æ–¹å·®ï¼Œä½¿å¾—æ”¶æ•›æ›´ç¨³å®šã€‚ can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient mini-batch very efficient. èƒ½éå¸¸å¥½çš„åˆ©ç”¨çŸ©é˜µä¼˜åŒ–çš„æ–¹å¼æ¥åŠ é€Ÿè®¡ç®—ï¼Œè¿™åœ¨å„ç§æ·±åº¦å­¦ä¹ æ¡†æ¶é‡Œé¢éƒ½å¾ˆå¸¸è§ã€‚ Challenges Choosing a proper learning rate. é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ã€‚ Learning rate schedules. try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a datasetâ€™s characteristics. å­¦ä¹ ç‡è®¡åˆ’ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å­¦ä¹ ç‡ï¼Œè­¬å¦‚é€€ç«ï¼Œé¢„å…ˆå®šä¹‰å¥½çš„è®¡åˆ’ï¼Œå½“ä¸€ä¸ª epoch ç»“æŸåï¼Œç›®æ ‡å‡½æ•°ï¼ˆlossï¼‰ å‡å°çš„å€¼ä½äºæŸä¸ªé˜ˆå€¼æ—¶ï¼Œå¯ä»¥è°ƒæ•´å­¦ä¹ ç‡ã€‚ the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features. å¯¹æ‰€æœ‰çš„å‚æ•°ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ã€‚å¦‚æœä½ çš„æ•°æ®æ˜¯ç¨€ç–çš„ï¼Œå¹¶ä¸”ä¸åŒçš„ç‰¹å¾çš„é¢‘ç‡æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å¹¶ä¸å¸Œæœ›å¯¹æ‰€æœ‰çš„å‚æ•°ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ï¼Œè€Œæ˜¯å¯¹æ›´ç½•è§çš„ç‰¹å¾æ‰§è¡Œæ›´å¤§çš„å­¦ä¹ ç‡ã€‚ Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. [5] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions. å¯¹äºéå‡¸æŸå¤±å‡½æ•°çš„ä¼˜åŒ–é—®é¢˜ï¼Œéœ€è¦é¿å…é™·å…¥å…¶ä¼—å¤šçš„æ¬¡ä¼˜å±€éƒ¨æå°å€¼ã€‚Dauphin et al. [5] åˆ™è®¤ä¸ºï¼Œ ç›¸æ¯”å±€éƒ¨æå°å€¼ï¼Œéç‚¹çš„æ˜¯æ›´éš¾è§£å†³çš„é—®é¢˜ã€‚éç‚¹æ˜¯ä¸€ä¸ªç»´åº¦ä¸Šå‡ï¼Œä¸€ä¸ªç»´åº¦ä¸‹é™ã€‚è¯¦ç»†çš„å…³äºéç‚¹ä»¥åŠ SGD å¦‚ä½•é€ƒç¦»éç‚¹å¯å‚è€ƒï¼šçŸ¥ä¹ï¼šå¦‚ä½•é€ƒç¦»éç‚¹ . Gradient descent optimization algorithmsMomentum [17] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Figure 2b. It does this by padding a fraction $gamma$ of the update vector of the past time step to the current update vector. Momentumpaper: [Neural networks : the official journal of the International Neural Network Society]() without Momentum: $$\\theta += -lr * \\nabla_{\\theta}J(\\theta)$$ with Momentum: $$v_t=\\gamma v_{t-1}+\\eta \\nabla_{\\theta}J(\\theta)$$ $$\\theta=\\theta-v_t$$ åŠ¨é‡æ¢¯åº¦ä¸‹é™çš„ç†è§£ï¼š The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. å¦‚ä¸Šå›¾ä¸­å‚ç›´æ–¹å‘çš„æ¢¯åº¦æ–¹å‘æ˜¯ä¸€è‡´çš„ï¼Œé‚£ä¹ˆå®ƒçš„åŠ¨é‡ä¼šç´¯ç§¯ï¼Œå¹¶åœ¨è¿™ä¸ªæ–¹å‘çš„é€Ÿåº¦è¶Šæ¥è¶Šå¤§ã€‚è€Œåœ¨æŸä¸ªæ°´å¹³æ–¹å‘ï¼Œå…¶æ¢¯åº¦æ–¹å‘æ€»æ˜¯å˜åŒ–ï¼Œé‚£ä¹ˆå®ƒçš„é€Ÿåº¦ä¼šå‡å°ï¼Œä¹Ÿå°±æ˜¯åœ¨è¿™ä¸ªæ–¹å‘çš„æ³¢åŠ¨å¹…åº¦ä¼šå¾—åˆ°æŠ‘åˆ¶ã€‚ å…¶å®å°±æ˜¯æŠŠæ¢¯åº¦çœ‹åšåŠ é€Ÿåº¦ï¼Œå‚æ•°çš„æ›´æ–°é‡çœ‹åšé€Ÿåº¦ã€‚é€Ÿåº¦è¡¨ç¤ºä¸€ä¸ªstepæ›´æ–°çš„å¤§å°ã€‚åŠ é€Ÿåº¦æ€»æ˜¯æœç€ä¸€ä¸ªæ–¹å‘ï¼Œé€Ÿåº¦å¿…ç„¶è¶Šæ¥è¶Šå¿«ã€‚åŠ é€Ÿåº¦æ–¹å‘æ€»æ˜¯å˜åŒ–ï¼Œé€Ÿåº¦å°±ä¼šç›¸å¯¹è¾ƒå°ã€‚ $\\gamma$ çœ‹åšæ‘©æ“¦ç³»æ•°ï¼Œ é€šå¸¸è®¾ç½®ä¸º 0.9ã€‚$\\eta$ æ˜¯å­¦ä¹ ç‡ã€‚ Nesterov accelerate gradient(NAG)paper: [Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2).]() We would like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again. Nesterov accelerated gradient (NAG) [14] is a way to give our momentum term this kind of prescience. å¦‚æœé‡‡ç”¨ momentumï¼Œåœ¨æ¥è¿‘ç›®æ ‡å‡½æ•°æœ€ä¼˜å€¼æ—¶ï¼Œç”±äºé€Ÿåº¦åœ¨å‚ç›´æ–¹å‘æ˜¯ä¸€ç›´å¢åŠ çš„ï¼Œæ‰€ä»¥é€Ÿåº¦ä¼šå¾ˆå¤§ï¼Œè¿™ä¸ªæ—¶å€™å°±ä¼šè¶Šè¿‡æœ€å°å€¼ï¼Œç„¶åè¿˜å¾—ç»•å›æ¥ï¼Œå¢åŠ äº†è®­ç»ƒæ—¶é—´ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å‚æ•°çš„æ›´æ–°å…·æœ‰å…ˆè§ä¹‹æ˜ï¼ŒçŸ¥é“åœ¨æ¥è¿‘æœ€ä¼˜è§£æ—¶ï¼Œé™ä½å‚æ•°æ›´æ–°çš„é€Ÿåº¦å¤§å°ã€‚ $$v_t=\\gamma v_{t-1}+\\eta \\nabla_{\\theta}J(\\theta-\\gamma v_{t-1})$$ $$\\theta=\\theta-v_t$$ åœ¨ momentum ä¸­ï¼Œæˆ‘ä»¬ç”¨é€Ÿåº¦ $\\gamma v_{t-1}$ æ¥æ›´æ–°å‚æ•°ã€‚ äº‹å®ä¸Šåœ¨æ¥è¿‘å±€éƒ¨æœ€ä¼˜è§£æ—¶ï¼Œç›®æ ‡å‡½æ•°å¯¹äº $\\theta$ çš„æ¢¯åº¦ä¼šè¶Šæ¥è¶Šå°ï¼Œç”šè‡³æ¥è¿‘äº 0. ä¹Ÿå°±æ˜¯è¯´ï¼Œå°½ç®¡é€Ÿåº¦åœ¨å¢åŠ ï¼Œä½†æ˜¯é€Ÿåº¦å¢åŠ çš„ç¨‹åº¦è¶Šæ¥è¶Šå°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡é€Ÿåº¦å¢åŠ çš„ç¨‹åº¦æ¥åˆ¤æ–­æ˜¯å¦è¦æ¥è¿‘å±€éƒ¨æœ€ä¼˜è§£äº†ã€‚$\\nabla_{\\theta}J(\\theta-\\gamma v_{t-1})$ å°±è¡¨ç¤ºé€Ÿåº¦å˜åŒ–çš„ç¨‹åº¦ï¼Œä»£æ›¿ä¸€ç›´ä¸ºæ­£çš„ $\\nabla_{\\theta}J(\\theta)$ï¼Œåœ¨æ¥è¿‘å±€éƒ¨æœ€ä¼˜è§£æ—¶ï¼Œè¿™ä¸ªå€¼åº”è¯¥æ˜¯è´Ÿçš„ï¼Œç›¸åº”çš„å‚æ•°æ›´æ–°çš„é€Ÿåº¦ä¹Ÿä¼šå‡å°. åœ¨ä»£ç å®ç°æ—¶ï¼Œå¯¹äº $J(\\theta-\\gamma v_{t-1})$ çš„æ¢¯åº¦è®¡ç®—ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼Œå¯ä»¥ä»¤ï¼š $$\\phi = \\theta-\\gamma v_{t-1}$$ ç„¶åè¿›è¡Œè®¡ç®—ï¼Œå…·ä½“å¯å‚è€ƒ tensorflow æˆ– pytorch ä¸­ä»£ç ã€‚ Adagradpaper: [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization]() Adagrad [8] is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. å¯¹äºä¸åŒçš„å‚æ•°ï¼Œè‡ªé€‚åº”çš„è°ƒæ•´å¯¹åº”çš„æ¢¯åº¦å¤§å°ã€‚å¯¹ä½é¢‘å‚æ•°æˆ–ç‰¹å¾ï¼Œä½¿å…¶æ›´æ–°çš„æ¢¯åº¦è¾ƒå¤§ï¼Œå¯¹é«˜é¢‘çš„å‚æ•°æˆ–ç‰¹å¾ï¼Œä½¿å…¶æ›´æ–°çš„æ¢¯åº¦è¾ƒå°ã€‚æ¯”å¦‚åœ¨è®­ç»ƒ Glove è¯å‘é‡æ—¶ï¼Œä½é¢‘è¯åœ¨æŸä¸€æ­¥è¿­ä»£ä¸­å¯èƒ½å¹¶æ²¡æœ‰å‚ä¸ loss çš„è®¡ç®—ï¼Œæ‰€ä»¥æ›´æ–°çš„ä¼šç›¸å¯¹è¾ƒæ…¢ï¼Œæ‰€ä»¥éœ€è¦äººä¸ºçš„å¢å¤§å®ƒçš„æ¢¯åº¦ã€‚ ä¸åŒçš„æ—¶é—´æ­¥ t,ä¸åŒçš„å‚æ•° i å¯¹åº”çš„æ¢¯åº¦ï¼š $$g_{t,i}=\\nabla_{\\theta_t}J(\\theta_t,i)$$ $$\\theta_{t+1,i}=\\theta_{t,i}-\\eta \\cdot g_{t,i}$$ $$\\theta_{t+1,i}=\\theta_{t,i}-\\dfrac{\\eta}{\\sqrt G_{t,ii}+\\epsilon} g_{t,i}$$ $G_{t,ii}$ æ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ æ˜¯å¯¹åº”çš„æ¢¯åº¦å¤§å°ã€‚ 12345cache += dx**2x += -lr * dx/(np.sqrt(cache) + 1e-7) RMSpropGeoff Hinton Lecture 6e Adagrad ä¸­éšç€ cache çš„ç´¯ç§¯ï¼Œæœ€åçš„æ¢¯åº¦ä¼šå˜ä¸º 0ï¼ŒRMSprop åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œç»™äº† cache ä¸€ä¸ªè¡°å‡ç‡ï¼Œç›¸å½“äºå€¼è€ƒè™‘äº†æœ€è¿‘æ—¶åˆ»çš„æ¢¯åº¦å€¼ï¼Œè€Œå¾ˆæ—©ä¹‹å‰çš„æ¢¯åº¦å€¼ç»è¿‡è¡°å‡åå½±å“å¾ˆå°ã€‚ $$E[g^2]_ t=0.9E[g^2]_ {t-1}+0.1g^2_t$$ $$\\theta_{t+1}=\\theta_t-\\dfrac{\\eta}{E[g^2]_ t+\\epsilon}g_t$$ 12345cache = decay_rate*cache + (1-decay_rate)*dx**2x += -lr * dx/(np.sqrt(cache) + 1e-7) ä½¿ç”¨æŒ‡æ•°è¡°å‡çš„å½¢å¼æ¥ä¿å­˜ cache èƒ½æœ‰æ•ˆçš„èŠ‚çœå†…å­˜ï¼Œåªéœ€è¦è®°å½•å½“å‰çš„æ¢¯åº¦å€¼å³å¯ï¼Œè€Œä¸ç”¨ä¿å­˜æ‰€æœ‰çš„æ¢¯åº¦å€¼ã€‚ Adam(Adaptive Moment Estimation)Adam: a Method for Stochastic Optimization. In addition to storing an exponentially decaying average of past squared gradients $v_t$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum: similar like momentum: $$m_t=\\beta_1m_{t-1}+(1-\\beta_1)g_t$$ similar like autograd/RMSprop: $$v_t=\\beta_2v_{t-1}+(1-\\beta_2)g_t^2$$ $m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$ are initialized as vectors of 0â€™s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. Î²1 and Î²2 are close to 1). They counteract these biases by computing bias-corrected first and second moment estimates: $$\\hat m_t=\\dfrac{m_t}{1-\\beta^t_1}$$ $$\\hat v_t=\\dfrac{v_t}{1-\\beta^t_2}$$ They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule: $$\\theta_{t+1}=\\theta_t-\\dfrac{\\eta}{\\sqrt{\\hat a}+ \\epsilon}{\\hat m_t}$$ $m_t$ æ˜¯ç±»ä¼¼äº Momentum ä¸­å‚æ•°æ›´æ–°é‡ï¼Œæ˜¯æ¢¯åº¦çš„å‡½æ•°. $\\beta_1$ æ˜¯æ‘©æ“¦ç³»æ•°ï¼Œä¸€èˆ¬è®¾ä¸º 0.9. $v_t$ æ˜¯ç±»ä¼¼äº RMSprop ä¸­çš„ cacheï¼Œç”¨æ¥è‡ªé€‚åº”çš„æ”¹å˜ä¸åŒå‚æ•°çš„æ¢¯åº¦å¤§å°ã€‚ $\\beta_2$ æ˜¯ cache çš„è¡°å‡ç³»æ•°ï¼Œä¸€èˆ¬è®¾ä¸º 0.999. AdaMaxAdam: a Method for Stochastic Optimization. åœ¨ Adam ä¸­, ç”¨æ¥å½’ä¸€åŒ–æ¢¯åº¦çš„å› å­ $v_t$ ä¸è¿‡å»çš„æ¢¯åº¦(åŒ…å«åœ¨ $v_{t-1}$ ä¸­)ä»¥åŠå½“å‰çš„æ¢¯åº¦ $|g_t|^2$ çš„ l2 èŒƒå¼æˆåæ¯”ã€‚ $$v_t=\\beta_2v_{t-1}+(1-\\beta_2)g_t^2$$ å¯ä»¥å°†å…¶æ³›åŒ–åˆ° $l_p$ èŒƒå¼ã€‚åŒæ ·çš„ $\\beta_2$ å˜ä¸º $\\beta_2^p$. Norms for large p values generally become numerically unstable, which is why $l_1$ and $l_2$ norms are most common in practice. However, $l_{\\infty}$ also generally exhibits stable behavior. For this reason, the authors propose AdaMax [10] and show that $v_t$ with $l_{\\infty}$ converges to the following more stable value. To avoid confusion with Adam, we use ut to denote the infinity norm-constrained $v_t$: $$\\mu_t=\\beta_2^{\\infty}v_{t-1}+(1-\\beta_2^{\\infty})|g_t|^{\\infty}$$ $$=max(\\beta_2\\cdot v_{t-1}, |g_t|)$$ ç„¶åç”¨ $\\mu_t$ ä»£æ›¿ Adam ä¸­çš„ $\\sqrt(v_t)+\\epsilon$: $$\\theta_{t+1}=\\theta_t-\\dfrac{\\eta}{\\mu_t}{\\hat m_t}$$ Note that as $\\mu_t$ relies on the max operation, it is not as suggestible to bias towards zero as $m_t$ and $v_t$ in Adam, which is why we do not need to compute a bias correction for ut. Good default values are again: $$\\eta = 0.002, \\beta_1 = 0.9, \\beta_2 = 0.999.$$ Visualization of algorithms we see the path they took on the contours of a loss surface (the Beale function). All started at the same point and took different paths to reach the minimum. Note that Adagrad, Adadelta, and RMSprop headed off immediately in the right direction and converged similarly fast, while Momentum and NAG were led off-track, evoking the image of a ball rolling down the hill. NAG, however, was able to correct its course sooner due to its increased responsiveness by looking ahead and headed to the minimum. å¦‚æœç›®æ ‡å‡½æ•°æ˜¯ Beale è¿™ç§ç±»å‹çš„å‡½æ•°ï¼Œè‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•èƒ½æ›´ç›´æ¥çš„æ”¶æ•›åˆ°æœ€å°å€¼ã€‚è€Œ Momentum å’Œ NAG åˆ™åç¦»äº†è½¨é“ï¼Œå°±åƒçƒä»å±±ä¸Šæ»šä¸‹ä¸€æ ·ï¼Œåˆ¹ä¸ä½è½¦ã€‚ä½†æ˜¯ NAG å› ä¸ºå¯¹æœªæ¥å…·æœ‰ä¸€å®šçš„é¢„è§æ€§ï¼Œæ‰€ä»¥èƒ½æ›´æ—©çš„çº æ­£ä»è€Œæé«˜å…¶å“åº”èƒ½åŠ›ã€‚ shows the behaviour of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope, which pose a difficulty for SGD as we mentioned before. Notice here that SGD, Momentum, and NAG find it difficulty to break symmetry, although the latter two eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope, with Adadelta leading the charge. å„ç§ä¼˜åŒ–ç®—æ³•éç‚¹çš„è¡¨ç°ã€‚ Momentum, SGD, NAG å¾ˆéš¾æ‰“ç ´å¹³è¡¡ï¼Œè€Œè‡ªé€‚åº”æ€§çš„ç®—æ³• Adadelta, RMSprop, Adadelta èƒ½å¾ˆå¿«çš„é€ƒç¦»éç‚¹ã€‚ examplemodel123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.nn as nnimport torch.optim as optimclass TestNet(nn.Module): def __init__(self): super(TestNet, self).__init__() self.linear1 = nn.Linear(10, 5) self.linear2 = nn.Linear(5, 1) self.loss = nn.BCELoss() def forward(self, x, label): &quot;&quot;&quot; x: [batch, 10] label: [batch] &quot;&quot;&quot; out = self.linear2(self.linear1(x)).squeeze() loss = self.loss(out, label) return out, loss 12345model = TestNet()model TestNet( (linear1): Linear(in_features=10, out_features=5, bias=True) (linear2): Linear(in_features=5, out_features=1, bias=True) (loss): BCELoss() ) 123list(model.named_parameters()) [('linear1.weight', Parameter containing: tensor([[ 0.2901, -0.0022, -0.1515, -0.1064, -0.0475, -0.0324, 0.0404, 0.0266, -0.2358, -0.0433], [-0.1588, -0.1917, 0.0995, 0.0651, -0.2948, -0.1830, 0.2356, 0.1060, 0.2172, -0.0367], [-0.0173, 0.2129, 0.3123, 0.0663, 0.2633, -0.2838, 0.3019, -0.2087, -0.0886, 0.0515], [ 0.1641, -0.2123, -0.0759, 0.1198, 0.0408, -0.0212, 0.3117, -0.2534, -0.1196, -0.3154], [ 0.2187, 0.1547, -0.0653, -0.2246, -0.0137, 0.2676, 0.1777, 0.0536, -0.3124, 0.2147]], requires_grad=True)), ('linear1.bias', Parameter containing: tensor([ 0.1216, 0.2846, -0.2002, -0.1236, 0.2806], requires_grad=True)), ('linear2.weight', Parameter containing: tensor([[-0.1652, 0.3056, 0.0749, -0.3633, 0.0692]], requires_grad=True)), ('linear2.bias', Parameter containing: tensor([0.0450], requires_grad=True))] add model parameters to optimizer123456789import torch.optim as optim# parameters = model.parameters()parameters_filters = filter(lambda p: p.requires_grad, model.parameters()) 12345678910111213optimizer = optim.Adam( params=parameters_filters, lr=0.001, betas=(0.8, 0.999), eps=1e-8, weight_decay=3e-7) 123optimizer.state_dict &lt;bound method Optimizer.state_dict of Adam ( Parameter Group 0 amsgrad: False betas: (0.8, 0.999) eps: 1e-08 lr: 0.001 weight_decay: 3e-07 )&gt; ä¸åŒçš„æ¨¡å—è®¾ç½®ä¸åŒçš„å‚æ•°12345parameters = [{&quot;params&quot;: model.linear1.parameters()}, {&quot;params&quot;:model.linear2.parameters(), &quot;lr&quot;: 3e-4}] 12345678910111213optimizer2 = optim.Adam( params=parameters, lr=0.001, betas=(0.8, 0.999), eps=1e-8, weight_decay=3e-7) 123optimizer2.state_dict &lt;bound method Optimizer.state_dict of Adam ( Parameter Group 0 amsgrad: False betas: (0.8, 0.999) eps: 1e-08 lr: 0.001 weight_decay: 3e-07 Parameter Group 1 amsgrad: False betas: (0.8, 0.999) eps: 1e-08 lr: 0.0003 weight_decay: 3e-07 )&gt; zero_gradåœ¨è¿›è¡Œåå‘ä¼ æ’­ä¹‹å‰ï¼Œå¦‚æœä¸éœ€è¦æ¢¯åº¦ç´¯åŠ çš„è¯ï¼Œå¿…é¡»è¦ç”¨zero_grad()æ¸…ç©ºæ¢¯åº¦ã€‚å…·ä½“çš„æ–¹æ³•æ˜¯éå†self.param_groupsä¸­å…¨éƒ¨å‚æ•°ï¼Œæ ¹æ®gradå±æ€§åšæ¸…é™¤ã€‚ 123456789101112131415def zero_grad(self): r&quot;&quot;&quot;Clears the gradients of all optimized :class:`torch.Tensor` s.&quot;&quot;&quot; for group in self.param_groups: for p in group['params']: if p.grad is not None: p.grad.detach_() p.grad.zero_() 12345group_parameters = [{&quot;params&quot;: model.linear1.parameters()}, {&quot;params&quot;:model.linear2.parameters(), &quot;lr&quot;: 3e-4}] 123456789x = torch.randn(2, 10)label = torch.Tensor([1,0])out, loss = model(x, label)loss.backward() 123optimizer2.zero_grad() 1234567891011for group in group_parameters: for p in group[&quot;params&quot;]: if p.grad is not None: p.grad.detach_() p.grad.zero_() è¿™é‡Œå¹¶æ²¡æœ‰ä½¿ç”¨ backward() æ‰€ä»¥æš‚æ—¶ä¸å­˜åœ¨æ¢¯åº¦ã€‚ åœ¨åå‘ä¼ æ’­ backward() è®¡ç®—å‡ºæ¢¯åº¦ä¹‹åï¼Œå°±å¯ä»¥è°ƒç”¨step()å®ç°å‚æ•°æ›´æ–°ã€‚ä¸è¿‡åœ¨ Optimizer ç±»ä¸­ï¼Œstep()å‡½æ•°å†…éƒ¨æ˜¯ç©ºçš„ï¼Œå¹¶ä¸”ç”¨raise NotImplementError æ¥ä½œä¸ºæé†’ã€‚åé¢ä¼šæ ¹æ®å…·ä½“çš„ä¼˜åŒ–å™¨æ¥åˆ†æstep()çš„å®ç°æ€è·¯ã€‚ è¾…åŠ©ç±»lr_schedulerlr_schedulerç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®è½®æ¬¡çµæ´»è°ƒæ§å­¦ä¹ ç‡ã€‚è°ƒæ•´å­¦ä¹ ç‡çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼Œä½†æ˜¯å…¶ä½¿ç”¨æ–¹æ³•æ˜¯å¤§è‡´ç›¸åŒçš„ï¼šç”¨ä¸€ä¸ªScheduleæŠŠåŸå§‹Optimizerè£…é¥°ä¸Šï¼Œç„¶åå†è¾“å…¥ä¸€äº›ç›¸å…³å‚æ•°ï¼Œç„¶åç”¨è¿™ä¸ªScheduleåšstep()ã€‚ 1234567# lambda1 = lambda epoch: epoch // 30lambda1 = lambda epoch: 0.95 ** epochscheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1) 123scheduler.step() warm up scheduler12345678910111213141516171819202122232425262728293031323334353637import mathparameters = filter(lambda p: p.requires_grad, model.parameters())lr_warm_up_num = 1000optimizer = optim.Adam( params=parameters, lr=0.001, betas=(0.8, 0.999), eps=1e-8, weight_decay=3e-7)cr = 1.0 / math.log(lr_warm_up_num)scheduler = optim.lr_scheduler.LambdaLR( optimizer, lr_lambda=lambda ee: cr * math.log(ee + 1) if ee &lt; lr_warm_up_num else 1)","link":"/2018/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"title":"è®ºæ–‡ç¬”è®°-CoQA","text":"paper: CoQA: A Conversational Question Answering Challenge Motivation We introduce CoQA, a novel dataset for building Conversational Question Answering systems.1 Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. CoQA, å¯¹è¯å¼é˜…è¯»ç†è§£æ•°æ®é›†ã€‚ä» 7 ä¸ªä¸åŒé¢†åŸŸçš„ 8k å¯¹è¯ä¸­è·å–çš„ 127k é—®ç­”å¯¹ã€‚ The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. CoQA è·Ÿä¼ ç»Ÿçš„ RC æ•°æ®é›†æ‰€é¢ä¸´çš„æŒ‘æˆ˜ä¸ä¸€æ ·ï¼Œä¸»è¦æ˜¯æŒ‡ä»£å’Œæ¨ç†ã€‚ We ask other people a question to either seek or test their knowledge about a subject. Depending on their answer, we follow up with another question and their answer builds on what has already been discussed. This incremental aspect makes human conversations succinct. An inability to build up and maintain common ground in this way is part of why virtual assistants usually donâ€™t seem like competent conversational partners. æˆ‘ä»¬é—®å…¶ä»–äººä¸€ä¸ªé—®é¢˜ï¼Œæ¥å¯»æ±‚æˆ–è€…æµ‹è¯•ä»–ä»¬å¯¹æŸä¸€ä¸ªä¸»é¢˜çš„çŸ¥è¯†ã€‚ç„¶åä¾èµ–äºä»–çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œä»–æ ¹æ®åˆšæ‰æˆ‘ä»¬è®¨è®ºçš„æ¥å›ç­”è¿™ä¸ªæ–°çš„é—®é¢˜ã€‚ è¿™ä½¿å¾—å¯¹è¯å˜å¾—å¾ˆç®€çŸ­ã€‚è€Œæ­£æ˜¯è¿™ç§å»ºç«‹å’Œç»´æŒå…±åŒç‚¹çš„èƒ½åŠ›ç¼ºå¤±ï¼Œä½¿å¾—è™šæ‹ŸåŠ©æ‰‹çœ‹èµ·æ¥å¹¶ä¸æ˜¯ä¸€ä¸ªæœ‰èƒ½åŠ›çš„å¯¹è¯è€…ã€‚ è€Œ CoQA å°±æ˜¯è¦æµ‹è¯•è¿™ç§èƒ½åŠ›ã€‚ Introduction In CoQA, a machine has to understand a text passage and answer a series of questions that appear in a conversation. We develop CoQA with three main goals in mind. The first concerns the nature of questions in a human conversation. Posing short questions is an effective human conversation strategy, but such questions are a pain in the neck for machines. ç¬¬ä¸€ç‚¹ï¼šäººç±»åœ¨å¯¹è¯æ—¶ï¼Œä¼šæå‡ºå¾ˆç®€çŸ­çš„é—®é¢˜ï¼Œä½†è¿™å¯¹äºæœºå™¨æ¥è¯´å´å¾ˆéš¾ã€‚æ¯”å¦‚ Q5 â€œWho?â€ The second goal of CoQA is to ensure the naturalness of answers in a conversation. Many existing QA datasets restrict answers to a contiguous span in a given passage, also known as extractive answers (Table 1). Such answers are not always natural, for example, there is no extractive answer for Q4 (How many?) in Figure 1. In CoQA, we propose that the answers can be free-form text (abstractive answers), while the extractive spans act as rationales for the actual answers. Therefore, the answer for Q4 is simply Three while its rationale is spanned across multiple sentences. ç¬¬äºŒç‚¹ï¼šç­”æ¡ˆä¸æ˜¯æŠ½å–å¼çš„ extractiveï¼Œè€Œæ˜¯æ€»ç»“æ€§çš„ abstractive, free-from text. æ¯”å¦‚ Q4. å¥½éš¾å•Šï¼ï¼ï¼ The third goal of CoQA is to enable building QA systems that perform robustly across domains. The current QA datasets mainly focus on a single domain which makes it hard to test the generalization ability of existing models. ç¬¬ä¸‰ç‚¹ï¼šæ•°æ®æ¥è‡ªå¤šç§ domainï¼Œæé«˜æ³›åŒ–æ€§ã€‚ Dataset collectionæ•°æ®é›†å…·ä½“è¯¦æƒ…ï¼š It consists of 127k conversation turns collected from 8k conversations over text passages (approximately one conversation per passage). The average conversation length is 15 turns, and each turn consists of a question and an answer. It contains free-form answers. Each answer has an extractive rationale highlighted in the passage. Its text passages are collected from seven diverse domains â€” five are used for in-domain evaluation and two are used for out-of-domain evaluation. Almost half of CoQA questions refer back to conversational history using coreferences, and a large portion requires pragmatic reasoning making it challenging for models that rely on lexical cues alone. å¤§éƒ¨åˆ†æ¶‰åŠåˆ°å¯¹è¯å†å²çš„é—®é¢˜éƒ½ç”¨åˆ°äº†æŒ‡ä»£å’Œé€»è¾‘æ¨ç†ï¼Œè¿™å¯¹äºä»…ä»…æ˜¯ä¾èµ–äºè¯æ±‡æç¤ºï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰çš„æ¨¡å‹æ¥è¯´ä¼šå¾ˆéš¾ã€‚ The best-performing system, a reading comprehension model that predicts extractive rationales which are further fed into a sequence-to-sequence model that generates final answers, achieves a F1 score of 65.1%. In contrast, humans achieve 88.8% F1, a superiority of 23.7% F1, indicating that there is a lot of headroom for improvement. Baseline æ˜¯å°†æŠ½å–å¼é˜…è¯»ç†è§£æ¨¡å‹è½¬æ¢æˆ seq2seq å½¢å¼ï¼Œç„¶åä» rationale ä¸­è·å–ç­”æ¡ˆï¼Œæœ€ç»ˆå¾—åˆ°äº† 65.1% çš„ F1 å€¼ã€‚ question and answer collection We want questioners to avoid using exact words in the passage in order to increase lexical diversity. When they type a word that is already present in the passage, we alert them to paraphrase the question if possible. questioner æå‡ºçš„é—®é¢˜åº”å°½å¯èƒ½é¿å…ä½¿ç”¨å‡ºç°åœ¨ passage ä¸­çš„è¯ï¼Œè¿™æ ·å¯ä»¥å¢åŠ è¯æ±‡çš„å¤šæ ·æ€§ã€‚ For the answers, we want answerers to stick to the vocabulary in the passage in order to limit the number of possible answers. We encourage this by automatically copying the highlighted text into the answer box and allowing them to edit copied text in order to generate a natural answer. We found 78% of the answers have at least one edit such as changing a wordâ€™s case or adding a punctuation. å¯¹äºç­”æ¡ˆå‘¢ï¼Œå°½å¯èƒ½çš„ä½¿ç”¨ passage ä¸­å‡ºç°çš„è¯ï¼Œä»è€Œé™åˆ¶å‡ºç°å¾ˆå¤šä¸­ç­”æ¡ˆçš„å¯èƒ½æ€§ã€‚ä½œè€…é€šè¿‡å¤åˆ¶ highlighted text(ä¹Ÿå°±æ˜¯ rationale å§) åˆ° answer boxï¼Œç„¶åè®© answerer å»ç”Ÿæˆç›¸åº”çš„ answer. å…¶ä¸­ 78% çš„ç­”æ¡ˆæ˜¯éœ€è¦ä¸€ä¸ªç¼–è¾‘è·ç¦»ï¼Œæ¯”å¦‚ä¸€ä¸ªè¯çš„å¤§å°å†™æˆ–å¢åŠ æ ‡ç‚¹ç¬¦å·ã€‚ passage collection Not all passages in these domains are equally good for generating interesting conversations. A passage with just one entity often result in questions that entirely focus on that entity. Therefore, we select passages with multiple entities, events and pronominal references using Stanford CoreNLP (Manning et al., 2014). We truncate long articles to the first few paragraphs that result in around 200 words. å¦‚æœä¸€ä¸ª passage åªæœ‰ä¸€ä¸ª entityï¼Œé‚£ä¹ˆæ ¹æ®å®ƒç”Ÿæˆçš„å¯¹è¯éƒ½ä¼šæ˜¯å›´ç»•è¿™ä¸ª entity çš„ã€‚æ˜¾ç„¶è¿™ä¸æ˜¯è¿™ä¸ªæ•°æ®é›†æƒ³è¦çš„ã€‚å› æ­¤ï¼Œä½œè€…ä½¿ç”¨ Stanford CoreNLP æ¥å¯¹ passage è¿›è¡Œåˆ†æåé€‰æ‹©å¤šä¸ª entity å’Œ event çš„ passage. Table 2 shows the distribution of domains. We reserve the Science and Reddit domains for out-ofdomain evaluation. For each in-domain dataset, we split the data such that there are 100 passages in the development set, 100 passages in the test set, and the rest in the training set. For each out-of-domain dataset, we just have 100 passages in the test set. In domain ä¸­åŒ…å« Children, Literature, Mid/HIgh school, News, Wikipedia. ä»–ä»¬åˆ†å‡º 100 passage åˆ°å¼€å‘é›†(dev dataset), å…¶ä½™çš„åœ¨è®­ç»ƒé›† (train dataset). out-of-diomain åŒ…å« Science Reddit ï¼Œåˆ†åˆ«æœ‰ 100 passage åœ¨å¼€å‘é›†ä¸­ã€‚ test dataset: Collection multiple answers Some questions in CoQA may have multiple valid answers. For example, another answer for Q4 in Figure 2 is A Republican candidate. In order to account for answer variations, we collect three additional answers for all questions in the development and test data. ä¸€ä¸ªé—®é¢˜å¯èƒ½å‡ºç°å¤šç§å›ç­”ï¼Œå› æ­¤åœ¨dev dataset å’Œ test dataset ä¸­æœ‰ä¸‰ä¸ªå€™é€‰ç­”æ¡ˆã€‚ In the previous example, if the original answer was A Republican Candidate, then the following question Which party does he belong to? would not have occurred in the first place. When we show questions from an existing conversation to new answerers, it is likely they will deviate from the original answers which makes the conversation incoherent. It is thus important to bring them to a common ground with the original answer. æ¯”å¦‚ä¸Šå›¾ä¸­ Q4, å¦‚æœå›ç­”æ˜¯ A Republican candidate. ä½†æ˜¯æ•´ä¸ªå¯¹è¯æ˜¯ç›¸å…³çš„ï¼Œæ‰€ä»¥æ¥ä¸‹æ¥çš„é—®é¢˜å°±ä¼šä½¿æ•´ä¸ªå¯¹è¯æ˜¾å¾—æ··ä¹±äº†ã€‚ We achieve this by turning the answer collection task into a game of predicting original answers. First, we show a question to a new answerer, and when she answers it, we show the original answer and ask her to verify if her answer matches the original. For the next question, we ask her to guess the original answer and verify again. We repeat this process until the conversation is complete. In our pilot experiment, the human F1 score is increased by 5.4% when we use this verification setup. å› ä¸ºæœºå™¨åœ¨å­¦ä¹ çš„æ—¶å€™æ˜¯æœ‰ original answer è¿›è¡Œå¯¹æ¯”çš„ï¼ŒåŒæ ·çš„è¿™ä¸ªè¿‡ç¨‹åœ¨äººå·¥é˜¶æ®µä¹Ÿæ˜¯éœ€è¦çš„ï¼Œå¯ä»¥å‡å°‘ä¸Šè¯‰çš„æ··ä¹±æƒ…å†µï¼Œanswerer åœ¨ç»™å‡ºä¸€ä¸ªç­”æ¡ˆåï¼Œä½œè€…ä¼šå‘Šè¯‰ä»–ä»¬æ˜¯å¦ä¸ original åŒ¹é…ï¼Œç„¶åç›´åˆ°æ•´ä¸ªè¿‡ç¨‹å®Œæˆã€‚ Dataset AnalysisWhat makes the CoQA dataset conversational compared to existing reading comprehension datasets like SQuAD? How does the conversation flow from one turn to the other? What linguistic phenomena do the questions in CoQA exhibit? We answer these questions below. åœ¨ question ä¸­ï¼š æŒ‡ä»£è¯(he, him, she, it, they)å‡ºç°çš„æ›´ä¸ºé¢‘ç¹ï¼Œ SQuAD åˆ™å‡ ä¹æ²¡æœ‰ã€‚ SQuAD ä¸­ what å‡ ä¹å äº†ä¸€åŠï¼ŒCoQA ä¸­é—®é¢˜ç±»å‹åˆ™æ›´ä¸ºå¤šæ ·ï¼Œ æ¯”å¦‚ did, was, is, does çš„é¢‘ç‡å¾ˆé«˜ã€‚ CoQA çš„é—®é¢˜æ›´åŠ ç®€çŸ­ã€‚è§å›¾ 3. answer æœ‰ 33% çš„æ˜¯ abstractive. è€ƒè™‘åˆ°äººå·¥å› ç´ ï¼ŒæŠ½å–å¼çš„ answer æ˜¾ç„¶æ›´å¥½å†™ï¼Œæ‰€ä»¥è¿™é«˜äºä½œè€…é¢„æœŸäº†ã€‚yes/no çš„ç­”æ¡ˆä¹Ÿæœ‰ä¸€å®šæ¯”é‡ã€‚ Conversation FlowA coherent conversation must have smooth transitions between turns. ä¸€æ®µå¥½çš„å¯¹è¯æ˜¯å…·æœ‰å¼•å¯¼æ€§çš„ï¼Œä¸æ–­æ·±å…¥æŒ–æ˜ passage çš„ä¿¡æ¯ã€‚ ä½œè€…å°† passage å‡åŒ€åˆ†æˆ 10 chunksï¼Œç„¶ååˆ†æéšç€å¯¹è¯ turn çš„å˜åŒ–ï¼Œå…¶å¯¹åº”çš„ passage chunks å˜åŒ–çš„æƒ…å†µã€‚ Linguistic Phenomena Relationship between a question and its passageï¼š lexical match: question å’Œ passage ä¸­è‡³å°‘æœ‰ä¸€ä¸ªè¯æ˜¯åŒ¹é…çš„ã€‚ Paraphrasing: è§£é‡Šå‹ã€‚è™½ç„¶ question æ²¡æœ‰ä¸ passage çš„è¯ï¼Œä½†æ˜¯ç¡®å®å¯¹ rationale çš„ä¸€ç§è§£é‡Šï¼Œä¹Ÿå°±æ˜¯æ¢äº†ä¸€ç§è¯´æ³•ï¼Œå½“ä½œé—®é¢˜æå‡ºäº†ã€‚é€šå¸¸è¿™é‡Œé¢åŒ…å«ï¼š synonymy(åŒä¹‰è¯), antonymy(åä¹‰è¯), hypernymy(ä¸Šä½è¯), hyponymy(ä¸‹ä½è¯) and negation(å¦å®šè¯). Pragmatics: éœ€è¦æ¨ç†çš„ã€‚ Relationship between a question and its conversation historyï¼š No coref Explicit coref. Implicit coref.","link":"/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/"},{"title":"è®ºæ–‡ç¬”è®°-DETR and Deformable DETR","text":"DETR: End-to-End Object Detection with Transformers Deformable DETRï¼š Deformable Transformer for End-to-End Object Detection DETR: End-to-End Object Detection with Transformerspaper link: https://arxiv.org/abs/2005.12872 Motivationä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹èŒƒå¼ï¼š ä½¿ç”¨CNN Backboneæå–feature map; ä½¿ç”¨ Region Proposal Network (RPN) åœ¨feature mapä¸Šæšä¸¾æ‰€æœ‰çš„windows,è¾“å‡ºNä¸ªå€™é€‰boxes ä½¿ç”¨åˆ†ç±»å™¨å¯¹æ¯ä¸ªboxè¿›è¡Œåˆ†ç±» è¿™æ ·å­˜åœ¨å¾ˆå¤šé—®é¢˜ï¼š é—®é¢˜1ï¼šEnumerate candidate boxes in RPN æšä¸¾æ‰€æœ‰çš„pixel å¯¹æ¯ä¸ªpixelï¼Œæšä¸¾å‡ºæ‰€æœ‰é¢„å®šä¹‰å¤§å°çš„boxes å¤§éƒ¨åˆ†çš„å€™é€‰æ¡†éƒ½æ˜¯æ— æ•ˆçš„ï¼Œå› æ­¤ Inefficient, slow é—®é¢˜2ï¼šRedundant boxes and NMS RPN è¾“å‡ºå¤§é‡çš„boxes Non-maximum suppression (NMS) merges/removes redundant boxes These hand-designed components have a few hyperparameters Model tuning is complex Architecture of DETRå› æ­¤ï¼Œä½œè€…æå‡ºäº†æ›´ä¸ºç®€å•çš„end-to-endçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼š Self-attention vs. Convolution in VisionTransformer å·²ç»éå¸¸ç†Ÿæ‚‰äº†ï¼Œå°±ä¸è¯¦ç»†ä»‹ç»äº†ã€‚è¿™é‡Œå¯¹æ¯”ä¸‹ self-attention å’Œ CNN åœ¨è§†è§‰é¢†åŸŸçš„åº”ç”¨ æ¯ä¸ªpixelå¯ä»¥çœ‹ä½œæ˜¯è‡ªç„¶è¯­è¨€ä¸­çš„ token convolution is used to integrate pixels Recognize patterns within a small window of pixels Difficult to integrate non-local pixels Have to make network very deep to â€œsee the big pictureâ€ Self-attention (transformer) also integrates multiple pixels Works when the correlated pixels are non-local Trunk, tail, legs of an elephant to a whole elephant Architecture æ•´ä¸ªç»“æ„å¯ä»¥çœ‹ä½œå››éƒ¨åˆ†ï¼š CNN backbone Transformer Encoder Transformer Decoder bipartite matching loss CNN backbone + Transformer Encoderå‰ä¸¤ä¸ªå¥½ç†è§£ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ position encoding ä¸NLPä¸­ä¸åŒã€‚è€ƒè™‘åˆ°è¾“å…¥æ˜¯äºŒç»´å›¾åƒï¼Œå¯¹åº”çš„ä½ç½® (x,y) ä¹Ÿæ˜¯äºŒç»´çš„ã€‚ æ€»ç»“ä¸‹å’ŒåŸå§‹transformerç¼–ç å™¨ä¸åŒçš„åœ°æ–¹ï¼š è¾“å…¥ç¼–ç å™¨çš„ä½ç½®ç¼–ç éœ€è¦è€ƒè™‘2-Dç©ºé—´ä½ç½®ã€‚ ä½ç½®ç¼–ç å‘é‡éœ€è¦åŠ å…¥åˆ°æ¯ä¸ªEncoder Layerä¸­ã€‚ åœ¨ç¼–ç å™¨å†…éƒ¨ä½ç½®ç¼–ç Positional Encodingä»…ä»…ä½œç”¨äºQueryå’ŒKeyï¼Œå³åªä¸Queryå’ŒKeyç›¸åŠ ï¼ŒValueä¸åšä»»ä½•å¤„ç†ã€‚ Transformer Decoderdecoderä¸åŸå§‹çš„transformer decoderçš„åŒºåˆ«åœ¨äºä¸¤ç‚¹ï¼š å…¶è¾“å…¥æ˜¯ Object queries. æ˜¯å¯å­¦ä¹ çš„ nn.Embedding, ç»´åº¦ä¸º [100, bsz, 256]. å…¶å®å¯ä»¥ç†è§£æˆå¯å­¦ä¹ çš„ä½ç½®ç¼–ç ã€‚ éè‡ªå›å½’çš„å¯å¹¶è¡Œè§£ç ã€‚ bipartite matching lossdecoder çš„è¾“å‡ºå¼ é‡çš„ç»´åº¦æ˜¯ åˆ†ç±»åˆ†æ”¯ï¼š[bsz, 100, class + 1] å’Œå›å½’åˆ†æ”¯ï¼š[bsz, 100, 4]. å…¶ä¸­ class + 1 è¡¨ç¤ºç±»åˆ«æ€»æ•°+èƒŒæ™¯ã€‚æœ‰ç‰©ä½“çš„boxesè®¡ç®—å›å½’ä»»åŠ¡ï¼›æ²¡æœ‰ç‰©ä½“çš„èƒŒæ™¯æ¡†ï¼Œåˆ™ä¸ç”¨å›å½’ã€‚ é—®é¢˜æ¥äº†ï¼šè¿™100ä¸ªå€™é€‰æ¡†å¦‚ä½•å»ä¸ ä¸ç¡®å®šæ•°é‡çš„ targets è¿›è¡ŒåŒ¹é…å‘¢ï¼Œé¢„æµ‹æ¡†å’ŒçœŸå€¼æ˜¯æ€ä¹ˆä¸€ä¸€å¯¹åº”çš„ï¼Ÿæ¢å¥è¯è¯´ï¼šä½ æ€ä¹ˆçŸ¥é“ç¬¬47ä¸ªé¢„æµ‹æ¡†å¯¹åº”å›¾ç‰‡é‡Œçš„ç‹—ï¼Œç¬¬88ä¸ªé¢„æµ‹æ¡†å¯¹åº”å›¾ç‰‡é‡Œçš„è½¦ï¼Ÿç­‰ç­‰ã€‚ ç›¸æ¯”Faster R-CNNç­‰åšæ³•ï¼ŒDETRæœ€å¤§ç‰¹ç‚¹æ˜¯å°†ç›®æ ‡æ£€æµ‹é—®é¢˜è½¬åŒ–ä¸ºæ— åºé›†åˆé¢„æµ‹é—®é¢˜(set prediction)ã€‚è®ºæ–‡ä¸­ç‰¹æ„æŒ‡å‡ºFaster R-CNNè¿™ç§è®¾ç½®ä¸€å¤§å †anchorï¼Œç„¶ååŸºäºanchorè¿›è¡Œåˆ†ç±»å’Œå›å½’å…¶å®å±äºä»£ç†åšæ³•å³ä¸æ˜¯æœ€ç›´æ¥åšæ³•ï¼Œç›®æ ‡æ£€æµ‹ä»»åŠ¡å°±æ˜¯è¾“å‡ºæ— åºé›†åˆï¼Œè€ŒFaster R-CNNç­‰ç®—æ³•é€šè¿‡å„ç§æ“ä½œï¼Œå¹¶ç»“åˆå¤æ‚åå¤„ç†æœ€ç»ˆæ‰å¾—åˆ°æ— åºé›†åˆå±äºç»•è·¯äº†ï¼Œè€ŒDETRå°±æ¯”è¾ƒçº¯ç²¹äº†ã€‚ç°åœ¨æ ¸å¿ƒé—®é¢˜æ¥äº†ï¼šè¾“å‡ºçš„ [bsz, 100] ä¸ªæ£€æµ‹ç»“æœæ˜¯æ— åºçš„ï¼Œå¦‚ä½•å’Œ ground-truth bounding box è®¡ç®—lossï¼Ÿè¿™å°±éœ€è¦ç”¨åˆ°ç»å…¸çš„åŒè¾¹åŒ¹é…ç®—æ³•äº†ï¼Œä¹Ÿå°±æ˜¯å¸¸è¯´çš„ åŒˆç‰™åˆ©ç®—æ³•ï¼Œè¯¥ç®—æ³•å¹¿æ³›åº”ç”¨äºæœ€ä¼˜åˆ†é…é—®é¢˜ã€‚ æ•´ä¸ªloss functionçš„è®¡ç®—åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š ä¾æ®åŒˆç‰™åˆ©ç®—æ³•å…ˆæ‰¾åˆ°æœ€ä¼˜çš„åŒ¹é… æ ¹æ®æ‰¾åˆ°çš„åŒ¹é…ï¼Œè®¡ç®—æœ€ç»ˆçš„loss å¦‚ä½•æ‰¾åˆ°æœ€ä¼˜çš„åŒ¹é…å‘¢ï¼Ÿå‡è®¾ä¸€å¼ å›¾ç‰‡æœ‰3ä¸ªç›®æ ‡ï¼šdog, cat, car. é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ª [100, 3] çš„çŸ©é˜µ, çŸ©é˜µçš„å€¼åˆ†åˆ«è¡¨ç¤ºè¿™100ä¸ªå€™é€‰tokenä¸ºå…¶ä¸­æŸä¸€ç›®æ ‡çš„â€œæŸå¤±â€. è®©è¿™ä¸ªæŸå¤±æœ€å°çš„åŒ¹é…å°±æ˜¯æˆ‘ä»¬è¦æ‰¾çš„æœ€ä¼˜åŒ¹é…ã€‚è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯åŒˆç‰™åˆ©ç®—æ³•ã€‚ å†ä»‹ç»åŒˆç‰™åˆ©ç®—æ³•ä¹‹å‰ï¼Œæˆ‘ä»¬è¦çŸ¥é“è¿™ä¸ªçŸ©é˜µå¯¹åº”çš„æŸå¤±å€¼æ˜¯å•¥å‘¢ï¼Ÿä¹Ÿå°±æ˜¯æ€ä¹ˆè¡¡é‡è¿™ä¸ªå€™é€‰tokenæ˜¯æŸä¸ªç›®æ ‡çš„æŸå¤±å€¼ï¼Ÿ å‡è®¾ $y_i$ å¯¹åº”çš„é¢„æµ‹æ˜¯ $\\hat y_{\\sigma_i}$,é‚£ä¹ˆè¿™ä¸ªæ„æˆè¿™ä¸ªæ­é…çš„æŸå¤±å°±æ˜¯ $\\hat y_{\\sigma_i}$ å¯¹åº”çš„ç±»åˆ«ä¸º $c_i$ ä»¥åŠ box çš„ mse å€¼ã€‚ $$L_{match}(y_i, \\hat y_{\\sigma(i)})=-\\mathbb{1}{c_i\\ne \\varnothing}\\hat p{\\sigma(i)}(c_i) + 1_{c_i\\ne \\varnothing}L_{box}(b_i, \\hat b_{\\sigma_i})$$ $L_{match}$ å°±æ˜¯çŸ©é˜µä¸­çš„å…ƒç´ ã€‚ç„¶åé€šè¿‡åŒˆç‰™åˆ©ç®—æ³•æ‰¾åˆ°å¯¹åº”çš„åŒ¹é…ã€‚ æ‰¾åˆ°åŒ¹é…ä¹‹åè®¡ç®—æœ€ç»ˆçš„loss Deformable DETRDETR çš„ä¼˜ç¼ºç‚¹ï¼š ä¼˜ç‚¹ çº¯ç«¯åˆ°ç«¯ï¼Œä¸éœ€è¦æ‰‹å·¥è®¾è®¡çš„NMSä¹‹ç±» DETR ç¼ºç‚¹ï¼š å°ç›®æ ‡æ£€æµ‹çš„ä½ç²¾åº¦ï¼Œé«˜ç²¾åº¦çš„feature mapçš„å¤æ‚åº¦æ˜¯DETRæ‰€ä¸èƒ½å¿å—çš„ã€‚ è¿™æ˜¯å› ä¸º attention æœºåˆ¶çš„åŸå› ï¼Œå¤æ‚åº¦æ˜¯ O(L^2) æ”¶æ•›é€Ÿåº¦æ…¢ï¼Œslow convergenceã€‚åŸå› æ˜¯ pattern æ˜¯ç¨€ç–çš„ï¼Œå¾ˆéš¾å¿«é€Ÿå­¦åˆ°å§ when $N_k$ is large, it will lead $N_k$to ambiguous gradients for input features. Thus, long training schedules are required so that the attention weights can focus on specific keys. Deformable DETR çš„è®¾è®¡åˆè¡·ï¼š motivation: mitigates the slow convergence and high complexity issues of DETR å…·ä½“è®¾è®¡æ–¹æ³• èåˆDeformable convçš„ç©ºé—´ç¨€ç–é‡‡æ ·å’Œtransformerçš„å…³ç³»å»ºæ¨¡çš„ä¼˜åŠ¿ combines the best of the sparse spatial sampling of deformable convo- lution, and the relation modeling capability of Transformers deformable attention modulesï¼š ä»£æ›¿Transformer attention module Multi-Head Attention in Transformerså¸¸è§„çš„ attention: $A_{mqk}$ æ˜¯attention matrix. $z_q\\in R^C$ æ˜¯query feature. $x_k\\in R^C$ æ˜¯key/value feature. $\\Omega_k$ æ˜¯key indexçš„é›†åˆ. $m$ æ˜¯attention head index. æ€»çš„è®¡ç®—é‡æ˜¯: $O(N_qC^2) + 2O(N_kC^2) + 2O(N_qN_kC^2)$ å¤æ‚åº¦ï¼š$O(N_qC^2 + N_kC^2 + N_qN_kC)$ åœ¨é•¿æ–‡æœ¬ï¼Œæˆ–è€…image patchedä¹‹åï¼Œ$N_q=N_k &gt;&gt; C$. æ‰€ä»¥å¤æ‚åº¦å–å†³äº $O(N_qN_kC)$ Deformable attention module ç›¸æ¯”å¸¸è§„çš„attention, éœ€è¦è€ƒè™‘æ‰€æœ‰çš„key featuresï¼Œä¹Ÿå°±æ˜¯ HW ä¸ªã€‚ deformable attentionåªè€ƒè™‘Kä¸ªkeys. å¦‚ä¸‹é¢ä¸¤ä¸ªå…¬å¼æ‰€ç¤º $\\sum_{k\\in \\Omega_k} \\rightarrow \\sum_{k=1}^K$. å› æ­¤å¤æ‚åº¦ä¼šå¤§å¤§é™ä½ï¼Œæ”¶æ•›é€Ÿåº¦ä¹Ÿä¼šåŠ å¿«ã€‚ å…¶ä¸­ï¼š m è¡¨ç¤ºattention head index $z_q$ è¡¨ç¤ºquery feature $K &lt;&lt; HW$ è¡¨ç¤ºsampling number $p_q$ è¡¨ç¤º $z_q$ å¯¹åº”çš„å‚è€ƒ2Dç‚¹ $\\Delta p_{mqk}$ è¡¨ç¤ºç›¸å¯¹å‚è€ƒç‚¹çš„sampling offset $A_{mqk}$ åˆ™æ˜¯å¯¹åº”é‡‡æ ·ç‚¹çš„weights, $\\sum_{k=1}^{K}A_{mqk}=1$ ç°åœ¨çš„é—®é¢˜åœ¨äºï¼Œè¿™Kä¸ªkeysæ˜¯æ€ä¹ˆé€‰æ‹©çš„å‘¢ï¼Ÿä¹Ÿå°±æ˜¯ offset $\\Delta p_{mqk}$ æ˜¯æ€ä¹ˆæ¥çš„ï¼Ÿæ–‡ä¸­æ˜¯è¿™ä¹ˆè§£é‡Šçš„ï¼š è¿™ä¸ªè¿‡ç¨‹å¦‚å›¾2æ‰€ç¤ºï¼š é€šè¿‡ linear projection å¾—åˆ° 2D å®æ•°å€¼ $\\Delta p_{mqk}$ï¼Œ ç„¶åé€šè¿‡çº¿æ€§æ’å€¼å¾—åˆ° $p_q + \\Delta p_{mqk}$. å¯¹åº”çš„æƒé‡ $A_{mqk}$ ä¹Ÿæ˜¯é€šè¿‡linear projectionå¾—åˆ°çš„ã€‚ è¿™æ · deformable attention çš„å¤æ‚åº¦ï¼š$N_qC^2 + O(N_qKC)$. Multi-scale Deformable Attention ModuleDeformable attention å¯ä»¥å¾ˆè‡ªç„¶çš„æ‹“å±•åˆ°multi-scaleæƒ…å†µä¸‹ã€‚ m è¡¨ç¤º attention head index l è¡¨ç¤º input feature level k è¡¨ç¤º sampling index $A_{mlqk}$ è¡¨ç¤º $k^{th}$ sampling pointåœ¨ $m^{th}$ headå’Œ $l^{th}$ levelçš„æƒé‡, $\\sum_{l=1}^{L}\\sum_{k=1}^{K}A_{mlqk}=1$. $\\Delta p_{mlqk}$ è¡¨ç¤ºsampling offset ä¸ºäº†ç¡®ä¿multi-scaleä¸­ pointçš„å¯¹åº”å…³ç³»ï¼Œä½œè€…ç”¨ normalized coordinates $\\hat p_q\\in [0,1]^2$ æ¥è¡¨ç¤ºå‚è€ƒç‚¹ã€‚ $(0,0), (1,1)$ è¡¨ç¤ºtop-leftå’Œbottom-right. $\\phi_l(\\hat p_q)$ è¡¨ç¤ºå°†å½’ä¸€åŒ–ä¹‹åçš„ $\\hat p_q$ rescale åˆ° $l^{th}$ level featureä¸­ã€‚ Deformable Transformer Encoderä» resnet $C_3$ åˆ° $C_5$ æŠ½å–multi-scaleç‰¹å¾å›¾. å…¶ä¸­ $C_l$ è¡¨ç¤ºåˆ†è¾¨ç‡æ˜¯è¾“å…¥å›¾åƒçš„ $\\dfrac{1}{2^l}$. è¿™æ ·å°±æœ‰3å±‚feature mapäº†ï¼Œç„¶åæœ€åä¸€å±‚feature mapæ˜¯é€šè¿‡ kernel=$3\\times 3$, stride=$3$ çš„å·ç§¯åœ¨ $C_5$ ä¸Šå¾—åˆ°çš„ã€‚ æ€»å…±4 levels feature map. keyå’Œqueryæ¥è‡ªfeature mapä¸­çš„pixels. å¯¹äºæ¯ä¸€ä¸ªquery pixel, å…¶reference pointæ˜¯å…¶æœ¬èº«ã€‚é™¤äº†ä½ç½®ç¼–ç ä¹‹å¤–ï¼Œä½œè€…è¿˜åŠ å…¥äº†levelç¼–ç  $e_l$ï¼Œç”¨æ¥è¡¨ç¤ºåœ¨å“ªä¸€å±‚level. ä½ç½®ç¼–ç æ˜¯å›ºå®šçš„ï¼Œlevelç¼–ç æ˜¯å¯è®­ç»ƒçš„ã€‚","link":"/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/"},{"title":"è®ºæ–‡ç¬”è®°-Clip!!!","text":"paper list: CLIP: Learning Transferable Visual Models From Natural Language Supervision ActionCLIP: A New Paradigm for Video Action Recognition CLIPLearning Transferable Visual Models From Natural Language Supervision æ€è·¯å¾ˆç®€å•ï¼Œå°±æ˜¯imageå’Œtextåšå¯¹æ¯”å­¦ä¹ ï¼Œzero shotèƒ½åŠ›å¼ºå¤§ã€‚ ActionCLIP","link":"/2021/07/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Clip/"},{"title":"æ·±åº¦å­¦ä¹ -æƒé‡åˆå§‹åŒ–","text":"ä¸ºä»€ä¹ˆè¦æƒé‡åˆå§‹åŒ– Xavieråˆå§‹åŒ–çš„æ¨å¯¼ æƒé‡åˆå§‹åŒ–In order to avoid neurons becoming too correlated and ending up in poor local minimize, it is often helpful to randomly initialize parameters. ä¸ºäº†é¿å…ç¥ç»å…ƒé«˜åº¦ç›¸å…³å’Œå±€éƒ¨æœ€ä¼˜åŒ–ï¼Œå¸¸å¸¸éœ€è¦é‡‡ç”¨éšæœºåˆå§‹åŒ–æƒé‡å‚æ•°ï¼Œæœ€å¸¸ç”¨çš„å°±æ˜¯Xavier initiazation. ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æƒé‡åˆå§‹åŒ–ï¼Ÿå¦‚æœæƒé‡å‚æ•°å¾ˆå°çš„è¯ï¼Œè¾“å…¥ä¿¡å·åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¼šä¸æ–­å‡å°ï¼ˆåœ¨0åˆ°1ä¹‹é—´ï¼‰ï¼Œé‚£ä¹ˆæ¯ä¸€å±‚layeréƒ½ä¼šä½¿å¾—è¾“å…¥å˜å°ã€‚åŒæ ·çš„é“ç†ï¼Œå¦‚æœæƒé‡å‚æ•°è¿‡å¤§çš„è¯ï¼Œä¹Ÿä¼šé€ æˆå‰å‘è¾“å…¥è¶Šæ¥è¶Šå¤§ã€‚è¿™æ ·ä¼šå¸¦æ¥ä»€ä¹ˆæ ·çš„åæœå‘¢ï¼Ÿä»¥æ¿€æ´»å‡½æ•°sogmoidä¸ºä¾‹ï¼š å¦‚æœä»¥sigmoidä¸ºæ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œåœ¨æ¯ä¸€å±‚layerè¾“å‡º $W^Tx$ ï¼Œä¹Ÿå°±æ˜¯æ¿€æ´»å‡½æ•°çš„è¾“å…¥ï¼Œå…¶å€¼è¶Šæ¥è¿‘äº0çš„æ—¶å€™ï¼Œå‡½æ•°è¿‘ä¼¼äºçº¿æ€§çš„ï¼Œå› è€Œå°±å¤±å»äº†éçº¿æ€§çš„æ€§è´¨ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°±å¤±å»äº†å¤šå±‚ç¥ç»ç½‘ç»œçš„ä¼˜åŠ¿äº†ã€‚ å¦‚æœåˆå§‹æƒé‡è¿‡å¤§ï¼Œåœ¨å‰å‘ä¼ æ’­çš„è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥æ•°æ®çš„æ–¹å·®varianceä¼šå¢é•¿å¾ˆå¿«ã€‚æ€ä¹ˆç†è§£è¿™å¥è¯ï¼Ÿ ä»¥one layerä¸ºä¾‹ï¼Œå‡è®¾è¾“å…¥æ˜¯ $x\\in R^{1000}$, çº¿æ€§è¾“å‡ºæ˜¯ $y\\in R^{100}$. $$y_j=w_{j,1}x_1+w_{j,2}x_2+â€¦+w_{(j,1000)}x_{1000}$$ xå¯ä»¥çœ‹ä½œæ˜¯1000ç»´çš„æ­£æ€åˆ†å¸ƒï¼Œæ¯ä¸€ç»´ $x_i\\sim N(0,1)$, å¦‚æœ $w_j$å€¼å¾ˆå¤§ï¼Œæ¯”å¦‚ $w_j=[100,100,â€¦,100]$ï¼Œé‚£ä¹ˆè¾“å‡ºç¥ç»å…ƒ $y_i$ çš„æ–¹å·®å°±æ˜¯10000ï¼Œæ‰€ä»¥å°±ä¼šå¾ˆå¤§,å‡å€¼è¿˜æ˜¯0. é‚£ä¹ˆæ¿€æ´»å‡½æ•°çš„è¾“å…¥å¾ˆæœ‰å¯èƒ½æ˜¯ä¸€ä¸ªè¿œå°äº-1æˆ–è¿œå¤§äº1çš„æ•°ï¼Œé€šè¿‡æ¿€æ´»å‡½æ•°æ‰€å¾—çš„å€¼ä¼šéå¸¸æ¥è¿‘äº0æˆ–è€…1ï¼Œä¹Ÿå°±æ˜¯éšè—å±‚ç¥ç»å…ƒå¤„äºé¥±å’ŒçŠ¶æ€(saturated)ï¼Œå…¶æ¢¯åº¦ä¹Ÿå°±æ¥è¿‘äº0äº†ã€‚ æ‰€ä»¥åˆå§‹åŒ–æƒé‡ç‹ ç‹ ç‹ é‡è¦ã€‚é‚£ä¹ˆåº”è¯¥å¦‚ä½•åˆå§‹åŒ–å‘¢ï¼Œä¹Ÿå°±æ˜¯éœ€è¦ä¿è¯ç»è¿‡æ¯ä¸€å±‚layerï¼Œè¦ä¿è¯çº¿æ€§è¾“å‡ºçš„æ–¹å·®ä¿æŒä¸å˜ã€‚è¿™æ ·å°±å¯ä»¥é¿å…æ•°å€¼æº¢å‡ºï¼Œæˆ–æ˜¯æ¢¯åº¦æ¶ˆå¤±ã€‚ Xavier Initializationæˆ‘ä»¬çš„ç›®çš„æ˜¯ä¿æŒçº¿æ€§è¾“å‡ºçš„æ–¹å·®ä¸å˜ã€‚ ä»¥çº¿æ€§è¾“å‡ºçš„ä¸€ä¸ªç¥ç»å…ƒä¸ºä¾‹ï¼Œä¹Ÿå°±æ˜¯yçš„ä¸€ä¸ªç»´åº¦ï¼š $$y_j=w_{j,1}x_1+w_{j,2}x_2+â€¦+w_{j,N} x_N+b$$ å…¶æ–¹å·®ï¼š $$var(y_j) = var(w_{j,1}x_1+w_{j,2}x_2+â€¦+w_{j,N} x_N+b)$$ å…¶ä¸­æ¯ä¸€é¡¹æ ¹æ®æ–¹å·®å…¬å¼å¯å¾—ï¼š $$var(w_{j,i}x_i) = E(x_i)^2var(w_{j,i}) + E(w_{j,i})^2var(xi) + var(w_{j,i})var(x_i)$$ æ¥è‡ªç»´åŸºç™¾ç§‘ï¼š https://en.wikipedia.org/wiki/Variance å…¶ä¸­æˆ‘ä»¬å‡è®¾è¾“å…¥å’Œæƒé‡éƒ½æ˜¯æ¥è‡ªäºå‡å€¼ä¸º0çš„æ­£æ€åˆ†å¸ƒã€‚ $$var(w_{j,i}x_i)=var(w_{j,i})var(x_i)$$ å…¶ä¸­bæ˜¯å¸¸é‡ï¼Œé‚£ä¹ˆï¼š $$var(y_j) = var(w_{j,1})var(x_1) + â€¦ + var(w_{j,N})var(x_N)$$ å› ä¸º $x_1,x_2,..,x_N$ éƒ½æ˜¯ç›¸åŒçš„åˆ†å¸ƒï¼Œ$W_{j,i}$ ä¹Ÿæ˜¯ï¼Œé‚£ä¹ˆå°±æœ‰ï¼š $$var(y_j) = N * var(w{j,i}) * var(x_i)$$ å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœè¾“å…¥ç¥ç»å…ƒæ•°ç›®Nå¾ˆå¤§ï¼Œå‚æ•°æƒé‡Wçš„å€¼ä¹Ÿå¾ˆå¤§çš„è¯ï¼Œä¼šé€ æˆçº¿æ€§è¾“å‡ºçš„å€¼çš„æ–¹å·®å¾ˆå¤§ã€‚ æˆ‘ä»¬éœ€è¦ä¿è¯ $y_j$ çš„æ–¹å·®å’Œ $x_j$ çš„æ–¹å·®ä¸€æ ·ï¼Œæ‰€ä»¥ï¼š $$N*var(W_{j,i})=1$$ $$var(W_{j,i})=1/N$$ There we go! è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†Xavier initializationçš„åˆå§‹åŒ–å…¬å¼ï¼Œä¹Ÿå°±æ˜¯è¯´å‚æ•°æƒé‡åˆå§‹åŒ–ä¸ºå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º 1/N çš„é«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­Nè¡¨ç¤ºå½“å‰å±‚è¾“å…¥ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚åœ¨caffeä¸­å°±æ˜¯è¿™æ ·å®ç°çš„ã€‚ æ›´å¤šåˆå§‹åŒ–æ–¹å¼Understanding the difficulty of training deep feedforward neural networks åœ¨è¿™ç¯‡paperä¸­æå‡º $$var(w)=2/(N_{in}+N_{out})$$ Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification é’ˆå¯¹ä¸€ç§ä¸“é—¨çš„åˆå§‹åŒ–æ–¹å¼ï¼Œä½¿å¾— $var(w)=2.0/N$, åœ¨å®é™…å·¥ç¨‹ä¸­é€šå¸¸ä½¿ç”¨è¿™ç§æ–¹å¼ã€‚ 123456789101112131415161718192021222324252627### æ­£æ€åˆ†å¸ƒ w = np.random.randn(N) * sqrt(2.0/N)### å‡åŒ€åˆ†å¸ƒdef _xavier_initializer(shape, **kwargs): &quot;&quot;&quot; Args: shape: Tuple or 1-d array that species dimensions of requested tensor. Returns: out: tf.Tensor of specified shape sampled from Xavier distribution. &quot;&quot;&quot; epsilon = np.sqrt(6/np.sum(shape)) out = tf.Variable(tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon)) return out å‡åŒ€åˆ†å¸ƒ[a,b]çš„æ–¹å·®ï¼š$\\dfrac{(b-a)^2}{12}$ å‚è€ƒèµ„æ–™ï¼š Understanding the difficulty of training deep feedforward neural networks cs231nï¼šWeight Initialization understanding-xavier-initialization-in-deep-neural-networks","link":"/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"title":"æœºå™¨å­¦ä¹ ä¸­çš„ä¸€äº› tricks","text":"L2æ­£åˆ™åŒ–çš„æ•°å­¦åŸç† L2æ­£åˆ™åŒ–ï¼šTo avoid parameters from exploding or becoming highly correlated, it is helpful to augment our cost function with a Gaussian prior: this tends to push parameter weights closer to zero, without constraining their direction, and often leads to classifiers with better generalization ability. If we maximize log-likelihood (as with the cross-entropy loss, above), then the Gaussian prior becomes a quadratic term 1 (L2 regularization): $$J_{reg}(\\theta)=\\dfrac{\\lambda}{2}[\\sum_{i,j}{W_1}{i,j}^2+\\sum{iâ€™jâ€™}{W_2}_{i,j}^2]$$ å¯ä»¥è¯æ˜ï¼š $$W_{ij} âˆ¼ N (0; 1=Î»)$$ ä»ä¸¤ç§è§’åº¦ç†è§£æ­£åˆ™åŒ–ï¼šçŸ¥ä¹ RNNä¸ºä»€ä¹ˆå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜reluä¸ºå•¥èƒ½æœ‰æ•ˆçš„è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜å¾ˆéš¾ç†è§£ä¸ºå•¥ç”¨reluèƒ½å¾ˆå¥½çš„è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œçš„ç¡®reluçš„æ¢¯åº¦ä¸º1ï¼Œä½†è¿™ä¹Ÿå¤ªç®€å•äº†å§ã€‚ã€‚ã€‚æ‰€ä»¥å¾—çœ‹çœ‹åŸè®ºæ–‡ A Simple Way to Initialize Recurrent Networks of Rectified Linear Units","link":"/2018/04/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%AD%A3%E5%88%99%E5%8C%96/"},{"title":"ç®—æ³•-æ’åºç®—æ³•","text":"å‰‘æŒ‡offer ç¬¬äºŒç« -é¢è¯•éœ€è¦çš„åŸºç¡€çŸ¥è¯† å†’æ³¡æ’åºéå†æ•°ç»„ï¼Œå°†æœ€å¤§çš„ç§»åˆ°æœ€åã€‚é€æ¸ç¼©å°æ•°ç»„èŒƒå›´ã€‚ å…·ä½“æ­¥éª¤ï¼š ç¬¬ä¸€ä¸ª for å¾ªç¯ P=N-1ï¼šå°†æœ€å¤§çš„æ•°æ”¾åˆ°æœ€åé¢ ç¬¬ä¸€è¶Ÿå†’æ³¡ï¼Œä»ç¬¬ä¸€ä¸ªæ•°å¼€å§‹é€ä¸ªå‘åæ¯”è¾ƒï¼Œå°†æœ€å¤§çš„æ•°å‘åç§»åŠ¨ã€‚ ç¬¬äºŒä¸ª for å¾ªç¯ P=N-2ï¼šå°†æ¬¡å¤§çš„ä¹¦æ”¾åˆ°å€’æ•°ç¬¬äºŒä¸ªä½ç½®ä¸Š ç¬¬äºŒè¶Ÿå†’æ³¡ï¼Œä»ç¬¬ä¸€ä¸ªæ•°å¼€å§‹é€ä¸ªå‘åæ¯”è¾ƒï¼Œæ¯”è¾ƒåˆ°ç¬¬äºŒä¸ªä½ç½®ä¸ºæ­¢ å¤æ‚åº¦åˆ†æï¼š æœ€å¥½æƒ…å†µ,æ•°ç»„æœ¬èº«æ˜¯é¡ºåºçš„ï¼Œé‚£ä¹ˆå¤–å¾ªç¯åªéœ€è¦æ‰§è¡Œä¸€æ¬¡å°±ä¼šå‘ç° flag æ ‡è¯†æ²¡å˜ T=O(N), æœ€å·®æƒ…å†µï¼Œæ•°ç»„æœ¬èº«æ˜¯å€’åºçš„ï¼Œé‚£ä¹ˆå¤–å¾ªç¯ä¼šæ‰§è¡Œ N-1 æ¬¡ï¼Œæ€»çš„è®¡ç®—é‡æ˜¯ (N-1 + N-2 +â€¦+ 1), åˆ™æ—¶é—´å¤æ‚åº¦æ˜¯ T=O(N^2) 12345678910111213141516171819202122232425262728293031323334353637void Bubble_sort(int num[], int N){ for ( int P=N-1; P&gt;0; P--) { int flag = 0; for (int i=0; i&lt;P; i++) { if(num[i] &gt; num[i+1]) { int temp = num[i]; num[i] = num[i+1]; num[i+1] = temp; flag = 1; } } if (flag == 0) break; // å¦‚æœflag æ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼Œè¯æ˜ä¸Šä¸€æ¬¡çš„éå†æ²¡æœ‰å‘ç”Ÿäº¤æ¢ï¼Œè¯´æ˜å‰©ä½™çš„æ•°ç»„å·²ç»æ’åºå¥½äº†ï¼Œå°±ä¸éœ€è¦ç¼©å°æ•°ç»„èŒƒå›´ï¼Œç»§ç»­æ¯”è¾ƒäº†ã€‚ }} æ’å…¥æ’åºæ’å…¥æ’åºæ˜¯å¢åŠ å¢å¤§æ¯”è¾ƒèŒƒå›´ã€‚ ä»æ•°ç»„ä¸­å…ˆå–ä¸€ä¸ªæ•°; å†å–ç¬¬äºŒä¸ªæ•°ï¼Œä¸å‰é¢çš„è¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æœæ¯”å‰é¢çš„å°ï¼Œåˆ™äº¤æ¢ä½ç½®ã€‚ å†å–ç¬¬ä¸‰ä¸ªæ•°ï¼Œä¸å‰é¢çš„è¿›è¡Œæ¯”è¾ƒï¼Œç›´åˆ°æ‰¾åˆ°æ¯”å®ƒå¤§çš„æ•°ï¼Œæ’å…¥ã€‚ ä¾æ¬¡å¾ªç¯ã€‚ã€‚ æœ€å¥½æƒ…å†µï¼Œæœ¬èº«å°±æ˜¯ä¸ªæ­£åºï¼Œå¤–å¾ªç¯è¿˜æ˜¯ä¼šæ‰§è¡Œå®Œ N-1 æ¬¡ï¼Œä½†æ˜¯å†…å¾ªç¯å¯¹åº”çš„åªéœ€è¦åªæ‰§è¡Œä¸€æ¬¡ï¼Œå°±ä¼šå‘ç°å¢åŠ çš„æ•°ä½ç½®æ­£ç¡®ã€‚æ‰€ä»¥æ€»çš„å¤æ‚åº¦è¿˜æ˜¯ O(N) æœ€å·®æƒ…å†µï¼Œæœ¬èº«æ˜¯ä¸ªå€’åºçš„, å¤–å¾ªç¯ä¼šæ‰§è¡Œ N-1æ¬¡ï¼Œ å†…å¾ªç¯å¯¹åº”æ¬¡æ•°ï¼Œæ‰€ä»¥æ€»çš„æ˜¯ O(N^2). 123456789101112131415161718192021222324252627282930313233343536373839void InsertOrder(int num[], int N){ for(int i=1; i&lt;N; i++) { for(int j=i; j&gt;0; j--) { int flag = 0; if (num[j] &lt; num[j-1]) { int temp = num[j]; num[j] = num[j-1]; num[j-1] = temp; flag = 1; } if(flag == 0) break; // è¿™é‡Œçš„ flag æ˜¯æ”¾åœ¨ç¬¬äºŒä¸ªå¾ªç¯é‡Œé¢çš„ï¼Œå› ä¸ºæ‰¾åˆ°äº†ä½ç½®ï¼Œå°±åœä¸‹æ¥äº†ï¼Œåé¢çš„æ’å¥½åºçš„è‚¯å®šæ¯”ä»–å¤§ï¼Œå°±ä¸ç”¨æ¯”è¾ƒäº† } }} å¯¹æ¯”å†’æ³¡æ’åºå’Œæ’å…¥æ’åº å†’æ³¡æ’åºæ˜¯ä»æ•°ç»„æœ«å°¾åˆ°å¤´é€æ¸æœ‰åºï¼Œæ’å…¥æ’åºæ˜¯ä»å¤´åˆ°å°¾é€æ¸æœ‰åºã€‚ flagçš„ä½ç½®ä¸åŒã€‚å†’æ³¡æ’åºçš„å¤–å¾ªç¯ä¸ä¸€å®šèƒ½æ‰§è¡Œå®Œï¼Œä½†å†…å¾ªç¯éƒ½ä¼šæ‰§è¡Œå¯¹åº”çš„æ¬¡æ•°ã€‚æ’å…¥æ’åºå¤–å¾ªç¯ä¸€å®šä¼šæ‰§è¡Œå®Œï¼Œä½†å†…å¾ªç¯ä¸ä¸€å®šã€‚ å †æ’åºé€šè¿‡é€‰æ‹©æ’åºå¼•å…¥å †æ’åº é€‰æ‹©æ’åºï¼šéå†é€‰æ‹©æœ€å°çš„å…ƒç´ ï¼Œæ”¾å…¥æ•°ç»„é‡Œé¢ã€‚ç„¶ååœ¨æ— åºåºåˆ—é‡Œé¢å†éå†é€‰æ‹©æœ€å°å…ƒç´ ï¼Œå¾ªç¯ã€‚ã€‚ã€‚å¯è§æ—¶é—´å¤æ‚åº¦æœ€åæƒ…å†µï¼šO(N2) ä¸Šå›¾å¯ä»¥çœ‹å‡º for å¾ªç¯è¦æ‰§è¡Œ N æ¬¡ï¼Œæ€»çš„å¤æ‚åº¦å–å†³äºå¯»æ‰¾æœ€å°å…ƒã€‚çœ‹åˆ°æœ€å°å…ƒï¼Œè‡ªç„¶æƒ³åˆ°æœ€å°å †ã€‚æœ€å°å †çš„æ ¹èŠ‚ç‚¹å°±æ˜¯æœ€å°å€¼ã€‚ å †æ’åºç®—æ³•1ï¼šé’ˆå¯¹é€‰æ‹©æ’åºçš„æ”¹è¿›ï¼Œæ€ä¹ˆæ ·æ‰¾åˆ°æœ€å°å…ƒï¼Ÿå¯ä»¥å°†åŸåºåˆ—è°ƒæ•´æˆæœ€å°å †(æ—¶é—´å¤æ‚åº¦æ˜¯çº¿æ€§çš„ O(N))ï¼Œç„¶åä¾æ¬¡å¼¹å‡ºæ ¹ç»“ç‚¹ O(NlogN)ã€‚å†å°†å¼¹å‡ºçš„åºåˆ—å¤åˆ¶åˆ°åŸåºåˆ—é‡Œé¢ O(N)ã€‚å¯è§æ—¶é—´å¤æ‚åº¦ä¸ºï¼šT(N)=O(N)+O(Nlog(N))+O(N)=O(Nlog(N)) ä½†æ˜¯åŸæ•°ç»„å æœ‰O(N)å†…å­˜ï¼Œè°ƒæ•´æˆæœ€å°å †åå¼¹å‡ºçš„åºåˆ—å­˜å‚¨ä¹Ÿéœ€è¦O(N)å†…å­˜ï¼Œå› æ­¤ï¼Œç©ºé—´å¤æ‚åº¦ä¸Šéœ€è¦é¢å¤–çš„O(N) å †æ’åºç®—æ³•2å…ˆè°ƒæ•´æˆæœ€å¤§å †O(N),å†å°†æ ¹ç»“ç‚¹ä¸æœ€åä¸€ä¸ªå…ƒç´ äº¤æ¢ä½ç½®O(logN)ï¼Œç„¶åå†å°†å‰N-1ä¸ªå…ƒç´ è°ƒæ•´æˆæœ€å¤§å †ï¼Œä¾æ¬¡å¾ªç¯ã€‚ã€‚æœ€åç›´æ¥å°±æ˜¯æ’åºçš„ç»“æœï¼Œè€Œä¸ç”¨é‡æ–°å ç”¨å†…å­˜ç©ºé—´ã€‚æ—¶é—´å¤æ‚åº¦ï¼šO(N)+O(Nlog(N))â€¦ä½†é™ˆå§¥å§¥ç»™çš„æ˜¯ä¸‹é¢è¿™æ ·â€¦soï¼Ÿ ä»£ç å®ç°123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//å †æ’åº/*å› ä¸ºå †æ’åºæ˜¯å®Œå…¨äºŒå‰æ ‘ï¼Œç”¨æ•°ç»„è¡¨ç¤ºçš„ï¼Œæ‰€ä»¥å¯ä»¥æŠŠåŸå§‹åºåˆ—çœ‹æˆæ ‘ï¼Œç„¶åè¿›è¡Œè°ƒæ•´*/void HeapSort(int num[], int N) { /*build max heap*/ int i,temp; for (i = 1; i &lt; N; i++) { //ä»æ•°ç»„ä¸‹æ ‡ä¸º1çš„å…ƒç´ å¼€å§‹ï¼Œç±»ä¼¼äºæœ€å¤§å †çš„æ’å…¥ä¸€æ ·ï¼Œä¾æ¬¡ä¸çˆ¶èŠ‚ç‚¹æ¯”è¾ƒ temp = num[i];// å› ä¸ºiä¸€ç›´åœ¨å˜åŒ–ï¼Œå› æ­¤æŠŠä»–ä¿å­˜ for (; num[i] &gt; num[(i-1)/2]; i = (i - 1) / 2) num[i] = num[(i - 1) / 2]; //æŠŠæ¯”num[i]å°çš„çˆ¶èŠ‚ç‚¹å¾€ä¸‹ä¸€å±‚ç§» num[i] = temp; } /*delete max and swaps the max with the last one of the unoder sequence*/ int j,k, tempp; for (j = N - 1; j &gt; 0; j--) { tempp = num[0]; num[0] = num[j]; num[j] = tempp; for (k = 1; k &lt; j; k++) { //ä»æ•°ç»„ä¸‹æ ‡ä¸º1çš„å…ƒç´ å¼€å§‹ï¼Œç±»ä¼¼äºæœ€å¤§å †çš„æ’å…¥ä¸€æ ·ï¼Œä¾æ¬¡ä¸çˆ¶èŠ‚ç‚¹æ¯”è¾ƒ tempp = num[k];// å› ä¸ºiä¸€ç›´åœ¨å˜åŒ–ï¼Œå› æ­¤æŠŠä»–ä¿å­˜ for (; num[k] &gt; num[(k - 1) / 2]; k = (k - 1) / 2) num[k] = num[(k - 1) / 2]; //æŠŠæ¯”num[i]å°çš„çˆ¶èŠ‚ç‚¹å¾€ä¸‹ä¸€å±‚ç§» num[k] = tempp; } }} å¿«é€Ÿæ’åºç®—æ³•æ¦‚è¿°ï¼šåˆ†è€Œæ²»ä¹‹ï¼ˆé€’å½’ï¼‰ å¿«é€Ÿæ’åºçš„ç»†èŠ‚å¾ˆé‡è¦ï¼š é€‰ä¸»å…ƒï¼šä¸­åˆ†æ˜¾ç„¶æ˜¯æœ€å¥½çš„ æ€ä¹ˆåˆ†æˆä¸¤ä¸ªç‹¬ç«‹å­é›† æœ€å¥½æƒ…å†µï¼š æ¯æ¬¡é€‰æ‹©çš„ä¸»å…ƒä½ç½®æ­£å¥½åœ¨ä¸­åˆ†ä½ç½®ï¼Œ T(N) = O(NlogN) é€‰ä¸»å…ƒ éšæœºé€‰ä¸»å…ƒï¼šéšæœºå‡½æ•°ä¹Ÿå¾ˆæµªè´¹æ—¶é—´ã€‚ã€‚ é€‰å¤´ä¸­å°¾ä¸‰ä¸ªæ•°çš„ä¸­ä½æ•° è¿™é‡Œä¸»å…ƒæ˜¯ 12345678910111213141516171819202122232425262728293031// ä¸»å…ƒæ‰€åœ¨çš„ä½ç½®int Center = (Left + Right) / 2;// æ¯”è¾ƒ å·¦ä¸­å³ä¸‰ä¸ªæ•°ï¼Œå¹¶æ’åºif (A[Left] &gt; A[Center]) Swap (&amp;A[Left], &amp;A[Center]);if (A[Left] &gt; A[Right]) Swap (&amp;A[Left], &amp;A[Right]);if (A[Center] &gt; A[Right]) Swap (&amp;A[Center], &amp;A[Right]);// å†è€ä¸ªå°èªæ˜ï¼Œ å°† pivot è—åœ¨æœ€å³è¾¹Swap (&amp;A[Center], &amp;A[Right-1]);// åªéœ€è€ƒè™‘ A[Left+1]...A[Right-2]return A[Right-1]; // è¿”å› pivot å­é›†åˆ’åˆ†è°ƒç”¨è¿‡ä¸Šé¢çš„ Median3 å‡½æ•°ä¹‹åå¾—åˆ°çš„æ•°ç»„æ˜¯ï¼š {8, 1, 4, 9, 0, 3, 5, 2, 7, 6}ï¼Œ Left å’Œ Right ä¸ç”¨è€ƒè™‘äº†ï¼Œå·²ç»åœ¨æ­£ç¡®çš„å­é›†äº†ï¼Œpivot ä¹Ÿæ”¾åœ¨äº†å‰©ä¸‹éƒ¨åˆ†çš„æœ€å³è¾¹ã€‚ è¿™é‡Œ i j æ˜¯ä¸¤ä¸ªæŒ‡é’ˆï¼Œå…¶å®å°±æ˜¯å…ƒç´ ä¸‹æ ‡ã€‚å½“ä¸¤è¾¹æŒ‡é’ˆié‡åˆ°æ¯”pivotå¤§å¼ï¼Œåœæ­¢ã€‚è½¬å»è€ƒè™‘å³è¾¹çš„æŒ‡é’ˆjï¼Œå½“jé‡åˆ°å°äºpivotçš„æ•°æ—¶ä¹Ÿåœæ­¢ã€‚æ­¤æ—¶ä¸¤è¾¹éƒ½å‘å‡ºçº¢è‰²è­¦å‘Šï¼ˆä¹Ÿå°±æ˜¯å·¦è¾¹å¤§äºpivotï¼Œ å³è¾¹å°äºpivotï¼‰ï¼Œåˆ™äº¤æ¢ä¸¤è¾¹çš„æ•°ã€‚ é€’å½’ï¼šå¯¹å·¦å­é›†é‡å¤ä¸Šé¢çš„æ“ä½œï¼Œå¯¹å³å­é›†é‡å¤ä¸Šé¢çš„æ“ä½œã€‚ å¿«é€Ÿæ’åºä¸ºä»€ä¹ˆå¿«ï¼Ÿ å› ä¸ºé€‰æ‹©äº†ä¸€ä¸ªä¸»å…ƒï¼Œç„¶ååˆ’åˆ†å®Œå­é›†åï¼Œè¿™ä¸ªä¸»å…ƒå°±å¤„äºå®ƒæœ€ç»ˆæ‰€åœ¨çš„ä½ç½®ä¸Šã€‚ æ¯”å¦‚æ’å…¥æ’åºçš„æ•°çš„ä½ç½®éƒ½æ˜¯ä¸´æ—¶çš„ï¼Œæ’å…¥æ’åºæ–°å¢åŠ ä¸€ä¸ªæ•°ï¼Œä»åå¾€å‰é€æ¸å¯¹æ¯”ï¼Œå‘ç°æ¯”å‰é¢çš„å°çš„è¯ï¼Œç›¸å½“äºæŠŠå‰é¢çš„æ•°éƒ½å¾€åæŒªã€‚ å¦‚æœæœ‰ä¸¤å…ƒç´ æ­£å¥½ç­‰äº pivot æ€ä¹ˆåŠï¼Ÿ åœä¸‹æ¥äº¤æ¢ï¼Œè¿™æ ·çš„ç›®çš„æ˜¯è®© pivot æœ€ç»ˆçš„ä½ç½®æ›´é è¿‘æ•°ç»„ä¸­åˆ†çš„ä½ç½®ã€‚ä¹‹å‰åšè¿‡æ—¶é—´å¤æ‚åº¦åˆ†ææ—¶ï¼Œæˆ‘ä»¬çŸ¥é“è¿™æ ·çš„æ—¶é—´å¤æ‚åº¦æ˜¯æœ€å°çš„ã€‚ å°è§„æ¨¡æ•°æ®çš„å¤„ç† å½“é€’å½’çš„æ•°æ®è§„æ¨¡è¾ƒå°æ—¶ï¼Œåˆ™åœæ­¢é€’å½’ï¼Œä½¿ç”¨æ’å…¥æ’åºã€‚ ä»£ç å®ç°123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165#include&lt;stdio.h&gt;#include&lt;malloc.h&gt;#include&lt;stdbool.h&gt;void InsertSort(int num[], int N) { int i, j, temp; for (i = 1; i &lt; N; i++) { for (j = i; j &gt; 0; j--) { int flag = 0; if (num[j] &lt; num[j - 1]) { temp = num[j]; num[j] = num[j - 1]; num[j - 1] = temp; flag = 1; } if (flag == 0) break; } }}/*é€‰ä¸»å…ƒ*/void swap(int num[],int m, int n) { int temp; temp = num[m]; num[m] = num[n]; num[n] = temp;;}int Median3(int num[], int left,int right) { int median = (left + right) / 2; if (num[left] &gt; num[median]) swap(num,left,median); if (num[left] &gt; num[right]) swap(num, left, right); if (num[median] &gt; num[right]) swap(num, right, median); swap(num,median,right-1); return num[right - 1];}void Quick_sort(int num[],int left,int right) { int Cutoff = 10; if (Cutoff &lt;= right - left) { int pivot; pivot = Median3(num, left, right); int i = left, j = right - 1; for (;;) { while (num[i++]&lt;pivot) {} while (num[j--]&gt;pivot) {} if (i &lt; j) swap(num,i,j); else break; } swap(num,i,right-1); Quick_sort(num, left, i - 1); Quick_sort(num, i + 1, right); } else InsertSort(num + left, right - left + 1); //num+leftè¿™ç§å†™æ³•ï¼Ÿï¼Ÿ}void QuickSort(int num[], int N){ Quick_sort(num, 0, N - 1);}int main() { int N, i, j; scanf_s(&quot;%d&quot;, &amp;N); int *num = (int*)malloc(N * sizeof(int)); for (i = 0; i &lt; N; i++) { scanf_s(&quot;%d&quot;, &amp;num[i]); } QuickSort(num, N); //int pivot = Median3(num, 0, N - 1); //printf(&quot;%d &quot;, pivot); printf(&quot;%d&quot;, num[0]); for (j = 1; j &lt; N; j++) printf(&quot; %d&quot;, num[j]);} åœ¨æ­¤è¿‡ç¨‹ä¸­å‡ºç°çš„é—®é¢˜ï¼š åœ¨å¿«é€Ÿæ’åºçš„Cä»£ç å‡ºç°äº†â€œæ®µé”™è¯¯â€Segmentation fault (core dumped) é”™è¯¯ä»£ç ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041void Quick_sort(int num[],int left,int right) { int Cutoff = 0; if (Cutoff &lt;= right - left) { int pivot; pivot = Median3(num, left, right); int i = left, j = right - 1; for (;;) { while (num[++i]&lt;pivot) {} while (num[--j]&gt;pivot) {} if (i &lt; j) swap(num,i,j); else break; } swap(num,i,right-1); Quick_sort(num, left, i - 1); Quick_sort(num, i + 1, right); } else InsertSort(num + left, right - left + 1); //num+leftè¿™ç§å†™æ³•ï¼Ÿï¼Ÿ} å‡ºç°bugï¼Œåœ¨Linuxç³»ç»Ÿä¸‹ä½¿ç”¨gdbè¿›è¡Œè°ƒè¯•ï¼š åˆ†æå‡ºç°bugåŸå› ï¼š å¿«é€Ÿæ’åºï¼Œé€’å½’çš„æœ€åä¼šå‡ºç° left = right-1 = 0,æ­¤æ—¶num[]ä¸­åªæœ‰ä¸€ä¸ªå…ƒç´ num[0],å› æ­¤è¿›å…¥while (num[++i]&lt;pivot)å¾ªç¯æ—¶ï¼Œ++iä¼šå¯¼è‡´å†…å­˜è¶Šç•Œã€‚ è§£å†³åŠæ³•æ˜¯cutoff&gt;0å°±å¥½äº†ï¼Œå½“å…ƒç´ è¾ƒå°‘çš„æ—¶å€™é‡‡ç”¨æ’å…¥æ’åº~","link":"/2018/07/06/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"title":"è®ºæ–‡ç¬”è®°-Denoising Diffusion Probabilistic Models","text":"What are Diffusion Models? (lilianweng.github.io) Yang Song | Generative Modeling by Estimating Gradients of the Data Distribution (yang-song.github.io) Diffusion Models Beat GANs on Image Synthesis (2105.05233.pdf (arxiv.org)) Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes 2111.12701.pdf (arxiv.org) Cascaded Diffusion Models for High Fidelity Image Generation cascaded_diffusion.pdf (cascaded-diffusion.github.io) DDPM 1: https://arxiv.org/pdf/1503.03585.pdf DDPM 2: https://arxiv.org/pdf/2006.11239.pdf WaveNet: https://arxiv.org/pdf/1609.03499.pdf DiffWave: https://arxiv.org/pdf/2009.09761.pdf How to Train Your Energy-Based Models https://arxiv.org/pdf/1906.02691.pdfï¼› Generating Diverse High-Fidelity Images with VQ-VAE-2 https://arxiv.org/pdf/1906.00446.pdfï¼› PIXELCNN++: Improving The PlxelcnnI With Discretized Logistic Mixture Likelihood And Other Modifications https://openreview.net/pdf?id=BJrFC6cegï¼› U-Net: Convolutional Networks for Biomedical Image Segmentation https://arxiv.org/pdf/1505.04597.pdfï¼› On Maximum Likelihood Training of Score-Based Generative Models https://arxiv.org/pdf/2101.09258.pdfï¼› Flow-based Deep Generative Models https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.htmlï¼› From Autoencoder to Beta-VAE https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.htmlï¼› WaveNet: A generative model for raw audio https://deepmind.com/blog/artic","link":"/2021/11/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Denoising-Diffusion-Probabilistic-Models/"},{"title":"è®ºæ–‡ç¬”è®°-Discrete Latent Variables Based Generation","text":"VQ-VAE: Neural Discrete Representation Learning (NIPS2017) VQ-VAE2: Generating Diverse High-Resolution Images with VQ-VAE-2 DALL-E: Zero-Shot Text-to-Image Generation VideoGPT: Video Generation using VQ-VAE and Transformers LVT: Latent Video Transformer Feature Quantization Improves GAN Training (ICML2020) DVT-NAT: Fast Decoding in Sequence Models Using Discrete Latent Variables (ICML2018) NWT: Towards natural audio-to-video generation with representation learning NUWA: Visual Synthesis Pre-training for Neural visUal World creAtion VQ-VAEåœ¨è®¤è¯†VQ-VAEä¹‹å‰ï¼Œå…ˆå›é¡¾ä¸€ä¸‹AEã€VAEã€‚å¾ˆæ—©ä»¥å‰çœ‹æå®æ¯…è€å¸ˆçš„è§†é¢‘å­¦è¿‡ä¸€æ¬¡ï¼Œè¿™é‡Œå°±ç›´æ¥ä½¿ç”¨ä¹‹å‰æ•´ç†çš„PPTç¬”è®°äº†. ä»¥åŠå¯¹åº”çš„ä»£ç å¯ä»¥çœ‹è¿™é‡Œ (vae.ipynb - Colaboratory (google.com)) Auto-Encoderæˆ‘ä»¬åœ¨é‡æ„ä¸€ä¸ªå›¾åƒæ—¶ï¼Œé€šå¸¸è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯å›¾åƒæœ¬èº«ã€‚ä¸ºäº†ä¿è¯ç¥ç»ç½‘ç»œä¸æ˜¯ç›´æ¥çš„copyï¼Œå¾ˆè‡ªç„¶ä¼šæƒ³åˆ°è¿™ç§é™ç»´å†å‡ç»´çš„æ–¹å¼ã€‚è¿™é‡Œçš„latent vector/codings å°±æ˜¯æˆ‘ä»¬å¸Œæœ›å­¦åˆ°çš„ä¸€ä¸ªåŸå›¾åƒå‹ç¼©åçš„ä½ç»´ç‰¹å¾è¡¨ç¤ºã€‚ æ¨¡å‹ä»£ç å¦‚ä¸‹ï¼š 12345678910111213141516171819202122232425262728293031class Encoder(nn.Module): def __init__(self, latent_dims): super(Encoder, self).__init__() self.linear1 = nn.Linear(784, 512) self.linear2 = nn.Linear(512, latent_dims) def forward(self, x): z = torch.flatten(x, start_dim=1) z = F.relu(self.linear1(z)) return self.linear2(z)class Decoder(nn.Module): def __init__(self, latent_dims): super(Decoder, self).__init__() self.linear1 = nn.Linear(latent_dims, 512) self.linear2 = nn.Linear(512, 784) def forward(self, z): x_hat = F.relu(self.linear1(z)) x_hat = torch.sigmoid(self.linear2(x_hat)) return x_hat.reshape((-1, 1, 28, 28)) class Autoencoder(nn.Module): def __init__(self, latent_dims): super(Autoencoder, self).__init__() self.encoder = Encoder(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z = self.encoder(x) return self.decoder(z) çœ‹æºä»£ç å¯ä»¥ç›´è§‚çš„å‘ç°ï¼Œå¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬å­¦åˆ°äº†å¯¹åº”çš„å‹ç¼©åçš„ç‰¹å¾è¡¨ç¤ºçš„ç»´åº¦æ˜¯ latent_dims=2.ï¼ˆæ½œç©ºé—´çš„ç»´åº¦å½“ç„¶ä¹Ÿå¯ä»¥æ›´å¤§ï¼Œè¿™é‡Œä¸ºäº†å¯è§†åŒ–æ›´æ–¹ä¾¿ï¼Œæˆ‘ä»¬è®¾ç½®ä¸º2ï¼‰ã€‚ å¯è§†åŒ–ä»£ç ï¼š 12345678def plot_latent(autoencoder, data, num_batches=100): for i, (x, y) in enumerate(data): z = autoencoder.encoder(x.to(device)) z = z.to('cpu').detach().numpy() plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10') if i &gt; num_batches: plt.colorbar() break å°†å¾—åˆ°çš„è¿™äº›2 ç»´çš„ latent vectorså¯è§†åŒ–å¦‚ä¸‹å›¾ï¼š é€šè¿‡ä¸Šå›¾æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œæˆ‘ä»¬è®­ç»ƒçš„æ¯ä¸€ä¸ªæ ·æœ¬å¯¹åº”ç€æ½œç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œè¿™äº›ç‚¹æ˜¯ç¦»æ•£çš„ã€‚è¿™æ„å‘³ç€ï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½ç”Ÿæˆæ–°çš„å›¾åƒï¼Œè€Œåªèƒ½â€œå¤åˆ¶â€åŸæœ‰çš„å›¾åƒï¼ˆä¹Ÿå¯ä»¥è¯´ï¼Œå¦‚æœåœ¨æ½œç©ºé—´é‡‡æ ·å¾—åˆ°çš„ç‚¹åœ¨è¿™äº›ç¦»æ•£çš„ç‚¹ä¹‹é—´/å¤–æ—¶ï¼Œç”Ÿæˆçš„æ ·æœ¬ä¼šæ¯”è¾ƒæ¨¡ç³Šï¼‰ã€‚ä½†ä¹Ÿæœ‰æœ‰è¶£çš„å‘ç°ï¼Œé™ç»´ç¡®å®æ˜¯èµ·åˆ°äº†ä¸€ä¸ªèšç±»çš„æ•ˆæœï¼Œåªæ˜¯æ•ˆæœä¸€èˆ¬å§ï¼Œç±»ä¸ç±»ä¹‹é—´å¹¶æ²¡æœ‰å®Œå…¨çš„åˆ†å¼€ã€‚è¿™ä¸ªåº”è¯¥æ˜¯å¯ä»¥äººä¸ºæ§åˆ¶çš„ã€‚ æˆ‘ä»¬å¯ä»¥è¯•ç€å»ç”Ÿæˆä¸€äº›æ½œç©ºé—´çš„vectorï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šå»é‡æ„ï¼Œçœ‹çœ‹ä¼šå‘ç°ä»€ä¹ˆï¼Ÿ é‡æ„ä»£ç ï¼š 12345678910def plot_reconstructed(autoencoder, r0=(-10, 5), r1=(-5, 10), n=12): w = 28 img = np.zeros((n*w, n*w)) for i, y in enumerate(np.linspace(*r1, n)): for j, x in enumerate(np.linspace(*r0, n)): z = torch.Tensor([[x, y]]).to(device) x_hat = autoencoder.decoder(z) x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy() img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat plt.imshow(img, extent=[*r0, *r1]) ä»£ç ä¸­æˆ‘ä»¬é€‰å–çš„æ½œç©ºé—´çš„èŒƒå›´æ˜¯ $x\\in [-5, 10]$ , $y\\in [10, 5]$ ,åœ¨ç»“åˆå¯è§†åŒ–çš„å›¾ï¼Œå¯ä»¥çœ‹åˆ°ä¸»è¦æ˜¯é›†ä¸­åœ¨æ•°å­— â€œ0â€ çš„åŒºåŸŸã€‚å³ä¸Šè§’æ˜¯ (10, 5) å¯ä»¥ä»å¯è§†åŒ–çš„å›¾ä¸­ä¹Ÿèƒ½çœ‹åˆ°ï¼Œæœ‰éƒ¨åˆ†æ¥è¿‘â€œ1â€çš„åŒºåŸŸï¼Œä½†ç”Ÿæˆçš„æ ·æœ¬å¾ˆæ¨¡ç³Šã€‚ è‡ªç¼–ç é™¤äº†æ½œç©ºé—´æ˜¯ç¦»æ•£çš„ï¼Œè¿˜æœ‰ä¸ªç¼ºç‚¹ï¼Œå°±æ˜¯å½“ç¥ç»ç½‘ç»œå¤ªå¼ºæ—¶ï¼Œä¼šoverfittingã€‚å»å™ªè‡ªç¼–ç å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ VAEåœ¨å‰é¢æåˆ°AEçš„ç¼ºç‚¹æ˜¯ï¼Œå…¶å¾—åˆ°çš„æ½œç©ºé—´æ˜¯éè¿ç»­çš„ï¼Œå¯¼è‡´é‡‡æ ·å‘ç”Ÿåœ¨ç¦»æ•£ç‚¹ä¹‹é—´/å¤–æ—¶ï¼Œä¼šç”Ÿæˆæ¯”è¾ƒæ¨¡ç³Šçš„å›¾ç‰‡ã€‚è€ŒVAEå°±æ˜¯æˆ‘ä»¬å‡è®¾å…ˆéªŒ $p(z)$ çš„æ¯ä¸ªç»´åº¦å°±æ˜¯ä¸€ä¸ªè¿ç»­çš„distribution. AEæ˜¯ç”¨æ½œç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹(fixed vector)æ¥å¯¹åº”ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼ŒVAEåˆ™æ˜¯ç”¨ä¸€ä¸ªè¿ç»­çš„åˆ†å¸ƒæ¥å¯¹åº”ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ã€‚é€šè¿‡ä»£ç æ¥ç†è§£è¿™å¥è¯ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940class VariationalEncoder(nn.Module): def __init__(self, latent_dims): super(VariationalEncoder, self).__init__() self.linear1 = nn.Linear(784, 512) self.linear2 = nn.Linear(512, latent_dims) self.linear3 = nn.Linear(512, latent_dims) self.kl = 0 def forward(self, x): x = torch.flatten(x, start_dim=1) x = F.relu(self.linear1(x)) mu = self.linear2(x) sigma = torch.exp(self.linear3(x)) z = mu + sigma*torch.randn_like(sigma) self.kl = 0.5*(sigma**2 + mu**2 - torch.log(sigma) - 1).sum() return zclass Decoder(nn.Module): def __init__(self, latent_dims): super(Decoder, self).__init__() self.linear1 = nn.Linear(latent_dims, 512) self.linear2 = nn.Linear(512, 784) def forward(self, z): x_hat = F.relu(self.linear1(z)) x_hat = torch.sigmoid(self.linear2(x_hat)) return x_hat.reshape((-1, 1, 28, 28)) class VariationalAutoencoder(nn.Module): def __init__(self, latent_dims): super(VariationalAutoencoder, self).__init__() self.encoder = VariationalEncoder(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z = self.encoder(x) return self.decoder(z) ç¬¬16è¡Œä»£ç  z = mu + sigma*torch.randn_like(sigma) ä¸­å¾—åˆ°çš„æ˜¯æ½œç©ºé—´çš„å¤šç»´ç‰¹å¾è¡¨ç¤º z, æ¯ä¸ªç»´åº¦éƒ½æ˜¯ä¸€ä¸ªæ­£æ€åˆ†å¸ƒ(å‡å€¼ä¸º mu, æ–¹å·®ä¸º sigma). ç¬¬17è¡Œä»£ç çš„klæ•£åº¦çœ‹ç€å¾ˆç–‘æƒ‘ï¼Œè¿™é‡Œéœ€è¦æ¨å¯¼å¾—åˆ°ï¼Œå…ˆæš‚æ”¾ä¸€è¾¹ã€‚ å…ˆéªŒ $p_{\\theta}(z)$ ä¼¼ç„¶ $p_{\\theta}(x|z)$ åéªŒ $p_{\\theta}(z|x)$ å³å›¾ä¸­å®çº¿æ—¶ç”Ÿæˆæ¨¡å‹ï¼ˆå…ˆéªŒ*ä¼¼ç„¶ï¼‰ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡å¯ä»¥æœ‰ä¸¤ç§é€‰æ‹©ï¼Œä¸€ä¸ªæ˜¯æœ€å¤§åŒ–ä¼¼ç„¶æ¦‚ç‡(MLH)ï¼Œå¦ä¸€ä¸ªæ˜¯æœ€å¤§åŒ–åéªŒæ¦‚ç‡(MAP). ä½†æ˜¯å¯¹äº $p(x|z)p(z)$ è¿™ä¸ªçš„ä¼˜åŒ–æ˜¯å¾ˆéš¾çš„ï¼Œæˆ‘ä»¬ä¸å¯èƒ½å»é‡‡æ ·æ‰€æœ‰çš„ $p(z)$ã€‚ äºæ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨å˜åˆ†è¿‘ä¼¼çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬é€šè¿‡å¦ä¸€ä¸ªåˆ¤åˆ«æ¨¡å‹æ¥ä»£æ›¿è¿™ä¸ªåéªŒæ¦‚ç‡ã€‚ä¹Ÿå°±æ˜¯ $q_{\\phi}(z|x)\\sim p_{\\theta}(z|x)$ . è¿™ä¸ªåˆ¤åˆ«æ¨¡å‹å°±æ˜¯ä¸‹å›¾ä¸­çš„encoderã€‚ å¦‚ä»£ç ä¸­13-14è¡Œæ˜¾ç¤ºé‚£æ ·ï¼Œæˆ‘ä»¬é€šè¿‡encoderç›´æ¥ç”Ÿæˆæ¯ä¸€ä¸ªç»´åº¦çš„å‡å€¼å’Œæ–¹å·®ï¼Œå¾—åˆ°ä¸€ä¸ªå¤šç»´çš„æ­£æ€åˆ†å¸ƒï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šå»è§£ç ã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬å†å›è¿‡å¤´æ¥è¯´è¯´ç¬¬17è¡Œä»£ç çš„klæ•£åº¦æ˜¯å’‹å›äº‹ã€‚æˆ‘ä»¬äº†è§£çš„klæ•£åº¦é€šå¸¸ä½œä¸ºä¸€ä¸ªlossæ¥è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒæ˜¯å¦æ¥è¿‘ï¼Œé‚£è¿™é‡Œæˆ‘ä»¬ä¹Ÿç¡®å®æœ‰ä¸¤ä¸ªåˆ†å¸ƒéœ€è¦æ‹‰è¿‘ï¼Œå°±æ˜¯ä¸‹å¼ä¸­çš„ç¬¬ä¸€é¡¹ï¼Œå…ˆéªŒå’ŒåéªŒæˆ‘ä»¬éœ€è¦å°½é‡ä¸€è‡´ï¼š åéªŒ$q_{\\phi}(z|x)$å°±æ˜¯æˆ‘ä»¬çš„encoderï¼Œå…ˆéªŒ$p_{\\theta}(z|x)$ å°±æ˜¯æˆ‘ä»¬çš„å…ˆéªŒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ç¬¬äºŒé¡¹æå¤§ä¼¼ç„¶ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è§‚æµ‹çš„æ ·æœ¬çš„æ¦‚ç‡ï¼‰ã€‚ å¯¹äºå¦‚ä½•é€šè¿‡æå¤§ä¼¼ç„¶æ¨å¯¼å¾—åˆ°æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°ï¼Œæå®æ¯…è€å¸ˆPPTä¸­è®²çš„æˆ‘è§‰å¾—æ²¡æœ‰ä¸‹é¢è¿™ä¸ªæ¸…æ¥šã€‚è¿™ä¸ªå°±å¾ˆç›´è§‚äº†ã€‚ å‡è®¾ä¹‹å‰çš„æˆ‘ä»¬æ‡‚äº†ï¼Œæˆ‘ä»¬æ€»å½’æ˜¯å¾—åˆ°äº†è¿™æ ·çš„ä¸€ä¸ªç›®æ ‡æŸå¤±å‡½æ•°ã€‚ä¸€ä¸ªæ˜¯é‡æ„lossï¼Œä¸€ä¸ªæ˜¯klæ•£åº¦ã€‚ å¯¹äºç¬¬ä¸€é¡¹klæ•£åº¦ï¼Œæˆ‘ä»¬å‡è®¾å…ˆéªŒ $p(z)$ æ˜¯æ­£æ€åˆ†å¸ƒï¼Œé€šè¿‡æ¨å¯¼å¯ä»¥å¾—åˆ°ï¼š è¿™æ ·å°±è·Ÿç¬¬17è¡Œä¸­çš„ä»£ç ä¸€æ ·äº†ã€‚ä¹Ÿå°±æ˜¯ lower boundçš„ç¬¬ä¸€é¡¹ã€‚ å¯¹äºç¬¬äºŒé¡¹é‡æ„æŸå¤±ï¼š è¿™é‡Œç”¨åˆ°çš„å°±æ˜¯ä¸€ç§åœ¨å‚æ•°æŠ€å·§ã€‚ç›´æ¥ç”Ÿæˆå‡å€¼å’Œæ–¹å·®ï¼ŒæŠŠåéªŒåˆ†å¸ƒè¿™ä¸ªè½¬æ¢æˆä¸€ä¸ªå¤šç»´çš„æ­£æ€åˆ†å¸ƒã€‚ è‡³æ­¤ï¼Œlossä¸­çš„ä¸¤é¡¹å°±è®²å®Œäº†ã€‚è®­ç»ƒä»£ç ï¼š 12345678910111213141516def train(model, data, epochs=20): opt = torch.optim.Adam(model.parameters()) for epoch in range(epochs): print(f&quot;epoch: {epoch+1}/{epochs}&quot;) for x, y in data: x = x.to(device) # GPU opt.zero_grad() x_hat = model(x) loss = ((x - x_hat)**2).sum() + model.encoder.kl loss.backward() opt.step() return model latent_dims = 2vae = VariationalAutoencoder(latent_dims).to(device) # GPUvae = train(vae, data) åŒæ ·ä¸ºäº†å¯è§†åŒ–æ–¹ä¾¿ï¼Œæˆ‘ä»¬æŠŠlatent_dims=2. å¯è§†åŒ–ä»£ç ä¸ä¹‹å‰ä¸€è‡´ï¼š è¾¹ç¼˜çš„ä½ç½®è¿˜æ˜¯çœ‹èµ·æ¥å¹¶ä¸è¿ç»­ï¼Œå¯èƒ½æ˜¯å¯è§†åŒ–æ ·æœ¬æ¯”è¾ƒå°‘ã€‚ é‡æ„ä»£ç ä¸ä¹‹å‰ä¸€è‡´ï¼š æ•ˆæœç¡®å®å¥½äº†ä¸€äº›ã€‚ã€‚é‚£ä¹ˆVAEçš„â€œåéªŒå´©å¡Œâ€åœ¨è¿™ä¸ªå›¾é‡Œæœ‰ä½“ç°å—ï¼Œå¥½åƒç¡®å®æ›´å€¾å‘äºç”Ÿæˆâ€œ1â€å’Œâ€œ7â€ ï¼Ÿ VQ-VAEVAEç”Ÿæˆçš„æ˜¯è¿ç»­çš„æ½œç©ºé—´ï¼ŒAEç”Ÿæˆçš„æ˜¯ç¦»æ•£çš„æ½œç©ºé—´ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œè¿™ä¸æ˜¯å›åˆ°åŸå§‹AEäº†å—ã€‚ç¡®å®ï¼ŒVQ-VAEæœ¬è´¨ä¸Šæ›´åƒAEï¼Œè€Œä¸æ˜¯VAEã€‚ä½†æ˜¯ä¸åŒäºAEçš„æ˜¯ï¼Œå¯¹äºvanilla AEï¼Œä¸€ä¸ªè®­ç»ƒæ ·æœ¬/å›¾åƒå¯¹åº”ä¸€ä¸ªfixed vector(ç»´åº¦ä¸ºlatent_dims)ï¼Œä¹Ÿå°±æ˜¯æ½œç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ã€‚è€ŒVQ-VAEåˆ™æ˜¯å°†å›¾åƒä¸­çš„ä¸€ä¸ªpatchæ˜ å°„åˆ°ä¸€ä¸ªcodebookä¸­çš„ä¸€ä¸ªembedding vector(å¯è§†åŒ–è¿™äº›codeså¯èƒ½ä¼šæœ‰æœ‰è¶£çš„å‘ç°)ã€‚å¯ä»¥è¿™ä¹ˆè¯´ï¼Œä¸€ä¸ªæ ·æœ¬/å›¾åƒæ˜¯ç”±æ½œç©ºé—´ä¸­çš„å¤šä¸ªç‚¹æ„æˆçš„ï¼Œè¿™ä¸ªæ½œç©ºé—´å°±æˆ‘ä»¬éœ€è¦å­¦ä¹ /ç»´æŠ¤çš„codebookã€‚ æˆ‘çš„ç†è§£ï¼Œä¸åŒçš„æ½œç©ºé—´ä¸­çš„ç‚¹ç»„åˆå°±å¯ä»¥ç”Ÿæˆæ–°çš„æ ·æœ¬/å›¾åƒäº†ï¼Ÿè€Œä¸”ç›¸æ¯”AEï¼Œæ½œç©ºé—´æ˜æ˜¾æ›´å¼ºå¤§äº†ï¼Œæ‰€ä»¥å¯ä»¥é¿å…â€œåéªŒå´©å¡Œâ€ï¼ˆdecoderç›´æ¥å¿½ç•¥æ½œå‘é‡å»ç”Ÿæˆæ ·æœ¬/å›¾åƒï¼‰ â€œIntroducing the VQ-VAE model, which is simple, uses discrete latents, does not suffer from â€œposterior collapseâ€ and has no variance issues. â€œ è¿™æ˜¯è®ºæ–‡ä¸­çš„ä¸€å¥è¯ã€‚ ä¸‹é¢æ˜¯redditç½‘å‹/å¤§ä½¬çš„è§£é‡Šï¼š Unlike VAEs with continuous latent spaces, VQVAEs use a discrete space by having a codebook containing a large number of continuous vectors. The VQVAE encoding process maps an image to a continuous space then for each spatial location changes each vector to the closest one in the codebook. Brief recap of posterior collapse in case youâ€™re not sure: my understanding is that VAE models struggle with posterior collapse when (a) the latents contain little information about the input data and (b) a powerful generative model (e.g. an autoregressive decoder) is used that can model the data distribution without any latents. At the start of training the latents often contain little information about the data so the generative model can ignore them and focus on modelling the data distribution on its own. This results in a lack of gradients to the encoder amplifying the problem and so the latents are never used (i.e. posterior collapse). Specifically with VQVAEs the latents (although discrete) are pretty high dimensional so can store a LOT of information about the input, so this helps with (a). As for (b), the decoder tends to be a fairly simple conv net so the latents are definitely needed to reconstruct the input. As for the variance issues since VAE encoders are probabilistic, training requires sampling the encoder outputs which can have high variance. Gaussian VAEs bypass this using the reparameterisation trick (here is a great discussion on this https://ermongroup.github.io/cs228-notes/extras/vae/). While VQVAEs have deterministic encoders, the discretisation process can introduce variance, however, they use the straight through estimator which is biased but has low variance, I believe. This leads to other issues such as codebook collapse, where some codes are never used. DALL-E on the other hand uses Gumbel Softmax. è¿™æ®µè¯è§£é‡Šäº†VQ-VAEä¸ºå•¥èƒ½è§£å†³â€œåéªŒå´©å¡Œâ€çš„é—®é¢˜ï¼Œä½†æ˜¯ä¹Ÿå¸¦æ¥äº†æ–°çš„é—®é¢˜ï¼Œâ€œcodebook collapseâ€ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡encoderå‹ç¼©ä¹‹åçš„vectoræ˜ å°„åˆ°codebookä¸­çš„å‘é‡æ—¶ï¼Œåªä¼šä½¿ç”¨åˆ°codebookä¸­çš„å¾ˆå°ä¸€éƒ¨åˆ†vectorsã€‚ æ¨¡å‹å‰å‘çš„æ¡†æ¶å¦‚ä¸‹ï¼š å¾ˆå¥½ç†è§£çš„ä¸€ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ $z_e \\rightarrow z_q$ çœ‹ä½œæ˜¯ä¸€ä¸ªèšç±»çš„è¿‡ç¨‹ï¼ˆä¹Ÿç¡®å®å¯ä»¥ç”¨kmeansæ–¹æ³•ï¼‰ï¼Œç”šè‡³ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªå†å‚æ•°åŒ–çš„è¿‡ç¨‹ï¼Œåªä¸è¿‡è¿™ä¸ªè¿‡ç¨‹ä¸å¯å¯¼ã€‚ å› ä¸ºå­˜åœ¨argminæ‰€ä»¥ä¸å¯å¯¼ï¼Œé‚£ä¹ˆè¿™ä¸ªå°±æ˜¯æˆ‘ä»¬éœ€è¦è§£å†³çš„ä¸€ä¸ªé—®é¢˜ã€‚ æ€ä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼ŒæŠŠ $z_q$ çš„æ¢¯åº¦ä¼ é€’åˆ° $z_e$ï¼Œè¿™å¬èµ·æ¥å¤ªç„ä¹äº†ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç å§ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class Encoder(nn.Module): def __init__(self, latent_dims, pic_channels=1): super(Encoder, self).__init__() self.conv1 = nn.Conv2d(in_channels=pic_channels, out_channels=latent_dims//2, kernel_size=4) self.conv2 = nn.Conv2d(in_channels=latent_dims//2, out_channels=latent_dims, kernel_size=4) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.conv2(x) #print(x) return x class Decoder(nn.Module): def __init__(self, latent_dims, pic_channels=1): super(Decoder, self).__init__() self.conv_trans1 = nn.ConvTranspose2d( in_channels=latent_dims, out_channels=latent_dims//2, kernel_size=4) self.conv_trans2 = nn.ConvTranspose2d( in_channels=latent_dims//2, out_channels=pic_channels, kernel_size=4) def forward(self, x): x = self.conv_trans1(x) x = F.relu(x) x = self.conv_trans2(x) return x class VectorQuantizer(nn.Module): def __init__(self, latent_dims, num_codes=32, beta=0.25): super(VectorQuantizer, self).__init__() self.K = num_codes self.D = latent_dims self.beta = beta self.codebook = nn.Embedding(self.K, self.D) self.codebook.weight.data.uniform_(-1 / self.K, 1 / self.K) self.vq_loss = 0 def forward(self, latents): ''' latents: (batch, dim, height, width) codebook: (K, dim) ''' # convert latents from BCHW -&gt; BHWC latents = latents.permute(0, 2, 3, 1).contiguous() # (B, H, W, dim) latents_shape = latents.shape # Flatten latent flat_latent = latents.view(-1, self.D) # (BHW, dim) # Compute L2 distance between latents and codes in codebook dist = (flat_latent.unsqueeze(1) - self.codebook.weight.unsqueeze(0)) ** 2 # (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim) dist = dist.sum(-1) # (BHW, K) # Get the code index that has the min distance nearest_idxs = torch.argmin(dist, dim=1).unsqueeze(1) # (BHW, 1) # Convert to one-hot nearest_one_hot = torch.zeros(nearest_idxs.size(0), self.K, device=latents.device) # (BHW, K) nearest_one_hot.scatter_(1, nearest_idxs, 1) # .scatter(dim,index,src) # Quantize the latents quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) # (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim) # Compute the VQ Losses commitment_loss = F.mse_loss(quantized_latents.detach(), latents) codebook_loss = F.mse_loss(quantized_latents, latents.detach()) self.vq_loss = commitment_loss * self.beta + codebook_loss # convert quantized from BHWC -&gt; BCHW return quantized_latents.permute(0, 3, 1, 2).contiguous() class VQVariationalAutoencoder(nn.Module): def __init__(self, latent_dims, ema=True): super(VQVariationalAutoencoder, self).__init__() self.encoder = Encoder(latent_dims) if ema: self.vector_quantizer = VectorQuantizerEMA(latent_dims) else: self.vector_quantizer = VectorQuantizer(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z_e = self.encoder(x) z_q = self.vector_quantizer(z_e) # (batch, dim, 22, 22) return self.decoder(z_q) çœ‹ä»£ç å…¶å®ä¹ŸæŒºç®€å•çš„ï¼š 52-65è¡Œè®¡ç®— l2 è·ç¦»ï¼Œç„¶åé€‰æ‹©è·ç¦»æœ€å°çš„indexï¼Œå†é€šè¿‡è¿™ä¸ªindexå»codebookä¸­é€‰å–å¯¹åº”çš„vector è¿™é‡Œçš„ quantized_latents ä¹Ÿå°±æ˜¯æˆ‘ä»¬æƒ³åˆ°å¾—åˆ°çš„ $z_q$ 68-69è¡Œä»£ç å¯¹åº”ä¸‹å¼ä¸­åä¸¤ä¸ªloss è¿™é‡Œæœ‰ä¸ªä¸å¤ªæ‡‚çš„é—®é¢˜ï¼Œ$z_q$çš„è®¡ç®—è¿‡ç¨‹å› ä¸ºç¬¬58è¡Œargminè‚¯å®šæ˜¯ä¸å¯å¯¼çš„ã€‚å¯æ˜¯ä»£ç ä¸­ä¹Ÿæ²¡æœ‰è§åˆ°åšä»»ä½•å¤„ç†ã€‚åœ¨[è‹å‰‘æ—çš„åšå®¢](VQ-VAEçš„ç®€æ˜ä»‹ç»ï¼šé‡å­åŒ–è‡ªç¼–ç å™¨ - ç§‘å­¦ç©ºé—´|Scientific Spaces)ä¸­è®²è§£äº†è¿™ä¸€ç‚¹ï¼š æŒ‰ç…§è¿™ä¸ªæ€è·¯ï¼Œæˆ‘è§‰å¾—ç¬¬74è¡Œä»£ç åº”è¯¥æ”¹æˆä¸‹é¢è¿™æ ·æ‰å¯¹ï¼Œè¿™æ ·ç¡®ç¡®å®å®åšåˆ°äº†æŠŠ $z_q$ çš„æ¢¯åº¦ä¼ é€’ç»™ $z_e$. 12quantized_latents = latents + (quantized_latents - latents).detach()return quantized_latents.permute(0, 3, 1, 2).contiguous() æä¾›çš„ä»£ç é“¾æ¥ç¡®å®æ˜¯æœ‰é—®é¢˜çš„ï¼Œæ”¹æˆä¸Šè¿°ä¸¤è¡Œä»£ç ä¹‹åå°±å¯ä»¥è·‘å‡ºæ•ˆæœäº†ï½ æ¥ä¸‹æ¥æˆ‘ä»¬è®­ç»ƒVQ-VAE,å¹¶å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„codebookå’Œé‡æ„æƒ…å†µã€‚ä¸ºäº†æ–¹ä¾¿å¯è§†åŒ–ï¼Œæˆ‘ä»¬åŒæ ·è®¾ç½®latent_dims=2 12345678910111213141516171819202122232425262728293031323334353637383940def plot_codebook(autoencoder): codes = autoencoder.vector_quantizer.codebook.weight.detach().cpu().numpy() for i in range(codes.shape[0]): plt.scatter(codes[i][0], codes[i][1]) plt.show() def plot_recon(autoencoder, data, n=10): x = next(iter(data))[0][:n] x_hat = autoencoder(x.to(device)) x_hat = x_hat.to('cpu').detach().numpy().squeeze(1) w = x_hat.shape[1] img = np.zeros((w, n*w)) print(&quot;original:&quot;) for i in range(x_hat.shape[0]): img[:, i*w:(i+1)*w] = x[i] plt.imshow(img) plt.show() print(&quot;reconstructed:&quot;) for i in range(x_hat.shape[0]): img[:, i*w:(i+1)*w] = x_hat[i] plt.imshow(img) plt.show() def train(autoencoder, data, epochs=20): opt = torch.optim.Adam(autoencoder.parameters()) for epoch in range(epochs): print(f&quot;epoch: {epoch+1}/{epochs}&quot;) for x, y in data: x = x.to(device) # GPU opt.zero_grad() x_hat = autoencoder(x) loss = ((x - x_hat)**2).sum() + autoencoder.vector_quantizer.vq_loss loss.backward() opt.step() plot_codebook(autoencoder) plot_recon(autoencoder, data) return autoencoder è®­ç»ƒçš„å¯è§†åŒ–è¿‡ç¨‹å¦‚ä¸‹ï¼š è¿™æ•ˆæœæœ‰ç‚¹ç„å­¦ï¼Œç¬¬ä¸€æ¬¡è·‘å‡ºæ¥æ•ˆæœæŒºå¥½ï¼Œä¹‹åè·‘å‡ºæ¥å°±å¾ˆæ¨¡ç³Šã€‚ã€‚ æŠŠç»´åº¦æ‰©å¤§åˆ° latent_dims=32åæ•ˆæœä¼šå˜å¥½ï¼Œæ•ˆæœå¦‚ä¸‹ï¼š EMA update codebookï¼š å¯¹äºloss functionä¸­çš„ç¬¬äºŒé¡¹ $||sg(E(x))- e_{k}||$ åªæ˜¯ç”¨æ¥æ›´æ–°codebookï¼Œè¿™ä¸ªå¯ä»¥ç”¨ EMAçš„æ–¹æ³•æ¥æ›´æ–°ã€‚ä¹Ÿå°±æ˜¯å¯¹äºæŸä¸€ä¸ªcode $e_i$ï¼Œå®ƒæ˜¯è¢« $z_{i,1},z_{i,2},â€¦,z_{i,n}$ é€‰ä¸­çš„code vectorï¼Œå› æ­¤å®ƒåº”è¯¥äºè¿™äº›encoder output vectorçš„å‡å€¼ $\\frac{1}{n}\\sum_{j}^{n}z_{i,j}$ æ›´æ¥è¿‘ã€‚ å½“ä½¿ç”¨å°æ‰¹é‡(minibatches) è®­ç»ƒæ—¶ï¼Œç”±äºæ•°æ®é‡ä¸è¶³ï¼Œç›´æ¥ç”¨è¿™ä¸ªå‡å€¼æ¥æ›´æ–° $e_i$æ˜¯ä¸å‡†ç¡®çš„ã€‚æ‰€ä»¥ç”¨ æŒ‡æ•°æ»‘åŠ¨å¹³å‡(exponential moving average) æ¥æ›´æ–° $e_i$ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class VectorQuantizerEMA(nn.Module): def __init__(self, latent_dims, num_codes=32, beta=0.25, gamma=0.99, epsilon=1e-5): super(VectorQuantizerEMA, self).__init__() self.K = num_codes self.D = latent_dims self.beta = beta self.gamma = gamma self.epsilon = 1e-5 self.codebook = nn.Embedding(self.K, self.D) self.codebook.weight.data.normal_() self.N = None self.m = None self.vq_loss = 0 def forward(self, latents): ''' latents: (batch, dim, height, width) codebook: (K, dim) ''' # convert latents from BCHW -&gt; BHWC latents = latents.permute(0, 2, 3, 1).contiguous() # (B, H, W, dim) latents_shape = latents.shape # Flatten latent flat_latent = latents.view(-1, self.D) # (BHW, dim) # Compute L2 distance between latents and codes in codebook dist = (flat_latent.unsqueeze(1) - self.codebook.weight.unsqueeze(0)) ** 2 # (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim) dist = dist.sum(-1) # (BHW, K) # Get the code index that has the min distance nearest_idxs = torch.argmin(dist, dim=1).unsqueeze(1) # (BHW, 1) # Convert to one-hot nearest_one_hot = torch.zeros(nearest_idxs.size(0), self.K, device=latents.device) # (BHW, K) nearest_one_hot.scatter_(1, nearest_idxs, 1) # .scatter(dim,index,src) # Quantize the latents quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) # (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim) # Compute the VQ Losses commitment_loss = F.mse_loss(quantized_latents.detach(), latents) self.vq_loss = commitment_loss * self.beta # EMA update cookbook n = torch.sum(nearest_one_hot, 0) # (K) self.N = self.N * self.gamma + (1 - self.gamma) * n if self.N is not None else n N_ = torch.sum(self.N.data) # Laplace smoothing of the cluster size self.N = (self.N + self.epsilon) / (N_ + self.K * self.epsilon) * N_ z = torch.matmul(nearest_one_hot.T, flat_latent) # (K, BHW) * (BHW, dim) = (K, dim) self.m = nn.Parameter(self.m * self.gamma + (1 - self.gamma) * z) if self.m is not None else nn.Parameter(z) self.codebook.weight = nn.Parameter(self.m / self.N.unsqueeze(1)) # convert quantized from BHWC -&gt; BCHW quantized_latents = latents + (quantized_latents - latents).detach() return quantized_latents.permute(0, 3, 1, 2).contiguous() æ½œç©ºé—´ç»´åº¦ latent_dims=2ï¼Œ å¯è§†åŒ–å¦‚ä¸‹ï¼š æ½œç©ºé—´ç»´åº¦ä¸º32ï¼Œ å¯è§†åŒ–å¦‚ä¸‹ï¼š å‰é¢è¿™äº›å¯è§†åŒ–åªæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡æ„ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ˜¯å¦å¯ä»¥åƒVAEé‚£æ ·ï¼Œä¸¢æ‰encoderï¼Œç›´æ¥ä» latent spaceä¸­é‡‡æ ·å»ç”Ÿæˆæ–°çš„æ ·æœ¬/å›¾åƒå‘¢ï¼Ÿ ä½†æ˜¯éšæœºé‡‡æ ·å‡ºæ¥çš„codesç»„åˆèµ·æ¥çš„imageå¤§æ¦‚ä¹±ä¸ƒå…«ç³Ÿçš„ã€‚ã€‚è¿™ä¸ªæ—¶å€™å°±åœ¨è¿™ä¸ªä½ç»´ç©ºé—´å»è®­ç»ƒä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œæ¥æ­£ç¡®é‡‡æ ·å‡ºæœ‰æ•ˆæœ‰æ„ä¹‰çš„codes åœ¨è®­ç»ƒVQ-VAEå®Œä¹‹åã€‚æˆ‘ä»¬å¹¶ä¸èƒ½å»ç”Ÿæˆæ–°çš„æ ·æœ¬/å›¾åƒã€‚æ€ä¹ˆå»ä»æ½œç©ºé—´é‡‡æ ·å‡ºè¿™äº›ç¦»æ•£çš„codeså‘¢ï¼Ÿç°åœ¨æˆ‘ä»¬éœ€è¦è®­ä¸€ä¸ªè‡ªå›å½’çš„æ¨¡å‹ï¼Œè®©è¿™äº›codesçš„æ’åˆ—å˜å¾—æœ‰æ„ä¹‰ã€‚æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›codesçš„åºåˆ—çœ‹æˆä¸€å¥è¯ï¼Œç„¶åè®­ç»ƒè¿™æ ·ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯stage2 prior trainingã€‚ä¸‹å›¾æ˜¯VQ-VAE-2çš„ä¼ªä»£ç ï¼ˆ2.0ç‰ˆæœ¬å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„VQ-VAE). ç®€å•è§£é‡Šä¸‹ï¼š $e_{top}\\leftarrow Quantize(h_{top})$ ä¸­ $h_{top}$ å°±æ˜¯encoderçš„è¾“å‡ºï¼Œé€šè¿‡é‡åŒ–(æ˜ å°„)ä¹‹åå¾—åˆ° $e_{top}$ å¯¹äºbottomä¹Ÿæ˜¯ä¸€æ ·çš„ prior training: $T_{top}$ å°±æ˜¯ $e_{top}$ çš„åºåˆ— $p_{top}=TrainPixelCNN(T_{top})$ è®­ç»ƒè‡ªå›å½’æ¨¡å‹ VideoGPT å·¦è¾¹å°±æ˜¯åŸå§‹çš„VQ-VAE. å³ä¾§æ˜¯è®­ç»ƒä¸€ä¸ªè‡ªå›å½’ç½‘ç»œã€‚è¿™ä¹ˆçœ‹è¿™ç¯‡paperç¡®å®æ²¡å•¥åˆ›æ–°ç‚¹å•Šã€‚ã€‚å°±æ˜¯æŠŠvq-vaeåº”ç”¨åœ¨äº†videoä¸Šï¼Œä¸è¿‡ä»£ç å…¶å®å¤æ‚äº†å¾ˆå¤šã€‚ åˆ†ä¸¤ä¸ªé˜¶æ®µï¼š VQ-VAE training VideoGPT training çœ‹å®Œä»£ç ï¼Œç¬¬äºŒé˜¶æ®µforwardè¿‡ç¨‹å¦‚ä¸‹ï¼š éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡vqvae.encoderä¹‹åçš„ç»´åº¦å˜æˆäº† [t/4, h/4,w/4], è¿™å°±æ˜¯ latent vectorsçš„ä¸ªæ•°ï¼Œåç»­å°±æ˜¯åœ¨è¿™ä¸ªä¸Šé¢åšself-attentionä»¥åŠè‡ªå›å½’ (æ³¨æ„è‡ªå›å½’éœ€è¦decoder input right-shift). LVTè¿™ç¯‡è·Ÿ VideoGPTå¾ˆåƒï¼Œéƒ½æ˜¯åœ¨æ½œç©ºé—´å†…åšè‡ªå›å½’ï¼Œä½†æ˜¯è¿™ç¯‡paperåœ¨VideoGPTä¹‹å‰å‘è¡¨ã€‚è€Œä¸”çœ‹äº†openviewerçš„å®¡ç¨¿æ„è§ï¼Œreviewers å°±æ€¼VideoGPTè·Ÿè¿™ç¯‡å¾ˆåƒã€‚ã€‚ä½†æ˜¯ä¸¤è€…çš„æ¨¡å‹ç»“æ„æ˜¯æœ‰åŒºåˆ«çš„ DVT-NAT Fast Decoding in Sequence Models Using Discrete Latent Variables, (ICML2018) è¿™ç¯‡paperå¥½å‘€å¥½å‘€ï¼Œæ˜¯çœŸå¥½ï¼ï¼å…³é”®è¿˜æ˜¯18å¹´å°±å‘è¡¨çš„ï¼Œæ˜¯çœŸçš„ç‰›é€¼ï¼åæ€ä¸€ä¸‹ä¸ºå•¥åˆ«äººå°±èƒ½åœ¨ç´§è·Ÿå‰æ²¿ï¼Œè€Œä¸”è¿˜èƒ½ä»CVé¢†åŸŸfollowå‡ºè¿™ä¹ˆå‰å®³çš„NLPçš„å·¥ä½œå‘¢ï¼Ÿ Abstract æå‡ºäº†ä¸€ç§å¯¹ç›®æ ‡åºåˆ—è¿›è¡Œç¦»æ•£å»ºæ¨¡çš„æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆæé«˜è‡ªå›å½’å»ºæ¨¡çš„æ•ˆç‡ï¼Œä»è€Œæå‡è§£ç é€Ÿåº¦ To overcome this limitation, we propose to introduce a sequence of discrete latent variables $l_1 . . . l_m$, with $m &lt; n$, that summarizes the relevant information from the sequence $y_1 . . . y_n$. We will still generate $l_1 . . . l_m$ autoregressively, but it will be much faster as $m &lt; n$ (in our experiments we mostly use $m = n/8$ ). Then, we reconstruct each position in the sequence $y_1 . . . y_n$ from $l_1 . . . l_m$ in parallel. ä»¥å‰çš„seq2seqæ–¹å¼æ˜¯ $(x_1,â€¦,x_L) \\rightarrow (y_1,â€¦,y_n)$ ï¼Œç°åœ¨æ˜¯ $(x_1,â€¦,x_L) \\rightarrow (l_1,..,l_m)\\rightarrow (y_1,â€¦,y_n)$ ,å…¶ä¸­ $l_1,â€¦,l_m$ çš„ç”Ÿæˆä¾ç„¶æ˜¯è‡ªå›å½’çš„ï¼Œä½†æ˜¯é•¿åº¦æ›´çŸ­ï¼Œ æ‰€ä»¥æé«˜äº†æ•ˆç‡ã€‚ å°† latent transformer åº”ç”¨åœ¨æœºå™¨ç¿»è¯‘ä¸Šï¼Œæå‡ç¿»è¯‘æ•ˆç‡ã€‚ä½†æ˜¯åœ¨BLEUä¸Šä»ç„¶æ¯”è‡ªå›å½’çš„æ–¹æ³•è¦å·®å¾ˆå¤šã€‚ Contribution æå‡ºäº†ä¸€ä¸ªåŸºäºç¦»æ•£æ½œå˜é‡çš„å¿«é€Ÿè§£ç æ¡†æ¶ æå‡ºæ–°çš„ç¦»æ•£åŒ–æŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆç¼“è§£VQ-VAEä¸­çš„index collapseé—®é¢˜ å°† latent transofmer åº”ç”¨åœ¨æœºå™¨ç¿»è¯‘ä¸Šï¼Œæå‡äº†ç¿»è¯‘é€Ÿåº¦ Discretization Techniques Gumbel-Softmax å°†argmax/argminå¯å¾®åŒ–ã€‚å°†é‡‡æ ·è¿‡ç¨‹ç”¨å¯å¯¼çš„softmaxä»£æ›¿ï¼ŒåŒæ—¶åŠ ä¸Šgumbel noiseä½¿å¾—æœ€ç»ˆçš„ç»“æœè·Ÿ PyTorch 32.Gumbel-Softmax Trick - ç§‘æŠ€çŒ›å…½çš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/166632315 Improved Semantic Hashing Vector Quantization Decomposed Vector Quantizationmotivationindex collapse, where only a few of the embedding vectors get trained due to a rich getting richer phenomena Sliced Vector Quantization break up the encoder output enc(y) into n_d smaller slices, like multi-head attention in transformer (eq. 11) Latent Transformer Main Steps VAE encoder encodes target sentence $y$ into shorter discrete latent variables $l$ (parallel) Latent prediction model - Transformer, is trained to predict $l$ from source sentence $x$ (autoregressive) VAE decoder decodes predicted $l$ back to sequence $y$ (parallel) Loss Function reconstruction loss $l_r$ from VAE latent prediction loss $l_{lp}$ from Latent Transformer in first 10k steps, true targets $y$ is given to transformer-decoder instead of decompressed latents $l$ which ensures self-attention part has reasonable gradients to train the whole architecture Architectures of VAE Encoder conv residual blocks + attention + conv to scale down the dimension $C = n/m, C=2^c$, in the setting, $C=8, c=3$ Decoder conv residual blocks + attention + up-conv to scale up the dimension Transformer Decoder","link":"/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/"},{"title":"è®ºæ–‡ç¬”è®°-GAN tutorial NIPS 2016","text":"ä¸ºä»€ä¹ˆè¦å­¦ä¹  GANï¼Ÿ High-dimensional probability distributions, ä»é«˜ç»´æ¦‚ç‡åˆ†å¸ƒä¸­è®­ç»ƒå’Œé‡‡æ ·çš„ç”Ÿæˆæ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„èƒ½åŠ›æ¥è¡¨ç¤ºé«˜ç»´æ¦‚ç‡åˆ†å¸ƒã€‚ Reinforcement learning. å’Œå¼ºåŒ–å­¦ä¹ ç»“åˆã€‚ Missing data. ç”Ÿæˆæ¨¡å‹èƒ½æœ‰æ•ˆçš„åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ï¼Œä¹Ÿå°±æ˜¯åŠç›‘ç£å­¦ä¹  semi-supervised learningã€‚ ç”Ÿæˆæ¨¡å‹å¦‚ä½•å·¥ä½œçš„Maximum likehood estimationæå¤§ä¼¼ç„¶ä¼°è®¡å°±æ˜¯åœ¨ç»™å®šæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åˆç†çš„æ¨¡å‹ï¼ˆæ¯”å¦‚è¯­è¨€æ¨¡å‹ï¼‰è®¡ç®—ç›¸åº”çš„ä¼¼ç„¶ï¼ˆæ¦‚ç‡ï¼‰ï¼Œä½¿å¾—è¿™ä¸ªæ¦‚ç‡æœ€å¤§çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡æ¨¡å‹çš„å‚æ•°ã€‚ $$\\sum_i^mp_{\\text{model}}(x^{(i)}; \\theta)$$ m æ˜¯æ ·æœ¬æ•°é‡ã€‚ å°†ä¸Šè¯‰æ¦‚ç‡è½¬æ¢åˆ° log ç©ºé—´ï¼ˆlog å‡½æ•°æ˜¯å•è°ƒé€’å¢çš„ï¼‰ï¼Œå¯ä»¥å°†ä¹˜ç§¯è½¬æ¢ä¸ºç›¸åŠ ï¼Œç®€åŒ–äº†è®¡ç®—ã€‚ åŒæ ·çš„ï¼Œä¹Ÿå¯ä»¥æŠŠä¸Šè¯‰ä¼¼ç„¶ä¼°è®¡çš„æå¤§åŒ–çœ‹åšæ˜¯æœ€å°åŒ– KL æ•£åº¦ï¼ˆæˆ–è€…äº¤å‰ç†µï¼‰ã€‚è¿™æ ·ä¸€æ¥ï¼Œç›¸å½“äºæŠŠæ— ç›‘ç£é—®é¢˜è½¬æ¢æˆäº†æœ‰ç›‘ç£é—®é¢˜ã€‚ï¼ˆä¸çŸ¥ç†è§£çš„æ˜¯å¦æ­£ç¡®ï¼Ÿï¼‰ ç›¸å¯¹ç†µç†µæ˜¯ä¿¡æ¯é‡ $log{\\dfrac{1}{p(i)}}$ (æ¦‚ç‡è¶Šå¤§ä¿¡æ¯é‡è¶Šå°‘)ã€‚ p æ˜¯çœŸå®åˆ†å¸ƒï¼Œ q æ˜¯éçœŸå®åˆ†å¸ƒã€‚ äº¤å‰ç†µæ˜¯ç”¨ q åˆ†å¸ƒæ¥ä¼°è®¡ p åˆ†å¸ƒæ‰€éœ€è¦æ¶ˆé™¤çš„ä»£ä»· cost: $$H(p,q) = \\sum_ip(i)log{\\dfrac{1}{q(i)}}$$ ç”¨çœŸå®åˆ†å¸ƒ p ä¼°è®¡çœŸå®åˆ†å¸ƒæ‰€éœ€è¦çš„ cost: $$H(p) = \\sum_ip(i)log{\\dfrac{1}{p(i)}}$$ ä»è¿™é‡Œä¹Ÿèƒ½çœ‹å‡ºï¼Œå½“æ¦‚ç‡ p ä¸º 1 æ—¶ï¼Œæ‰€éœ€è¦çš„ cost å°±ä¸º 0 äº†ã€‚ ç›¸å¯¹ç†µï¼Œåˆç§° KL æ•£åº¦(Kullbackâ€“Leibler divergence)ï¼Œå°±æ˜¯æŒ‡ä¸Šè¯‰ä¸¤è€…æ‰€éœ€è¦æ¶ˆè€—çš„ cost çš„å·®å€¼ï¼š $$D(p||q) = H(p,q)-H(p) = \\sum_ip(i)log{\\dfrac{p(i)}{q(i)}}$$ GAN æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼ŸGAN æ¡†æ¶ åˆ¤åˆ«å™¨ discriminator ç”Ÿæˆå™¨ gererator åœ¨å·¦è¾¹åœºæ™¯ï¼Œåˆ¤åˆ«å™¨å­¦ä¹ å¦‚ä½•åˆ†åˆ«æ ·æœ¬æ˜¯ real or fake. æ‰€ä»¥å·¦è¾¹çš„è¾“å…¥æ˜¯ half real and half fake. åœ¨å³è¾¹çš„våœºæ™¯ï¼Œåˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨éƒ½å‚ä¸äº†ã€‚G ç”¨æ¥ç”Ÿæˆ G(z), D ç”¨æ¥åˆ¤åˆ« G(z) æ˜¯ real or fake. ç»“æ„åŒ–æ¦‚ç‡æ¨¡å‹ Structured Probabilistic Modelsfor Deep Learning containing latent variables z and observed variables x. z æ˜¯éœ€è¦å­¦ä¹ çš„éšè—å˜é‡ï¼Œx æ˜¯å¯è§‚å¯Ÿåˆ°çš„å˜é‡ã€‚ åˆ¤åˆ«å™¨ D çš„è¾“å…¥æ˜¯ xï¼Œå…¶éœ€è¦å­¦ä¹ çš„å‚æ•°æ˜¯ $\\theta^{(D)}$. ç”Ÿæˆå™¨ G çš„è¾“å…¥æ˜¯ z, å…¶éœ€è¦å­¦ä¹ çš„å‚æ•°æ˜¯ $\\theta^{(G)}$. ä¸¤ä¸ªç©å®¶éƒ½æœ‰å„è‡ªçš„æŸå¤±å‡½æ•°ã€‚ $J^{(D)}(\\theta^{(D)}, \\theta^{(G)})$, ä½†æ˜¯ $J^{(D)}$ å¹¶ä¸å½±å“å‚æ•° $\\theta^{(G)}$. åŒæ ·çš„é“ç†ï¼Œåè¿‡æ¥ä¹Ÿæ˜¯ $J^{(G)}(\\theta^{(G)}, \\theta^{(D)})$. æ¯ä¸ªç©å®¶çš„æŸå¤±å‡½æ•°ä¾èµ–äºå¦ä¸€ä¸ªç©å®¶çš„å‚æ•°ï¼Œä½†æ˜¯ç¡®ä¸èƒ½æ§åˆ¶å®ƒçš„å‚æ•°ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ç§ Nash equilibrium é—®é¢˜ã€‚æ‰¾åˆ°è¿™æ ·ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜è§£ $tuple(\\theta^{(G)}, \\theta^{(D)})$ ä½¿å¾— $J^{(D)}$ å…³äº $\\theta^{(D)}$ å±€éƒ¨æœ€å°ï¼Œ$J^{(G)}$ å…³äº $\\theta^{(G)}$ å±€éƒ¨æœ€å°ã€‚ Generatordifferentiable function G, å¯å¾®åˆ†å‡½æ•° G. å®é™…ä¸Šå°±æ˜¯ ç¥ç»ç½‘ç»œã€‚z æ¥è‡ªç®€å•çš„å…ˆéªŒåˆ†å¸ƒï¼ŒG(z) é€šè¿‡æ¨¡å‹ $p_{model}$ ç”Ÿæˆæ ·æœ¬ x. å®è·µä¸­ï¼Œå¯¹äº G çš„è¾“å…¥ä¸ä¸€å®šåªåœ¨ç¬¬ä¸€å±‚ layer, ä¹Ÿå¯ä»¥åœ¨ ç¬¬äºŒå±‚ ç­‰ç­‰ã€‚æ€»ä¹‹ï¼Œç”Ÿæˆå™¨çš„è®¾è®¡å¾ˆçµæ´»ã€‚ Training processä¸¤ä¸ª minibatch çš„è¾“å…¥ï¼š x æ¥è‡ª training dataset. z æ¥è‡ªå…ˆéªŒéšè—å˜é‡æ„æˆçš„æ¨¡å‹ $p_\\text{model}$ã€‚é€šè¿‡ä¼˜åŒ–ç®—æ³• SGD/Adam å¯¹ä¸¤ä¸ªæŸå¤± $J^{(D)} J^{(G)}$ é€šè¿‡è¿›è¡Œä¼˜åŒ–ã€‚æ¢¯åº¦ä¸‹é™ä¸€èˆ¬æ˜¯åŒæ­¥çš„ï¼Œä¹Ÿæœ‰äººè®¤ä¸ºä¸¤è€…çš„è¿­ä»£æ­¥æ•°å¯ä»¥ä¸ä¸€æ ·ã€‚åœ¨è¿™ç¯‡ tutorial çš„æ—¶å€™ï¼Œå¾—åˆ°çš„å…±è¯†æ˜¯åŒæ­¥çš„ã€‚ cost functionç›®å‰å¤§å¤šæ•° GAN çš„æŸå¤±å‡½æ•°ï¼Œ$J(D)$ éƒ½æ˜¯ä¸€æ ·çš„ã€‚åŒºåˆ«åœ¨äº $J(G)$. The discriminatorâ€™s cost, J (D) è¿™æ˜¯ä¸ªæ ‡å‡†çš„ç”¨äºäºŒåˆ†ç±»çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚åªä¸è¿‡è¿™é‡Œï¼Œæ­£åˆ†ç±»éƒ½æ¥è‡ªäºè®­ç»ƒé›†ï¼Œæ­£åˆ†ç±»çš„æ¦‚ç‡æ˜¯ $\\dfrac{1}{2}E_{xï½d_{data}}$, è´Ÿåˆ†ç±»æ¥è‡ªäºç”Ÿæˆå™¨ï¼Œåˆ™å…¶æ¦‚ç‡æ˜¯ $\\dfrac{1}{2}E_z$ é€šè¿‡è®­ç»ƒåˆ¤åˆ«å™¨ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°è¿™æ ·ä¸€ä¸ªæ¯”ä¾‹: $$\\dfrac{p_{data}(x)}{p_{\\text{model}}(x)}$$ GANs é€šè¿‡ç›‘ç£å­¦ä¹ æ¥è·å¾—è¿™ä¸ª ratio çš„ä¼°è®¡ï¼Œè¿™ä¹Ÿæ˜¯ GANs ä¸åŒäº å˜åˆ†è‡ªç¼–ç  å’Œ æ³¢å°”å…¹æ›¼æœº (variational autoencoders and Boltzmann machines) çš„åŒºåˆ«ã€‚ ä½†æ˜¯ GANs é€šè¿‡ç›‘ç£å­¦ä¹ æ¥ä¼°è®¡è¿™ä¸ª ratio,ä¼šå‘ç›‘ç£å­¦ä¹ ä¸€æ ·é‡åˆ°åŒæ ·çš„é—®é¢˜ï¼šè¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆã€‚ä½†æ˜¯é€šè¿‡è¶³å¤Ÿçš„æ•°æ®å’Œå®Œç¾çš„ä¼˜åŒ–å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ Minimax, zero-sum gameè®¾è®¡ cost function for generator. å‰é¢æˆ‘ä»¬çŸ¥é“ï¼Œä¸¤ä¸ªç©å®¶ player æˆ–è€…è¯´ ç¥ç»ç½‘ç»œ Gï¼ŒD å®é™…ä¸Šæ˜¯ä¸€ç§åšå¼ˆï¼ŒD å¸Œæœ›èƒ½æ‰¾å‡º G(z) æ˜¯ fakeï¼Œè€Œ G å¸Œæœ›èƒ½è®© G(z) å°½å¯èƒ½åƒ real. æ‰€ä»¥æœ€ç®€å•çš„ä¸€ç§æ–¹å¼å°±æ˜¯ all playerâ€™s cost is always zero. $$J^{(G)} = -J^{(D)}$$ è¿™æ · $J^{(G)}, J^{(D)}$ éƒ½å¯ä»¥ç”¨ value function è¡¨ç¤ºï¼š $$V(\\theta^{(D)}, \\theta^{(G)})=-J^{(D)}(\\theta^{(D)}, \\theta^{(G)})$$ é‚£ä¹ˆæ•´ä¸ª game ä¹Ÿå°±æ˜¯ä¸€ä¸ª minmax æ¸¸æˆï¼š outer loop æ˜¯å…³äº $\\theta^{(G)}$ çš„æœ€å°åŒ–ï¼Œinner loop æ˜¯å…³äº $\\theta^{(D)}$ çš„æœ€å¤§åŒ–ã€‚ ä½†æ˜¯è¿™ç§ function åœ¨å®é™…ä¸­å¹¶ä¸èƒ½ä½¿ç”¨ï¼Œå› ä¸ºå…¶éå‡¸æ€§ã€‚ In practice, the players are represented with deep neural nets and updates are made in parameter space, so these results, which depend on convexity, do not apply Heuristic, non-saturating game Minimizing the cross-entropy between a target class and a classifierâ€™s predicted distribution is highly effective because the cost never saturates when the classifier has the wrong output. å¯¹ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œæœ€å°åŒ– ç›®æ ‡ç±»å’Œé¢„æµ‹æ¦‚ç‡çš„äº¤å‰ç†µ æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå› ä¸ºå½“ åˆ†ç±»å™¨ å­˜åœ¨è¯¯åˆ†ç±»æ—¶ï¼ŒæŸå¤±å‡½æ•°å°±æ°¸è¿œä¸å¯èƒ½é¥±å’Œã€‚ æ‰€ä»¥ï¼Œå¯¹äºç”Ÿæˆå™¨çš„ cost ä¾æ—§ä½¿ç”¨äº¤å‰ç†µï¼Œä½†å¦‚æœä½¿ç”¨å’Œ åˆ¤åˆ«å™¨ä¸€æ¨¡ä¸€æ ·çš„ cost(è¿™é‡Œåº”è¯¥å°±æ˜¯æŠŠæ­£è´Ÿåˆ†ç±»åè¿‡æ¥ï¼Ÿ)ï¼š $$J^{(G)}(\\theta^{(D)}, \\theta^{(G)})=-\\dfrac{1}{2}E_zlogD(G(z))-\\dfrac{1}{2}E_{xï½p_{data}}log(1-D(x))$$ çŒœæƒ³åº”è¯¥æ˜¯è¿™æ ·ï¼Œæ–‡ä¸­æ²¡æœ‰ç»™å‡ºã€‚ ä¸å¹¸çš„æ˜¯è¿™æ ·çš„åœ¨æ˜¯å®é™…ä¸­ï¼Œä¹Ÿå¹¶ä¸å¯è¡Œã€‚å½“ åˆ¤åˆ«å™¨ æ‹’ç»ä¸€ä¸ªé«˜ç½®ä¿¡åº¦(high confidence) çš„æ ·æœ¬æ—¶ï¼Œç”Ÿæˆå™¨ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±ã€‚ æ‰€ä»¥ï¼Œæ”¹è¿›ä¹‹åå°±æ˜¯: $$J^{(G)}(\\theta^{(D)}, \\theta^{(G)})=-\\dfrac{1}{2}E_zlogD(G(z))$$ In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the logprobability of the discriminator being mistaken. åœ¨ Minmaxï¼Œzero-game ä¸­ï¼Œç”Ÿæˆå™¨çš„ç›®çš„æ˜¯æœ€å°åŒ– åˆ¤åˆ«å™¨ è‡ªè®¤ä¸ºè‡ªå·±åˆ¤åˆ«å¯¹äº†çš„ log-probability, è€Œåœ¨ non-saturating game ä¸­ï¼Œç”Ÿæˆå™¨æ˜¯æœ€å¤§åŒ– åˆ¤åˆ«å™¨åˆ¤åˆ«é”™è¯¯ çš„ log-probability. Maximum likelihood gameå‰é¢æåˆ°æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯æœ€å°åŒ–æ¨¡å‹ä¸æ•°æ®ä¹‹é—´çš„ KL æ•£åº¦ã€‚ GANs ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡åˆ™æ˜¯å¯¹æ¯”ä¸åŒçš„æ¨¡å‹ã€‚ $$J^{(G)}=-\\dfrac{1}{2}E_zexp(\\sigma^{-1}(D(G(z))))$$ in practice, both stochastic gradient descent on the KL divergence and the GAN training procedure will have some variance around the true expected gradient due to the use of sampling (of x for maximum likelihood and z for GANs) to construct the estimated gradient. åœ¨å®è·µä¸­ï¼Œç”±äºä½¿ç”¨é‡‡æ ·ï¼ˆxè¡¨ç¤ºæœ€å¤§ä¼¼ç„¶ï¼Œzè¡¨ç¤ºGANï¼‰æ¥æ„å»ºä¼°è®¡çš„æ¢¯åº¦ï¼Œå› æ­¤KLæ•£åº¦å’ŒGANè®­ç»ƒè¿‡ç¨‹çš„éšæœºæ¢¯åº¦ä¸‹é™éƒ½ä¼šåœ¨çœŸå®é¢„æœŸæ¢¯åº¦é™„è¿‘äº§ç”Ÿä¸€äº›å˜åŒ–ã€‚ Is the choice of divergence a distinguishing feature of GANs?Jensen-Shannon divergenceï¼Œ reverse KLè®¸å¤šäººè®¤ä¸º GANs èƒ½å¤Ÿç”Ÿæˆæ›´åŠ æ¸…æ™°ã€é€¼çœŸçš„æ ·æœ¬æ˜¯å› ä¸ºä»–ä»¬æœ€å°åŒ–çš„æ˜¯ Jensen-Shannon divergence. è€Œ VAEs ç”Ÿæˆæ¨¡ç³Šçš„æ ·æœ¬æ˜¯å› ä¸ºä»–ä»¬æœ€å°åŒ–çš„æ˜¯ KL divergence between the data and the model. KL æ•£åº¦å¹¶ä¸æ˜¯å¯¹ç§°çš„ã€‚$D_{KL}(p_{data}||q_{model})$ ä¸ $D_{KL}(p_{model}||q_{data})$ æ˜¯ä¸ä¸€æ ·çš„ã€‚æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯å‰è€…ï¼Œæœ€å°åŒ– Jensen-Shannon divergence åˆ™æ›´åƒåè€…ã€‚ æ¯”è¾ƒ $D_{KL}(p_{data}||q_{model})$ ä¸ $D_{KL}(p_{model}||q_{data})$ åŒºåˆ«ã€‚åœ¨æ¨¡å‹èƒ½åŠ›ä¸è¶³ä»¥æ‹Ÿåˆæ•°æ®çš„åˆ†å¸ƒæ—¶ï¼Œè¡¨ç°çš„å°¤ä¸ºæ˜æ˜¾ï¼Œå¦‚ä¸Šå›¾ã€‚ç»™å®šçš„æ•°æ®æ˜¯ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒæ··åˆçš„åˆ†å¸ƒã€‚è€Œæ¨¡å‹æ˜¯ä¸€ä¸ªé«˜æ–¯æ¨¡å‹ã€‚ç„¶ååˆ†åˆ«ç”¨æå¤§ä¼¼ç„¶ Maximum likehood ã€reverse KL æ•£åº¦ä½œä¸º criterionï¼Œä¹Ÿå°±æ˜¯ cost. å¯ä»¥çœ‹åˆ°å‰è€…é€‰æ‹©å»å¹³å‡ä¸¤ä¸ªæ¨¡æ€ï¼Œå¹¶å¸Œæœ›åœ¨ä¸¤ä¸ªæ¨¡æ€ä¸Šéƒ½èƒ½å¾—åˆ°è¾ƒé«˜çš„æ¦‚ç‡ã€‚è€Œåè€…åªé€‰æ‹©å…¶ä¸­ä¸€ä¸ªæ¨¡æ€ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯å¦å¤–ä¸€ä¸ªæ¨¡æ€ï¼Œä¸¤ä¸ªæ¨¡æ€å¯¹äº reverse KL éƒ½å«æœ‰å±€éƒ¨æœ€ä¼˜è§£ã€‚ æ‰€ä»¥ï¼Œä»è¿™ä¸ªè§†è§’æ¥çœ‹ï¼Œ Maximum likehood å€¾å‘äºç»™ data å‡ºç°çš„ä½ç½®æ›´é«˜çš„æ¦‚ç‡ï¼Œè€Œ reverse KL åˆ™å€¾å‘äºç»™æ²¡æœ‰å‡ºç° data çš„ä½ç½®è¾ƒä½çš„æ¦‚ç‡ã€‚æ‰€ä»¥ $D_{KL}(p_{model}||q_{data})$ å¯ä»¥ç”Ÿæˆæ›´æ£’çš„æ ·æœ¬ï¼Œå› ä¸ºæ¨¡å‹ä¸ä¼šç”Ÿæˆä¸å¸¸è§çš„æ ·æœ¬ï¼Œå› ä¸ºæ•°æ®ä¹‹é—´å…·æœ‰æ¬ºéª—æ€§çš„æ¨¡æ€ã€‚ ç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›æ–°çš„ç ”ç©¶è¡¨æ˜ï¼ŒJensen-Shannon divergence å¹¶ä¸æ˜¯ GANs èƒ½ç”Ÿæˆæ›´æ¸…æ™°æ ·æœ¬çš„åŸå› ã€‚ f-GAN è¯æ˜ï¼ŒKL æ•£åº¦ä¹Ÿèƒ½ç”Ÿæˆæ¸…æ™°çš„sampleï¼Œå¹¶ä¸”ä¹Ÿåªé€‰æ‹©å°‘é‡çš„modes, è¯´æ˜ Jensen-Shannon divergence å¹¶ä¸æ˜¯ GANs ä¸åŒäºå…¶ä»–æ¨¡å‹çš„ç‰¹å¾ã€‚ GANs é€šå¸¸é€‰æ‹©å°‘é‡çš„ mode æ¥ç”Ÿæˆæ ·æœ¬ï¼Œè¿™ä¸ªå°‘é‡æŒ‡çš„æ˜¯å°äºæ¨¡å‹çš„èƒ½åŠ›ã€‚ è€Œ reverse KL åˆ™æ˜¯é€‰æ‹©æ›´å¯èƒ½å¤šçš„ mode of the data distribution åœ¨æ¨¡å‹èƒ½åŠ›èŒƒå›´å†…ã€‚å®ƒé€šå¸¸ä¸ä¼šé€‰æ‹©æ›´å°‘çš„ mode. è¿™ä¹Ÿè§£é‡Šäº† mode collapse å¹¶ä¸æ˜¯æ•£åº¦é€‰æ‹©çš„åŸå› ã€‚ Altogether, this suggests that GANs choose to generate a small number of modes due to a defect in the training procedure, rather than due to the divergence they aim to minimize. æ‰€ä»¥ï¼ŒGANs é€‰æ‹©å°‘é‡çš„ mode æ˜¯å› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å…¶ä»–ç¼ºé™·ï¼Œè€Œä¸æ˜¯ æ•£åº¦ é€‰æ‹©çš„é—®é¢˜ã€‚ Comparison of cost functionsç”Ÿæˆå¯¹æŠ—ç½‘ç»œå¯ä»¥çœ‹åšä¸€ç§ reinforcement learning. ä½†æ˜¯$j^{(G)}$ å¹¶æ²¡æœ‰ç›´æ¥å‚è€ƒ training dataï¼Œæ‰€æœ‰å…³äº training data çš„ä¿¡æ¯éƒ½æ¥è‡ªäº åˆ¤åˆ«å™¨ çš„å­¦ä¹ ã€‚ æ‰€ä»¥å’Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ˜¯æœ‰åŒºåˆ«çš„ï¼š æ¯”è¾ƒä¸åŒçš„ cost functionï¼š $D(G(z))$ è¡¨ç¤º åˆ¤åˆ«å™¨ ç»™ generate sample ä¸ºçœŸçš„æ¦‚ç‡ã€‚ åœ¨å·¦ä¾§ï¼ŒMinimax å’Œ Maximum likehood éƒ½è¶‹å‘äºé¥±å’Œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ä¸€ä¸ªæ ·æœ¬å¾ˆæ˜æ˜¾ä¸º fake æ—¶ï¼Œcost æ¥è¿‘äº 0. ä¼¼ç„¶ä¼°è®¡è¿˜æœ‰ä¸ªé—®é¢˜ï¼Œcost ä¸»è¦æ¥æºäº ç‰¹åˆ«åƒçœŸ çš„å°‘éƒ¨åˆ†æ ·æœ¬ï¼Œè¿™ä¹Ÿæ˜¯ä¸å¥½çš„ã€‚éœ€è¦ç”¨åˆ° variance reduction techniques. Maximum likelihood also suffers from the problem that nearly all of the gradient comes from the right end of the curve, meaning that a very small number of samples dominate the gradient computation for each minibatch. This suggests that variance reduction techniques could be an important research area for improving the performance of GANs, especially GANs based on maximum likelihood. The DCGAN architecture DCGAN çš„ç»“æ„ã€‚ GANï¼ŒNCEï¼Œ MLE çš„å¯¹æ¯” ç›¸åŒç‚¹ï¼š MiniMax GAN å’Œ NCE çš„ cost function ç›¸åŒ ä¸åŒç‚¹ï¼š æ›´æ–°ç­–ç•¥ä¸ä¸€æ ·ï¼ŒGAN å’Œ MLE éƒ½æ˜¯æ¢¯åº¦ä¸‹é™ï¼Œè€Œ MLE copies the density model learned inside the discriminator and converts it into a sampler to be used as the generator. NCE never updates the generator; it is just a fixed source of noise. Tips and TricksHow to train a GAN: https://github.com/soumith/ganhacks Research FrontiersNon-convergencemode collapse","link":"/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/"},{"title":"è®ºæ–‡ç¬”è®°-Explicit Semantic Analysis","text":"paper: Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysi, IJCAI2008 Wikipedia-based Semantic Interpretation for Natural Language Processing Motivationå¯¹äºè‡ªç„¶è¯­è¨€çš„è¯­ä¹‰è¡¨ç¤ºï¼Œéœ€è¦çš„å¤§é‡çš„ common sense å’Œ world knowledge. å‰äººçš„ç ”ç©¶ä½¿ç”¨ç»Ÿè®¡çš„æ–¹æ³•ï¼Œä¾‹å¦‚ WordNetï¼Œä»…ä»…åªåˆ©ç”¨äº†æœ‰é™çš„è¯å…¸çŸ¥è¯†ï¼ˆlexicographic knowledgeï¼‰ï¼Œå¹¶ä¸èƒ½æœ‰æ•ˆçš„åˆ©ç”¨è¯­è¨€æœ¬èº«èƒŒåçš„èƒŒæ™¯çŸ¥è¯†ï¼ˆ background knowledgeï¼‰ã€‚ ä½œè€…æå‡ºäº† Explicit Semantic Analysis (ESA)ï¼Œèƒ½å¤Ÿå¯¹æ–‡æœ¬æˆ–å•è¯è¿›è¡Œå¯è§£é‡Šæ€§çš„ç»†ç²’åº¦çš„è¯­ä¹‰è¡¨ç¤ºã€‚ Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. æ˜¾ç¤ºçš„ä½¿ç”¨ wiki ä¸­çš„æ¦‚å¿µï¼ˆconceptsï¼‰æ¥è¡¨ç¤ºä»»æ„é•¿åº¦çš„ text. ä½œè€…é€šè¿‡æ–‡æœ¬åˆ†ç±»å’Œè®¡ç®—è‡ªç„¶è¯­è¨€çš„æ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥éªŒè¯ ESA çš„æœ‰æ•ˆæ€§ã€‚ç”±äºè¯­ä¹‰è¡¨ç¤ºä½¿ç”¨çš„æ˜¯ natural conceptsï¼ŒESA æ¨¡å‹çš„å¯è§£é‡Šæ€§éå¸¸å¼ºã€‚ ä¼ ç»Ÿçš„æ–¹æ³•ï¼š è¯è¢‹æ¨¡å‹: å°† text çœ‹ä½œæ˜¯ unordered bags of words, æ¯ä¸€ä¸ªå•è¯çœ‹ä½œæ˜¯ä¸€ç»´ç‰¹å¾ã€‚ä½†æ˜¯è¿™å¹¶ä¸èƒ½è§£å†³ NLP ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š ä¸€è¯å¤šä¹‰å’ŒåŒä¹‰è¯ï¼ˆpolysemy and synonymyï¼‰ã€‚ éšè¯­ä¹‰åˆ†æï¼šLatent Semantic Analysis (LSA) LSA is a purely statistical technique, which leverages word co-occurrence information from a large unlabeled corpus of text. LSA does not use any explicit human-organized knowledge; rather, it â€œlearnsâ€ its representation by applying Singular Value Decomposition (SVD) to the words-by-documents co-occurrence matrix. LSA is essentially a dimensionality reduction technique that identifies a number of most prominent dimensions in the data, which are assumed to correspond to â€œlatent conceptsâ€. Meanings of words and documents are then represented in the space defined by these concepts. LSA æ˜¯ä¸€ç§çº¯ç²¹çš„ç»Ÿè®¡æŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨æ¥è‡ªå¤§é‡æœªæ ‡è®°æ–‡æœ¬è¯­æ–™åº“çš„å•è¯å…±ç°ä¿¡æ¯ã€‚ LSAä¸ä½¿ç”¨ä»»ä½•æ˜ç¡®çš„äººç±»ç»„ç»‡çŸ¥è¯†; ç›¸åï¼Œå®ƒé€šè¿‡å°†å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰åº”ç”¨äºé€ä¸ªæ–‡æ¡£çš„å…±ç°çŸ©é˜µæ¥â€œå­¦ä¹ â€å…¶è¡¨ç¤ºã€‚ LSAæœ¬è´¨ä¸Šæ˜¯ä¸€ç§é™ç»´æŠ€æœ¯ï¼Œå®ƒè¯†åˆ«æ•°æ®ä¸­çš„è®¸å¤šæœ€çªå‡ºçš„ç»´åº¦ï¼Œå‡è®¾å®ƒä»¬å¯¹åº”äºâ€œæ½œåœ¨æ¦‚å¿µâ€ã€‚ ç„¶åï¼Œåœ¨è¿™äº›æ¦‚å¿µå®šä¹‰çš„ç©ºé—´ä¸­è¡¨ç¤ºå•è¯å’Œæ–‡æ¡£çš„å«ä¹‰ã€‚ è¯æ±‡æ•°æ®åº“ï¼ŒWordNet. However, lexical resources offer little information about the different word senses, thus making word sense disambiguation nearly impossible to achieve.Another drawback of such approaches is that creation of lexical resources requires lexicographic expertise as well as a lot of time and effort, and consequently such resources cover only a small fragment of the language lexicon. Specifically, such resources contain few proper names, neologisms, slang, and domain-specific technical terms. Furthermore, these resources have strong lexical orientation in that they predominantly contain information about individual words, but little world knowledge in general. è¯æ±‡èµ„æºå‡ ä¹æ²¡æœ‰æä¾›å…³äºä¸åŒè¯ä¹‰çš„ä¿¡æ¯ï¼Œå› æ­¤å‡ ä¹ä¸å¯èƒ½å®ç°è¯ä¹‰æ¶ˆæ­§ã€‚è¿™ç§æ–¹æ³•çš„å¦ä¸€ä¸ªç¼ºç‚¹æ˜¯è¯æ±‡èµ„æºçš„åˆ›å»ºéœ€è¦è¯å…¸ä¸“ä¸šçŸ¥è¯†ä»¥åŠå¤§é‡çš„æ—¶é—´å’Œç²¾åŠ›ï¼Œå› æ­¤ èµ„æºåªæ¶µç›–è¯­è¨€è¯å…¸çš„ä¸€å°éƒ¨åˆ†ã€‚ å…·ä½“è€Œè¨€ï¼Œæ­¤ç±»èµ„æºåŒ…å«å¾ˆå°‘çš„ä¸“æœ‰åç§°ï¼Œæ–°è¯ï¼Œä¿šè¯­å’Œç‰¹å®šäºåŸŸçš„æŠ€æœ¯æœ¯è¯­ã€‚ æ­¤å¤–ï¼Œè¿™äº›èµ„æºå…·æœ‰å¼ºçƒˆçš„è¯æ±‡å–å‘ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦åŒ…å«å…³äºå•ä¸ªå•è¯çš„ä¿¡æ¯ï¼Œä½†æ€»ä½“ä¸Šç¼ºä¹ä¸–ç•ŒçŸ¥è¯†ã€‚ concept å®šä¹‰ Observe that an encyclopedia consists of a large collection of articles, each of which provides a comprehensive exposition focused on a single topic. Thus, we view an encyclopedia as a collection of concepts (corresponding to articles), each accompanied with a large body of text (the article contents). ç»´åŸºç™¾ç§‘ä¸­æ¯ä¸€ä¸ªè¯æ¡å¯¹åº”ä¸€ä¸ª concept. example: å¯¹äºæ–‡æœ¬ï¼šâ€Bernanke takes chargeâ€ é€šè¿‡ç®—æ³•æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ç»´åŸºç™¾ç§‘ä¸­ç›¸å…³çš„ concept: Ben Bernanke, Federal Reserve, Chairman of the Federal Reserve, Alan Greenspan (Bernankeâ€™s predecessor), Monetarism (an economic theory of money supply and central banking), inflation and deflation. å¯¹äºæ–‡æœ¬ï¼šâ€Apple patents a Tablet Macâ€ ç›¸å…³çš„ concept: Apple Computer 2 , Mac OS (the Macintosh operating system) Laptop (the general name for portable computers, of which Tablet Mac is a specific example), Aqua (the GUI of Mac OS X), iPod (another prominent product by Apple), and Apple Newton (the name of Appleâ€™s early personal digital assistant). ESA å¯¹ä¸€ä¸ª texts çš„è¡¨ç¤ºæ˜¯ wiki ä¸­æ‰€æœ‰çš„ concept çš„ weighted combinationï¼Œè¿™é‡Œä¸ºäº†å±•ç¤ºæ–¹ä¾¿ï¼Œåªåˆ—ä¸¾äº†æœ€ç›¸å…³çš„ä¸€äº› concept. ESA(explicit semantic analysis)é€šè¿‡ wiki å¾—åˆ°ä¸€ä¸ª basic concepts: $C_1, C_2,â€¦, C_n$, å…¶ä¸­ $C_k$ éƒ½æ˜¯æ¥æºäº wiki. è¡¨ç¤ºä¸€ä¸ªé€šç”¨çš„ n ç»´è¯­ä¹‰ç©ºé—´ã€‚ ç„¶åå°†ä»»æ„é•¿åº¦çš„æ–‡æœ¬ t è¡¨ç¤ºæˆä¸ä¸Šè¿°å‘é‡é•¿åº¦ç›¸åŒçš„ æƒé‡å‘é‡ $w_1, w_2,â€¦, w_n$ åˆ†åˆ«è¡¨ç¤º t ä¸ $C_k$ ä¹‹é—´çš„ç›¸å…³ç¨‹åº¦ã€‚ æ¥ä¸‹æ¥ä¸¤ä¸ªæ­¥éª¤å°±æ˜¯ï¼š the set of basic concepts the algorithm that maps text fragments into interpretation vectors å¦‚ä½•æ„å»º concept é›†åˆ1.using Wikipedia as a Repository of Basic Concepts ç»´åŸºç™¾ç§‘è¯æ¡ä¸­çš„å†…å®¹ä¹Ÿå¾ˆå…³é”®ï¼Œç”¨æ¥è®¡ç®— concept ä¸è¾“å…¥æ–‡æœ¬ä¸­å•è¯çš„ç›¸ä¼¼åº¦ã€‚ 2.building a semantic interpreter æ ¹æ® wiki å¾—åˆ°åŸºæœ¬çš„ conceptï¼Œä»¥åŠå¯¹åº”çš„æ–‡æ¡£ï¼Œ $d_1,..,d_n$. æ„å»ºä¸€ä¸ª sparse è¡¨æ ¼ Tï¼Œ å…¶ä¸­ï¼Œåˆ—è¡¨ç¤º conceptï¼Œè¡Œè¡¨ç¤ºæ–‡æ¡£ä¸­çš„å•è¯å¯¹åº”çš„ TDIDF å€¼ã€‚ä¹Ÿå°±æ˜¯è®¡ç®—æ–‡æ¡£ä¸­çš„å•è¯ä¸æ‰€æœ‰æ–‡æ¡£ $\\bigcup_{i=1..n}d_i$ çš„é¢‘ç‡å…³ç³»ã€‚ $$T[i,j]=tf(t_i, d_j)\\cdot log\\dfrac{n}{df_i}$$ å…¶ä¸­ï¼ŒTerm Frequency - Inverse Document Frequencyï¼š TF è¡¨ç¤ºåœ¨æ–‡æ¡£ $d_j$ ä¸­ï¼Œå•è¯ $t_i$ å‡ºç°çš„é¢‘ç‡ã€‚ $$tf(t_i, d_j)=\\begin{cases} 1 + log\\ count(t_i, d_j), &amp;\\text{if count(t_i, d_j) &gt; 0} \\ 0, &amp;\\text{otherwise} \\end{cases}$$ IDF è¡¨ç¤ºé€†æ–‡æ¡£é¢‘ç‡ã€‚ååº”ä¸€ä¸ªè¯åœ¨ä¸åŒçš„æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡è¶Šå¤§ï¼Œé‚£ä¹ˆå®ƒçš„ IDF å€¼åº”è¯¥ä½ï¼Œæ¯”å¦‚ä»‹è¯â€œtoâ€ã€‚è€Œåè¿‡æ¥å¦‚æœä¸€ä¸ªè¯åœ¨æ¯”è¾ƒå°‘çš„æ–‡æœ¬ä¸­å‡ºç°ï¼Œé‚£ä¹ˆå®ƒçš„ IDF å€¼åº”è¯¥é«˜ã€‚ $$IDF=log\\dfrac{n}{df_i}$$ $df_i=|{d_k:t_i\\in d_k}|$ è¡¨ç¤ºå‡ºç°è¯¥å•è¯çš„æ–‡æ¡£ä¸ªæ•°ï¼Œn è¡¨ç¤ºæ€»çš„æ–‡æ¡£ä¸ªæ•°ã€‚ æ­£åˆ™åŒ–ï¼Œcosine normalization: $$T[i,j]\\leftarrow \\dfrac{T[i,j]}{\\sqrt{\\sum_{l=1}^r T[i,j]^2}}$$ r è¡¨ç¤ºå•è¯çš„æ€»é‡ã€‚ä¹Ÿå°±æ˜¯é™¤ä»¥æ‰€æœ‰å•è¯å¯¹åº”çš„å‘é‡äºŒèŒƒæ•°ä¹‹å’Œå¹³æ–¹ã€‚ å¾—åˆ° table T ä¹‹åï¼Œä¸€ä¸ªå•è¯çš„å‘é‡ $t_i$ è¡¨ç¤ºå°±æ˜¯ç¬¬ i è¡Œã€‚ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µ $&lt;t_1,..,t_k&gt;$ çš„å‘é‡è¡¨ç¤ºæ˜¯æ–‡æœ¬ä¸­æ‰€æœ‰å•è¯çš„è´¨å¿ƒã€‚ å¦‚ä½•å°†æ–‡æœ¬ç‰‡æ®µæ˜ å°„æˆå‘é‡è¡¨ç¤º","link":"/2019/02/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Explicit-Semantic-Analysis/"},{"title":"è®ºæ–‡ç¬”è®° Pointer Networks and copy mechanism","text":"paper: Pointer Networks, NIPS, 2015 Incorporating Copying Mechanism in Sequence-to-Sequence Learning Pointer NetworkMotivation We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. æå‡ºæ¥äº†ä¸€ç§æ–°çš„æ¶æ„æ¥å­¦ä¹ å¾—åˆ°è¿™æ ·çš„è¾“å‡ºåºåˆ—çš„æ¡ä»¶æ¦‚ç‡ï¼Œå…¶ä¸­è¾“å‡ºåºåˆ—ä¸­çš„å…ƒç´ æ˜¯è¾“å…¥åºåˆ—ä¸­ç¦»æ•£çš„ tokens. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. è¿™æ ·ç®€å•çš„ä»è¾“å…¥åºåˆ—ä¸­ copy è¾“å‡ºç›¸å…³çš„åºåˆ—åœ¨ seq2seq æˆ–æ˜¯ç¥ç»å›¾çµæœºéƒ½å¾ˆéš¾å®ç°ï¼Œå› ä¸ºåœ¨ decoder çš„æ¯ä¸€æ­¥è¾“å‡ºçš„æ¬¡çš„ç±»åˆ«ä¾èµ–äºè¾“å…¥åºåˆ—çš„é•¿åº¦ï¼Œè¿™ä¸ªé•¿åº¦æ˜¯å˜åŒ–çš„ã€‚ Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. å’Œè¿™ç±»é—®é¢˜ç±»ä¼¼çš„è¿˜æœ‰ç»™ä¸å®šé•¿åºåˆ—çš„æ’åºï¼Œç»„åˆä¼˜åŒ–ç­‰é—®é¢˜ã€‚ It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. åŒä¹‹å‰çš„ attention ä¸åŒçš„æ˜¯ï¼Œä¹‹å‰çš„ attention æ˜¯ decoder æ—¶æ¯ä¸€æ­¥è®¡ç®—é€šè¿‡ RNN ç¼–ç åçš„è¾“å…¥åºåˆ—çš„éšè—å˜é‡ä¸å½“å‰å‘é‡è¡¨ç¤ºçš„ attention vectorï¼Œç„¶åç”Ÿæˆå½“å‰è¯ã€‚è€Œ Ptr-Net åˆ™æ˜¯ä½¿ç”¨ attention ä½œä¸ºæŒ‡é’ˆï¼Œä»è¾“å…¥åºåˆ—ä¸­é€‰æ‹©æˆå‘˜ä½œä¸ºè¾“å‡ºã€‚ We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems â€“ finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem â€“ using training examples alone. Ptr-Net å¯ä»¥ç”¨æ¥å­¦ä¹ ç±»ä¼¼çš„ä¸‰ä¸ªå‡ ä½•é—®é¢˜ã€‚ Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. Ptr-Net ä¸ä»…å¯ä»¥æå‡ seq2seq with attention,è€Œä¸”èƒ½å¤Ÿæ³›åŒ–åˆ°å˜åŒ–çš„ dictionayies. ä»æ‘˜è¦ä»¥åŠ Introduction æ¥è¯´ï¼Œ Ptr-Net ä¸»è¦æ˜¯è§£å†³ä¸¤ä¸ªæ–¹é¢çš„é—®é¢˜ã€‚ ä¸€æ˜¯ï¼Œç®€å•çš„ copy åœ¨ä¼ ç»Ÿçš„æ–¹æ³•ä¸­å¾ˆéš¾å®ç°ï¼Œè€Œ Ptr-Net åˆ™æ˜¯ç›´æ¥ä»è¾“å…¥åºåˆ—ä¸­ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚ è€Œæ˜¯ï¼Œå¯ä»¥è§£å†³è¾“å‡º dictionary æ˜¯å˜åŒ–çš„æƒ…å†µã€‚æ™®é€šçš„ Seq2Seq çš„ output dictionary å¤§å°æ˜¯å›ºå®šçš„ï¼Œå¯¹è¾“å‡ºä¸­åŒ…å«æœ‰è¾“å…¥å•è¯(å°¤å…¶æ˜¯ OOV å’Œ rare word) çš„æƒ…å†µå¾ˆä¸å‹å¥½ã€‚ä¸€æ–¹é¢ï¼Œè®­ç»ƒä¸­ä¸å¸¸è§çš„å•è¯çš„ word embedding è´¨é‡ä¹Ÿä¸é«˜ï¼Œå¾ˆéš¾åœ¨ decoder æ—¶é¢„æµ‹å‡ºæ¥ï¼Œå¦ä¸€æ–¹é¢ï¼Œå³ä½¿ word embedding å¾ˆå¥½ï¼Œå¯¹ä¸€äº›å‘½åå®ä½“ï¼Œåƒäººåç­‰ï¼Œword embedding éƒ½å¾ˆç›¸ä¼¼ï¼Œä¹Ÿå¾ˆéš¾å‡†ç¡®çš„ reproduce å‡ºè¾“å…¥æåˆ°çš„å•è¯ã€‚Point Network ä»¥åŠåœ¨æ­¤åŸºç¡€ä¸Šåç»­çš„ç ”ç©¶ CopyNet ä¸­çš„ copy mechanism å°±å¯ä»¥å¾ˆå¥½çš„å¤„ç†è¿™ç§é—®é¢˜ï¼Œdecoder åœ¨å„ time step ä¸‹ï¼Œä¼šå­¦ä¹ æ€æ ·ç›´æ¥ copy å‡ºç°åœ¨è¾“å…¥ä¸­çš„å…³é”®å­—ã€‚ Model Architecture åœ¨ä»‹ç» Ptr-Net ä¹‹å‰ï¼Œä½œè€…å…ˆå›é¡¾äº†ä¸€ä¸‹åŸºæœ¬æ¨¡å‹ seq2seq å’Œ input-attention. sequence-to-sequence Modelå®é™…ä¸Š seq2seq è§£å†³çš„é—®é¢˜æ˜¯åœ¨å½“å‰æ ·æœ¬ç©ºé—´é‡Œé¢ï¼Œç»™å®šè¾“å…¥ä¸‹ï¼Œä½¿å¾—è¾“å‡ºåºåˆ—çš„æ¦‚ç‡æœ€å¤§åŒ–ã€‚å…¶å®ç±»ä¼¼çš„ MTï¼ŒQAï¼ŒSummarization éƒ½å¯ä»¥çœ‹ä½œæ˜¯è¿™ä¸€ç±»é—®é¢˜ã€‚åªä¸è¿‡æ ¹æ®è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å…³ç³»ï¼Œè°ƒæ•´ç›¸åº”çš„æ¨¡å‹ã€‚ $$p(C^P|P;\\theta)=\\sum_{i=1}^m(P)p_{\\theta}(C_i|C_1,â€¦,C_{i-1},P;\\theta)$$ é€šè¿‡è®­ç»ƒå­¦ä¹ å¾—åˆ°å‚æ•°ä½¿å¾—æ¡ä»¶æ¦‚ç‡æœ€å¤§ï¼š $$\\theta^* = {argmax}{\\theta}\\sum{P,C^P}logp(C^P|P;\\theta)$$ å…¶ä¸­ç±»å’Œæ˜¯åœ¨è®­ç»ƒæ ·æœ¬ä¸Šã€‚ In this sequence-to-sequence model, the output dictionary size for all symbols $C_i$ is fixed and equal to n, since the outputs are chosen from the input. Thus, we need to train a separate model for each n. This prevents us from learning solutions to problems that have an output dictionary with a size that depends on the input sequence length. åœ¨ seq2seq æ¨¡å‹ä¸­ï¼Œè¾“å‡ºçš„ dictionary æ˜¯å›ºå®šå¤§å°çš„ã€‚å› ä¸ºä¸èƒ½è§£å†³ dictionary æ˜¯å˜åŒ–çš„æƒ…å†µã€‚ Content Based Input Attention åœ¨æ¯ä¸€ä¸ª decoder stepï¼Œå…ˆè®¡ç®— $e_{ij}$ å¾—åˆ°å¯¹é½æ¦‚ç‡(æˆ–è€…è¯´ how well input position j matches output position i)ï¼Œç„¶ååšä¸€ä¸ª softmax å¾—åˆ° $a_{ij}$ï¼Œå†å¯¹ $a_{ij}$ åšä¸€ä¸ªåŠ æƒå’Œä½œä¸º context vector $c_i$ï¼Œå¾—åˆ°è¿™ä¸ª context vector ä¹‹ååœ¨å›ºå®šå¤§å°çš„ output dictionary ä¸Šåš softmax é¢„æµ‹è¾“å‡ºçš„ä¸‹ä¸€ä¸ªå•è¯ã€‚ This model performs significantly better than the sequence-to-sequence model on the convex hull problem, but it is not applicable to problems where the output dictionary size depends on the input. Nevertheless, a very simple extension (or rather reduction) of the model allows us to do this easily. Ptr-Netseq2seq æ¨¡å‹çš„è¾“å‡ºè¯æ˜¯åœ¨å›ºå®šçš„ dictionary ä¸­è¿›è¡Œ softmaxï¼Œå¹¶é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯ï¼Œä»è€Œå¾—åˆ°è¾“å‡ºåºåˆ—ã€‚ä½†è¿™é‡Œçš„è¾“å‡º dictionary size æ˜¯å–å†³äº input åºåˆ—çš„é•¿åº¦çš„ã€‚æ‰€ä»¥ä½œè€…æå‡ºäº†æ–°çš„æ¨¡å‹ï¼Œå…¶å®å¾ˆç®€å•ã€‚ $$u_j^i=v^Ttanh(W_1e_j+W_2d_i) ï¼Œj\\in(1,â€¦,n)$$ $$p(C_i|C_1,â€¦,C_{i-1},P)=softmax(u^i)$$ i è¡¨ç¤ºdecoder çš„æ—¶é—´æ­¥ï¼Œj è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­çš„index. æ‰€ä»¥$e_j$ æ˜¯ encoder ç¼–ç åçš„éšè—å‘é‡ï¼Œ$d_i$ æ˜¯ decoder å½“å‰æ—¶é—´æ­¥ i çš„éšè—å‘é‡ã€‚è·Ÿä¸€èˆ¬çš„ attention åŸºæœ¬ä¸Šä¸€è‡´ã€‚åªä¸è¿‡å¾—åˆ°çš„ softmax æ¦‚ç‡åº”ç”¨åœ¨è¾“å…¥åºåˆ— $C_1,â€¦,C_{i-1}$ ä¸Šã€‚ Dataset Structure TensorFlow implementation of â€œPointer Networksâ€ï¼šhttps://github.com/devsisters/pointer-network-tensorflow Datasetï¼šhttps://drive.google.com/drive/folders/0B2fg8yPGn2TCMzBtS0o4Q2RJaEU CopyNetMotivation We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. è¿˜æ˜¯å‰é¢æåˆ°çš„é—®é¢˜ï¼Œseq2seq å¾ˆéš¾è§£å†³ç®€å•çš„ copy é—®é¢˜ã€‚è€Œåœ¨äººç±»çš„å¯¹è¯ä¸­ï¼Œå‡ºç° copy çš„ç°è±¡æ˜¯å¾ˆå¸¸è§çš„ã€‚å°¤å…¶æ˜¯ å‘½ä»¤å®ä½“ æˆ–è€…æ˜¯é•¿çŸ­è¯­ã€‚ The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. è¿™ä¹Ÿæ˜¯ seq2seq æ¨¡å‹æ‰€éœ€é¢å¯¹çš„æŒ‘æˆ˜ã€‚ For example: å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äº Chandralekha è¿™ç±»å®ä½“è¯ï¼Œå¯èƒ½æ˜¯ OOVï¼Œä¹Ÿå¯èƒ½æ˜¯å…¶ä»–å®ä½“æˆ–è€…æ˜¯æ—¥æœŸç­‰å¾ˆéš¾è¢« decoder â€œè¿˜åŸâ€ å‡ºæ¥çš„ä¿¡æ¯ï¼ŒCopyNet å¯ä»¥æ›´å¥½çš„å¤„ç†è¿™ç±»çš„ä¿¡æ¯ã€‚ é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼š What to copy: è¾“å…¥ä¸­çš„å“ªäº›éƒ¨åˆ†åº”è¯¥è¢« copy? Where to paste: åº”è¯¥æŠŠè¿™éƒ¨åˆ†ä¿¡æ¯ paste åˆ°è¾“å‡ºçš„å“ªä¸ªä½ç½®ï¼Ÿ Model Architectureä½œè€…ä»ä¸¤ä¸ªè§’åº¦æ¥ç†è§£ CopyNet: From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity. ä»è®¤çŸ¥å­¦è§’åº¦ï¼Œcopyæœºåˆ¶è¿‘ä¼¼äºæ­»è®°ç¡¬èƒŒï¼Œä¸éœ€è¦å¤ªå¤šçš„ç†è§£ï¼Œä½†æ˜¯è¦ä¿è¯æ–‡å­—çš„ä¿çœŸåº¦ã€‚ From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model. ä»æ¨¡å‹çš„è§’åº¦ï¼Œcopy æ“ä½œæ›´åŠ æ­»æ¿å’Œç¬¦å·åŒ–ï¼Œè¿™ä¹Ÿä½¿å¾—ç›¸æ¯” soft attention æœºåˆ¶æ›´éš¾æ•´åˆåˆ°ä¸€ä¸ªå®Œæ•´çš„å¯å¾®åˆ†çš„ç¥ç»æ¨¡å‹ä¸­å»ã€‚ æ•´ä½“è¿˜æ˜¯åŸºäº encoder-decoder æ¨¡å‹ã€‚ Encoder: LSTM å°† source sequence è½¬æ¢ä¸ºéšè—çŠ¶æ€ M(emory) $h_1,â€¦,h_{T_S}$. Decoder: åŒ cannonical çš„ decoder ä¸€æ ·ï¼Œä½¿ç”¨ RNN è¯»å– encoder çš„éšè—çŠ¶æ€ M. ä½†å’Œä¼ ç»Ÿçš„ decoder ä¸ä¸€æ ·ï¼Œä»–æœ‰å¦‚ä¸‹åŒºåˆ«ï¼š Prediction: COPYNET predicts words based on a mixed probabilistic model of two modes, namely the generate-mode and the copymode, where the latter picks words from the source sequence. ä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ç”±ä¸¤ç§æ¨¡å¼æ··åˆè€Œæˆã€‚ç”Ÿæˆ generate-mode å’Œ copy-mode. åè€…å°±åƒå‰é¢ Ptr-Net æ‰€è¯´çš„ï¼Œåœ¨ source sentence è·å–è¯ã€‚ State Update: the predicted word at time tâˆ’1 is used in updating the state at t, but COPYNET uses not only its word-embedding but also its corresponding location-specific hidden state in M (if any). æ›´æ–° decoder ä¸­çš„éšè—çŠ¶æ€æ—¶ï¼Œt æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä¸ä»…ä¸ t-1 æ­¥ç”Ÿæˆè¯çš„ embedding vector æœ‰å…³ï¼Œè¿˜ä¸è¿™ä¸ªè¯å¯¹åº”äº source sentence ä¸­çš„éšè—çŠ¶æ€çš„ä½ç½®æœ‰å…³ã€‚ Reading M: in addition to the attentive read to M, COPYNET also hasâ€œselective readâ€ to M, which leads to a powerful hybrid of content-based addressing and location-based addressing. ä»€ä¹ˆæ—¶å€™éœ€è¦ copyï¼Œä»€ä¹ˆæ—¶å€™ä¾èµ–ç†è§£æ¥å›ç­”ï¼Œæ€ä¹ˆæ··åˆè¿™ä¸¤ç§æ¨¡å¼å¾ˆé‡è¦ã€‚ ä¸ªäººæ€è€ƒï¼š æ„Ÿè§‰ä¸ç®¡è¦ä¸è¦ copy éƒ½åº”è¯¥æ˜¯åœ¨åŸºäºç†è§£çš„åŸºç¡€ä¸Šè¿›è¡Œçš„ã€‚ä½†æ˜¯å› ä¸º OOV æˆ–è€…å½“å‰è¯çš„ embedding vector è®­ç»ƒçš„ä¸å¥½ï¼Œé‚£å°±æ— æ³•ç†è§£äº†å¯¹å§ï¼Ÿ æ˜¯å¦å¯ä»¥æ·»åŠ  gate æœºåˆ¶å‘¢ï¼Ÿ æœºå™¨åˆ°åº•è¿˜æ˜¯æ²¡ç†è§£è¯­è¨€å¯¹å§ï¼Ÿ è²Œä¼¼æ˜¯ä¸ªå¯ä»¥åˆ›æ–°çš„ç‚¹ã€‚ æ¥ä¸‹æ¥ä¼šè¯¦ç»†è®²è§£è¿™ä¸‰ä¸ªä¸åŒä¹‹å¤„æ€ä¹ˆå®ç°çš„ã€‚ Prediction with Copying and Generation:$s_t\\rightarrow y_t$è¿™éƒ¨åˆ†æ˜¯ä» decoder éšè—çŠ¶æ€ $s_t$ åˆ°è¾“å‡ºè¯ $y_t$ çš„è¿‡ç¨‹ã€‚ä¼ ç»Ÿçš„encoder-decoder æ˜¯ä¸€ä¸ªçº¿æ€§æ˜ å°„å°±å¯ä»¥äº†ã€‚ è¯è¡¨ $\\mathcal{V}={v_1,â€¦,v_N}$, æœªç™»å½•è¯ OOV(out of vocabulary) ç”¨ UNK æ¥è¡¨ç¤ºï¼ˆunkåº”è¯¥ä¹Ÿä¼šæœ‰å¯¹åº”çš„ embedding vectorï¼‰. ä»¥åŠç”¨æ¥è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­çš„ unique words $X={x_1,â€¦,x_{T_S}}$. å…¶ä¸­ X ä½¿å¾— copynet è¾“å‡º OOV. å¯¹äºä¸‰è€…æœ‰è¿™æ ·çš„é›†åˆå…³ç³»ï¼ˆå…ˆä¸è¦çœ‹å…¬å¼ï¼Œåé¢ä¼šè¯´åˆ°ï¼‰ï¼š ç®€è€Œè¨€ä¹‹(In a nutshell), å¯¹äºå½“å‰ source sentence X è¾“å‡ºçš„è¯è¡¨èŒƒå›´ $\\mathcal{V}\\cup \\text{UNK} \\cup X$. ç»™å®š decoder ä¸­å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ $s_t$, ä»¥åŠ encoder çš„éšè—çŠ¶æ€åºåˆ— M. $$p(y_t|s_t,y_{t-1},c_t,M)=p(y_t,g|s_t,y_{t-1},c_t,M) + p(y_t,c|s_t,y_{t-1},c_t,M)$$ å…¶ä¸­ g ä»£è¡¨ generate mode. c ä»£è¡¨ copy mode. æˆ‘ä»¬çŸ¥é“å¯¹äº encoder éƒ¨åˆ†çš„è¾“å‡º $h_1,â€¦,h_{T_S}$ï¼Œ è®°åš Mï¼ŒM å…¶å®åŒæ—¶åŒ…å«äº†è¯­ä¹‰å’Œä½ç½®ä¿¡æ¯ã€‚é‚£ä¹ˆ decoder å¯¹ M çš„è¯»å–æœ‰ä¸¤ç§å½¢å¼ï¼š Content-base Attentive read from word-embedding location-base Selective read from location-specific hidden units ä¸¤ç§æ¨¡å¼å¯¹åº”çš„æ¦‚ç‡è®¡ç®—ï¼Œä»¥åŠ score function: $$p(y_t,g|\\cdot)=\\begin{cases} \\dfrac{1}{Z}e^{\\psi_g(y_t)}&amp;y_t\\in V\\ 0,&amp;y_t\\in X \\bigcap \\overline V\\ \\dfrac{1}{Z}e^{\\psi_g(UNK)},&amp;y_t\\notin V\\cup X \\end{cases}$$ $$p(y_t,c|\\cdot)=\\begin{cases}\\dfrac{1}{Z}\\sum_{j:x_j=y_t}{e^{\\psi_c(x_j)}},&amp;y_t\\in X\\0&amp;\\text {otherwise}\\end{cases}$$ ä¸Šé¢ä¸¤ä¸ªå…¬å¼å åŠ (ç›¸åŠ )å¯ä»¥è¡¨ç¤ºä¸ºä¸‹å›¾ï¼ˆå¯ä»¥å°†ç›®æ ‡è¯çœ‹ä½œç±»åˆ«ä¸º 4 çš„åˆ†ç±»ã€‚ï¼‰ï¼š å…¶ä¸­ $\\psi_g(\\cdot)$ å’Œ $\\psi_c(\\cdot)$ æ˜¯ generate mode å’Œ copy mode çš„ score function. Z æ˜¯ä¸¤ç§æ¨¡å‹å…±äº«çš„å½’ä¸€åŒ–é¡¹ï¼Œ$Z=\\sum_{v\\in V\\cup{UNK}}e^{\\psi_g(v)}+\\sum_{x\\in X}e^{\\psi_c(x)}$. ç„¶åå¯¹ç›¸åº”çš„ç±»åˆ«è®¡ç®—å¯¹åº”çš„ score. Generate-Mode: $$\\psi_g(y_t=v_i)=\\nu_i^TW_os_t, v_i\\in V\\cup UNK$$ $W_o\\in R^{(N+1)\\times d_s}$ $\\nu_i$ æ˜¯ $v_i$ å¯¹åº”çš„ one-hot å‘é‡. å¾—åˆ°çš„ç»“æœæ˜¯å½“å‰è¯çš„æ¦‚ç‡ã€‚ generate-mode çš„ score $\\psi(y_t=v_i)$ å’Œæ™®é€šçš„ encoder-decoder æ˜¯ä¸€æ ·çš„ã€‚å…¨é“¾æ¥ä¹‹åçš„ softmax. copy-mode: $$\\psi(y_t=x_j)=\\sigma(h_j^TW_c)s_t,x_j\\in \\mathcal{V}$$ $h_j$ æ˜¯ encoder hidden state. j è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­çš„ä½ç½®ã€‚ $W_c\\in R^{d_h\\times d_s}$ å°† $h_j$ æ˜ å°„åˆ°è·Ÿ $s_t$ ä¸€æ ·çš„è¯­ä¹‰ç©ºé—´ã€‚ ä½œè€…å‘ç°ä½¿ç”¨ tanh éçº¿æ€§å˜æ¢æ•ˆæœæ›´å¥½ã€‚åŒæ—¶è€ƒè™‘åˆ° $y_t$ è¿™ä¸ªè¯å¯èƒ½åœ¨è¾“å…¥ä¸­å‡ºç°å¤šæ¬¡ï¼Œæ‰€ä»¥éœ€è¦è€ƒè™‘è¾“å…¥åºåˆ—ä¸­æ‰€æœ‰çš„ä¸º $y_t$ çš„è¯çš„æ¦‚ç‡çš„ç±»å’Œã€‚ state updateä¸Šé¢ä¸€éƒ¨åˆ†è®²çš„æ˜¯æ€ä¹ˆä» decoder ä¸­çš„éšè—çŠ¶æ€è®¡ç®—å¯¹åº”çš„ vocabularyï¼Œä¹Ÿå°±æ˜¯ $s_t\\rightarrow y_t$. é‚£ä¹ˆæ€ä¹ˆè®¡ç®—å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€å‘¢ï¼Ÿ æˆ‘ä»¬çŸ¥é“ä¼ ç»Ÿçš„ encoder-decoder ä¸­éšè—çŠ¶æ€å°±æ˜¯ content-based atention vector. ä½†æ˜¯åœ¨ copynet é‡Œé¢ï¼Œä½œè€…å¯¹ $y_{t-1}\\rightarrow s_t$ è¿™ä¸ªè®¡ç®—æ–¹å¼åšäº†ä¸€å®šçš„ä¿®æ”¹ã€‚ å…ˆå›é¡¾ä¸‹åŸºæœ¬çš„ attention æ¨¡å—ï¼Œdecoder ä¸­éšè—çŠ¶æ€çš„æ›´æ–° $s_t=f(y_{t-1},s_{t-1},c_t)$, å…¶ä¸­ $c_t$ ä¹Ÿå°±æ˜¯ attention æœºåˆ¶ï¼š $$c_t=\\sum_{\\tau=1}^{T_S}\\alpha_{t\\tau}$$ $$\\alpha_{t\\tau}=\\dfrac{e^{\\eta(s_{t-1},h_{\\tau})}}{\\sum_{\\tauâ€™}e^{\\eta(s_{t-1},h_{\\tauâ€™})}}$$ CopyNet çš„ $y_{t-1}$ åœ¨è¿™é‡Œæœ‰æ‰€ä¸åŒã€‚ä¸ä»…ä»…è€ƒè™‘äº†è¯å‘é‡ï¼Œè¿˜ä½¿ç”¨äº† M çŸ©é˜µä¸­ç‰¹å®šä½ç½®çš„ hidden stateï¼Œæˆ–è€…è¯´ï¼Œ$y_{tâˆ’1}$ çš„è¡¨ç¤ºä¸­å°±åŒ…å«äº†è¿™ä¸¤ä¸ªéƒ¨åˆ†çš„ä¿¡æ¯ $[e(y_{tâˆ’1});\\zeta(y_{tâˆ’1})]$ï¼Œ$e(y_{tâˆ’1})$ æ˜¯è¯å‘é‡ï¼Œåé¢å¤šå‡ºæ¥çš„ä¸€é¡¹ $\\zeta(y_{tâˆ’1})$ å«åš selective read, æ˜¯ä¸ºäº†è¿ç»­æ‹·è´è¾ƒé•¿çš„çŸ­è¯­ã€‚å’Œattention çš„å½¢å¼å·®ä¸å¤šï¼Œæ˜¯ M çŸ©é˜µä¸­ hidden state çš„åŠ æƒå’Œ. $$\\zeta(y_{t-1})=\\sum_{\\tau=1}^{T_S}\\rho_{t\\tau}h_{\\tau}$$ $$\\rho_{t\\tau}=\\begin{cases}\\dfrac{1}{K}p(x_{\\tau},c|s_{t-1},M),&amp; x_{\\tau}=y_{t-1}\\ 0,&amp; \\text{otherwise} \\end{cases}$$ å½“ $y_{t-1}$ æ²¡æœ‰å‡ºç°åœ¨ source sentenceä¸­æ—¶ï¼Œ $\\zeta(y_{t-1})=0$. è¿™é‡Œçš„ $K=\\sum{\\tauâ€™:x_{\\tauâ€™}=y_{t-1}}p(x_{\\tauâ€™},c|s_{t-1},M)$ æ˜¯ç±»å’Œã€‚è¿˜æ˜¯å› ä¸ºè¾“å…¥åºåˆ—ä¸­å¯èƒ½å‡ºç°å¤šä¸ªå½“å‰è¯ï¼Œä½†æ˜¯æ¯ä¸ªè¯åœ¨ encoder hidden state çš„å‘é‡è¡¨ç¤ºæ˜¯ä¸ä¸€æ ·çš„ï¼Œå› ä¸ºä»–ä»¬çš„æƒé‡ä¹Ÿæ˜¯ä¸ä¸€æ ·çš„ã€‚ è¿™é‡Œçš„ p æ²¡æœ‰ç»™å‡ºè§£é‡Šï¼Œæˆ‘çŒœè·Ÿå‰é¢è®¡ç®— copy çš„ score æ˜¯ä¸€è‡´çš„ï¼Ÿ ç›´è§‚ä¸Šæ¥çœ‹ï¼Œå½“ $\\zeta(y_{t-1})$ å¯ä»¥çœ‹ä½œæ˜¯é€‰æ‹©æ€§è¯»å– M (selective read). å…ˆè®¡ç®—è¾“å…¥åºåˆ—ä¸­å¯¹åº”æ‰€æœ‰ $y_{t-1}$ çš„æƒé‡ï¼Œç„¶ååŠ æƒæ±‚å’Œï¼Œä¹Ÿå°±æ˜¯ $\\zeta(y_{t-1})$. Hybrid Adressing of MåŒ…æ‹¬ä¸¤ç§ Addressing æ–¹å¼ï¼š content-based and location-based assressing. location-based Addressing: $$\\zeta(y_{t-1}) \\longrightarrow{update} \\ s_t \\longrightarrow predict \\ y_t \\longrightarrow sel. read \\zeta(y_t)$$ Learningæœ€å°åŒ–æ¦‚ç‡çš„è´Ÿå¯¹æ•°ï¼š $$L=-\\dfrac{1}{N}\\sum_{k=1}^N\\sum_{t=1}^Tlog[p(y_t^{(k)}|y_{&lt;t}^{(k)}, X^{(k)})]$$ N æ˜¯batch sizeï¼ŒT æ˜¯ object sentence é•¿åº¦ã€‚","link":"/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/"},{"title":"è®ºæ–‡ç¬”è®°-QA BiDAF","text":"paper: BiDAF:Bidirectional Attention Flow for Machine Comprehension Match-LSTM:Machine Comprehension Using Match-LSTM and Answer Pointer Motivation Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. æœºå™¨é˜…è¯»çš„å®šä¹‰ï¼Œquery å’Œ context ä¹‹é—´çš„äº¤äº’ã€‚ Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. ä¼ ç»Ÿçš„ä½¿ç”¨ attention æœºåˆ¶çš„æ–¹æ³•ã€‚ In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. æœ¬æ–‡æå‡ºçš„æ–¹æ³• BiDAF. ä½¿ç”¨å¤šé˜¶å±‚æ¬¡åŒå‘ attention flow æœºåˆ¶æ¥è¡¨ç¤ºå†…å®¹çš„ä¸åŒ levels çš„ç²’åº¦ï¼Œä»è€Œè·å¾— query-aware çš„ contextï¼Œè€Œä¸ä½¿ç”¨ summarization. Introduction Attention mechanisms in previous works typically have one or more of the following characteristics. First, the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed-size vector. Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step. Third, they are usually uni-directional, wherein the query attends on the context paragraph or the image. å¯¹ atention åœ¨ä»¥å‰çš„ç ”ç©¶ä¸­çš„ç‰¹æ€§åšäº†ä¸€ä¸ªæ€»ç»“ã€‚ 1.attention çš„æƒé‡ç”¨æ¥ä» context ä¸­æå–æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œå…¶ä¸­ context å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®š size çš„å‘é‡ã€‚ 2.åœ¨æ–‡æœ¬é¢†åŸŸï¼Œcontext ä¸­çš„è¡¨ç¤ºåœ¨æ—¶é—´ä¸Šæ˜¯åŠ¨æ€çš„ã€‚æ‰€ä»¥å½“å‰æ—¶é—´æ­¥çš„ attention æƒé‡ä¾èµ–äºä¹‹å‰æ—¶é—´æ­¥çš„å‘é‡ã€‚ 3.å®ƒä»¬é€šå¸¸æ˜¯å•å‘çš„ï¼Œç”¨ query æŸ¥è¯¢å†…å®¹æ®µè½æˆ–å›¾åƒã€‚ Model ArchitectureBiDAF ç›¸æ¯”ä¼ ç»Ÿçš„å°† attention åº”ç”¨äº MC ä»»åŠ¡ä½œå‡ºå¦‚ä¸‹æ”¹è¿›: First, our attention layer is not used to summarize the context paragraph into a fixed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer. This reduces the information loss caused by early summarization. 1ï¼‰å¹¶æ²¡æœ‰æŠŠ context ç¼–ç åˆ°å›ºå®šå¤§å°çš„å‘é‡è¡¨ç¤ºä¸­ï¼Œè€Œæ˜¯è®©æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®—å¾—åˆ°çš„ attended vactor å¯ä»¥æµåŠ¨ï¼ˆåœ¨ modeling layer é€šè¿‡ biLSTM å®ç°ï¼‰è¿™æ ·å¯ä»¥å‡å°‘æ—©æœŸåŠ æƒå’Œé€ æˆçš„ä¿¡æ¯ä¸¢å¤±ã€‚ Second, we use a memory-less attention mechanism. That is, while we iteratively compute attention through time as in Bahdanau et al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step. 2ï¼‰memory-lessï¼Œåœ¨æ¯ä¸€ä¸ªæ—¶åˆ»ï¼Œä»…ä»…å¯¹ query å’Œå½“å‰æ—¶åˆ»çš„ context paragraph è¿›è¡Œè®¡ç®—ï¼Œå¹¶ä¸ç›´æ¥ä¾èµ–ä¸Šä¸€æ—¶åˆ»çš„ attention. We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer. It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation (the output of the attention layer). It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps. ä¹Ÿå°±æ˜¯å¯¹ attention layer å’Œ modeling layer è¿›è¡Œåˆ†å·¥ï¼Œå‰è€…å…³æ³¨äº context å’Œ query ä¹‹é—´çš„äº¤äº’ã€‚è€Œåè€…åˆ™å…³æ³¨äº query-aware context ä¸­è¯äºè¯ä¹‹é—´çš„äº¤äº’ï¼Œä¹Ÿå°±æ˜¯åŠ æƒäº† attention weights ä¹‹åçš„ context è¡¨ç¤ºã€‚è¿™ä½¿å¾— attention åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸å—ä¹‹å‰é”™è¯¯çš„å½±å“ã€‚ Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other. è®¡ç®—äº† query-to-contextï¼ˆQ2Cï¼‰ å’Œ context-to-queryï¼ˆC2Qï¼‰ä¸¤ä¸ªæ–¹å‘çš„ attention ä¿¡æ¯ï¼Œè®¤ä¸º C2Q å’Œ Q2C å®é™…ä¸Šèƒ½å¤Ÿç›¸äº’è¡¥å……ã€‚å®éªŒå‘ç°æ¨¡å‹åœ¨å¼€å‘é›†ä¸Šå»æ‰ C2Q ä¸ å»æ‰ Q2C ç›¸æ¯”ï¼Œåˆ†åˆ«ä¸‹é™äº† 12 å’Œ 10 ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾ç„¶ C2Q è¿™ä¸ªæ–¹å‘ä¸Šçš„ attention æ›´ä¸ºé‡è¦ è®ºæ–‡æå‡º6å±‚ç»“æ„ï¼š Character Embedding Layer and Word Embedding Layer -&gt; Contextual Embedding Layer -&gt; Attention Flow Layer -&gt; Modeling Layer -&gt; Output Layer Character Embedding Layer and word embedding alyer charatter embedding of each word using CNN.The outputs of the CNN are max-pooled over the entire width to obtain a fixed-size vector for each word. pre-trained word vectors, GloVe concatenation of them above and is passed to a two-layer highway networks. context -&gt; $X\\in R^{d\\times T}$ query -&gt; $Q\\in R^{d\\times J}$ contextual embedding layermodel the temporal interactions between words using biLSTM. context -&gt; $H\\in R^{2d\\times T}$ query -&gt; $U\\in R^{2d\\times J}$ å‰ä¸‰å±‚ç½‘ç»œæ˜¯åœ¨ä¸åŒçš„ç²’åº¦å±‚é¢æ¥æå– context å’Œ query çš„ç‰¹å¾ã€‚ attention flow layer the attention flow layer is not used to summarize the query and context into single feature vectors. Instead, the attention vector at each time step, along with the embeddings from previous layers, are allowed to flow through to the subsequent modeling layer. è¾“å…¥æ˜¯ H å’Œ Gï¼Œè¾“å‡ºæ˜¯ query-aware vector G, ä»¥åŠä¸Šä¸€å±‚çš„ contextual layer. è¿™ä¸€å±‚åŒ…å«ä¸¤ä¸ª attentionï¼ŒContext-to-query Attention å’Œ Query-to-context Attention. å®ƒä»¬å…±äº«ç›¸ä¼¼çŸ©é˜µ $S\\in R^{T\\times J}$(ä¸æ˜¯ç®€å•çš„çŸ©é˜µç›¸ä¹˜ï¼Œè€Œæ˜¯ç±»ä¼¼äº Dynamic Memory Networks ä¸­çš„è®¡ç®—æ–¹å¼). $$S_{tj}=\\alpha(H_{:t},U_{:j})\\in R$$ å…¶ä¸­ $\\alpha(h,u)=w_{(S)}^T[h,u,h\\circ u]$, $w_{(S)}\\in R^{6d}$ Context-to-query Attention: è®¡ç®—å¯¹æ¯ä¸€ä¸ª context word è€Œè¨€å“ªäº› query words å’Œå®ƒæœ€ç›¸å…³ã€‚æ‰€ä»¥ è®¡ç®— t-th context word å¯¹åº”çš„ query æ¯ä¸ªè¯çš„æƒé‡: $$a_t=softmax(S_{t:})\\in R^J$$ ç„¶åå°†æƒé‡èµ‹äºˆåˆ° query ä¸Šç„¶åå†åŠ æƒæ±‚å’Œ(å åŠ èµ‹äºˆäº†æƒé‡çš„ query ä¸­çš„æ¯ä¸€ä¸ªè¯)ï¼Œå¾—åˆ° t-th å¯¹åº”çš„ query-aware query: $$\\tilde U_{:t}=\\sum_j a_{tj}U_{:j}\\in R^{2d}$$ ç„¶å context ä¸­çš„æ¯ä¸€ä¸ªè¯éƒ½è¿™æ ·è®¡ç®—ï¼Œ$\\tilde U\\in R^{2d\\times T}$ å°±æ˜¯é€šè¿‡ context å’Œ query è®¡ç®—ç›¸ä¼¼æ€§åï¼Œé€šè¿‡ sortmax è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œç„¶åä½œä¸ºæƒé‡èµ‹äºˆåˆ° query ä¸Šï¼Œå¾—åˆ° context æ¯ä¸€ä¸ªè¯å¯¹åº”çš„ attended-query. Query-to-context Attention: è·Ÿ C2Q ä¸€æ ·è®¡ç®—ç›¸ä¼¼çŸ©é˜µ S åï¼Œè®¡ç®—å¯¹æ¯ä¸€ä¸ª query word è€Œè¨€å“ªäº› context words å’Œå®ƒæœ€ç›¸å…³ï¼Œè¿™äº› context words å¯¹å›ç­”é—®é¢˜å¾ˆé‡è¦ã€‚ å…ˆè®¡ç®—ç›¸å…³æ€§çŸ©é˜µæ¯ä¸€åˆ—ä¸­çš„æœ€å¤§å€¼ï¼Œmax function $max_{col}(S)\\in R^T$, ç„¶åsoftmaxè®¡ç®—æ¦‚ç‡: $$b=softmax(max_{col}(S))\\in R^T$$ æƒé‡ b è¡¨ç¤ºä¸æ•´ä¸ª query æ¯”è¾ƒä¹‹åï¼Œcontext ä¸­æ¯ä¸€ä¸ªè¯çš„é‡è¦ç¨‹åº¦ï¼Œç„¶åä¸ context åŠ æƒå’Œï¼š $$\\tilde h = \\sum_tb_tH_{:t}\\in R^{2d}$$ åœ¨ tile T æ¬¡åå¾—åˆ° $\\tilde H\\in R^{2d\\times T}$. æ¯”è¾ƒ C2Q å’Œ Q2Cï¼Œæ˜¾ç„¶ Q2C æ›´é‡è¦ï¼Œå› ä¸ºæœ€ç»ˆæˆ‘ä»¬è¦æ‰¾çš„ç­”æ¡ˆæ˜¯ context ä¸­çš„å†…å®¹ã€‚è€Œä¸”ä¸¤è€…çš„ attention è®¡ç®—æ–¹å¼æœ‰åŒºåˆ«æ˜¯ï¼šå¯¹ query è¿›è¡ŒåŠ æƒå’Œæ—¶ï¼Œæˆ‘ä»¬è€ƒè™‘çš„æ˜¯ context ä¸­çš„æ¯ä¸€ä¸ªè¯ï¼Œè€Œåœ¨å¯¹ context è¿›è¡ŒåŠ æƒå’Œæ—¶ï¼Œæˆ‘ä»¬è¦è€ƒè™‘æ‰€æœ‰çš„ query ä¸­ç›¸å…³æ€§æœ€å¤§çš„è¯ï¼Œæ˜¯å› ä¸º context ä¸­æŸä¸ªè¯åªè¦ä¸ query ä¸­ä»»ä½•ä¸€ä¸ªè¯æœ‰å…³ï¼Œéƒ½éœ€è¦è¢« attend. å°†ä¸‰ä¸ªçŸ©é˜µæ‹¼æ¥èµ·æ¥ï¼Œå¾—åˆ° G: $$G_{:t}=\\beta (H_{:t},\\tilde U_{:t}, \\tilde H_{:t})\\in R^{d_G}$$ function $\\beta$ å¯ä»¥æ˜¯ multi-layers perceptron. åœ¨ä½œè€…çš„å®éªŒä¸­ï¼š $$\\beta(h,\\tilde u,\\tilde h)=[h;\\tilde u;h\\circ \\tilde u;h\\circ \\tilde h]\\in R^{8d\\times T}$$ Modeling Layercaptures the interaction among the context words conditioned on the query. ä½¿ç”¨ biLSTM, å•å‘ LSTM çš„è¾“å‡ºç»´åº¦æ˜¯dï¼Œæ‰€ä»¥æœ€ç»ˆè¾“å‡ºï¼š $M\\in R^{2d\\times T}$. Output Layerè¾“å‡º layer æ˜¯åŸºäºåº”ç”¨ç¡®å®šçš„ã€‚å¦‚æœæ˜¯ QAï¼Œå°±ä»æ®µè½ä¸­æ‰¾å‡º start p1 å’Œ end p2. è®¡ç®— start index: $$p^1=softmax(W^T(p^1)[G;M])$$ å…¶ä¸­ $w_{(p^1)}\\in R^{10d}$ è®¡ç®— end indexï¼Œå°† M é€šè¿‡å¦ä¸€ä¸ª biLSTM å¤„ç†ï¼Œå¾—åˆ° $M^2\\in R^{2d\\times T}$ $$p^2=softmax(W^T(p^2)[G;M^2])$$ Trainingç›®æ ‡æŸå¤±å‡½æ•°ï¼š $$L(\\theta)=-{1 \\over N} \\sum^N_i[log(p^1_{y_i^1})+log(p^2_{y_i^2})]$$ $\\theta$ åŒ…æ‹¬å‚æ•°ï¼š the weights of CNN filters and LSTM cells $w_{S}$,$w_{p^1},w_{p^2}$ $y_i^1,y_i^2$ è¡¨ç¤ºiæ ·æœ¬ä¸­å¼€å§‹å¯ç»“æŸä½ç½®åœ¨ context ä¸­çš„ index. $p^1,p^2\\in R^T$ æ˜¯ç»è¿‡ softmax å¾—åˆ°çš„æ¦‚ç‡ï¼Œå¯ä»¥å°† gold truth çœ‹ä½œæ˜¯ one-hot å‘é‡ [0,0,â€¦,1,0,0,0]ï¼Œæ‰€ä»¥å¯¹å•ä¸ªæ ·æœ¬äº¤å‰ç†µæ˜¯: $$- log(p^1_{y_i^1})-log(p^2_{y_i^2})$$ TestThe answer span $(k; l)$ where $k \\le l$ with the maximum value of $p^1_kp^2_l$ is chosen, which can be computed in linear time with dynamic programming.","link":"/2018/08/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QA%20BiDAF/"},{"title":"è®ºæ–‡ç¬”è®°-character embedding and ELMO","text":"paper: Character-Aware Neural Language Models paper: Deep contextualized word representations character embeddingMotivation A language model is formalized as a probability distribution over a sequence of strings (words), and traditional methods usually involve making an n-th order Markov assumption and estimating n-gram probabilities via counting and subsequent smoothing (Chen and Goodman 1998). The count-based models are simple to train, but probabilities of rare n-grams can be poorly estimated due to data sparsity (despite smoothing techniques). å¯¹è¯­è¨€æ¨¡å‹çš„æè¿°ï¼šè¯­è¨€æ¨¡å‹æ˜¯ ä¸€ä¸ªå•è¯åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒ çš„å½¢å¼åŒ–æè¿°ï¼ˆä»€ä¹ˆæ„æ€ï¼Ÿå°±æ˜¯æ¯”å¦‚è¿™ä¸ªå¥å­é•¿åº¦ä¸º 10, é‚£ä¹ˆæ¯ä¸ªä½ç½®å¯èƒ½æ˜¯è¯è¡¨ä¸­çš„ä»»æ„ä¸€ä¸ªè¯ï¼Œè€Œå‡ºç°å½“å‰è¯æ˜¯æœ‰ä¸€ä¸ªæ¦‚ç‡çš„, è¿™ä¸ªæ¦‚ç‡æ˜¯ä¾èµ–äºä¹‹å‰çš„è¯çš„ï¼‰ã€‚ åœ¨ä¼ ç»Ÿçš„æ–¹æ³•ä¸»è¦æ˜¯è¿ç”¨ né˜¶é©¬å°”å¯å¤«å‡è®¾æ¥ä¼°è®¡ n-gram çš„æ¦‚ç‡ï¼Œé€šè¿‡ç»Ÿè®¡è®¡æ•°ï¼Œä»¥åŠå­åºåˆ—å¹³æ»‘çš„æ–¹å¼ã€‚è¿™ç§åŸºäºè®¡æ•°çš„æ¨¡å‹è™½ç„¶ç®€å•ï¼Œä½†æ˜¯åœ¨æ•°æ®ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œå¯¹ä¸å¸¸è§çš„ n-gram çš„æ¦‚ç‡ä¼°è®¡ä¼šå¾ˆå·®ã€‚ While NLMs have been shown to outperform count-based n-gram language models (Mikolov et al. 2011), they are blind to subword information (e.g. morphemes). For example, they do not know, a priori, that eventful, eventfully, uneventful, and uneventfully should have structurally related embeddings in the vector space. Embeddings of rare words can thus be poorly estimated, leading to high perplexities for rare words (and words surrounding them). This is especially problematic in morphologically rich languages with long-tailed frequency distributions or domains with dynamic vocabularies (e.g. social media). neural language models å°†è¯åµŒå…¥åˆ°ä½ç»´çš„å‘é‡ä¸­ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´çš„ä½ç½®ä¹Ÿæ˜¯ç›¸è¿‘çš„ã€‚ç„¶å Mikolov word2vec è¿™ç§æ–¹å¼ä¸èƒ½æœ‰æ•ˆçš„è§£å†³å­å•è¯çš„ä¿¡æ¯é—®é¢˜ï¼Œæ¯”å¦‚ä¸€ä¸ªå•è¯çš„å„ç§å½¢æ€ï¼Œä¹Ÿä¸èƒ½è®¤è¯†å‰ç¼€ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œä¸å¯é¿å…çš„ä¼šé€ æˆä¸å¸¸è§è¯çš„å‘é‡è¡¨ç¤ºä¼°è®¡å¾ˆå·®ï¼Œå¯¹äºä¸å¸¸è§è¯ä¼šæœ‰è¾ƒé«˜çš„å›°æƒ‘åº¦ã€‚è¿™å¯¹äºè¯è¯­å½¢æ€å¾ˆä¸°å¯Œçš„è¯­è¨€æ˜¯ä¸€ä¸ªéš¾é¢˜ï¼ŒåŒæ ·è¿™ç§é—®é¢˜ä¹Ÿæ˜¯åŠ¨æ€è¯è¡¨çš„é—®é¢˜æ‰€åœ¨ï¼ˆæ¯”å¦‚ç¤¾äº¤åª’ä½“ï¼‰ã€‚ Recurrent Neural Network Language Modelç»™å®šè¯è¡¨ä¸º Vï¼Œä¹‹å‰çš„åºåˆ—æ˜¯ $w_{1:t}=[w_1,..,w_t]$,åœ¨ RNN-LM ä¸­é€šè¿‡å…¨é“¾æ¥ affine transformation è®¡ç®— $w_{t+1}$ ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼š $$Pr(w_{t+1}=j|w_{1:t})=\\dfrac{exp(h_t\\cdot p^j+q^j)}{\\sum_{jâ€™\\in V}exp(h_t\\cdot p^{jâ€™}+q^{jâ€™})}$$ å…¶ä¸­ $h_t$ æ˜¯å½“å‰ t æ—¶åˆ»çš„éšè—çŠ¶æ€ã€‚ä¹Ÿå°±æ˜¯å…ˆé€šè¿‡å…¨é“¾æ¥æ˜ å°„åˆ°è¯è¡¨çš„ V çš„ç»´åº¦ï¼Œç„¶åé€šè¿‡ softmax è®¡ç®—å…¶æ˜¯è¯è¡¨ä¸­ç¬¬ j ä¸ªè¯çš„æ¦‚ç‡ã€‚ ç„¶åå‡è®¾è®­ç»ƒé¢„æ–™åº“çš„ sentence æ˜¯ $w_{1:T}=[w_1,â€¦,w_T]$,é‚£ä¹ˆè®­ç»ƒä¹Ÿå°±æ˜¯æœ€å°åŒ–è¿™ä¸ªåºåˆ—çš„ ä¼¼ç„¶æ¦‚ç‡çš„è´Ÿå¯¹æ•°ï¼š $$NLL=-\\sum_{T}^{t=1}logPr(w_t|w_{1:t-1})$$ Chracter-level Convolution Neural Network ä»¥å•è¯ absurdity ä¸ºä¾‹ï¼Œæœ‰ l ä¸ªå­—ç¬¦ï¼ˆé€šå¸¸ä¼š padded åˆ°ä¸€ä¸ªå›ºå®šsizeï¼‰ï¼Œé€šè¿‡ character embedding æ˜ å°„æˆçŸ©é˜µ $C\\in R^{d\\times l}$. d æ˜¯ embedding size. å›¾ä¸­ embedding size ä¸º 4. ç„¶åä½¿ç”¨å·ç§¯æ ¸ kernel H åšå·ç§¯è¿ç®—, $H\\in R^{d\\times w}$ï¼Œæ‰€ä»¥å¾—åˆ°çš„ feature map $f^k\\in R^{l-w+1}$. è·Ÿä¹‹å‰ CNN åšæ–‡æœ¬åˆ†ç±»å…¶å®æŒºåƒçš„, kernel çš„é•¿æ˜¯ embedding size d, å®½åº¦ w åˆ†åˆ«æ˜¯ 2,3,4. ä¸Šå›¾ä¸­è“è‰²åŒºåŸŸä¸ºä¾‹ï¼Œfilter å®½åº¦ä¸º 2 çš„ä¸ªæ•°æ˜¯3, é‚£ä¹ˆå·ç§¯å¾—åˆ°çš„ featur map æ˜¯ $3 \\times (9-2+1) = 3\\times 8$. $$f^k[i]=tanh(&lt;C^k[* ,i:i-w+1], H&gt; +b)$$ &lt;&gt;è¡¨ç¤ºåšå·ç§¯è¿ç®—(Frobenius inner product). ç„¶ååŠ ä¸Š bias å’Œ éçº¿æ€§æ¿€æ´»å‡½æ•° tanh. æ¥ç€åŸºäº times ç»´åº¦åš max pooling. ä¸Šå›¾ä¸­ filter å®½åº¦ä¸º 3,2,4 çš„ä¸ªæ•°åˆ†åˆ«ä¸º 4,3,5.æ‰€ä»¥å¾—åˆ°é•¿åº¦ä¸º 4+3+5=12 çš„å‘é‡ã€‚ è¿™é‡Œæ¯ä¸€ä¸ª filter matrix å¾—åˆ°ä¸€ä¸ªç›¸åº”çš„ç‰¹å¾ feature. åœ¨é€šå¸¸çš„ NLP ä»»åŠ¡ä¸­è¿™äº› filter çš„æ€»æ•° $h\\in[100, 1000]$ Highway Networké€šè¿‡å·ç§¯å±‚å¾—åˆ°å•è¯ k çš„å‘é‡è¡¨ç¤ºä¸º $y^k$. Highway Network åˆ†ä¸ºä¸¤å±‚ layer. one layer of an MLP applies an affine transformation: $$z=g(W_y+b)$$ one layer æœ‰ç‚¹ç±»ä¼¼ LSTM ä¸­çš„ gate æœºåˆ¶ï¼š $$z=t\\circ g(W_Hy+b_H)+(1-t)\\circ y$$ å…¶ä¸­ g æ˜¯éçº¿æ€§å‡½æ•°ã€‚$t=\\sigma(W_Ty+b_T)$. t æˆä¸º transform gate, (1-t) æ˜¯ carry gate. åŒ LSTM ç±»ä¼¼ï¼Œ highway network å…è®¸è¾“å‡ºèƒ½è‡ªé€‚åº”çš„ä» $y^k$ ä¸­ç›´æ¥è·å–ä¿¡æ¯ã€‚ ELMoä¼ ç»Ÿçš„æå– word embedding çš„æ–¹æ³•ï¼Œæ¯”å¦‚ word2vec å’Œ language modelï¼Œ å‰è€…æ˜¯é€šè¿‡è¯ä¸è¯ä¹‹é—´çš„å…±ç°ï¼Œåè€…æ˜¯ contextualï¼Œä½†ä»–ä»¬éƒ½æ˜¯è·å¾—å›ºå®šçš„ embeddingï¼Œä¹Ÿå°±æ˜¯æ¯ä¸€ä¸ªè¯å¯¹åº”ä¸€ä¸ªå•ä¸€çš„ embedding. è€Œå¯¹äºå¤šä¹‰è¯æ˜¾ç„¶è¿™ç§åšæ³•ä¸ç¬¦åˆç›´è§‰, è€Œå•è¯çš„æ„æ€åˆå’Œä¸Šä¸‹æ–‡ç›¸å…³, ELMoçš„åšæ³•æ˜¯æˆ‘ä»¬åªé¢„è®­ç»ƒ language model, è€Œ word embedding æ˜¯é€šè¿‡è¾“å…¥çš„å¥å­å®æ—¶è¾“å‡ºçš„, è¿™æ ·å•è¯çš„æ„æ€å°±æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³çš„äº†, è¿™æ ·å°±å¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†æ­§ä¹‰çš„å‘ç”Ÿ. ä¸” ELMo è¾“å‡ºå¤šä¸ªå±‚çš„ embedding è¡¨ç¤º, è¯•éªŒä¸­å·²ç»å‘ç°æ¯å±‚ LM è¾“å‡ºçš„ä¿¡æ¯å¯¹äºä¸åŒçš„ä»»åŠ¡æ•ˆæœä¸åŒ, å› æ­¤å¯¹æ¯ä¸ª token ç”¨ä¸åŒå±‚ embedding è¡¨ç¤ºä¼šæå‡æ•ˆæœ. ä¸ªäººè§‰å¾—ï¼Œå¯ä»¥ä»è¿™ä¸ªè§’åº¦å»ç†è§£ã€‚RNN å¯ä»¥çœ‹åšä¸€ä¸ªé«˜é˜¶é©¬å°”å¯å¤«é“¾ï¼Œè€Œä¸åŒäº é©¬å°”å¯å¤«æ¨¡å‹ï¼ŒRNN ä¸­çš„çŠ¶æ€è½¬ç§»çŸ©é˜µæ˜¯ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿçš„ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è®¡ç®—éšè—å±‚æ‰€ç”¨çš„ $h_t=tanh(w_{hh}h_{t-1}+w_{hx}x_t)$. è¿™ä¸ªçŠ¶æ€è½¬ç§»æ˜¯åŠ¨æ€çš„ï¼Œä¹Ÿæ˜¯ä¸æ–­æ›´æ–°çš„ã€‚è€Œä½¿ç”¨ è¯­è¨€æ¨¡å‹ æ¥è®­ç»ƒ RNN/LSTM ç›®çš„å°±æ˜¯å¾—åˆ°è¿™æ ·çš„ä¸€å¥—å‚æ•°ï¼Œä½¿å¾—å®ƒèƒ½å­¦ä¹ åˆ°ä»»ä½• åˆç†çš„ï¼Œè‡ªç„¶çš„ sentence. æ‰€ä»¥ï¼Œè¿™ä¸ªè¯­æ–™åº“è¶Šå¤§è¶Šå¥½ã€‚äº‹å®ä¸Šï¼Œæœ‰ç›‘ç£çš„è®­ç»ƒä¹Ÿå¯ä»¥è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œä½†æ˜¯æœ‰ç›‘ç£çš„æ•°æ®æœ‰é™ï¼Œå¹¶ä¸”æ•´ä¸ªæ¨¡å‹æ˜¯æœ‰åç½®çš„ï¼Œæ¯”å¦‚æ–‡æœ¬åˆ†ç±»çš„ä»»åŠ¡å»è®­ç»ƒï¼Œé‚£ä¹ˆå®ƒæ›´å€¾å‘äº å±€éƒ¨ä¿¡æ¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœºå™¨ç¿»è¯‘ä½œä¸ºæœ‰ç›‘ç£çš„æ•ˆæœä¼šæ›´å¥½ï¼Œæœ€å¥½çš„è¿˜æ˜¯è¯­è¨€æ¨¡å‹å‘¢ï¼Œä¸ä»…å¯ç”¨çš„æ•°æ®é‡å¾ˆå¤§ï¼Œè€Œä¸”å› ä¸ºè¦é¢„æµ‹æ¯ä¸€ä¸ªè¯çš„ä¿¡æ¯ï¼Œå®ƒä¼šåŠªåŠ›ç»“åˆæ¯ä¸€ä¸ªè¯çš„ä¸Šä¸‹æ–‡å»å­¦ä¹ è¿™ä¸ªè¯çš„è¡¨ç¤ºã€‚è¿™ä¹Ÿæ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„ã€‚ELMo å’Œ BERT éƒ½æ˜¯è¿™æ ·çš„é“ç†ï¼Œè€Œ BERT çš„ä¼˜åŠ¿å‰ä¸€ç¯‡ blog è¯´è¿‡äº†ã€‚ Bidirectional language modelsç»™å®š sentence $t_1, t_2,â€¦,t_N$, é€šè¿‡å‰é¢çš„è¯ $t_1,..,t_{k-1}$ è®¡ç®— token $t_k$ çš„æ¦‚ç‡åˆ†å¸ƒ: åå‘ï¼š è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå°±æ˜¯é‡‡ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæœ€å¤§åŒ–è¿™ä¸ªæ¦‚ç‡ï¼š ä¼ ç»Ÿçš„æ–¹æ³•å°±æ˜¯ æå–å‡ºå¯¹åº”ä½ç½®çš„å‘é‡è¡¨ç¤ºä½œä¸ºå¯¹åº”ä½ç½®çš„è¯å‘é‡ context-independent token representation $x_k^{LM}$. ELMo ELMo is a task specific combination of the intermediate layer representations in the biLM. ELMo å®é™…ä¸Šåªæ˜¯ä¸‹æ¸¸ä»»åŠ¡çš„ä¸­é—´å±‚ï¼Œè·Ÿ BERT ä¸€æ ·ã€‚ä½†ä¹Ÿæœ‰ä¸åŒçš„æ˜¯ï¼Œ ELMo æ¯ä¸€å±‚çš„å‘é‡è¡¨ç¤ºä¼šè·å¾—ä¸åŒçš„ ä¿¡æ¯ã€‚åº•å±‚æ›´èƒ½æ•æ‰ syntax and semantics ä¿¡æ¯ï¼Œæ›´é€‚ç”¨äº part-of-speech tagging ä»»åŠ¡ï¼Œé«˜å±‚æ›´èƒ½è·å¾— contextual ä¿¡æ¯ï¼Œæ›´é€‚ç”¨äº word sense disambiguation ä»»åŠ¡ã€‚æ‰€ä»¥å¯¹ä¸åŒçš„ä»»åŠ¡ï¼Œä¼šå¯¹ä¸åŒå±‚çš„å‘é‡è¡¨ç¤ºçš„åˆ©ç”¨ä¸åŒã€‚ åœ¨ä½¿ç”¨ ELMo è¿›è¡Œä¸‹æ¸¸æœ‰ç›‘ç£è®­ç»ƒæ—¶ï¼Œé€šå¸¸æ˜¯è¿™æ · $[x_k; ELMo_k^{task}]$. å¯¹äº SQuAD è¿™æ ·çš„ä»»åŠ¡ï¼Œ$[h_k, ELMo_k^{task}]$. Model architecture The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation. å…·ä½“æ¨¡å‹è¿˜æ˜¯å¾—çœ‹ä»£ç ã€‚","link":"/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/"},{"title":"è®ºæ–‡ç¬”è®°-Multi-cast Attention Networks","text":"paper: [Multi-Cast Attention Networks for Retrieval-based Question Answering and Response Prediction](https://arxiv.org/abs/1806.00778) Motivation Our approach performs a series of soft attention operations, each time casting a scalar feature upon the inner word embeddings. The key idea is to provide a real-valued hint (feature) to a subsequent encoder layer and is targeted at improving the representation learning process. åœ¨ encoder layer ä¹‹å‰å°† document å’Œ query è¿›è¡Œäº¤äº’ï¼Œç„¶åå°†æƒé‡èµ‹äºˆåˆ° document å’Œ query ä¹‹åï¼Œåœ¨é€šè¿‡ contextual/encoder layer ç¼–ç èåˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å‘é‡è¡¨ç¤ºã€‚è¿™æ ·åšçš„ç›®åœ°ï¼Œæ˜¯ä¸ºåç»­å±‚æä¾›æç¤ºï¼ˆç‰¹å¾ï¼‰ï¼Œæå‡è¡¨ç¤ºå­¦ä¹ çš„æ€§èƒ½ã€‚ The key idea of attention is to extract only the most relevant information that is useful for prediction. In the context of textual data, attention learns to weight words and sub-phrases within documents based on how important they are. In the same vein, co-attention mechanisms [5, 28, 50, 54] are a form of attention mechanisms that learn joint pairwise attentions, with respect to both document and query. attention æ³¨æ„åŠ›çš„å…³é”®æ€æƒ³æ˜¯ä»…æå–å¯¹é¢„æµ‹æœ‰ç”¨çš„æœ€ç›¸å…³ä¿¡æ¯ã€‚åœ¨æ–‡æœ¬æ•°æ®çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæ³¨æ„åŠ›å­¦ä¹ æ ¹æ®æ–‡æ¡£ä¸­çš„å•è¯å’Œå­çŸ­è¯­çš„é‡è¦æ€§æ¥å¯¹å®ƒä»¬è¿›è¡ŒåŠ æƒã€‚ Attention is traditionally used and commonly imagined as a feature extractor. Itâ€™s behavior can be thought of as a dynamic form of pooling as it learns to select and compose different words to form the final document representation. ä¼ ç»Ÿçš„ attention å¯ä»¥çœ‹åšä¸ºä¸€ä¸ªç‰¹å¾æå–å™¨ã€‚å®ƒçš„è¡Œä¸ºå¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ç§åŠ¨æ€çš„pooingå½¢å¼ï¼Œå› ä¸ºå®ƒå­¦ä¹ é€‰æ‹©å’Œç»„åˆä¸åŒçš„è¯æ¥å½¢æˆæœ€ç»ˆçš„æ–‡æ¡£è¡¨ç¤ºï¼Œã€‚ This paper re-imagines attention as a form of feature augmentation method. Attention is casted with the purpose of not compositional learning or pooling but to provide hints for subsequent layers. To the best of our knowledge, this is a new way to exploit attention in neural ranking models. è¿™ç¯‡ paper å°† attention é‡æ–°è®¾æƒ³ä¸ºä¸€ç§ç‰¹å¾å¢å¼ºçš„æ–¹å¼ï¼ŒAttentionçš„ç›®çš„ä¸æ˜¯ç»„åˆå­¦ä¹ æˆ–æ±‡é›†ï¼Œè€Œæ˜¯ä¸ºåç»­å±‚æä¾›æç¤ºï¼ˆç‰¹å¾ï¼‰ã€‚è¿™æ˜¯ä¸€ç§åœ¨ç¥ç»æ’åºæ¨¡å‹ä¸­çš„æ–°æ–¹æ³•ã€‚ ä¸ç®¡è¿™ç¯‡paperæä¾›çš„æ–°çš„ attention ä½¿ç”¨æ–¹å¼æ˜¯å¦æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œä½†è¿™é‡Œå¯¹ attention çš„å¾ˆå¤šè§£é‡Šè®©äººè€³ç›®ä¸€æ–°ï¼Œå¯ä»¥è¯´ç†è§£çš„å¾ˆé€å½»äº†ã€‚ An obvious drawback which applies to many existing models is that they are generally restricted to one attention variant. In the case where one or more attention calls are used (e.g., co-attention and intra-attention, etc.), concatenation is generally used to fuse representations [20, 28]. Unfortunately, this incurs cost in subsequent layers by doubling the representation size per call. å¾ˆå¤š paper ä¸­åªå—é™äºä¸€ç§ attentionï¼Œè¿™æ˜æ˜¾æ˜¯ä¸å¤Ÿå¥½çš„ã€‚ä¹Ÿæœ‰ä½¿ç”¨å¤šç§ attention çš„ï¼Œæ¯”å¦‚ co-attention å’Œ intra-attentionï¼Œ ç„¶åç›´æ¥æ‹¼æ¥èµ·æ¥ã€‚è¿™æ ·ä¼šä½¿å¾—æ¥ä¸‹æ¥çš„ modeling layer ç»´åº¦åŠ å€ã€‚ã€‚ï¼ˆåœ¨ ai challenge çš„æ¯”èµ›ä¸­å°±æ˜¯è¿™ä¹ˆå¹²çš„ã€‚ã€‚æ²¡å•¥ä¸å¥½å•Šã€‚ã€‚ï¼‰ The rationale for desiring more than one attention call is intuitive. In [20, 28], Co-Attention and Intra-Attention are both used because each provides a different view of the document pair, learning high quality representations that could be used for prediction. Hence, this can significantly improve performance. ç›´è§‰ä¸Šï¼Œä½¿ç”¨å¤šç§ multi-attention æ˜¯é è°±çš„ã€‚ co-attention å’Œ intra-attention æä¾›äº†ä¸åŒçš„è§†è§’å»å®¡è§† document,ç”¨ä»¥å­¦ä¹ é«˜è´¨é‡çš„å‘é‡è¡¨ç¤ºã€‚ å…³äºå„ç§ attentionï¼š https://zhuanlan.zhihu.com/p/35041012 [50] co-attention: dynamic coattention networks for question answering [5] Attentive Pooling Networks [28] [Inter-Weighted Alignment Network for Sentence Pair Modeling](https://aclanthology.info/pdf/D/D17/D17-1122.pdf) [54] Attentive Interactive Neural Networks for Answer Selection in Community Question intra-attention:Attention is all your need Moreover, Co-Attention also comes in different flavors and can either be used with extractive max-mean pooling [5, 54] or alignment-based pooling [3, 20, 28]. Each co-attention type produces different document representations. In max-pooling, signals are extracted based on a wordâ€™s largest contribution to the other text sequence. Mean-pooling calculates its contribution to the overall sentence. Alignment-pooling is another flavor of co-attention, which aligns semantically similar sub-phrases together. co-attentionå¯ä»¥ç”¨äºæå–max-mean poolingæˆ–alignment-based poolingã€‚æ¯ç§co-attentionéƒ½ä¼šäº§ç”Ÿä¸åŒçš„æ–‡æ¡£è¡¨ç¤ºã€‚åœ¨max-poolingä¸­ï¼ŒåŸºäºå•è¯å¯¹å¦ä¸€æ–‡æœ¬åºåˆ—çš„æœ€å¤§è´¡çŒ®æ¥æå–ç‰¹å¾ï¼›mean-poolingè®¡ç®—å…¶å¯¹æ•´ä¸ªå¥å­çš„è´¡çŒ®ï¼›alignment-based poolingæ˜¯å¦ä¸€ç§ååŒæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå°†è¯­ä¹‰ç›¸ä¼¼çš„å­çŸ­è¯­å¯¹é½åœ¨ä¸€èµ·ã€‚å› æ­¤ï¼Œä¸åŒçš„poolingæ“ä½œæä¾›äº†ä¸åŒçš„å¥å­å¯¹è§†å›¾ã€‚ [3] Enhanced LSTM for Natural Language Inference [20] [A Decomposable Attention Model for Natural Language Inference](https://arxiv.org/abs/1606.01933) Our approach is targeted at serving two important purposes: (1) It removes the need for architectural engineering of this component by enabling attention to be called for an arbitrary k times with hardly any consequence and (2) concurrently it improves performance by modeling multiple views via multiple attention calls. As such, our method is in similar spirit to multi-headed attention, albeit efficient. To this end, we introduce Multi-Cast Attention Networks (MCAN), a new deep learning architecture for a potpourri of tasks in the question answering and conversation modeling domains. ä¸¤ä¸ªæ–¹é¢çš„è´¡çŒ®ï¼š ï¼ˆ1ï¼‰æ¶ˆé™¤è°ƒç”¨ä»»æ„kæ¬¡æ³¨æ„åŠ›æœºåˆ¶æ‰€éœ€æ¶æ„å·¥ç¨‹çš„éœ€è¦ï¼Œä¸”ä¸ä¼šäº§ç”Ÿä»»ä½•åæœã€‚ ï¼ˆ2ï¼‰é€šè¿‡å¤šæ¬¡æ³¨æ„åŠ›è°ƒç”¨å»ºæ¨¡å¤šä¸ªè§†å›¾ä»¥æé«˜æ€§èƒ½ï¼Œä¸multi-headed attentionç›¸ä¼¼ã€‚ In our approach, attention is casted, in contrast to the most other works that use it as a pooling operation. We cast co-attention multiple times, each time returning a compressed scalar feature that is re-attached to the original word representations. The key intuition is that compression enables scalable casting of multiple attention calls, aiming to provide subsequent layers with a hint of not only global knowledge but also cross sentence knowledge. ä¸å¤§å¤šæ•°å…¶ä»–ç”¨ä½œæ± åŒ–æ“ä½œçš„å·¥ä½œç›¸åï¼Œåœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæ³¨æ„åŠ›è¢«æŠ•å°„ã€‚é€šè¿‡å¤šæ¬¡æŠ•å°„co-attentionï¼Œæ¯æ¬¡è¿”å›ä¸€ä¸ªå‹ç¼©çš„æ ‡é‡ç‰¹å¾ï¼Œé‡æ–°é™„åŠ åˆ°åŸå§‹çš„å•è¯è¡¨ç¤ºä¸Šã€‚å‹ç¼©å‡½æ•°å¯ä»¥å®ç°å¤šä¸ªæ³¨æ„åŠ›è°ƒç”¨çš„å¯æ‰©å±•æŠ•å°„ï¼Œæ—¨åœ¨ä¸ä»…ä¸ºåç»­å±‚æä¾›å…¨å±€çŸ¥è¯†è€Œä¸”è¿˜æœ‰è·¨å¥å­çŸ¥è¯†çš„æç¤ºï¼ˆç‰¹å¾ï¼‰ã€‚ Model Architecture: Multi-cast Attention Networks Figure 1: Illustration of our proposed Multi-Cast Attention Networks (Best viewed in color). MCAN is a wide multi-headed attention architecture that utilizes compression functions and attention as features. æ¨¡å‹è¾“å…¥æ˜¯ document/query è¯­å¥å¯¹ã€‚ Input Encoderembedding layeræ˜ å°„åˆ°å‘é‡ç©ºé—´ï¼š $w\\in W^d$ Highway Encoderï¼š Highway Networkså¯ä»¥å¯¹ä»»æ„æ·±åº¦çš„ç½‘ç»œè¿›è¡Œä¼˜åŒ–ã€‚è¿™æ˜¯é€šè¿‡ä¸€ç§æ§åˆ¶ç©¿è¿‡ç¥ç»ç½‘ç»œçš„ä¿¡æ¯æµçš„é—¸é—¨æœºåˆ¶æ‰€å®ç°çš„ã€‚é€šè¿‡è¿™ç§æœºåˆ¶ï¼Œç¥ç»ç½‘ç»œå¯ä»¥æä¾›é€šè·¯ï¼Œè®©ä¿¡æ¯ç©¿è¿‡åå´æ²¡æœ‰æŸå¤±ï¼Œå°†è¿™ç§é€šè·¯ç§°ä¸ºinformation highwaysã€‚å³highway networksä¸»è¦è§£å†³çš„é—®é¢˜æ˜¯ç½‘ç»œæ·±åº¦åŠ æ·±ã€æ¢¯åº¦ä¿¡æ¯å›æµå—é˜»é€ æˆç½‘ç»œè®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚ highway encoders can be interpreted as data-driven word filters. As such, we can imagine them to parametrically learn which words have an inclination to be important and not important to the task at hand. For example, filtering stop words and words that usually do not contribute much to the prediction. Similar to recurrent models that are gated in nature, this highway encoder layer controls how much information (of each word) is flowed to the subsequent layers. åœ¨æœ¬æ–‡æ¨¡å‹ä¸­ï¼Œæ¯ä¸ªè¯å‘é‡éƒ½é€šè¿‡highwayç¼–ç å™¨å±‚ã€‚highwayç½‘ç»œæ˜¯é—¨æ§éçº¿æ€§å˜æ¢å±‚ï¼Œå®ƒæ§åˆ¶åç»­å±‚çš„ä¿¡æ¯æµã€‚è®¸å¤šå·¥ä½œéƒ½é‡‡ç”¨ä¸€ç§è®­ç»ƒè¿‡çš„æŠ•å½±å±‚æ¥ä»£æ›¿åŸå§‹è¯å‘é‡ã€‚è¿™ä¸ä»…èŠ‚çœäº†è®¡ç®—æˆæœ¬ï¼Œè¿˜å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æœ¬æ–‡å°†æ­¤æŠ•å½±å±‚æ‰©å±•ä¸ºä½¿ç”¨highwayç¼–ç å™¨ï¼Œå¯ä»¥è§£é‡Šä¸ºæ•°æ®é©±åŠ¨çš„è¯æ»¤æ³¢å™¨ï¼Œå®ƒä»¬å¯ä»¥å‚æ•°åŒ–åœ°äº†è§£å“ªäº›è¯å¯¹äºä»»åŠ¡å…·æœ‰é‡è¦æ€§å’Œé‡è¦æ€§ã€‚ä¾‹å¦‚ï¼Œåˆ é™¤é€šå¸¸å¯¹é¢„æµ‹æ²¡æœ‰å¤šå¤§è´¡çŒ®çš„åœç”¨è¯å’Œå•è¯ã€‚ä¸è‡ªç„¶é—¨æ§çš„å¾ªç¯æ¨¡å‹ç±»ä¼¼ï¼Œhighwayç¼–ç å™¨å±‚æ§åˆ¶æ¯ä¸ªå•è¯æµå…¥ä¸‹ä¸€å±‚å¤šå°‘ä¿¡æ¯ã€‚ $$y=H(x,W_H)\\cdot T(x,W_T) + (1-T(x,W_T))\\cdot x$$ å…¶ä¸­ $W_H, W_T\\in R^{r\\times d}$ æ˜¯å¯å­¦ä¹ å‚æ•°. H(.) å’Œ T(.) åˆ†åˆ«æ˜¯å…¨è¿æ¥åŠ ä¸Š relu å’Œ sigmoid çš„å‡½æ•°ï¼Œç”¨ä»¥æ§åˆ¶ä¿¡æ¯çš„æµå‘ä¸‹ä¸€å±‚ã€‚ co-attention Co-Attention [50] is a pairwise attention mechanism that enables attending to text sequence pairs jointly. In this section, we introduce four variants of attention, i.e., (1) max-pooling, (2) mean-pooling, (3) alignment-pooling, and finally (4) intra-attention (or self attention). ååŒæ³¨æ„åŠ›æœºåˆ¶æ˜¯æˆå¯¹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶å…³æ³¨æ–‡æœ¬åºåˆ—å¯¹ã€‚ä½œè€…å¼•å…¥äº† 4 ä¸­æ³¨æ„åŠ›æœºåˆ¶ã€‚ 1.affinity/similarity matrix$$s_{ij}=F(q_i)^TF(d_j)$$ å…¶ä¸­ï¼ŒF(.) æ˜¯å¤šå±‚æ„ŸçŸ¥æœºï¼Œé€šå¸¸å¯é€‰æ‹©çš„è®¡ç®—ç›¸ä¼¼çŸ©é˜µçš„æ–¹å¼æœ‰ï¼š $$s_{ij}=q_i^TMd_j, s_{ij}=F[q_i;d_j]$$ ä»¥åŠåœ¨ BiDAF å’Œ QANet ä¸­ä½¿ç”¨çš„ $s_{ij}=F[q_i;d_j;q_i\\circ d_j]$. 2. Extractive poolingmax-poolingå…³æ³¨äºå¦ä¸€ä¸ªåºåˆ—äº¤äº’åï¼Œæœ€åŒ¹é…çš„é‚£ä¸ªè¯ã€‚ $$qâ€™=soft(max_{col}(s))^Tq, dâ€™=soft(max_{row}(s))^Td$$ soft(.) æ˜¯ softmax å‡½æ•°ã€‚$qâ€™,dâ€™$ æ˜¯ co-attentive representations of q and d respectively. mean-poolingå…³æ³¨å¦ä¸€ä¸ªå¥å­çš„å…¨éƒ¨ï¼Œå–å¹³å‡å€¼ã€‚ $$qâ€™=soft(mean_{col}(s))^Tq, dâ€™=soft(mean_{row}(s))^Td$$ each pooling operator has different impacts and can be intuitively understood as follows: max-pooling selects each word based on its maximum importance of all words in the other text. Mean-pooling is a more wholesome comparison, paying attention to a word based on its overall influence on the other text. This is usually dataset-dependent, regarded as a hyperparameter and is tuned to see which performs best on the held out set. ä¸åŒçš„ pooling æ“ä½œæœ‰ä¸åŒçš„å½±å“ï¼Œè·å–çš„ä¿¡æ¯ä¹Ÿä¸ä¸€æ ·ã€‚ max-poolingæ ¹æ®æ¯ä¸ªå•è¯åœ¨å…¶ä»–æ–‡æœ¬ä¸­æ‰€æœ‰å•è¯çš„æœ€å¤§é‡è¦æ€§é€‰æ‹©æ¯ä¸ªå•è¯ã€‚mean-poolingæ˜¯åŸºäºæ¯ä¸ªè¯åœ¨å…¶ä»–æ–‡æœ¬ä¸Šçš„æ€»ä½“å½±å“æ¥å…³æ³¨æ¯ä¸ªè¯ã€‚ è¿™å…¶å®æ˜¯ä¸æ•°æ®é›†å’Œä»»åŠ¡ç›¸å…³çš„ã€‚å¯ä»¥çœ‹ä½œè¶…å‚æ•°ï¼Œç„¶åè°ƒæ•´çœ‹å“ªä¸ªåœ¨å¯¹åº”çš„ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¡¨ç°æ›´ä½³ã€‚ 3. Alignment-Pooling$$d_iâ€™:=\\sum^{l_q}{j=1}\\dfrac{exp(s{ij})}{\\sum_{k=1}^{l_q}exp(s_{ik})}q_j$$ å…¶ä¸­ $d_iâ€™$ æ˜¯ q å’Œ $d_i$ çš„è½¯å¯¹é½ã€‚ç›´è§‚çš„è¯´ï¼Œ$d_iâ€™$ æ˜¯å…³äº $d_i$ çš„ ${q_j}^{l_q}_{j=1}$ çš„åŠ æƒå’Œã€‚ $$q_iâ€™:=\\sum^{l_d}{j=1}\\dfrac{exp(s{ij})}{\\sum_{k=1}^{l_d}exp(s_{ik})}d_j$$ $q_iâ€™$ æ˜¯ $q_i$ å’Œ d çš„è½¯å¯¹é½ã€‚ä¹Ÿå°±æ˜¯ï¼Œ$q_iâ€™$ æ˜¯ å…³äº $q_i$ çš„ ${d_j}^{l_d}_{j=1}$ çš„åŠ æƒå’Œã€‚ 4. intra-Attention$$x_iâ€™:=\\sum^{l}{j=1}\\dfrac{exp(s{ij})}{\\sum_{k=1}^{l}exp(s_{ik})}x_j$$ ä¹Ÿå°±æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚ç›¸æ¯” attention is all your need,å¯èƒ½å°±æ˜¯ç›¸ä¼¼çŸ©é˜µä¸ä¸€æ ·å§ã€‚ Multi-Cast AttentionCasted Attentionç”¨ $x$ æ¥è¡¨ç¤º q æˆ– dï¼Œ$\\overline x$ è¡¨ç¤º ç»è¿‡ co-attention å’Œ soft-attention alignment åçš„åºåˆ—è¡¨ç¤º $qâ€™, dâ€™$. $$f_c=F_c[\\overline x, x]$$ $$f_m=F_m[\\overline x \\circ x]$$ $$f_s=F_m[\\overline x-x]$$ å…¶ä¸­ $\\circ$ æ˜¯ Hadamard product. $F(.)$ æ˜¯å‹ç¼©å‡½æ•°ï¼Œå°†ç‰¹å¾å‹ç¼©åˆ° scalar. Intuitively, what is achieved here is that we are modeling the influence of co-attention by comparing representations before and after co-attention. For soft-attention alignment, a critical note here is that x and $\\overline x$ (though of equal lengths) have â€˜exchangedâ€™ semantics. In other words, in the case of q, $\\overline q$ actually contains the aligned representation of d. Compression Function The rationale for compression is simple and intuitive - we do not want to bloat subsequent layers with a high dimensional vector which consequently incurs parameter costs in subsequent layers. We investigate the usage of three compression functions, which are capable of reducing a n dimensional vector to a scalar. æœ¬èŠ‚å®šä¹‰äº†Fc(.) ä½¿ç”¨çš„å‹ç¼©å‡½æ•°ï¼Œä¸å¸Œæœ›ä½¿ç”¨é«˜ç»´å‘é‡è†¨èƒ€åç»­å±‚ï¼Œè¿™ä¼šåœ¨åç»­å±‚ä¸­ä¼šäº§ç”Ÿå‚æ•°æˆæœ¬ã€‚å› æ­¤æœ¬æ–‡ç ”ç©¶äº†ä¸‰ç§å‹ç¼©å‡½æ•°çš„ç”¨æ³•ï¼Œå®ƒä»¬èƒ½å¤Ÿå°†nç»´å‘é‡å‡å°‘åˆ°æ ‡é‡ã€‚ Sum Sumï¼ˆSMï¼‰å‡½æ•°æ˜¯ä¸€ä¸ªéå‚æ•°åŒ–å‡½æ•°ï¼Œå®ƒå¯¹æ•´ä¸ªå‘é‡æ±‚å’Œï¼Œå¹¶è¾“å‡ºæ ‡é‡ $$F(x)=\\sum_i^nx_i, x_i\\in x$$ Neural networks $$F(x)=RELU(W_cx+b)$$ å…¶ä¸­ $W_c\\in R^{n\\times 1}$ Factorization Machines å› å­åˆ†è§£æœºæ˜¯ä¸€ç§é€šç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæ¥å—å®å€¼ç‰¹å¾å‘é‡ $x\\in R^n$ å¹¶è¿”å›æ ‡é‡è¾“å‡ºã€‚ FMæ˜¯è¡¨è¾¾æ¨¡å‹ï¼Œä½¿ç”¨åˆ†è§£å‚æ•°æ•è·ç‰¹å¾ä¹‹é—´çš„æˆå¯¹ç›¸äº’ä½œç”¨ã€‚ kæ˜¯FMæ¨¡å‹çš„å› å­æ•°ã€‚ Multi-Castæˆ‘ä»¬çš„æ¶æ„èƒŒåçš„å…³é”®æ€æƒ³æ˜¯ä¿ƒè¿›kä¸ªæ³¨æ„åŠ›æŠ•å°„ï¼Œæ¯ä¸ªæŠ•å°„éƒ½ç”¨ä¸€ä¸ªå®å€¼æ³¨æ„åŠ›æç¤ºæ¥å¢å¼ºåŸå§‹è¯å‘é‡ã€‚ å¯¹äºæ¯ä¸ªquery-documentå¯¹ï¼Œåº”ç”¨Co-Attention with mean-poolingï¼ŒCo-Attention with max-Poolingå’ŒCo-Attention with alignment-poolingã€‚ æ­¤å¤–ï¼Œå°†Intra-Attentionåˆ†åˆ«å•ç‹¬åº”ç”¨äºqueryå’Œdocumentã€‚ æ¯ä¸ªæ³¨æ„åŠ›æŠ•å°„äº§ç”Ÿä¸‰ä¸ªæ ‡é‡ï¼ˆæ¯ä¸ªå•è¯ï¼‰ï¼Œå®ƒä»¬ä¸è¯å‘é‡è¿æ¥åœ¨ä¸€èµ·ã€‚æœ€ç»ˆçš„æŠ•å°„ç‰¹å¾å‘é‡æ˜¯ $z\\in R^{12}$ã€‚ å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªå•è¯ w_{i} ï¼Œæ–°çš„è¡¨ç¤ºæˆä¸º $\\bar{w_{i}}=[w_{i};z_{i}]$ Long Short-Term Memory Encoderæ¥ä¸‹æ¥ï¼Œå°†å¸¦æœ‰casted attetnionçš„å•è¯è¡¨ç¤º $\\bar{w_{1}},\\bar{w_{2}},â€¦,\\bar{w_{l}}$ ä¼ é€’åˆ°åºåˆ—ç¼–ç å™¨å±‚ã€‚é‡‡ç”¨æ ‡å‡†çš„é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ç¼–ç å™¨. As such, the key idea behind casting attention as features right before this layer is that it provides the LSTM encoder with hints that provide information such as (1) longterm and global sentence knowledge and (2) knowledge between sentence pairs (document and query). LSTMåœ¨documentå’Œqueryä¹‹é—´å…±äº«æƒé‡ã€‚ å…³é”®æ€æƒ³æ˜¯LSTMç¼–ç å™¨é€šè¿‡ä½¿ç”¨éçº¿æ€§å˜æ¢ä½œä¸ºé—¨æ§å‡½æ•°æ¥å­¦ä¹ è¡¨ç¤ºåºåˆ—ä¾èµ–æ€§çš„è¡¨ç¤ºã€‚å› æ­¤ï¼Œåœ¨è¯¥å±‚ä¹‹å‰å¼•äººæ³¨æ„åŠ›ä½œä¸ºç‰¹å¾çš„å…³é”®æ€æƒ³æ˜¯å®ƒä¸ºLSTMç¼–ç å™¨æä¾›äº†å¸¦æœ‰ä¿¡æ¯çš„æç¤ºï¼Œä¾‹å¦‚é•¿æœŸå’Œå…¨å±€å¥å­çŸ¥è¯†å’Œå¥å­å¯¹ï¼ˆæ–‡æ¡£å’ŒæŸ¥è¯¢ï¼‰ä¹‹é—´çš„çŸ¥è¯†ã€‚ Pooling Operation æœ€åï¼Œåœ¨æ¯ä¸ªå¥å­çš„éšè—çŠ¶æ€ $h_{1},â€¦h_{l}$ ä¸Šåº”ç”¨æ± åŒ–å‡½æ•°ã€‚å°†åºåˆ—è½¬æ¢ä¸ºå›ºå®šç»´åº¦çš„è¡¨ç¤ºã€‚ $$h=MeanMax[h_1,â€¦,h_l]$$ æ‰€ä»¥å¾—åˆ°çš„ q å’Œ d çš„æœ€ç»ˆè¡¨ç¤ºæ˜¯ [1, hiden_size]? Prediction Layer and Optimization$$y_{out} = H_2(H_1([x_q; x_d ; x_q \\circ x_d ; x_q âˆ’ x_d ]))$$ å…¶ä¸­ $H_1,H_2$ æ˜¯å…·æœ‰ ReLU æ¿€æ´»çš„ highway ç½‘ç»œå±‚ã€‚ç„¶åå°†è¾“å‡ºä¼ é€’åˆ°æœ€ç»ˆçº¿æ€§ softmax å±‚ã€‚ $$y_{pred} = softmax(W_F Â· y_{out} + b_F )$$ å…¶ä¸­ $W_F\\in R^{h\\times 2}, b_F\\in R^2$. ä½¿ç”¨ multi-class cross entropyï¼Œå¹¶å¸¦æœ‰ L2 æ­£åˆ™åŒ–ã€‚","link":"/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/"},{"title":"è®ºæ–‡ç¬”è®°-constrast learning in NLP","text":"paper list: An efficient framework for learning sentence representations. CLEAR: Contrastive Learning for Sentence Representation Declutr: Deep contrastive learn- ing for unsupervised textual representations. arXiv SimCSE: Simple Contrastive Learning of Sentence Embeddings R-Drop: R-Drop: Regularized Dropout for Neural Networks Coco-lm: Correcting and contrasting text sequences for language model pretraining. Learning dense representations of phrases at scale. An unsupervised sentence embedding method by mutual information maximization. Representation degeneration problem in training natural language generation models CosReg: Improving neural language generation with spectrum control. CLAPS: Contrastive Learning with Adversarial Perturbations for Conditional Text Generation LINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding Adversarial PerturbationInterpretable Adversarial Perturbation in Input Embedding Space for Text Contrastive Learning for Many-to-many Multilingual Neural Machine Translation Unsupervised Data Augmentation for Consistency Training CLEAR çœ‹å›¾å°±èƒ½ç†è§£äº†ã€‚å…³é”®åœ¨äºç¦»æ•£æ–‡æœ¬çš„ augmentation å¦‚ä½•å®ç°çš„ã€‚ è¿™æ ·åšçš„é—®é¢˜åœ¨äºï¼Œå¯¹äºç¦»æ•£çš„æ–‡æœ¬ï¼Œåˆ é™¤ä¸€ä¸ªè¯éƒ½å¯èƒ½æ”¹å˜æ•´ä¸ªå¥å­çš„è¯­ä¹‰å§ã€‚è¿™æ ·ç»§ç»­ä½œä¸ºæ­£æ ·æœ¬ä¼¼ä¹æœ‰ç‚¹é—®é¢˜ã€‚ simCSE æ— ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ï¼š æ„é€ æ­£æ ·æœ¬ï¼štwo different dropout è´Ÿæ ·æœ¬ï¼š mini-batchä¸­çš„å…¶ä»–æ ·æœ¬ æœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ï¼šåˆ©ç”¨äº†NLIçš„æ•°æ®é›†ï¼Œentailment pairsä½œä¸ºæ­£æ ·æœ¬å¯¹ï¼Œcontradiction pairs ä½œä¸ºè´Ÿæ ·æœ¬å¯¹ã€‚ R-DropåŒä¸€ç»„æ•°æ®ï¼Œç»è¿‡ä¸¤æ¬¡ç½‘ç»œã€‚ç„¶åä½¿å¾— $p(y|x, drop1), p(y|x, drop2)$ ä¸¤è€…çš„ kl æ•£åº¦æœ€å°ã€‚ CosRegpaper: Improving neural language generation with spectrum control. å®šä¹‰äº†representation degeneration problem. ç”Ÿæˆæ¨¡å‹é¢„è®­ç»ƒåçš„æ¨¡å‹ï¼Œä¸åŒçš„è¯çš„è¯­è¨€ä¼šé›†ä¸­åˆ°ä¸€ä¸ªå¾ˆå°çš„ç©ºé—´å†…ï¼Œå› è€Œé™ä½äº†è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚ ä½œè€…åˆ†æä¼šäº§ç”Ÿè¿™ç§ç°è±¡çš„åŸå› æ˜¯ï¼Œä¸€ä¸ªè¯çš„é¢‘ç‡ç›¸æ¯”æ•´ä¸ªè¯è¡¨æ˜¯å¾ˆæœ‰é™çš„ã€‚è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œå½“è¿™ä¸ªè¯ä½œä¸ºlabelæ—¶ï¼Œæˆ‘ä»¬ä¼šå¢å¤§å…¶å¯¹åº”çš„æœ€å¤§ä¼¼ç„¶ï¼Œä½†åŒæ—¶ï¼Œä¹Ÿåœ¨å…¶ä»–çš„cls-lossä¸­æˆ‘ä»¬ä¹Ÿä¼šé™ä½è¿™ä¸ªè¯çš„ä¼¼ç„¶ã€‚å› æ­¤ï¼Œæ‰€æœ‰çš„è¯éƒ½ä¼šä»¥é™ä½èµ·å…¶ä¼¼ç„¶çš„æ–¹å‘æ¨å‘negativeæ–¹å‘ã€‚è¿™å°±ä¼šå¯¼è‡´ä¸Šé¢è¯´çš„é—®é¢˜ã€‚ ä½œè€…å°† word/category embedding å¯è§†åŒ–å¾—åˆ°å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ å› è€Œä½œè€…é‡‡å–çš„æ–¹å¼å¢å¤§è¿™ä¸ªcaneçš„å®¹é‡ã€‚ Then a straightforward approach is to improve the aperture of the cone which is defined as the maximum angle between any two boundaries of the cone. For the ease of optimization, we minimize the cosine similarities between any two word embeddings to increase the expressiveness. u1s1 ç†è®ºæ¨å¯¼æ²¡å¤ªæ‡‚ï¼ˆæœç„¶æˆ‘è¿™è¾ˆå­éƒ½å‘ä¸äº†ICLR/ICMLâ€¦ ä¸è¿‡å¤§æ¦‚æ„æ€æ‡‚äº†,å°±æ˜¯ä¸€ä¸ªå‡å°è¯ä¹‹é—´ç›¸ä¼¼åº¦çš„æ­£åˆ™åŒ–é¡¹ã€‚ CLASP ä½œè€…è®¤ä¸ºç›´æ¥åˆ©ç”¨mini-batchçš„å…¶ä»–å®ä¾‹ä½œä¸ºè´Ÿæ ·æœ¬å¤ªç®€å•äº†ï¼Œéœ€è¦å¾ˆå¤§çš„batch sizeï¼Œå¦åˆ™å­¦ä¸åˆ°æœ‰ç”¨çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œä½œä¸ºæå‡ºäº†ä¸€ç§é€šè¿‡åœ¨hidden sizeä¸Šå¢åŠ æ‰°åŠ¨çš„æ–¹æ³•æ¥æ„é€ æ­£è´Ÿæ ·æœ¬ã€‚ æ­£æ ·æœ¬ Distant-Targetsï¼šå¢åŠ è¾ƒå¤§çš„æ‰°åŠ¨ï¼Œä½†æ˜¯æœ€å¤§åŒ–å…¶ä¼¼ç„¶ï¼Œä¿è¯è¯­ä¹‰ä¸åŸå¥å­è¯­ä¹‰ä¸€è‡´ã€‚ åœ¨æ­£æ¢¯åº¦æ–¹å‘å¢åŠ è¾ƒå¤§çš„æ‰°åŠ¨ã€‚ è´Ÿæ ·æœ¬ Imposters: å¢åŠ è¾ƒå°çš„æ‰°åŠ¨ï¼Œä½†æ˜¯æœ€å°åŒ–å…¶ä¼¼ç„¶ï¼Œä¿è¯ç”Ÿæˆé”™è¯¯ä¸”è¾ƒéš¾åˆ†è¾¨çš„æ ·æœ¬ã€‚åœ¨è´Ÿæ¢¯åº¦æ–¹å‘å¢åŠ è¾ƒå°çš„æ‰°åŠ¨ã€‚","link":"/2021/07/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-constrast-learning-in-NLP/"},{"title":"è®ºæ–‡ç¬”è®°-Match LSTM","text":"Motivation SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. é’ˆå¯¹ SQuAD è¿™æ ·çš„é˜…è¯»ç†è§£å¼ä»»åŠ¡æå‡ºçš„ç«¯åˆ°ç«¯çš„æ¨¡å‹ã€‚ SQuAD çš„ç­”æ¡ˆä¸æ˜¯ä»å€™é€‰è¯ä¸­æå–ï¼Œè€Œæ˜¯ç±»ä¼¼äºäººç±»çš„å›ç­”ï¼Œæ˜¯ä¸åŒé•¿åº¦çš„å¥å­ã€‚ The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. ä¸»è¦æ˜¯åŸºäº Pointer Networks å…³äºé˜…è¯»ç†è§£çš„æ•°æ®é›† benchmark datasetï¼š MCTest: A challenge dataset for the open-domain machine comprehension of text. Teaching machines to read and comprehend. The Goldilocks principle: Reading childrenâ€™s books with explicit memory representations. Towards AI-complete question answering: A set of prerequisite toy tasks. SQuAD: 100,000+ questions for machine comprehension of text. SQuAD Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering, including syntactic parsing, named entity recognition, question classification, semantic parsing, etc. Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension. ä¼ ç»Ÿçš„æ™ºèƒ½é—®ç­”ä»»åŠ¡æ•´ä¸ªæµç¨‹åŒ…æ‹¬ å¥æ³•åˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€é—®é¢˜åˆ†ç±»ã€è¯­ä¹‰åˆ†æç­‰ã€‚ã€‚éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œç«¯åˆ°ç«¯çš„æ¨¡å‹å¼€å§‹å‡ºç°ã€‚ End-to-end model architecture: Teaching machines to read and comprehend. The Goldilocks principle: Reading childrenâ€™s books with explicit memory representations. Attention-based convolutional neural network for machine comprehension Text understanding with the attention sum reader network. Consensus attention-based neural networks for chinese reading comprehension. However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset. ä¹‹å‰çš„æ¨¡å‹çš„ answer è¦ä¹ˆæ˜¯ä»å€™é€‰ç­”æ¡ˆä¸­é€‰æ‹©ï¼Œè¦ä¹ˆæ˜¯ä¸€ä¸ªç®€å•çš„ç¬¦å·ã€‚è¿™éƒ½ä¸é€‚åˆ SQuDA. æ¨¡å‹æ˜¯åŸºäºä½œè€…æ—©æœŸæå‡ºçš„ç”¨äº textual entailment çš„ match-LSTMLearning natural language inference with LSTMï¼Œç„¶åè¿›ä¸€æ­¥åº”ç”¨äº† Pointer Net(https://papers.nips.cc/paper/5866-pointer-networks), ä»è€Œå…è®¸é¢„æµ‹çš„ç»“æœèƒ½å¤Ÿä»è¾“å…¥ä¸­è·å¾—ï¼Œè€Œä¸æ˜¯ä»ä¸€ä¸ªå›ºå®šçš„è¯è¡¨ä¸­è·å–ã€‚ We propose two ways to apply the Ptr-Net model for our task: a sequence model and a boundary model. We also further extend the boundary model with a search mechanism. ä½œè€…æå‡ºçš„ä¸¤ç§æ¨¡å‹ã€‚ Model ArchitectureMatch-LSTMPointer NetworkPointer Network (Ptr-Net) model : to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence. Instead of picking an output token from a fixed vocabulary, Ptr-Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol. ä»è¾“å…¥ sentences ä¸­ç”Ÿæˆ answer. ç±»ä¼¼äº Pointer Network çš„æ¨¡å‹ï¼š Incorporating copying mechanism in sequence-to-sequence learning. Text understanding with the attention sum reader network. MATCH-LSTM AND ANSWER POINTER æ¨¡å‹ä¸»è¦åˆ†ä¸º3éƒ¨åˆ†ï¼š An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs. ä½¿ç”¨ LSTM å¤„ç† question å’Œ passage. A match-LSTM layer that tries to match the passage against the question. ä½¿ç”¨ match-LSTM å¯¹lstmç¼–ç åçš„ question å’Œ passage è¿›è¡ŒåŒ¹é…ã€‚ An Answer Pointer (Ans-Ptr) layer that uses Ptr-Net to select a set of tokens from the passage as the answer. The difference between the two models only lies in the third layer. ä½¿ç”¨ Pointer æ¥é€‰æ‹© tokens. LSTM preprocessing Layer$$H^p=\\overrightarrow {LSTM}(P), H^q=\\overrightarrow {LSTM}(Q)$$ ç›´æ¥ä½¿ç”¨å•å‘LSTMï¼Œæ¯ä¸€ä¸ªæ—¶åˆ»çš„éšå«å±‚å‘é‡è¾“å‡º $H^p\\in R^{l\\times P}, H^q\\in R^{l\\times Q}$ åªåŒ…å«å·¦ä¾§ä¸Šä¸‹æ–‡ä¿¡æ¯. Match-LSTM Layer$$\\overrightarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\\overrightarrow {h^r}_{i-1}+b^p)\\otimes e_Q)\\in R^{l\\times Q}$$ $$\\overrightarrow \\alpha_i=softmax(w^T\\overrightarrow G_i + b\\otimes e_Q)\\in R^{1\\times Q}$$ the resulting attention weight $\\overrightarrow Î±_{i,j}$ above indicates the degree of matching between the $i^{th}$ token in the passage with the $j^{th}$ token in the question. å…¶ä¸­ $W^q,W^p,W^r \\in R^{l\\times l}, b^p,w\\in R^l, b\\in R$ æ‰€ä»¥ $\\overrightarrow Î±_{i}$ è¡¨ç¤ºæ•´ä¸ª question ä¸ passage ä¸­çš„ç¬¬ i ä¸ªè¯ä¹‹é—´çš„ match ç¨‹åº¦ï¼Œä¹Ÿå°±æ˜¯é€šå¸¸ç†è§£çš„ attention ç¨‹åº¦ã€‚ ä¼ ç»Ÿçš„ attention å°±æ˜¯å°† passage å’Œ question çŸ©é˜µç›¸ä¹˜ï¼Œæ¯”å¦‚ transformer ä¸­ query å’Œ keys ç›¸ä¹˜ã€‚å¤æ‚ä¸€ç‚¹å¯èƒ½å°±æ˜¯ dynamic memory networks ä¸­çš„å°† ä¸¤ä¸ªéœ€è¦ match çš„å‘é‡ç›¸å‡ã€element-wiseç›¸ä¹˜ä¹‹åï¼Œä½¿ç”¨ä¸¤å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œæ¥è¡¨ç¤ºã€‚ è¿™é‡Œçš„ attention score çš„è®¡ç®—æ–¹å¼åˆä¸ä¸€æ ·äº†ã€‚ $\\overrightarrow{h^r_{i-1}}$ æ˜¯é€šè¿‡ LSTM è€¦åˆ weighted queston å’Œ passage ä¸­ä¸Šä¸€ä¸ªè¯å¾—åˆ°çš„ä¿¡æ¯ã€‚ å…¶ä¸­ï¼š $$\\overrightarrow z_i=\\begin{bmatrix} h^p \\ H^q\\overrightarrow {\\alpha_i^T} \\ \\end{bmatrix} $$ $$h^r=\\overrightarrow{LSTM}(\\overrightarrow{z_i},\\overrightarrow{h^r_{i-1}})$$ ç„¶åç±»ä¼¼äºLSTMå°† $\\overrightarrow{h_{i-1}^r}$ å’Œ å½“å‰ passage çš„è¡¨ç¤º $H^p_i$ è€¦åˆå¾—åˆ°çš„ $R^{l\\times 1}$ çš„å‘é‡é‡å¤Q æ¬¡ï¼Œå¾—åˆ° $R^{l\\times Q}$ï¼Œæ‰€ä»¥ $\\overrightarrow G_i\\in R^{l\\times Q}$, åœ¨é€šè¿‡ä¸€ä¸ªsoftmax-affineç½‘ç»œå¾—åˆ° attention weights. æ•´ä¸ªæ€è·¯ä¸‹æ¥ï¼Œå°±æ˜¯ attention score ä¸æ˜¯é€šè¿‡çŸ©é˜µç›¸ä¹˜ï¼Œä¹Ÿä¸æ˜¯å‘é‡ $h^p_i, H^q$ ç›¸å‡ä¹‹åé€šè¿‡ç¥ç»ç½‘ç»œå¾—åˆ°ã€‚ä½†æ˜¯ä¹Ÿç›¸ä¼¼ï¼Œå°±æ˜¯å¯¹å½“å‰è¦åŒ¹é…çš„ä¸¤ä¸ªå‘é‡ $h^p_i, H^q$ é€šè¿‡ä¸¤å±‚ç¥ç»ç½‘ç»œå¾—åˆ°,å…¶ä¸­çš„å¯¹å½“å‰å‘é‡ $H_i^p$ å’Œ $\\overrightarrow {h_{i-1}^r}$ è¦é‡å¤ Q æ¬¡ã€‚ã€‚ã€‚å…¶å®è·Ÿ DMN è¿˜æ˜¯ç›¸ä¼¼çš„ï¼Œåªä¸è¿‡ä¸æ˜¯ç®€å•çš„ attention å½“å‰çš„å‘é‡ï¼Œè¿˜ç”¨äº† LSTM æ¥è€¦åˆä¹‹å‰çš„ä¿¡æ¯ã€‚ æœ€ç»ˆå¾—åˆ°æƒ³è¦çš„ç»“åˆäº† attention å’Œ LSTM çš„è¾“å‡º $\\overrightarrow h^r$. ä½œè€…åšäº†ä¸€ä¸ªåå‘çš„ LSTM. æ–¹å¼æ˜¯ä¸€æ ·çš„ï¼š $$\\overleftarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\\overleftarrow {h^r}_{i-1}+b^p)\\otimes e_Q)$$ $$\\overleftarrow \\alpha_i=softmax(w^T\\overleftarrow G_i + b\\otimes e_Q)$$ åŒæ ·å¾—åˆ° $\\overleftarrow {h_i^r}$. $\\overrightarrow {H^r}\\in R^{l\\times P}$ è¡¨ç¤ºéšè—çŠ¶æ€ $[\\overrightarrow {h^r_1}, \\overrightarrow {h^r_2},â€¦,\\overrightarrow {h^r_P}]$. $\\overleftarrow {H^r}\\in R^{l\\times P}$ è¡¨ç¤ºéšè—çŠ¶æ€ $[\\overleftarrow {h^r_1}, \\overleftarrow {h^r_2},â€¦,\\overleftarrow {h^r_P}]$. ç„¶åæŠŠä¸¤è€…å †å èµ·æ¥å¾—åˆ°é€šè¿‡ question åŒ¹é…ä¹‹åçš„ passage å‘é‡è¡¨ç¤ºï¼š $H^r=\\begin{bmatrix} \\overrightarrow H^r \\ \\overleftarrow H^r \\end{bmatrix} \\in R^{2l\\times P}$ Answer Pointer LayerThe Sequence ModelThe answer is represented by a sequence of integers $a=(a_1,a_2,â€¦)$ indicating the positions of the selected tokens in the original passage. å†ä¸€æ¬¡åˆ©ç”¨ attentionï¼Œ$\\beta_{k,j}$ è¡¨ç¤º answer ä¸­ç¬¬ k ä¸ªtokené€‰æ‹© passage ä¸­ç¬¬ j ä¸ªæ¬¡çš„æ¦‚ç‡ã€‚æ‰€ä»¥ $\\beta_k\\in R^{P+1}$. $$F_k=tanh(V\\tilde {H^r}+(W^ah^a_{k-1}+b^a)\\otimes e_{P+1})\\in R^{l\\times P+1}$$ $$\\beta_k=softmax(v^TF_k+c\\otimes e_{P+1}) \\in R^{1\\times (P+1)}$$ å…¶ä¸­ $\\tilde H\\in R^{2l\\times (P+1)}$ è¡¨ç¤º $H^r$ å’Œ zero vector çš„å åŠ , $\\tilde H=[H^r, 0], V\\in R^{l\\times 2l}, W^a\\in R^{l\\times l}, b^a,v\\in R, c\\in R$. æ‰€ä»¥è¿˜æ˜¯è·Ÿ match-LSTM ä¸€æ ·ï¼Œå…ˆå¯¹ $H^r$ ä¸­çš„æ¯ä¸€ä¸ªè¯é€šè¿‡å…¨é“¾æ¥è¡¨ç¤º $W^ah^a_{k+1}+b^a$, ç„¶åé‡å¤ P+1 æ¬¡ï¼Œå¾—åˆ° $R^{l\\times (P+1)}$. åœ¨é€šè¿‡æ¿€æ´»å‡½æ•° tanhï¼Œ å†é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œç„¶åä½¿ç”¨ softmax è¿›è¡Œå¤šåˆ†ç±»ã€‚ $$h_k^a=\\overrightarrow{LSTM}(\\tilde {H^r}\\beta_k^T, h^a_{k-1})$$ è¿™é‡Œæ˜¯æŠŠ $\\tilde H^r$ ä¸æƒé‡ $\\beta_k$ çŸ©é˜µç›¸ä¹˜ä¹‹åçš„ç»“æœä½œä¸º LSTM k æ—¶åˆ»çš„è¾“å…¥ã€‚å¾ˆç„å­¦ï¼Œ æ„Ÿè§‰å¯ä»¥çœ‹ä½œæ˜¯ self-attention ç»“åˆäº† LSTM. å¯¹ç”Ÿæˆ answer sequence çš„æ¦‚ç‡è¿›è¡Œå»ºæ¨¡ï¼š $$p(a|H^r)=\\prod_k p(a_k|a_1,a_2,â€¦,a_{k-1}, H^r)$$ å…¶ä¸­ï¼š $$p(a_k=j|a_1,a_2,â€¦,a_{k-1})=\\beta_{k,j}$$ ç›®æ ‡å‡½æ•° loss function: $$-\\sum_{n=1}^N logp(a_n|P_n,Q_n)$$ The Boundary ModelSo the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to Hr, and the probability of generating an answer is simply modeled as: $$p(a|H^r)=p(a_s|H^r)p(a_e|a_s, H^r)$$ Search mechanism, and bi-directional Ans-Ptr. TrainingDatasetSQuAD: Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics. Each passage is a single paragraph from a Wikipedia article, and each passage has around 5 questions associated with it. In total, there are 23,215 passages and 107,785 questions. The data has been split into a training set (with 87,599 question-answer pairs), a development set (with 10,570 questionanswer pairs) and a hidden test set configuration dimension l of the hidden layers is set to 150 or 300. Adammax: $\\beta_1=0.9, \\beta_2=0.999$ minibatch size = 30 no L2 regularization. Result","link":"/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/"},{"title":"è®ºæ–‡ç¬”è®°-image-based contrastive learning","text":"simCLR MoCo BYOL Swin-ssl BraVe What Makes for Good Views for Contrastive Learning? BYOL works even without batch statistics Understanding Self-Supervised Learning Dynamics without Contrastive Pairs Big Self-Supervised Models are Strong Semi-Supervised Learners Understanding contrastive representation learning through alignment and uniformity on the hypersphere. simCLRA Simple Framework for Contrastive Learning of Visual Representations ä½œè€…æå‡ºäº†ä¸€ä¸ªç®€å•çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œä¸éœ€è¦ç‰¹æ®Šçš„ç½‘ç»œç»“æ„å’Œmemory bank. Introductionç°æœ‰çš„æ— ç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šç”Ÿæˆå¼å’Œåˆ¤åˆ«å¼ã€‚ ç”Ÿæˆå¼ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸‰ç±»ï¼š deep belief nets Auto-encoding Generative adversarial nets pixel-level ç”Ÿæˆå¼ç®—æ³•éå¸¸æ¶ˆè€—è®¡ç®—èµ„æºï¼Œå› è€Œå¯¹äºæœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ å¹¶ä¸æ˜¯å¿…é¡»çš„ã€‚ åˆ¤åˆ«å¼æ–¹æ³•çš„ç›®æ ‡å‡½æ•°æ›´æ¥è¿‘ç›‘ç£å­¦ä¹ ï¼Œä¸è¿‡å…¶å¯¹åº”çš„ç›‘ç£ä»»åŠ¡æ˜¯ä»æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®é›†ä¸­è‡ªè¡Œæ„é€ çš„ï¼Œå› è€Œå­¦åˆ°çš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›å—é™äºé¢„å®šä¹‰çš„ä»»åŠ¡ï¼Œè€Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ç°æœ‰çš„è¾¾åˆ°sotaçš„å‡ ç¯‡paper[4][5][6] ä½œè€…æå‡ºäº†ä¸€ä¸ªç®€å•çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä¸ä»…è¾¾åˆ°äº†sotaï¼Œè€Œä¸”ä¸éœ€è¦å¤æ‚çš„ç½‘ç»œç»“æ„[6][7]ï¼Œä¹Ÿä¸éœ€è¦memory bank[8][9][10][11]. ä¸ºäº†ç³»ç»Ÿçš„ç†è§£æ€æ ·æ‰èƒ½è·å¾—æœ‰æ•ˆçš„çš„å¯¹æ¯”å­¦ä¹ ï¼Œä½œè€…ç ”ç©¶äº†ä»¥ä¸‹å‡ ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼š data augmentationï¼šç›¸æ¯”æœ‰ç›‘ç£å­¦ä¹ ï¼Œå¯¹æ¯”å­¦ä¹ æ›´éœ€è¦æ•°æ®å¢å¼º $t\\sim T$ nonlinear projectionï¼šå¦‚å›¾æ‰€ç¤ºï¼Œåœ¨è§†è§‰è¡¨ç¤ºå’Œcontrast lossä¹‹é—´å¢åŠ ä¸€ä¸ªéçº¿æ€§projection $g(\\cdot)$ å¾ˆæœ‰å¿…è¦ normalized embeddings and an appropriately adjusted temperature parameter: å½’ä¸€åŒ–çš„embeddingå’Œå¯è°ƒæ•´çš„temperature parameter. larger batch size and more training steps Methodå¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒsimCLR ä¸»è¦åŒ…æ‹¬å››éƒ¨åˆ†ï¼š A stochastic data augmentation module: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur A neural network base encoder $f(\\cdot)$ï¼Œä½œè€…é‡‡ç”¨çš„æ˜¯ ResNet. $h_i = f(\\tilde x_i) = ResNet(\\tilde x_i)$ A small neural network projection head $g(\\cdot)$, $z_i = g(h_i) = W^{(2)}Ïƒ(W^{(1)}h_i)$. ä½œè€…å‘ç°åœ¨ $z_i$ ä¸Šè®¡ç®— contrast lossï¼Œæ¯” $h_i$ æ•ˆæœæ›´å¥½ã€‚ A contrastive loss functionï¼šNT-Xent (the normalized temperature-scaled cross entropy loss. å…¶ä¸­ $sim(u,v)=\\dfrac{u^Tv}{\\lVert u \\rVert \\lVert v\\rVert}$. Training with Large Batch Sizeä½œè€…é‡‡ç”¨äº†æ›´å¤§çš„batch size(256 $\\rightarrow$ 8192)ï¼Œå› è€Œä¸éœ€è¦memory bank. è¿™æ ·ä¸€ä¸ªbatchæœ‰ ($8192\\times 2=16382$) ä¸ªè´Ÿæ ·æœ¬ã€‚ åœ¨è¶…å¤§çš„batch sizeæƒ…å†µä¸‹ï¼Œä½¿ç”¨SGD/Momentumå­¦ä¹ ç‡ä¸ç¨³å®šï¼Œå› æ­¤ä½œè€…ä½¿ç”¨LARS optimizer. ä½œè€…ä½¿ç”¨ 32-128 cores TPUè¿›è¡Œè®­ç»ƒã€‚ï¼ˆè¿™çœŸçš„åŠé€€ã€‚ã€‚ã€‚ Global BNåœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„åœºæ™¯ä¸‹ï¼ŒBNçš„å‡å€¼å’Œæ–¹å·®æ˜¯åœ¨å•ä¸ªdeviceä¸Šè®¡ç®—çš„ã€‚è€Œä¸¤ä¸ªæ­£æ ·æœ¬æ˜¯åœ¨åŒä¸€ä¸ªdeviceä¸Šè®¡ç®—çš„ï¼Œå› æ­¤åœ¨æ‹‰è¿›ä¸¤ä¸ªæ­£æ ·æœ¬ä¹‹é—´çš„agreementæ—¶ï¼ŒBNä¼šé€ æˆä¿¡æ¯æ³„éœ²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…é‡‡ç”¨çš„æ–¹æ³•æ˜¯åœ¨æ‰€æœ‰çš„deviceä¸Šè®¡ç®—BNçš„å‡å€¼å’Œæ–¹å·®ã€‚ç±»ä¼¼åœ°è§£å†³è¿™ä¸€é—®é¢˜çš„æ–¹æ³•è¿˜æœ‰ï¼šshuffling data examples across devices[^10], replacing BN with layer norm[^7]. è¿™ç‚¹å…¶å®ä¸å¤ªç†è§£ï¼Œä¸ºå•¥BNä¼šé€ æˆä¿¡æ¯æ³„éœ²ï¼Ÿ Evaluation ProtocolDataset and Metrics.ä½œè€…å…ˆåœ¨CIFAR-10ä¸Šè¿›è¡Œè¯•éªŒï¼Œå¾—åˆ°äº†94.0%çš„å‡†ç¡®ç‡ï¼ˆæœ‰ç›‘ç£çš„å‡†ç¡®ç‡æ˜¯95.1%ï¼‰. ä¸ºäº†éªŒè¯å­¦ä¹ å¾—åˆ°çš„è§†è§‰è¡¨ç¤ºï¼Œä½œè€…é‡‡ç”¨å¹¿æ³›ä½¿ç”¨çš„linear evaluation protocol[^5][^6]: a linear classifier is trained on top of the frozen base network, and test accuracy is used as a proxy for representation quality. é™¤äº†linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning. Default settingWe use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space. As the loss, we use NT-Xent, optimized using LARS with learning rate of 4.8 (= 0.3 Ã— BatchSize/256) and weight decay of 10âˆ’6. We train at batch size 4096 for 100 epochs. Data Augmentation for Contrastive Representation LearningData augmentation defines predictive tasksæ•°æ®å¢å¼ºå®šä¹‰é¢„é¢„æµ‹ä»»åŠ¡ã€‚ éšæœºè£å‰ªæ—¢åŒ…æ‹¬äº† global and local views, ä¹ŸåŒ…æ‹¬äº† adjacent views. Composition of data augmentation operations is crucial for learning good representations ä¸ºäº†éªŒè¯ä¸åŒçš„æ•°æ®å¢å¼ºå¯¹äºè¡¨ç¤ºå­¦ä¹ çš„å½±å“ï¼Œä½œè€…è¿›è¡Œäº†ablationå®éªŒï¼Œåªå¯¹å›¾2ä¸­çš„æŸä¸€åˆ†æ”¯è¿›è¡Œtransformation. å®éªŒç»“æœå¦‚å›¾5æ‰€ç¤ºï¼Œå¯¹è§’çº¿åªæœ‰ä¸€ç§augmentationæ–¹æ³•ï¼Œéå¯¹è§’çº¿æ˜¯ä¸¤ç§ç»„åˆã€‚ ç»“æœè¡¨æ˜ï¼Œå•ä¸€çš„å¢å¼ºæ–¹æ³•éƒ½ä¸èƒ½å­¦åˆ°å¥½çš„è¡¨ç¤ºã€‚ä¸¤ç§ç»„åˆæ—¶ï¼Œé¢„æµ‹ä»»åŠ¡è¶Šéš¾ï¼Œå­¦ä¹ åˆ°çš„è¡¨ç¤ºèƒ½åŠ›è¶Šå¥½ã€‚æœ€å¥½çš„ç»„åˆæ˜¯ random crop å’Œ color distortion. ä½†æ˜¯åªæ˜¯å•ç‹¬ç”¨å…¶ä¸­æŸä¸€ç§æ•ˆæœéƒ½ä¸å¥½ã€‚ ä½œè€…å¯¹åªç”¨å•ç‹¬ä¸€ç§æ•°æ®å¢å¼ºæ–¹æ³•ä¸å¥½çš„åŸå› è¿›è¡Œäº†è§£é‡Šï¼š We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Figure 6 shows that color histograms alone suffice to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features. Contrastive learning needs stronger data augmentation than supervised learningç›¸æ¯”ç›‘ç£å­¦ä¹ ï¼Œstrongeræ•°æ®å¢å¼ºå¯¹contrastive learningæ›´ä¸ºé‡è¦ã€‚ When training supervised models with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning. Architectures for Encoder and HeadUnsupervised contrastive learning benefits (more) from bigger modelså¯¹æ¯”å­¦ä¹ åœ¨å¤§æ¨¡å‹ä¸‹è·ç›Šæ›´å¤šã€‚ A nonlinear projection head improves the representation quality of the layer before itä½œè€…æ¢ç©¶äº†projectionçš„ä¸‰ç§æ–¹å¼ï¼š identity mapping linear projection non-linear projection ç»“æœè¡¨æ˜ï¼Œéçº¿æ€§projectionæ›´å¥½ï¼Œæ²¡æœ‰çš„è¯æ•ˆæœå¾ˆå·®ã€‚ é™¤æ­¤ä¹‹å¤–ï¼Œä½¿ç”¨project headä¹‹å‰çš„hidden layer $h(i)$ æ¯”project layerä¹‹åçš„è¡¨ç¤º $z(i)$ æ•ˆæœæ›´å¥½, $\\ge 10%$ã€‚ ä¸ºä»€ä¹ˆä½¿ç”¨ non-linear projection head ä¹‹å‰çš„hidden layeræ•ˆæœæ›´å¥½ï¼Ÿ ä½œè€…è®¤ä¸ºå¯¹æ¯”lossä¼šæŸå¤±ä¿¡æ¯ã€‚z = g(h) è¢«è®­ç»ƒæˆtransformation invariant å˜æ¢ä¸å˜æ€§ï¼ˆå› ä¸ºcontrast lossè¦æ‹‰è¿‘ä¸¤ä¸ªä¸åŒå˜æ¢çš„æ­£æ ·æœ¬ï¼‰ã€‚å› æ­¤ï¼Œ$g(\\cdot)$ ä¼šä¸¢å¤±ä¿¡æ¯ã€‚ ä½œè€…é€šè¿‡å®éªŒéªŒè¯è¿™ä¸€çŒœæƒ³ï¼Œåœ¨ä¿è¯æœ€ç»ˆçš„dimensionä¸å˜çš„æƒ…å†µä¸‹ï¼Œ Loss Functions and Batch SizeNormalized cross entropy loss with adjustable temperature works better than alternatives å®éªŒè¡¨æ˜ NT-Xent æ•ˆæœæœ€å¥½ã€‚è¿™æ˜¯å› ä¸ºå…¶ä»–çš„ç›®æ ‡å‡½æ•°å¹¶æ²¡æœ‰è¡¡é‡è´Ÿæ ·æœ¬çš„éš¾åº¦ï¼šunlike cross-entropy, other objective functions do not weight the negatives by their relative hardness. $l_2$ normalization å’Œ temperture å¾ˆé‡è¦ï¼š$l_2$ normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives; æ²¡æœ‰ $l_2$ normalization,å°½ç®¡å¯¹æ¯”å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†æ˜¯å­¦ä¹ åˆ°çš„è¡¨ç¤ºèƒ½åŠ›å¹¶ä¸å¥½ã€‚ åˆé€‚çš„tempertureä¹Ÿå¾ˆé‡è¦ Contrastive learning benefits (more) from larger batch sizes and longer training å®éªŒè¡¨æ˜ï¼Œbatch sizeå¾ˆé‡è¦ï¼Œè¶Šå¤§æ”¶æ•›çš„è¶Šå¿«ï¼Œä½†æœ€ç»ˆæ•ˆæœä¹Ÿä¸æ˜¯è¶Šå¤§è¶Šå¥½ã€‚éšç€è®­ç»ƒçš„å¢åŠ ï¼Œbatch sizeé€ æˆçš„è¡¨ç°å·®å¼‚ä¹Ÿéšç€é€æ¸æ¶ˆå¤±ã€‚ Comparison with State-of-the-artä½œè€…é‡‡ç”¨äº†ä¸‰ç§æ–¹æ³•æ¥éªŒè¯performanceã€‚ Linear evaluationç›¸æ¯”fine-tuneï¼Œlinear evaluation çš„åŒºåˆ«åœ¨äºå­¦ä¹ ç‡çš„è®¾ç½®ã€‚ æ²¡ææ‡‚ä¸ºå•¥å« linear evaluationï¼Ÿ å’Œfine-tuneçš„åŒºåˆ«å°±åœ¨äºå­¦ä¹ ç‡çš„è®¾ç½®ï¼Ÿ Semi-supervised learningsample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (âˆ¼12.8 and âˆ¼128 images per class respectively). Transfer learningåœ¨imageNetä¸Šè®­ç»ƒï¼Œåœ¨å…¶ä»–æ•°æ®é›†ä¸Šæµ‹è¯•ã€‚åŒä¸Šï¼Œé‡‡ç”¨äº†ä¸¤ç§æ–¹å¼ï¼Œ Linear evaluation å’Œ fine-tune. ### speech-basedRepresentation Learning with Contrastive Predictive Codingwav2vec: Unsupervised pre-training for speech recognition encoder: $X \\rightarrow Z$ content-network: $C\\rightarrow C$ noise contrastive binary classification task. $\\sigma$ æ˜¯ sigmoid å‡½æ•° $c_i$ æ˜¯å½“å‰stepçš„content feature $h_k(x_i)$ æ˜¯step-specific affine transformation $h_k(c_i) = W_kc_i+b_k$ $z_{i+k}$ æ˜¯è·ç¦»å½“å‰stepä¸ºkçš„encoded feature, ä¸ºæ­£æ ·æœ¬ $\\hat z\\sim p_n$æ˜¯ (T-k) å¸§ä¸­å‡åŒ€é€‰æ‹©10ä¸ªè´Ÿæ ·æœ¬ï¼Œå¾—åˆ°å¯¹æ•°æ¦‚ç‡çš„æœŸæœ›åï¼Œå†ä¹˜ä»¥ $\\lambda=10$. â€“&gt; MoCo BYOLpaper: Bootstrap Your Own Latent A New Approach to Self-Supervised Learning [1] : A fast learning algorithm for deep belief nets.[2]: Auto-encoding variational bayes.[3]: Generative adversarial nets. NIPS2014[4]: Discriminative unsupervised feature learning with convolutional neural networks. NIPS2014[5]: Representation learning with contrastive predictive coding. arXiv2018[6]: Learning representations by maximizing mutual information across views. NIPS2019[7]: CPC: Data-efficient image recognition with contrastive predictive coding, arXiv2019[8]: Unsupervised feature learning via non-parametric instance discrimination. CVPR2018[9]: Contrastive multiview coding. arXiv2019[10]: MoCo: Momentum contrast for unsupervised visual representation learning, arXiv2019[11]: Self-supervised learning of pretext-invariant representations. arXiv2019","link":"/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/"},{"title":"è®ºæ–‡ç¬”è®°-batch,layer,weights normalization","text":"paper: Batch Normalization Layer Normalization weights Normalization Batch Normalizationåœ¨ä¹‹å‰çš„ç¬”è®°å·²ç»è¯¦ç»†çœ‹è¿‡äº†:æ·±åº¦å­¦ä¹ -Batch Normalization Layer NormalizationMotivation batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. å…³äº batch normalisztion. ä» Ng çš„è¯¾ä¸Šæˆªæ¥çš„ä¸€å¼ å›¾ï¼Œå…¨é“¾æ¥å±‚ç›¸æ¯”å·ç§¯å±‚æ›´å®¹æ˜“ç†è§£ç‚¹ï¼Œä½†å½¢å¼ä¸Šæ˜¯ä¸€æ ·çš„. æ ·æœ¬æ•°é‡æ˜¯ mï¼Œç¬¬ l å±‚ç»è¿‡æ¿€æ´»å‡½æ•°è¾“å‡ºæ˜¯ç¬¬ l+1 å±‚çš„è¾“å…¥ï¼Œå…¶ä¸­ç¬¬ i ä¸ªç¥ç»å…ƒçš„å€¼: çº¿æ€§è¾“å‡ºï¼š $z_i^l={w_i^l}^Th^l$. éçº¿æ€§è¾“å‡ºï¼š $h_i^{l+1} = a_i^l=f(z_i^l+b_i^l)$ å…¶ä¸­ f æ˜¯éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œ$a_i^l$ æ˜¯ä¸‹ä¸€å±‚çš„ summed inputs. å¦‚æœ $a_i^l$ çš„åˆ†å¸ƒå˜åŒ–è¾ƒå¤§ï¼ˆchange in a highly correlated wayï¼‰,ä¸‹ä¸€å±‚çš„æƒé‡ $w^{l+1}$ çš„æ¢¯åº¦ä¹Ÿä¼šç›¸åº”å˜åŒ–å¾ˆå¤§ï¼ˆåå‘ä¼ æ’­ä¸­ $w^{l+1}$ çš„æ¢¯åº¦å°±æ˜¯ $a_i^l$ï¼‰ã€‚ Batch Normalization å°±æ˜¯å°†çº¿æ€§è¾“å‡ºå½’ä¸€åŒ–ã€‚ å…¶ä¸­ $u_i^l$ æ˜¯å‡å€¼ï¼Œ$\\sigma_i^l$ æ˜¯æ–¹å·®ã€‚ $\\overline a_i^l$ æ˜¯å½’ä¸€åŒ–ä¹‹åçš„è¾“å‡ºã€‚ $g_i^l$ æ˜¯éœ€è¦å­¦ä¹ çš„å‚æ•°ï¼Œä¹Ÿå°±æ˜¯ scale. æœ‰ä¸ªç–‘é—®ï¼Ÿä¸ºä»€ä¹ˆ BN è¦åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰è¿›è¡Œï¼Œè€Œä¸æ˜¯ä¹‹åè¿›è¡Œå‘¢ï¼Ÿ ä¸Šå›¾ä¸­æ˜¯å•ä¸ªæ ·æœ¬ï¼Œè€Œæ‰€æœ‰çš„æ ·æœ¬å…¶å®æ˜¯å…±äº«å±‚ä¸å±‚ä¹‹é—´çš„å‚æ•°çš„ã€‚æ ·æœ¬ä¸æ ·æœ¬ä¹‹é—´ä¹Ÿå­˜åœ¨å·®å¼‚ï¼Œæ‰€ä»¥åœ¨æŸä¸€ä¸ªç‰¹å¾ç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œï¼ˆæ¯ä¸€å±‚å…¶ä¸­çš„ä¸€ä¸ªç¥ç»å…ƒå¯ä»¥çœ‹ä½œä¸€ä¸ªç‰¹å¾ç»´åº¦ï¼‰ã€‚ batch normalization requires running averages of the summed input statistics. In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps. BN ä¸æ˜¯ç”¨äº RNN æ˜¯å› ä¸º batch ä¸­çš„ sentence é•¿åº¦ä¸ä¸€è‡´ã€‚æˆ‘ä»¬å¯ä»¥æŠŠæ¯ä¸€ä¸ªæ—¶é—´æ­¥çœ‹ä½œä¸€ä¸ªç»´åº¦çš„ç‰¹å¾æå–ï¼Œå¦‚æœåƒ BN ä¸€æ ·åœ¨è¿™ä¸ªç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œæ˜¾ç„¶åœ¨ RNN ä¸Šæ˜¯è¡Œä¸é€šçš„ã€‚æ¯”å¦‚è¿™ä¸ª batch ä¸­æœ€é•¿çš„åºåˆ—çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œä»–çš„å‡å€¼å°±æ˜¯å®ƒæœ¬èº«äº†ï¼Œå²‚ä¸æ˜¯å‡ºç°äº† BN åœ¨å•ä¸ªæ ·æœ¬ä¸Šè®­ç»ƒçš„æƒ…å†µã€‚ In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. æ‰€ä»¥ä½œè€…åœ¨è¿™ç¯‡ paper ä¸­æå‡ºäº† Layer Normalization. åœ¨å•ä¸ªæ ·æœ¬ä¸Šè®¡ç®—å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ã€‚ç„¶è€Œæ˜¯æ€ä¹ˆè¿›è¡Œçš„å‘¢ï¼Ÿ Layer Normalizationlayer normalization å¹¶ä¸æ˜¯åœ¨æ ·æœ¬ä¸Šæ±‚å¹³å‡å€¼å’Œæ–¹å·®ï¼Œè€Œæ˜¯åœ¨ hidden units ä¸Šæ±‚å¹³å‡å€¼å’Œæ–¹å·®ã€‚ å…¶ä¸­ H æ˜¯ hidden units çš„ä¸ªæ•°ã€‚ BN å’Œ LN çš„å·®å¼‚ï¼š Layer normalisztion åœ¨å•ä¸ªæ ·æœ¬ä¸Šå–å‡å€¼å’Œæ–¹å·®ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µéƒ½æ˜¯ä¸€è‡´çš„ã€‚ å¹¶ä¸”ï¼Œå°½ç®¡æ±‚å‡å€¼å’Œæ–¹å·®çš„æ–¹å¼ä¸ä¸€æ ·ï¼Œä½†æ˜¯åœ¨è½¬æ¢æˆ beta å’Œ gamma çš„æ–¹å¼æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯åœ¨ channels æˆ–è€…è¯´ hidden_size ä¸Šè¿›è¡Œçš„ã€‚ Layer normalized recurrent neural networks RNN is common among the NLP tasks to have different sentence lengths for different training cases. This is easy to deal with in an RNN because the same weights are used at every time-step. But when we apply batch normalization to an RNN in the obvious way, we need to to compute and store separate statistics for each time step in a sequence. This is problematic if a test sequence is longer than any of the training sequences. Layer normalization does not have such problem because its normalization terms depend only on the summed inputs to a layer at the current time-step. It also has only one set of gain and bias parameters shared over all time-steps. è¿™ä¸€éƒ¨åˆ†ä¹Ÿè§£é‡Šäº† BN ä¸é€‚ç”¨äº RNN çš„åŸå› ï¼Œä» test sequence longer çš„è§’åº¦ã€‚RNN çš„æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®—å…±äº«å‚æ•°æƒé‡. $a^t=W_{hh}h^{t-1}+W_{xh}x^t$ å…¶ä¸­ b å’Œ g æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚ layer normalize åœ¨ LSTM ä¸Šçš„ä½¿ç”¨ï¼š tensorflow å®ç°batch Normalization12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061tf.reset_default_graph()from tensorflow.python.training.moving_averages import assign_moving_averagefrom tensorflow.contrib.layers import batch_norm### batch normalizationdef batch_norm(inputs, decay=0.9, is_training=True, epsilon=1e-6): &quot;&quot;&quot; :param inputs: [batch, length, width, channels] :param is_training: :param eplison: :return: &quot;&quot;&quot; pop_mean = tf.Variable(tf.zeros(inputs.shape[-1]), trainable=False, name=&quot;pop_mean&quot;) pop_var = tf.Variable(tf.ones(inputs.shape[-1]), trainable=False, name=&quot;pop_variance&quot;) def update_mean_and_var(): axes = list(range(inputs.shape.ndims)) batch_mean, batch_var = tf.nn.moments(inputs, axes=axes) moving_average_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1-decay)) # ä¹Ÿå¯ç”¨ assign_moving_average(pop_mean, batch_mean, decay) moving_average_var = tf.assign(pop_var, pop_var * decay + batch_var * (1-decay)) # ä¹Ÿå¯ç”¨ assign_moving_average(pop_var, batch_var, decay) with tf.control_dependencies([moving_average_mean, moving_average_var]): return tf.identity(batch_mean), tf.identity(batch_var) mean, variance = tf.cond(tf.equal(is_training, True), update_mean_and_var, lambda: (pop_mean, pop_var)) beta = tf.Variable(initial_value=tf.zeros(inputs.get_shape()[-1]), name=&quot;shift&quot;) gamma = tf.Variable(initial_value=tf.ones(inputs.get_shape()[-1]), name=&quot;scale&quot;) return tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon) layer normalization12345678910111213141516171819202122232425262728293031323334353637import tensorflow as tfbatch = 60hidden_size = 64whh = tf.random_normal(shape=[batch, hidden_size], mean=5.0, stddev=10.0)whh_norm = tf.contrib.layers.layer_norm(inputs=whh, center=True, scale=True)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(whh) print(whh_norm) print(sess.run([tf.reduce_mean(whh[0]), tf.reduce_mean(whh[1])])) print(sess.run([tf.reduce_mean(whh_norm[0]), tf.reduce_mean(whh_norm[5]), tf.reduce_mean(whh_norm[59])])) print(sess.run([tf.reduce_mean(whh_norm[:,0]), tf.reduce_mean(whh_norm[:,1]), tf.reduce_mean(whh_norm[:,63])])) print(&quot;\\n&quot;) for var in tf.trainable_variables(): print(var) print(sess.run(var)) 12345678910111213141516171819202122232425262728293031Tensor(&quot;random_normal:0&quot;, shape=(60, 64), dtype=float32)Tensor(&quot;LayerNorm/batchnorm/add_1:0&quot;, shape=(60, 64), dtype=float32)[5.3812757, 4.607581][-1.4901161e-08, -2.9802322e-08, -3.7252903e-09][-0.22264712, 0.14112064, -0.07268284]&lt;tf.Variable 'LayerNorm/beta:0' shape=(64,) dtype=float32_ref&gt;[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]&lt;tf.Variable 'LayerNorm/gamma:0' shape=(64,) dtype=float32_ref&gt;[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] å‘ç°ä¸€ä¸ªå¾ˆå¥‡æ€ªçš„é—®é¢˜ï¼Œ layer norm æ˜¯åœ¨æ¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ä¸Šæ±‚å‡å€¼å’Œæ–¹å·®ï¼Œä¸ºå•¥ beta å’Œ gamma çš„shapeå´æ˜¯ [hidden_size]. æŒ‰ç†è¯´ä¸åº”è¯¥æ˜¯ [batch,] å—ï¼Ÿ å¸¦ç€ç–‘é—®å»çœ‹äº†æºç ï¼ŒåŸæ¥æ˜¯è¿™æ ·çš„ã€‚ã€‚ å°†æºç ç”¨ç®€ä»‹çš„æ–¹å¼å†™å‡ºæ¥äº†ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 import tensorflow as tfdef layer_norm_mine(inputs, epsilon=1e-12, center=True, scale=True): &quot;&quot;&quot; inputs: [batch, sequence_len, hidden_size] or [batch, hidden_size] &quot;&quot;&quot; inputs_shape = inputs.shape inputs_rank = inputs_shape.ndims params_shape = inputs_shape[-1:] beta, gamma = None, None if center: beta = tf.get_variable( name=&quot;beta&quot;, shape=params_shape, initializer=tf.zeros_initializer(), trainable=True ) if scale: gamma = tf.get_variable( name=&quot;gamma&quot;, shape=params_shape, initializer=tf.ones_initializer(), trainable=True ) norm_axes = list(range(1, inputs_rank)) mean, variance = tf.nn.moments(inputs, norm_axes, keep_dims=True) # [batch] inv = tf.rsqrt(variance + epsilon) inv *= gamma return inputs*inv + ((beta-mean)*inv if beta is not None else - mean * inv)batch = 60hidden_size = 64whh = tf.random_normal(shape=[batch, hidden_size], mean=5.0, stddev=10.0)whh_norm = layer_norm_mine(whh) layer_norm_mine å¾—åˆ°çš„ç»“æœä¸æºç ä¸€è‡´ã€‚å¯ä»¥å‘ç° è®¡ç®—å‡å€¼å’Œæ–¹å·®æ—¶ï¼Œ tf.nn.moments ä¸­ axes=[1:-1]. ï¼ˆtf.nn.moments ä¸­ axes çš„å«ä¹‰æ˜¯åœ¨è¿™äº›ç»´åº¦ä¸Šæ±‚å‡å€¼å’Œæ–¹å·®ï¼‰. ä¹Ÿå°±æ˜¯è¯´å¾—åˆ°çš„å‡å€¼å’Œæ–¹å·®ç¡®å®æ˜¯ [batch,]. åªæ˜¯åœ¨è½¬æ¢æˆ beta å’Œ gamma çš„åˆ†å¸ƒæ—¶ï¼Œä¾æ—§æ˜¯åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œçš„ã€‚æœ‰æ„æ€ï¼Œæ‰€ä»¥æœ€ç»ˆçš„æ•ˆæœåº”è¯¥å’Œ batch normalization æ•ˆæœæ˜¯ä¸€è‡´çš„ã€‚åªä¸è¿‡æ˜¯å¦ç¬¦åˆå›¾åƒæˆ–æ–‡æœ¬çš„ç‰¹æ€§å°±å¦è¯´äº†ã€‚ LayerNormBasicLSTMCell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245class LayerNormBasicLSTMCell(rnn_cell_impl.RNNCell): &quot;&quot;&quot;LSTM unit with layer normalization and recurrent dropout. This class adds layer normalization and recurrent dropout to a basic LSTM unit. Layer normalization implementation is based on: https://arxiv.org/abs/1607.06450. &quot;Layer Normalization&quot; Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton and is applied before the internal nonlinearities. Recurrent dropout is base on: https://arxiv.org/abs/1603.05118 &quot;Recurrent Dropout without Memory Loss&quot; Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth. &quot;&quot;&quot; def __init__(self, num_units, forget_bias=1.0, input_size=None, activation=math_ops.tanh, layer_norm=True, norm_gain=1.0, norm_shift=0.0, dropout_keep_prob=1.0, dropout_prob_seed=None, reuse=None): &quot;&quot;&quot;Initializes the basic LSTM cell. Args: num_units: int, The number of units in the LSTM cell. forget_bias: float, The bias added to forget gates (see above). input_size: Deprecated and unused. activation: Activation function of the inner states. layer_norm: If `True`, layer normalization will be applied. norm_gain: float, The layer normalization gain initial value. If `layer_norm` has been set to `False`, this argument will be ignored. norm_shift: float, The layer normalization shift initial value. If `layer_norm` has been set to `False`, this argument will be ignored. dropout_keep_prob: unit Tensor or float between 0 and 1 representing the recurrent dropout probability value. If float and 1.0, no dropout will be applied. dropout_prob_seed: (optional) integer, the randomness seed. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. &quot;&quot;&quot; super(LayerNormBasicLSTMCell, self).__init__(_reuse=reuse) if input_size is not None: logging.warn(&quot;%s: The input_size parameter is deprecated.&quot;, self) self._num_units = num_units self._activation = activation self._forget_bias = forget_bias self._keep_prob = dropout_keep_prob self._seed = dropout_prob_seed self._layer_norm = layer_norm self._norm_gain = norm_gain self._norm_shift = norm_shift self._reuse = reuse @property def state_size(self): return rnn_cell_impl.LSTMStateTuple(self._num_units, self._num_units) @property def output_size(self): return self._num_units def _norm(self, inp, scope, dtype=dtypes.float32): shape = inp.get_shape()[-1:] gamma_init = init_ops.constant_initializer(self._norm_gain) beta_init = init_ops.constant_initializer(self._norm_shift) with vs.variable_scope(scope): # Initialize beta and gamma for use by layer_norm. vs.get_variable(&quot;gamma&quot;, shape=shape, initializer=gamma_init, dtype=dtype) vs.get_variable(&quot;beta&quot;, shape=shape, initializer=beta_init, dtype=dtype) normalized = layers.layer_norm(inp, reuse=True, scope=scope) return normalized def _linear(self, args): out_size = 4 * self._num_units proj_size = args.get_shape()[-1] dtype = args.dtype weights = vs.get_variable(&quot;kernel&quot;, [proj_size, out_size], dtype=dtype) out = math_ops.matmul(args, weights) if not self._layer_norm: bias = vs.get_variable(&quot;bias&quot;, [out_size], dtype=dtype) out = nn_ops.bias_add(out, bias) return out def call(self, inputs, state): &quot;&quot;&quot;LSTM cell with layer normalization and recurrent dropout.&quot;&quot;&quot; c, h = state args = array_ops.concat([inputs, h], 1) concat = self._linear(args) dtype = args.dtype i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1) if self._layer_norm: i = self._norm(i, &quot;input&quot;, dtype=dtype) j = self._norm(j, &quot;transform&quot;, dtype=dtype) f = self._norm(f, &quot;forget&quot;, dtype=dtype) o = self._norm(o, &quot;output&quot;, dtype=dtype) g = self._activation(j) if (not isinstance(self._keep_prob, float)) or self._keep_prob &lt; 1: g = nn_ops.dropout(g, self._keep_prob, seed=self._seed) new_c = ( c * math_ops.sigmoid(f + self._forget_bias) + math_ops.sigmoid(i) * g) if self._layer_norm: new_c = self._norm(new_c, &quot;state&quot;, dtype=dtype) new_h = self._activation(new_c) * math_ops.sigmoid(o) new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h) return new_h, new_state","link":"/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/"},{"title":"è®ºæ–‡ç¬”è®°-Contextual Augmentation","text":"paper 1Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations åœ¨ NLP é¢†åŸŸï¼Œæ•°æ®å¢å¼ºæ¯”å›¾åƒé¢†åŸŸè¦å¤æ‚çš„å¤šã€‚å®ƒå¾ˆéš¾æœ‰ä¸€ä¸ªç»Ÿä¸€çš„è§„åˆ™çš„å»é€‚ç”¨äºå„ç§ domain. ç›®å‰å¸¸ç”¨çš„æ–¹æ³•æ˜¯ åŸºäº WordNet çš„åŒä¹‰è¯æ›¿æ¢ï¼Œä»¥åŠæ ¹æ®è·ç¦»è®¡ç®—çš„è¯çš„ç›¸ä¼¼åº¦ã€‚ä½†æ˜¯ï¼ŒåŒä¹‰è¯æ˜¯å¾ˆå°‘çš„ï¼Œå¹¶ä¸”ï¼Œä¸€ä¸ªè¯æœ¬èº«ä¹Ÿæ˜¯å¤šä¹‰çš„ï¼Œå®ƒåœ¨ WordNet ä¸­çš„åŒä¹‰è¯ä¹Ÿè®¸å¹¶ä¸é€‚åˆå½“å‰çš„è¯­å¢ƒï¼Œè€Œè¿™ä¸€é™åˆ¶ä¹Ÿæ˜¯å¾ˆéš¾é€šè¿‡ä¸€ä¸ªè§„åˆ™å»é™å®šã€‚æ‰€ä»¥ï¼Œä¼ ç»Ÿçš„æ–¹æ³•éƒ½å¾ˆéš¾å‘æŒ¥ã€‚ ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼š åŸºäºè§„åˆ™ï¼Œèšç±»ï¼Œäººå·¥å¹²é¢„ã€‚ Wang and Yang(2015) åŸºäºè¿‘ä¹‰è¯ Character-level convolutional networks for text classification ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ŒåŸºäºè¯­è¨€æ¨¡å‹çš„ contextual augmentation. æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹å¾—åˆ°çš„ä¸åŒçš„è¯å…·æœ‰èŒƒå¼å…³ç³» paradigmatic relations. æ›´è¿›ä¸€æ­¥çš„ï¼Œä¸ºäº†è®©ç”Ÿæˆçš„è¯ä¸å½“å‰å¥å­çš„ label æ˜¯å…¼å®¹çš„ (compatible)ï¼Œä½œè€…åŠ å…¥äº† label-conditional. æ ¹æ®ä¸Šæ–‡é¢„æµ‹ target position i $w_i$, $P(\\cdot|S/{w_i})$. åŠ ä¸Š label é™åˆ¶æ¡ä»¶ä¹‹åå°±æ˜¯ $P(\\cdot|y, S/{w_i})$ ä½œè€…å¹¶ä¸æ˜¯ç›´æ¥ä½¿ç”¨ top-k. è€Œæ˜¯åˆ©ç”¨çš„é€€ç«çš„æ–¹æ³•ï¼Œtemperature parameter $\\tau$, è¿™æ ·é¢„æµ‹åˆ†å¸ƒå°±æ˜¯ $$P(\\cdot|y, S/{w_i})^{1/\\tau}$$ å½“ $\\tau \\rightarrow \\infty$ æ—¶ï¼Œé‚£ä¹ˆå¯¹åº”çš„é¢„æµ‹è¯åˆ†å¸ƒæ˜¯ uniform distribution. å½“ $\\tau \\rightarrow 0$ æ—¶ï¼Œé¢„æµ‹å¾—åˆ°çš„å°±æ˜¯æ¦‚ç‡æœ€å¤§çš„è¯ã€‚æˆ‘çŒœä½œè€…è¿™æ ·åšçš„ç›®çš„æ˜¯è®©ç”Ÿæˆçš„è¯æ›´ä¸°å¯Œï¼Œé¿å…å•ä¸€åŒ–ã€‚ åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å¯¹æ¯”å®éªŒï¼Œä¸ synonym åŒä¹‰è¯æ›¿æ¢è¿›è¡Œçš„å¯¹æ¯”ï¼Œåˆ†ç±»æ¨¡å‹éƒ½ç”¨çš„ CNN. æ•ˆæœè¿˜å¯ä»¥ï¼Œä½†æ˜¯å¹¶ä¸æ˜æ˜¾ã€‚è€Œ w/synonym åè€Œä¼šæœ‰åä½œç”¨ã€‚ã€‚ å…¶ä¸­æŸä¸ªæ ·æœ¬çš„æ•ˆæœå±•ç¤ºï¼š paper 2Conditional BERT Contextual Augmentation çœ‹å®ŒpaperçœŸçš„æƒ³åæ§½ä¸‹ä¸­å›½äººå†™çš„è®ºæ–‡çœŸçš„ç»™äººä¸€ç§ç²—åˆ¶æ»¥é€ çš„æ„Ÿè§‰ã€‚ã€‚å°½ç®¡paperå‡ºæ¥çš„å¾ˆåŠæ—¶ï¼Œä¸ Bert ç»“åˆä¹Ÿå¾ˆèµï¼Œä½†æ˜¯ ã€‚ã€‚ã€‚å…¶å®è¿˜å¯ä»¥å¥½å¥½å†™ï¼Œè¿˜å¯ä»¥å¤šåˆ†æåˆ†æï¼Œçœ‹ paper éƒ½èƒ½æ„Ÿè§‰åˆ°ä¸€ç§ä¸ºäº†å‘è®ºæ–‡è€Œå‘è®ºæ–‡çš„æ„Ÿè§‰ï¼Œæˆ‘è‡ªå·±åˆä½•å°ä¸æ˜¯å‘¢ã€‚ã€‚ã€‚ Bert æƒ³å¯¹äº LSTM æ›´è·å¾—æ›´æ·±å±‚çš„å«ä¹‰ã€‚ å°† segmentation embedding æ¢æˆ label embedding. ç„¶åä½¿ç”¨é¢„è®­ç»ƒçš„ BERT æ¨¡å‹è¿›è¡Œ fine-tune. è¿­ä»£ç›´åˆ°æ”¶æ•›ï¼Œç”Ÿæˆæ–°çš„å¥å­åï¼Œåœ¨è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ã€‚ When the task-specific dataset is with more than two different labels,we should re-train a label size compatible label embeddings layer instead of directly fine-tuning the pre-trained one. å¯¹äºå¤šæ ‡ç­¾ä»»åŠ¡ï¼Œä½œè€…æ˜¯è¿™ä¹ˆè¯´çš„ï¼Œä¸å¤ªæ˜ç™½ã€‚","link":"/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/"},{"title":"è®ºæ–‡ç¬”è®°-fast transformer","text":"Long range arena: A benchmark for efficient transformers Convolution and Transformer Swin transformer: Hierarchical vision transformer using shifted windows. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding Incorporating convolution designs into visual transformers On the relationship between self-attention and convolutional layers local windows Image transformer Blockwise self\u0002-attention for long document understanding Axial pattern attention in multidimensional transformers. Adaptive span Adaptive attention span in transformers. approximation Linformer: Self-attention with linear complexity Rethinking attention with performers. Linear Transformer: Transformers are RNNs: Fast autoregressive transformers with linear attention. Efficient attention: Attention with linear complexities NystrÃ¶mformer: A nystrÃ¶m-based algorithm for approximating self-attention. Fnet: Mixing tokens with fourier transforms. XCiT: Cross-Covariance Image Transformers Scatterbrain: Unifying Sparse and Low-rank Attention Approximation Transformer dissection: An unified understanding for transformerâ€™s attention via the lens of kernel.","link":"/2021/11/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-fast-transformer/"},{"title":"è®ºæ–‡ç¬”è®° memory networks","text":"Memory Networks ç›¸å…³è®ºæ–‡ç¬”è®°ã€‚ Memory Network with strong supervision End-to-End Memory Network Dynamic Memory Network Paper reading 1: Memory Networks, Jason WestonMotivationRNNs å°†ä¿¡æ¯å‹ç¼©åˆ°final stateä¸­çš„æœºåˆ¶ï¼Œä½¿å¾—å…¶å¯¹ä¿¡æ¯çš„è®°å¿†èƒ½åŠ›å¾ˆæœ‰é™ã€‚è€Œmemory workçš„æå‡ºå°±æ˜¯å¯¹è¿™ä¸€é—®é¢˜è¿›è¡Œæ”¹å–„ã€‚ However, their memory (encoded by hidden states and weights) is typically too small, and is not compartmentalized enough to accurately remember facts from the past (knowledge is compressed into dense vectors). RNNs are known to have difficulty in performing memorization. Memory Networks æå‡ºçš„åŸºæœ¬åŠ¨æœºæ˜¯æˆ‘ä»¬éœ€è¦ é•¿æœŸè®°å¿†ï¼ˆlong-term memoryï¼‰æ¥ä¿å­˜é—®ç­”çš„çŸ¥è¯†æˆ–è€…èŠå¤©çš„è¯­å¢ƒä¿¡æ¯ï¼Œè€Œç°æœ‰çš„ RNN åœ¨é•¿æœŸè®°å¿†ä¸­è¡¨ç°å¹¶æ²¡æœ‰é‚£ä¹ˆå¥½ã€‚ Memory Networks four components: I:(input feature map) æŠŠè¾“å…¥æ˜ å°„ä¸ºç‰¹å¾å‘é‡ï¼Œå¯ä»¥åŒ…æ‹¬å„ç§ç‰¹å¾å·¥ç¨‹ï¼Œæ¯”å¦‚parsing, coreference, entity resolution,ä¹Ÿå¯ä»¥æ˜¯RNN/LSTM/GRUã€‚é€šå¸¸ä»¥å¥å­ä¸ºå•ä½ï¼Œå°†sentenceç”¨å‘é‡è¡¨ç¤ºï¼Œä¸€ä¸ªå¥å­å¯¹åº”ä¸€ä¸ªsparse or dense feature vector. G:(generalization) ä½¿ç”¨æ–°çš„è¾“å…¥æ•°æ®æ›´æ–° memories O:(output feature map) ç»™å®šæ–°çš„è¾“å…¥å’Œç°æœ‰çš„ memory stateï¼Œåœ¨ç‰¹å¾ç©ºé—´é‡Œäº§ç”Ÿè¾“å‡º R:(response) å°†è¾“å‡ºè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ è¯¦ç»†æ¨å¯¼è¿‡ç¨‹ 1.I component: :encode input text to internal feature representation. å¯ä»¥é€‰æ‹©å¤šç§ç‰¹å¾ï¼Œæ¯”å¦‚bag of words, RNN encoder states, etc. 2.G component: generalization å°±æ˜¯ç»“åˆ old memorieså’Œè¾“å…¥æ¥æ›´æ–° memories. $m_i=G(m_i, I(x),m), âˆ€i$ æœ€ç®€å•çš„æ›´æ–°memoryçš„æ–¹æ³•æ˜¯ $m_{H(x)}=I(x)$, $H(x)$ æ˜¯ä¸€ä¸ªå¯»å€å‡½æ•°slot selecting functionï¼ŒGæ›´æ–°çš„æ˜¯ m çš„indexï¼Œå¯ä»¥æŠŠæ–°çš„memory mï¼Œä¹Ÿå°±æ˜¯æ–°çš„è¾“å…¥ I(x) ä¿å­˜åˆ°ä¸‹ä¸€ä¸ªç©ºé—²çš„åœ°å€ $m_n$ ä¸­ï¼Œå¹¶ä¸æ›´æ–°åŸæœ‰çš„memory. æ›´å¤æ‚çš„ G å‡½æ•°å¯ä»¥å»æ›´æ–°æ›´æ—©çš„memoryï¼Œç”šè‡³æ˜¯æ‰€æœ‰çš„memory. è¿™é‡Œçš„æ–°çš„inputï¼Œå¦‚æœåœ¨QAä¸­å°±æ˜¯question å’Œ old memmoryçš„ç»„åˆ $[I(x), m_i]$. 3.O component: reading from memories and performing inference, calculating what are the relevant memories to perform a good response. ç»™å®šæ–°çš„è¾“å…¥å’Œmemoryï¼Œåœ¨memoriesä¸­å¯»æ‰¾æœ€ç›¸å…³çš„kä¸ªè®°å¿† å¦‚æœk=2ï¼š $$o_1=O_1(q,m)=argmax_{i=1,2,..,N}s_O(q,m_i)$$ $$o_2=O_2(q,m)=argmax_{i=1,2,..,N}s_O([q,o_1],m_i)$$ output: $[q,o_1, o_2]$ ä¹Ÿæ˜¯module Rçš„è¾“å…¥. $s_O$ is a function that scores the match between the pair of sentences x and mi. $s_O$ ç”¨æ¥è¡¨å¾ question x å’Œ è®°å¿† $m_i$ çš„ç›¸å…³ç¨‹åº¦ã€‚ $$s_O=qUU^Tm$$ $s_O$ è¡¨ç¤ºé—®é¢˜qå’Œå½“å‰memory mçš„ç›¸å…³ç¨‹åº¦ Uï¼šbilinear regressionå‚æ•°ï¼Œç›¸å…³äº‹å®çš„ $qUU^Tm_{true}$ çš„scoreé«˜äºä¸ç›¸å…³äº‹å®çš„åˆ†æ•° $qUU^Tm_{random}$ 4.R component : å¯¹ output feature o è¿›è¡Œè§£ç ï¼Œå¾—åˆ°æœ€åçš„response: r=R(o) $$r=argmax_{w\\in W}s_R([q,m_{o_1},m_{o_2}],w)$$ W æ˜¯è¯å…¸ï¼Œ$s_R$ è¡¨ç¤ºä¸output feature o æœ€ç›¸å…³çš„å•è¯ã€‚ $s_R$ å’Œ $s_O$ çš„å½¢å¼æ˜¯ç›¸åŒçš„ã€‚ $$s(x,y)=xUU^Ty$$ Huge Memory é—®é¢˜å¦‚æœmemoryå¤ªå¤§ï¼Œæ¯”å¦‚ Freebase or Wikipediaï¼Œ å¯ä»¥æŒ‰ entity æˆ–è€… topic æ¥å­˜å‚¨ memoryï¼Œè¿™æ · G å°±ä¸ç”¨åœ¨æ•´ä¸ª memories ä¸Šæ“ä½œäº† å¦‚æœ memory æ»¡äº†ï¼Œå¯ä»¥å¼•å…¥ forgetting æœºåˆ¶ï¼Œæ›¿æ¢æ‰æ²¡é‚£ä¹ˆæœ‰ç”¨çš„ memoryï¼ŒH å‡½æ•°å¯ä»¥è®¡ç®—æ¯ä¸ª memory çš„åˆ†æ•°ï¼Œç„¶åé‡å†™ è¿˜å¯ä»¥å¯¹å•è¯è¿›è¡Œ hashingï¼Œæˆ–è€…å¯¹ word embedding è¿›è¡Œèšç±»ï¼Œæ€»ä¹‹æ˜¯æŠŠè¾“å…¥ I(x) æ”¾åˆ°ä¸€ä¸ªæˆ–å¤šä¸ª bucket é‡Œé¢ï¼Œç„¶ååªå¯¹ç›¸åŒ bucket é‡Œçš„ memory è®¡ç®—åˆ†æ•° æŸå¤±å‡½æ•°æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼Œé€‰å®š 2 æ¡ supporting fact (k=2)ï¼Œresponse æ˜¯å•è¯çš„æƒ…å†µï¼š å¤šç±»æ”¯æŒå‘é‡æœºæŸå¤±: minimize: $L_i = \\sum_{j\\ne y_i}max(0,s_j - s_{y_i}+\\Delta)$ å…¶ä¸­ $\\overline f, \\overline fâ€™,\\overline r$ è¡¨ç¤ºè´Ÿé‡‡æ ·ã€‚æ¯”å¦‚ï¼ˆ8ï¼‰å¼ä¸­rè¡¨ç¤º true response, è€Œ $\\overline r$ è¡¨ç¤ºéšæœºæŠ½æ ·è¯å…¸ä¸­çš„å…¶ä»–è¯ã€‚ QAå®ä¾‹ï¼š (6) æœ‰æ²¡æœ‰æŒ‘é€‰å‡ºæ­£ç¡®çš„ç¬¬ä¸€å¥è¯ (7) æ­£ç¡®æŒ‘é€‰å‡ºäº†ç¬¬ä¸€å¥è¯åèƒ½ä¸èƒ½æ­£ç¡®æŒ‘å‡ºç¬¬äºŒå¥è¯ (6)+(7) åˆèµ·æ¥å°±æ˜¯èƒ½ä¸èƒ½æŒ‘é€‰å‡ºæ­£ç¡®çš„è¯­å¢ƒï¼Œç”¨æ¥è®­ç»ƒ attention å‚æ•° (8) æŠŠæ­£ç¡®çš„ supporting fact ä½œä¸ºè¾“å…¥ï¼Œèƒ½ä¸èƒ½æŒ‘é€‰å‡ºæ­£ç¡®çš„ç­”æ¡ˆï¼Œæ¥è®­ç»ƒ response å‚æ•° Paper reading 2 End-To-End Memory Networksmotivationä¸Šä¸€ç¯‡paperä¸­çš„ç¼ºé™·ï¼š The model in that work was not easy to train via backpropagation, and required supervision at each layer of the network. è¿™ç¯‡è®ºæ–‡å¯ä»¥çœ‹ä½œæ˜¯ä¸Šä¸€ç¯‡è®ºæ–‡memory networksçš„æ”¹è¿›ç‰ˆã€‚ Our model can also be seen as a version of RNNsearch with multiple computational steps (which we term â€œhopsâ€) per output symbol. ä¹Ÿå¯ä»¥çœ‹åšæ˜¯å°†multiple hopsåº”ç”¨åˆ°RNNsearchè¿™ç¯‡è®ºæ–‡ä¸Š Neural Machine Translation by Jointly Learning to Align and Translateã€‚ Model architectureSingle layer è¾“å…¥ï¼š input: $x_1,â€¦,x_i$ query: q answer: a å¯¹äºå•å±‚ç½‘ç»œï¼Œä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š 1.å°†inputå’Œqueryæ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ memory vector {$m_i$}: ${x_i}\\stackrel A\\longrightarrow {m_i}$ internal state u: $q\\stackrel B \\longrightarrow u$ 2.è®¡ç®—attentionï¼Œä¹Ÿå°±æ˜¯queryçš„å‘é‡è¡¨ç¤ºuï¼Œå’Œinputä¸­å„ä¸ªsentenceçš„å‘é‡è¡¨ç¤º $m_i$ çš„åŒ¹é…åº¦ã€‚compute the match between u and each memory mi by taking the inner product followed by a softmax. $$p_i=softmax(u^Tm_i)$$ p is a probability vector over the inputs. 3.å¾—åˆ°context vector output vector: ${x_i}\\stackrel C\\longrightarrow {c_i}$ The response vector from the memory o is then a sum over the transformed inputs ci, weighted by the probability vector from the input: $$o = \\sum_ip_ic_i$$ å’Œ Memory Networks with Strong Supervision ç‰ˆæœ¬ä¸åŒï¼Œè¿™é‡Œçš„ output æ˜¯åŠ æƒå¹³å‡è€Œä¸æ˜¯ä¸€ä¸ª argmax 4.é¢„æµ‹æœ€åç­”æ¡ˆï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå•è¯ $$\\hat a =softmax(Wu^{k+1})= softmax(W(o^k+u^k))$$ Wå¯ä»¥çœ‹åšåå‘embeddingï¼ŒW.shape=[embed_size, V] 5.å¯¹ $\\hat a$ è¿›è¡Œè§£ç ï¼Œå¾—åˆ°è‡ªç„¶è¯­è¨€çš„response $$\\hat a \\stackrel C \\longrightarrow a$$ å…¶ä¸­ï¼š A: intput embedding matrix C: output embedding matrix W: answer prediction matrix B: question embedding matrix å•å±‚ç½‘ç»œå®ä¾‹ï¼š è¿™é‡Œçš„ memory {$m_i$} ç›´æ¥ç”¨äºè¾“å‡ºå‘é‡ $c_i$. å…¶å®æˆ‘ä¹Ÿç–‘æƒ‘ï¼Œä¸ºå•¥è¦é‡æ–°ç”¨ä¸€ä¸ªoutput embedding Cï¼Œç›´æ¥ç”¨ $m_i$ ä¸å¥½å—ã€‚å…¶å®è¿™äº›å°tricksä¹Ÿè¯´ä¸å‡†å¥½ä¸å¥½ï¼Œéƒ½æ˜¯è¯•å‡ºæ¥çš„å§ï¼Œå› ä¸ºæ€ä¹ˆè¯´éƒ½åˆç†ã€‚ã€‚ã€‚ Multiple Layers/ Multiple hopså¤šå±‚ç»“æ„ï¼ˆK hopsï¼‰ä¹Ÿå¾ˆç®€å•ï¼Œç›¸å½“äºåšå¤šæ¬¡ addressing/å¤šæ¬¡ attentionï¼Œæ¯æ¬¡ focus åœ¨ä¸åŒçš„ memory ä¸Šï¼Œä¸è¿‡åœ¨ç¬¬ k+1 æ¬¡ attention æ—¶ query çš„è¡¨ç¤ºéœ€è¦æŠŠä¹‹å‰çš„ context vector å’Œ query æ‹¼èµ·æ¥ï¼Œå…¶ä»–è¿‡ç¨‹å‡ ä¹ä¸å˜ã€‚ $$u_{k+1}=u^k+o^k$$ å¯¹æ¯”ä¸Šä¸€ç¯‡paperæ¥ç†è§£å¤šå±‚ç½‘ç»œä¹Ÿå¯ä»¥çœ‹åšæ˜¯å››ä¸ªç»„ä»¶æ„æˆçš„ï¼š input components: å°±æ˜¯å°†queryå’Œsentencesæ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ä¸­ generalization componentsï¼š æ›´æ–°memoryï¼Œè¿™é‡Œçš„memoryä¹Ÿæ˜¯åœ¨å˜åŒ–çš„ï¼Œ${m_i}=AX$ï¼Œ ä½†æ˜¯embedding matrix A æ˜¯é€å±‚å˜åŒ–çš„ output components: attentionå°±æ˜¯æ ¹æ®inner productåsoftmaxè®¡ç®—memoryå’Œqueryä¹‹é—´çš„åŒ¹é…åº¦ï¼Œç„¶åæ›´æ–°inputï¼Œä¹Ÿå°±æ˜¯[u_k,o_k]ï¼Œ å¯ä»¥æ˜¯ç›¸åŠ /æ‹¼æ¥ï¼Œæˆ–è€…ç”¨RNN. åŒºåˆ«æ˜¯ï¼Œåœ¨ä¸Šä¸€ç¯‡è®ºæ–‡ä¸­æ˜¯argmaxï¼Œ$o_2=O_2(q,m)=argmax_{i=1,2,..,N}s_O([q,o_1],m_i)$, ä¹Ÿå°±æ˜¯é€‰å‡ºåŒ¹é…ç¨‹åº¦æœ€å¤§çš„ memory $m_i$, è€Œè¿™ç¯‡è®ºæ–‡æ˜¯å¯¹æ‰€æœ‰çš„memoryè¿›è¡ŒåŠ æƒæ±‚å’Œ response components: è·Ÿoutput componentsç±»ä¼¼å•Šï¼Œä¸Šä¸€ç¯‡è®ºæ–‡æ˜¯ä¸è¯å…¸ä¸­æ‰€æœ‰çš„è¯è¿›è¡ŒåŒ¹é…ï¼Œæ±‚å‡ºç›¸ä¼¼åº¦æœ€å¤§çš„ $r=argmax_{w\\in W}s_R([q,m_{o_1},m_{o_2}],w)$ï¼Œè€Œè¿™ç¯‡è®ºæ–‡æ˜¯ $\\hat a=softmax(Wu^{k+1})=softmax(W(u^k+o^k))$ æœ€å°åŒ–äº¤å‰ç†µæŸå¤±å‡½æ•°è®­ç»ƒå¾—åˆ° answer prediction matrix W. Overall, it is similar to the Memory Network model in [23], except that the hard max operations within each layer have been replaced with a continuous weighting from the softmax. ä¸€äº›æŠ€æœ¯ç»†èŠ‚æ¯ä¸€å±‚éƒ½æœ‰ mebedding matrices $A^k, C^k$,ç”¨æ¥embed inputs {$x_i$},ä¸ºäº†å‡å°‘è®­ç»ƒå‚æ•°.ä½œè€…å°è¯•äº†ä»¥ä¸‹ä¸¤ç§æƒ…å†µï¼š Adjacent ä¸Šä¸€å±‚çš„output embedding matrix æ˜¯ä¸‹ä¸€å±‚çš„ input embedding matrix, å³ $A^{k+1}=C^k$ æœ€åä¸€å±‚çš„output embedding å¯ç”¨ä½œ prediction embedding matrixï¼Œ å³ $W^T=C^k$ question embedding matrix = input embedding matrix of the first layer, $B=A^1$ Layer-wise (RNN-like) $A^1=A^2=â€¦=A^k, C^1=C^2=â€¦C^k$ $u^{k+1} = Hu^k+o^k$ ExperimentsDatasetæ•°æ®é›†æ¥æºï¼šTowards AI-complete question answering: A set of prerequisite toy tasks æ€»å…±æœ‰ 20 QA tasksï¼Œå…¶ä¸­æ¯ä¸ªtaskæœ‰ $I(I\\le 320)$ ä¸ªsentence {$x_i$}, è¯å…¸å¤§å° V=170, å¯ä»¥çœ‹åšè¿™æ˜¯ä¸ªç©å…·çº§çš„ä»»åŠ¡ã€‚æ¯ä¸ªtaskæœ‰1000ä¸ªproblems Modle detailsSentence representationsä¹Ÿå°±æ˜¯å°†inputå’Œqueryæ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ï¼Œæœ‰ä¸¤ç§æ–¹å¼ï¼š 1.Bag of words(BOW) representation $$m_i=\\sum_jAx_{ij}$$ $$c_i=\\sum_jCx_{ij}$$ $$u=\\sum_jBq_j$$ åˆ†åˆ«å¯¹æ¯ä¸ªè¯embedï¼Œç„¶åsumï¼Œç¼ºç‚¹æ˜¯æ²¡æœ‰è€ƒè™‘è¯åº 2.encodes the position of words within the sentence è€ƒè™‘è¯åºçš„ç¼–ç  $$m_i=\\sum_jl_j\\cdot Ax_{ij}$$ iè¡¨ç¤ºç¬¬iä¸ªsentenceï¼Œjè¡¨ç¤ºè¿™ä¸ªsentenceä¸­çš„ç¬¬jä¸ªword $$l_{kj}=(1-j/J)-(k/d)(1-2j/J)$$ æŸ¥çœ‹æºç æ—¶å‘ç°å¾ˆå¤šä»£ç çš„position encoderä¸åŸpaperä¸ä¸€æ ·ï¼Œæ¯”å¦‚domluna/memn2nä¸­å…¬å¼æ˜¯ï¼š $$l_{kj} = 1+4(k- (d+1)/2)(j-(J+1)/2)/d/J$$ åŸæœ¬è¯ $x_{ij}$ çš„å‘é‡è¡¨ç¤ºå°±æ˜¯embededåçš„ $Ax_{ij},(shape=[1, embed_size])$, ä½†ç°åœ¨è¦ç»™è¿™ä¸ªå‘é‡åŠ ä¸€ä¸ªæƒé‡ $l_j$,è€Œä¸”è¿™ä¸ªæƒé‡ä¸æ˜¯ä¸€ä¸ªå€¼ï¼Œè€Œæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå¯¹ $Ax_{ij}$ ä¸­æ¯ä¸€ä¸ªç»´åº¦çš„æƒé‡ä¹Ÿæ˜¯ä¸ä¸€æ ·çš„ã€‚ ä»¤J=20, d=50. å…·ä½“ä¸¤ä¸ªå…¬å¼çš„å·®åˆ«å¯ä»¥æŸ¥çœ‹ wolframalpha1 wolframalpha2 ä¹Ÿå°±æ˜¯è¯´ä¸ä»…è·Ÿwordåœ¨sentenceä¸­çš„ä½ç½®æœ‰å…³ï¼Œè¿˜å’Œembed_sizeä¸­çš„ç»´åº¦æœ‰å…³ã€‚è¿™å°±å¾ˆéš¾ç†è§£äº†ã€‚ã€‚ã€‚ å¥½åƒè·Ÿå¥å­çš„ç»“æ„ç›¸å…³ï¼ŒåŒ—å¤§æœ‰ç¯‡ç›¸å…³çš„è®ºæ–‡A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification å…¶ä¸­ J è¡¨ç¤ºsentenceçš„é•¿åº¦ï¼Œdè¡¨ç¤º dimension of the embedding. è¿™ç§sentence representationç§°ä¸º position encoding(PE).ä¹Ÿå°±æ˜¯è¯åºä¼šå½±å“memory $m_i$. position encoding ä»£ç å®ç° 12345678910111213141516171819202122232425262728293031323334353637def position_encoding(sentence_size, embedding_size): &quot;&quot;&quot; Position Encoding described in section 4.1 [1] &quot;&quot;&quot; encoding = np.ones((embedding_size, sentence_size), dtype=np.float32) le = embedding_size+1 ls = sentence_size + 1 for k in range(1, le): for j in range(1, ls): # here is different from the paper. # the formulation in paper is: l_{kj}=(1-j/J)-(k/d)(1-2j/J) # here the formulation is: l_{kj} = 1+4(k- (d+1)/2)(j-(J+1)/2)/d/J, # å…·ä½“è¡¨ç°å¯æŸ¥çœ‹ https://www.wolframalpha.com/input/?i=1+%2B+4+*+((y+-+(20+%2B+1)+%2F+2)+*+(x+-+(50+%2B+1)+%2F+2))+%2F+(20+*+50)+for+0+%3C+x+%3C+50+and+0+%3C+y+%3C+20 encoding[k-1, j-1] = (k - (embedding_size+1)/2) * (j - (sentence_size+1)/2) encoding = 1 + 4 * encoding / embedding_size / sentence_size # Make position encoding of time words identity to avoid modifying them encoding[:, -1] = 1.0 # æœ€åä¸€ä¸ªsentenceçš„æƒé‡éƒ½ä¸º1 return np.transpose(encoding) # [sentence_size, embedding_size] Temporal Encodingå°†memoryæ”¹è¿›ä¸ºï¼š $$m_i=\\sum_jAx_{ij}+T_A(i)$$ å…¶ä¸­ $T_A(i)$ is the ith row of a special matrix $T_A$ that encodes temporal information. ç”¨ä¸€ä¸ªç‰¹æ®Šçš„çŸ©é˜µ $T_A$ æ¥ç¼–ç æ—¶é—´ä¿¡æ¯ã€‚$T_A(i)$ iè¡¨ç¤ºç¬¬iä¸ªsentenceçš„åŒ…å«æ—¶é—´ä¿¡æ¯ï¼Ÿï¼Ÿ åŒæ ·çš„output embedding: $$c_i=\\sum_jCx_{ij}+T_C(i)$$ Learning time invariance by injecting random noisewe have found it helpful to add â€œdummyâ€ memories to regularize TA. Training Details1.learning rate decay 2.gradient clip 3.linear start training 4.null padding, zero padding å®Œæ•´ä»£ç å®ç°https://github.com/PanXiebit/text-classification/blob/master/06-memory%20networks/memn2n_model.py Paper reading 3 Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingMotivationMost tasks in natural language processing can be cast into question answering (QA) problems over language input. å¤§éƒ¨åˆ†çš„è‡ªç„¶è¯­è¨€å¤„ç†çš„ä»»åŠ¡éƒ½å¯ä»¥çœ‹ä½œæ˜¯QAé—®é¢˜ï¼Œæ¯”å¦‚QA, sentiment analysis, part-of-speech tagging. Model Architecture å¯ä»¥åˆ†ä¸ºä»¥ä¸‹4ä¸ªæ¨¡å—ï¼š Input Module: å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºdistribution representations Question Module: å°†questionç¼–ç ä¸ºdistribution representations Episodic Memory Module: é€šè¿‡attentionæœºåˆ¶é€‰æ‹©focus onè¾“å…¥æ–‡æœ¬ä¸­çš„æŸäº›éƒ¨åˆ†ï¼Œç„¶åç”Ÿæˆmemory vector representation. Answer Module: ä¾æ®the final memory vectorç”Ÿæˆanswer Detailed visualization: Input Moduleä¸»è¦åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼š 1.è¾“å…¥æ˜¯single sentenceï¼Œé‚£ä¹ˆinput moduleè¾“å‡ºçš„å°±æ˜¯é€šè¿‡RNNè®¡ç®—å¾—åˆ°çš„éšè—çŠ¶æ€ $T_C= T_I$, $T_I$ è¡¨ç¤ºä¸€ä¸ªsentenceä¸­çš„è¯çš„ä¸ªæ•°ã€‚ 2.è¾“å…¥æ˜¯a list of sentencesï¼Œåœ¨æ¯ä¸ªå¥å­åæ’å…¥ä¸€ä¸ªç»“æŸç¬¦å· end-of-sentence token, ç„¶åæ¯ä¸ªsentenceçš„final hiddenä½œä¸ºè¿™ä¸ªsentenceçš„representation. é‚£ä¹ˆinput moduleè¾“å‡º $T_C$, $T_C$ç­‰äºsequenceçš„sentenceä¸ªæ•°ã€‚ ç„¶åRNNä½¿ç”¨çš„æ˜¯GRUï¼Œä½œè€…ä¹Ÿå°è¯•è¿‡LSTMï¼Œå‘ç°æ•ˆæœå·®ä¸å¤šï¼Œä½†LSTMè®¡ç®—é‡æ›´å¤§ã€‚ Question ModuleåŒæ ·çš„ä½¿ç”¨GRUç¼–ç ï¼Œåœ¨tæ—¶é—´æ­¥ï¼Œ éšè—çŠ¶æ€ $$q_t=GRU(L[w_t^Q],q_{t-1})$$ Lä»£è¡¨embedding matrix. æœ€åè¾“å‡º final hidden state. $$q=q_{T_Q}$$ $T_Q$ æ˜¯questionçš„è¯çš„ä¸ªæ•°ã€‚ Episodic Memory Moduleç”± internal memory, attention mechansim, memory update mechanism ç»„æˆã€‚ è¾“å…¥æ˜¯ input module å’Œ question module çš„è¾“å‡ºã€‚ æŠŠ input module ä¸­æ¯ä¸ªå¥å­çš„è¡¨è¾¾ï¼ˆfact representation cï¼‰æ”¾åˆ° episodic memory module é‡Œåšæ¨ç†ï¼Œä½¿ç”¨ attention åŸç†ä» input module ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼ŒåŒæ ·æœ‰ multi-hop architectureã€‚ 1.Needs for multiple Episodes: é€šè¿‡è¿­ä»£ä½¿å¾—æ¨¡å‹å…·æœ‰äº†ä¼ é€’æ¨ç†èƒ½åŠ› transitive inference. 2.Attention Mechanism: ä½¿ç”¨äº†ä¸€ä¸ªgating functionä½œä¸ºattentionæœºåˆ¶ã€‚ç›¸æ¯”åœ¨ end-to-end MemNN ä¸­attentionä½¿ç”¨çš„æ˜¯linear regressionï¼Œå³å¯¹inner productioné€šè¿‡softmaxæ±‚æƒé‡ã€‚ è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¸¤å±‚å‰å‘ç¥ç»ç½‘ç»œ G å‡½æ•°. $$g_t^i=G(c_t,m^{i-1},q)$$ $c_t$ æ˜¯candidate fact, $m_{i-1}$ æ˜¯previous memoryï¼Œ question q. t è¡¨ç¤ºsentenceä¸­çš„ç¬¬tæ—¶é—´æ­¥ï¼Œiè¡¨ç¤ºepisodicçš„è¿­ä»£æ¬¡æ•°ã€‚ è¿™é‡Œä½œè€…å®šä¹‰äº† a large feture $z(c,m,q)$ æ¥è¡¨å¾input, memory, questionä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ $$z_t^i=[c_t, m^{i-1},q, c_t\\circ q,c_t\\circ m^{i-1},|c_t-q|,|c_t-m^{i-1}|, c_t^TW^{(b)}q, c_t^TW^{(b)}m^{i-1}]$$ æ€»çš„æ¥è¯´ï¼Œå°±æ˜¯æ ¹æ®å‘é‡å†…ç§¯ï¼Œå‘é‡ç›¸å‡æ¥è¡¨ç¤ºç›¸ä¼¼åº¦ã€‚ è·Ÿcs224d-lecture16 dynamic Memory networkRichard Socheræœ¬äººè®²çš„æœ‰ç‚¹åŒºåˆ«ï¼Œä¸è¿‡è¿™ä¸ªæ—¢ç„¶æ˜¯äººå·¥å®šä¹‰çš„ï¼Œå¥½åƒæ€ä¹ˆè¯´éƒ½å¯ä»¥ã€‚ ç„¶åé€šè¿‡Gå‡½æ•°ï¼Œä¹Ÿå°±æ˜¯ä¸¤å±‚å‰å‘ç¥ç»ç½‘ç»œå¾—åˆ°ä¸€ä¸ªscale score. $$G = \\sigma(W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)})$$ å°† $c_t$, $m^{i-1}, q å¸¦å…¥åˆ°Gå‡½æ•°ï¼Œå³å¯æ±‚å¾—$$g_i^t$ï¼Œä¹Ÿå°±æ˜¯candidate fact $c_i$ çš„score. è®¡ç®—å®Œæ¯ä¸€æ¬¡è¿­ä»£åçš„åˆ†æ•°åï¼Œæ¥æ›´æ–°episode $e^i$, ç›¸å½“äº context vector, soft attention åœ¨ä¹‹å‰çš„attentionæœºåˆ¶ä¸­ï¼Œæ¯”å¦‚cs224d-lecture10-æœºå™¨ç¿»è¯‘å’Œæ³¨æ„åŠ›æœºåˆ¶ä»‹ç»çš„attentionå¾—åˆ°çš„context vectorï¼Œåœ¨end-to-end MemNNä¸­attentionä¹Ÿæ˜¯fact representationçš„åŠ æƒæ±‚å’Œã€‚ attention based GRU ä½†è¿™ç¯‡è®ºæ–‡ä¸­åº”ç”¨äº†GRUï¼Œå¯¹fact representation c è¿›è¡Œå¤„ç†ï¼Œç„¶ååŠ ä¸Šgate $$h_t^i=g_t^iGRU(c_t,h_{t-1}^i)+(1-g_t^i)h_{i-1}^t$$ æ‰€ä»¥è¿™é‡Œçš„GRUåº”è¯¥æ˜¯ $T_C$æ­¥å§ï¼Ÿï¼Ÿ æ¯æ¬¡è¿­ä»£çš„context vectoræ˜¯å¯¹ input module çš„è¾“å‡ºè¿›è¡Œ attention-based GRUç¼–ç çš„æœ€åçš„éšè—çŠ¶æ€: $$e^i=h_{T_C}^i$$ æ€»ç»“ä¸€ä¸‹ï¼š è¿™éƒ¨åˆ†attention mechanismç›®çš„å°±æ˜¯ç”Ÿæˆepisode $e^i$,$e^i$ æ˜¯ç¬¬iè½®è¿­ä»£çš„æ‰€æœ‰inputç›¸å…³ä¿¡æ¯çš„summary.ä¹Ÿå°±æ˜¯ context vector,å°†input textå‹ç¼©åˆ°ä¸€ä¸ªå‘é‡è¡¨ç¤ºä¸­ï¼Œend-to-end MemNNç”¨äº†soft attentionï¼Œå°±æ˜¯åŠ æƒæ±‚å’Œã€‚è€Œè¿™é‡Œç”¨äº†GRUï¼Œå„ä¸ªæ—¶é—´æ­¥çš„æƒé‡ä¸æ˜¯ç›´æ¥ç›¸ä¹˜ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªgateæœºåˆ¶ã€‚ 3.Memory Update Mechanism ä¸Šä¸€æ­¥è®¡ç®—çš„episode $e^i$ ä»¥åŠä¸Šä¸€è½®è¿­ä»£çš„memory $m^{i-1}$ ä½œä¸ºè¾“å…¥æ¥æ›´æ–°memory $m_i$ $$m_i=GRU(e^i,m^{i-1})$$ $m^0=q$, æ‰€ä»¥è¿™é‡Œçš„GRUæ˜¯å•æ­¥çš„å§ ç»è¿‡ $T_M$ æ¬¡è¿­ä»£ï¼š $m=m^{T_M}$, ä¹Ÿå°±æ˜¯episodic memory moduleçš„è¾“å‡ºï¼Œå³answer moduleçš„è¾“å…¥ã€‚ åœ¨end-to-end MemNNçš„memory updateä¸­ï¼Œ$u_{k+1}=u^k+o^k$, è€Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­,å¦‚æœä¹Ÿé‡‡ç”¨è¿™ç§å½¢å¼çš„è¯å°±æ˜¯ $m^{i}=e^i+m^{i-1}$ï¼Œä½†ä½œè€…é‡‡ç”¨äº† RNN åšéçº¿æ€§æ˜ å°„ï¼Œç”¨ episode $e_i$ å’Œä¸Šä¸€ä¸ª memory $m_{iâˆ’1}$ æ¥æ›´æ–° episodic memoryï¼Œå…¶ GRU çš„åˆå§‹çŠ¶æ€åŒ…å«äº† question ä¿¡æ¯ï¼Œ$m_0=q$ã€‚ 4.Criteria for stopping Episodic Memory Module éœ€è¦ä¸€ä¸ªåœæ­¢è¿­ä»£çš„ä¿¡å·ã€‚ä¸€èˆ¬å¯ä»¥åœ¨è¾“å…¥ä¸­åŠ å…¥ä¸€ä¸ªç‰¹æ®Šçš„ end-of-passes çš„ä¿¡å·ï¼Œå¦‚æœ gate é€‰ä¸­äº†è¯¥ç‰¹æ®Šä¿¡å·ï¼Œå°±åœæ­¢è¿­ä»£ã€‚å¯¹äºæ²¡æœ‰æ˜¾æ€§ç›‘ç£çš„æ•°æ®é›†ï¼Œå¯ä»¥è®¾ä¸€ä¸ªè¿­ä»£çš„æœ€å¤§å€¼ã€‚ Answer Moduleä½¿ç”¨äº†GRUçš„decoderã€‚è¾“å…¥æ˜¯question moduleçš„è¾“å‡ºqå’Œä¸Šä¸€ä¸ªæ—¶åˆ»çš„hidden state $a_{t-1}$,åˆå§‹çŠ¶æ€æ˜¯episodic memory moduleçš„è¾“å‡º $a_0=m^{T_M}$. $$y_t=softmax(W^{(a)}a_t)$$ $$a_t=GRU([y_{t-1},q],a_{t-1})$$ è¿™é‡Œåº”è¯¥å°±æ˜¯å•æ­¥GRUå§ï¼Œæ¯•ç«Ÿquestionçš„å‘é‡è¡¨ç¤ºqåªæœ‰ä¸€ä¸ªå‘€ã€‚ Trainä½¿ç”¨ cross-entroy ä½œä¸ºç›®æ ‡å‡½æ•°ã€‚å¦‚æœ æ•°æ®é›†æœ‰ gate çš„ç›‘ç£æ•°æ®ï¼Œè¿˜å¯ä»¥å°† gate çš„ cross-entroy åŠ åˆ°æ€»çš„ costä¸Šå»ï¼Œä¸€èµ·è®­ç»ƒã€‚è®­ç»ƒç›´æ¥ä½¿ç”¨ backpropagation å’Œ gradient descent å°±å¯ä»¥ã€‚ æ€»ç»“:å¯¹æ¯”ä¸Šä¸€ç¯‡è®ºæ–‡End-to-end memory networks input components: end2end MemNN é‡‡ç”¨embeddingï¼Œè€ŒDMNä½¿ç”¨GRU generalization components: ä¹Ÿå°±æ˜¯memory updateï¼ŒEnd2End MemNNé‡‡ç”¨çº¿æ€§ç›¸åŠ  $u^{k+1}=u^k+o^k$,å…¶ä¸­çš„ $o^k$ å°±æ˜¯ç»è¿‡attentionä¹‹åå¾—åˆ°çš„memory vector output components: end2end MemNNé‡‡ç”¨çš„æ˜¯å¯¹æ¯”memoryå’Œquery,ç”¨å†…ç§¯æ±‚ç›¸ä¼¼åº¦ï¼Œç„¶åsoftmaxæ±‚æƒé‡ï¼Œæœ€åä½¿ç”¨åŠ æƒæ±‚å’Œå¾—åˆ°context vector. è€ŒDMNé‡‡ç”¨çš„æ˜¯äººå·¥å®šä¹‰ç›¸ä¼¼åº¦çš„è¡¨ç¤ºå½¢å¼ï¼Œç„¶åç”¨ä¸¤å±‚å‰å‘ç¥ç»ç½‘ç»œè®¡ç®—å¾—åˆ°scoreï¼Œå†å¯¹scoreç”¨softmaxæ±‚æƒé‡ï¼Œå†ç„¶åæŠŠæƒé‡å½“åšgateæœºåˆ¶ï¼Œä½¿ç”¨GRUæ±‚context vector response components: end2end MemNN ç›´æ¥ä½¿ç”¨æœ€åçš„ top memory layer é¢„æµ‹ï¼Œè€ŒDMNæ˜¯æŠŠtop memory å½“åšinit hidden state æ€»ä¹‹ï¼ŒDMNå®åœ¨æ˜¯å¤ªå¤ªå¤ªå¤æ‚äº†ã€‚ã€‚æ¯ä¸€ä¸ªmoduleéƒ½ç”¨åˆ°äº†RNN Paper reading 4 DMN+paper:Dynamic Memory Networks for Visual and Textual Question Answering (2016) Motivateæå‡ºäº†DMN+ï¼Œæ˜¯DMNçš„æ”¹è¿›ç‰ˆï¼ŒåŒæ—¶å°†å…¶åº”ç”¨åˆ° Visual Question Answering è¿™ä¸€ä»»åŠ¡ä¸Šã€‚ However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. è¿™æ®µè¯æ˜¯æè¿°DMNçš„ç¼ºç‚¹çš„ï¼Œåœ¨æ²¡æœ‰æ ‡æ³¨ supporting factsçš„æƒ…å†µä¸‹è¡¨ç°ä¸å¥½ã€‚ä½†æ˜¯DMNè²Œä¼¼ä¹Ÿå¹¶ä¸éœ€è¦æ ‡æ³¨ supporting factså•Šã€‚ã€‚ã€‚ Like the original DMN, this memory network requires that supporting facts are labeled during QA training. End-toend memory networks (Sukhbaatar et al., 2015) do not have this limitation. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. è¿™ç¯‡æ–‡ç« å¯¹DMNä¸­çš„ input moduleè¿›è¡Œäº†ä¿®æ”¹ï¼Œå¹¶ä¸”æå‡ºäº†æ–°çš„æ¨¡å‹æ¶æ„é€‚ç”¨äºå›¾åƒçš„ã€‚ DMN å­˜åœ¨çš„ä¸¤ä¸ªé—®é¢˜ï¼š è¾“å…¥æ¨¡å—åªè€ƒè™‘äº†è¿‡å»ä¿¡æ¯ï¼Œæ²¡è€ƒè™‘åˆ°å°†æ¥ä¿¡æ¯ åªç”¨ word level çš„ GRUï¼Œå¾ˆéš¾è®°å¿†è¿œè·ç¦» supporting sentences ä¹‹é—´çš„ä¿¡æ¯ã€‚ æ€»çš„æ¥è¯´è¿™ç¯‡æ–‡ç« è´¡çŒ®ä¸»è¦è¿˜æ˜¯åœ¨åº”ç”¨åˆ°å›¾åƒä¸Šäº†ï¼Œè‡³äºä½œè€…æ‰€è¯´çš„ input moduleçš„æ”¹è¿›ï¼Œåªæ˜¯ä¸ºäº†å‡å°‘è®¡ç®—é‡ï¼Œè€Œä¸”æ”¹è¿›ç‰ˆä¸­çš„ bi-RNN å’Œ position encoding éƒ½æ˜¯åœ¨åˆ«äººçš„è®ºæ–‡ä¸­å‡ºç°äº†çš„ã€‚ Model ArchitectureåŒDMNä¸€æ ·ï¼Œä¹Ÿåˆ†ä¸º input module, question module, episodic module å’Œ answer module. Input Moduleinput module for text QA ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªç»„ä»¶ï¼š sentence reader å’Œ input fusion layer. sentence reader: ç”¨encoding positionä»£æ›¿RNNå¯¹å•ä¸ªsentenceè¿›è¡Œç¼–ç ã€‚ç”¨ positional encoding çš„åŸå› æ˜¯åœ¨è¿™é‡Œç”¨ GRU/LSTM ç¼–ç å¥å­è®¡ç®—é‡å¤§è€Œä¸”å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆæ¯•ç«Ÿ bAbI çš„å•è¯é‡å¾ˆå°å°±å‡ åä¸ªå•è¯ã€‚ã€‚ï¼‰ï¼Œè¿™ç§æ–¹æ³•åè€Œæ›´å¥½ã€‚ input fusion layer: ä½¿ç”¨ bi-directional GRU æ¥å¾—åˆ°context ä¿¡æ¯ï¼Œå…¼é¡¾è¿‡å»å’Œæœªæ¥çš„ä¿¡æ¯ã€‚ æ€»çš„æ¥è¯´ï¼š DMN+ æŠŠ single GRU æ›¿æ¢æˆäº†ç±»ä¼¼ hierarchical RNN ç»“æ„ï¼Œä¸€ä¸ª sentence reader å¾—åˆ°æ¯ä¸ªå¥å­çš„ embeddingï¼Œä¸€ä¸ª input infusion layer æŠŠæ¯ä¸ªå¥å­çš„ embedding æ”¾å…¥å¦ä¸€ä¸ª GRU ä¸­ï¼Œå¾—åˆ° context ä¿¡æ¯ï¼Œæ¥è§£å†³å¥å­è¿œè·ç¦»ä¾èµ–çš„é—®é¢˜ã€‚ input module for VQA 1.Local region feature extraction: è·å–å±€éƒ¨ç‰¹å¾ä¿¡æ¯ï¼Œä½¿ç”¨VGGé¢„è®­ç»ƒå¾—åˆ°çš„ç‰¹å¾ã€‚å±€éƒ¨ç‰¹å¾ feature vector é€šè¿‡ä¸€ä¸ªlinear layer å’Œ tanh activation å¾—åˆ° feature embedding. 2.Input fusion layer: å°† feature embedding æ”¾å…¥åˆ° bi-GRU ä¸­ã€‚ Without global information, their representational power is quite limited, with simple issues like object scaling or locational variance causing accuracy problems. å¼ºè°ƒäº†ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ input fusion layer. Question Moduleè¿™éƒ¨åˆ†è·ŸDMNæ˜¯ä¸€æ ·çš„, question éƒ½æ˜¯æ–‡æœ¬ï¼Œç”¨RNNç¼–ç ã€‚ Episodic Modulescore mechanisminput module çš„è¾“å‡ºæ˜¯: $$\\overleftrightarrow F=[\\overleftrightarrow f_1, â€¦,\\overleftrightarrow f_N]$$ åŒDMNä¸€æ ·ï¼Œä½œè€…ä¹Ÿæ˜¯ç”¨äº†äººå·¥ç‰¹å¾ï¼Œç›¸æ¯”DMNç®€åŒ–ä¸€ç‚¹ï¼š $$z_i^t=[\\overleftrightarrow f_i\\circ q,\\overleftrightarrow f_i\\circ m^{t-1},|\\overleftrightarrow f_i-q|,|\\overleftrightarrow f_i-m^{i-1}|]$$ è¿™é‡Œä¸å‰é¢DMNçš„å…¬å¼æœ‰ç‚¹åŒºåˆ«ï¼Œå°±æ˜¯è¿™é‡Œçš„iè¡¨ç¤ºinput moduleä¸­çš„æ—¶é—´æ­¥ï¼Œ t è¡¨ç¤ºepisodicè¿­ä»£æ¬¡æ•°ã€‚ åŒæ ·ä½¿ç”¨ä¸€ä¸ªä¸¤å±‚å‰å‘ç¥ç»ç½‘ç»œï¼š $$G = W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)}$$ ä½†æ˜¯è¿™é‡Œä¸æ˜¯ä½¿ç”¨ sigmoid å‡½æ•°æ¥æ±‚çš„ scoreï¼Œè€Œæ˜¯ä½¿ç”¨softmax æ¥æ±‚score $g_i^t$. $$g_i^t=\\dfrac{Z_i^t}{\\sum_{k=1}^{M_i}exp(Z_k^t)}$$ attention mechanismæ¯”è¾ƒäº† soft attention å’Œ attention-based-GRU.ç›¸æ¯”DMNé‚£ç¯‡è®ºæ–‡ï¼Œè¿™é‡Œç»™å‡ºäº†è¯¦ç»†çš„æ¯”è¾ƒã€‚ soft attention, å°±æ˜¯ç®€å•çš„åŠ æƒæ±‚å’Œã€‚$$c^t=\\sum_{i=1}^Ng_i^t\\overleftrightarrow f_i$$ å…¶ç¼ºç‚¹åœ¨äºä¸¢å¤±äº†ä½ç½®ä¿¡æ¯å’Œè¯åºä¿¡æ¯ã€‚ æ„Ÿè§‰ç®€å•çš„attentionå·²ç»å¾ˆå¥½äº†å§ã€‚ã€‚å‰é¢ $\\overleftrightarrow f_i$ ä¸å°±æ˜¯è€ƒè™‘äº†è¯åºä¿¡æ¯çš„ä¹ˆï¼Œç„¶åå†ç”¨GRUå¯¹ $\\overleftrightarrow f_i$ å¤„ç†ä¸ä¼šè¿‡æ‹Ÿåˆå—ï¼Ÿï¼Ÿï¼Ÿ attention based GRUä½¿ç”¨attention gate $g_i^t$ ä»£æ›¿ update gate $u_i$. æˆ‘ä»¬çŸ¥é“ $u_i$ æ˜¯é€šè¿‡ current input å’Œ previous hidden stateå¾—åˆ°çš„ã€‚ è€Œä½¿ç”¨ attention gate $g_i^t$ èƒ½å¤Ÿè€ƒè™‘åˆ° question å’Œ previous memory. å› ä¸ºæˆ‘ä»¬è¿™é‡Œæ˜¯è¦æ›´æ–°memoryï¼Œ æ‰€ä»¥è¿™æ ·å¾ˆåˆç†å‘€ã€‚ã€‚å‰å®³äº† $$h_i=g_i^t\\circ \\tilde h_i+(1-g_i^t)\\circ h_{i-1}$$ memory update mechanismåœ¨DMNä¸­ï¼Œæ›´æ–°memoryåœ¨åŸºäº previous memory å’Œ å½“å‰çš„ context vector çš„GRUç¼–ç å¾—åˆ°çš„ã€‚ DMN+é‡‡ç”¨çš„æ˜¯å°† previous memory $m^{t-1}$, å½“å‰ context $c^t$ï¼Œå’Œquestion q æ‹¼æ¥èµ·æ¥ï¼Œç„¶åé€šè¿‡å…¨è¿æ¥å±‚ï¼Œä»¥åŠreluæ¿€æ´»å‡½æ•°å¾—åˆ°çš„ï¼š $$m_t = ReLU(W^t[m^{t-1},c^t,q]+b)$$ ä½¿ç”¨reluçš„å…¨è¿æ¥å±‚èƒ½æå‡0.5%çš„å‡†ç¡®ç‡ã€‚ Answer ModuleåŒDMN. reference: å¾é˜¿è¡¡-è®ºæ–‡ç¬”è®° - Memory Networks","link":"/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/"},{"title":"è®ºæ–‡ç¬”è®°-dynamic convolution and involution","text":"paper list: CARAFE: Content-Aware ReAssembly of FEatures Involution: Inverting the Inherence of Convolution for Visual Recognition Pay less attention with lightweight and dynamic convolutions ConvBERT: Improving BERT with Span-based Dynamic Convolution Dynamic Region-Aware Convolution Involutionä¸ºä»€ä¹ˆè¦å«åå·ç§¯å‘¢ï¼Ÿå·ç§¯çš„ç‰¹æ€§æ˜¯ï¼š space-agnosticï¼šç©ºé—´ä¸å˜æ€§ï¼Œä¹Ÿå°±æ˜¯ç”¨ä¸€ä¸ªkernelåœ¨feature mapä¸Šæ»‘åŠ¨ã€‚è¿™æ ·å­¦çš„åˆ°featureæ˜¯å•ä¸€çš„ä¸ºäº†å¢åŠ featureçš„ä¸°å¯Œæ€§ï¼Œé‡‡ç”¨å¾ˆå¤§çš„channel channel-specific: é€šé“ç‰¹å¼‚æ€§ã€‚å°½ç®¡channelå¢åŠ èƒ½å­¦åˆ°æ›´å¤šç‰¹å¾ï¼Œä½†æ˜¯é€šé“å¤ªå¤§å…¶å®æ˜¯æœ‰å†—ä½™çš„ï¼Œæœ‰äººåšä½ç§©å®éªŒï¼Œå‘ç°å¾ˆå¤šchannelå¯¹åº”çš„å‚æ•°æ˜¯çº¿æ€§ç›¸å…³çš„ äºæ˜¯ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªä¸å·ç§¯å®Œå…¨ç›¸åçš„ç®—å­ï¼Œåå·ç§¯ï¼š space-specific: æ ¹æ®contentç”Ÿæˆç›¸åº”çš„weights channel-agnostic: å…±äº«å‚æ•°ã€‚ç±»ä¼¼attentionçš„projectionå’Œfeedforward. 12345678910111213141516def forward(self, x): # 1. ç”Ÿæˆpixel-wiseå¯¹åº”çš„æƒé‡ï¼Œæ¯ä¸ªpixelå¯¹åº”çš„æƒé‡æ˜¯ [kernel_size^2*group]. # print(&quot;after avgpool: &quot;, self.avgpool(x).shape) # print(&quot;after conv1: &quot;, self.conv1(x if self.stride == 1 else self.avgpool(x)).shape) weight = self.conv2(self.conv1(x if self.stride == 1 else self.avgpool(x))) # print(&quot;weight: &quot;, weight.shape) b, c, h, w = weight.shape weight = weight.view(b, self.groups, self.kernel_size**2, h, w).unsqueeze(2) print(&quot;weight: &quot;, weight.shape) # 2. å°†xé€šè¿‡unfoldä»¥kernel_sizeä¸ºå¤§å°ï¼Œstrideä¸ºæ­¥é•¿iå¥³æ€§å±•å¼€ print(&quot;after unfold: &quot;, self.unfold(x).shape) # [bs, channel*kernel*2, ((h-kernel+1+2*pad)/stride))^2] out = self.unfold(x).view(b, self.groups, self.group_channels, self.kernel_size**2, h, w) print(&quot;out: &quot;, out.shape) out = (weight * out).sum(dim=3).view(b, self.channels, h, w) return out æ€è·¯å¾ˆåƒlocal attention, åŒºåˆ«åœ¨äºè¿™ä¸ªweightæ˜¯é€šè¿‡content+linearå¾—åˆ°çš„ï¼Œè€Œä¸æ˜¯é€šè¿‡pixelä¹‹é—´çš„relationå¾—åˆ°çš„ã€‚è€Œä¸”ï¼Œçœ‹æºä»£ç è¿™ä¸ªweightså¹¶æ²¡æœ‰åšnormalization. ç„¶åæŠŠç”Ÿæˆçš„weightsä¸å¯¹åº”çš„pixelå‘¨å›´çš„[kernel,kernel]çš„pixelsè¿›è¡ŒåŠ æƒæ±‚å’Œã€‚","link":"/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/"},{"title":"è®ºæ–‡ç¬”è®°-Using monoligual data in machine transaltion","text":"Monolingual Data in NMT Why Monolingual data enhancement Large scale source-side data: enhancing encoder network to obtain high quality context vector representation of source sentence. Large scale target-side data: boosting fluency for machine translation when decoding. The methods of using monolingual data Multi-task learningTarget-side language model: Integrating Language Model into the Decoder shallow fusion both an NMT model (on parallel corpora) as well as a recurrent neural network language model (RNNLM, on larger monolingual corpora) have been pre-trained separately before being integrated. Shallow fusion: rescore the probability of the candidate words. deep fusion multi-task learning Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning, EMNLP, 2017 åˆ©ç”¨ target-side çš„å•è¯­å¤šäº†ä¸€ä¸ªè®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ã€‚äº‹å®ä¸Šï¼ˆbï¼‰å°±æ˜¯ä¸Šä¸€å¼  PPT ä¸­çš„æ–¹æ³•ï¼Œè¿™ç¯‡paperåœ¨è¿™ä¸ªåŸºç¡€ä¸Šå¢åŠ äº†è¯­è¨€æ¨¡å‹çš„ lossã€‚ $\\sigma$ å‚æ•°åœ¨ä¸¤ä¸ªä»»åŠ¡è®­ç»ƒæ—¶éƒ½ä¼šæ›´æ–°ã€‚è€Œ $\\theta$ å‚æ•°ä»…ä»…åœ¨è®­ç»ƒç¿»è¯‘æ¨¡å‹æ—¶æ‰ä¼šæ›´æ–°å‚æ•°ã€‚ auto-encoder é€šè¿‡ è‡ªç¼–ç  çš„å½¢å¼ï¼Œé‡æ„å¯¹åº”çš„ mono-dataï¼Œä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œä¸ NMT æ¨¡å‹å…±äº« encoder å‚æ•°ã€‚ Semi-Supervised Learning for Neural Machine Translation, ACL, 2016 Back-translationWhat is back-translation?Synthetic pseudo parallel data from target-side monolingual data using a reverse translation model. why back-translation and motivation?It mitigates the problem of overfitting and fluency by exploiting additional data in the target language. ç›®æ ‡è¯­è¨€å¿…é¡»å§‹ç»ˆæ˜¯çœŸå®å¥å­æ‰èƒ½è®©ç¿»è¯‘æ¨¡å‹ç¿»è¯‘çš„ç»“æœæ›´æµç•…ã€æ›´å‡†ç¡®ï¼Œè€Œæºè¯­è¨€å³ä¾¿æœ‰å°‘é‡ç”¨è¯ä¸å½“ã€è¯­åºä¸å¯¹ã€è¯­æ³•é”™è¯¯ï¼Œåªè¦ä¸å½±å“ç†è§£å°±æ— æ‰€è°“ã€‚å…¶å®äººåšç¿»è¯‘çš„æ—¶å€™ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼šç¿»è¯‘è´¨é‡å–å†³äºä¸€ä¸ªäººè¯‘å‡ºè¯­è¨€çš„æ°´å¹³ï¼Œè€Œä¸æ˜¯æºè¯­è¨€çš„æ°´å¹³ï¼ˆæºè¯­è¨€çš„æ°´å¹³åªè¦è¶³å¤Ÿçœ‹æ‡‚å¥å­å³å¯ï¼‰ Different aspects of the BT which influence the performance of translation: Size of the Synthetic Data Direction of Back-Translation Quality of the Synthetic Data Size of the Synthetic Data Direction of Back-Translation Quality of the Synthetic Data copy mechanism ä½œè€…çš„å®éªŒè®¾ç½®ï¼šç”¨ target-side mono-data æ¥æ„å»ºä¼ªå¹³è¡Œè¯­æ–™ï¼Œä¸€éƒ¨åˆ†æ˜¯ç›´æ¥ copyï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯é€šè¿‡ back-translate å¾—åˆ°çš„ã€‚ä¹Ÿå°±æ˜¯ mono-data å‡ºç°äº†ä¸¤æ¬¡ã€‚ æ€»è§‰å¾—å“ªé‡Œä¸å¯¹ã€‚ã€‚ã€‚ Dummy source sentencePseudo parallel data: + target-side mono-data The downside: the network â€˜unlearnsâ€™ its conditioning on the source context if the ratio of monolingual training instances is too high. Improving Neural Machine Translation Models with Monolingual Data, Sennrich et al, ACL 2016 Self-learningSynthetic target sentences from source-side mono-data: Build a baseline machine translation (MT) system on parallel data Translate source-side mono-data into target sentences Real parallel data + pseudo parallel data reference Improving Neural Machine Translation Models with Monolingual Data, Sennrich et al, ACL 2016 Using Monolingual Data in Neural Machine Translation: a Systematic Study, Burlot et al. ACL 2018 Copied Monolingual Data Improves Low-Resource Neural Machine Translation, Currey et al. 2017 In Proceedings of the Second Conference on Machine Translation Semi-Supervised Learning for Neural Machine Translation, Cheng et al. ACL 2016 Exploiting Source-side Monolingual Data in Neural Machine Translation, Zhang et al. EMNLP 2016 Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning, Domhan et al. EMNLP 2018 On Using Monolingual Corpora in Neural Machine Translation, Gulcehre, 2015 Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation, EMNLP 2018 Understanding Back-Translation at Scale, Edunov et al. EMNLP 2018","link":"/2019/03/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Using-monoligual-data-in-machine-transaltion/"},{"title":"è®ºæ–‡ç¬”è®°-QANet","text":"paper: combining local convolution with local self-attention for reading comprehension Motivation Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. encoder ç¼–ç æ–¹å¼ä»…ä»…ç”± å·ç§¯ å’Œ è‡ªæ³¨æ„åŠ› æœºåˆ¶æ„æˆï¼Œæ²¡äº† rnn é€Ÿåº¦å°±æ˜¯å¿«ã€‚ The key motivation behind the design of our model is the following: convolution captures the local structure of the text, while the self-attention learns the global interaction between each pair of words. è¿™ç¯‡è®ºæ–‡æœ€ä¸»è¦çš„åˆ›æ–°ç‚¹ï¼šä½¿ç”¨ CNN æ¥æ•æ‰æ–‡æœ¬ç»“æ„çš„å±€éƒ¨ä¿¡æ¯ï¼Œä½¿ç”¨ self-attention æ¥å­¦ä¹ å…¨å±€ä¸­æ¯ä¸¤ä¸ªè¯ä¹‹é—´çš„äº¤äº’ä¿¡æ¯ï¼Œä½¿å¾—å…¶èƒ½è€¦åˆä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç›¸æ¯” RNNï¼Œattention èƒ½å¤Ÿæœ‰æ•ˆçš„è§£å†³é•¿æœŸä¾èµ–é—®é¢˜ã€‚åªæ˜¯ç›¸æ¯”å°‘äº†è¯åºä¿¡æ¯ã€‚è¯´åˆ°åº•ï¼Œä¹Ÿæ˜¯ä¸€ç§ contextualize çš„ encoder æ–¹å¼ã€‚ we propose a complementary data augmentation technique to enhance the training data. This technique paraphrases the examples by translating the original sentences from English to another language and then back to English, which not only enhances the number of training instances but also diversifies the phrasing. ä½¿ç”¨äº†ä¸€ç§æ•°æ®å¢å¼ºçš„æ–¹å¼ï¼Œå…ˆå°†æºè¯­è¨€è½¬æ¢æˆå¦ä¸€ç§è¯­è¨€ï¼Œç„¶åå†ç¿»è¯‘å›è‹±è¯­ã€‚è¿™æ ·èƒ½æœ‰æ•ˆå¢åŠ è®­ç»ƒæ ·æœ¬ï¼ŒåŒæ—¶ä¹Ÿä¸°å¯Œäº†çŸ­è¯­çš„å¤šæ ·æ€§ã€‚ Model æ¨¡å‹åˆ†ä¸º5éƒ¨åˆ†: an embedding layer an embedding encoder layer a context-query attention layer a model encoder layer an output layer. the combination of convolutions and self-attention is novel, and is significantly better than self-attention alone and gives 2.7 F1 gain in our experiments. The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth (layer dropout) (Huang et al., 2016), which gives an additional gain of 0.2 F1 in our experiments. CNN å’Œ self-attention çš„ç»“åˆæ¯”å•ç‹¬çš„ self-attention æ•ˆæœè¦å¥½ã€‚åŒæ—¶ä½¿ç”¨äº† CNN ä¹‹åèƒ½å¤Ÿä½¿ç”¨å¸¸ç”¨çš„æ­£åˆ™åŒ–æ–¹å¼ dropout, è¿™ä¹Ÿèƒ½å¸¦æ¥ä¸€ç‚¹å¢ç›Šã€‚ Input embedding layer obtain the embedding of each word w by concatenating its word embedding and character embedding. ç”±è¯å‘é‡å’Œå­—ç¬¦å‘é‡æ‹¼æ¥è€Œæˆã€‚å…¶ä¸­è¯å‘é‡é‡‡ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ Gloveï¼Œå¹¶ä¸”ä¸å¯è®­ç»ƒï¼Œfixed. åªæœ‰ OOV (out of vocabulary) æ˜¯å¯è®­ç»ƒçš„ï¼Œç”¨æ¥æ˜ å°„æ‰€æœ‰ä¸åœ¨è¯è¡¨å†…çš„è¯ã€‚ Each character is represented as a trainable vector of dimension p2 = 200, meaning each word can be viewed as the concatenation of the embedding vectors for each of its characters. The length of each word is either truncated or padded to 16. We take maximum value of each row of this matrix to get a fixed-size vector representation of each word. å­—ç¬¦å‘é‡çš„å¤„ç†ã€‚æ¯ä¸ªå­—æ¯æ˜¯å¯è®­ç»ƒçš„ï¼Œå¯¹åº”çš„ç»´åº¦æ˜¯ 200 ç»´ã€‚ç„¶åæ¯ä¸ªè¯éƒ½ truncated æˆ–è€… padded æˆ16ä¸ªå­—æ¯ï¼Œä¿è¯æ¯ä¸ªè¯çš„å‘é‡ç»´åº¦æ˜¯ä¸€æ ·å¤§å°ã€‚ æ‰€ä»¥ä¸€ä¸ªè¯çš„å‘é‡ç»´åº¦æ˜¯ $300+200=500$. Embedding encoding layer The encoder layer is a stack of the following basic building block: [convolution-layer Ã— # + self-attention-layer + feed-forward-layer] å…¶ä¸­ï¼š convolution: ä½¿ç”¨ depthwise separable convolutions è€Œä¸æ˜¯ç”¨ä¼ ç»Ÿçš„ convolutionï¼Œå› ä¸ºä½œè€…å‘ç° it is memory efficient and has better generalization. æ€ä¹ˆç†è§£è¿™ä¸ªï¼Œè¿˜å¾—çœ‹åŸ paper. The kernel size is 7, the number of filters is d = 128. self-attention: the multi-head attention mechanism è®ºæ–‡ç¬”è®°, Attention Is All You Need Each of these basic operations (conv/self-attention/ffn) is placed inside a residual block, shown lower-right in Figure 1. For an input x and a given operation f, the output is f(layernorm(x))+x. åœ¨ cnn/self-attention/ffn å±‚éƒ½æœ‰ layer normalization. ä¸ºä»€ä¹ˆè¦ç”¨ CNNï¼šç”¨æ¥è·å–å±€éƒ¨ä¿¡æ¯ k-gram features ç›¸ä¿¡çœ‹äº†è¿™ä¸ªå›¾èƒ½å¯¹ QANet ä¸­çš„ cnn æ€ä¹ˆå®ç°çš„æ›´æ¸…æ¥šäº†ã€‚ä¸Šå›¾ä¸­æ¯ä¸ªå·ç§¯æ ¸çš„å°ºå¯¸åˆ†åˆ«æ˜¯ [2, embed_size], [3, embed_size], [3, embed_size]. paddingå‚æ•° ä½¿ç”¨çš„æ˜¯ â€œSAMEâ€. å¾—åˆ° 3 ä¸ª [1, sequence_len]ï¼Œç„¶åæ‹¼æ¥èµ·æ¥, å¾—åˆ°æœ€ç»ˆç»“æœ [filters_num, sequence_len]. åœ¨ QANet çš„å®ç°ä¸­ï¼Œkernel_size éƒ½è®¾ç½®ä¸º7, num_filters=128. ä¸ºä»€ä¹ˆè¦ç”¨ self-attentionç”¨æ¥è·å–å…¨å±€ä¿¡æ¯ã€‚ ä¸Šå›¾ä¸­çš„è¿™ç§æ–¹å¼æ˜¾ç„¶ä¸å¤ªå¥½ï¼Œå¤æ‚åº¦é«˜ä¸”æ•ˆæœä¸å¥½ã€‚äºæ˜¯æœ‰äº† self-attention. çŸ©é˜µå†…éƒ¨å‘é‡ä¹‹é—´ä½œå…§ç§¯ï¼Œå¹¶é€šè¿‡ softmax å¾—åˆ°å…¶ä»–è¯å¯¹äº â€œTheâ€ è¿™ä¸ªè¯çš„æƒé‡å¤§å°ï¼ˆæƒé‡æ¯”ä¾‹ä¸ç›¸ä¼¼åº¦æˆæ­£æ¯”ï¼Œè¿™é‡Œçœ‹ä¼¼ä¸å¤ªåˆç† similarity == match??ï¼Œä½†å®é™…ä¸Šæ•ˆæœå¾ˆä¸é”™ï¼Œå¯èƒ½è·Ÿè¯å‘é‡çš„è®­ç»ƒæœ‰å…³ï¼‰ã€‚ ç„¶åå°†å¯¹åº”çš„æƒé‡å¤§å° $[w_1,w_2,w_3,w_4,w_5]$ ä¸å¯¹åº”çš„è¯ç›¸ä¹˜ï¼Œç´¯å’Œå¾—åˆ°è•´å«äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ contextualized â€œTheâ€. å¹¶ä¸”ï¼Œè¿™æ˜¯å¯ä»¥å¹¶è¡ŒåŒ–çš„ã€‚å¤§å¤§åŠ é€Ÿäº†è®­ç»ƒé€Ÿåº¦ã€‚ Context-Query Attention Layerè·Ÿ BIDAF æ˜¯ä¸€æ ·çš„ã€‚æ¥ï¼Œä¸çœ‹ç¬”è®°æŠŠå…¬å¼è¿‡ä¸€éã€‚ content: $C={c_1, c_2,â€¦,c_n}$ query: $Q={q_1,q_2,â€¦q_m}$. æ‰€ä»¥ embeded ä¹‹åï¼Œ content: [batch, content_n, embed_size] query: [batch, query_m, embed_size] åšçŸ©é˜µç›¸ä¹˜å¾—åˆ°ç›¸ä¼¼çŸ©é˜µ similarity matrix $S\\in R^{n\\times m}$: sim_matrix: [batch, content_n, query_m] The similarity function used here is the trilinear function (Seo et al., 2016). $f(q,c)=W_0[q,c,q\\circ c]$. ç›¸ä¼¼çŸ©é˜µçš„è®¡ç®—å¯ä»¥ä¸æ˜¯ç›´æ¥çŸ©é˜µç›¸ä¹˜ï¼Œè€Œæ˜¯åŠ ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œã€‚æ¯•ç«Ÿ similarity ä¸ä¸€å®šç­‰äº match. content-to-queryå¯¹ S æ¯ä¸€è¡Œ row åš softmax å¾—åˆ°å¯¹åº”çš„æ¦‚ç‡ï¼Œå¾—åˆ°æƒé‡çŸ©é˜µ $\\tilde S\\in R^{n\\times m}$, shape = [batch, content_n, query_m]. ç„¶åä¸ query $Q^T$ [batch, query_m, embed_size] çŸ©é˜µç›¸ä¹˜å¾—åˆ°ç¼–ç äº† query ä¿¡æ¯çš„ content: $A = \\tilde SQ^T$, shape = [batch, content_n, embed_size] query_to_content Empirically, we find that, the DCN attention can provide a little benefit over simply applying context-to-query attention, so we adopt this strategy. è¿™é‡Œæ²¡æœ‰é‡‡ç”¨ BiDAF é‡Œé¢çš„æ–¹æ³•ï¼Œè€Œæ˜¯é‡‡ç”¨ DCN ä¸­çš„æ–¹å¼ï¼Œåˆ©ç”¨äº† $\\tilde S$. å¯¹ S æ¯ä¸€åˆ— column åš softmax å¾—åˆ°çŸ©é˜µ $\\overline S$, shape = [batch, content_n, query_n]. ç„¶åçŸ©é˜µç›¸ä¹˜å¾—åˆ° $B=\\tilde S \\overline S^T C^T$. $\\tilde S$.shape=[batch, content_n, query_m] $\\overline S^T$.shape=[batch, query_m, content_n] $C^T$.shape=[batch, query_m, embed_size] æ‰€ä»¥æœ€å B.shape=[batch, content_n, embed_size] Model Encoder LayeråŒ BiDAF ä¸€æ ·è¾“å…¥æ˜¯ $[c,a,c\\circ a,c\\circ b]$ï¼Œ å…¶ä¸­ a, b åˆ†åˆ«æ˜¯ attention matrix Aï¼ŒB çš„è¡Œå‘é‡ã€‚ä¸è¿‡ä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œä¸åŒ bi-LSTMï¼Œè€Œæ˜¯ç±»ä¼¼äº encoder æ¨¡å—çš„ [conv + self-attention + ffn]. å…¶ä¸­ conv å±‚æ•°æ˜¯ 2, æ€»çš„ blocks æ˜¯7. Ouput layer$$p^1=softmax(W_1[M_0;M_1]), p^2=softmax(W_2[M_0;M_2])$$ å…¶ä¸­ $W_1, w_2$ æ˜¯å¯è®­ç»ƒçš„å‚æ•°çŸ©é˜µï¼Œ$M_0, M_1, M_2$ å¦‚å›¾æ‰€ç¤ºã€‚ ç„¶åè®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼š $$L(\\theta)=-\\dfrac{1}{N}\\sum_i^N[log(p^1_{y^1})+log(p^2_{y^2})]$$ QANet å“ªé‡Œå¥½ï¼Œå¥½åœ¨å“ªå„¿ï¼Ÿ separable conv ä¸ä»…å‚æ•°é‡å°‘ï¼Œé€Ÿåº¦å¿«ï¼Œè¿˜æ•ˆæœå¥½ã€‚å°† sep å˜æˆä¼ ç»Ÿ cnn, F1 å€¼å‡å° 0.7. å»æ‰ CNNï¼Œ F1å€¼å‡å° 2.7. å»æ‰ self-attention, F1å€¼å‡å° 1.3. layer normalization residual connections L2 regularization å‚è€ƒæ–‡çŒ® Dynamic Coattention Networks For Question Answering Xception: Deep Learning with Depthwise Separable Convolutions Attention Is All You Need qanet_talk_v1.pdf","link":"/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/"},{"title":"è®ºæ–‡ç¬”è®°-sign language recognition and translation","text":"paper list to read: BABEL: Bodies, Action and Behavior with English Labels, CVPR2021 Fingerspelling Detection in American Sign Language, CVPR2021 American Sign Language fingerspelling recognition in the wild. SLT2018 Fingerspelling recognition in the wild with iterative visual attention. ICCV2019 Fingerspelling Detectionæ‰‹æŒ‡æ‹¼å†™çš„ä½œç”¨ï¼š ä¸“æœ‰åè¯ æŠ€æœ¯æœ¯è¯­ ç¼©å†™ç­‰æ²¡æœ‰å¯¹åº”æ‰‹åŠ¿çš„è¯æ±‡ ä¹Ÿç”¨äºå¼ºè°ƒå’Œæ–¹ä¾¿ æ‰‹æŒ‡æ‹¼å†™å ASLçš„ 12%-35%. è¿™ä¸ªæ¯”ä¾‹æ¯”å¤§éƒ¨åˆ†æ‰‹è¯­è¯æ±‡é‡éƒ½è¦å¤§ã€‚ Fingerspelling is used for multiple purposes, including for words that do not have their own signs (such as many proper nouns, technical terms, and abbreviations) [39] but also sometimes for emphasis or expediency. Fingerspelling accounts for 12% to 35% of ASL, where it is used more than in other sign languages [40]. æ‰‹æŒ‡æ‹¼å†™å¯¹åº”çš„å­—æ¯ä¸ç¿»è¯‘å‡ºæ¥çš„è‹±è¯­æ˜¯å•è°ƒå¯¹é½çš„ã€‚è¿™æœ‰ç‚¹ç±»ä¼¼äºç¿»è¯‘ä¸­çš„ç›´æ¥éŸ³è¯‘ã€‚ æ‰‹æŒ‡æ‹¼å†™çš„æ£€æµ‹å¯¹äºä¸‹æ¸¸æ‰‹è¯­è¯†åˆ«ä»»åŠ¡æœ‰æ˜¾è‘—æå‡ä½œç”¨ã€‚","link":"/2021/07/11/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sign-language-recognition-and-translation/"},{"title":"è®ºæ–‡ç¬”è®°-sentence embedding","text":"å¥å­çš„å‘é‡è¡¨ç¤ºæ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ã€‚ ä¸€ç±»æ˜¯é€šè¿‡æ— ç›‘ç£çš„æ–¹æ³•å¾—åˆ° universal sentence embedding, å¦ä¸€ç±»æ˜¯åŸºäºç‰¹å®šçš„ä»»åŠ¡ï¼Œæœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æœ‰ç›‘ç£å­¦ä¹ å¾—åˆ° sentence embedding. supervised learninga structured self-attentive sentence embeddingpaper: A Structured Self-attentive Sentence Embedding, ICLR 2017 ä¼ ç»Ÿçš„åŸºäº RNN/LSTM å¾—åˆ° sentence çš„å‘é‡è¡¨ç¤ºçš„æ–¹æ³•é€šå¸¸æ˜¯åˆ©ç”¨éšè—çŠ¶æ€çš„æœ€åä¸€ä¸ªçŠ¶æ€ï¼Œæˆ–è€…æ˜¯æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œç„¶åé‡‡ç”¨ sum/average/max pooling çš„çš„æ–¹å¼å¾—åˆ° sentence embedding. æœ¬æ–‡ä½¿ç”¨çš„æ–¹æ³•å°±æ˜¯åœ¨ LSTM ä¸ŠåŠ ä¸Šä¸€å±‚ attentionï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨é€‰æ‹© sentence ä¸­çš„æŸäº›æ–¹é¢ï¼Œä¹Ÿå°±æ˜¯èµ‹äºˆ sentence ä¸­çš„æ¯ä¸€ä¸ªè¯ä¸€ä¸ªæƒé‡ï¼Œç„¶ååŠ æƒæ±‚å’Œå¾—åˆ°ä¸€ä¸ª vector. æœ¬æ–‡å¦ä¸€ä¸ªåˆ›æ–°ç‚¹åœ¨äºï¼Œä¸ä»…ä»…å¾—åˆ°ä¸€ä¸ª vectorï¼Œè€Œæ˜¯ä¸€ä¸ª matrixï¼Œç”¨æ¥è¡¨ç¤ºä¸€ä¸ª sentence ä¸­çš„ä¸åŒæ–¹é¢ã€‚ Model Architecture: æ¨¡å‹ä¹Ÿå¾ˆç®€å•ï¼Œè¾“å…¥ sentence n tokens. word embedding: $S\\in R^{n\\times d}$, d è¡¨ç¤ºè¯å‘é‡ç»´åº¦ $$S=(w_1,w_2,â€¦,w_n)$$ bidirection-LSTM: $H\\in R^{n\\times 2u}$, u è¡¨ç¤ºéšè—çŠ¶æ€ç»´åº¦ $$H=(h_1,h_2,â€¦,h_n)$$ single self-attention: $a\\in R^n$, è¡¨ç¤º sentence ä¸­å¯¹åº”ä½ç½®çš„æƒé‡ã€‚ä¸ encoder ä¹‹åçš„ sentence åŠ æƒæ±‚å’Œå¾—åˆ° attention vector $m\\in R^{2u}$. $$a=softmax(w_{s2}tanh(W_{s1}H^T))$$ r-dim self-attentionï¼šæœ‰ r ä¸ªä¸Šè¿°çš„ attention vectorï¼Œå¹¶è½¬æ¢æˆçŸ©é˜µå½¢å¼ï¼Œ$A\\in R^{n\\times r}$ ä¸ encode ä¹‹åçš„å¥å­è¡¨ç¤º H åŠ æƒæ±‚å’Œå¾—åˆ° embedding matrix $M\\in R^{r\\times 2u}$ $$A=softmax(W_{s2}tanh(W_{s1}H^T))$$ $$M=AH$$ penalization termä¸Šé¢æ¨¡å‹ä¸­ä½¿ç”¨ r ä¸ª attentionï¼Œå¾ˆå¯èƒ½ä¼šå‡ºç°å†—ä½™çš„æƒ…å†µï¼Œä¹Ÿå°±æ˜¯å¾—åˆ°çš„ r ä¸ª attention vector( è®ºæ–‡ä¸­è¯´çš„æ˜¯ summation weight vectors) å¯èƒ½å¾—åˆ°çš„æ˜¯åŒä¸€ä¸ªä¸œè¥¿ï¼Œæ‰€ä»¥éœ€è¦ diversity. The best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors. However, we found that not very stable in our case. We conjecture it is because we are maximizing a set of KL divergence (instead of minimizing only one, which is the usual case), we are optimizing the annotation matrix A to have a lot of sufficiently small or even zero values at different softmax output units, and these vast amount of zeros is making the training unstable. There is another feature that KL doesnâ€™t provide but we want, which is, we want each individual row to focus on a single aspect of semantics, so we want the probability mass in the annotation softmax output to be more focused. but with KL penalty we cant encourage that. ä¸€ä¸ªæœ€ç›´è§‚çš„æ–¹æ³•æ˜¯ Kullback Leibler divergenceï¼Œä¹Ÿå°±æ˜¯ç›¸å¯¹ç†µã€‚å› ä¸ºå¾—åˆ°çš„ attention vector æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ (distribution ), ä»»æ„ä¸¤ä¸ªåˆ†å¸ƒå·®å¼‚è¶Šå¤§ï¼Œå¯¹åº”çš„ç›¸å¯¹ç†µè¶Šå¤§ã€‚ä½†æ˜¯ä½œè€…å®éªŒå‘ç°è¿™ç§æ–¹æ³•ä¸ç¨³å®šï¼ŒåŸå› ä½œè€…æ¨æµ‹æ˜¯è¿™é‡Œéœ€è¦æœ€å¤§åŒ–çš„æ˜¯å¤šä¸ª KL æ•£åº¦çš„é›†åˆï¼Œå¹¶ä¸”åœ¨ä¼˜åŒ– annotation matrix A æ—¶ï¼Œåœ¨ä¸åŒçš„softmaxè¾“å‡ºå•å…ƒä¸Šæœ‰å¾ˆå¤šè¶³å¤Ÿå°ç”šè‡³é›¶å€¼ï¼Œè€Œè¿™äº›å¤§é‡çš„é›¶ç‚¹ä½¿å¾—è®­ç»ƒä¸ç¨³å®š. å¦ä¸€æ–¹é¢ï¼ŒKL æ•£åº¦ä¸èƒ½ focus on è¯­ä¹‰ä¸­çš„å•ä¸ªæ–¹é¢ã€‚ é’ˆå¯¹ä¸Šé¢è¿™ä¸¤ç‚¹ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„æ­£åˆ™é¡¹ï¼š $$P=||(AA^T-I)||^2_{F}$$ $AA^T$ æ˜¯åæ–¹å·®çŸ©é˜µï¼Œå¯¹è§’çº¿å…ƒç´ æ˜¯åŒä¸€å‘é‡çš„å†…ç§¯ï¼Œéå¯¹è§’çº¿å…ƒç´ ä¸åŒå‘é‡çš„å†…ç§¯ã€‚å°†å…¶ä½œä¸ºæƒ©ç½šé¡¹åŠ åˆ° original loss ä¸Šï¼ŒæœŸæœ›å¾—åˆ°çš„æ˜¯ä¸åŒ vector å†…ç§¯è¶Šå°è¶Šå¥½ï¼ˆå†…ç§¯è¶Šå°ï¼Œå·®å¼‚è¶Šå¤§ï¼‰ï¼Œå¹¶ä¸”å‘é‡çš„æ¨¡é•¿è¶Šå¤§è¶Šå¥½ï¼ˆæ¦‚ç‡åˆ†å¸ƒæ›´é›†ä¸­äºæŸä¸€ä¸¤ä¸ªè¯ï¼‰ã€‚ æœ€ç»ˆå¾—åˆ°çŸ©é˜µçº§åˆ«çš„å¥å­å‘é‡è¡¨ç¤ºã€‚ training3 different datasets: the Age dataset the Yelp dataset the Stanford Natural Language Inference (SNLI) Corpus æ ¹æ®ä¸åŒçš„ä»»åŠ¡æœ‰ç›‘ç£çš„è®­ç»ƒã€‚ unsupervised learningsent2vecpaper: Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features","link":"/2019/02/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sentence-embedding/"},{"title":"è®ºæ–‡ç¬”è®°-unlikelihood training","text":"paper list: Neural Text Generation with Unlikelihood Training Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning åŸºäºæ ‡å‡†çš„ä¼¼ç„¶è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨è§£ç æ—¶ä¼šå‡ºç°å¾ˆdullçš„é‡å¤é—®é¢˜ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š è¯šç„¶å°†likelihoodä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œèƒ½å¾—åˆ°å¼ºå¤§çš„å…·æœ‰è¯­è¨€ç†è§£èƒ½åŠ›çš„æ¨¡å‹ã€‚ä½†æ˜¯å°†ä¼¼ç„¶ä½œä¸ºç›®æ ‡ä¼šå¯¼è‡´è¯­è¨€çš„å¹³æ·¡å’Œå¥‡æ€ªçš„é‡å¤ã€‚è€Œä¸”åŸºäºå¼ºå¤§çš„GPT2-117Mï¼Œä½¿ç”¨beam searchï¼Œç†è®ºä¸Šbeam sizeè¶Šå¤§ï¼Œç”Ÿæˆçš„å¥å­æ¦‚ç‡è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½æ‰å¯¹ã€‚ä½†äº‹å®å´æ˜¯åç›´è§‰çš„ï¼Œbeam sizeè¶Šå¤§ï¼Œè¯­è¨€çš„é€€åŒ–æ•ˆæœæ›´æ˜æ˜¾ã€‚ï¼ˆæˆ‘çŒœè¿™ä¹Ÿæ˜¯å¾ˆå¤šæ—¶å€™beam sizeè®¾ä¸º10å·¦å³ï¼Œè€Œä¸ä¼šæ›´å¤§ã€‚ï¼‰ Holtzman[^1] æ­ç¤ºäº†è¯­è¨€æ¨¡å‹çš„è¿™ç§é€€åŒ–ç°è±¡ï¼Œä»–ä»¬å°†æœºå™¨ç”Ÿæˆçš„è¯­è¨€å’Œäººç±»è¯­è¨€è¿›è¡Œå¯¹æ¯”ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œäººç±»æ–‡æœ¬åœ¨ç”Ÿæˆæ¯ä¸ªtokençš„å›°æƒ‘åº¦ä¸­è¡¨ç°å‡ºç›¸å½“å¤§çš„æ³¢åŠ¨ï¼Œè€Œç”±æœ€å¤§ä¼¼ç„¶è§£ç äº§ç”Ÿçš„æœºå™¨æ–‡æœ¬çš„åˆ†å¸ƒåˆ™å‡ºç°ä¸è‡ªç„¶çš„å¹³å¦å’Œè¾ƒé«˜çš„tokenæ¦‚ç‡. äº‹å®ä¸Šï¼Œè¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¯çš„æ—¶å€™ï¼Œå¤§éƒ¨åˆ†çš„æ¦‚ç‡é›†ä¸­åœ¨å‡ ç™¾ä¸ªtokensä¸Šã€‚è¿™é™åˆ¶äº†æœºå™¨æ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œä¹Ÿæ˜¯å¯¼è‡´æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆé€€åŒ–çš„åŸå› ã€‚åŸºäºæ­¤ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œtop-k, nucleus samplingæ˜¯ä¸é”™çš„æ–¹æ³•ã€‚ ä½†æ˜¯samplingçš„æ–¹æ³•å¹¶æ²¡æœ‰æ”¹å˜æ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡å…¶æœ¬èº«ã€‚ç”Ÿæˆæ–‡æœ¬é€€åŒ–çš„çœŸæ­£åŸå› è¿˜æœªçŸ¥ï¼Œå·²æœ‰çš„è®ºæ–‡è®¤ä¸ºæœ‰ä»¥ä¸‹å‡ ç§å¯èƒ½ï¼š æ¨¡å‹æ¶æ„é€‰æ‹©çš„by-productï¼Œä¾‹å¦‚ Transformer æ›´å–œæ¬¢é‡å¤; äººç±»è¯­è¨€çš„å†…åœ¨å±æ€§ï¼Œè€Œä¸æ˜¯æ¨¡å‹ç¼ºé™·ï¼› è¯­æ–™æœ‰é™; ç›¸æ¯”ä¹‹ä¸‹ï¼ŒWelleck[^2] è®¤ä¸ºç”Ÿæˆæ¨¡å‹ä»¥æœ€å¤§ä¼¼ç„¶æœ€ä¸ºè®­ç»ƒç›®æ ‡ä¼šå¯¼è‡´æ–‡æœ¬ç”Ÿæˆé€€åŒ–ã€‚å…¶åŸå› æœ‰ï¼š å°†æ³¨æ„åŠ›é›†ä¸­åœ¨argmaxæˆ–top-k,è€Œä¸æ˜¯ä¼˜åŒ–æ•´ä¸ªdistribution åªæ˜¯é›†ä¸­äºä¸‹ä¸€ä¸ªtokençš„ç”Ÿæˆï¼Œè€Œä¸æ˜¯æ•´ä¸ªå¥å­çš„ä¼˜åŒ– ä¸ºæ­¤ï¼ŒWelleck[^2] æå‡ºäº† unlikelihood training: ä¾æ®likelihoodæ¥ä¼˜åŒ–target tokensï¼Œå¹¶ç»™äºˆè¾ƒå¤§çš„æ¦‚ç‡ ä¾æ®unlikelihoodæ¥æ›´æ–°ï¼Œé¿å…ç»™äºˆtarget tokenså¤ªå¤§çš„æ¦‚ç‡ã€‚ï¼ˆä¸çŸ¥ç†è§£çš„å¯¹é”™ï¼ŒåŸæ–‡å¦‚ä¸‹ï¼‰ Unlikelihood training works by combining two types of updates: a likelihood update on the true target tokens so they are assigned high probability, and an unlikelihood update on tokens that are otherwise assigned too high a probability. unlikelihood lossunlikelihood lossçš„æ ¸å¿ƒå°±æ˜¯é™ä½negative condidates $C^t$ çš„ä¼¼ç„¶æ¦‚ç‡ã€‚ token-level unlikelihood lossä»¥è‡ªå›å½’çš„è¯­è¨€æ¨¡å‹ä¸ºä¾‹ï¼Œä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„lossè®¡ç®—åŒ…æ‹¬targetçš„ä¼¼ç„¶æœ€å¤§åŒ–ï¼Œä»¥åŠnegative candidatesçš„ä¼¼ç„¶æœ€å°åŒ–ã€‚ negative candidates æ˜¯å½“å‰è¯•äº†ä¹‹å‰çš„è¯æ±‡ã€‚ sequence_level unlikelihood lossæˆ‘ä»¬çŸ¥é“åŸºäºè‡ªå›å½’æ¨¡å‹çš„è®­ç»ƒå’Œè§£ç æ˜¯å­˜åœ¨exposure biasçš„ï¼Œä¹Ÿå°±æ˜¯è§£ç çš„æ—¶å€™æœ‰è¯¯å·®ç´¯ç§¯ã€‚å…¶å®è¿™ç§distribution mismatchæ˜¯maxmimum-likelihoodè¿½æ±‚å½“å‰æ—¶åˆ»çš„æ¦‚ç‡æœ€å¤§è¯ã€‚è€Œæ²¡æœ‰ä»æ•´ä¸ªå¥å­çš„å±‚é¢å»è€ƒè™‘ã€‚æ¯”å¦‚é‡å¤é—®é¢˜ï¼Œä½ ä¸Šä¸€ä¸ªè¯å‡ºç°è¿‡äº†ï¼Œä¸‹ä¸€ä¸ªè¯è¿˜å‡ºç°å®ƒï¼›ä½ è¿™å¥è¯è¯´è¿‡ä¸€éäº†ï¼Œä½ è¿˜è¦å†è¯´ä¸€éã€‚ã€‚è¿™ä¸ç¦»è°±å—ã€‚ä½†æ˜¯æ¨¡å‹å°±æ˜¯è¿™ä¹ˆå‚»ï¼Œæˆ–è€…è¯´æ¨¡å‹æ²¡æœ‰å¤§å±€è§‚çš„åŸå› æ˜¯ä¹‹å‰çš„ä¼˜åŒ–éƒ½æ˜¯åœ¨token-levelå±‚é¢ã€‚ å› æ­¤ï¼ŒWelleck[^2] æå‡ºäº†sequence-level unlikelihood loss. è¿™ä¸ªå…¬å¼çœ‹èµ·æ¥è·Ÿå‰é¢ä¸€æ ·ï¼Œä½†æ˜¯åŒºåˆ«åœ¨äº negative condidatesçš„é€‰æ‹©ã€‚è¿™é‡Œçš„negativeæ˜¯ä»n-gramå±‚é¢æ¥è€ƒé‡çš„ã€‚ ä¹Ÿå°±æ˜¯å¯¹äºå½“å‰æ—¶é—´æ­¥ï¼Œå¦‚æœå®ƒæ˜¯é‡å¤çš„n-gramçš„ä¸€éƒ¨åˆ†ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯negative candidate. è¿™é‡Œæœ‰ç‚¹ä¸å¤ªå¥½ç†è§£ï¼Œtoken-levelå°±æ˜¯æŠŠä¹‹å‰å‡ºç°çš„è¯ä½œä¸ºnegative candidate. å¦‚æœæœ‰äº›è¯åœ¨å‰é¢å¹¶æ²¡æœ‰å‡ºç°ï¼Œä½†å®ƒçš„å‡ºç°ä¼šå¯¼è‡´é‡å¤çš„n-gramï¼Ÿè¿™ä¸åˆç†å•Šï¼Œå®ƒéƒ½æ²¡å‡ºç°ï¼Œæ€ä¹ˆå¯èƒ½å‡ºç°åŒ…å«å®ƒçš„n-gramå‘¢ï¼Ÿï¼Ÿ è¿™æ ·æƒ³ sequence-level ä¸å°±æ˜¯ token-levelçš„ä¸€ç§ç‰¹æ®Šå½¢å¼ã€‚ å¸¦ç€ç–‘é—®å»çœ‹ä»£ç å§ã€‚çœ‹å®Œä»£ç ï¼Œsequence-levelæ˜¯åœ¨è®­ç»ƒå®Œtoken-levelä¹‹åï¼Œå†è¿›è¡Œfinetuneï¼Œå¯¹å¯¼è‡´å‡ºç°é‡å¤çš„ngramçš„æŸä¸ªtime-stepè¿›è¡Œæƒ©ç½šã€‚ã€‚åœ¨æˆ‘çš„å®éªŒä¸Šä¸å¤ªé è°±ï¼Œé‡å¤åè€Œå˜å¤šäº† [^1]: The Curious Case of Neural Text Degeneration[^2]: Neural text degeneration with unlikelihood training","link":"/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/"},{"title":"è®ºæ–‡ç¬”è®°-video generation","text":"GenHFi: Generating high fidelity images with subscale pixel networks and multidimensional upscaling. (ICLR2019) paper2: Scaling autoregressive video models (ICLR2020) Video pixel networks. (CoRR2016) Parallel: Parallel multiscale autoregressive density estimation VQGAN: Taming Transformers for High-Resolution Image Synthesis TeCoGAN: Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation ImaGINator: Conditional Spatio-Temporal GAN for Video Generation Temporal Shift GAN for Large Scale Video Generation MoCoGAN: Decomposing Motion and Content for Video Generation Playable Video Generation (CVPR2021) GenHFi title: Generating high fidelity images with subscale pixel networks and multidimensional upscaling Abstract: Subscale Pixel Network (SPN): a conditional decoder architecture that generates an image as a sequence of sub-images of equal size Multidimensional Upscaling: grow an image in both size and depth via intermediate stages utilising distinct SPNs Introduction The multi-facted relationship between MLE scores and the fidelity of samples MLE is a well-defined measure as improvements in held-out scores generally produce improvements in the visual fidelity of the samples. MLE forces the model to support the entire empirical distribution. This guarantees the modelâ€™s ability to generalize at the cost of allotting capacity to parts of the distribution that are irrelevant to fidelity. A 256 Ã— 256 Ã— 3 image has a total of 196,608 positions that need to be architecturally connected in order to learn dependencies among them. Contribution Multidimensional Upscaling Small size, lower depth -&gt; large size, lower depth -&gt; large size, high depth Subscale Pixel Network (SPN) architecture divides an image of size $N\\times N$ into sub-images of size $\\dfrac{N}{S}\\times \\dfrac{N}{S}$ sliced out at interleaving positions SPN consists of two networks, a conditioning network that embeds previous slices and a decoder proper that predicts a single target slice given the context embedding. Architecture VQ-GAN Contribution take the convolution and transformer together: use a convolutional approach to efficiently learn a codebook of context-rich visual parts and, subsequently, learn a model of their global compositions. The long-range interactions within these compositions require an expressive transformer architecture to model distributions over their consituent visual parts. utilize an adversarial approach to ensure that the dictionary of local parts captures perceptually important local structure to alleviate the need for modeling low-level statistics with the transformer architecture. Approach Learning an Effective Codebook of Image Constituents VA-VAE: $$\\hat x = G(z_q) = G(q(E(x)))$$ Learning a Perceptually Rich Codebook using GAN and perception loss Learning the Composition of Images with Transformers latent transformer Conditioned Synthesis TeCoGAN paper: Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation Contribution propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. Related works Sequential generation tasks: [Kim et al. 2019; Xie et al. 2018]. Conditional video generation tasks [JamriÅ¡ka et al. 2019; Sitzmann et al. 2018; Wronski et al. 2019; Zhang et al. 2019] motion estimation [Dosovitskiy et al. 2015; Liu et al. 2019] explicitly using variants of optical flow networks [Caballero et al. 2017; Sajjadi et al. 2018; Shi et al. 2016] GANS: Zhu et al. [2017] focuses on images without temporal constrains RecycleGAN [Bansal et al. 2018] proposes to use a prediction network in addition to a generator a concurrent work [Chen et al. 2019] chose to learn motion translation in addition to the spatial content translation. Metrics: perceptual metrics [Prashnani et al. 2018; Zhang et al. 2018] are proposed to reliably consider semantic features instead of pixel-wise errors. Adversarial temporal losses: tempoGAN for fluid flow [Xie et al. 2018] vid2vid for video translation [Wang et al. 2018a] 3D discriminator DeepFovea [Kaplanyan et al. 2019] Bashkirova et al. [2018] For tracking and optical flow estimation, L2-based time-cycle losses [Wang et al. 2019b] MethodSpatio-Temporal Adversarial Learning Notation: $\\alpha$ Input domain, $b$ target domain, $g$ generated domain $w$ motion compensation, Self-Supervision for Long-term Temporal Consistency When inferring this in a frame-recurrent manner, the generated result should not strengthen any invalid features from frame to frame. Rather, the result should stay close to valid information and be symmetric, i.e., the forward result $g_t=G(a_t, {t-1})$ and the one generated from the reversed part, ${g_t}^{â€˜}=G(a_t, {g{t+1}}^{â€˜})$, should be identical. bi-directional â€œPing-Pongâ€ loss â€‹ $$\\mathcal{L}{pp}=\\sum{t=1}^{n-1}||g_t-{g_t}^{â€˜}||_2$$ Parallel multiscale","link":"/2021/09/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-generation/"},{"title":"è®ºæ–‡ç¬”è®°-video transformer","text":"paper list: Training data-efficient image transformers &amp; distillation through attention. An image is worth 16x16 words: Transformers for image recognition at scale. ViViT: A Video Vision Transformer. Is space-time attention all you need for video understanding Video transformer network. Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows What Makes for Hierarchical Vision Transformer? WiderNet Go Wider Instead of Deeper CoAtNet: Marrying Convolution and Attention for All Data Sizes DeiTåˆ©ç”¨attentionæœºåˆ¶è®¾è®¡äº†ä¸€ç§ç‰¹æ®Šçš„çŸ¥è¯†è’¸é¦æœºåˆ¶ï¼Œèƒ½æ˜¾è‘—æå‡æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶ä¸€å®šç¨‹åº¦æé«˜å‡†ç¡®ç‡ã€‚ ViViT: A Video Vision Transformerä½œè€…é€šè¿‡å¤§é‡å®éªŒæ¥ç ”ç©¶ vision transformerï¼Œå¹¶æ¢ç´¢æœ€åˆé€‚çš„ä¸”efficientçš„ç»“æ„ä½¿å…¶èƒ½é€‚ç”¨äºsmall datasets. å…¶ä¸­ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢çš„æ¢ç´¢ï¼š tokenization strategies model architecture regularisation methods. transformer-based architectureçš„ç¼ºç‚¹ï¼šç›¸æ¯”å·ç§¯ç½‘ç»œï¼Œtransformerç¼ºä¹äº†ä¸€å®šçš„å½’çº³åç½®èƒ½åŠ›ï¼Œæ¯”å¦‚å¹³ç§»ä¸å˜æ€§ã€‚å› æ­¤ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®æ¥å­¦ä¹ ã€‚ WiderNet CoAtNet ä½œè€…ä»æ³›åŒ–æ€§generalizationå’Œæ¨¡å‹å®¹é‡model capacityä¸¤ä¸ªæ–¹é¢æ¥åˆ†æconvå’Œattnçš„åŒºåˆ«ï¼š convolutional layers tend to have better generalization with faster converging speed thanks to their strong prior of inductive bias attention layers have higher model capacity that can benefit from larger datasets. Motivation how to effectively combine convolution and attention to achieve better trade-offs between accuracy and efficiency. ContributionIn this paper, we investigate two key insights: First, we observe that the commonly used depthwise convolution can be effectively merged into attention layers with simple relative attention; Second, simply stacking convolutional and attention layers, in a proper way, could be surprisingly effective to achieve better generalization and capacity. SOTA performances under comparable resource constraints across different data sizes Model Architecture How to combine the convolution and self-attention within one basic computational block? How to vertically stack different types of computational blocks together to form a complete network? Merging convolution and self-attention convolution relies on a fixed kernel to gather information from a local receptive field self-attention allows the receptive field to be the entire spatial locations and computes the weights based on the re-normalized pairwise similarity between the pair $(x_i, x_j)$ the good properties of conv and attn: the conv kernel is input-independent, but the attention weight dynamically depends on the input, it is much easier for the self-attention to capture complicated relational interactions between different spatial positions. translation equivalence of the conv is lacked in attn. the size of the receptive field is different between conv and attn. Combine the desirable propertiesâ€‹ $y_i^{pre}$ corresponds to a particular variant of relative self-attention. ï¼ˆpreå°±æ˜¯ç›¸å¯¹ä½ç½®attentionçš„å˜ç§å•Šï¼è¿™ä¹ˆè§£é‡Šï¼Œå¾ˆç¥å¥‡ï¼ï¼‰ Vertical Layout Designto overcome quadratic complexity of self-attention, there three options: (A) down-sampling to reduce the spatial size and employ the global relative attention after the feature map reaches manageable level. (B) Enforce local attention (C) Replace the quadratic Softmax attention with certain linear attention variant which only has a linear complexity w.r.t. the spatial size the authors experiment with C without getting reasonable good results. B requires many shape formatting operation which are slow on TPU. THus they select the option A. Filve variants: ViT$_{rel}$ , CTTT, CCTT, CCCT, CCCC. From the ImageNet-1K results, in terms of generalization capability: As for model capacity, from the JFT comparison: â€‹ To decide the CCTT and CTTT, they conduct another transferability test","link":"/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/"},{"title":"è®ºæ–‡ç¬”è®°-å†çœ‹ Capsules ä»¥åŠ capsules åœ¨æ–‡æœ¬åˆ†ç±»ä¸Šçš„åº”ç”¨","text":"å‚è€ƒï¼š blog: è‹å‰‘æ—ï¼šæ­å¼€è¿·é›¾ï¼Œæ¥ä¸€é¡¿ç¾å‘³çš„Capsuleç››å®´ paperï¼šInvestigating Capsule Networks with Dynamic Routing for Text Classification å†çœ‹ Capsulesè‹å‰‘æ—åŒå­¦åœ¨ä»–çš„é‚£ç¯‡åšå®¢ä¸­å¯¹ capsule çš„ç†è§£å¾ˆæœ‰é“ç†ï¼Œèƒ¶å›Šä¹Ÿå°±æ˜¯ç”¨ â€œvector in vector outâ€ å–ä»£äº† â€œscaler in scaler outâ€ã€‚ åœ¨æˆ‘çš„ä¸Šä¸€ç¯‡ blog ä¸­åœ¨ PrimaryCaps æ¨¡å—ä¸­ï¼Œå°†å‰ä¸€æ­¥é€šè¿‡å·ç§¯å¾—åˆ°çš„è¾“å‡ºæ˜¯ [batch, 20, 20, 256]. ç»è¿‡ PrimaryCaps ç¬¬ä¸€æ­¥ affine-transform è½¬æ¢æˆ [batch, 6, 6, 32, 8]. å®é™…ä¸Šå°±æ˜¯åœ¨è¿™ä¸€æ­¥å°†ä¸€ä¸ªåƒç´ ç‚¹çš„ç‰¹å¾è½¬æ¢æˆäº†ä¸€ä¸ª 8d-vector çš„èƒ¶å›Šã€‚ äº‹å®ä¸Šï¼Œåœ¨ NLP çš„ä»»åŠ¡ä¸­ï¼Œè¿™ç§ç”¨å‘é‡æ¥è¡¨ç¤ºä¸€ç»´ç‰¹å¾çš„åšæ³•ç¡®å®æœ€åŸºæœ¬çš„ã€‚æ¯”å¦‚ one-hot å‘é‡ï¼Œword2vec å°†è¯è½¬æ¢æˆ dense vector. åœ¨ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œä¸­ï¼Œä»ä½å±‚æ¬¡çš„ç‰¹å¾é€æ­¥æŠ½è±¡ï¼Œå½’çº³ä¸ºé«˜å±‚æ¬¡çš„ç‰¹å¾ï¼Œæ˜¯é€šè¿‡æƒé‡åŠ æƒæ±‚å’Œå¾—åˆ°çš„ï¼Œæ¯”å¦‚å·ç§¯å•Šï¼Œå…¨è¿æ¥éƒ½æ˜¯è¿™æ ·ï¼Œç„¶åé€šè¿‡æ¢¯åº¦åå‘ä¼ æ’­ï¼Œæ›´æ–°è¿™äº›æƒé‡å‚æ•°ã€‚ è¿™ä¸ªè¿‡ç¨‹æŸç§ç¨‹åº¦ä¸Šæ¨¡æ‹Ÿäº†äººçš„å±‚æ¬¡åˆ†ç±»åšæ³•ï¼Œä»è€Œå®Œæˆå¯¹æœ€ç»ˆç›®æ ‡çš„è¾“å‡ºï¼Œå¹¶ä¸”å…·æœ‰æ¯”è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚çš„ç¡®ï¼Œç¥ç»ç½‘ç»œåº”è¯¥æ˜¯è¿™æ ·åšçš„ï¼Œç„¶è€Œå®ƒå¹¶ä¸èƒ½å‘Šè¯‰æˆ‘ä»¬å®ƒç¡®ç¡®å®å®æ˜¯è¿™æ ·åšçš„ï¼Œè¿™å°±æ˜¯ç¥ç»ç½‘ç»œçš„éš¾è§£é‡Šæ€§ï¼Œä¹Ÿå°±æ˜¯å¾ˆå¤šäººä¼šå°†æ·±åº¦å­¦ä¹ è§†ä¸ºé»‘ç®±çš„åŸå› ä¹‹ä¸€ã€‚ è€Œ Hiton æå‡ºäº† Capsule å°±å…·æœ‰å¾ˆå¥½çš„å¯è§£é‡Šæ€§ï¼Œé‚£ä¹ˆå…¶ä¸­çš„ â€œæŠ›å¼ƒæ¢¯åº¦ä¸‹é™â€ åˆæ˜¯æ€ä¹ˆä¸€å›äº‹å‘¢ï¼Ÿè‹ç¥åœ¨ blog ä¸­ç»™äº†å¾ˆå¥½çš„è§£é‡Šã€‚ èƒ¶å›Šçš„è®¡ç®—åœ¨å‰é¢çš„ blog ä¸­æˆ‘ä»¬å·²ç»ç†è§£äº†ä»€ä¹ˆæ˜¯â€œèƒ¶å›Šâ€ã€‚ç¥ç»å…ƒæ˜¯æ ‡é‡ï¼Œèƒ¶å›Šå°±æ˜¯å‘é‡ï¼Hintonçš„ç†è§£æ˜¯ï¼šæ¯ä¸€ä¸ªèƒ¶å›Šè¡¨ç¤ºä¸€ä¸ªå±æ€§ï¼Œè€Œèƒ¶å›Šçš„å‘é‡åˆ™è¡¨ç¤ºè¿™ä¸ªå±æ€§çš„â€œæ ‡æ¶â€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä»¥å‰åªæ˜¯ç”¨ä¸€ä¸ªæ ‡é‡è¡¨ç¤ºæœ‰æ²¡æœ‰è¿™ä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚æœ‰æ²¡æœ‰ç¾½æ¯›ï¼‰ï¼Œç°åœ¨æˆ‘ä»¬ç”¨ä¸€ä¸ªå‘é‡æ¥è¡¨ç¤ºï¼Œä¸ä»…ä»…è¡¨ç¤ºæœ‰æ²¡æœ‰ï¼Œè¿˜è¡¨ç¤ºâ€œæœ‰ä»€ä¹ˆæ ·çš„â€ï¼ˆæ¯”å¦‚æœ‰ä»€ä¹ˆé¢œè‰²ã€ä»€ä¹ˆçº¹ç†çš„ç¾½æ¯›ï¼‰ï¼Œå¦‚æœè¿™æ ·ç†è§£ï¼Œå°±æ˜¯è¯´åœ¨å¯¹å•ä¸ªç‰¹å¾çš„è¡¨è¾¾ä¸Šæ›´ä¸°å¯Œäº†ã€‚ä¸ä»…å¦‚æ­¤ï¼Œä¸Šä¸€ç¯‡ blog ä¸­æœ‰æåˆ°çš„ CNN ä¸­çš„ä¸è¶³ï¼Œä¸»è¦åœ¨äºä¸¤ç‚¹ï¼Œä¸€æ˜¯ max pooling ä¸¢å¤±äº†éƒ¨åˆ†ä¿¡æ¯ï¼ˆè¿™åœ¨ä½å±‚æ¬¡çš„layerä¸­å¯èƒ½å½±å“ä¸å¤§ï¼Œä½†æ˜¯åœ¨é«˜å±‚æ¬¡çš„layerå°±ä¼šæœ‰æ¯”è¾ƒå¤§çš„å½±å“ï¼‰ï¼ŒäºŒæ˜¯ CNN ä¸èƒ½æå–ä½ç»´ç‰¹å¾ä¸é«˜ç»´ç‰¹å¾ä¹‹é—´åœ¨ç©ºé—´ä¸­çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚è€Œèƒ¶å›Šçš„æ–¹å‘èƒ½è¡¨ç¤ºè¿™ä¸€éƒ¨åˆ†ä¿¡æ¯ã€‚æ¯”å¦‚ä¸‹é¢è¿™å¼ å›¾å°±å¾ˆæ˜æ˜¾çš„è¡¨ç¤ºå‡ºæ¥äº†ã€‚ ä»ä½å±‚æ¬¡èƒ¶å›Šåˆ°é«˜å±‚æ¬¡èƒ¶å›Šçš„è®¡ç®—ç»†èŠ‚ï¼Œä¸»è¦åˆ†ä¸º 3 ä¸ªæ­¥éª¤ï¼š affine transform weighting and sum squash å¦‚æœè€ƒè™‘æ›´å¤šçš„èƒ¶å›Šï¼Œå¯ä»¥æŠ½è±¡åˆ°ä¸‹é¢è¿™å¼ å›¾ã€‚ æˆ‘ä»¬åªå…³æ³¨å…¶ä¸­çš„æŸä¸€éƒ¨åˆ†å°±æ˜¯ï¼š å…³äºä»ä½å±‚æ¬¡ç‰¹å¾å¦‚ä½•æ•´åˆåˆ°é«˜å±‚æ¬¡ç‰¹å¾ä»¥åŠè¿™æ ·åšçš„åŸå› æ˜¯å•¥ï¼Œè‹åŒå­¦è¿™é‡Œè¯´çš„æ˜¯ç›¸åŒé€å½»äº†ã€‚ åŠ¨æ€è·¯ç”±åœ¨å‰é¢çš„blogä¸­æˆ‘ä»¬å·®ä¸å¤šèƒ½ç†è§£ï¼šåŠ¨æ€è·¯ç”±å®é™…ä¸Šå°±æ˜¯ ä½å±‚æ¬¡çš„èƒ¶å›Šå°†éƒ¨åˆ†çš„è‡ªå·±äº¤ä»˜ç»™é«˜å±‚æ¬¡çš„èƒ¶å›Šï¼Œè€Œè¿™ä¸ªéƒ¨åˆ†çš„æƒé‡å´åˆå–å†³äº ä½å±‚æ¬¡èƒ¶å›Šå’Œé«˜å±‚æ¬¡èƒ¶å›Šçš„ç›¸å…³æ€§ã€‚ æˆ‘ä»¬åŸæœ¬ä¹Ÿæ˜¯å¯ä»¥é€šè¿‡åå‘ä¼ æ’­æ¥è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼ˆä¸æ˜¾ç¤ºçš„è®¡ç®—å‡ºç›¸å…³æ€§ï¼Œè€Œæ˜¯ç›´æ¥ç”¨ç¥ç»ç½‘ç»œæ¥ä»£æ›¿ï¼Œä¸çŸ¥æ˜¯å¦å¯ç”¨æ¢¯åº¦ä¸‹é™æ¥å¤„ç†èƒ¶å›Šï¼Ÿï¼‰ï¼Œè€Œ Hinton ä½¿ç”¨çš„åŠ¨æ€è·¯ç”±ç®—æ³•å¯è§£é‡Šæ›´å¼ºã€‚ åŠ¨æ€è·¯ç”±ç®—æ³•å°±æ˜¯æ¥è§£å†³è¿™ä¸ªæƒé‡åˆ†é…çš„é—®é¢˜ã€‚ å¯¹äºåŠ¨æ€è·¯ç”±çš„ç†è§£ï¼Œè‹åŒå­¦ç»™ä¸Šäº†ä¸¤é“å°èœã€‚æ€»çš„ç†è§£å°±æ˜¯ï¼Œé«˜å±‚èƒ¶å›Šåœ¨å¯åŠ¨é˜¶æ®µï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“å®ƒæ˜¯å¤šå°‘ï¼Œé‚£ä¹ˆå‰é¢çš„ç›¸ä¼¼åº¦ä¹Ÿå°±æ²¡æ³•è®¡ç®—ã€‚äºæ˜¯ï¼Œæˆ‘ä»¬åªèƒ½åˆå§‹åŒ–ä¸€ä¸ªå€¼ï¼Œä¹Ÿå°±æ˜¯å–ä½å±‚æ¬¡èƒ¶å›Šçš„å‡å€¼ã€‚å¦‚åŒä¸Šå›¾ä¸­ç¬¬äºŒæ­¥å°† $b_ij$ è®¾ç½®æˆ 0ï¼Œé‚£ä¹ˆä½å±‚æ¬¡èƒ¶å›Šåˆ†é…ç»™é«˜å±‚æ¬¡èƒ¶å›Šçš„æƒé‡ $c_{ij}$ å°±éƒ½æ˜¯ç›¸ç­‰çš„ã€‚ $$c_{ij}=\\dfrac{exp(b_{ij})}{\\sum_k exp(b_ik)}$$ ç„¶ååå¤è¿­ä»£ã€‚è¯´ç™½äº†ï¼Œè¾“å‡ºæ˜¯è¾“å…¥çš„èšç±»ç»“æœï¼Œè€Œèšç±»é€šå¸¸éƒ½éœ€è¦è¿­ä»£ç®—æ³•ï¼Œè¿™ä¸ªè¿­ä»£ç®—æ³•å°±ç§°ä¸ºâ€œåŠ¨æ€è·¯ç”±â€ã€‚è‡³äºè¿™ä¸ªåŠ¨æ€è·¯ç”±çš„ç»†èŠ‚ï¼Œå…¶å®æ˜¯ä¸å›ºå®šçš„ï¼Œå–å†³äºèšç±»çš„ç®—æ³•ï¼Œæ¯”å¦‚å…³äºCapsuleçš„æ–°æ–‡ç« ã€ŠMATRIX CAPSULES WITH EM ROUTINGã€‹å°±ä½¿ç”¨äº†Gaussian Mixture Modelæ¥èšç±»ã€‚ å…±äº«ç‰ˆ or å…¨è¿æ¥ç‰ˆå…¨è¿æ¥ç‰ˆåœ¨ Capsule ä¸­ï¼Œä½å±‚æ¬¡ç‰¹å¾æ˜¯é€šè¿‡æ™®é€šçš„å·ç§¯ç¥ç»ç½‘ç»œæå–ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªçŸ©é˜µå˜æ¢å¾—åˆ°çš„ã€‚å…¶ä¸­çš„ $W$ æ˜¯éœ€è¦å­¦ä¹ çš„å‚æ•°ã€‚$v_j$ æ˜¯ä½œä¸ºè¾“å…¥ $u_i$ çš„æŸç§èšç±»ä¸­å¿ƒå‡ºç°çš„ï¼Œè€Œä»ä¸åŒè§’åº¦çœ‹è¾“å…¥ï¼Œå¾—åˆ°çš„èšç±»ç»“æœæ˜¾ç„¶æ˜¯ä¸ä¸€æ ·çš„ã€‚é‚£ä¹ˆä¸ºäº†å®ç°â€œå¤šè§’åº¦çœ‹ç‰¹å¾â€ï¼Œäºæ˜¯å¯ä»¥åœ¨æ¯ä¸ªèƒ¶å›Šä¼ å…¥ä¸‹ä¸€ä¸ªèƒ¶å›Šä¹‹å‰ï¼Œéƒ½è¦å…ˆä¹˜ä¸Šä¸€ä¸ªçŸ©é˜µåšå˜æ¢. $$v_j = \\text{squash}\\sum_i\\dfrac{e^{&lt;\\hat u_{j|i}, v_j&gt;}}{\\sum_k e^{&lt;\\hat u_{k|i}, v_k&gt;}}\\hat u_{j|i}, \\hat u_{j|i}=W_{ij}u_i$$ å…±äº«ç‰ˆå…¨è¿æ¥å±‚åªèƒ½å¤„ç†å®šé•¿è¾“å…¥ï¼Œå…¨è¿æ¥ç‰ˆçš„Capsuleä¹Ÿä¸ä¾‹å¤–ã€‚è€ŒCNNå¤„ç†çš„å›¾åƒå¤§å°é€šå¸¸æ˜¯ä¸å®šçš„ï¼Œæå–çš„ç‰¹å¾æ•°ç›®å°±ä¸å®šäº†ï¼Œè¿™ç§æƒ…å½¢ä¸‹ï¼Œå…¨è¿æ¥å±‚çš„Capsuleå°±ä¸é€‚ç”¨äº†ã€‚å› ä¸ºåœ¨å‰ä¸€å›¾å°±å¯ä»¥çœ‹åˆ°ï¼Œå‚æ•°çŸ©é˜µçš„ä¸ªæ•°ç­‰äºè¾“å…¥èƒ¶å›Šæ•°ç›®ä¹˜ä»¥è¾“å‡ºèƒ¶å›Šæ•°ç›®ï¼Œæ—¢ç„¶è¾“å…¥æ•°ç›®ä¸å›ºå®šï¼Œé‚£ä¹ˆå°±ä¸èƒ½ç”¨å…¨è¿æ¥äº†ã€‚ æ‰€ä»¥è·ŸCNNçš„æƒå€¼å…±äº«ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦ä¸€ä¸ªæƒå€¼å…±äº«ç‰ˆçš„Capsuleã€‚æ‰€è°“å…±äº«ç‰ˆï¼Œæ˜¯æŒ‡å¯¹äºå›ºå®šçš„ä¸Šå±‚èƒ¶å›Šjï¼Œå®ƒä¸æ‰€æœ‰çš„åº•å±‚èƒ¶å›Šçš„è¿æ¥çš„å˜æ¢çŸ©é˜µæ˜¯å…±ç”¨çš„ï¼Œå³ $W_{ji}â‰¡W_j$. é‡‡ç”¨ Hiton è®ºæ–‡ä¸­çš„å‚æ•°æ¥è®¡ç®—å°±æ˜¯ï¼š è¾“å…¥ [batch, 6, 6, 32, 8]=[batch, 1152, 8], è¾“å…¥æœ‰ 6x6x32=1152 ä¸ª capsules. è¾“å‡º [batch, 16, 10]ï¼Œ è¾“å‡ºæœ‰ 10 ä¸ª capsules. å¯¹äºå…¨è¿æ¥ç‰ˆï¼Œæƒé‡å‚æ•°æ˜¯ $1152\\times 8\\times 16\\times N + 1152\\times N + 1152\\times N$. N è¡¨ç¤º low-level capsules çš„æ•°ç›®ã€‚ å¯¹äºå…±äº«ç‰ˆï¼Œ æƒé‡å‚æ•°æ˜¯ $8\\times 16\\times 1152 + 1152 + 1152$. åå‘ä¼ æ’­ç°åœ¨åˆæœ‰äº† $W_{ji}$ï¼Œé‚£ä¹ˆè¿™äº›å‚æ•°æ€ä¹ˆè®­ç»ƒå‘¢ï¼Ÿç­”æ¡ˆæ˜¯åå‘ä¼ æ’­ã€‚è¯»è€…ä¹Ÿè®¸æ¯”è¾ƒæ™•çš„æ˜¯ï¼šç°åœ¨æ—¢æœ‰åŠ¨æ€è·¯ç”±ï¼Œåˆæœ‰åå‘ä¼ æ’­äº†ï¼Œç©¶ç«Ÿä¸¤è€…æ€ä¹ˆé…åˆï¼Ÿå…¶å®è¿™ä¸ªçœŸçš„å°±æœ€ç®€å•ä¸è¿‡äº†ã€‚ä»å½¢å¼ä¸Šæ¥çœ‹ï¼Œå°±æ˜¯å¾€æ¨¡å‹ä¸­æ·»åŠ äº†ä¸‰å±‚ç½¢äº†ï¼Œå‰©ä¸‹çš„è¯¥åšä»€ä¹ˆè¿˜æ˜¯ä»€ä¹ˆï¼Œæœ€åæ„å»ºä¸€ä¸ªlossæ¥åå‘ä¼ æ’­ã€‚ è¿™æ ·çœ‹æ¥ï¼ŒCapsuleé‡Œè¾¹ä¸ä»…æœ‰åå‘ä¼ æ’­ï¼Œè€Œä¸”åªæœ‰åå‘ä¼ æ’­ï¼Œå› ä¸ºåŠ¨æ€è·¯ç”±å·²ç»ä½œä¸ºäº†æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œéƒ½ä¸ç®—åœ¨è¿­ä»£ç®—æ³•é‡Œè¾¹äº†ã€‚ capsules åœ¨æ–‡æœ¬åˆ†ç±»ä¸Šçš„åº”ç”¨Investigating Capsule Networks with Dynamic Routing for Text Classification Model Architecture N-gram Convolutional Layeræ™®é€šçš„å·ç§¯æ“ä½œã€‚ è¾“å…¥ï¼š[batch, L, embed_size, 1] è¾“å‡ºï¼š[batch, L-k1+1, 1, B] å…¶ä¸­ï¼š kernel: [k1, embed_size, 1, B] B è¡¨ç¤ºå·ç§¯æ ¸çš„ä¸ªæ•° k1 æ˜¯sentence é•¿åº¦ç»´åº¦ä¸Šçš„ sliding-window å°ºå¯¸ Primary Capsule Layeråˆå§‹åŒ–æˆ capsules, ä½†ä¾ç„¶åªæ˜¯ç®€å•çš„å·ç§¯æ“ä½œã€‚ è¾“å…¥ï¼š[batch, L-k1+1, 1, B] è¾“å‡ºï¼š[batch, L-k1+1, 1, C, d] å…¶ä¸­ï¼š d è¡¨ç¤º capsule çš„ç»´åº¦ å®é™…ä¸Šä¾ç„¶æ˜¯æ™®é€šçš„å·ç§¯æ“ä½œï¼Œä¸åŒçš„æ˜¯ï¼ŒåŸæœ¬æ˜¯ä» channels B åˆ° channels C.ç°åœ¨æ¯ä¸ª channels C å¯¹åº”çš„æœ‰ d ä¸ªã€‚ä¹Ÿå°±æ˜¯åˆå§‹åŒ–çš„ capsules. kernel: [1, 1, B, Cd], å®ç°æ—¶å…ˆç”Ÿæˆ Cd channels, ç„¶å split. Convolutional Capsule Layerä»ä½å±‚æ¬¡ feature åˆ°é«˜å±‚æ¬¡ feature, åœ¨ Hinton ä¸­æ˜¯capsulesç‰ˆçš„å…¨è¿æ¥ï¼Œåœ¨è¿™é‡Œæ˜¯ capsules ç‰ˆçš„å·ç§¯æ“ä½œï¼Œå…¶ä¸­æ¶‰åŠåˆ°åŠ¨æ€è·¯ç”±ç®—æ³•ã€‚ è¾“å…¥ï¼š[batch, L-k1+1, 1, C, d] è¾“å‡ºï¼š[batch, L-k1-k2+2, 1, D, d] å…¶ä¸­ï¼š è¾“å‡ºçš„ capsules ç»´åº¦ä¾æ—§æ˜¯ d ä½†æ˜¯ capsules çš„ä¸ªæ•°å‘ç”Ÿäº†å˜åŒ–ï¼Œåœ¨ Hinton è®ºæ–‡ä¸­æ˜¯é€šè¿‡å…¨è¿æ¥ç»´åº¦çš„å˜æ¢ï¼Œè¿™é‡Œæ˜¯é€šè¿‡å·ç§¯çš„æ“ä½œæ¥å®ç° capsules ä¸ªæ•°çš„å˜æ¢çš„ã€‚ ä¸ Hinton çš„è®ºæ–‡ç±»ä¼¼ï¼Œç¬¬ä¸€æ­¥æ˜¯ affine transform çŸ©é˜µå˜æ¢æ“ä½œï¼Œåœ¨è¿™ç¯‡ paper ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ–¹å¼ï¼Œå®é™…ä¸Šå°±æ˜¯è‹åŒå­¦åšå®¢ä¸­çš„å…¨è¿æ¥ç‰ˆå’Œå…±äº«ç‰ˆï¼ˆä½å±‚æ¬¡çš„ capsules æ˜¯å¦å…±äº«åŒæ ·çš„çŸ©é˜µå˜æ¢å‚æ•°ï¼‰ã€‚ shared: $W\\in R^{N\\times d\\times d}$. N æ˜¯ capsules çš„ä¸ªæ•° no-shared: $W\\in R^{H\\times N\\times d\\times d}$.H æ˜¯ä½ç»´çš„ capsules çš„ä¸ªæ•°ã€‚","link":"/2019/01/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%86%8D%E7%9C%8B-Capsules-%E4%BB%A5%E5%8F%8A-capsules-%E5%9C%A8%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8/"},{"title":"è®ºæ–‡ç¬”è®°-å¯¹è¯ç³»ç»Ÿ","text":"paperA Survey on Dialogue Systems: Recent Advances and New Frontiers, Chen et al. 2018 motivationè¿™æ˜¯ä¸€ç¯‡å…³äºå¯¹è¯ç³»ç»Ÿçš„ç»¼è¿°ã€‚ å¯¹è¯ç³»ç»Ÿä¸»è¦åˆ†ä¸ºä¸¤å¤§ç±»ï¼š ä»»åŠ¡å¯¼å‘å‹ï¼ˆtask-oriented) å¯¹è¯ç³»ç»Ÿ éä»»åŠ¡å¯¼å‘å‹ï¼ˆnon-task-orientedï¼‰å¯¹è¯ç³»ç»Ÿ åºåˆ—åˆ°åºåˆ—æ¨¡å‹ sequence-to-sequence models æ£€ç´¢å¼æ¨¡å‹ retrieval-based methods task-oriented dialogue systemé¢å‘ä»»åŠ¡çš„ç³»ç»Ÿæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å®Œæˆå®é™…å…·ä½“çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¸®åŠ©ç”¨æˆ·æ‰¾å¯»å•†å“ï¼Œé¢„è®¢é…’åº—é¤å…ç­‰ã€‚ æœ‰ä¸¤ç§æ–¹å¼ï¼š pipeline methods end-to-end methods pipeline methods å…¶æµç¨‹æ˜¯4ä¸ªæ­¥éª¤ï¼š language understanding dialogue state tracking policy learning natural language generation language understandingç¬¬ä¸€æ­¥æ˜¯ utterance ç†è§£ã€‚å°†ç»™å®šçš„ utterance æ˜ å°„æˆå¯¹åº”çš„è¯­ä¹‰æ§½ (semantic slots). Given an utterance, natural language understanding maps it into semantic slots. The slots are pre-defined according to different scenarios. slots éƒ½æ˜¯æ ¹æ®ç‰¹å®šçš„åœºæ™¯å®šä¹‰å¥½çš„ã€‚ çœ‹è¡¨æ ¼èƒ½å‘ç°ï¼ŒåŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼š intent dection: è¿™ä¸ªæ˜¯ utterance-level classificationï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ domain classification: ä¹Ÿæ˜¯åˆ†ç±»ä»»åŠ¡ slot filling: è¿™æ˜¯ word-level çš„ä»»åŠ¡ï¼Œå¯ä»¥å®šä¹‰æˆåºåˆ—æ ‡æ³¨é—®é¢˜ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ª utteranceï¼Œè¾“å‡ºæ˜¯å¯¹åº”æ¯ä¸ª word çš„ semantic label. å…³äº slot filling çš„ paper: CRF baseline DBNs: Deep belief network based semantic taggers for spoken language understanding Use of kernel deep convex networks and end-to-end learning for spoken language understanding RNN: Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding, 2013 Deep belief nets for natural language call-routing. 2011 Recurrent neural networks for language understanding, 2013 Spoken language understanding using long short-term memory neural networks. 2014 Dialogue State Trackingå¯¹è¯çš„çŠ¶æ€è·Ÿè¸ªï¼Œé¢„æµ‹æ¯ä¸€è½®å¯¹è¯ user çš„ goal. å¯¹è¯çŠ¶æ€è·Ÿè¸ªæ˜¯ç¡®ä¿å¯¹è¯ç³»ç»Ÿå¥å£®æ€§çš„æ ¸å¿ƒç»„ä»¶ã€‚å®ƒåœ¨å¯¹è¯çš„æ¯ä¸€è½®æ¬¡å¯¹ç”¨æˆ·çš„ç›®æ ‡è¿›è¡Œé¢„ä¼°ï¼Œç®¡ç†æ¯ä¸ªå›åˆçš„è¾“å…¥å’Œå¯¹è¯å†å²ï¼Œè¾“å‡ºå½“å‰å¯¹è¯çŠ¶æ€ã€‚è¿™ç§å…¸å‹çš„çŠ¶æ€ç»“æ„é€šå¸¸ç§°ä¸ºæ§½å¡«å……æˆ–è¯­ä¹‰æ¡†æ¶ã€‚ æ‰€ä»¥å¯¹åº”çš„ state æ˜¯æ ¹æ®åœºæ™¯é¢„å®šä¹‰å¥½äº†çš„å˜›ï¼Ÿæ¯”å¦‚ online shoppingï¼Œå¯¹åº”çš„ state å¯èƒ½å°±æœ‰ æ¨èï¼Œæ¯”è¾ƒï¼Œä¸‹å•ç­‰ç­‰ï¼Ÿ åŸºäºä¼ ç»Ÿæ–¹æ³•çš„æœ‰å¾ˆå¤šï¼ŒåŸºäº deep learning çš„æœ‰ï¼š [26] Deep neural network approach for the dialog state tracking challenge. 2013 [58] Multi-domain dialog state tracking using recurrent neural networks. 2015 [59] Neural belief tracker: Data-driven dialogue state tracking, 2017 Policy learningæ ¹æ®ä¸Šä¸€æ­¥å¾—åˆ°çš„ stateï¼Œæ¥åˆ¶å®šä¸‹ä¸€æ­¥çš„ action. å¾ˆç¬¦åˆå¼ºåŒ–å­¦ä¹ çš„ç†å¿µå•Šï¼Œä¸è¿‡éœ€è¦è§£å†³ çƒ­å¯åŠ¨ (warm-start) çš„é—®é¢˜ã€‚ åŸºäºè§„åˆ™çš„ç›‘ç£å­¦ä¹ ï¼ˆstate çš„çŠ¶æ€éœ€è¦è§„åˆ™æ¥å®šä¹‰ï¼‰: [111] Building task-oriented dialogue systems for online shopping, deep reinforcement learning: [14] Strategic dialogue management via deep reinforcement learning, 2015 åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•å·²ç»è¶…è¿‡ç›‘ç£å­¦ä¹ äº†ã€‚ natural language generationä¸€ä¸ªå¥½çš„ç”Ÿæˆå™¨é€šå¸¸ä¾èµ–äºå‡ ä¸ªå› ç´ :é€‚å½“æ€§ã€æµç•…æ€§ã€å¯è¯»æ€§å’Œå˜åŒ–æ€§ã€‚ä¼ ç»Ÿçš„NLGæ–¹æ³•é€šå¸¸æ˜¯æ‰§è¡Œå¥å­è®¡åˆ’ã€‚å®ƒå°†è¾“å…¥è¯­ä¹‰ç¬¦å·æ˜ å°„åˆ°ä»£è¡¨è¯è¯­çš„ä¸­ä»‹å½¢å¼ï¼Œå¦‚æ ‘çŠ¶æˆ–æ¨¡æ¿ç»“æ„ï¼Œç„¶åé€šè¿‡è¡¨é¢å®ç°å°†ä¸­é—´ç»“æ„è½¬æ¢ä¸ºæœ€ç»ˆå“åº”ã€‚æ·±åº¦å­¦ä¹ æ¯”è¾ƒæˆç†Ÿçš„æ–¹æ³•æ˜¯åŸºäºLSTMçš„encoder-decoderå½¢å¼ï¼Œå°†é—®é¢˜ä¿¡æ¯ã€è¯­ä¹‰æ§½å€¼å’Œå¯¹è¯è¡Œä¸ºç±»å‹ç»“åˆèµ·æ¥ç”Ÿæˆæ­£ç¡®çš„ç­”æ¡ˆã€‚åŒæ—¶åˆ©ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†å¯¹è§£ç å™¨å½“å‰è§£ç çŠ¶æ€çš„å…³é”®ä¿¡æ¯ï¼Œæ ¹æ®ä¸åŒçš„è¡Œä¸ºç±»å‹ç”Ÿæˆä¸åŒçš„å›å¤ã€‚ [123] Context-aware nat- ural language generation for spoken dialogue systems. Zhou et al, 2016 COLING adopted an encoder-decoder LSTM-based structure to incorporate the question information, semantic slot values, and dialogue act type to generate correct answers. It used the attention mechanism to attend to the key information conditioned on the current decoding state of the decoder. Encoding the di- alogue act type embedding, the neural network-based model is able to generate variant answers in response to different act types. end-to-end modelä¼ ç»Ÿçš„ pipeline çš„æ–¹æ³•çš„ç¼ºç‚¹ï¼š user çš„åé¦ˆå¾ˆéš¾ä¼ é€’åˆ°æ¯ä¸€ä¸ª module æ¯ä¸€ä¸ª module éƒ½æ˜¯ç›¸äº’ä¾èµ–çš„ (process interde- pendence) ä¹Ÿå°±æ˜¯åœ¨ä¸åŒçš„ domain æˆ–è€… scenarios æ—¶ï¼Œpipeline è®¾è®¡çš„å¯¹è¯ç³»ç»Ÿå¯èƒ½å°±ä¸ä½¿ç”¨çš„ï¼Œå› ä¸º slots å’Œ features éƒ½æ˜¯ task-specificedï¼Œéƒ½ä¼šç›¸åº”çš„æ”¹å˜ã€‚è€Œè¿™äº›è¿‡ç¨‹éƒ½éœ€è¦å¤§é‡çš„äººå·¥å·¥ç¨‹ã€‚ å› æ­¤æˆ‘ä»¬éœ€è¦ end-to-end modelã€‚ä¸ä¼ ç»Ÿçš„ pipeline æ¨¡å‹ä¸åŒï¼Œç«¯åˆ°ç«¯æ¨¡å‹ä½¿ç”¨ä¸€ä¸ªæ¨¡å—ï¼Œå¹¶ä¸ç»“æ„åŒ–çš„å¤–éƒ¨æ•°æ®åº“äº¤äº’ã€‚ network-based end-to-end æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ® Learning end-to-end goal-oriented dialog, Bordes et al, 2017 ICLR A network-based end-to-end trainable task-oriented di- alogue system, 2017 ACL end-to-end reinforcement learning åœ¨å¯¹è¯ç®¡ç†ä¸­ï¼Œè”åˆè®­ç»ƒ state tracking å’Œ policy learningï¼Œ ä»è€Œä½¿å¾—æ¨¡å‹é²æ£’æ€§æ›´å¼ºã€‚ Towards end-to-end learn- ing for dialog state tracking and management us- ing deep reinforcement learningï¼Œ2016 ACL task-completion neural dialogue system, å…¶ç›®æ ‡å°±æ˜¯å®Œæˆä¸€ä¸ªä»»åŠ¡ã€‚ End-to-end task- completion neural dialogue systemsï¼Œ2017 ä»¥ä»»åŠ¡ä¸ºå¯¼å‘çš„å¯¹è¯ç³»ç»Ÿé€šå¸¸è¿˜éœ€è¦æŸ¥è¯¢å¤–éƒ¨çŸ¥è¯†åº“ã€‚ä¼ ç»Ÿçš„é‡‡ç”¨çš„æ–¹æ³•å°±æ˜¯é€šè¿‡ semantic parsing å½¢æˆä¸€ä¸ª queryï¼Œç„¶åå»åŒ¹é…å¤–éƒ¨çŸ¥è¯†åº“ï¼Œé€šè¿‡æ£€ç´¢å¾—åˆ°æƒ³è¦çš„ entries. å…¶ç¼ºç‚¹æ˜¯ï¼š æ£€ç´¢çš„ç»“æœä¸åŒ…å«æœ‰å…³è¯­ä¹‰åˆ†æä¸­çš„ä¸ç¡®å®šæ€§ä¿¡æ¯ æ£€ç´¢çš„è¿‡ç¨‹æ˜¯ä¸å¯å¾®çš„ (non-differentiabl), å› æ­¤ semantic parsing å’Œ dialogue policy åªèƒ½åˆ†åˆ«è®­ç»ƒï¼Œå¯¼è‡´ online end-to-end çš„æ¨¡å‹å¾ˆéš¾éƒ¨ç½²ã€‚ è§£å†³è¿™ä¸ªé—®é¢˜çš„ paper: Key-value retrieval networks for task-oriented dialogue, 2017 augmented existing recurrent network architectures with a differentiable attention-based key-value retrieval mechanism over the entries of a knowledge base, which is inspired by key-value memory networks. Towards end-to-end reinforcement learning of dialogue agents for information ac- cess, 2017 ACL replaced symbolic queries with an induced â€œsoftâ€ posterior distribution over the knowledge base that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. 2017 ACL combined an RNNwith domain-specific knowledge encoded as software and system action templates. Non-task-oriented dialogue systeméä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¹Ÿå°±æ˜¯èŠå¤©æœºå™¨äººï¼Œ æ˜¯é€šè¿‡ç”Ÿæˆæ¨¡å‹æˆ–åŸºäºæ£€ç´¢çš„æ–¹æ³•å®ç°çš„ã€‚ ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´åˆé€‚çš„å›å¤ï¼ˆä¹Ÿå°±æ˜¯è·Ÿä¸Šä¸‹æ–‡è¯­ä¹‰æ›´æ¥è¿‘ï¼‰ï¼Œè€Œè¿™äº›å›å¤å¯èƒ½ä»æ¥æ²¡æœ‰å‡ºç°åœ¨è¯­æ–™åº“ä¸­ï¼Œè€ŒåŸºäºæ£€ç´¢çš„æ¨¡å‹åˆ™èƒ½å¾—åˆ°å…·æœ‰ä¿¡æ¯å……è£• (informative) å’Œ fluent çš„å›å¤ã€‚ Neural Generative modelsæ·±åº¦å­¦ä¹ åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„æˆåŠŸåº”ç”¨ï¼Œå³ç¥ç»æœºå™¨ç¿»è¯‘ï¼Œæ¿€å‘äº†äººä»¬å¯¹ç¥ç»ç”Ÿæˆå¯¹è¯ç ”ç©¶çš„çƒ­æƒ…ã€‚ æœ€å¼€å§‹ä¹Ÿæœ‰ä¸€ç¯‡ paperï¼Œå°†å¯¹è¯å½“æœ€æœºå™¨ç¿»è¯‘æ¥åšçš„ paper. æŠŠå¯¹è¯çœ‹ä½œæ˜¯å°† post ç¿»è¯‘æˆ responseã€‚ä½†æ˜¯åŒºåˆ«åœ¨äº response çš„èŒƒå›´å¾ˆå¹¿ï¼Œè€Œä¸” post å’Œ response å¹¶ä¸åƒç¿»è¯‘çš„ä¸¤ä¸ªå¥å­ä¹‹é—´å­˜åœ¨å¯¹é½å…³ç³»ã€‚ ç›®å‰ç¥ç»ç”Ÿæˆæ¨¡å‹çš„çƒ­é—¨ç ”ç©¶è¯¾é¢˜ï¼Œä¸»è¦æ˜¯è®¨è®ºï¼š response diversity modeling topics and personalities leveraging outside knowledge base the interactive learning evaluation Sequence-to-Sequence Models è¿™ä¸ªå°±æ˜¯åŸºæœ¬çš„ seq2seq æ¨¡å‹ã€‚å¥½å¥‡çš„æ˜¯ï¼Œå¦‚ä½•è§£å†³å¤šè½®å¯¹è¯ï¼Œå¦‚ä½•ç»“åˆ history ä¿¡æ¯ï¼Œå¦‚ä½•æ§åˆ¶å¯¹è¯çš„çŠ¶æ€ï¼Œè¿™äº›éƒ½éœ€è¦æ·±å…¥çœ‹ paper å§ã€‚ Dialogue Contextè€ƒè™‘å†å²å¯¹è¯çš„å†å²ä¿¡æ¯çš„èƒ½åŠ›æ˜¯å»ºç«‹å¯ä¿æŒå¯¹è¯æ´»è·ƒçš„å¯¹è¯ç³»ç»Ÿçš„å…³é”®ã€‚ åŸºäº RNN language model çš„å½¢å¼ï¼š A neural network approach to context-sensitive generation of conversational responses, ACL 2015 é€šè¿‡è¿ç»­çš„è¡¨ç¤ºæˆ–å•è¯å’ŒçŸ­è¯­çš„åµŒå…¥æ¥è¡¨ç¤ºæ•´ä¸ªå¯¹è¯å†å²ï¼ˆåŒ…æ‹¬å½“å‰æ¶ˆæ¯ï¼‰ï¼Œç±»ä¼¼äº RNN language model çš„ decoder è¿‡ç¨‹ï¼Œè¿™æ ·å°±èƒ½ä¿è¯å‰åç”Ÿæˆçš„ response æ˜¯æœ‰å…³ç³»çš„ï¼Œåè€…æ˜¯ä¾èµ–äºå‰è€…ã€‚[12] ä¹Ÿæ˜¯è¿™ä¹ˆå¹²çš„ã€‚ ä½¿ç”¨ hierarchical model: [68] å…ˆåˆ†åˆ«å¯¹ individual utterance è¿›è¡Œå»ºæ¨¡ï¼Œç„¶åå°†ä»–ä»¬æ•´åˆåœ¨ä¸€èµ·ã€‚Hierarchical recurrent attention network for response generation, 2017 å¼•å…¥äº† attention mechanism, ä»è€Œ focus æœ€ç›¸å…³æˆ–è€…æ˜¯æœ€é‡è¦çš„éƒ¨åˆ† word-level or utterance-level. How to make context more useful? an empirical study on context-aware neural conversational models., ACL 2017 å¯¹æœ‰æ— å±‚æ¬¡ç»“æ„çš„æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ï¼Œè¯æ˜æœ‰å±‚æ¬¡ç»“æ„çš„æ¨¡å‹æ•ˆæœæ›´å¥½ã€‚ Response Diversity A challenging problem in current sequence-to-sequence dialogue systems is that they tend to generate trivial or non-committal, universally relevant responses with little meaning, which are often involving high frequency phrases along the lines of I dont know or Im OK. åœ¨å½“å‰çš„åºåˆ—å¯¹è¯ç³»ç»Ÿä¸­ï¼Œä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æ˜¯ï¼Œå®ƒä»¬å€¾å‘äºäº§ç”Ÿæ„ä¹‰ä¸å¤§çš„æ™®é€šæˆ–ä¸é‡è¦çš„ã€æ™®é€‚çš„å›ç­”ï¼Œè€Œè¿™äº›å›ç­”å¾€å¾€æ¶‰åŠåˆ°â€œæˆ‘ä¸çŸ¥é“â€æˆ–è€…â€œæˆ‘å¾ˆå¥½â€è¿™æ ·çš„é«˜é¢‘ç‡çŸ­è¯­ã€‚ MMI and IDF æ¨¡å‹çš„è¿™ç§è¡Œä¸ºå¯ä»¥å½’å’äºæ¨¡å‹èµ‹äºˆäº† â€œsafeâ€ response æ›´é«˜çš„æ¦‚ç‡ã€‚A diversity-promoting objective function for neural con- versation models. ACL 2016 ä½¿ç”¨äº† Maximum Mutual Information ä½œä¸ºä¼˜åŒ–ç›®æ ‡,è¿™æ˜¯æœ€åˆåœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå¼•å…¥çš„ã€‚ å®ƒæµ‹é‡äº†è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ï¼Œå¹¶è€ƒè™‘äº†æ¶ˆæ¯å›å¤çš„é€†å‘ä¾èµ–æ€§ã€‚ An attentional neural conversation model with improved speci- ficit, 2016 ç»“åˆé€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰åˆ°è®­ç»ƒè¿‡ç¨‹æ¥è¯„ä»·å›å¤çš„å¤šæ ·æ€§ã€‚ï¼ˆåœ¨ä¸åŒçš„ document ä¸­å‡ºç°çš„å›å¤æ¬¡æ•°è¶Šå¤šï¼Œå…¶ç›¸åº”çš„æƒé‡è¶Šä½ï¼‰ã€‚ beam-search ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œè§£ç çš„è¿‡ç¨‹ä¹Ÿæ˜¯å›å¤å†—ä½™çš„å¦ä¸€ä¸ªç¼˜ç”±ã€‚[86][72][42] å‘ç° beam-search åœ¨ç”Ÿæˆå€™é€‰ç­”æ¡ˆæ—¶ä¸­ç¼ºä¹å¤šæ ·æ€§ã€‚[86] æå‡ºäº†ä¸€ç§è¡¡é‡ä¸åŒå›å¤ä¹‹é—´çš„ç›¸ä¼¼åº¦çš„æ–¹æ³•ï¼Œç±»ä¼¼äºæ­£åˆ™æƒ©ç½šé¡¹å§ï¼Œæ¥å¢å¼º beam-search çš„ç›®æ ‡å‡½æ•°ã€‚[72] æå‡ºäº†ä¸€ç§éšæœº beam-search çš„æ–¹æ³•ï¼Œ[42] åˆ™ä½¿ç”¨äº†ä¸€ä¸ªæƒ©ç½šé¡¹æ¥æƒ©ç½šæ¥è‡ªåŒä¸€çˆ¶èŠ‚ç‚¹ä¸­çš„å­èŠ‚ç‚¹çš„å±•å¼€ã€‚ re-ranking [38][77][72] ç»“åˆå…¨å±€ç‰¹å¾ï¼Œé‡æ–°æ‰§è¡Œ re-ranking çš„æ­¥éª¤ï¼Œä»è€Œé¿å…ç”Ÿæˆ dull or generic çš„å›å¤ã€‚ PMI [57] çŒœæµ‹é—®é¢˜ä¸ä»…ä»…åœ¨äºè§£ç å’Œ respones çš„é¢‘ç‡ï¼Œè€Œä¸”æ¶ˆæ¯æœ¬èº«ä¹Ÿç¼ºä¹è¶³å¤Ÿçš„ä¿¡æ¯ã€‚ å®ƒæå‡ºä½¿ç”¨é€ç‚¹äº’ä¿¡æ¯ï¼ˆPMIï¼‰æ¥é¢„æµ‹åè¯ä½œä¸ºå…³é”®è¯ï¼Œåæ˜ ç­”å¤çš„ä¸»è¦ä¾æ®ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªåŒ…å«ç»™å®šå…³é”®å­—çš„ç­”å¤. latent variable å¦ä¸€ç³»åˆ—å·¥ä½œç€é‡äºé€šè¿‡å¼•å…¥éšæœºéšå˜é‡æ¥äº§ç”Ÿæ›´å¤šä¸åŒçš„è¾“å‡ºã€‚ ä»–ä»¬è¡¨æ˜ï¼Œè‡ªç„¶å¯¹è¯ä¸æ˜¯ç¡®å®šæ€§çš„ â€”â€” å¯¹åŒä¸€ä¿¡æ¯çš„ç­”å¤å¯èƒ½ä¼šå› äººè€Œå¼‚ã€‚ ä½†æ˜¯ï¼Œå½“å‰å›å¤æ˜¯ä»ç¡®å®šæ€§ encoder-decoder æ¨¡å‹ä¸­é‡‡æ ·çš„ã€‚ é€šè¿‡æ•´åˆéšå˜é‡ï¼Œè¿™äº›æ¨¡å‹çš„ä¼˜ç‚¹æ˜¯ï¼Œåœ¨ç”Ÿæˆæ—¶ï¼Œä»–ä»¬å¯ä»¥é€šè¿‡é¦–å…ˆå¯¹éšå˜é‡çš„åˆ†é…è¿›è¡Œé‡‡æ ·ï¼Œç„¶åç¡®å®šæ€§åœ°è¿›è¡Œè§£ç ï¼Œä»åˆ†å¸ƒä¸­é‡‡æ ·å›å¤ã€‚ A hierarchical latent vari- able encoder-decoder model for generating dialogues, AAAI 2019 å°†å› å˜é‡å¼•å…¥åˆ° hierachical dialogue model frameworkï¼ŒThe latent variable is designed to make high-level decisions like topic or sentiment. A conditional variational framework for dialog generation. ACL 2017 conditioned the latent variable on explicit attributes to make the latent variable more interpretable. These attributes can be either manually assigned or automatically detected such topics, and personality. Topic and Personalityæ˜ç¡®å¯¹è¯çš„å†…åœ¨å±æ€§æ˜¯æé«˜å¯¹è¯å¤šæ ·æ€§å’Œä¿è¯ä¸€è‡´æ€§çš„å¦ä¸€ç§æ–¹æ³•ã€‚åœ¨ä¸åŒçš„å±æ€§ä¸­ï¼Œä¸»é¢˜å’Œä¸ªæ€§è¢«å¹¿æ³›åœ°è¿›è¡Œç ”ç©¶æ¢è®¨ã€‚ Topic aware neural response generation, AAAI 2017 æ³¨æ„åˆ°äººä»¬ç»å¸¸æŠŠä»–ä»¬çš„å¯¹è¯ä¸ä¸»é¢˜ç›¸å…³çš„æ¦‚å¿µè”ç³»èµ·æ¥ï¼Œå¹¶æ ¹æ®è¿™äº›æ¦‚å¿µåšå‡ºä»–ä»¬çš„å›å¤ã€‚ä»–ä»¬ä½¿ç”¨Twitter LDAæ¨¡å‹æ¥è·å–è¾“å…¥çš„ä¸»é¢˜ï¼Œå°†ä¸»é¢˜ä¿¡æ¯å’Œè¾“å…¥è¡¨ç¤ºè¾“å…¥åˆ°ä¸€ä¸ªè”åˆæ³¨æ„æ¨¡å—ä¸­ï¼Œå¹¶ç”Ÿæˆä¸ä¸»é¢˜ç›¸å…³çš„å“åº”ã€‚ Multiresolution recurrent neural networks: An application to dialogue response generation. AAAI 2017 å¯¹ç²—ç²’åº¦çš„ tokens sequence å’Œ dialogue generation è¿›è¡Œè”åˆå»ºæ¨¡ï¼Œç²—ç²’åº¦çš„ tokens ä¸»è¦æ˜¯ç”¨æ¥æ¢ç´¢ high-level çš„è¯­ä¹‰ä¿¡æ¯ï¼Œé€šå¸¸æ˜¯ name entity æˆ– nouns. Emotional chatting machine: Emotional conversation generation with internal and external memory å°†æƒ…æ„Ÿ embedding èå…¥åˆ°äº†å¯¹è¯ç”Ÿæˆä¸­ã€‚Affective neural response generation, 2017 é€šè¿‡ä¸‰ç§æ–¹å¼å¢å¼ºå›å¤çš„æƒ…æ„Ÿï¼š incorporating cognitive engineered affective word embeddings augmenting the loss objective with an affect-constrained objective function injecting affective dissimilarity in diverse beam-search inference procedure Assigning personality/identity to a chatting machine for coherent conversation generation è®©å¯¹è¯ä¸ªæ€§åŒ–ï¼Œå¹¶ä¸”ä¿æŒä¸€è‡´æ€§ã€‚Neural per- sonalized response generation as domain adaptation æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨å¤§è§„æ¨¡æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶åå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç”Ÿæˆä¸ªæ€§åŒ–å“åº”ã€‚ Personalizing a dialogue system with transfer reinforcement learning ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥æ¶ˆé™¤å¯¹è¯çš„å‰åä¸ä¸€è‡´æ€§ã€‚ Outside Knowledge Baseäººç±»å¯¹è¯ä¸å¯¹è¯ç³»ç»Ÿä¹‹é—´çš„ä¸€ä¸ªé‡è¦åŒºåˆ«æ˜¯å®ƒæ˜¯å¦ä¸ç°å®ç›¸ç»“åˆã€‚ç»“åˆå¤–éƒ¨çŸ¥è¯†åº“(KB)æ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥å¼¥è¡¥èƒŒæ™¯çŸ¥è¯†ä¹‹é—´çš„å·®è·ï¼Œå³å¯¹è¯ç³»ç»Ÿå’Œäººä¹‹é—´çš„å·®è·ã€‚è®°å¿†ç½‘ç»œï¼ˆMemory Networkï¼‰æ˜¯ä¸€ç§ä»¥çŸ¥è¯†åº“å¤„ç†é—®é¢˜çš„ç»å…¸æ–¹æ³•ã€‚å› æ­¤ï¼Œå®ƒéå¸¸ç›´æ¥çš„åˆ«ç”¨äºåœ¨å¯¹è¯ç”Ÿæˆä¸­ã€‚å®é™…ç ”ç©¶è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å‚è€ƒçŸ¥è¯†åº“ä¸­çš„äº‹å®æ¥ç”Ÿæˆå¯¹é—®é¢˜çš„è‡ªç„¶å’Œæ­£ç¡®ç­”æ¡ˆã€‚ A knowledge-grounded neural conversation model, 2017 A neural network approach for knowledge-driven response generation. COLING 2016 Neural generative question answering,IJCAI 2016 Interactive Dialogue learningé€šè¿‡äº¤äº’æ¥å­¦ä¹ æ˜¯å¯¹è¯ç³»ç»Ÿçš„æœ€ç»ˆç›®æ ‡ä¹‹ä¸€ã€‚Deep reinforcement learning for dialogue generation, ACL 2016 åˆ©ç”¨ä¸¤ä¸ªè™šæ‹Ÿæ™ºèƒ½ä½“æ¨¡æ‹Ÿå¯¹è¯ã€‚å®ƒä»¬å®šä¹‰äº†å¯¹æè¿°ä¸€ä¸ªè¾ƒå¥½çš„å¯¹è¯çš„æ±‡æŠ¥çš„ä¸€ä¸ªç®€å•çš„å¯å‘å¼çš„ä¼°è®¡ï¼šå¥½çš„å¯¹è¯æ˜¯æœ‰å‰ç»æ€§[1]æˆ–è€…äº¤äº’å¼çš„ï¼ˆå½“å‰è½®ä¸ºä¸‹ä¸€è½®å¯¹è¯é“ºå«ï¼‰ï¼Œæ˜¯ä¿¡æ¯ä¸°å¯Œçš„å’Œè¿è´¯çš„ã€‚ä¸€ä¸ªRNNçš„ç¼–ç å™¨-è§£ç å™¨æ‰€æœ‰å‚æ•°å®šä¹‰äº†ä¸€ä¸ªåœ¨æ— ç©·å¤§çš„åŠ¨ä½œç©ºé—´ä¸Šä»æ‰€æœ‰å¯èƒ½çš„è¯è¯­ä¸­è¿›è¡Œé€‰æ‹©çš„ç­–ç•¥ã€‚æ™ºèƒ½ä½“æ˜¯é€šè¿‡ç­–ç•¥æ¢¯åº¦æ–¹æ³• Simple statistical gradient-following al- gorithms for connectionist reinforcement learning, 1992 æ¥ä¼˜åŒ–ç”±å¼€å‘è€…å®šä¹‰çš„é•¿æœŸå¥–åŠ±ï¼Œè€Œä¸æ˜¯é€šè¿‡æ ‡å‡†seq2seqçš„MLEç›®æ ‡å‡½æ•°æ¥å­¦ä¹ ç­–ç•¥ã€‚[32]è¿›ä¸€æ­¥è¯•å›¾æé«˜æœºå™¨äººä»äº¤äº’ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ–‡æœ¬å’Œæ•°å­—åé¦ˆä½¿ç”¨ç­–ç•¥å­¦ä¹ å’Œå‰å‘é¢„æµ‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€šè¿‡ï¼ˆåŠï¼‰åœ¨çº¿æ–¹å¼ä¸äººè¿›è¡Œäº¤äº’æ¥æé«˜è‡ªèº«æ€§èƒ½ã€‚ ç”±äºå¤§å¤šæ•°äººç±»åœ¨å¯¹ç­”æ¡ˆå¹¶ä¸è‡ªä¿¡æ—¶é€šå¸¸ä¼šè¦æ±‚æä¾›ä¸€äº›æ¾„æ¸…æˆ–è€…æç¤ºï¼Œæ‰€æœ‰æœºå™¨äººæ‹¥æœ‰è¿™ç§èƒ½åŠ›ä¹Ÿæ˜¯ç›¸å½“è‡ªç„¶çš„ã€‚Learning through dialogue interactions by asking questions. 2017 å®šä¹‰äº†æœºå™¨äººåœ¨å›ç­”é—®é¢˜æ—¶é‡åˆ°å›°éš¾æ—¶çš„ä¸‰ç§æƒ…å†µã€‚ä¸ä¸é‡‡ç”¨æé—®çš„å®éªŒç»“æœç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸€äº›æƒ…å†µä¸‹æœ‰äº†å¾ˆå¤§çš„æ”¹è¿›ã€‚Deal or no deal? end-to-end learning of negotiation dialogues, ACL 2017 åœ¨è°ˆåˆ¤ä»»åŠ¡ä¸­è¿›è¡Œäº†æ¢ç´¢ã€‚ç”±äºä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹æ¨¡æ‹Ÿäººç±»çš„å¯¹è¯æ²¡æœ‰ä¼˜åŒ–å…·ä½“çš„ç›®æ ‡ï¼Œè¿™é¡¹å·¥ä½œé‡‡å–äº†é¢å‘ç›®æ ‡çš„è®­ç»ƒå’Œè§£ç æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ä¸€ä¸ªæœ‰ä»·å€¼çš„è§†è§’ã€‚ Evaluationè¯„ä»·ç”Ÿæˆå›å¤çš„è´¨é‡æ˜¯å¯¹è¯ç³»ç»Ÿçš„ä¸€ä¸ªé‡è¦æ–¹é¢ã€‚ä»»åŠ¡å¯¼å‘å‹çš„å¯¹è¯ç³»ç»Ÿå¯ä»¥åŸºäºäººå·¥ç”Ÿæˆçš„ç›‘ç£ä¿¡å·è¿›è¡Œè¯„ä¼°ï¼Œä¾‹å¦‚ä»»åŠ¡å®Œæˆæµ‹è¯•æˆ–ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ç­‰ï¼Œç„¶è€Œï¼Œç”±äºé«˜å›å¤çš„å¤šæ ·æ€§ï¼Œè‡ªåŠ¨è¯„ä¼°éä»»åŠ¡å¯¼å‘çš„å¯¹è¯ç³»ç»Ÿæ‰€äº§ç”Ÿçš„å“åº”çš„è´¨é‡ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ç›®å‰çš„æ–¹æ³•æœ‰ä»¥ä¸‹å‡ ç§ï¼š BLEU, METEOR, and ROUGE å€¼ï¼Œä¹Ÿå°±æ˜¯ç›´æ¥è®¡ç®— word overlapã€ground truthå’Œä½ ç”Ÿæˆçš„å›å¤ã€‚ç”±äºä¸€å¥è¯å¯èƒ½å­˜åœ¨å¤šç§å›å¤ï¼Œå› æ­¤ä»æŸäº›æ–¹é¢æ¥çœ‹ï¼ŒBLEU å¯èƒ½ä¸å¤ªé€‚ç”¨äºå¯¹è¯è¯„æµ‹ã€‚ è®¡ç®— embeddingçš„è·ç¦»ï¼Œè¿™ç±»æ–¹æ³•åˆ†ä¸‰ç§æƒ…å†µï¼šç›´æ¥ç›¸åŠ æ±‚å¹³å‡ã€å…ˆå–ç»å¯¹å€¼å†æ±‚å¹³å‡å’Œè´ªå©ªåŒ¹é…ã€‚ è¿›è¡Œå›¾çµæµ‹è¯•ï¼Œç”¨ retrieval çš„ discriminator æ¥è¯„ä»·å›å¤ç”Ÿæˆã€‚ Retrieval-based MethodsåŸºäºæ£€ç´¢çš„æ–¹æ³•ä»å€™é€‰å›å¤ä¸­é€‰æ‹©å›å¤ã€‚æ£€ç´¢æ–¹æ³•çš„å…³é”®æ˜¯æ¶ˆæ¯-å›å¤åŒ¹é…ï¼ŒåŒ¹é…ç®—æ³•å¿…é¡»å…‹æœæ¶ˆæ¯å’Œå›å¤ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚ single-turn response match$$match(x,y)=x^TAy$$ Convolutional neu- ral network architectures for matching natural lan- guage sentences, 2014 åˆ©ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œä½“ç³»ç»“æ„æ”¹è¿›æ¨¡å‹ï¼Œå­¦ä¹ æ¶ˆæ¯å’Œå“åº”çš„è¡¨ç¤ºï¼Œæˆ–ç›´æ¥å­¦ä¹ ä¸¤ä¸ªå¥å­çš„ç›¸äº’ä½œç”¨è¡¨ç¤ºï¼Œç„¶åç”¨å¤šå±‚æ„ŸçŸ¥å™¨æ¥è®¡ç®—åŒ¹é…çš„åˆ†æ•°ã€‚ multi-turn response The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems, ACL 2015 encoded the context (a concatenation of all previous utterances and current message) and candidate response into a context vector and a response vector through a RNN/LSTM based structure, respectively, and then computed the matching degree score based on those two vectors. Learning to respond with deep neural networks for retrieval-based human- computer conversation system, ACM 2016 selected the previous utterances in different strategies and combined them with current messages to form a reformulated context. Multi-view response selection for human-computer conversation. ACL 2016 performed context-response matching on not only the general word level context vector but also the utterance level context vector. Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots. ACL 2017 further improved the leveraging of ut- terances relationship and contextual information by match- ing a response with each utterance in the context on multi- ple levels of granularity with a convolutional neural network, and then accumulated the vectors in a chronological order through a recurrent neural network to model relationships among utterances Hybrid Methodså°†ç”Ÿæˆå’Œæ£€ç´¢æ–¹æ³•ç»“åˆèµ·æ¥èƒ½å¯¹ç³»ç»Ÿæ€§èƒ½èµ·åˆ°æ˜¾è‘—çš„æå‡ä½œç”¨ã€‚åŸºäºæ£€ç´¢çš„ç³»ç»Ÿé€šå¸¸ç»™å‡ºç²¾ç¡®ä½†æ˜¯è¾ƒä¸ºç”Ÿç¡¬çš„ç­”æ¡ˆï¼Œè€ŒåŸºäºç”Ÿæˆçš„ç³»ç»Ÿåˆ™å€¾å‘äºç»™å‡ºæµç•…ä½†å´æ˜¯æ¯«æ— æ„ä¹‰çš„å›ç­”ã€‚åœ¨é›†æˆæ¨¡å‹ä¸­ï¼Œè¢«æŠ½å–çš„å€™é€‰å¯¹è±¡å’ŒåŸå§‹æ¶ˆæ¯ä¸€èµ·è¢«è¾“å…¥åˆ°åŸºäºRNNçš„å›å¤ç”Ÿæˆå™¨ä¸­ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè¿™åœ¨æ€§èƒ½ä¸Šå…·å¤‡å¾ˆå¤§çš„ä¼˜åŠ¿ã€‚ Two are better than one: An ensemble of retrieval- and generation-based dialog systems, 2016 Alime chat: A sequence to sequence and rerank based chatbot engine. ACL 2017 A deep reinforcement learning chatbot, 2017 å±•æœ›ç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸ä»…åœ¨éé¢å‘ä»»åŠ¡çš„èŠå¤©å¯¹è¯ç³»ç»Ÿä¸­æµè¡Œï¼Œè€Œä¸”åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ç³»ç»Ÿä¸­é€æ­¥æµè¡Œèµ·æ¥ã€‚æ·±åº¦å­¦ä¹ èƒ½å¤Ÿåˆ©ç”¨å¤§é‡çš„æ•°æ®ï¼Œä»è€Œæ¨¡ç³Šäº†ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿå’Œéä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¹‹é—´çš„ç•Œé™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç›®å‰çš„ç«¯åˆ°ç«¯æ¨¡å‹ä»ç„¶è¿œéå®Œç¾ã€‚å°½ç®¡å–å¾—äº†ä¸Šè¿°æˆå°±ï¼Œä½†è¿™äº›é—®é¢˜ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›å¯èƒ½çš„ç ”ç©¶æ–¹å‘ã€‚ Swift Warm-Upï¼Œåœ¨ä¸€äº›æ–°çš„é¢†åŸŸï¼Œç‰¹å®šé¢†åŸŸå¯¹è¯æ•°æ®çš„æ”¶é›†å’Œå¯¹è¯ç³»ç»Ÿçš„æ„å»ºæ˜¯æ¯”è¾ƒå›°éš¾çš„ã€‚æœªæ¥çš„è¶‹åŠ¿æ˜¯å¯¹è¯æ¨¡å‹æœ‰èƒ½åŠ›ä»ä¸äººçš„äº¤äº’ä¸­ä¸»åŠ¨å»å­¦ä¹ ã€‚ Deep Understanding. æ·±åº¦ç†è§£ã€‚ç°é˜¶æ®µåŸºäºç¥ç»ç½‘ç»œçš„å¯¹è¯ç³»ç»Ÿæå¤§åœ°ä¾èµ–äºå¤§é‡æ ‡æ³¨å¥½çš„æ•°æ®ï¼Œç»“æ„åŒ–çš„çŸ¥è¯†åº“ä»¥åŠå¯¹è¯è¯­æ–™æ•°æ®ã€‚åœ¨æŸç§æ„ä¹‰ä¸Šäº§ç”Ÿçš„å›å¤ä»ç„¶ç¼ºä¹å¤šæ ·æ€§ï¼Œæœ‰æ—¶å¹¶æ²¡æœ‰å¤ªå¤šçš„æ„ä¹‰ï¼Œå› æ­¤å¯¹è¯ç³»ç»Ÿå¿…é¡»èƒ½å¤Ÿæ›´åŠ æœ‰æ•ˆåœ°æ·±åº¦ç†è§£è¯­è¨€å’ŒçœŸå®ä¸–ç•Œã€‚ Privacy Protection. ç›®å‰å¹¿æ³›åº”ç”¨çš„å¯¹è¯ç³»ç»ŸæœåŠ¡äºè¶Šæ¥è¶Šå¤šçš„äººã€‚å¾ˆæœ‰å¿…è¦æ³¨æ„åˆ°çš„äº‹å®æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªå¯¹è¯åŠ©æ‰‹ã€‚é€šè¿‡äº’åŠ¨ã€ç†è§£å’Œæ¨ç†çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¯¹è¯åŠ©æ‰‹å¯ä»¥æ— æ„ä¸­éšè”½åœ°å­˜å‚¨ä¸€äº›è¾ƒä¸ºæ•æ„Ÿçš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œåœ¨æ„å»ºæ›´å¥½çš„å¯¹è¯æœºåˆ¶æ—¶ï¼Œä¿æŠ¤ç”¨æˆ·çš„éšç§æ˜¯éå¸¸é‡è¦çš„ã€‚","link":"/2019/03/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"},{"title":"è®ºæ–‡ç¬”è®°-æ— ç›‘ç£æœºå™¨ç¿»è¯‘","text":"Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translationç‹å¨å»‰è€å¸ˆç»„çš„ä¸€ç¯‡æ–‡ç« ï¼Œå¤§è‡´çœ‹äº†ä¸‹è·Ÿæœ€è¿‘è‡ªå·±åšçš„ç ”ç©¶ç›¸å…³æ€§æŒºå¤§çš„ã€‚æ–‡ä¸­ä¹Ÿç®€å•çš„ä»‹ç»äº†æ— ç›‘ç£æœºå™¨ç¿»è¯‘çš„ä¸€äº›æ–¹æ³•ï¼Œæ‰€ä»¥å€Ÿè¿™ä¸ªæœºä¼šæŠŠæ— ç›‘ç£æœºå™¨ç¿»è¯‘ä¹Ÿå¥½å¥½äº†è§£ä¸‹ã€‚è®°å¾—åœ¨ä¸‰æ˜Ÿç ”ç©¶é™¢å®ä¹ æ—¶ï¼Œæœ‰ä¸ªä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€çš„å¸ˆå§ï¼ˆæ®è¯´æ˜¯å®—æˆåº†è€å¸ˆçš„å­¦ç”Ÿï¼‰è¯´è¿‡ä¸€å¥è¯ï¼Œ2018å¹´æ˜¯æ— ç›‘ç£æœºå™¨ç¿»è¯‘å…ƒå¹´ã€‚ä½†å½“æ—¶æˆ‘åœ¨æQAï¼Œå°±æ²¡æ€ä¹ˆæ·±å…¥ç ”ç©¶ã€‚æ„Ÿè§‰å¾ˆå¤šNLPå…¶ä»–æ–¹å‘çš„åšæ³•éƒ½æ˜¯æºäº NMTï¼Œæ‰€ä»¥è¿˜æ˜¯å¾ˆæœ‰å¿…è¦çœ‹ä¸€ä¸‹çš„ã€‚ MotivationBack-translation å¾—åˆ°çš„ä¼ªå¹³è¡Œè¯­æ–™ï¼Œæ˜¯åŸºäº pure target sentence å¾—åˆ° pesudo source sentenceï¼Œç„¶åæŠŠ prue target sentence ä½œä¸º label è¿›è¡Œç›‘ç£å­¦ä¹ (ä¿è¯target ç«¯æ˜¯pure sentenceï¼Œsourceç«¯çš„sentenceå¯ä»¥ç¨å¾® noisy)ã€‚è¿™å®è´¨ä¸Šå°±æ˜¯ä¸€ä¸ª reconstruction loss. å…¶ç¼ºç‚¹åœ¨äº pesudo source sentence è´¨é‡æ— æ³•ä¿è¯ï¼Œä¼šå¯¼è‡´è¯¯å·®ç´¯ç§¯ï¼ˆpesudo source sentence å¹¶æ²¡æœ‰å¾—åˆ°æ›´æ–°ï¼Œæ‰€ä»¥å¹¶æ²¡æœ‰çº æ­£å­˜åœ¨çš„é”™è¯¯ï¼‰ã€‚ åŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œextract-edit. related workå•è¯­è¯­æ–™çš„é€‰æ‹©neural-based methods aim to select potential parallel sentences from monolingual corpora in the same domain. However, these neural models need to be trained on a large parallel dataset first, which is not applicable to language pairs with limited supervision. é€šè¿‡å¹³è¡Œè¯­æ–™è®­ç»ƒç¿»è¯‘æ¨¡å‹ï¼Œè¿›è€Œä»å•è¯­ä¸­é€‰æ‹© domain related sentences. è¿™å¹¶ä¸æ˜¯å®Œå…¨çš„æ— ç›‘ç£ï¼Œè¿˜æ˜¯éœ€è¦æœ‰é™çš„å¹³è¡Œè¯­æ–™å¾—åˆ° NMT æ¨¡å‹ä¹‹åï¼Œå»é€‰æ‹©åˆé€‚çš„å•è¯­ã€‚ Parallel sentence extraction from comparable corpora with neural network features, LERC 2016 Bilingual word embeddings with bucketed cnn for parallel sentence extraction, ACL 2017 Extracting parallel sentences with bidirectional recurrent neural networks to improve machine translation, COLING 2018 å®Œå…¨çš„æ— ç›‘ç£æœºå™¨ç¿»è¯‘The main technical protocol of these approaches can be summarized as three steps: Initialization Language Modeling Back-Translation InitializationGiven the ill-posed nature of the unsupervised NMT task, a suitable initialization method can help model the natural priors over the mapping of two language spaces we expect to reach. åˆå§‹åŒ–çš„ç›®çš„åŸºäºè‡ªç„¶è¯­è¨€çš„ä¸€äº›å…ˆéªŒçŸ¥è¯†æ¥å¯¹ä¸¤ç§è¯­è¨€çš„æ˜ å°„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ there two main initiazation methods: bilingual dictionary inference åŸºäºåŒè¯­è¯å…¸çš„æ¨ç† Word translation without parallel data. Conneau, et al. ICLR 2018 Unsupervised neural machine translation, ICLR 2018 Unsupervised machine translation using monolingual corpora only, ICLR 2018a BPE Phrase-based &amp; neural unsupervised machine translation. emnlp Lample et al. 2018b æœ¬æ–‡ä½œè€…é‡‡ç”¨çš„æ˜¯ Conneau, et al. ä¸­çš„æ–¹å¼ï¼Œå¹¶ä¸”ç±»ä¼¼äº Lample 2018b ä¸­çš„æ–¹å¼ä¸¤ç§è¯­è¨€å…±äº« bpe(éœ€è¦åœ¨çœ‹ä¸‹ç›¸å…³è®ºæ–‡). è¿™é‡Œå®é™…ä¸Šå°±æ˜¯è®­ç»ƒå¾—åˆ°ä¸¤ç§è¯­è¨€çš„ word embeddingï¼Œå¹¶ä¸æ˜¯ word2vec é‚£ç§å¯¹å•ç§è¯­è¨€çš„æ— ç›‘ç£ï¼Œè€Œæ˜¯è®­ç»ƒå¾—åˆ°ä¸¤ç§è¯­è¨€çš„ share embedding. language modelingTrain language models on both source and target languages. These models express a data-driven prior about the composition of sentences in each language. åœ¨åˆå§‹åŒ–ä¹‹åï¼Œåœ¨ share embedding çš„åŸºç¡€ä¸Šåˆ†åˆ«å¯¹ source å’Œ target çš„è¯­è¨€è¿›è¡Œå»ºæ¨¡ã€‚ In NMT, language modeling is accomplished via denosing autoencoding, by minimizing: æœ¬æ–‡ä½œè€…é‡‡ç”¨çš„ Lample 2018a çš„æ–¹å¼ã€‚å…±äº« encoder å’Œ decoder çš„å‚æ•°ï¼Ÿï¼Ÿï¼Ÿ Back-Translation Dual learning for machine translation, NIPS 2016 Improving neural machine translation models with monolingual data. ACL 2016 Extract-Edit Extract: å…ˆæ ¹æ®å‰ä¸¤æ­¥å¾—åˆ°çš„ sentence è¡¨ç¤ºï¼Œä» target language space ä¸­é€‰æ‹©ä¸ source sentence æœ€æ¥è¿‘çš„ sentenceï¼ˆä¾æ®ç›¸ä¼¼åº¦ï¼Ÿï¼‰. Edit: ç„¶åå¯¹é€‰æ‹©çš„ sentence è¿›è¡Œ edit. ä½œè€…è¿˜æå‡ºäº†ä¸€ä¸ª comparative translation lossã€‚ Extractå› ä¸ºåœ¨ language model é˜¶æ®µä½œè€…å·²ç»å…±äº«äº† encoder å’Œ decoderï¼Œæ‰€ä»¥åœ¨è¿™ä¸ªåœºæ™¯ä¸‹å¯¹äº two language çš„è¡¨ç¤ºï¼Œéƒ½å¯ä»¥ç”¨ encoder å¾—åˆ°ã€‚ åœ¨ target language space ä¸­é€‰æ‹©å‡ºä¸ source sentence æœ€æ¥è¿‘çš„ top-k extracted sentences. ä¸ºä»€ä¹ˆæ˜¯ top-k è€Œä¸æ˜¯ top-1 å‘¢ï¼Œç¡®ä¿å¬å›ç‡ï¼Œå¹¶è·å¾—æ›´å¤šæ›´ç›¸å…³çš„ samples. Editç®€å•ç‚¹å°±æ˜¯ max-pooling + decode employ a maxpooling layer to reserve the more significant features between the source sentence embedding $e_s$ and the extracted sentence embedding $e_t$ ($t\\in M$), and then decode it into a new sentence $tâ€™$. å…·ä½“æ˜¯æ€ä¹ˆæ“ä½œçš„å‘¢ï¼Œè¿™ä¼¼ä¹éœ€è¦çœ‹ä»£ç ã€‚ $e_s$: [es_length, encoder_size] $e_t$: [et_length, encoder_size] è¿™æ€ä¹ˆ max-pooling å‘¢ï¼ˆå¥å­é•¿åº¦éƒ½å¯èƒ½ä¸ä¸€æ ·ï¼‰ï¼Œç„¶å decode å¾—åˆ°æ–°çš„ sentence å§ã€‚ã€‚ Evaluateè™½ç„¶ Mâ€™ ä¸­å¯èƒ½å­˜åœ¨æ½œåœ¨çš„ parallel sentence å¯¹åº” source sentence s. ä½†æ˜¯ä¾ç„¶ä¸èƒ½ç”¨ (s, tâ€™) ä½œä¸º ground-truth stence pairs æ¥è®­ç»ƒ NMT æ¨¡å‹ã€‚å› ä¸º NMT æ¨¡å‹å¯¹å™ªå£°éå¸¸æ•æ„Ÿã€‚ ä½œè€…æå‡ºäº†ä¸€ä¸ª evaluation network R, å®é™…ä¸Šå°±æ˜¯å¤šå±‚æ„ŸçŸ¥æœºï¼Œä¹Ÿè®¸æ˜¯ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œå§ï¼Œå…·ä½“æ²¡è¯´ã€‚two labguage å…±äº« R. $$r_s=f(W_2f(W_1e_s+b_1)+b_2)$$ $$r_t=f(W_2f(W_1e_tâ€™+b_1)+b_2)$$ å‡è®¾æ˜¯è¿™æ ·ï¼Œä¹Ÿå°±æ˜¯å°† tâ€™ è½¬æ¢æˆ t* äº†ã€‚ ç†è§£é”™äº† å…¶ç›®çš„æ˜¯å°† s å’Œ tâ€™ æ˜ å°„åˆ°åŒä¸€å‘é‡ç©ºé—´ï¼Œç„¶åè®¡ç®—ä¸¤è€…çš„ç›¸ä¼¼åº¦ï¼š æ¥ä¸‹æ¥å°† $\\alpha$ è½¬æ¢æˆæ¦‚ç‡åˆ†å¸ƒã€‚ ä¹Ÿå°±æ˜¯è®¡ç®— top-k ä¸ª extracted-edited å¾—åˆ°çš„ target sentences t* ä¸ source sentence s ç›¸ä¼¼çš„æ¦‚ç‡ï¼Œå¹¶ä¸”è¿™äº›æ¦‚ç‡ç›¸åŠ ä¸º 1. å…¶ä¸­ $\\lambda$ å¯ä»¥çœ‹ä½œæ˜¯ inverse temperatureï¼Œ $\\lambda$ è¶Šå°ï¼Œè¡¨ç¤ºæ‰€æœ‰ t* å¹³ç­‰çœ‹å¾…ï¼Œè¶Šå¤§ï¼Œè¡¨ç¤ºæ›´çœ‹é‡ $\\alpha$ æœ€å¤§çš„é‚£ä¸€å¥ã€‚æ˜¾ç„¶å‰é¢çš„ $\\alpha$ æ˜¯é€šè¿‡ cosine è®¡ç®—çš„ï¼Œä¹Ÿå°±æ˜¯æ›´çœ‹é‡ k ä¸ª t* ä¸­ä¸ s è·ç¦»æœ€è¿‘çš„é‚£ä¸ª sentence. learningComparative Translation cosine ç›¸ä¼¼åº¦è¶Šå¤§è¶Šæ¥è¿‘ï¼Œæ‰€ä»¥ -logP è¶Šå°è¶Šå¥½ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°çš„å‚æ•° $\\theta_{enc}, \\theta_R$ Basically, the translation model is trying to minimize the relative distance of the translated sentence t* to the source sentence s compared to the top-k extracted-and-edited sentences in the target language space. Intuitively, we view the top-k extracted-and-edited sentences as the anchor points to locate a probable region in the target language space, and iteratively improve the source-to-target mapping via the comparative learning scheme. Adversarial Objective we can view our translation system as a â€œgeneratorâ€ that learns to generate a good translation with a higher similarity score than the extracted-and-edited sentences, and the evaluation network R as a â€œdiscriminatorâ€ that learns to rank the extracted- and-edited sentences (real sentences in the target language space) higher than the translated sentences. å€ŸåŠ©äºå¯¹æŠ—å­¦ä¹ çš„æ€æƒ³ï¼Œå¯ä»¥æŠŠ translation system çœ‹ä½œæ˜¯ ç”Ÿæˆå™¨ generatorï¼Œ ç”¨æ¥å­¦ä¹ å¾—åˆ° translated target sentenceï¼Œä½¿å¾—å…¶ä¼˜äº extracted-and-edited sentences. æŠŠ evalution newtork R çœ‹ä½œæ˜¯åˆ¤åˆ«å™¨ï¼Œå…¶ç›®çš„å°±æ˜¯åˆ¤åˆ« extracted-and-edited sentences ä¼˜äº translated target sentences. å› æ­¤å¯¹äº evaluation network Rï¼Œæœ‰ final adversarial objective Model selectionæ— ç›‘ç£å­¦ä¹ å› ä¸ºæ²¡æœ‰å¹³è¡Œè¯­æ–™ï¼Œæ‰€ä»¥éœ€è¦ä¸€ä¸ªæŒ‡æ ‡æ¥è¡¨ç¤ºæ¨¡å‹çš„å¥½åï¼Œä¹Ÿå°±æ˜¯ç¿»è¯‘è´¨é‡ã€‚ Basically, we choose the hyper-parameters with the maximum expectation of the ranking scores of all translated sentences. Implementation detailsInitializationcross-lingual BPE embedding, set BPE number 60000. ç„¶åç”¨ Fasttext è®­ç»ƒå¾—åˆ° embeddingï¼Œ 512 dimension. å…¶ä¸­ Fasettext è®¾ç½® window size 5 and 10 negative samples Model structureall encoder parameters are shared across two languages. Similarly, we share all decoder parameters across two languages. The Î» for calculating ranking scores is 0.5. As for the evaluation network R, we use a multilayer perceptron with two hidden layers of size 512.","link":"/2019/04/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E6%97%A0%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91-Extract-and-Eit/"},{"title":"è¿ç§»å­¦ä¹ ç³»åˆ—-0-NLP classification with transfer learning and weak supervision","text":"paper: Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision A Brief Introduction to Weakly Supervised Learning motivationç°åœ¨çš„ state-of-the-art æŠ€æœ¯éƒ½ä¸¥é‡ä¾èµ–äºå¤§é‡çš„æ•°æ®ï¼Œå¯ä»¥è¯´æ•°æ®æ˜¯ NLP åº”ç”¨çš„ç“¶é¢ˆ(bottleneck)ã€‚æ¯”å¦‚ï¼Œæ ‡æ³¨åŒ»å­¦é¢†åŸŸçš„ç”µå­å¥åº·è®°å½•éœ€è¦å¤§é‡çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ã€‚ éšç€ transfer learning, multi-task learning ä»¥åŠ weak supervision çš„å‘å±•ï¼ŒNLP å¯ä»¥å°è¯•ç€å»è§£å†³è¿™äº›é—®é¢˜ã€‚ è¿™é‡Œå°†ä»‹ç»å¦‚ä½•åœ¨æ²¡æœ‰å…¬å¼€æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å°‘é‡çš„æ•°æ®ï¼Œæ¥æ„å»ºä¸€ä¸ª dectect anti-smitic tweets åˆ†ç±»å™¨ã€‚åˆ†ä¸ºä»¥ä¸‹ 3 ä¸ªæ­¥éª¤ï¼š Collect a small number of labeled examples (~600) Use weak supervision to build a training set from many unlabeled examples using weak supervision Use a large pre-trained language model for transfer learning Weak Supervisionä½•ä¸ºå¼±ç›‘ç£å­¦ä¹ ï¼Ÿ paper: A Brief Introduction to Weakly Supervised Learning ç¿»è¯‘ç‰ˆ å¼±ç›‘ç£å­¦ä¹ æ˜¯ä¸€ä¸ªæ€»æ‹¬æ€§çš„æœ¯è¯­ï¼Œå®ƒæ¶µç›–äº†è¯•å›¾é€šè¿‡è¾ƒå¼±çš„ç›‘ç£æ¥æ„å»ºé¢„æµ‹æ¨¡å‹çš„å„ç§ç ”ç©¶ã€‚å¼±ç›‘ç£é€šå¸¸åˆ†ä¸ºä¸‰ç§ç±»å‹ã€‚ ä¸å®Œå…¨ç›‘ç£(Incomplete Supervision): åªæœ‰è®­ç»ƒæ•°æ®é›†çš„ä¸€ä¸ªï¼ˆé€šå¸¸å¾ˆå°çš„ï¼‰å­é›†æœ‰æ ‡ç­¾ï¼Œå…¶å®ƒæ•°æ®åˆ™æ²¡æœ‰æ ‡ç­¾ã€‚ ä¸ç¡®åˆ‡ç›‘ç£(inexact supervision): åªæœ‰ç²—ç²’åº¦çš„æ ‡ç­¾ã€‚ä»¥å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ã€‚æˆ‘ä»¬å¸Œæœ›å›¾ç‰‡ä¸­çš„æ¯ä¸ªç‰©ä½“éƒ½è¢«æ ‡æ³¨ï¼›ç„¶è€Œæˆ‘ä»¬åªæœ‰å›¾ç‰‡çº§çš„æ ‡ç­¾è€Œæ²¡æœ‰ç‰©ä½“çº§çš„æ ‡ç­¾ã€‚ ä¸å‡†ç¡®ç›‘ç£(inaccurate supervision)ï¼Œå³ç»™å®šçš„æ ‡ç­¾å¹¶ä¸æ€»æ˜¯çœŸå€¼ã€‚ å›¾1:ä¸‰ç§å¼±ç›‘ç£å­¦ä¹ çš„ç¤ºæ„å›¾ã€‚é•¿æ–¹å½¢è¡¨ç¤ºç‰¹å¾å‘é‡ï¼›çº¢è‰²æˆ–è“è‰²è¡¨ç¤ºæ ‡ç­¾ï¼›â€œï¼Ÿâ€è¡¨ç¤ºæ ‡æ³¨å¯èƒ½æ˜¯ä¸å‡†ç¡®çš„ã€‚ä¸­é—´çš„å­å›¾è¡¨ç¤ºäº†å‡ ç§å¼±ç›‘ç£çš„æ··åˆæƒ…å½¢ ä¸å®Œå…¨ç›‘ç£å¯ä»¥å½¢å¼åŒ–ä¸ºï¼š$D={(x_1,y_1),â€¦,(x_l,y_l),x_{l+1},â€¦, x_m}$ å³æœ‰ $l$ ä¸ªæ•°æ®æœ‰æ ‡ç­¾ï¼ˆå¦‚ $y_i$ æ‰€ç¤ºï¼‰ï¼Œ$u = m-l$ ä¸ªæ•°æ®æ²¡æœ‰æ ‡ç­¾ã€‚ è§£å†³è¿™ç±»é—®é¢˜æœ‰ä¸¤ç§æŠ€æœ¯ï¼š ä¸»åŠ¨å­¦ä¹ (active learning), ä¹Ÿå°±æ˜¯æœ‰ä¸ªä¸“å®¶æ¥æ ‡æ³¨ unlabeled æ•°æ®. åŠç›‘ç£å­¦ä¹ (semi-supervision), æœ‰ä¸€ç§ç‰¹æ®Šçš„åŠç›‘ç£å­¦ä¹ ï¼Œå« transductive learning(ä¼ å¯¼å¼å­¦ä¹ )ï¼Œå®ƒä¸ï¼ˆçº¯ï¼‰åŠç›‘ç£å­¦ä¹ ä¹‹é—´çš„å·®åˆ«åœ¨äºï¼Œå¯¹æµ‹è¯•æ•°æ®ï¼ˆè®­ç»ƒæ¨¡å‹è¦é¢„æµ‹çš„æ•°æ®ï¼‰çš„å‡è®¾ä¸åŒã€‚ä¼ å¯¼å¼å­¦ä¹ æŒæœ‰â€œå°é—­ä¸–ç•Œâ€çš„å‡è®¾ï¼Œå³æµ‹è¯•æ•°æ®æ˜¯äº‹å…ˆç»™å®šçš„ï¼Œä¸”ç›®æ ‡å°±æ˜¯ä¼˜åŒ–æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½ï¼›æ¢å¥è¯è¯´ï¼Œæœªæ ‡æ³¨æ•°æ®å°±æ˜¯æµ‹è¯•æ•°æ®ã€‚çº¯åŠç›‘ç£å­¦ä¹ æŒæœ‰â€œå¼€æ”¾ä¸–ç•Œâ€çš„å‡è®¾ï¼Œå³æµ‹è¯•æ•°æ®æ˜¯æœªçŸ¥çš„ï¼Œä¸”æœªæ ‡æ³¨æ•°æ®ä¸ä¸€å®šæ˜¯æµ‹è¯•æ•°æ®ã€‚å®é™…ä¸­ï¼Œç»å¤§å¤šæ•°æƒ…å†µéƒ½æ˜¯çº¯åŠç›‘ç£å­¦ä¹ ã€‚ æœ‰äººä¸ºå¹²é¢„ä¸»åŠ¨å­¦ä¹  active learning. æ— äººä¸ºå¹²é¢„åŠç›‘ç£å­¦ä¹ [3-5]æ˜¯æŒ‡åœ¨ä¸è¯¢é—®äººç±»ä¸“å®¶çš„æ¡ä»¶ä¸‹æŒ–æ˜æœªæ ‡æ³¨æ•°æ®ã€‚ä¸ºä»€ä¹ˆæœªæ ‡æ³¨æ•°æ®å¯¹äºæ„å»ºé¢„æµ‹æ¨¡å‹ä¹Ÿä¼šæœ‰ç”¨ï¼Ÿåšä¸€ä¸ªç®€å•çš„è§£é‡Š[19]ï¼Œå‡è®¾æ•°æ®æ¥è‡ªä¸€ä¸ªç”± n ä¸ªé«˜æ–¯åˆ†å¸ƒæ··åˆçš„ é«˜æ–¯æ··åˆæ¨¡å‹(å‚è€ƒä»¥å‰çš„ç¬”è®°)ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š $$f(x | \\theta) = \\sum_{j=1}^n \\alpha_j f(x | \\theta_j)\\quad\\text{(1)}$$ å…¶ä¸­ $\\alpha_j$ ä¸ºæ··åˆç³»æ•°ï¼Œ$\\sum_{j=1}^n \\alpha_j = 1$ å¹¶ä¸” $\\theta = {\\theta_j}$ æ˜¯æ¨¡å‹å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ ‡ç­¾ $y_i$ å¯ä»¥çœ‹ä½œä¸€ä¸ªéšæœºå˜é‡ï¼Œå…¶åˆ†å¸ƒ $P(y_i | x_i, g_i)$ ç”±æ··åˆæˆåˆ† $g_i$ å’Œç‰¹å¾å‘é‡ $x_i$ å†³å®šã€‚æœ€å¤§åŒ–åéªŒæ¦‚ç‡æœ‰ï¼š $$h(x) = {argmax}c \\sum{j=1}^n P(y_i = c | g_i = j, x_i) \\times P(g_i = j | x_i)\\quad(2)$$ å…¶ä¸­ï¼š$P(g_i = j | x_i) = \\dfrac{\\alpha_j f(x_i | \\theta_j)} {\\sum_{k=1}^n \\alpha_k f(x_i | \\theta_k)}\\quad(3)$ $h(x)$ å¯ä»¥é€šè¿‡ç”¨è®­ç»ƒæ•°æ®ä¼°è®¡ $P(y_i = c | g_i = j, x_i)$ å’Œ $P(g_i = j | x_i)$ æ¥æ±‚å¾—ã€‚å¾ˆæ˜æ˜¾åªæœ‰ç¬¬ä¸€é¡¹éœ€è¦éœ€è¦æ ‡ç­¾ä¿¡æ¯ã€‚å› æ­¤ï¼Œæœªæ ‡æ³¨æ•°æ®å¯ä»¥ç”¨æ¥ä¼°è®¡æå‡å¯¹ç¬¬äºŒé¡¹çš„ä¼°è®¡ï¼Œä»è€Œæå‡å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚ å›¾ä¸­ $+,-$ è¡¨ç¤ºæ ‡æ³¨æ ·æœ¬ã€‚è€Œæµ‹è¯•æ ·æœ¬ $\\bigcirc$ æ­£å¥½åœ¨ä¸¤è€…ä¸­é—´ã€‚ å›¾3ç»™å‡ºäº†ä¸€ä¸ªç›´è§‚çš„è§£é‡Šã€‚å¦‚æœæˆ‘ä»¬åªèƒ½æ ¹æ®å”¯ä¸€çš„æ­£è´Ÿæ ·æœ¬ç‚¹æ¥é¢„æµ‹ï¼Œé‚£æˆ‘ä»¬å°±åªèƒ½éšæœºçŒœæµ‹ï¼Œå› ä¸ºæµ‹è¯•æ ·æœ¬æ°å¥½è½åœ¨äº†ä¸¤ä¸ªæ ‡æ³¨æ ·æœ¬çš„ä¸­é—´ä½ç½®ï¼›å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿè§‚æµ‹åˆ°ä¸€äº›æœªæ ‡æ³¨æ•°æ®ï¼Œä¾‹å¦‚å›¾ä¸­çš„ç°è‰²æ ·æœ¬ç‚¹ï¼Œæˆ‘ä»¬å°±èƒ½ä»¥è¾ƒé«˜çš„ç½®ä¿¡åº¦åˆ¤å®šæµ‹è¯•æ ·æœ¬ä¸ºæ­£æ ·æœ¬ã€‚åœ¨æ­¤å¤„ï¼Œå°½ç®¡æœªæ ‡æ³¨æ ·æœ¬æ²¡æœ‰æ˜ç¡®çš„æ ‡ç­¾ä¿¡æ¯ï¼Œå®ƒä»¬å´éšæ™¦åœ°åŒ…å«äº†ä¸€äº›æ•°æ®åˆ†å¸ƒçš„ä¿¡æ¯ï¼Œè€Œè¿™å¯¹äºé¢„æµ‹æ¨¡å‹æ˜¯æœ‰ç”¨çš„ã€‚ å®é™…ä¸Šï¼Œåœ¨åŠç›‘ç£å­¦ä¹ ä¸­æœ‰ä¸¤ä¸ªåŸºæœ¬å‡è®¾ï¼Œå³èšç±»å‡è®¾ï¼ˆcluster assumptionï¼‰å’Œæµå½¢å‡è®¾ï¼ˆmanifold assumptionï¼‰ï¼›ä¸¤ä¸ªå‡è®¾éƒ½æ˜¯å…³äºæ•°æ®åˆ†å¸ƒçš„ã€‚å‰è€…å‡è®¾æ•°æ®å…·æœ‰å†…åœ¨çš„èšç±»ç»“æ„ï¼Œå› æ­¤ï¼Œè½å…¥åŒä¸€ä¸ªèšç±»çš„æ ·æœ¬ç±»åˆ«ç›¸åŒã€‚åè€…å‡è®¾æ•°æ®åˆ†å¸ƒåœ¨ä¸€ä¸ªæµå½¢ä¸Šï¼Œå› æ­¤ï¼Œç›¸è¿‘çš„æ ·æœ¬å…·æœ‰ç›¸ä¼¼çš„é¢„æµ‹ã€‚ä¸¤ä¸ªå‡è®¾çš„æœ¬è´¨éƒ½æ˜¯ç›¸ä¼¼çš„æ•°æ®è¾“å…¥åº”è¯¥æœ‰ç›¸ä¼¼çš„è¾“å‡ºï¼Œè€Œæœªæ ‡æ³¨æ•°æ®æœ‰åŠ©äºæ­ç¤ºå‡ºæ ·æœ¬ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ åŠç›‘ç£å­¦ä¹ æœ‰å››ç§ä¸»è¦æ–¹æ³•ï¼Œå³ç”Ÿæˆå¼æ–¹æ³•ï¼ˆgenerative methodsï¼‰ï¼ŒåŸºäºå›¾çš„æ–¹æ³•ï¼ˆgraph-based methodsï¼‰ï¼Œä½å¯†åº¦åˆ†å‰²æ³•ï¼ˆlow-density separation methodsï¼‰ä»¥åŠåŸºäºåˆ†æ­§çš„æ–¹æ³•ï¼ˆdisagreement methodsï¼‰ã€‚ ç”Ÿæˆå¼æ–¹æ³•[19ï¼Œ20]å‡è®¾æ ‡æ³¨æ•°æ®å’Œæœªæ ‡æ³¨æ•°æ®éƒ½ç”±ä¸€ä¸ªå›ºæœ‰çš„æ¨¡å‹ç”Ÿæˆã€‚å› æ­¤ï¼Œæœªæ ‡æ³¨æ•°æ®çš„æ ‡ç­¾å¯ä»¥çœ‹ä½œæ˜¯æ¨¡å‹å‚æ•°çš„ç¼ºå¤±ï¼Œå¹¶å¯ä»¥é€šè¿‡EMç®—æ³•ï¼ˆæœŸæœ›-æœ€å¤§åŒ–ç®—æ³•ï¼‰ç­‰æ–¹æ³•è¿›è¡Œä¼°è®¡[21]ã€‚è¿™ç±»æ–¹æ³•éšç€ä¸ºæ‹Ÿåˆæ•°æ®è€Œé€‰ç”¨çš„ä¸åŒç”Ÿæˆæ¨¡å‹è€Œæœ‰æ‰€å·®åˆ«ã€‚ä¸ºäº†è¾¾åˆ°å¥½çš„æ€§èƒ½ï¼Œé€šå¸¸éœ€è¦ç›¸å…³é¢†åŸŸçš„çŸ¥è¯†æ¥é€‰æ‹©åˆé€‚çš„ç”Ÿæˆæ¨¡å‹ã€‚ä¹Ÿæœ‰ä¸€äº›å°†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹çš„ä¼˜ç‚¹ç»“åˆèµ·æ¥çš„å°è¯•[22]ã€‚ åŸºäºå›¾çš„æ–¹æ³•æ„å»ºä¸€ä¸ªå›¾ï¼Œå…¶èŠ‚ç‚¹å¯¹åº”è®­ç»ƒæ ·æœ¬ï¼Œå…¶è¾¹å¯¹åº”æ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼ˆé€šå¸¸æ˜¯æŸç§ç›¸ä¼¼åº¦æˆ–è·ç¦»ï¼‰ï¼Œè€Œåä¾æ®æŸäº›å‡†åˆ™å°†æ ‡æ³¨ä¿¡æ¯åœ¨å›¾ä¸Šè¿›è¡Œæ‰©æ•£ï¼›ä¾‹å¦‚æ ‡ç­¾å¯ä»¥åœ¨æœ€å°åˆ†å‰²å›¾ç®—æ³•å¾—åˆ°çš„ä¸åŒå­å›¾å†…ä¼ æ’­[23]ã€‚å¾ˆæ˜æ˜¾ï¼Œæ¨¡å‹çš„æ€§èƒ½å–å†³äºå›¾æ˜¯å¦‚ä½•æ„å»ºçš„[26-28]ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºmä¸ªæ ·æœ¬ç‚¹ï¼Œè¿™ç§æ–¹æ³•é€šç•…éœ€è¦O(m^2)å­˜å‚¨ç©ºé—´å’ŒO(m^3)è®¡ç®—æ—¶é—´å¤æ‚åº¦ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹æ³•ä¸¥é‡å—åˆ¶äºé—®é¢˜çš„è§„æ¨¡ï¼›è€Œä¸”ç”±äºéš¾ä»¥åœ¨ä¸é‡å»ºå›¾çš„æƒ…å†µä¸‹å¢åŠ æ–°çš„èŠ‚ç‚¹ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•å¤©ç”Ÿéš¾ä»¥è¿ç§»ã€‚ åŸºäºåˆ†æ­§çš„æ–¹æ³•[5ï¼Œ32ï¼Œ33]ç”Ÿæˆå¤šä¸ªå­¦ä¹ å™¨ï¼Œå¹¶è®©å®ƒä»¬åˆä½œæ¥æŒ–æ˜æœªæ ‡æ³¨æ•°æ®ï¼Œå…¶ä¸­ä¸åŒå­¦ä¹ å™¨ä¹‹é—´çš„åˆ†æ­§æ˜¯è®©å­¦ä¹ è¿‡ç¨‹æŒç»­è¿›è¡Œçš„å…³é”®ã€‚æœ€ä¸ºè‘—åçš„å…¸å‹æ–¹æ³•â€”â€”è”åˆè®­ç»ƒï¼ˆco-traingï¼‰ï¼Œé€šè¿‡ä»ä¸¤ä¸ªä¸åŒçš„ç‰¹å¾é›†åˆï¼ˆæˆ–è§†è§’ï¼‰è®­ç»ƒå¾—åˆ°çš„ä¸¤ä¸ªå­¦ä¹ å™¨æ¥è¿ä½œã€‚åœ¨æ¯ä¸ªå¾ªç¯ä¸­ï¼Œæ¯ä¸ªå­¦ä¹ å™¨é€‰æ‹©å…¶é¢„æµ‹ç½®ä¿¡åº¦æœ€é«˜çš„æœªæ ‡æ³¨æ ·æœ¬ï¼Œå¹¶å°†å…¶é¢„æµ‹ä½œä¸ºæ ·æœ¬çš„ä¼ªæ ‡ç­¾æ¥è®­ç»ƒå¦ä¸€ä¸ªå­¦ä¹ å™¨ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é€šè¿‡å­¦ä¹ å™¨é›†æˆæ¥å¾—åˆ°å¾ˆå¤§æå‡[34ï¼Œ35]ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºåˆ†æ­§çš„æ–¹æ³•æä¾›äº†ä¸€ç§å°†åŠç›‘ç£å­¦ä¹ å’Œä¸»åŠ¨å­¦ä¹ è‡ªç„¶åœ°ç»“åˆåœ¨ä¸€èµ·çš„æ–¹å¼ï¼šå®ƒä¸ä»…å¯ä»¥è®©å­¦ä¹ å™¨ç›¸äº’å­¦ä¹ ï¼Œå¯¹äºä¸¤ä¸ªæ¨¡å‹éƒ½ä¸å¤ªç¡®å®šæˆ–è€…éƒ½å¾ˆç¡®å®šä½†ç›¸äº’çŸ›ç›¾çš„æœªæ ‡æ³¨æ ·æœ¬ï¼Œè¿˜å¯ä»¥è¢«é€‰å®šè¯¢é—®â€œå…ˆçŸ¥â€ã€‚ ä¸ç¡®åˆ‡ç›‘ç£ä¸ç¡®åˆ‡ç›‘ç£æ˜¯æŒ‡åœ¨æŸç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›ç›‘ç£ä¿¡æ¯ï¼Œä½†æ˜¯å¹¶ä¸åƒæˆ‘ä»¬æ‰€æœŸæœ›çš„é‚£æ ·ç²¾ç¡®ã€‚ä¸€ä¸ªå…¸å‹çš„æƒ…å†µæ˜¯æˆ‘ä»¬åªæœ‰ç²—ç²’åº¦çš„æ ‡æ³¨ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨è¯ç‰©æ´»æ€§é¢„æµ‹ä¸­[40]ï¼Œç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªæ¨¡å‹å­¦ä¹ å·²çŸ¥åˆ†å­çš„çŸ¥è¯†ï¼Œæ¥é¢„æµ‹ä¸€ç§æ–°çš„åˆ†å­æ˜¯å¦èƒ½å¤Ÿç”¨äºæŸç§ç‰¹æ®Šè¯ç‰©çš„åˆ¶é€ ã€‚ä¸€ç§åˆ†å­å¯èƒ½æœ‰å¾ˆå¤šä½èƒ½é‡çš„å½¢æ€ï¼Œè¿™ç§åˆ†å­èƒ½å¦ç”¨äºåˆ¶ä½œè¯¥è¯ç‰©å–å†³äºè¿™ç§åˆ†å­æ˜¯å¦æœ‰ä¸€äº›ç‰¹æ®Šå½¢æ€ã€‚ç„¶è€Œï¼Œå³ä½¿å¯¹äºå·²çŸ¥çš„åˆ†å­ï¼Œäººç±»ä¸“å®¶ä¹ŸåªçŸ¥é“å…¶æ˜¯å¦åˆæ ¼ï¼Œè€Œå¹¶ä¸çŸ¥é“å“ªç§ç‰¹å®šå½¢æ€æ˜¯å†³å®šæ€§çš„ã€‚ å½¢å¼åŒ–è¡¨è¾¾ä¸ºï¼Œè¿™ä¸€ä»»åŠ¡æ˜¯å­¦ä¹  $f: X\\rightarrow Y$ ï¼Œå…¶è®­ç»ƒé›†ä¸º $D = {(X_1, y_1), â€¦, (X_m, y_m)}$ï¼Œå…¶ä¸­ $X_i = {x_{I, 1}, â€¦, x_{I, m_i}}$, $X_i$ å±äºXï¼Œä¸”è¢«ç§°ä¸ºä¸€ä¸ªåŒ…ï¼ˆbagï¼‰ï¼Œ$x_{i, j}$ å±äº Xï¼Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼ˆjå±äº ${1, â€¦, m_i}ï¼‰$ã€‚$m_i$ æ˜¯ $X_i$ ä¸­çš„æ ·æœ¬ä¸ªæ•°ï¼Œ$y_i$ å±äº $Y = {Y, N}$ã€‚å½“å­˜åœ¨ $x_{i, p}$ æ˜¯æ­£æ ·æœ¬æ—¶ï¼Œ$X_i$ å°±æ˜¯ä¸€ä¸ªæ­£åŒ…ï¼ˆpositive bagï¼‰ï¼Œå…¶ä¸­pæ˜¯æœªçŸ¥çš„ä¸” p å±äº ${1, â€¦, m_i}$ã€‚æ¨¡å‹çš„ç›®æ ‡å°±æ˜¯é¢„æµ‹æœªçŸ¥åŒ…çš„æ ‡ç­¾ã€‚è¿™è¢«ç§°ä¸ºå¤šç¤ºä¾‹å­¦ä¹ ï¼ˆmulti-instance learningï¼‰[40ï¼Œ41] å¤šç¤ºä¾‹å­¦ä¹ å·²ç»æˆåŠŸåº”ç”¨äºå¤šç§ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€æ£€ç´¢ã€æ³¨é‡Š[48-50]ï¼Œæ–‡æœ¬åˆ†ç±»[51ï¼Œ52]ï¼Œåƒåœ¾é‚®ä»¶æ£€æµ‹[53]ï¼ŒåŒ»ç–—è¯Šæ–­[54]ï¼Œäººè„¸ã€ç›®æ ‡æ£€æµ‹[55ï¼Œ56]ï¼Œç›®æ ‡ç±»åˆ«å‘ç°[57]ï¼Œç›®æ ‡è·Ÿè¸ª[58]ç­‰ç­‰ã€‚åœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆè‡ªç„¶åœ°å°†ä¸€ä¸ªçœŸå®çš„ç›®æ ‡ï¼ˆä¾‹å¦‚ä¸€å¼ å›¾ç‰‡æˆ–ä¸€ä¸ªæ–‡æœ¬æ–‡æ¡£ï¼‰çœ‹ä½œä¸€ä¸ªåŒ…ï¼›ç„¶è€Œï¼Œä¸åŒäºè¯ç‰©æ´»æ€§é¢„æµ‹ä¸­åŒ…é‡Œæœ‰å¤©ç„¶çš„ç¤ºä¾‹ï¼ˆå³åˆ†å­çš„ä¸åŒå½¢æ€ï¼‰ï¼Œè¿™é‡Œçš„ç¤ºä¾‹éœ€è¦ç”Ÿæˆã€‚ä¸€ä¸ªåŒ…ç”Ÿæˆå™¨æ˜ç¡®å¦‚ä½•ç”Ÿæˆç¤ºä¾‹æ¥ç»„æˆä¸€ä¸ªåŒ…ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œä»ä¸€å¹…å›¾åƒä¸­æå–çš„å¾ˆå¤šå°å›¾åƒå—å°±ä½œä¸ºå¯ä»¥è¿™ä¸ªå›¾åƒçš„ç¤ºä¾‹ï¼Œè€Œç« èŠ‚ã€æ®µè½ç”šè‡³æ˜¯å¥å­å¯ä»¥ä½œä¸ºä¸€ä¸ªæ–‡æœ¬æ–‡æ¡£çš„ç¤ºä¾‹ã€‚å°½ç®¡åŒ…ç”Ÿæˆå™¨å¯¹äºå­¦ä¹ æ•ˆæœæœ‰é‡è¦çš„å½±å“ï¼Œä½†ç›´åˆ°æœ€è¿‘æ‰å‡ºç°å…³äºå›¾åƒåŒ…ç”Ÿæˆå™¨çš„å…¨é¢ç ”ç©¶[59]ï¼›ç ”ç©¶è¡¨æ˜ä¸€äº›ç®€å•çš„å¯†é›†å–æ ·åŒ…ç”Ÿæˆå™¨è¦æ¯”å¤æ‚çš„ç”Ÿæˆå™¨æ€§èƒ½æ›´å¥½ã€‚å›¾5æ˜¾ç¤ºäº†ä¸¤ä¸ªç®€å•è€Œæœ‰æ•ˆçš„å›¾åƒåŒ…ç”Ÿæˆå™¨ã€‚ 51.Convex and Scalable Weakly Labeled SVMs [52.Towards making unlabeled data never hurt](http://www.icml-2011.org/papers/548_icmlpaper.pdf) ä¸å‡†ç¡®ç›‘ç£ä¸å‡†ç¡®ç›‘ç£å…³æ³¨ç›‘ç£ä¿¡æ¯ä¸æ€»æ˜¯çœŸå€¼çš„æƒ…å½¢ï¼›æ¢å¥è¯è¯´ï¼Œæœ‰äº›æ ‡ç­¾ä¿¡æ¯å¯èƒ½æ˜¯é”™è¯¯çš„ã€‚å…¶å½¢å¼åŒ–è¡¨ç¤ºä¸æ¦‚è¿°ç»“å°¾éƒ¨åˆ†å‡ ä¹å®Œå…¨ç›¸åŒï¼Œé™¤äº†è®­ç»ƒæ•°æ®é›†ä¸­çš„y_iå¯èƒ½æ˜¯é”™è¯¯çš„ã€‚ ä¸€ä¸ªæœ€è¿‘å‡ºç°çš„ä¸å‡†ç¡®ç›‘ç£çš„æƒ…æ™¯å‘ç”Ÿåœ¨ä¼—åŒ…æ¨¡å¼ä¸­(crowdsourcing)[74],å³ä¸€ä¸ªå°†å·¥ä½œå¤–åŒ…ç»™ä¸ªäººçš„æµè¡Œæ¨¡å¼ã€‚ åœ¨å¸¦æœ‰çœŸå€¼æ ‡ç­¾çš„å¤§é‡è®­ç»ƒæ ·æœ¬çš„å¼ºç›‘ç£æ¡ä»¶ä¸‹ï¼Œç›‘ç£å­¦ä¹ æŠ€æœ¯å·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨çœŸå®çš„ä»»åŠ¡ä¸­ï¼Œæ”¶é›†ç›‘ç£ä¿¡æ¯å¾€å¾€ä»£ä»·é«˜æ˜‚ï¼Œå› æ­¤æ¢ç´¢å¼±ç›‘ç£å­¦ä¹ é€šå¸¸æ˜¯æ›´å¥½çš„æ–¹å¼ã€‚ Snorkelpaper:Snorkel: Rapid Training Data Creation with Weak Supervision motivationdeep learning éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ã€‚è¿™å¯¹äºä¸€äº›å¤§å…¬å¸å°šä¸”èƒ½å¤Ÿé›‡ä½£æ ‡æ³¨äººå‘˜ï¼Œè€Œå¾ˆå¤šå°å…¬å¸åˆ™å°†ç›®æ ‡è½¬å‘å¼±ç›‘ç£å­¦ä¹ ã€‚å°¤å…¶æ˜¯ï¼Œå½“æ•°æ®çš„æ ‡æ³¨éœ€è¦é¢†åŸŸä¸“å®¶æ—¶(subject matter experts (SMEs))ï¼Œæ ‡æ³¨æ•°æ®å˜å¾—æ›´åŠ å›°éš¾ã€‚ å¼±ç›‘ç£å­¦ä¹ åŒ…æ‹¬ä¸€ä¸‹å½¢å¼ï¼š distant supervision: the records of an external knowledge base are heuristically aligned with data points to produce noisy labels [4,7,32] crowsourced labels[37,50] rules and heuristics for labeling data[39,52] Snorkel learns the accuracies of weak supervision sources withoust access to ground truth using a generative model [38]. Furthermore, it also learns correlations and other statistical dependencies among sources, correcting for dependencies in labeling functions that skew the estimated accuracies [5] Snorkel ç”Ÿæˆè®­ç»ƒæ•°æ®çš„æ–¹æ³•æ¥è‡ªäºä½œè€…çš„å¦å¤–ä¸€ç¯‡è®ºæ–‡ï¼šData programming: Creating large training sets, quickly, NIPS 2016,ä¸ä»…èƒ½ç»™å‡ºå¼±ç›‘ç£å¾—åˆ°çš„æ ·æœ¬çš„ç½®ä¿¡åº¦ï¼Œè¿˜èƒ½å­¦ä¹ å¾—åˆ°æ ·æœ¬ä¹‹é—´çš„ç›¸å…³æ€§å’Œç»Ÿè®¡ä¾èµ–ã€‚","link":"/2019/02/28/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-0-NLP%20classification%20with%20transfer%20learning%20and%20weak%20supervision/"},{"title":"è®ºæ–‡ç¬”è®°-é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹2-ULMFiT","text":"Motivationå¯¹æ¯”ä¹‹å‰çš„å‡ ç§æ¨¡å‹concatenate embeddings: ELMo Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness. è¿™ç¯‡ paper æ˜¯åœ¨ elmo ä¹‹åï¼Œè€Œ elmo è™½ç„¶ç›¸å¯¹å‡ºåï¼Œå½±å“åŠ›æ›´å¤§ï¼Œä½†æ˜¯ elmo ä»æ—§åªæ˜¯ä¸€ç§ word embedding çš„é¢„è®­ç»ƒï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿˜æ˜¯éœ€è¦ä»å¤´è®­ç»ƒæ¨¡å‹ã€‚ ELMoæœ‰ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š åˆ©ç”¨LMä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒ å†åˆ©ç”¨ç›®æ ‡é¢†åŸŸçš„è¯­æ–™å¯¹LMæ¨¡å‹åšå¾®è°ƒ æœ€åé’ˆå¯¹ç›®æ ‡ä»»åŠ¡è¿›è¡Œ concatenate embeddingï¼Œç„¶åè®­ç»ƒæ¨¡å‹ pretraining LM: In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability. ç›´æ¥ä½¿ç”¨åœ¨ general-domain ä¸Šé¢„è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹ï¼Œç„¶åé€šè¿‡ fine-tune è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œ ä»æ—§éœ€è¦å¤§é‡çš„ in-domain çš„æ–‡æ¡£æ‰èƒ½è·å¾—æ¯”è¾ƒå¥½çš„ performance. ULMFiT We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods. ä½œè€…è®¤ä¸ºï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–¹å¼å¹¶ä¸æ˜¯ä¸å¥½ï¼Œåªæ˜¯è®­ç»ƒæ–¹æ³•çš„é—®é¢˜å¯¼è‡´äº†ä»–ä»¬è¡¨ç°å±€é™æ€§ã€‚æƒ³å¯¹äº CVï¼Œ NLP ä¸­çš„å¾ˆå¤šä»»åŠ¡æ‰€éœ€è¦çš„è¯­ä¹‰æ›´æµ…å±‚ã€‚è€Œå°† LMs åœ¨å°æ•°æ®é›†ä¸Š fine-tune æ—¶ä¼šå¯¼è‡´ä¸¥é‡çš„é—å¿˜ã€‚ äºæ˜¯ï¼Œä½œè€…æå‡ºäº† Universal Language Model Fine-tuning(ULMFiT) é€šç”¨çš„è¯­è¨€æ¨¡å‹å¾®è°ƒ discriminative fine-tuning, slanted triangular learning rates gradual unfreezing Universal Language Model Fine-tuning ä¸»è¦åˆ†ä¸º 3 éƒ¨åˆ†ï¼š General-domain LM pretraining Target task LM fine-tuning Target task classifier fine-tuning General-domain LM pretrainingWikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words. åœ¨è¶³å¤Ÿå¤§çš„ general-domain è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ Target task LM fine-tuningdiscriminative fine-tuninåœ¨ç›®æ ‡è¯­æ–™åº“ in-domain ä¸Šè¿›è¡Œ fine-tune. è¿™éƒ¨åˆ†ä¼šæ”¶æ•›çš„å¾ˆå¿«ï¼Œå¹¶ä¸”åœ¨å°æ•°æ®é›†ä¸Šä¾æ—§ä¼šæœ‰å¾ˆå¥½çš„æ³›åŒ–æ€§ã€‚ As different layers capture different types of information (Yosinski et al., 2014), they should be fine-tuned to different extents. ä¸åŒçš„ layer èƒ½æ•æ‰ä¸åŒç¨‹åº¦çš„ä¿¡æ¯ï¼Œäºæ˜¯ï¼Œä½œè€…æå‡ºäº† discriminative fine-tuning. ä¸åŒçš„ layer å…·æœ‰ä¸åŒçš„ learning rate. L è¡¨ç¤ºæ€»çš„ layer æ•°ç›®ã€‚ $${\\theta^1,\\theta^2, â€¦, \\theta^L}$$ $${\\eta^1,\\eta^2, â€¦, \\eta^L}$$ Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. åŸæœ¬çš„ SGD æ˜¯è¿™æ ·çš„ï¼š $$\\theta_t = \\theta_{t-1}-\\eta\\cdot\\nabla_{\\theta}J(\\theta)$$ æ”¹è¿›ä¹‹åï¼š $$\\theta_t^l = \\theta_{t-1}^l-\\eta^l\\cdot\\nabla_{\\theta^l}J(\\theta)$$ ä½œè€…é€šè¿‡ç»éªŒå‘ç°ï¼šå…ˆé€‰æ‹©æœ€åä¸€å±‚çš„å­¦ä¹ ç‡ $\\eta^L$ï¼Œç„¶åè®¡ç®—æ¯ä¸€å±‚çš„å­¦ä¹ ç‡ $\\eta^{l-1}=\\eta^l/2.6$ Slanted triangular learning rates T æ˜¯è¿­ä»£æ¬¡æ•°ï¼Œè¿™é‡Œå®é™…ä¸Šæ˜¯ $epochs \\times \\text{number of per epoch}$ cut_frac æ˜¯å¢åŠ å­¦ä¹ ç‡çš„è¿­ä»£æ­¥æ•°æ¯”ä¾‹ cut æ˜¯å­¦ä¹ ç‡å¢åŠ å’Œå‡å°‘çš„ä¸´ç•Œè¿­ä»£æ­¥æ•° p æ˜¯ä¸€ä¸ªåˆ†æ®µå‡½æ•°ï¼Œåˆ†åˆ«é€’å¢å’Œé€’å‡ ratio è¡¨ç¤ºå­¦ä¹ ç‡æœ€å°æ—¶ï¼Œä¸æœ€å¤§å­¦ä¹ ç‡çš„æ¯”ä¾‹ã€‚æ¯”å¦‚ t=0æ—¶ï¼Œp=0, é‚£ä¹ˆ $\\eta_0=\\dfrac{\\eta_{max}}{ratio}$ ä½œè€…é€šè¿‡å®éªŒå‘ç°ï¼Œcut_frac=0.1, ratio=32, $\\eta_max=0.01$ Target task classifier fine-tuningé’ˆå¯¹åˆ†ç±»ä»»åŠ¡ï¼ŒåŠ ä¸Š two additional linear blocks. concat pooling gradul unfreezingé€æ¸ unfreeze layers: We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration. BPTT for Text Classificationbackpropagation through time(BPTT) We divide the document into fixed length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences (Merity et al., 2017a). ä»€ä¹ˆæ„æ€ï¼Ÿå¹¶ä¸æ˜¯ä¸€ä¸ª batch æ›´æ–°ä¸€æ¬¡æ¢¯åº¦ï¼Œè€Œæ˜¯ç´¯åŠ ä¸€å®šçš„ batch ä¹‹ååœ¨æ›´æ–°æ¢¯åº¦ï¼Ÿ èƒ½å¢åŠ æ³›åŒ–æ€§ï¼Ÿ Bidirectional language modelç‹¬ç«‹çš„å¯¹ forward-LM, backward-LM è¿›è¡Œ fine-tune, ç„¶åå¹³å‡ã€‚ experimentä¸å…¶ä»–æ¨¡å‹å¯¹æ¯” ablations â€œfrom scratchâ€: æ²¡æœ‰ fine-tune â€œsupervisedâ€: è¡¨ç¤ºä»…ä»…åœ¨ label examples è¿›è¡Œ fine-tune â€œsemi-supervisedâ€: è¡¨ç¤ºåœ¨ unable examples ä¸Šä¹Ÿè¿›è¡Œäº† fine-tune å¯¹ tricks è¿›è¡Œåˆ†æ â€œfullâ€ :fine-tuning the full model â€œdiscrâ€: discriminative fine-tuning â€œstlrâ€: slanted triangular learning rates","link":"/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/"},{"title":"è®ºæ–‡ç¬”è®°-BERT","text":"BERT(Bidirectional Encoder Representations from Transformers.)å¯¹äº BERT é‡ç‚¹åœ¨äºç†è§£ Bidirectional å’Œ masked language model. Why Bidirectional?å¯¹äºé¢„è®­ç»ƒçš„è¡¨ç¤ºï¼Œå•å‘è¯­è¨€æ¨¡å‹å› ä¸ºæ— æ³•èåˆä¸‹æ–‡çš„ä¿¡æ¯ï¼Œå…¶èƒ½åŠ›æ˜¯éå¸¸æœ‰é™çš„ï¼Œå°¤å…¶æ˜¯å¯¹ç±»ä¼¼äº SQuAD è¿™æ ·éœ€è¦ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä»»åŠ¡ã€‚ å¯¹æ¯” OpenAI GPT å’Œ BERT. ä¸ºä»€ä¹ˆ OpenAI GPT ä¸èƒ½é‡‡ç”¨åŒå‘ self-attention å‘¢ï¼Ÿ ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹çš„å®šä¹‰ï¼Œè®¡ç®—å¥å­çš„æ¦‚ç‡ï¼š $$P(S)=p(w_1,w_2, â€¦, w_n)=p(w1)p(w_2|w_1)â€¦p(w_n|w_1â€¦w_{n-1})=\\prod_{i=1}^m p(w_i|w_1â€¦w_{i-1})$$ å‰å‘ RNN è¯­è¨€æ¨¡å‹ï¼š $$P(S)=\\prod_{i=1}^m p(w_i|w_1â€¦w_{i-1})$$ ä¹Ÿå°±æ˜¯å½“å‰è¯çš„æ¦‚ç‡åªä¾èµ–å‰é¢å‡ºç°è¯çš„æ¦‚ç‡ã€‚ åå‘ RNN è¯­è¨€æ¨¡å‹ $$P(S)=\\prod_{i=1}^m p(w_i|w_{i+1}â€¦w_{m})$$ ä¹Ÿå°±æ˜¯å½“å‰è¯çš„æ¦‚ç‡åªä¾èµ–åé¢å‡ºç°çš„è¯çš„æ¦‚ç‡ã€‚ ELMo å°±æ˜¯è¿™æ ·çš„åŒå‘è¯­è¨€æ¨¡å‹(BiLM) ä½†æ˜¯ RNN ç›¸æ¯” self-attention å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯ (contextual information)çš„åˆ©ç”¨ç›¸å¯¹æœ‰é™ï¼Œè€Œä¸” ELMo åªèƒ½æ˜¯ä¸€å±‚åŒå‘ï¼Œå¹¶ä¸èƒ½ä½¿ç”¨å¤šå±‚ã€‚å…¶åŸå› å’Œ GPT æ— æ³•ä½¿ç”¨ åŒå‘ ç¼–ç çš„åŸå› ä¸€æ ·ã€‚ å¯¹äº GPT å¦‚æœå®ƒä½¿ç”¨åŒå‘ï¼Œé‚£ä¹ˆæ¨¡å‹å°±èƒ½å‡†ç¡®çš„å­¦åˆ°åˆ°å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆï¼Œå¹¶èƒ½ 100% çš„é¢„æµ‹å‡ºä¸‹ä¸€ä¸ªè¯ã€‚æ¯”å¦‚ â€œI love to work on NLP.â€ åœ¨é¢„æµ‹ love çš„ä¸‹ä¸€ä¸ªè¯æ—¶ï¼Œæ¨¡å‹èƒ½çœ‹åˆ° toï¼Œæ‰€ä»¥èƒ½å¾ˆå¿«çš„é€šè¿‡è¿­ä»£å­¦ä¹ åˆ° â€œtoâ€ 100% å°±æ˜¯ love çš„ä¸‹ä¸€ä¸ªè¯ã€‚æ‰€ä»¥ï¼Œè¿™å¯¼è‡´æ¨¡å‹å¹¶ä¸èƒ½å­¦åˆ°æƒ³è¦çš„ä¸œè¥¿ï¼ˆå¥æ³•ã€è¯­ä¹‰ä¿¡æ¯ï¼‰ã€‚ é‚£ä¹ˆ BERT æ˜¯æ€ä¹ˆå¤„ç†åŒå‘è¿™ä¸ªé—®é¢˜çš„å‘¢ï¼Ÿ å®ƒæ”¹å˜äº†è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä»»åŠ¡å½¢å¼ã€‚æå‡ºäº†ä¸¤ç§æ–¹å¼ â€œmasked language modelâ€ and â€œnext sentence generationâ€. å†ä»‹ç»è¿™ä¸¤ç§è®­ç»ƒæ–¹å¼ä¹‹å‰ï¼Œå…ˆè¯´æ˜ä¸‹è¾“å…¥å½¢å¼ã€‚ Input representation position embedding: è·Ÿ Transformer ç±»ä¼¼ sentence embedding, åŒä¸€ä¸ªå¥å­çš„è¯çš„è¡¨ç¤ºä¸€æ ·ï¼Œéƒ½æ˜¯ $E_A$ æˆ– $E_B$. ç”¨æ¥è¡¨ç¤ºä¸åŒçš„å¥å­å…·æœ‰ä¸åŒçš„å«ä¹‰ å¯¹äº [Question, Answer] è¿™æ ·çš„ sentence-pairs çš„ä»»åŠ¡ï¼Œåœ¨å¥å­æœ«å°¾åŠ ä¸Š [SEP]. å¯¹äºæ–‡æœ¬åˆ†ç±»è¿™æ ·çš„ single-sentence çš„ä»»åŠ¡ï¼Œåªéœ€è¦åŠ ä¸Š [CLS], å¹¶ä¸” sentence embedding åªæœ‰ $E_A$. masked language modelä½•ä¸º â€œmasked LMâ€? idea æ¥æºäº closed tasked. åŸæœ¬çš„è¯­è¨€æ¨¡å‹æ˜¯é¢„æµ‹æ‰€æœ‰è¯­æ–™ä¸­çš„ä¸‹ä¸€ä¸ªè¯ï¼Œè€Œ MLM æ˜¯åœ¨æ‰€æœ‰çš„ tokens ä¸­éšæœºé€‰å– 15% çš„è¿›è¡Œ maskï¼Œç„¶ååªéœ€è¦é¢„æµ‹è¢« mask çš„è¯ã€‚è¿™æ ·ä»¥æ¥ï¼Œå°±èƒ½è®­ç»ƒåŒå‘è¯­è¨€æ¨¡å‹äº†ã€‚ ä½†æ˜¯å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œè¿™æ · pre-training è®­ç»ƒå‡ºæ¥çš„è¯­è¨€æ¨¡å‹å¹¶ä¸èƒ½æ‹¿å»åš fine-tune. åŸå› æ˜¯åœ¨ fine-token ä¸­ä»æ¥æ²¡æœ‰è§è¿‡ &lt;MASK&gt; è¿™ä¸ªè¯ã€‚ä½œè€…é‡‡ç”¨è¿™æ ·çš„ç­–ç•¥ï¼š å…·ä½“çš„æ“ä½œï¼Œä»¥ â€œMy dog is hairyâ€ ä¸ºä¾‹ï¼Œmask â€œhairyâ€ è¿™ä¸ªè¯ï¼š â€œMy dog is &lt;MASK&gt;â€œ. 80% è¢« ä»£æ›¿ â€œMy dog is appleâ€. 10% è¢«ä¸€ä¸ªéšæœºçš„ token ä»£æ›¿ â€œMy dog is hairyâ€. 10% ä¿æŒåŸæ¥çš„æ ·å­ ä¸ºä»€ä¹ˆä¸ç”¨ &lt;MASK&gt; ä»£æ›¿æ‰€æœ‰çš„ tokenï¼Ÿ If the model had been trained on only predicting â€˜&lt;MASK&gt;â€™ tokens and then never saw this token during fine-tuning, it would have thought that there was no need to predict anything and this would have hampered performance. Furthermore, the model would have only learned a contextual representation of the â€˜&lt;MASK&gt;â€™ token and this would have made it learn slowly (since only 15% of the input tokens are masked). By sometimes asking it to predict a word in a position that did not have a â€˜&lt;MASK&gt;â€™ token, the model needed to learn a contextual representation of all the words in the input sentence, just in case it was asked to predict them afterwards. å¦‚æœæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™ä»…ä»…åªé¢„æµ‹ &lt;MASK&gt;, ç„¶ååœ¨ fine-tune çš„æ—¶å€™ä»æœªè§è¿‡ &lt;MASK&gt; è¿™ä¸ªè¯ï¼Œé‚£ä¹ˆæ¨¡å‹å°±ä¸éœ€è¦é¢„æµ‹ä»»ä½•è¯ï¼Œåœ¨ fine-tune æ—¶ä¼šå½±å“æ€§èƒ½ã€‚ æ›´ä¸¥é‡çš„æ˜¯ï¼Œå¦‚æœä»…ä»…é¢„æµ‹ &lt;MASK&gt;, é‚£ä¹ˆæ¨¡å‹åªéœ€è¦å­¦ä¹  &lt;MASK&gt; çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œè¿™ä¼šå¯¼è‡´å®ƒå­¦ä¹ çš„å¾ˆæ…¢ã€‚ å¦‚æœè®©æ¨¡å‹åœ¨æŸä¸ªä½ç½®å»é¢„æµ‹ä¸€ä¸ªä¸æ˜¯ &lt;MASK&gt; çš„è¯ï¼Œé‚£ä¹ˆæ¨¡å‹å°±éœ€è¦å­¦ä¹ æ‰€æœ‰ tokens çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå› ä¸ºä¸‡ä¸€éœ€è¦é¢„æµ‹è¿™ä¸ªè¯å‘¢ã€‚ åªéœ€è¦ random tokens è¶³å¤Ÿå—ï¼Ÿä¸ºä»€ä¹ˆè¿˜éœ€è¦ 10% çš„å®Œæ•´çš„ sentence? Well, ideally we want the modelâ€™s representation of the masked token to be better than random. By sometimes keeping the sentence intact (while still asking the model to predict the chosen token) the authors biased the model to learn a meaningful representation of the masked tokens. ä½¿å¾—æ¨¡å‹å…·æœ‰åç½®ï¼Œæ›´å€¾å‘äºè·å¾—æœ‰æ„ä¹‰çš„ masked token. åœ¨çŸ¥ä¹ä¸Šé—®äº†è¿™ä¸ªé—®é¢˜ï¼Œå¤§ä½¬çš„å›å¤è·Ÿè¿™ç¯‡ blog æœ‰ç‚¹å·®å¼‚ï¼Œä½†å®é™…ä¸Šæ„æ€æ˜¯ä¸€æ ·çš„ï¼š æ€»ç»“ä¸‹ï¼š ä¸ºä»€ä¹ˆä¸èƒ½å®Œå…¨åªæœ‰ &lt;MASK&gt; ? å¦‚æœåªæœ‰ &lt;MASK&gt;, é‚£ä¹ˆè¿™ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ˜¯æœ‰åç½®çš„ï¼Œä¹Ÿå°±æ˜¯å­¦åˆ°ä¸€ç§æ–¹å¼ï¼Œç”¨ä¸Šä¸‹æ–‡å»é¢„æµ‹ä¸€ä¸ªè¯ã€‚è¿™å¯¼è‡´åœ¨ fine-tune æ—¶ï¼Œä¼šä¸¢ä¸€éƒ¨åˆ†ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯çŸ¥ä¹å¤§ä½¬ç¬¬ä¸€éƒ¨åˆ†æ‰€è¯´çš„ã€‚ æ‰€ä»¥åŠ ä¸Š random å’Œ ture token æ˜¯è®©æ¨¡å‹çŸ¥é“ï¼Œæ¯ä¸ªè¯éƒ½æ˜¯æœ‰æ„ä¹‰çš„ï¼Œé™¤äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿˜è¦ç”¨åˆ°å®ƒæœ¬èº«çš„ä¿¡æ¯ï¼Œå³ä½¿æ˜¯ &lt;MASK&gt;. ä¹Ÿå°±æ˜¯çŸ¥ä¹ä¸Šè¯´çš„ï¼Œæå–è¿™ä¸¤æ–¹é¢çš„ä¿¡æ¯ã€‚ å†å›è¿‡å¤´ï¼Œä»è¯­è¨€æ¨¡å‹çš„è§’åº¦æ¥çœ‹ï¼Œä¾ç„¶æ˜¯éœ€è¦é¢„æµ‹æ¯ä¸€ä¸ªè¯ï¼Œä½†æ˜¯ç»å¤§å¤šæ•°è¯å®ƒçš„ cross entropy loss ä¼šå¾ˆå°ï¼Œè€Œä¸»è¦å»ä¼˜åŒ–å¾—åˆ° &lt;MASK&gt; å¯¹åº”çš„è¯ã€‚è€Œ random/true token å‘Šè¯‰æ¨¡å‹ï¼Œä½ éœ€è¦æé˜²æ¯ä¸€ä¸ªè¯ï¼Œä»–ä»¬ä¹Ÿéœ€è¦å¥½å¥½é¢„æµ‹ï¼Œå› ä¸ºä»–ä»¬ä¸ä¸€å®šå°±æ˜¯å¯¹çš„ã€‚ æ„Ÿè°¢çŸ¥ä¹å¤§ä½¬ï¼ random tokens ä¼š confuse æ¨¡å‹å—ï¼Ÿä¸ä¼šï¼Œ random tokens åªå  15% * 10% = 1.5%. è¿™ä¸ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚ è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œ &lt;MASK&gt; æ‰€å çš„æ¯”ä¾‹å¾ˆå°ï¼Œä¸»è¦ä¼˜åŒ–å¯¹è±¡è¿­ä»£ä¸€æ¬¡å¯¹æ•´ä¸ªæ¨¡å‹å½±å“ä¼šå¾ˆå°ï¼Œå› è€Œéœ€è¦æ›´å¤šæ¬¡è¿­ä»£. next sentence generationå¯¹äºä¸‹æ¸¸æ˜¯ Question Answering(QA), Natural Language Inference(NLI) è¿™æ ·éœ€è¦ç†è§£å¥å­ä¹‹é—´çš„ç›¸å…³æ€§çš„ä»»åŠ¡ï¼Œä»…ä»…é€šè¿‡è¯­è¨€æ¨¡å‹å¹¶ä¸èƒ½è·å¾—è¿™æ–¹é¢çš„ä¿¡æ¯ã€‚ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿç†è§£å¥å­ä¹‹é—´çš„å…³ç³»ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ª binarized next sentence prediction. å…·ä½“æ–¹å¼æ˜¯ï¼š 50% æ˜¯æ­£ç¡®çš„ç›¸é‚»çš„å¥å­ã€‚ 50% æ˜¯éšæœºé€‰å–çš„ä¸€ä¸ªå¥å­ã€‚è¿™ä¸ªä»»åŠ¡åœ¨é¢„è®­ç»ƒä¸­èƒ½è¾¾åˆ° 97%-98% çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”èƒ½å¾ˆæ˜¾è‘—çš„æé«˜ QA NLI çš„ä»»åŠ¡ã€‚ pre-training procudureä½œè€…é¢„è®­ç»ƒä½¿ç”¨çš„è¯­æ–™ï¼šBooksCorpus (800M words)ï¼ŒEnglish Wikipedia (2,500M words)ã€‚ ä½¿ç”¨æ–‡æ¡£çº§åˆ«çš„è¯­æ–™å¾ˆå…³é”®ï¼Œè€Œä¸æ˜¯ shffule çš„å¥å­çº§åˆ«çš„è¯­æ–™ï¼Œè¿™æ ·å¯ä»¥è·å¾—æ›´é•¿çš„ sentence. è·å¾—è®­ç»ƒæ ·æœ¬ï¼šä»é¢„æ–™åº“ä¸­æŠ½å–å¥å­å¯¹ï¼Œå…¶ä¸­ 50% çš„ä¸¤ä¸ªå¥å­ä¹‹é—´æ˜¯ç¡®å®ç›¸é‚»çš„ï¼Œ50% çš„ç¬¬äºŒä¸ªå¥å­æ˜¯éšæœºæŠ½å–çš„ã€‚å…·ä½“æ“ä½œçœ‹ä»£ç å§ batch_size 256. æ¯ä¸€ä¸ª sentences å¯¹ï¼š 512 tokens 40 epochs Adam lr=1e-4, $\\beta_1=0.9$, $\\beta_2=0.999$, L2 weight decay 0.01 learning rate warmup 10000 steps 0.1 dropout gelu instead of relu Fine-tune proceduresequence-level tasks æ¯”å¦‚ sentences pairs çš„ Quora Question Pairs(QQP) é¢„æµ‹ä¸¤ä¸ªå¥å­ä¹‹é—´è¯­ä¹‰æ˜¯å¦ç›¸åŒã€‚å¦‚ä¸‹å›¾ä¸­ï¼ˆaï¼‰. å¦‚æœæ˜¯ single sentence classification æ¯”å¦‚ Stanford Sentiment Treebankï¼ˆSST-2ï¼‰å’Œ Corpus of Linguistic Acceptabilityï¼ˆCoLAï¼‰è¿™ç§åˆ†ç±»é—®é¢˜ã€‚å¦‚ä¸‹å›¾ï¼ˆbï¼‰ åªéœ€è¦è¾“å‡º Transformer æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ä¸­çš„ç¬¬ä¸€ä¸ª tokenï¼Œä¹Ÿå°±æ˜¯ [CLS]. ç„¶åæ¥ä¸Šä¸€ä¸ªå…¨é“¾æ¥æ˜ å°„åˆ°ç›¸åº”çš„ label ç©ºé—´å³å¯ã€‚ fine-tune æ—¶çš„è¶…å‚æ•°è·Ÿ pre-training æ—¶çš„å‚æ•°å¤§è‡´ç›¸åŒã€‚ä½†æ˜¯è®­ç»ƒé€Ÿåº¦ä¼šå¾ˆå¿« Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Number of epochs: 3, 4 è¯­æ–™åº“è¶Šå¤§ï¼Œå¯¹å‚æ•°çš„æ•æ„Ÿåº¦è¶Šå°ã€‚ token-level tasks. å¯¹äºtoken-level classification(ä¾‹å¦‚NER)ï¼Œå–æ‰€æœ‰tokençš„æœ€åå±‚transformerè¾“å‡ºï¼Œå–‚ç»™softmaxå±‚åšåˆ†ç±»ã€‚ å¦‚ä½•ä½¿ç”¨ BERTæ–‡æœ¬åˆ†ç±»https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py ä¸»è¦æ¶‰åŠåˆ°ä¸¤ä¸ª ç±»: æ•°æ®é¢„å¤„ç† é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ 12345678910111213141516171819from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertConfig, BertAdamï¼Œ PYTORCH_PRETRAINED_BERT_CACHEtokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)tokenizer = BertTokenizer.from_pretrained(&quot;./pre_trained_models/bert-base-uncased-vocab.txt&quot;)model = BertForSequenceClassification.from_pretrained('bert-base-uncased', cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(args.local_rank), num_labels = num_labels)model = BertForSequenceClassification.from_pretrained(&quot;pre_trained_models/bert-base-uncased.tar.gz&quot;, num_labels=2) å…¶ä¸­ bert-base-uncased å¯ä»¥åˆ†åˆ«ç”¨å…·ä½“çš„ è¯è¡¨æ–‡ä»¶ å’Œ æ¨¡å‹æ–‡ä»¶ ä»£æ›¿ã€‚ä»æºä»£ç ä¸­æä¾›çš„é“¾æ¥ä¸‹è½½å³å¯ã€‚ æ•°æ®å¤„ç†123456789from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertConfig, BertAdamï¼Œ PYTORCH_PRETRAINED_BERT_CACHEtokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)tokenizer = BertTokenizer.from_pretrained(&quot;./pre_trained_models/bert-base-uncased-vocab.txt&quot;) å‰ä¸€ç§æ–¹å¼æ˜¯æ ¹æ®ä»£ç ä¸­æä¾›çš„ url å»ä¸‹è½½è¯è¡¨æ–‡ä»¶ï¼Œç„¶åç¼“å­˜åœ¨é»˜è®¤æ–‡ä»¶å¤¹ä¸‹ /home/panxie/.pytorch_pretrained_bert ã€‚åè€…æ˜¯ç›´æ¥ä¸‹è½½è¯è¡¨æ–‡ä»¶åï¼Œæ”¾åœ¨æœ¬åœ°ã€‚ç›¸å¯¹æ¥è¯´ï¼Œåè€…æ›´æ–¹ä¾¿ã€‚ è¿™éƒ¨åˆ†ä»£ç ç›¸å¯¹æ¯”è¾ƒç®€å•ï¼Œæ ¹æ®è‡ªå·±çš„ä»»åŠ¡ï¼Œç»§æ‰¿ DataProcessor è¿™ä¸ªç±»å³å¯ã€‚ ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œfeatures ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼š input_ids æ˜¯é€šè¿‡è¯å…¸æ˜ å°„æ¥çš„ input_mask åœ¨ fine-tune é˜¶æ®µï¼Œæ‰€æœ‰çš„è¯éƒ½æ˜¯ 1, padding çš„æ˜¯ 0 segment_ids åœ¨ text_a ä¸­æ˜¯ 0, åœ¨ text_b ä¸­æ˜¯ 1, padding çš„æ˜¯ 0 è¿™é‡Œå¯¹åº”äº†å‰é¢æ‰€è¯´çš„ï¼Œinput_idx å°±æ˜¯ token embedding, segment_ids å°±æ˜¯ Sentence Embedding. è€Œ input_mask åˆ™è¡¨ç¤ºå“ªäº›ä½ç½®è¢« mask äº†ï¼Œåœ¨ fine-tune é˜¶æ®µéƒ½æ˜¯ 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹123456789!tar -tf pre_trained_models/bert-base-uncased.tar.gz./pytorch_model.bin./bert_config.json ä¸‹è½½å¥½çš„æ–‡ä»¶åŒ…ä¸­å«æœ‰ä¸¤ä¸ªæ–‡ä»¶ï¼Œåˆ†åˆ«æ˜¯ config ä¿¡æ¯ï¼Œä»¥åŠæ¨¡å‹å‚æ•°ã€‚ å¦‚æœä¸ç”¨å…·ä½“çš„æ–‡ä»¶ï¼Œåˆ™éœ€è¦ä»ä»£ç ä¸­æä¾›çš„ url ä¸‹è½½ï¼Œå¹¶ç¼“å­˜åœ¨é»˜è®¤æ–‡ä»¶å¤¹ PYTORCH_PRETRAINED_BERT_CACHE = /home/panxie/.pytorch_pretrained_bert ä½œä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œ num_labels å‚æ•°é»˜è®¤ä¸º 2. è¿è¡Œæ—¶ä¼šå‘ç°æå–é¢„è®­ç»ƒæ¨¡å‹ä¼šè¾“å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565712/26/2018 17:00:41 - INFO - pytorch_pretrained_bert.modeling - loading archive file pre_trained_models/bert-base-uncased.tar.gz12/26/2018 17:00:41 - INFO - pytorch_pretrained_bert.modeling - extracting archive file pre_trained_models/bert-base-uncased.tar.gz to temp dir /tmp/tmpgm506dcx12/26/2018 17:00:44 - INFO - pytorch_pretrained_bert.modeling - Model config { &quot;attention_probs_dropout_prob&quot;: 0.1, &quot;hidden_act&quot;: &quot;gelu&quot;, &quot;hidden_dropout_prob&quot;: 0.1, &quot;hidden_size&quot;: 768, &quot;initializer_range&quot;: 0.02, &quot;intermediate_size&quot;: 3072, &quot;max_position_embeddings&quot;: 512, &quot;num_attention_heads&quot;: 12, &quot;num_hidden_layers&quot;: 12, &quot;type_vocab_size&quot;: 2, &quot;vocab_size&quot;: 30522}12/26/2018 17:00:45 - INFO - pytorch_pretrained_bert.modeling - Weights of BertForSequenceClassification not initialized from pretrained model:['classifier.weight', 'classifier.bias']12/26/2018 17:00:45 - INFO - pytorch_pretrained_bert.modeling - Weights from pretrained model not used in BertForSequenceClassification:['cls.predictions.bias', 'cls.predictions.transform.dense.weight','cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight','cls.seq_relationship.weight', 'cls.seq_relationship.bias','cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] ä¸å¾—ä¸å»è§‚å¯Ÿ from_pretrained çš„æºç ï¼šhttps://github.com/huggingface/pytorch-pretrained-BERT/blob/8da280ebbeca5ebd7561fd05af78c65df9161f92/pytorch_pretrained_bert/modeling.py#L448 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455missing_keys = []unexpected_keys = []error_msgs = []# copy state_dict so _load_from_state_dict can modify itmetadata = getattr(state_dict, '_metadata', None)state_dict = state_dict.copy()if metadata is not None: state_dict._metadata = metadatadef load(module, prefix=''): local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {}) module._load_from_state_dict( state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs) for name, child in module._modules.items(): if child is not None: load(child, prefix + name + '.')load(model, prefix='' if hasattr(model, 'bert') else 'bert.')if len(missing_keys) &gt; 0: logger.info(&quot;Weights of {} not initialized from pretrained model: {}&quot;.format( model.__class__.__name__, missing_keys))if len(unexpected_keys) &gt; 0: logger.info(&quot;Weights from pretrained model not used in {}: {}&quot;.format( model.__class__.__name__, unexpected_keys))if tempdir: # Clean up temp dir shutil.rmtree(tempdir)return model è¿™éƒ¨åˆ†å†…å®¹è§£é‡Šäº†å¦‚ä½•æå–æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°. missing_keys è¿™é‡Œæ˜¯æ²¡æœ‰ä»é¢„è®­ç»ƒæ¨¡å‹æå–å‚æ•°çš„éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯ classifier ['classifier.weight', 'classifier.bias']å±‚ï¼Œå› ä¸ºè¿™ä¸€å±‚æ˜¯åˆ†ç±»ä»»åŠ¡ç‹¬æœ‰çš„ã€‚ unexpected_keys åˆ™æ˜¯å¯¹äºåˆ†ç±»ä»»åŠ¡ä¸éœ€è¦çš„ï¼Œä½†æ˜¯åœ¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­æ˜¯å­˜åœ¨çš„ã€‚æŸ¥çœ‹ BertForMaskedLM çš„æ¨¡å‹å°±èƒ½çœ‹åˆ°ï¼Œcls å±‚ï¼Œæ˜¯ä¸“å±äºè¯­è¨€æ¨¡å‹çš„ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­éƒ½éœ€è¦å»æ‰ã€‚ æ‰€ä»¥è¿™éƒ¨åˆ†ä»£ç å®é™…ä¸Šå­¦åˆ°äº†å¦‚ä½•é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï½ï½æ£’å•Šï¼","link":"/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"title":"è¿ç§»å­¦ä¹ ç³»åˆ—-2-Combining semi-supervised learning with transfer learning","text":"paperStrong Baselines for Neural Semi-Supervised Learning under Domain Shift motivationè¿™ç¯‡paperçš„å·¥ä½œå°±æ˜¯æå‡ºäº†ä¸€ä¸ªç»å…¸æ–¹æ³•å®ç°çš„strong baseline.ä»–çš„motivationå°±æ˜¯å‰é¢å¾ˆå¤šç ”ç©¶æ¯”å¦‚åŸºäºdeep learningçš„ï¼Œå¯¹æ¯”çš„ç»å…¸ç®—æ³•éƒ½å¾ˆweakï¼Œæˆ–è€…æ˜¯åœ¨ä¸“æœ‰çš„æ•°æ®é›†ä¸Šè·‘ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰ã€‚ å¯¹æ¯”çš„ä¸‰ç§ä¼ ç»Ÿæ–¹æ³•ï¼Œ self-traning, tritraining, tri-training with disagreement self-training: ä½¿ç”¨æœ‰æ ‡ç­¾çš„æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹ ç”¨è¿™ä¸ªæ¨¡å‹å»é¢„æµ‹æ— æ ‡ç­¾çš„æ•°æ®ï¼Œå¾—åˆ°å¯¹åº”æ ·æœ¬å±äºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ é€‰æ‹©ä¸€ä¸ªé˜ˆå€¼ï¼Œå¤§äºè¿™ä¸ªé˜ˆå€¼çš„æ ·æœ¬ï¼Œå¯ä»¥æ‰“ä¸Šä¼ªæ ‡ç­¾ã€‚ä½†æ˜¯é€šå¸¸æ¥è¯´ï¼Œé˜ˆå€¼ä¸å¤ªå¥½ç¡®å®šï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ç›¸å¯¹é˜ˆå€¼ï¼Œä¹Ÿå°±æ˜¯é€‰å–æ¦‚ç‡ç›¸å¯¹è¾ƒé«˜çš„ top N. æ¨¡å‹çš„ç¼ºç‚¹åœ¨äºï¼šå¦‚æœé¢„æµ‹é”™äº†æŸäº›æ ·æœ¬ï¼Œé‚£ä¹ˆé”™è¯¯ä¼šç´¯ç§¯å¹¶æ”¾å¤§ã€‚ tri-training: ä½¿ç”¨æœ‰æ ‡ç­¾çš„æ•°æ®ï¼Œè®­ç»ƒä¸‰ä¸ªæ¨¡å‹ m1, m2, m3 ä½¿ç”¨ bootstrapping çš„æ–¹æ³•ï¼Œsampleéƒ¨åˆ†æ— æ ‡ç­¾çš„æ•°æ®ï¼Œç„¶åä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå½“ m1 é¢„æµ‹æ ·æœ¬å±äºæŸä¸€ç±»çš„æ¦‚ç‡ä½æ—¶ï¼Œè€Œ m2, m3 é¢„æµ‹æ ·æœ¬å±äºè¿™ä¸€ç±»çš„æ¦‚ç‡é«˜æ—¶ï¼Œå°†è¿™ä¸ªæ ·æœ¬æ‰“ä¸Šä¼ªæ ‡ç­¾ï¼ŒåŠ å…¥åˆ° m1 çš„è®­ç»ƒé›†ä¸­å» è¿­ä»£è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°åˆ†ç±»å™¨ä¸åœ¨å˜åŒ–ï¼Œå¯ä»¥åŒæ—¶æ›´æ–°ä¸‰ä¸ªåˆ†ç±»å™¨ï¼Ÿ motivationï¼šæ¨¡å‹åº”è¯¥å¢å¼ºå®ƒç›¸å¯¹è¾ƒå¼±çš„åœ°æ–¹ã€‚å…¶å®ä¹Ÿå°±æ˜¯ ensamble çš„ sense. ç¼ºç‚¹ï¼šè®¡ç®—é‡å¤ªå¤§ï¼Œ è€—è´¹æ—¶é—´å’Œç©ºé—´ multi-task tritraining: å¤šä»»åŠ¡è®­ç»ƒï¼Œè¿™é‡Œçš„ä»»åŠ¡å…¶å®å¯ä»¥çœ‹ä½œæ˜¯ä¸€è‡´çš„ï¼Œåº•å±‚ encoder å±‚å‚æ•°å…±äº«ï¼Œsoftmaxå±‚ï¼Œä¹Ÿå°±æ˜¯ decoder å±‚å‚æ•°ä¸ä¸€è‡´ã€‚ è¦å°½å¯èƒ½è®© m1, m2 å…·æœ‰å·®å¼‚æ€§ diversityï¼ŒåŠ ä¸Šäº†æ­£åˆ™åŒ–é¡¹ æ¨¡å‹ m3 åªåœ¨ä¼ªæ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å…¶ç›®çš„æ˜¯è®©æ¨¡å‹åœ¨ domain shift æƒ…å†µä¸‹é²æ£’æ€§æ›´å¼ºã€‚ paper2Semi-Supervised Sequence Modeling with Cross-View Training (EMNLP 2018)","link":"/2019/03/06/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-2-Combining-semi-supervised-learning-with-transfer-learning/"},{"title":"è¿ç§»å­¦ä¹ ç³»åˆ—-3-ç‹æ™‹ä¸œè¿ç§»å­¦ä¹ æ‰‹å†Œé˜…è¯»","text":"è¿ç§»å­¦ä¹ çš„å®šä¹‰è¿ç§»å­¦ä¹ ï¼Œæ˜¯æŒ‡åˆ©ç”¨æ•°æ®ã€ä»»åŠ¡ã€æˆ–æ¨¡å‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°†åœ¨æ—§é¢†åŸŸå­¦ä¹ è¿‡çš„æ¨¡å‹ï¼Œåº”ç”¨äºæ–°é¢†åŸŸçš„ä¸€ç§å­¦ä¹ è¿‡ç¨‹ã€‚ ç»¼è¿°æ–‡ç« ï¼š A survey on transfer learning [Pan and Yang, 2010] ä¸ºä»€ä¹ˆè¦å­¦ä¹ è¿ç§»å­¦ä¹ ï¼Ÿ ç¼ºå°‘æ•°æ®æ ‡æ³¨ ç¼ºå°‘è¶³å¤Ÿç®—åŠ› æ™®é€‚åŒ–æ¨¡å‹ä¸ä¸ªæ€§åŒ–éœ€æ±‚ä¹‹é—´çš„çŸ›ç›¾ ç‰¹å®šåº”ç”¨éœ€æ±‚ è¿ç§»å­¦ä¹ å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ï¼š å¤§æ•°æ®ä¸å°‘æ ‡æ³¨ï¼šè¿ç§»æ•°æ®æ ‡æ³¨ å¤§æ•°æ®ä¸å¼±è®¡ç®—ï¼šæ¨¡å‹è¿ç§» æ™®é€‚åŒ–æ¨¡å‹ä¸ä¸ªæ€§åŒ–éœ€æ±‚ï¼šè‡ªé€‚åº”å­¦ä¹  ç‰¹å®šåº”ç”¨çš„éœ€æ±‚ï¼šç›¸ä¼¼é¢†åŸŸçŸ¥è¯†è¿ç§»ï¼ˆæ¯”å¦‚cross-lingualï¼‰ è¿ç§»å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«ï¼š è¿ç§»å­¦ä¹ ä¸é¢†åŸŸè‡ªé€‚åº”çš„åŒºåˆ«ï¼š é¢†åŸŸè‡ªé€‚åº”é—®é¢˜æ˜¯è¿ç§»å­¦ä¹ çš„ç ”ç©¶å†…å®¹ä¹‹ä¸€ï¼Œå®ƒä¾§é‡äºè§£å†³ç‰¹å¾ç©ºé—´ä¸€è‡´ã€ç±»åˆ«ç©ºé—´ä¸€è‡´ï¼Œä»…ç‰¹å¾åˆ†å¸ƒä¸ä¸€è‡´çš„é—®é¢˜ã€‚è€Œè¿ç§»å­¦ä¹ ä¹Ÿå¯ä»¥è§£å†³ä¸Šè¿°å†…å®¹ä¸ä¸€è‡´çš„æƒ…å†µã€‚ è¿ç§»å­¦ä¹ çš„å¸¸ç”¨åˆ†ç±» æŒ‰ç…§ç›®æ ‡åŸŸæ ‡ç­¾åˆ†ç±» ç›‘ç£è¿ç§»å­¦ä¹  (Supervised Transfer Learning) åŠç›‘ç£è¿ç§»å­¦ä¹  (Semi-Supervised Transfer Learning) æ— ç›‘ç£è¿ç§»å­¦ä¹  (Unsupervised Transfer Learning) æŒ‰ç…§å­¦ä¹ æ–¹æ³•åˆ†ç±» åŸºäºå®ä¾‹çš„è¿ç§»å­¦ä¹ æ–¹æ³• (Instance based Transfer Learning)ï¼š åŸºäºç‰¹å¾çš„è¿ç§»å­¦ä¹ æ–¹æ³• (Feature based Transfer Learning) åŸºäºæ¨¡å‹çš„è¿ç§»å­¦ä¹ æ–¹æ³• (Model based Transfer Learning) åŸºäºå…³ç³»çš„è¿ç§»å­¦ä¹ æ–¹æ³• (Relation based Transfer Learning) åŸºäºå®ä¾‹çš„è¿ç§»ï¼Œç®€å•æ¥è¯´å°±æ˜¯é€šè¿‡æƒé‡é‡ç”¨ï¼Œå¯¹æºåŸŸå’Œç›®æ ‡åŸŸçš„æ ·ä¾‹è¿›è¡Œè¿ç§»ã€‚å°±æ˜¯è¯´ç›´æ¥å¯¹ä¸åŒçš„æ ·æœ¬èµ‹äºˆä¸åŒæƒé‡ï¼Œæ¯”å¦‚è¯´ç›¸ä¼¼çš„æ ·æœ¬ï¼Œæˆ‘å°±ç»™å®ƒé«˜æƒé‡ï¼Œè¿™æ ·æˆ‘å°±å®Œæˆäº† è¿ç§»ï¼Œéå¸¸ç®€å•éå¸¸éå¸¸ç›´æ¥ã€‚ åŸºäºç‰¹å¾çš„è¿ç§»ï¼Œå°±æ˜¯æ›´è¿›ä¸€æ­¥å¯¹ç‰¹å¾è¿›è¡Œå˜æ¢ã€‚æ„æ€æ˜¯è¯´ï¼Œå‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸçš„ç‰¹å¾ åŸæ¥ä¸åœ¨ä¸€ä¸ªç©ºé—´ï¼Œæˆ–è€…è¯´å®ƒä»¬åœ¨åŸæ¥é‚£ä¸ªç©ºé—´ä¸Šä¸ç›¸ä¼¼ï¼Œé‚£æˆ‘ä»¬å°±æƒ³åŠæ³•æŠŠå®ƒä»¬å˜æ¢åˆ°ä¸€ä¸ªç©ºé—´é‡Œé¢ï¼Œé‚£è¿™äº›ç‰¹å¾ä¸å°±ç›¸ä¼¼äº†ï¼Ÿè¿™ä¸ªæ€è·¯ä¹Ÿéå¸¸ç›´æ¥ã€‚è¿™ä¸ªæ–¹æ³•æ˜¯ç”¨å¾—éå¸¸å¤šçš„ï¼Œä¸€ ç›´åœ¨ç ”ç©¶ï¼Œç›®å‰æ˜¯æ„Ÿè§‰æ˜¯ç ”ç©¶æœ€çƒ­çš„ã€‚ åŸºäºæ¨¡å‹çš„è¿ç§»ï¼Œå°±æ˜¯è¯´æ„å»ºå‚æ•°å…±äº«çš„æ¨¡å‹ã€‚è¿™ä¸ªä¸»è¦å°±æ˜¯åœ¨ç¥ç»ç½‘ç»œé‡Œé¢ç”¨çš„ç‰¹åˆ«å¤šï¼Œå› ä¸ºç¥ç»ç½‘ç»œçš„ç»“æ„å¯ä»¥ç›´æ¥è¿›è¡Œè¿ç§»ã€‚æ¯”å¦‚è¯´ç¥ç»ç½‘ç»œæœ€ç»å…¸çš„ finetune å°±æ˜¯æ¨¡å‹å‚æ•°è¿ç§»çš„å¾ˆå¥½çš„ä½“ç°ã€‚ åŸºäºå…³ç³»çš„è¿ç§»ï¼Œè¿™ä¸ªæ–¹æ³•ç”¨çš„æ¯”è¾ƒå°‘ï¼Œè¿™ä¸ªä¸»è¦å°±æ˜¯è¯´æŒ–æ˜å’Œåˆ©ç”¨å…³ç³»è¿›è¡Œç±»æ¯”è¿ç§»ã€‚æ¯”å¦‚è€å¸ˆä¸Šè¯¾ã€å­¦ç”Ÿå¬è¯¾å°±å¯ä»¥ç±»æ¯”ä¸ºå…¬å¸å¼€ä¼šçš„åœºæ™¯ã€‚è¿™ä¸ªå°±æ˜¯ä¸€ç§å…³ç³»çš„è¿ç§»ã€‚ æŒ‰ç…§ç‰¹å¾åˆ†ç±» åŒæ„è¿ç§»å­¦ä¹  (Homogeneous Transfer Learning) å¼‚æ„è¿ç§»å­¦ä¹  (Heterogeneous Transfer Learning) è¿™ä¹Ÿæ˜¯ä¸€ç§å¾ˆç›´è§‚çš„æ–¹å¼ï¼šå¦‚æœç‰¹å¾è¯­ä¹‰å’Œç»´åº¦éƒ½ç›¸åŒï¼Œé‚£ä¹ˆå°±æ˜¯åŒæ„ï¼›åä¹‹ï¼Œå¦‚æœç‰¹å¾å®Œå…¨ä¸ç›¸åŒï¼Œé‚£ä¹ˆå°±æ˜¯å¼‚æ„ã€‚ä¸¾ä¸ªä¾‹å­æ¥è¯´ï¼Œä¸åŒå›¾ç‰‡çš„è¿ç§»ï¼Œå°±å¯ä»¥è®¤ä¸ºæ˜¯åŒæ„ï¼›è€Œå›¾ç‰‡åˆ°æ–‡æœ¬çš„è¿ç§»ï¼Œåˆ™æ˜¯å¼‚æ„çš„ã€‚ æŒ‰ç¦»çº¿ä¸åœ¨çº¿å½¢å¼åˆ† ç¦»çº¿è¿ç§»å­¦ä¹  (Offline Transfer Learning) åœ¨çº¿è¿ç§»å­¦ä¹  (Online Transfer Learning) è¿ç§»å­¦ä¹ çš„åº”ç”¨ è®¡ç®—æœºè§†è§‰åœ¨ CV é¢†åŸŸï¼Œè¿ç§»å­¦ä¹ ä¸»è¦æ˜¯æ–¹æ³•æ˜¯é¢†åŸŸè‡ªé€‚åº” domain adaption. ä¾§é‡äºè§£å†³ç‰¹å¾ç©ºé—´ä¸€è‡´ã€ç±»åˆ«ç©ºé—´ä¸€è‡´ï¼Œä»…ç‰¹å¾åˆ†å¸ƒä¸ä¸€è‡´çš„é—®é¢˜ã€‚ ä¸ªäººç†è§£ï¼šåœ¨å›¾åƒä¸Šï¼Œå³ä½¿ç±»åˆ«å·®åˆ«å¾ˆå¤§ï¼Œä½†æ˜¯åœ¨ç‰¹å¾ç©ºé—´ä¸Šä»ç„¶æ˜¯ä¸€è‡´çš„ï¼Œæˆ–è€…è¯´æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚æ¯”å¦‚äººå’Œç‹—ï¼Œä»è½®å»“ã€é¢œè‰²ã€äº”å®˜ç­‰ç­‰ç‰¹å¾éƒ½æ˜¯å…·æœ‰å¯è¿ç§»æ€§çš„ã€‚å†æ¯”å¦‚äººå’Œæ¡Œå­ï¼Œä¹Ÿæ˜¯å…·æœ‰ç›¸ä¼¼ç‰¹å¾çš„ã€‚ é‚£ä¹ˆé—®é¢˜æ˜¯ï¼Œç‰¹å¾ç©ºé—´å®Œå…¨ä¸€è‡´å—ï¼Ÿä¸ä¸€è‡´çš„éƒ¨åˆ†å‘¢ï¼Ÿ æ–‡æœ¬åˆ†ç±»ç”±äºæ–‡æœ¬æ•°æ®æœ‰å…¶é¢†åŸŸç‰¹æ®Šæ€§ï¼Œå› æ­¤ï¼Œåœ¨ä¸€ä¸ªé¢†åŸŸä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œä¸èƒ½ç›´æ¥æ‹¿æ¥ä½œç”¨åˆ°å¦ä¸€ä¸ªé¢†åŸŸä¸Šã€‚è¿™å°±éœ€è¦ç”¨åˆ°è¿ç§»å­¦ä¹ ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”µå½±è¯„è®ºæ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒå¥½çš„åˆ†ç±»å™¨ï¼Œä¸èƒ½ç›´æ¥ç”¨äºå›¾ä¹¦è¯„è®ºçš„é¢„æµ‹ã€‚è¿™å°±éœ€è¦è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚å›¾ 11æ˜¯ä¸€ä¸ªç”±ç”µå­äº§å“è¯„è®º è¿ç§»åˆ° DVD è¯„è®ºçš„è¿ç§»å­¦ä¹ ä»»åŠ¡ã€‚ é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œè¿™ç§ä»ä¸€ä¸ªdomain è¿ç§»åˆ°å¦ä¸€ä¸ª domainï¼Œåˆ°åº•æ”¹å˜äº†ä»€ä¹ˆï¼Ÿæ¯”å¦‚åŒæ ·ä¸€ä¸ªè¯ï¼Œåœ¨ ç”µå­äº§å“ä¸­å¯¹åº”çš„ vector ç»è¿‡è¿ç§»åï¼ˆfinetune?ï¼‰ï¼Œå‘ç”Ÿäº†æ€æ ·çš„å˜åŒ–ï¼Œè¿™ä¸ªå¯ä»¥å¯è§†åŒ–å˜›ï¼Ÿ æ—¶é—´åºåˆ— è¡Œä¸ºè¯†åˆ«(Activity Recognition) å®¤å†…å®šä½ (Indoor Location) åŒ»ç–—å¥åº·ä¸åŒäºå…¶ä»–é¢†åŸŸï¼ŒåŒ»ç–—é¢†åŸŸç ”ç©¶çš„éš¾ç‚¹é—®é¢˜æ˜¯ï¼Œæ— æ³•è·å–è¶³å¤Ÿæœ‰æ•ˆçš„åŒ»ç–—æ•°æ®ã€‚åœ¨è¿™ä¸€é¢†åŸŸï¼Œè¿ç§»å­¦ä¹ åŒæ ·ä¹Ÿå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚","link":"/2019/03/08/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-3-%E7%8E%8B%E6%99%8B%E4%B8%9C%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%E9%98%85%E8%AF%BB/"},{"title":"è¿ç§»å­¦ä¹ ç³»åˆ— 1-Neural Transfer Learning for NLP","text":"è¿ç§»å­¦ä¹ ä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ« training domain å’Œ target domain ä¸ä¸€è‡´æ—¶ï¼Œéœ€è¦çŸ¥è¯†è¿ç§»ã€‚ é‚£ä¹ˆæš‚æ—¶çš„é—®é¢˜æ¥äº†ï¼Ÿ 1.å¦‚ä½•ç•Œå®š domain çš„èŒƒå›´ï¼Œå°¤å…¶æ˜¯NLPé¢†åŸŸã€‚ä»åŒ»å­¦æ–‡æœ¬èƒ½è¿ç§»åˆ°ç§‘å¹»å°è¯´å—ï¼Œæ„Ÿè§‰ä¸å¯ä»¥ã€‚ã€‚ 2.ä» big domain åˆ° small domain çš„è¿ç§»ä¹Ÿæ˜¯å±äºè¿ç§»å­¦ä¹ çš„èŒƒç•´å§ï¼Ÿæ¯”å¦‚åƒ BERT è¿™æ ·åœ¨è¶…å¤§çš„è®­ç»ƒé›†ä¸Šè¿›è¡Œ trainingï¼Œç„¶ååœ¨å°çš„å­é›†ä¸Š fine-tuneï¼Œéƒ½èƒ½è¡¨ç°çš„å¾ˆå¥½æ˜¯å—ï¼Ÿ Why transfer learning ç›®å‰çš„ç›‘ç£æ¨¡å‹ä¾æ—§éå¸¸è„†å¼±ï¼ŒJia and Liang, EMNLP 2017 è¿™ç¯‡ paper è¯æ˜äº†ç›®å‰çš„ SOTA çš„æ¨¡å‹å¯¹å¯¹æŠ—æ ·æœ¬éå¸¸æ•æ„Ÿã€‚ è¿ç§»å­¦ä¹ èƒ½è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Œç–‘æƒ‘ï¼Ÿï¼Ÿ Abstractï¼š Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely. Synthetic and Natural Noise Both Break Neural Machine Translation, Belinkov and Bisk (ICLR 2018) è¿™ç¯‡ paper ä¸­æåˆ°åŸºäºå­—ç¬¦çº§åˆ«çš„ç¿»è¯‘æ¨¡å‹èƒ½æœ‰æ•ˆè§£å†³ OOV ç­‰é—®é¢˜ï¼Œä½†æ˜¯å´ä½¿å¾—æ¨¡å‹å¯¹ noise éå¸¸æ•æ„Ÿä¸”è„†å¼±ã€‚å¦‚æœå‡ºç° phonetic æ‹¼å†™é”™è¯¯ï¼Œomission çœç•¥ï¼Œ key swap å…³é”®å­—æ¯äº¤æ¢ï¼Œéƒ½ä¼šå¯¼è‡´ BLEU å€¼ä¸¥é‡ä¸‹é™ã€‚ Abstract Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. Iyyer et al. (NAACL 2018) è¿™ç¯‡ paper æå‡ºäº†ä¸€ä¸ªå¥æ³•è§„åˆ™æ§åˆ¶ä¸‹çš„é‡Šä¹‰ç”Ÿæˆæ¨¡å‹ï¼Œsyntactically controlled paraphrase networks (SCPNs). ç„¶åå‘ç°è¿™æ ·çš„å¯¹æŠ—æ ·æœ¬å¾ˆå®¹æ˜“æ„šå¼„è®­ç»ƒå¥½çš„ç›‘ç£æ¨¡å‹ã€‚ Abstractï¼š We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoderdecoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled)paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) â€œfoolâ€ pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data äººå·¥æ ‡æ³¨æ‰€æœ‰ domain æˆ–è€…ä»»ä½•è¯­è¨€çš„æ•°æ®æ˜¯ä¸å¯ç†çš„ï¼Œå› æ­¤éœ€è¦ transfering knowledge from a related setting to the target setting. NLP å¾ˆå¤šé‡å¤§çš„åŸºç¡€æ€§çš„ç ”ç©¶éƒ½å¯ä»¥çœ‹ä½œæ˜¯è¿ç§»å­¦ä¹ çš„ä¸€ç§å½¢å¼ã€‚ LSA Brown clusters word embedding å·²æœ‰å·¥ä½œçš„å±€é™æ€§ï¼š é™åˆ¶åº¦å¤ªé«˜ï¼šé¢„è®¾å®šå¥½çš„ç›¸ä¼¼åº¦æŒ‡æ ‡ï¼Œhard å‚æ•°å…±äº« æ¡ä»¶è®¾å®šå¤ªè¿‡äºå…·ä½“ï¼šå•ä¸€çš„ task baseline å¤ªå¼±ï¼šç¼ºå°‘ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯” æ¨¡å‹è„†å¼±ï¼šåœ¨ out-of-domain ä¸workï¼Œä¾èµ–äºç›¸ä¼¼çš„è¯­è¨€/ä»»åŠ¡ æ•ˆç‡ä½ï¼šéœ€è¦å¤§é‡å‚æ•°ï¼Œæ—¶é—´å’Œæ ·æœ¬ ç ”ç©¶ç›®æ ‡ è¿ç§»å­¦ä¹  ä¼ å¯¼å¼è¿ç§»å­¦ä¹ ï¼ˆç›¸åŒçš„ä»»åŠ¡ï¼Œåªæœ‰sourced domainæœ‰labelï¼‰ é¢†åŸŸè‡ªé€‚åº”ï¼ˆä¸åŒçš„ domainï¼‰ è·¨è¯­è¨€å­¦ä¹ ï¼ˆä¸åŒçš„ languageï¼‰ å½’çº³å¼è¿ç§»å­¦ä¹ ï¼ˆä¸åŒçš„ä»»åŠ¡ï¼Œ target domain ä¹Ÿæœ‰æ ‡ç­¾ï¼‰ å¤šä»»åŠ¡å­¦ä¹  åºåˆ—è¿ç§»å­¦ä¹  å¤§ä½¬å¤ªå¼ºäº†ã€‚ã€‚ã€‚ã€‚å¼ºåˆ°çˆ†ç‚¸å•Š domain adaptationPropose two novel methods that bridge the domain discrepancy by selecting relevant and informative data for unsupervised domain adaptation. æå‡ºä¸¤æ–¹æ³•ï¼Œæ›¿æ— ç›‘ç£çš„åŸŸé€‚åº”é€‰æ‹©ç›¸å…³çš„ï¼Œå…·æœ‰ä¿¡æ¯é‡çš„æ•°æ®æ¥å¼¥åˆåŸŸä¹‹é—´çš„å·®å¼‚ã€‚ Based on Bayesian OptimisationLearning to select data for transfer learning with Bayesian Optimization, EMNLP2017 è¿˜ä¸å¤ªæ‡‚ bayesian optimisation: https://zhuanlan.zhihu.com/p/29779000 https://www.jiqizhixin.com/articles/2017-08-18-5 A Tutorial on Bayesian Optimization Using semi-supervised learning and multi-task learningStrong Baselines for Neural Semi-supervised Learning under Domain Shift, Ruder &amp; Plank, ACL 2018 Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state-of-the-art for NLP. Hence classic approaches constitute an important and strong baseline. å¤§ä½¬çš„è®ºæ–‡çœŸçš„éš¾ã€‚ã€‚å¤ª hardcore äº†ã€‚ã€‚ cross-lingual Learning On the Limitations of Unsupervised Bilingual Dictionary Induction A Discriminative Latent-Variable Model for Bilingual Lexicon Induction multi-task learning sequential transfer learning","link":"/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/"},{"title":"é‚“å…¬æ•°æ®ç»“æ„ä¸ç®—æ³•2-å‘é‡","text":"è¿™ç« ä»¥å‘é‡ä¸ºä¾‹ï¼Œè®²è§£äº†å¦‚ä½•å†™æ¥å£å¹¶å®ç°ã€‚ æ¥å£ä¸å®ç°Abstract Data Type vs. Data Structure æŠ½è±¡æ•°æ®ç±»å‹ = æ•°æ®ç±»å‹ + å®šä¹‰åœ¨è¯¥æ¨¡å‹ä¸Šçš„ä¸€äº›åˆ—æ“ä½œ æŠ½è±¡å®šä¹‰ï¼Œ æ“ä½œå’Œå®šä¹‰ ä¸è€ƒè™‘æ—¶é—´å¤æ‚åº¦ï¼Œä¸æ¶‰åŠæ•°æ®çš„å­˜å‚¨æ–¹å¼ æ•°æ®ç»“æ„ = åŸºäºæŸç§ç‰¹å®šçš„è¯­è¨€ï¼Œå®ç° ADT çš„ä¸€æ•´å¥—ç®—æ³•ã€‚ ä¸å¤æ‚åº¦å¯†åˆ‡ç›¸å…³ è¦è€ƒè™‘æ•°æ®çš„å…·ä½“å­˜å‚¨æœºåˆ¶ å‘é‡å‘é‡å°±æ˜¯æŠ½è±¡æ•°æ®ç±»å‹ï¼šä»æ•°ç»„åˆ°å‘é‡ï¼Œå¾ªç§©è®¿é—®ã€‚ æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œé€šè¿‡æ¨¡æ¿ç±»æ¥å®ç° å‘é‡ ADT æ¥å£ size() get(r ) put(r, e) ç”¨eæ›¿æ¢ç§©ä¸º r å…ƒç´ çš„æ•°å€¼ insert(r, e) eä½œä¸ºç§©ä¸ºrå…ƒç´ æ’å…¥ï¼ŒåŸåç»§ä¾æ¬¡ç§»åŠ¨ remove(r) åˆ é™¤ç§©ä¸º r çš„å…ƒç´ ï¼Œè¿”å›è¯¥å…ƒç´ åŸå€¼ disordered() åˆ¤æ–­æ‰€æœ‰å…ƒç´ æ˜¯å¦å·²æŒ‰éé™åºæ’åˆ— sort() è°ƒæ•´å„å…ƒç´ çš„ä½ç½®ï¼Œä½¿ä¹‹æŒ‰éé™åºæ’åˆ— find(e) æŸ¥æ‰¾å…ƒç´  e search(e) æŸ¥æ‰¾eï¼Œè¿”å›ä¸å¤§äºeä¸”ç§©æœ€å¤§çš„å…ƒç´  ä»…é€‚ç”¨äºæœ‰åºå‘é‡ deduplicate(),uniquify() å‰”é™¤é‡å¤å…ƒç´  traverse() éå†å‘é‡å¹¶ç»Ÿä¸€å¤„ç†æ‰€æœ‰å…ƒç´  å‘é‡æ¨¡æ¿ç±»å®šä¹‰é‚£ä¹ˆå¦‚ä½•æ„é€  å‘é‡ çš„æ¨¡æ¿ç±»å‘¢ï¼Ÿ â€œmyvector.hâ€ å¤´æ–‡ä»¶ï¼Œä¸»è¦æ˜¯å¯¹å„ç§æˆå‘˜æ•°æ®ã€æˆå‘˜å‡½æ•°çš„å£°æ˜ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179#ifndef MYVECTOR_H_INCLUDED#define MYVECTOR_H_INCLUDEDtypedef int Rank; //ç§©#define DEFAULT_CAPACITY 3 //é»˜è®¤çš„åˆå§‹å®¹é‡ï¼ˆå®é™…åº”ç”¨ä¸­å¯è®¾ç½®ä¸ºæ›´å¤§ï¼‰template &lt;typename T&gt; class MyVector{protected: //åªèƒ½è¢«æˆå‘˜å‡½æ•°å’Œå‹å…ƒè®¿é—® Rank _size; // è§„æ¨¡ int _capacity; //å®¹é‡ T* _elem; // æ•°æ®åŒº _elem æ˜¯æŒ‡é’ˆï¼Œæ‰€ä»¥å¯ç”¨æ•°ç»„å½¢å¼æ¥è¡¨ç¤º vector_ void copyFrom(T const* A, Rank lo, Rank hi); //å¤åˆ¶æ•°æ®åŒºé—´ A[lo, hi) void expand(); //ç©ºé—´ä¸è¶³æ—¶æ‰©å®¹ void shrink(); //è£…å¡«å› å­è¿‡å°æ—¶å‹ç¼© bool bubble ( Rank lo, Rank hi ); //æ‰«æäº¤æ¢ void bubbleSort ( Rank lo, Rank hi ); //èµ·æ³¡æ’åºç®—æ³• Rank max ( Rank lo, Rank hi ); //é€‰å–æœ€å¤§å…ƒç´  void selectionSort ( Rank lo, Rank hi ); //é€‰æ‹©æ’åºç®—æ³• void merge ( Rank lo, Rank mi, Rank hi ); //å½’å¹¶ç®—æ³• void mergeSort ( Rank lo, Rank hi ); //å½’å¹¶æ’åºç®—æ³• Rank partition ( Rank lo, Rank hi ); //è½´ç‚¹æ„é€ ç®—æ³• void quickSort ( Rank lo, Rank hi ); //å¿«é€Ÿæ’åºç®—æ³• void heapSort ( Rank lo, Rank hi ); //å †æ’åºï¼ˆç¨åç»“åˆå®Œå…¨å †è®²è§£ï¼‰public: // å…¬æœ‰æˆå‘˜ï¼Œæä¾›ç»™ç”¨æˆ·ä½¿ç”¨çš„// æ„é€ å‡½æ•°çš„å‡ ç§å½¢å¼ MyVector ( int c = DEFAULT_CAPACITY, int s = 0, T v = 0 ) //å®¹é‡ä¸ºcã€è§„æ¨¡ä¸ºsã€æ‰€æœ‰å…ƒç´ åˆå§‹ä¸ºv { _elem = new T[_capacity = c]; for ( _size = 0; _size &lt; s; _elem[_size++] = v ); //s&lt;=c } MyVector ( T const* A, Rank n ) { copyFrom ( A, 0, n ); } // æ•°ç»„ æ•´ä½“å¤åˆ¶_ MyVector ( T const* A, Rank lo, Rank hi ) { copyFrom ( A, lo, hi ); } //å¤åˆ¶æ•°ç»„åŒºé—´ MyVector ( MyVector&lt;T&gt; const&amp; V ) { copyFrom ( V._elem, 0, V._size ); } //å¤åˆ¶ å‘é‡ æ•´ä½“_ MyVector ( MyVector&lt;T&gt; const&amp; V, Rank lo, Rank hi ) { copyFrom ( V._elem, lo, hi ); } //å¤åˆ¶å‘é‡åŒºé—´_// ææ„å‡½æ•° ~MyVector() { delete [] _elem; } //é‡Šæ”¾å†…éƒ¨ç©ºé—´_// åªè¯»è®¿é—®æ¥å£ const å…³é”®å­— // é€šå¸¸ä¸€ä¸ªç±»å¦‚æœæƒ³è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå…¶ä¸­ä¸ä¿®æ”¹ç±»æ•°æ®æˆå‘˜çš„æˆå‘˜å‡½æ•°åº”è¯¥å£°æ˜ä¸º const æˆå‘˜å‡½æ•° Rank size() const { return _size; } //è§„æ¨¡_ bool empty() const { return !_size; } //åˆ¤ç©º_ int disordered() const; //åˆ¤æ–­å‘é‡æ˜¯å¦å·²æ’åº Rank find ( T const&amp; e ) const { return find ( e, 0, _size ); } //æ— åºå‘é‡æ•´ä½“æŸ¥æ‰¾_ Rank find ( T const&amp; e, Rank lo, Rank hi ) const; //æ— åºå‘é‡åŒºé—´æŸ¥æ‰¾ Rank search ( T const&amp; e ) const //æœ‰åºå‘é‡æ•´ä½“æŸ¥æ‰¾ { return ( 0 &gt;= _size ) ? -1 : search ( e, 0, _size ); }// å¯å†™è®¿é—®æ¥å£_ T&amp; operator[] ( Rank r ); //é‡è½½ä¸‹æ ‡æ“ä½œç¬¦ï¼Œå¯ä»¥ç±»ä¼¼äºæ•°ç»„å½¢å¼å¼•ç”¨å„å…ƒç´ , å¯ä»¥ä½œä¸ºå·¦å€¼ const T&amp; operator[] ( Rank r ) const; //ä»…é™äºåšå³å€¼çš„é‡è½½ç‰ˆæœ¬ï¼Œconst T&amp; Vector&lt;T&gt; &amp; operator= ( Vector&lt;T&gt; const&amp; ); //é‡è½½èµ‹å€¼æ“ä½œç¬¦ï¼Œä»¥ä¾¿ç›´æ¥å…‹éš†å‘é‡ T remove ( Rank r ); //åˆ é™¤ç§©ä¸ºrçš„å…ƒç´  int remove ( Rank lo, Rank hi ); //åˆ é™¤ç§©åœ¨åŒºé—´[lo, hi)ä¹‹å†…çš„å…ƒç´  Rank insert ( Rank r, T const&amp; e ); //æ’å…¥å…ƒç´  Rank insert ( T const&amp; e ) { return insert ( _size, e ); } //é»˜è®¤ä½œä¸ºæœ«å…ƒç´ æ’å…¥_ void sort ( Rank lo, Rank hi ); //å¯¹[lo, hi)æ’åº void sort() { sort ( 0, _size ); } //æ•´ä½“æ’åº_ void unsort ( Rank lo, Rank hi ); //å¯¹[lo, hi)ç½®ä¹± void unsort() { unsort ( 0, _size ); } //æ•´ä½“ç½®ä¹±_ int deduplicate(); //æ— åºå»é‡ int uniquify(); //æœ‰åºå»é‡// éå† void traverse ( void (* ) ( T&amp; ) ); //éå†ï¼ˆä½¿ç”¨å‡½æ•°æŒ‡é’ˆï¼Œåªè¯»æˆ–å±€éƒ¨æ€§ä¿®æ”¹ template &lt;typename VST&gt; void traverse ( VST&amp; ); //éå†ï¼ˆä½¿ç”¨å‡½æ•°å¯¹è±¡ï¼Œå¯å…¨å±€æ€§ä¿®æ”¹ï¼‰};#endif // MYVECTOR_H_INCLUDED å¯ä»¥çœ‹åˆ°æ¨¡æ¿ç±»å®šä¹‰ä¸»è¦æ˜¯ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š å†…éƒ¨å‡½æ•° æ„é€ å‡½æ•° ææ„å‡½æ•° åªè¯»æ¥å£ å¯å†™æ¥å£ éå†æ¥å£ æ¨¡æ¿ç±»å’Œå‡½æ•°å®šä¹‰å…ˆå¤ä¹ ä¸€ä¸‹æ¨¡æ¿å‡½æ•°å’Œæ¨¡æ¿ç±»çš„æ„é€ ï¼Œå…³é”®å­— typename typenameå’Œclassçš„åŒºåˆ« åœ¨c++ Templateä¸­å¾ˆå¤šåœ°æ–¹éƒ½ç”¨åˆ°äº†typenameä¸classè¿™ä¸¤ä¸ªå…³é”®å­—ï¼Œè€Œä¸”å¥½åƒå¯ä»¥æ›¿æ¢ï¼Œæ˜¯ä¸æ˜¯è¿™ä¸¤ä¸ªå…³é”®å­—å®Œå…¨ä¸€æ ·å‘¢? ç›¸ä¿¡å­¦ä¹ C++çš„äººå¯¹classè¿™ä¸ªå…³é”®å­—éƒ½éå¸¸æ˜ç™½ï¼Œclassç”¨äºå®šä¹‰ç±»ï¼Œåœ¨æ¨¡æ¿å¼•å…¥c++åï¼Œæœ€åˆå®šä¹‰æ¨¡æ¿çš„æ–¹æ³•ä¸ºï¼š template&lt;class T&gt;â€¦â€¦ åœ¨è¿™é‡Œclasså…³é”®å­—è¡¨æ˜Tæ˜¯ä¸€ä¸ªç±»å‹ï¼Œåæ¥ä¸ºäº†é¿å…classåœ¨è¿™ä¸¤ä¸ªåœ°æ–¹çš„ä½¿ç”¨å¯èƒ½ç»™äººå¸¦æ¥æ··æ·†ï¼Œæ‰€ä»¥å¼•å…¥äº†typenameè¿™ä¸ªå…³é”®å­—ï¼Œå®ƒçš„ä½œç”¨åŒ classä¸€æ ·è¡¨æ˜åé¢çš„ç¬¦å·ä¸ºä¸€ä¸ªç±»å‹ï¼Œè¿™æ ·åœ¨å®šä¹‰æ¨¡æ¿çš„æ—¶å€™å°±å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„æ–¹å¼äº†ï¼š template&lt;typename T&gt;â€¦â€¦ åœ¨æ¨¡æ¿å®šä¹‰è¯­æ³•ä¸­å…³é”®å­—classä¸typenameçš„ä½œç”¨å®Œå…¨ä¸€æ ·ã€‚ typenameéš¾é“ä»…ä»…åœ¨æ¨¡æ¿å®šä¹‰ä¸­èµ·ä½œç”¨å—ï¼Ÿ å…¶å®ä¸æ˜¯è¿™æ ·ï¼Œtypenameå¦å¤–ä¸€ä¸ªä½œç”¨ä¸ºï¼šä½¿ç”¨åµŒå¥—ä¾èµ–ç±»å‹(nested depended name)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š 123456789101112131415161718192021222324252627class MyArray{ publicï¼š typedef int LengthType; .....}template&lt;class T&gt;void MyMethod( T myarr ){ typedef typename T::LengthType LengthType; LengthType length = myarr.GetLength;} è¿™ä¸ªæ—¶å€™typenameçš„ä½œç”¨å°±æ˜¯å‘Šè¯‰c++ç¼–è¯‘å™¨ï¼Œtypenameåé¢çš„å­—ç¬¦ä¸²ä¸ºä¸€ä¸ªç±»å‹åç§°ï¼Œè€Œä¸æ˜¯æˆå‘˜å‡½æ•°æˆ–è€…æˆå‘˜å˜é‡ï¼Œè¿™ä¸ªæ—¶å€™å¦‚æœå‰é¢æ²¡æœ‰typenameï¼Œç¼–è¯‘å™¨æ²¡æœ‰ä»»ä½•åŠæ³•çŸ¥é“T::LengthTypeæ˜¯ä¸€ä¸ªç±»å‹è¿˜æ˜¯ä¸€ä¸ªæˆå‘˜åç§°(é™æ€æ•°æ®æˆå‘˜æˆ–è€…é™æ€å‡½æ•°)ï¼Œæ‰€ä»¥ç¼–è¯‘ä¸èƒ½å¤Ÿé€šè¿‡ã€‚ æ¨¡æ¿æˆå‘˜å‡½æ•°å®šä¹‰æˆå‘˜å‡½æ•°çš„æ¨¡æ¿å‡½æ•°ï¼š â€œ#include â€œvector_functions.hâ€â€ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#ifndef VECTOR_CONSTRUCTOR_BY_COPYING_H_INCLUDED#define VECTOR_CONSTRUCTOR_BY_COPYING_H_INCLUDED//åŸºäºå¤åˆ¶çš„æ„é€ å‡½æ•°template &lt;typename T&gt; // å…ƒç´ ç±»å‹void Vector&lt;T&gt;::copyFrom(T const*A, Rank lo, Rank hi) //ä»¥æ•°ç»„åŒºé—´A[lo, hi)ä¸ºè“æœ¬å¤åˆ¶å‘é‡{ _elem = new T[_capacity = 2 * ( hi - lo ) ]; _size = 0; //åˆ†é…ç©ºé—´ï¼Œè§„æ¨¡æ¸…é›¶ while ( lo &lt; hi ) //A[lo, hi)å†…çš„å…ƒç´ é€ä¸€ _elem[_size++] = A[lo++]; //å¤åˆ¶è‡³_elem[0, hi - lo)}//éå†template &lt;typename T&gt; void Vector&lt;T&gt;::traverse ( void ( *visit ) ( T&amp; ) ) //å€ŸåŠ©å‡½æ•°æŒ‡é’ˆæœºåˆ¶{ for ( int i = 0; i &lt; _size; i++ ) visit ( _elem[i] ); } //éå†å‘é‡template &lt;typename T&gt; template &lt;typename VST&gt; //å…ƒç´ ç±»å‹ã€æ“ä½œå™¨void Vector&lt;T&gt;::traverse ( VST&amp; visit ) //å€ŸåŠ©å‡½æ•°å¯¹è±¡æœºåˆ¶{ for ( int i = 0; i &lt; _size; i++ ) visit ( _elem[i] ); } //éå†å‘é‡//åˆ é™¤template &lt;typename T&gt; T Vector&lt;T&gt;::remove ( Rank r ) { //åˆ é™¤å‘é‡ä¸­ç§©ä¸ºrçš„å…ƒç´ ï¼Œ0 &lt;= r &lt; size T e = _elem[r]; //å¤‡ä»½è¢«åˆ é™¤å…ƒç´  remove ( r, r + 1 ); //è°ƒç”¨åŒºé—´åˆ é™¤ç®—æ³•ï¼Œç­‰æ•ˆäºå¯¹åŒºé—´[r, r + 1)çš„åˆ é™¤ return e; //è¿”å›è¢«åˆ é™¤å…ƒç´ }template &lt;typename T&gt; int Vector&lt;T&gt;::remove ( Rank lo, Rank hi ) { //åˆ é™¤åŒºé—´[lo, hi) if ( lo == hi ) return 0; //å‡ºäºæ•ˆç‡è€ƒè™‘ï¼Œå•ç‹¬å¤„ç†é€€åŒ–æƒ…å†µï¼Œæ¯”å¦‚remove(0, 0) while ( hi &lt; _size ) _elem[lo++] = _elem[hi++]; //[hi, _size)é¡ºæ¬¡å‰ç§»hi - loä¸ªå•å…ƒ _size = lo; //æ›´æ–°è§„æ¨¡ï¼Œç›´æ¥ä¸¢å¼ƒå°¾éƒ¨[lo, _size = hi)åŒºé—´ return hi - lo; //è¿”å›è¢«åˆ é™¤å…ƒç´ çš„æ•°ç›®}//å†’æ³¡æ’åº//....#endif // VECTOR_CONSTRUCTOR_BY_COPYING_H_ INCLUDED å¯ä»¥çœ‹åˆ°æ¨¡æ¿ç±»å’Œæ¨¡æ¿å‡½æ•°çš„å®šä¹‰ï¼Œéœ€è¦åŠ ä¸Š template &lt;typename T&gt;â€¦ é€šè¿‡ä¸»å‡½æ•°è¿›è¡Œæµ‹è¯• â€œmain.cppâ€ 123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;using namespace std;#include &quot;myvector.h&quot;#include &quot;vector_functions.h&quot;int main(){ cout &lt;&lt; &quot;Hello world!&quot; &lt;&lt; endl; Vector&lt;int&gt; myvector(10, 10, 0); int res = myvector.remove(0); cout &lt;&lt; res &lt;&lt; endl; return 0;} æ‰€ä»¥æ¨¡æ¿ç±»å’Œæ¨¡æ¿å‡½æ•°çš„å®šä¹‰ï¼Œä»¥åŠä½¿ç”¨å¤§è‡´å°±æ˜¯è¿™ä¸ªæ ·å­ã€‚æ¥ä¸‹æ¥ï¼Œå¯¹ç±»å®šä¹‰ä¸­çš„å£°æ˜çš„æˆå‘˜å‡½æ•°è¿›è¡Œå®šä¹‰ã€‚æ—¢ç„¶æ˜¯ä½œä¸ºæ¨¡æ¿ç±»ï¼Œå½“ç„¶å‡½æ•°çš„æ•ˆç‡è¶Šé«˜è¶Šå¥½ï¼Œæ‰€ä»¥ä¼šæ¶‰åŠåˆ°å¾ˆå¤šç®—æ³•åˆ†æçš„å†…å®¹ï½ å¯æ‰©å……å‘é‡ï¼šæ‰©å®¹ expand()é€šè¿‡ç®—æ³•åˆ†æå¾—ï¼šåŠ å€æ‰©å®¹ï½ 1234567891011121314151617181920212223//åŠ å€æ‰©å®¹template &lt;typename T&gt; void Vector&lt;T&gt;::expand () // å‘é‡ä¸è¶³æ—¶æ‰©å®¹{ if (_size &lt; _capacity) return; // å°šæœªæ»¡å‘˜ï¼Œä¸éœ€è¦æ‰©å®¹_ if ( _capacity &lt; DEFAULT_CAPACITY ) _capacity = DEFAULT_CAPACITY; //å®¹é‡ä¸ä½äºæœ€å°å®¹é‡_ T* oldelem = _elem; // é‡æ–°ç”³è¯·åœ°å€ _elem = new T[_capacity &lt;&lt; 1]; //å®¹é‡åŠ å€ for (int i=0; i&lt;_size; i++) _elem[i] = oldelem[i]; //å¤åˆ¶åŸå‘é‡å†…å®¹ï¼ˆTä¸ºåŸºæœ¬ç±»å‹ï¼Œæˆ–å·²é‡è½½èµ‹å€¼æ“ä½œç¬¦'='ï¼‰_ delete [] oldElem; //é‡Šæ”¾åŸç©ºé—´_} éœ€è¦æ³¨æ„çš„æ˜¯æ‰©å®¹æ—¶ï¼Œæ˜¯é‡æ–°ç”³è¯·åœ°å€ç©ºé—´ï¼Œä¿è¯è¿ç»­ï¼Ÿ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨åŠ å€ç­–ç•¥å‘¢ï¼Ÿ é€šè¿‡å¹³æ‘Šæ³•åˆ†æå¯å¾—åŠ å€æ‰©å®¹ç´¯è®¡å¢å®¹æ—¶é—´æ˜¯ O(n). å…ƒç´ è®¿é—®éœ€è¦é‡è½½ä¸‹æ ‡æ“ä½œç¬¦ â€œ[]â€ï¼Œ å¤ä¹ ä¸€æ³¢æ“ä½œç¬¦é‡è½½ å¦‚æœ T ä¸æ˜¯å†…ç½®æ•°æ®ç±»å‹ï¼Œé‚£ä¹ˆå°±éœ€è¦å¯¹ä¸‹æ ‡æ“ä½œç¬¦è¿›è¡Œé‡è½½ã€‚ 1234567891011template &lt;typename T&gt; T&amp; Vector&lt;T&gt;::operator[] ( Rank r ) //é‡è½½ä¸‹æ ‡æ“ä½œç¬¦{ return _elem[r]; } // assert: 0 &lt;= r &lt; _sizeconst template &lt;typename T&gt; T&amp; Vector&lt;T&gt;::operator[] ( Rank r ) const //ä»…é™äºåšå³å€¼{ return _elem[r]; } // assert: 0 &lt;= r &lt; _size æ’å…¥ insert() ä¸»è¦æ˜¯å¦éœ€è¦æ‰©å®¹ æ³¨æ„å…ƒç´ å‘åç§»åŠ¨çš„é¡ºåºï¼Œè‡ªåå‘å‰ åˆ é™¤ remove() åˆ é™¤å•ä¸ªå…ƒç´ å’Œåˆ é™¤åŒºé—´ åˆ é™¤åŒºé—´æ—¶ï¼Œéœ€è¦æ³¨æ„åˆ é™¤é¡ºåºï¼Œä»å·¦åˆ°å³é€ä¸ªå¤åˆ¶ æœ‰åºå‘é‡çš„å»é‡12345678910111213141516171819202122232425template &lt;typename T&gt; int Vector&lt;T&gt;::uniquify(){ Rank i = 0, j = 0; //å„å¯¹äº’å¼‚â€œç›¸é‚»â€å…ƒç´ çš„ç§© while(++j &lt; _size){ if (_elem[j] != _elem[i]) _elem[++i] = _elem[j]; } _size = ++i; shrink(); return j-1; //è¢«åˆ é™¤å…ƒç´ çš„æ€»æ•°} å»é‡é«˜æ•ˆç‰ˆçš„å›¾è§£ã€‚å›è¿‡å¤´æ¥çœ‹ï¼Œå®é™…ä¸Šå°±æ˜¯è§£å†³äº†ä¸€ç§é—®é¢˜ï¼Œæ¯”å¦‚ 5 è¿™ä¸ªå…ƒç´ ä¸åœ¨æ˜¯ä¸€æ­¥æ­¥å‘å‰ï¼Œè€Œæ˜¯ç›´æ¥å°±åˆ°äº†æœ€ç»ˆçš„ä½ç½®ï¼Œè¿™æ˜¾ç„¶æ›´é«˜æ•ˆã€‚ æœ‰åºå‘é‡çš„æŸ¥æ‰¾å‡è€Œæ²»ä¹‹ï¼š fibdearch binsearch äºŒåˆ†æŸ¥æ‰¾123456789101112131415161718192021// äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼ˆç‰ˆæœ¬Aï¼‰ï¼šåœ¨æœ‰åºå‘é‡çš„åŒºé—´[lo, hi)å†…æŸ¥æ‰¾å…ƒç´ eï¼Œ0 &lt;= lo &lt;= hi &lt;= _sizetemplate &lt;typename T&gt; static Rank binSearch ( T* A, T const&amp; e, Rank lo, Rank hi ) { while ( lo &lt; hi ) { //æ¯æ­¥è¿­ä»£å¯èƒ½è¦åšä¸¤æ¬¡æ¯”è¾ƒåˆ¤æ–­ï¼Œæœ‰ä¸‰ä¸ªåˆ†æ”¯ Rank mi = ( lo + hi ) &gt;&gt; 1; //ä»¥ä¸­ç‚¹ä¸ºè½´ç‚¹ï¼ˆåŒºé—´å®½åº¦çš„æŠ˜åŠï¼Œç­‰æ•ˆäºå®½åº¦ä¹‹æ•°å€¼è¡¨ç¤ºçš„å³ç§»ï¼‰ if ( e &lt; A[mi] ) hi = mi; //æ·±å…¥å‰åŠæ®µ[lo, mi)ç»§ç»­æŸ¥æ‰¾ else if ( A[mi] &lt; e ) lo = mi + 1; //æ·±å…¥ååŠæ®µ(mi, hi)ç»§ç»­æŸ¥æ‰¾ else return mi; //åœ¨miå¤„å‘½ä¸­ } //æˆåŠŸæŸ¥æ‰¾å¯ä»¥æå‰ç»ˆæ­¢ return -1; //æŸ¥æ‰¾å¤±è´¥}//æœ‰å¤šä¸ªå‘½ä¸­å…ƒç´ æ—¶ï¼Œä¸èƒ½ä¿è¯è¿”å›ç§©æœ€å¤§è€…ï¼›æŸ¥æ‰¾å¤±è´¥æ—¶ï¼Œç®€å•åœ°è¿”å›-1ï¼Œè€Œä¸èƒ½æŒ‡ç¤ºå¤±è´¥çš„ä½ç½® å¤æ‚åº¦æ˜¯ 1.5log(n). fibonacci æŸ¥æ‰¾fibonacci æŸ¥æ‰¾å’Œ binary æŸ¥æ‰¾æœ¬è´¨ä¸Šå·®ä¸å¤šï¼ŒåŒºåˆ«å°±åœ¨äº middle çš„é€‰å–ã€‚åœ¨æ¯”å¯¹å½“å‰ä½ç½®å¤±è´¥ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒæŸ¥æ‰¾å…ƒç´ ä¸å½“å‰ä½ç½®å…ƒç´ çš„å¤§å°ï¼Œå‘å·¦ä¾§æ·±å…¥éœ€è¦ +1, å‘å³ä¾§æ·±å…¥éœ€è¦ +2. æ‰€ä»¥è¿™å°±é€ æˆäº†ä¸€ç§ä¸å¹³è¡¡ã€‚ é€šè¿‡å…¬å¼è¯æ˜ï¼Œ$\\lambda$ é€‰åœ¨ä½•å¤„æ—¶ï¼Œå¤æ‚åº¦æœ€å°ã€‚ äºŒåˆ†æŸ¥æ‰¾æ”¹è¿›ç‰ˆäºŒåˆ†æŸ¥æ‰¾çš„æ”¹è¿›ç‰ˆï¼š ä½¿å¾—å‘å·¦å³ä¸¤ä¾§æ·±å…¥çš„ä»£ä»·å¹³è¡¡ï¼Œä¸èƒ½åŠæ—¶å‘½ä¸­è½´ç‚¹æ‰€åœ¨çš„å…ƒç´ ã€‚ç‰ºç‰²äº†æœ€å¥½æƒ…å†µï¼Œä½†æ•´ä½“æ€§èƒ½æ›´è¶‹äºç¨³å®šã€‚ 123456789101112131415161718192021// äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼ˆç‰ˆæœ¬Bï¼‰ï¼šåœ¨æœ‰åºå‘é‡çš„åŒºé—´[lo, hi)å†…æŸ¥æ‰¾å…ƒç´ eï¼Œ0 &lt;= lo &lt; hi &lt;= _sizetemplate &lt;typename T&gt; static Rank binSearch(T* A, T const&amp; e, Rank lo, Rank hi){ while (1 &lt; hi-lo) //æ¯æ­¥è¿­ä»£ä»…éœ€åšä¸€æ¬¡æ¯”è¾ƒåˆ¤æ–­ï¼Œæœ‰ä¸¤ä¸ªåˆ†æ”¯ï¼›æˆåŠŸæŸ¥æ‰¾ä¸èƒ½æå‰ç»ˆæ­¢ { Rank mi = (lo + hi) &gt;&gt; 1; (e &lt; A[mi]) ? (hi = mi) : (lo = mi); //ç»æ¯”è¾ƒåç¡®å®šæ·±å…¥[lo, mi)æˆ–[mi, hi) } //å‡ºå£æ—¶hi = lo + 1ï¼ŒæŸ¥æ‰¾åŒºé—´ä»…å«ä¸€ä¸ªå…ƒç´ A[lo] return ( e == A[lo] ) ? lo : -1 ; //æŸ¥æ‰¾æˆåŠŸæ—¶è¿”å›å¯¹åº”çš„ç§©ï¼›å¦åˆ™ç»Ÿä¸€è¿”å›-1} //æœ‰å¤šä¸ªå‘½ä¸­å…ƒç´ æ—¶ï¼Œä¸èƒ½ä¿è¯è¿”å›ç§©æœ€å¤§è€…ï¼›æŸ¥æ‰¾å¤±è´¥æ—¶ï¼Œç®€å•åœ°è¿”å›-1ï¼Œè€Œä¸èƒ½æŒ‡ç¤ºå¤±è´¥çš„ä½ç½® äºŒåˆ†æŸ¥æ‰¾æœ€ç»ˆç‰ˆä½†æ˜¯ï¼Œåœ¨è¿™ä¹‹å‰æ‰€æœ‰çš„ search() ç®—æ³•éƒ½æœªä¸¥æ ¼æ»¡è¶³å®šä¹‰çš„è¯­ä¹‰ï¼Œä¹Ÿå°±æ˜¯è¿”å› ä¸å¤§äº e çš„å…ƒç´ çš„æœ€åä¸€ä¸ªå…ƒç´ çš„ ç§©ã€‚ å¤šä¸ªå…ƒç´ å‘½ä¸­ï¼Œå¿…é¡»è¿”å› ç§© æœ€å¤§è€… å¤±è´¥æ—¶ï¼Œåº”è¿”å›å°äº e çš„æœ€å¤§è€…ï¼ˆå«å“¨å…µ [lo-1]ï¼‰ 123456789101112131415161718192021// äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼ˆç‰ˆæœ¬Cï¼‰ï¼šåœ¨æœ‰åºå‘é‡çš„åŒºé—´[lo, hi)å†…æŸ¥æ‰¾å…ƒç´ eï¼Œ0 &lt;= lo &lt;= hi &lt;= _sizetemplate &lt;typename T&gt; static Rank binSearch(T* A, T const&amp; e, Rank lo, Rank hi){ while (lo &lt; hi) //æ¯æ­¥è¿­ä»£ä»…éœ€åšä¸€æ¬¡æ¯”è¾ƒåˆ¤æ–­ï¼Œæœ‰ä¸¤ä¸ªåˆ†æ”¯ { Rank mi = (lo + hi) &gt;&gt; 1; (e &lt; A[mi]) ? (hi=mi) : (lo = mi + 1); //ç»æ¯”è¾ƒåç¡®å®šæ·±å…¥[lo, mi)æˆ–(mi, hi) } //æˆåŠŸæŸ¥æ‰¾ä¸èƒ½æå‰ç»ˆæ­¢ return --lo; //å¾ªç¯ç»“æŸæ—¶ï¼Œloä¸ºå¤§äºeçš„å…ƒç´ çš„æœ€å°ç§©ï¼Œæ•…lo - 1å³ä¸å¤§äºeçš„å…ƒç´ çš„æœ€å¤§ç§©} //æœ‰å¤šä¸ªå‘½ä¸­å…ƒç´ æ—¶ï¼Œæ€»èƒ½ä¿è¯è¿”å›ç§©æœ€å¤§è€…ï¼›æŸ¥æ‰¾å¤±è´¥æ—¶ï¼Œèƒ½å¤Ÿè¿”å›å¤±è´¥çš„ä½ç½® æ’å€¼æŸ¥æ‰¾Interpolation search å¦‚æœæœ‰æ•°æ®çš„å…ˆéªŒåˆ†å¸ƒï¼Œæ¯”å¦‚æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œé‚£ä¹ˆå¯ä»¥æ ¹æ®å…ˆéªŒçŸ¥è¯†æœ‰æ•ˆçš„æé«˜æŸ¥æ‰¾æ•ˆç‡ã€‚ $$\\dfrac{mi-lo}{hi-lo} = \\dfrac{e-A[lo]}{A[hi]-A[lo]}$$ è¿™æ ·å¯ä»¥æ›´å¿«çš„æ¥è¿‘æ‰€éœ€æŸ¥æ‰¾çš„å…ƒç´ ã€‚ä½†æ˜¯åœ¨å°èŒƒå›´å†…ï¼Œå´å®¹æ˜“è¢«æ¶æ€§åˆ†å¸ƒæ‰€å¹²æ‰°ï¼Œæ¯”å¦‚ä¸æ»¡è¶³å‡åŒ€åˆ†å¸ƒçš„æƒ…å†µä¸‹ã€‚ æœ€å¥½çš„æŸ¥æ‰¾æ–¹å¼æ˜¯ï¼šæ’å€¼æŸ¥æ‰¾ -&gt; äºŒåˆ†æŸ¥æ‰¾ -&gt; é¡ºåºæŸ¥æ‰¾ æ’åºå‰é¢çš„æŸ¥æ‰¾ç®—æ³•éƒ½æ˜¯å¯¹åº”äºæœ‰åºå‘é‡çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”¨æ’åºç®—æ³•å…ˆæŠŠå‘é‡å˜å¾—æœ‰åºèµ·æ¥ï¼Œè¿™æ ·æ‰èƒ½æ›´é«˜æ•ˆçš„ æŸ¥æ‰¾ æˆ– å»é‡ã€‚ å†’æ³¡æ’åºå†’æ³¡æ’åºï¼šä»åå‘å‰é€æ¸æœ‰åº å…¶æ”¹è¿› æ”¹è¿›ä¸€ï¼šå¦‚æœåœ¨æŸä¸€è¶Ÿå¾ªç¯ä¹‹åï¼Œå·²ç»å®Œå…¨æœ‰åºäº†ï¼Œåé¢çš„å¾ªç¯ä¹Ÿå°±ä¸éœ€è¦äº†ã€‚æ‰€ä»¥éœ€è¦è®¾ç½®è®¾å¦æœ‰åºçš„æ ‡å¿— æ”¹è¿›äºŒï¼š å¦‚æœæ•°æ®åªæ˜¯åœ¨å‰ç¼€çš„ä¸€å°éƒ¨åˆ†ä¹±åºï¼Œåç¼€å¾ˆå¤šä¸€éƒ¨åˆ†å·²ç»æœ‰åºäº†ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬æ˜¯å¦è¿˜éœ€è¦äº¦æ­¥äº¦è¶‹çš„ä»åå¾€å‰ä¸€æ­¥æ­¥çš„æœ‰åºå‘¢ï¼Ÿ æ‰€ä»¥å¯ä»¥åœ¨ä¸€è¶Ÿäº¤æ¢èµ°å®Œä¹‹åï¼Œæ ‡è®°å‡ºäº¤æ¢çš„æœ€åä½ç½®ã€‚ç„¶ååªéœ€è¦è€ƒè™‘å‰ç¼€æ— åºçš„éƒ¨åˆ†å³å¯ã€‚ æ”¹è¿›ä¸€1234567891011121314151617181920212223242526272829template &lt;typename T&gt; //å‘é‡çš„èµ·æ³¡æ’åºvoid Vector&lt;T&gt;::bubbleSort ( Rank lo, Rank hi ) //assert: 0 &lt;= lo &lt; hi &lt;= size{ while ( !bubble ( lo, hi-- ) ); } //é€è¶Ÿåšæ‰«æäº¤æ¢ï¼Œç›´è‡³å…¨åº//ä¸€è¶Ÿæ‰«æäº¤æ¢template &lt;typename T&gt; bool Vector&lt;T&gt;::bubble ( Rank lo, Rank hi ) { bool sorted = true; //æ•´ä½“æœ‰åºæ ‡å¿— while ( ++lo &lt; hi ) //è‡ªå·¦å‘å³ï¼Œé€ä¸€æ£€æŸ¥å„å¯¹ç›¸é‚»å…ƒç´  if ( _elem[lo - 1] &gt; _elem[lo] ) { //è‹¥é€†åºï¼Œåˆ™_ sorted = false; //æ„å‘³ç€å°šæœªæ•´ä½“æœ‰åºï¼Œå¹¶éœ€è¦ swap ( _elem[lo - 1], _elem[lo] ); //é€šè¿‡äº¤æ¢ä½¿å±€éƒ¨æœ‰åº_ } return sorted; //è¿”å›æœ‰åºæ ‡å¿—} è‡ªå·±åŠ¨æ‰‹å®ç°ä¸€ä¸ªæ•´ä½“çš„å½¢å¼ã€‚ã€‚ 12345678910111213141516171819202122232425262728293031323334353637template &lt;typename T&gt; void Vector&lt;T&gt;::my_bubbleSort(Rank lo, Rank hi){ while (lo &lt; hi) { bool sorted = true; Rank a = lo; while ( ++a &lt; hi) //è‡ªå·¦å‘å³ï¼Œé€ä¸€æ£€æŸ¥å„å¯¹ç›¸é‚»å…ƒç´  { if ( _elem[a - 1] &gt; _elem[a] ) //è‹¥é€†åºï¼Œåˆ™_ { sorted = false; //æ„å‘³ç€å°šæœªæ•´ä½“æœ‰åºï¼Œå¹¶éœ€è¦ swap ( _elem[a - 1], _elem[a] ); //é€šè¿‡äº¤æ¢ä½¿å±€éƒ¨æœ‰åº_ } } if (sorted == true) break; hi--; }} æ”¹è¿›äºŒ123456789101112131415161718192021222324252627282930313233343536373839// æ”¹è¿›åçš„å†’æ³¡æ’åºtemplate &lt;typename T&gt; //å‘é‡çš„èµ·æ³¡æ’åºvoid Vector&lt;T&gt;::bubbleSort ( Rank lo, Rank hi ) //assert: 0 &lt;= lo &lt; hi &lt;= size{ while ( lo &lt; ( hi = bubble_fast(lo, hi) ) );} //é€è¶Ÿåšæ‰«æäº¤æ¢ï¼Œç›´è‡³å…¨åºtemplate &lt;typename T&gt; Rank Vector&lt;T&gt;::bubble_fast ( Rank lo, Rank hi ) { //ä¸€è¶Ÿæ‰«æäº¤æ¢ Rank last = lo; //æœ€å³ä¾§çš„é€†åºå¯¹åˆå§‹åŒ–ä¸º[lo - 1, lo] while ( ++lo &lt; hi ) //è‡ªå·¦å‘å³ï¼Œé€ä¸€æ£€æŸ¥å„å¯¹ç›¸é‚»å…ƒç´  { if ( _elem[lo - 1] &gt; _elem[lo] ) { //è‹¥é€†åºï¼Œåˆ™ last = lo; //æ›´æ–°æœ€å³ä¾§é€†åºå¯¹ä½ç½®è®°å½•ï¼Œå¹¶ swap ( _elem[lo - 1], _elem[lo] ); //é€šè¿‡äº¤æ¢ä½¿å±€éƒ¨æœ‰åº } } return last; //è¿”å›æœ€å³ä¾§çš„é€†åºå¯¹ä½ç½®} å½’å¹¶æ’åºåˆ†æ²»ç­–ç•¥ï¼š ä¸€åˆ†ä¸ºäºŒ å­åºåˆ—é€’å½’æ’åº åˆå¹¶ä¸¤ä¸ªå­åºåˆ— ä»£ç å½¢å¼å°±æ˜¯è¿™æ ·ï¼š 123456789101112131415template &lt;typename T&gt; //å‘é‡å½’å¹¶æ’åºvoid Vector&lt;T&gt;::mergeSort ( Rank lo, Rank hi ) { //0 &lt;= lo &lt; hi &lt;= size if ( hi - lo &lt; 2 ) return; //å•å…ƒç´ åŒºé—´è‡ªç„¶æœ‰åºï¼Œå¦åˆ™... int mi = ( lo + hi ) / 2; //ä»¥ä¸­ç‚¹ä¸ºç•Œ mergeSort ( lo, mi ); mergeSort ( mi, hi ); //åˆ†åˆ«æ’åº merge ( lo, mi, hi ); //å½’å¹¶} å‰ä¸¤ä¸ªæ¯”è¾ƒç®€å•ï¼Œé‚£ä¹ˆç¬¬ä¸‰æ­¥å½’å¹¶ï¼ˆmergeï¼‰æ€ä¹ˆå®ç°å‘¢ï¼Ÿ ä¸‹å›¾æ˜¯ä¸ªå®ä¾‹å›¾è§£ æ¯æ¬¡åªæ¯”è¾ƒé¦–ä¸ªå…ƒç´ ã€‚ æ¥ä¸‹æ¥å…·ä½“å¦‚ä½•å®ç°ä¸¤ä¸ªå­åºåˆ—çš„å½’å¹¶ã€‚ æŠŠä¸¤ä¸ªå­åºåˆ—åˆ†åˆ«æ˜¯ Bï¼ŒC. åˆå¹¶åçš„åºåˆ—æ˜¯ A. å…¶ä¸­å¤åˆ¶ [lo, mi) åˆ° B. C å­åºåˆ—ä¸éœ€è¦å¤åˆ¶ã€‚ 12345678910111213141516171819202122232425262728293031template &lt;typename T&gt; //æœ‰åºå‘é‡çš„å½’å¹¶void Vector&lt;T&gt;::merge ( Rank lo, Rank mi, Rank hi ) { //å„è‡ªæœ‰åºçš„å­å‘é‡[lo, mi)å’Œ[mi, hi) T* A = _elem + lo; //åˆå¹¶åçš„å‘é‡A[0, hi - lo) = _elem[lo, hi) // å¤åˆ¶å‰å­—å‘é‡ int lb = mi - lo; T* B = new T[lb]; //å‰å­å‘é‡B[0, lb) = _elem[lo, mi) for ( Rank i = 0; i &lt; lb; i++ ) B[i] = A[i]; //å¤åˆ¶å‰å­å‘é‡ // åå­å‘é‡ ä¸éœ€è¦å¤åˆ¶ int lc = hi - mi; T* C = _elem + mi; //åå­å‘é‡C[0, lc) = _elem[mi, hi) // B å’Œ C ä¸­è¾ƒå°è€…ç»­è‡³Aæœ«å°¾ for ( Rank i = 0, j = 0, k = 0; ( j &lt; lb ) || ( k &lt; lc ); ) { //B[j]å’ŒC[k]ä¸­çš„å°è€…ç»­è‡³Aæœ«å°¾ if ( ( j &lt; lb ) &amp;&amp; ( ! ( k &lt; lc ) || ( B[j] &lt;= C[k] ) ) ) A[i++] = B[j++]; if ( ( k &lt; lc ) &amp;&amp; ( ! ( j &lt; lb ) || ( C[k] &lt; B[j] ) ) ) A[i++] = C[k++]; } delete [] B; //é‡Šæ”¾ä¸´æ—¶ç©ºé—´B}//å½’å¹¶åå¾—åˆ°å®Œæ•´çš„æœ‰åºå‘é‡[lo, hi) å…¶ä¸­éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå°† B å’Œ C æ¥åˆ° A ä¸­æ—¶ï¼Œè·³å‡ºå¾ªç¯çš„æ¡ä»¶æ˜¯ï¼š ( j &lt; lb ) || ( k &lt; lc ) å³ä¸¤ä¸ªå­åºåˆ—éƒ½è¶Šç•Œäº†ã€‚ ä½†å…¶å® B å·²ç»è¶Šç•Œåï¼ŒC åé¢çš„å¹¶ä¸éœ€è¦ä¸€ä¸€å¤åˆ¶åˆ° A ä¸­ã€‚æ‰€ä»¥è¿™é‡Œä¹Ÿæ˜¯å¯ä»¥æ”¹è¿›çš„ã€‚ 123456789for ( Rank i = 0, j = 0, k = 0; j &lt; lb ; ) { //B[j]å’ŒC[k]ä¸­çš„å°è€…ç»­è‡³Aæœ«å°¾ if ( ( j &lt; lb ) &amp;&amp; ( B[j] &lt;= C[k] ) ) A[i++] = B[j++]; if ( ! ( j &lt; lb ) || ( C[k] &lt; B[j] ) ) A[i++] = C[k++];} æ‰€ä»¥ä¸¤ä¸ªæ¡ä»¶åˆ†åˆ«æ˜¯ï¼š B è¿˜æ²¡è¶Šç•Œï¼Œå¹¶ä¸” B ä¸­å…ƒç´ è¾ƒå°ï¼Œ åˆ™ B -&gt; A B å·²ç»è¶Šç•Œï¼Œæˆ–è€… B ä¸­å…ƒç´ è¾ƒå¤§ï¼Œ åˆ™ C -&gt; A. é‚“è€å¸ˆæä¾›äº†ä¸€ä¸ªæ€è·¯ï¼Œå‡è®¾ B å·²ç»è¶Šç•Œï¼Œå¯ä»¥ç†è§£ä¸ºåœ¨å“¨å…µå¤„æœ‰ä¸€ä¸ªæ— ç©·å¤§çš„å…ƒç´ ï¼Œé‚£ä¹ˆç­‰åŒäº B ä¸­å…ƒç´ å§‹ç»ˆè¾ƒå¤§ã€‚ è¯¾åä¹ é¢˜","link":"/2018/12/20/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%952-%E5%90%91%E9%87%8F/"},{"title":"é‚“å…¬æ•°æ®ç»“æ„ä¸ç®—æ³•4-æ ˆå’Œé˜Ÿåˆ—","text":"æ ˆå’Œé˜Ÿåˆ— éƒ½æ˜¯çº¿æ€§ç»“æ„çš„ç‰¹ä¾‹ï¼Œå› æ­¤å…¶å®ç°éƒ½å¯ä»¥ç”¨ å‘é‡æˆ–åˆ—è¡¨ æ¥å®ç°ã€‚ STL ä¸­ stack çš„æ¥å£ä½¿ç”¨æ ˆçš„ä½¿ç”¨æ ˆä¸é€’å½’é€’å½’ç®—æ³•æ‰€éœ€çš„ç©ºé—´é‡ï¼Œä¸»è¦å†³å®šäºæœ€å¤§é€’å½’æ·±åº¦ã€‚åœ¨è¾¾åˆ°è¿™ä¸€æ·±åº¦çš„æ—¶åˆ»ï¼ŒåŒæ—¶æ´»è·ƒçš„é€’å½’å®ä¾‹æœ€å¤šã€‚ é‚£ä¹ˆæ“ä½œç³»ç»Ÿæ˜¯å¦‚ä½•å®ç°å‡½æ•°ï¼ˆé€’å½’ï¼‰è°ƒç”¨çš„å‘¢ï¼Ÿå¦‚ä½•åŒæ—¶è®°å½•è°ƒç”¨ä¸è¢«è°ƒç”¨å‡½æ•°ï¼ˆé€’å½’ï¼‰å®ä¾‹ä¹‹é—´çš„å…³ç³»ï¼Ÿå¦‚ä½•å®ç°å‡½æ•°ï¼ˆé€’å½’ï¼‰è°ƒç”¨çš„è¿”å›ï¼Ÿåˆæ˜¯å¦‚ä½•ç»´æŠ¤åŒæ—¶æ´»è·ƒçš„æ‰€æœ‰å‡½æ•°ï¼ˆé€’å½’ï¼‰å®ä¾‹çš„ï¼Ÿ è¿™äº›é—®é¢˜ï¼Œéƒ½å½’ç»“äºæ ˆã€‚[è¯¾æœ¬ 88 é¡µ] åœºæ™¯ä¸€ï¼šé€†åºè¾“å‡ºé€†åºè¾“å‡ºï¼šconversion, è¾“å‡ºæ¬¡åºä¸å¤„ç†è¿‡ç¨‹é¢ å€’ï¼Œé€’å½’æ·±åº¦å’Œè¾“å‡ºé•¿åº¦ä¸æ˜“é¢„çŸ¥ã€‚ è¿›åˆ¶è½¬æ¢åœºæ™¯äºŒï¼šé€’å½’åµŒå¥—å…·æœ‰è‡ªç›¸ä¼¼æ€§çš„é—®é¢˜å¤šå¯åµŒå¥—çš„é€’å½’æè¿°ï¼Œä½†å› åˆ†æ”¯ä½ç½®å’ŒåµŒå¥—æ·±åº¦å¹¶ä¸å›ºå®šï¼Œå…¶é€’å½’ç®—æ³•çš„å¤æ‚åº¦ä¸æ˜“æ§åˆ¶ã€‚æ ˆç»“æ„åŠå…¶æ“ä½œå¤©ç„¶çš„å…·æœ‰é€’å½’åµŒå¥—æ€§ï¼Œæ•…å¯ä»¥é«˜æ•ˆçš„è§£å†³è¿™ç±»é—®é¢˜ã€‚ æ‹¬å·åŒ¹é…è¯¾æœ¬[92é¡µ] ç®—æ³•å…¶å®å¾ˆç®€å•ï¼Œä¸»è¦æ˜¯æ€è€ƒçš„è¿‡ç¨‹ã€‚ä½¿ç”¨å‡æ²» å’Œ åˆ†æ²» éƒ½ä¸èƒ½è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ ä»å¤§è§„æ¨¡é—®é¢˜åˆ°å°è§„æ¨¡é—®é¢˜å¹¶ä¸å®Œå…¨ç­‰æ•ˆã€‚ä¹Ÿå°±æ˜¯åˆ†æˆå­é—®é¢˜ä¹‹åï¼Œä¸æ»¡è¶³æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œåˆæˆå¤§é—®é¢˜å´å¯ä»¥æ»¡è¶³æƒ…å†µã€‚ ç”¨ æ ˆ çœŸçš„å¤ªåˆé€‚äº†ã€‚è€Œä¸”å¯ä»¥æ¨å¹¿åˆ°å¤šç§æ‹¬å·çš„æƒ…å†µã€‚ æ ˆæ··æ´—å€ŸåŠ©ä¸€ä¸ªä¸­é—´ æ ˆï¼Œ å°† æ ˆA çš„æ•°è½¬ç§»åˆ° æ ˆB ä¸­å»ã€‚ç§°ä½œ æ ˆæ··æ´—(stack permutation). åœ¨ pytorch ä¸­æ ¹æ® index è¿›è¡Œé‡æ–°æ’åˆ— permutationï¼Œå®é™…ä¸Šä¹Ÿæ˜¯ç”¨çš„ æ ˆ å¯¹å§ï¼Ÿ index_select n ä¸ªæ•°çš„æ ˆæ··æ´—çš„æ€»æ•°ï¼Œ æ°å¥½æ˜¯è‘—åçš„ catalan æ•° $$\\dfrac{2n!}{(n+1)!n!}$$ æ€ä¹ˆç”„åˆ«å‡ºå“ªäº›ä¸æ˜¯æ ˆæ··æ´—çš„åºåˆ—å‘¢ï¼Ÿ è¿™é‡Œæœ‰ä¸€ä¸ªç¦æ­¢çš„æƒ…å†µå‡ºç°ã€‚å¯¹äºä»»ä½•ä¸‰ä¸ªäº’å¼‚çš„æ•° $1\\le i &lt; i &lt; j &lt; k \\le n$ å‡ºç° $kâ€¦,i,â€¦,j$ åˆ™å¿…éä¸ºæ ˆæ··æ´—ã€‚ æ€ä¹ˆåˆ¤æ–­ä¸€ä¸ªåºåˆ— B æ˜¯å¦æ˜¯å¦ä¸€ä¸ªåºåˆ— A çš„ æ ˆæ··æ´—ï¼Ÿ é€šè¿‡ æ ˆ èƒ½å®ç°ï¼Œçº¿æ€§å¤æ‚åº¦çš„ç®—æ³•ã€‚ https://blog.csdn.net/became_a_wolf/article/details/49002593 https://www.cnblogs.com/Inkblots/p/4950331.html å»¶æ—¶é˜Ÿåˆ—","link":"/2019/01/09/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/"},{"title":"é‚“å…¬æ•°æ®ç»“æ„ä¸ç®—æ³•3-åˆ—è¡¨","text":"äº‹å®ä¸Š 2017 å¹´çš„è¿™ä¸ªæ—¶å€™æˆ‘çœ‹äº†ä¸€éæµ™å¤§é™ˆè¶Šè€å¸ˆçš„ã€Šæ•°æ®ç»“æ„ã€‹ï¼Œå¹¶ä¸”æœ‰è¯¦ç»†çš„åšç¬”è®°ã€‚ä½†ç°åœ¨å›è¿‡å¤´æ¥çœ‹ï¼Œè¿™äº›å¥½åƒåŸºæœ¬ä¸Šåˆéƒ½å¿˜äº†ã€‚ã€‚é—®é¢˜åº”è¯¥æ˜¯åœ¨äºå­¦è¿‡ä¹‹åä¸æ€ä¹ˆä½¿ç”¨ï¼Œå½“æ—¶ä¹Ÿåªæ˜¯çœ‹äº†è§†é¢‘ï¼Œè€Œæ²¡æœ‰å»åˆ·é¢˜ã€‚ æ‰€ä»¥è¿™æ¬¡æ€»ç»“ä¸‹ç»éªŒï¼Œåªçœ‹è§†é¢‘å¹¶åšç¬”è®°æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚è§†é¢‘çš„ç¼ºé™·åœ¨äºä¸åƒä¹¦æœ¬é‚£æ ·ï¼Œèƒ½å¤Ÿéšæ„çš„ç¿»åˆ°ä½ ä¸æ‡‚æˆ–è€…æ¨¡ç³Šçš„çŸ¥è¯†ç‚¹ã€‚æ‰€ä»¥ï¼Œè¿™æ¬¡æˆ‘åº”è¯¥åšçš„æ˜¯ï¼ŒæŠŠè§†é¢‘å¿«é€Ÿè¿‡ä¸€éï¼Œè®°ä¸‹è€å¸ˆè¯´çš„çŸ¥è¯†ç‚¹ï¼Œç„¶åå¯¹ç…§ä¹¦æœ¬å»æ•²ä»£ç ã€‚æ›´æ›´æ›´é‡è¦çš„æ˜¯ï¼Œä¸æ–­çš„ç»ƒä¹ ã€‚ åˆ—è¡¨ï¼ˆé“¾è¡¨ï¼‰è¿™éƒ¨åˆ†å†…å®¹çš„ç»“æ„è·Ÿ å‘é‡ é‚£ä¸€ç« æ˜¯ä¸€æ ·çš„ã€‚å…ˆä»‹ç»æ“ä½œæ¥å£ï¼Œç„¶åé€ä¸ªä»‹ç»å®ƒä»¬çš„å®ç°ã€‚ åˆ—è¡¨ï¼Œä¹Ÿå°±æ˜¯é“¾è¡¨ï¼Œä¸å‘é‡å¯¹ç§°ä¸”äº’è¡¥ã€‚ å‘é‡ï¼š å¾ªç§©è®¿é—® call-by-rank åˆ—è¡¨ï¼š å¾ªä½ç½®è®¿é—® call-by-position åŒå‘é“¾è¡¨å’Œå•å‘é“¾è¡¨ã€‚ é‚“è€å¸ˆè®²çš„æ˜¯åŒå‘é“¾è¡¨ï¼Œæ‰€ä»¥å«æœ‰ å¤´èŠ‚ç‚¹ header å’Œ å°¾èŠ‚ç‚¹ trailerã€‚ header å’Œ trailer æ˜¯ä¸ç”Ÿä¿±æ¥çš„ï¼Œä¹Ÿæ˜¯ä¸å¯è§çš„ã€‚æ‰€ä»¥åˆå§‹åŒ–çš„é“¾è¡¨èŠ‚ç‚¹ä¸­å«æœ‰ä¸¤ä¸ªç©ºèŠ‚ç‚¹ã€‚ åŒå‘é“¾è¡¨çš„å®ç°123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485# ifndef DOUBLE_LINK_HPP# define DOUBLE_LINK_HPP/*åŒå‘é“¾è¡¨çš„èŠ‚ç‚¹ç»“æ„*/template &lt;typename T&gt;struct Node{public: Node()= default; Node(T value, Node&lt;T&gt;* preptr, Node&lt;T&gt;* nextptr) :_value(value), pre_ptr(preptr), next_ptr(nextptr){}public: T _value; Node&lt;T&gt;* pre_ptr; Node&lt;T&gt;* next_ptr;};/** åŒå‘é“¾è¡¨ç±»*/template&lt;typename T&gt;class DoubleLink{public: typedef Node&lt;T&gt;* pointer;public: DoubleLink(); ~DoubleLink(){};public: Node&lt;T&gt;* insert(int index, T value); Node&lt;T&gt;* insert_front(T value); Node&lt;T&gt;* insert_last(T value); Node&lt;T&gt;* del(int index); Node&lt;T&gt;* delete_front(); Node&lt;T&gt;* delete_last(); bool isEmpty(); int size(); T get(int index); T get_front(); T get_last(); Node&lt;T&gt;* getHead();private: Node&lt;T&gt;* phead; int count;private : Node&lt;T&gt;* getNode(int index);};/** æ„é€ å‡½æ•°**/template &lt;typename T&gt;DoubleLink&lt;T&gt;::DoubleLink(){ phead = new Node&lt;T&gt;(0, NULL, NULL); phead-&gt;next_ptr = phead; phead-&gt;pre_ptr = phead; count = 0;};template&lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::getHead(){ return phead;}/**è¿”å›é“¾è¡¨é•¿åº¦*/template &lt;typename T&gt;int DoubleLink&lt;T&gt;::size(){ return count;}/*è·å–æŒ‡å®šä¸‹æ ‡çš„å…ƒç´ */template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::getNode(int index){ if (index &gt;= count || index &lt; 0) return NULL; if (index&lt;=count / 2) //å¦‚æœåœ¨å‰åŠéƒ¨åˆ† { Node&lt;T&gt;* pnode = phead-&gt;next_ptr; while (index) { pnode = pnode-&gt;next_ptr; index--; } return pnode; } //åœ¨ååŠéƒ¨åˆ† index = count - index-1; Node&lt;T&gt;* pnode = phead-&gt;pre_ptr; while (index) { pnode = pnode-&gt;pre_ptr; index--; } return pnode;};/**å°†æ–°èŠ‚ç‚¹æ’åˆ°ç¬¬ä¸€ä¸ªä½ç½®*/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::insert_front(T value){ Node&lt;T&gt;* newNode = new Node&lt;int&gt;(value, phead, phead-&gt;next_ptr); phead-&gt;next_ptr -&gt;pre_ptr= newNode; phead-&gt;next_ptr = newNode; count++; return newNode;};/**å°†æ–°èŠ‚ç‚¹æ’åˆ°é“¾è¡¨å°¾éƒ¨*/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::insert_last(T value){ Node&lt;T&gt; * newNode = new Node&lt;int&gt;(value, phead-&gt;pre_ptr, phead); phead-&gt;pre_ptr-&gt;next_ptr = newNode; phead-&gt;pre_ptr = newNode; count++; return newNode;};/**å°†èŠ‚ç‚¹ä½ç½®æ’åˆ°indexä½ç½®ä¹‹å‰*/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::insert(int index, T value){ if (index == 0) return insert_front(value); Node&lt;T&gt;* pNode = getNode(index); if (pNode == NULL) return NULL; Node&lt;T&gt;* newNode = new Node&lt;T&gt;(value, pNode-&gt;pre_ptr, pNode); pNode-&gt;pre_ptr-&gt;next_ptr = newNode; pNode-&gt;pre_ptr = newNode; count++; return newNode;};/**åˆ é™¤é“¾è¡¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹*è¿”å›åˆ é™¤åé“¾è¡¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹*/template&lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::delete_front(){ if (count == 0) //ç©ºæ ‘ï¼Œè¿”å›NULL { return NULL; } Node&lt;T&gt;* pnode = phead-&gt;next_ptr; phead-&gt;next_ptr = pnode-&gt;next_ptr; pnode-&gt;next_ptr-&gt;pre_ptr = phead; delete pnode; count--; return phead-&gt;next_ptr;};/**åˆ é™¤é“¾è¡¨çš„æœ«å°¾èŠ‚ç‚¹*è¿”å›åˆ é™¤åé“¾è¡¨å°¾éƒ¨å…ƒç´ */template&lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::delete_last(){ if (count == 0) { return NULL; } Node&lt;T&gt;*pnode = phead-&gt;pre_ptr; pnode-&gt;pre_ptr-&gt;next_ptr = phead; phead-&gt;pre_ptr = pnode-&gt;pre_ptr; delete pnode; count--; return phead-&gt;pre_ptr;}/**åˆ é™¤æŒ‡å®šä½ç½®çš„å…ƒç´ **/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::del(int index){ if (index == 0) return delete_front(); if (index == count - 1) return delete_last(); if (index &gt;= count) return NULL; Node&lt;T&gt;* pnode = getNode(index); pnode-&gt;pre_ptr-&gt;next_ptr = pnode-&gt;next_ptr; pnode-&gt;next_ptr-&gt;pre_ptr = pnode-&gt;pre_ptr; Node&lt;T&gt;* ptemp = pnode-&gt;pre_ptr; delete pnode; count--; return ptemp;};template &lt;typename T&gt;bool DoubleLink&lt;T&gt;::isEmpty(){ return count == 0;};/**è·å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„å€¼*/template&lt;typename T&gt;T DoubleLink&lt;T&gt;::get_front(){ return phead-&gt;next_ptr-&gt;_value;};/**è·å–æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„å€¼*/template &lt;typename T&gt;T DoubleLink&lt;T&gt;::get_last(){ return phead-&gt;pre_ptr-&gt;_value;};/**è·å–æŒ‡å®šä½ç½®èŠ‚ç‚¹çš„å€¼*/template &lt;typename T&gt;T DoubleLink&lt;T&gt;::get(int index){ Node&lt;T&gt; pnode = getNode(index); return pnode-&gt;_value;};# endif æ’åºleetcode 148: https://leetcode.com/problems/sort-list/submissions/ é€‰æ‹©æ’åºå®é™…ä¸Š å†’æ³¡æ’åº ä¹Ÿå¯ä»¥çœ‹åšæ˜¯é€‰æ‹©æ’åºï¼Œä¸åŒçš„æ˜¯ï¼Œåœ¨é€‰æ‹©æœ€å¤§å…ƒç´ çš„æ—¶å€™ï¼Œå†’æ³¡æ’åºæ¯æ¯”è¾ƒä¸€æ¬¡éƒ½ä¼šè¿›è¡Œä¸€æ¬¡äº¤æ¢ï¼Œè¿™æ ·äº¦æ­¥äº¦è¶‹çš„æ–¹å¼å¢åŠ äº†æ¶ˆè€—ã€‚è€Œé€‰æ‹©æ’åºï¼Œåªæ˜¯è¿›è¡Œæ¯”è¾ƒï¼Œæ¯ä¸€è¶Ÿéå†åªäº¤æ¢ä¸€æ¬¡ã€‚ é‚“è€å¸ˆè¿™é‡Œçš„ä»£ç ä¼šæ¯”è¾ƒå¤æ‚ï¼Œæ˜¯åŸºäºåŒå‘é“¾è¡¨å®ç°çš„ã€‚è¿™é‡Œæˆ‘ä»¬åªéœ€è¦ç†è§£ç®—æ³•æ€æƒ³ã€‚ åˆ—è¡¨çš„æ’åºç®—æ³•ï¼Œ å¯¹äºèµ·å§‹äºä½ç½® p çš„ n ä¸ªå…ƒç´ çš„æ’åºã€‚ 1234567891011121314151617181920212223#define ListNodePosi(T) ListNode&lt;T&gt;* //åˆ—è¡¨èŠ‚ç‚¹ä½ç½®template &lt;typename T&gt; //åˆ—è¡¨çš„é€‰æ‹©æ’åºç®—æ³•ï¼šå¯¹èµ·å§‹äºä½ç½®pçš„nä¸ªå…ƒç´ æ’åºvoid List&lt;T&gt;::selectionSort ( ListNodePosi(T) p, int n ) { //valid(p) &amp;&amp; rank(p) + n &lt;= size ListNodePosi(T) head = p-&gt;pred; ListNodePosi(T) tail = p; for ( int i = 0; i &lt; n; i++ ) tail = tail-&gt;succ; //å¾…æ’åºåŒºé—´ä¸º(head, tail) while ( 1 &lt; n ) { //åœ¨è‡³å°‘è¿˜å‰©ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹å‰ï¼Œåœ¨å¾…æ’åºåŒºé—´å†… ListNodePosi(T) max = selectMax ( head-&gt;succ, n ); //æ‰¾å‡ºæœ€å¤§è€…ï¼ˆæ­§ä¹‰æ—¶åè€…ä¼˜å…ˆï¼‰ insertB ( tail, remove ( max ) ); //å°†å…¶ç§»è‡³æ— åºåŒºé—´æœ«å°¾ï¼ˆä½œä¸ºæœ‰åºåŒºé—´æ–°çš„é¦–å…ƒç´ ï¼‰ tail = tail-&gt;pred; n--; }} 1234567891011121314151617template &lt;typename T&gt; //ä»èµ·å§‹äºä½ç½®pçš„nä¸ªå…ƒç´ ä¸­é€‰å‡ºæœ€å¤§è€…ListNodePosi(T) List&lt;T&gt;::selectMax ( ListNodePosi(T) p, int n ) { ListNodePosi(T) max = p; //æœ€å¤§è€…æš‚å®šä¸ºé¦–èŠ‚ç‚¹p for ( ListNodePosi(T) cur = p; 1 &lt; n; n-- ) //ä»é¦–èŠ‚ç‚¹på‡ºå‘ï¼Œå°†åç»­èŠ‚ç‚¹é€ä¸€ä¸maxæ¯”è¾ƒ if ( !lt ( ( cur = cur-&gt;succ )-&gt;data, max-&gt;data ) ) //è‹¥å½“å‰å…ƒç´ ä¸å°äºmaxï¼Œåˆ™ max = cur; //æ›´æ–°æœ€å¤§å…ƒç´ ä½ç½®è®°å½• return max; //è¿”å›æœ€å¤§èŠ‚ç‚¹ä½ç½®} æ’å…¥æ’åºå½’å¹¶æ’åºå¯¹åˆ—è¡¨çš„å½’å¹¶æ’åºï¼Œç›¸æ¯”å‘é‡è¦æ›´å¤æ‚äº›ï¼Œä¸»è¦åœ¨æ€ä¹ˆæ‰¾åˆ°ä¸­é—´ç‚¹ å½’å¹¶æ’åºçš„æ•´ä½“æ€æƒ³ æ‰¾åˆ°ä¸€ä¸ªé“¾è¡¨çš„ä¸­é—´èŠ‚ç‚¹çš„æ–¹æ³• åˆå¹¶ä¸¤ä¸ªå·²æ’å¥½åºçš„é“¾è¡¨ä¸ºä¸€ä¸ªæ–°çš„æœ‰åºé“¾è¡¨ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: ListNode* getMid(ListNode* head) { if (!head) return NULL; if (!head-&gt;next) return head; ListNode* slow = head; ListNode* fast = head-&gt;next; while(fast &amp;&amp; fast-&gt;next) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; } return slow; } ListNode* mergeLists(ListNode* l1, ListNode* l2) { if (l1 == NULL) return l2; if (l2 == NULL) return l1; ListNode dummy(-1); ListNode *tail = &amp;dummy; while(l1 &amp;&amp; l2) { if (l1-&gt;val &lt; l2-&gt;val) { tail-&gt;next = l1; tail = tail-&gt;next; l1 = l1-&gt;next; } else { tail-&gt;next = l2; tail = tail-&gt;next; l2 = l2-&gt;next; } } if (l1) tail-&gt;next = l1; if (l2) tail-&gt;next = l2; return dummy.next; } ListNode* sortList(ListNode* head) { if (!head) return NULL; if (head-&gt;next==NULL) return head; // æ‰¾åˆ°ä¸­é—´ç‚¹ ListNode *mid = getMid(head); // å°†é“¾è¡¨åˆ†æˆä¸¤ä¸ªå­é“¾è¡¨ ListNode *nextPart = NULL; if (mid) { nextPart = mid-&gt;next; mid-&gt;next = NULL; } return mergeLists(sortList(head), sortList(nextPart)); }};","link":"/2018/12/21/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%953-%E5%88%97%E8%A1%A8/"},{"title":"è®ºæ–‡ç¬”è®°-dropblock","text":"paper: DropBlock: DropBlock: A regularization method for convolutional networks Variational Dropoutï¼šA Theoretically Grounded Application of Dropout in Recurrent Neural Networks Zoneoutï¼šZoneout: Regularizing RNNs by Randomly Preserving Hidden Activations dropblock æ˜¯å…³äº CNN çš„ï¼Œåä¸¤ç¯‡æ˜¯å…³äº RNN çš„æ­£åˆ™åŒ–ã€‚ DropBlockMotivation Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. é€šå¸¸æ·±åº¦ç¥ç»ç½‘ç»œåœ¨è¿‡å‚æ•°åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶åŠ ä¸Šå¤§é‡çš„å™ªå£°å’Œæ­£åˆ™åŒ–ï¼Œæ¯”å¦‚æƒé‡è¡°å‡å’Œ dropoutï¼Œè¿™ä¸ªæ—¶å€™ç¥ç»ç½‘ç»œèƒ½å¾ˆå¥½çš„ work. ä½†æ˜¯ dropout å¯¹äºå…¨é“¾æ¥ç½‘ç»œæ˜¯ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå®ƒå¯¹äºå·ç§¯ç¥ç»ç½‘ç»œå´æ²¡å•¥æ•ˆæœã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºå·ç§¯ç¥ç»ç½‘ç»œçš„æ¿€æ´»æ˜¯ç©ºé—´ç›¸å…³çš„ï¼Œå³ä½¿ drop æ‰éƒ¨åˆ† unitï¼Œä¿¡æ¯ä»ç„¶ä¼šä¼ é€’åˆ°ä¸‹ä¸€å±‚ç½‘ç»œä¸­å»ã€‚ Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. ä½œè€…ä¸ºå·ç§¯ç¥ç»ç½‘ç»œæå‡ºäº†ä¸“é—¨çš„æ­£åˆ™åŒ–æ–¹å¼ï¼Œ dropblock. åŒæ—¶ drop æ‰ä¸€ä¸ªè¿ç»­çš„ç©ºé—´ã€‚ä½œè€…å‘ç°å°† dropblock åº”ç”¨åˆ° ResNet èƒ½æœ‰æ•ˆçš„æé«˜å‡†ç¡®ç‡ã€‚åŒæ—¶å¢åŠ  drop çš„æ¦‚ç‡èƒ½æé«˜å‚æ•°çš„é²æ£’æ€§ã€‚ å›é¡¾äº†ä¸€ä¸‹ skip/shortcut connection: ç›®çš„æ˜¯é¿å…æ¢¯åº¦æ¶ˆå¤±ã€‚å¯ä»¥ç›´æ¥çœ‹ GRU çš„å…¬å¼ï¼šå‚è€ƒç¬”è®° dropblock In this paper, we introduce DropBlock, a structured form of dropout, that is particularly effective to regularize convolutional networks. In DropBlock, features in a block, i.e., a contiguous region of a feature map, are dropped together. As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data (see Figure 1). å…·ä½“çš„ç®—æ³•å¾ˆç®€å•ï¼Œä¸»è¦å…³æ³¨ä¸¤ä¸ªå‚æ•°çš„è®¾ç½®ï¼š block_size å’Œ $\\gamma$. block_size is the size of the block to be dropped $\\gamma$ controls how many activation units to drop. We experimented with a shared DropBlock mask across different feature channels or each feature channel has its DropBlock mask. Algorithm 1 corresponds to the latter, which tends to work better in our experiments. å¯¹äº channelsï¼Œ ä¸åŒçš„ feature map å…·æœ‰ä¸åŒçš„ dropblock ç›¸æ¯”æ‰€æœ‰çš„ channels å…±äº« dropblock æ•ˆæœè¦å¥½ã€‚ Similar to dropout we do not apply DropBlock during inference. This is interpreted as evaluating an averaged prediction across the exponentially-sized ensemble of sub-networks. These sub-networks include a special subset of sub-networks covered by dropout where each network does not see contiguous parts of feature maps. å…³äº infer æ—¶ï¼Œ dropblock çš„å¤„ç†å’Œ dropout ç±»ä¼¼ã€‚ block_size: In our implementation, we set a constant block_size for all feature maps, regardless the resolution of feature map. DropBlock resembles dropout [1] when block_size = 1 and resembles SpatialDropout [20] when block_size covers the full feature map. block_size è®¾ç½®ä¸º 1 æ—¶, ç±»ä¼¼äº dropout. å½“ block_size è®¾ç½®ä¸ºæ•´ä¸ª feature map çš„ size å¤§å°æ—¶ï¼Œå°±ç±»ä¼¼äº SpatialDropout. setting the value of $\\gamma$: In practice, we do not explicitly set $\\gamma$. As stated earlier, $\\gamma$ controls the number of features to drop. Suppose that we want to keep every activation unit with the probability of keep_prob, in dropout [1] the binary mask will be sampled with the Bernoulli distribution with mean 1 âˆ’ keep_prob. However, to account for the fact that every zero entry in the mask will be expanded by block_size2 and the blocks will be fully contained in feature map, we need to adjust $\\gamma$ accordingly when we sample the initial binary mask. In our implementation, $\\gamma$ can be computed as ä½œè€…å¹¶æ²¡æœ‰æ˜¾ç¤ºçš„è®¾ç½® $\\gamma$. å¯¹äº dropoutï¼Œæ¯ä¸€ä¸ª unit æ»¡è¶³æ¦‚ç‡ä¸º keep_prob çš„ Bernoulli åˆ†å¸ƒï¼Œä½†æ˜¯å¯¹äº dropblock, éœ€è¦è€ƒè™‘åˆ° block_size çš„å¤§å°ï¼Œä»¥åŠå…¶ä¸ feature map size çš„æ¯”ä¾‹å¤§å°ã€‚ keep_prob æ˜¯ä¼ ç»Ÿçš„ dropout çš„æ¦‚ç‡ï¼Œé€šå¸¸è®¾ç½®ä¸º 0.75-0.9. feat_size æ˜¯æ•´ä¸ª feature map çš„ size å¤§å°ã€‚ (feat_size - block_size + 1) æ˜¯é€‰æ‹© dropblock ä¸­å¿ƒä½ç½®çš„æœ‰æ•ˆåŒºåŸŸã€‚ The main nuance of DropBlock is that there will be some overlapped in the dropped blocks, so the above equation is only an approximation. æœ€ä¸»è¦çš„é—®é¢˜æ˜¯ï¼Œä¼šå‡ºç° block_size çš„é‡å ã€‚æ‰€ä»¥ä¸Šè¯‰å…¬å¼ä¹Ÿåªæ˜¯ä¸ªè¿‘ä¼¼ã€‚ Scheduled DropBlock: We found that DropBlock with a fixed keep_prob during training does not work well. Applying small value of keep_prob hurts learning at the beginning. Instead, gradually decreasing keep_prob over time from 1 to the target value is more robust and adds improvement for the most values of keep_prob. å®šåˆ¶åŒ–çš„è®¾ç½® keep_prob, åœ¨ç½‘ç»œåˆæœŸä¸¢å¤±ç‰¹å¾ä¼šé™ä½ preformance, æ‰€ä»¥åˆšå¼€å§‹è®¾ç½®ä¸º 1,ç„¶åé€æ¸å‡å°åˆ° target value. æ‰€ä»¥æ˜¯éšç€ç½‘ç»œæ·±åº¦åŠ æ·±è€Œå˜åŒ–ï¼Œè¿˜æ˜¯éšç€è¿­ä»£æ­¥æ•°å˜åŒ–ï¼Œåº”è¯¥æ˜¯åè€…å§ï¼Œç±»ä¼¼äº scheduled learning rate. Experiments In the following experiments, we study where to apply DropBlock in residual networks. We experimented with applying DropBlock only after convolution layers or applying DropBlock after both convolution layers and skip connections. To study the performance of DropBlock applying to different feature groups, we experimented with applying DropBlock to Group 4 or to both Groups 3 and 4. å®éªŒä¸»è¦åœ¨è®¨è®ºåœ¨å“ªå„¿åŠ  dropblock ä»¥åŠ å¦‚ä½•åœ¨ channels ä¸­åŠ  dropblockã€‚ Variational Dropout","link":"/2018/12/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dropblock/"},{"title":"é‚“å…¬æ•°æ®ç»“æ„ä¸ç®—æ³•5-æ ‘","text":"æ ‘æ ‘ï¼š æ— ç¯è¿é€šå›¾ æå°è¿é€šå›¾ æå¤§æ— ç¯å›¾ ä»»ä¸€èŠ‚ç‚¹ v ä¸æ ¹èŠ‚ç‚¹ r å­˜åœ¨å”¯ä¸€çš„è·¯å¾„ path(v, r) = path( r) æ ‘çš„è¡¨ç¤ºäºŒå‰æ ‘äºŒå‰æ ‘çš„å®ç°å…ˆåºéå†ä¸­åºéå†å±‚æ¬¡éå†é‡æ„","link":"/2019/02/07/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%955-%E6%A0%91/"},{"title":"é‚“å…¬æ•°æ®ç»“æ„ä¸ç®—æ³•1-ç®—æ³•åˆ†æ","text":"ç®—æ³•åˆ†æ è¿­ä»£ã€é€’å½’ åŠ¨æ€è§„åˆ’ ç®—æ³•åˆ†æ çº§æ•°çš„å¤§ O åˆ†æ å¾ªç¯ vs çº§æ•° éæç«¯å…ƒç´ +å†’æ³¡æ’åº ç®—æ³•åˆ†æï¼š é€šè¿‡æŒ–æ˜ç®—æ³•ä¸­çš„ä¸å˜æ€§ã€å•è°ƒæ€§ï¼Œè¿›è€Œè¯æ˜æ­£ç¡®æ€§æ˜¯ç®—æ³•åˆ†æçš„é‡è¦æŠ€å·§ã€‚ è¿­ä»£ä¸é€’å½’å‡è€Œæ²»ä¹‹ (decrease and conquer)æ±‚è§£ä¸€ä¸ªå¤§è§„æ¨¡é—®é¢˜ï¼Œå¯ä»¥å°†å…¶åˆ’åˆ†ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼Œä¸€ä¸ª naiveï¼Œä¸€ä¸ªè§„æ¨¡ç¼©å‡ã€‚ ç®—æ³•åˆ†æï¼šçº¿æ€§é€’å½’ï¼Œä½¿ç”¨é€’å½’è·Ÿè¸ªï¼Œä»…ä½¿ç”¨äºç®€æ˜çš„é€’å½’ example: é¢ å€’æ•°ç»„ä»»æ„ç»™å®šæ•°ç»„ A[0, n), å°†å…¶ä¸­çš„å­åŒºé—´ A[lo, hi] é¢ å€’ 1234567891011121314151617void reverse (int* A, int lo, int hi){ if (lo &lt; hi) { swap (A[lo], A[hi]); reverse (A, lo++, hi--) }} è¿­ä»£ï¼š 123456789101112131415void reverse (int* A, int lo, int hi){ while(lo &lt; hi) { swap(A[lo--], A[hi++]) }} åˆ†è€Œæ²»ä¹‹ (divide and conquer)æ±‚è§£ä¸€ä¸ªå¤§è§„æ¨¡é—®é¢˜ï¼Œå¯ä»¥å°†å…¶åˆ’åˆ†ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼Œè§„æ¨¡å¤§ä½“ç›¸å½“ã€‚ åˆ†åˆ«æ±‚è§£å­é—®é¢˜ ç”±å­é—®é¢˜çš„è§£ï¼Œå¾—åˆ°åŸé—®é¢˜çš„è§£ã€‚ Exampleï¼š MAX2ï¼ŒäºŒåˆ†é€’å½’ä»æ•°ç»„åŒºé—´ A[lo,hi] ä¸­æ‰¾å‡ºæœ€å¤§çš„ä¸¤ä¸ªæ•´æ•°ã€‚ è¿­ä»£è§£æ³• éå†æ•´ä¸ªæ•°ç»„ï¼Œåˆ†åˆ«ä¸ x1, x2 æ¯”è¾ƒã€‚ æ¯è¿­ä»£ä¸€æ¬¡ï¼Œæ¯”è¾ƒ 1/2 æ¬¡ã€‚O(2n-3). é€’å½’+åˆ†æ²»è§£æ³• åˆ†è€Œæ²»ä¹‹ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455void max2(int A[], int lo, int hi, int x1, int x2){ // é€’å½’åŸº, æ€»å…±åªæœ‰ 3ä¸ª/4ä¸ª å…ƒç´  if (lo + 2 == hi) {* ... * ; return;} if (lo + 3 == hi) {* ... * ; return;} // é€’å½’è¿‡ç¨‹ï¼Œåˆ†ä¸ºä¸¤ç»„ï¼Œä¸¤è¾¹è‡³å°‘2ä¸ªå…ƒç´  int mi = (lo + hi)/2; // é€’å½’ int x1L, x2L; max2(A, lo, mi, x1L, x2L); int x1R, x2R; max2(A, mi, hi, x1R, x2R); // æ¯ä¸ªé€’å½’å®ä¾‹æ‰€éœ€æ“ä½œ if (A[x1L] &gt; A[x1R]) { x1 = x1L; x2 = (A[x2L] &gt; A[x1R]) ? x2L : x1R; } else { x1 = x1R; x2 = (A[x1L] &gt; A[x2R]) ? x1L : x2R; }} åŠ¨æ€è§„åˆ’è®°å¿†æ³•ï¼šexample Fabonacci$$fib(n)=fib(n-1)+fib(n-2)$$ 123456789int fib(n){ return (2 &gt; n) ? n : fib(n-1) + fib(n-2);} é€Ÿåº¦å¾ˆæ…¢å¾ˆæ…¢ï¼ å› ä¸ºå¤§é‡çš„é€’å½’å®ä¾‹è¢«é‡å¤çš„è°ƒç”¨ã€‚ è§£å†³æ–¹æ³•ï¼š è®°å¿†ï¼Œé€šè¿‡åˆ¶è¡¨ï¼Œé¿å…é‡å¤è°ƒç”¨ åŠ¨æ€è§„åˆ’ï¼šé¢ å€’è®¡ç®—æ–¹å‘ï¼Œè‡ªé¡¶è€Œä¸‹é€’å½’ï¼Œè½¬æ¢ä¸ºè‡ªåº•è€Œä¸Šè¿­ä»£ è¿™ä¸ªå¯ä»¥ç±»æ¯”ä¸Šå°é˜¶ï¼Œæ¯ä¸€æ­¥å¯ä»¥æ˜¯ 1 æˆ– 2 çº§å°é˜¶ã€‚ é‚£ä¹ˆèµ°åˆ°ç¬¬ n çº§å°é˜¶çš„æ–¹å¼æ˜¯ ç¬¬ n-1 é˜¶ å’Œ ç¬¬n-2 é˜¶çš„æ–¹å¼ä¹‹å’Œã€‚ 123456789101112131415f = 1; g = 0;while( 0 &lt; n-- ){ g = g + f; f = g - f;}return g; ä¸æ–­çš„ä»ä¸‹è€Œä¸Šçš„æ›´æ–° g, f. example: LCS (longest common sequence)é¢˜ç›®è¯¦æƒ…ï¼ˆå­åºåˆ—ä¸åŒäºå­ä¸²ï¼Œå­åºåˆ—å¯ä»¥æ˜¯ä¸è¿ç»­çš„ï¼‰ï¼š æ€è€ƒï¼šä»é€’å½’çš„è§’åº¦å»æƒ³ï¼Œç¼©å°è§„æ¨¡æ— éæ˜¯ åˆ†è€Œæ²»ä¹‹ å’Œ å‡è€Œæ²»ä¹‹ã€‚ å°†è§„æ¨¡ç¼©å°ï¼ŒA[0, n], B[0, m], LCS[A, B] åªæœ‰ä¸‰ç§æƒ…å†µï¼š n=-1 æˆ– m=-1, åˆ™ä¸ºç©º A[n] = B[m] = â€œXâ€, å‡è€Œæ²»ä¹‹ï¼ŒåŒæ—¶ç¼©å°ä¸¤ä¸ªå­—ä¸² LCS(A[0, n-1], B[0, m-1]) + â€œXâ€ $A[n] \\ne B[m]$, åˆ†è€Œæ²»ä¹‹ï¼ŒLCS(A[0, n-1], B[0,m]) å’Œ LCS(A[0, n],B[0, m-1]) ä¸­çš„è¾ƒå¤§å€¼ã€‚ åˆ†æç®—æ³•çš„å¯è¡Œæ€§ï¼š è§£å†³è¿™ç§é€’å½’å®ä¾‹åå¤è°ƒç”¨çš„æ–¹æ³•ï¼š å°†æ‰€æœ‰å­é—®é¢˜åˆ—æˆä¸€å¼ è¡¨ é¢ å€’è®¡ç®—æ–¹å‘ï¼Œä» LCS(A[0], B[0]) ä¾æ¬¡è®¡ç®—æ‰€æœ‰é¡¹ã€‚ ä»ä¸Šå›¾å¯ä»¥å¾ˆæ¸…æ¥šçš„æ˜ç™½åˆ¶è¡¨çš„è¿‡ç¨‹ï¼Œæ¯ä¸€ä¸ªé€’å½’å®ä¾‹éƒ½å¯èƒ½è¢«åå¤çš„ç»è¿‡ã€‚ æ¯”å¦‚ï¼š $A[n] \\ne B[m]$. é‚£ä¹ˆåˆ†ä¸ºä¸¤æ¡è·¯å¾„ï¼š LCS(A[0, n-1], B[0,m]) å’Œ LCS(A[0, n],B[0, m-1]). è¿™ä¸¤ä¸ªçš„è®¡ç®—éƒ½éœ€è¦è®¡ç®— LCS(A[0, n-1], B[0,m-1]), æ‰€ä»¥ä¼šé€ æˆé‡å¤è®¡ç®—ã€‚ æ‰€ä»¥å¦‚ä½•åˆ¶è¡¨å‘¢ï¼Ÿ é€’å½’å…¬å¼ï¼Œ ç”¨ $C[i,j]$ æ¥è¡¨ç¤ºè¡¨æ ¼ä¸­ $[A_i, B_j]$ å¤„çš„é•¿åº¦ã€‚ å…·ä½“å¡«è¡¨çš„è¿‡ç¨‹å‚è€ƒï¼š https://blog.csdn.net/hrn1216/article/details/51534607 æƒ³æ¸…æ¥šè¡¨æ ¼çš„ç‰©ç†æ„ä¹‰ï¼š å¯ä»¥çœ‹ä½œæŸä¸€ä¸ªåºåˆ—å›ºå®šï¼ˆåˆ—ï¼‰ï¼Œç„¶åé€æ¸å¢åŠ å¦ä¸€ä¸ªåºåˆ—çš„é•¿åº¦ï¼ˆè¡Œï¼‰ï¼Œä¹Ÿå°±æ˜¯é€æ¸å¡«å…¥è¡Œã€‚ åˆå§‹çš„æƒ…å†µä¹Ÿè¦æƒ³æ¸…æ¥šï¼ŒæŸä¸€ä¸ªåºåˆ—é•¿åº¦ä¸º 0 æ—¶ï¼Œå…¬å…±åºåˆ—è‚¯å®šä¹Ÿä¸º 0ã€‚ $m\\times n$ çš„è¡¨æ ¼ åˆå§‹åŒ–ï¼Œ i=0 æˆ– j=0 å…ˆå¡«å…¥ç¬¬ä¸€è¡Œï¼Œä¸€ä¸ªå­—ç¬¦ä¸º â€œ3â€ï¼Œæ²¡æœ‰ä¸ä¹‹ç›¸åŒçš„å­—ç¬¦ï¼Œæ‰€æœ‰è¿™ä¸€è¡Œéƒ½ä¸º 0. å†å¡«å…¥ç¬¬äºŒè¡Œï¼Œå¡« [2,1] å¤„ï¼Œéƒ½ä¸º3ï¼ŒC[2,1]=C[1,1]+1ï¼Œç„¶åæŒ‰ç…§é€’å½’å…¬å¼èµ°ä¸‹å»ã€‚ ç¬¬äºŒè¡Œå…¶ä»–ç©ºæ ¼ $A[i]\\ne B[j]$ï¼ŒC[i,j]=max(C[i-1,j], C[i, j-1]) å›æº¯æ„é€  LCS æˆ‘ä»¬æ ¹æ®é€’å½’å…¬å¼æ„å»ºäº†ä¸Šè¡¨ï¼Œæˆ‘ä»¬å°†ä»æœ€åä¸€ä¸ªå…ƒç´ c[8][9]å€’æ¨å‡ºS1å’ŒS2çš„LCSã€‚ C[8, 9] = 5ï¼Œä¸”$S_1[8] \\ne S_2[9]$ï¼Œæ‰€ä»¥å€’æ¨å›å»ï¼ŒC[8,9]çš„å€¼æ¥æºäº C[8,8]çš„å€¼(å› ä¸ºC[8,8] &gt; c[7,9]) C[8, 8] = 5, ä¸” $S_1[8] = S_2[8]$, æ‰€ä»¥å€’æ¨å›å»ï¼ŒC[8,8]çš„å€¼æ¥æºäº C[7, 7]ã€‚ å¯èƒ½ä¼šå‡ºç°åˆ†æ­§çš„æƒ…å†µï¼Œ$S_1[i] \\ne S_2[j]$, ä¸” C[i-1,j]=C[i, j-1]. è¿™ä¸ªæ—¶å€™é€‰æ‹©ä¸€ä¸ªæ–¹å‘ã€‚ ä¸¤ç§ç»“æœï¼š åˆ†æ­§å¤„é€‰æ‹©å¦ä¸€ä¸ªæ–¹å‘ã€‚ æ—¶é—´å¤æ‚åº¦åˆ†æï¼š æ„å»ºè¡¨æ ¼éœ€è¦ O(mn), å›æº¯è¾“å‡ºä¸€ä¸ª LCS éœ€è¦ O(m+n) leetcode 300. Longest increasing sequenceGiven an unsorted array of integers, find the length of longest increasing subsequence. Example: Input: [10,9,2,5,3,7,101,18] Output: 4 Explanation: The longest increasing subsequence is [2,3,7,101], therefore the length is 4. Note: There may be more than one LIS combination, it is only necessary for you to return the length. Your algorithm should run in O(n2) complexity. Follow up: Could you improve it to O(n log n) time complexity? Brute Force [Time Limit Exceeded]: recursive12345678910111213141516171819202122232425262728293031323334353637int lenofLIS(vector&lt;int&gt; &amp;nums, int prev, int curpos);class Solution {public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) { return lenofLIS(nums, INT_MIN, 0); }};int lenofLIS(vector&lt;int&gt; &amp;nums, int prev, int curpos){ if (curpos == nums.size()) return 0; int taken = 0; if (nums[curpos] &gt; prev) taken = 1 + lenofLIS(nums, nums[curpos], curpos+1); int notaken = lenofLIS(nums, prev, curpos+1); return max(taken, notaken);} æ—¶é—´å¤æ‚åº¦åˆ†æï¼š æ•°ç»„ä¸­æ¯ä¸€ä¸ªä½ç½®éƒ½å¯èƒ½å‡ºç°æˆ–è€…ä¸å‡ºç°åœ¨ LIS ä¸­ï¼Œä¹Ÿå°±æ˜¯è¯´æ¯ä¸€ä¸ªé€’å½’å®ä¾‹çš„æ“ä½œæ˜¯ 2ï¼Œæ‰€ä»¥æœ€ç»ˆæ—¶é—´å¤æ‚åº¦æ˜¯ $O(2^n)$ ç©ºé—´å¤æ‚åº¦åˆ†æï¼š$O(n^2)$ Recursion with memorization [Memory Limit Exceeded]è§£å†³ä¸Šé¢è¿™ç§é€’å½’å®ä¾‹åå¤è°ƒç”¨çš„é—®é¢˜ï¼Œé€šå¸¸æœ‰ä¸¤ç§æ–¹æ³•ï¼Œåœ¨ä¸Šé¢å­¦ä¹ ä¸­ä¹Ÿè¯´åˆ°äº†ã€‚è¿™é‡Œå…ˆé‡‡ç”¨è®°å¿†æ³•ï¼Œå°†æ‰€æœ‰å­é—®é¢˜åˆ—æˆä¸€å¼ è¡¨ã€‚ Dynamic Programming [Accepted]å‚è€ƒï¼š https://www.geeksforgeeks.org/longest-increasing-subsequence-dp-3/ Optimal Substructure: Let arr[0..n-1] be the input array and L(i) be the length of the LIS ending at index i such that arr[i] is the last element of the LIS. Then, L(i) can be recursively written as: L(i) = 1 + max( L(j) ) where 0 &lt; j &lt; i and arr[j] &lt; arr[i]; or L(i) = 1, if no such j exists. To find the LIS for a given array, we need to return max(L(i)) where 0 &lt; i &lt; n. Thus, we see the LIS problem satisfies the optimal substructure property as the main problem can be solved using solutions to subproblems. Following is a simple recursive implementation of the LIS problem. It follows the recursive structure discussed above. 1234567891011121314151617181920212223242526272829303132333435363738394041int lenofLIS(vector&lt;int&gt; &amp;nums, int prev, int curpos);class Solution {public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) { if (nums.size() == 0) return 0; int dp[nums.size()]; dp[0] = 1; for (int i=1; i&lt; nums.size(); i++){ dp[i] = 1; for (int j=0; j &lt; i; j++){ if (nums[i] &gt; nums[j] &amp;&amp; dp[i] &lt; dp[j]+1){ dp[i] = dp[j] + 1; } } } return *max_element(dp, dp+nums.size()); }}; å¤æ‚åº¦åˆ†æï¼š Time complexity: $O(n^2)$ Two loops of n. Space complexity: $O(n)$ dp array size n is used. æ€»ç»“å†å›è¿‡å¤´æ€è€ƒä¸‹åŠ¨æ€è§„åˆ’çš„å«ä¹‰ï¼š åŠ¨æ€è§„åˆ’å¸¸å¸¸é€‚ç”¨äºæœ‰é‡å å­é—®é¢˜[1]å’Œæœ€ä¼˜å­ç»“æ„æ€§è´¨çš„é—®é¢˜ï¼ŒåŠ¨æ€è§„åˆ’æ–¹æ³•æ‰€è€—æ—¶é—´å¾€å¾€è¿œå°‘äºæœ´ç´ è§£æ³•ã€‚ åŠ¨æ€è§„åˆ’èƒŒåçš„åŸºæœ¬æ€æƒ³éå¸¸ç®€å•ã€‚å¤§è‡´ä¸Šï¼Œè‹¥è¦è§£ä¸€ä¸ªç»™å®šé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦è§£å…¶ä¸åŒéƒ¨åˆ†ï¼ˆå³å­é—®é¢˜ï¼‰ï¼Œå†æ ¹æ®å­é—®é¢˜çš„è§£ä»¥å¾—å‡ºåŸé—®é¢˜çš„è§£ã€‚ é€šå¸¸è®¸å¤šå­é—®é¢˜éå¸¸ç›¸ä¼¼ï¼Œä¸ºæ­¤åŠ¨æ€è§„åˆ’æ³•è¯•å›¾ä»…ä»…è§£å†³æ¯ä¸ªå­é—®é¢˜ä¸€æ¬¡ï¼Œä»è€Œå‡å°‘è®¡ç®—é‡ï¼šä¸€æ—¦æŸä¸ªç»™å®šå­é—®é¢˜çš„è§£å·²ç»ç®—å‡ºï¼Œåˆ™å°†å…¶è®°å¿†åŒ–å­˜å‚¨ï¼Œä»¥ä¾¿ä¸‹æ¬¡éœ€è¦åŒä¸€ä¸ªå­é—®é¢˜è§£ä¹‹æ—¶ç›´æ¥æŸ¥è¡¨ã€‚è¿™ç§åšæ³•åœ¨é‡å¤å­é—®é¢˜çš„æ•°ç›®å…³äºè¾“å…¥çš„è§„æ¨¡å‘ˆæŒ‡æ•°å¢é•¿æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚ å¯¹äº longest increasing subsequence é‡Œé¢å°±å­˜åœ¨æœ€æœ‰å­é—®é¢˜ã€‚å¯¹äºä¸€ä¸ªåºåˆ—ï¼Œæ¯å¢åŠ ä¸€ä¸ªå…ƒç´ ï¼Œéƒ½å¯ä»¥çœ‹ä½œä¸€ä¸ªå­é—®é¢˜ã€‚ å­é—®é¢˜çš„è§£æ˜¯å›ºå®šçš„ï¼Œä½†æ˜¯å­é—®é¢˜ä¸å½“å‰æ­¥çš„ç»“åˆåˆæ˜¯åŠ¨æ€å˜åŒ–çš„ã€‚æ¯”å¦‚è¿™é‡Œï¼Œå½“å‰ i ä½ç½®çš„å€¼å¤§äº j çš„å€¼å’Œå°äº j çš„å€¼çš„å¤„ç†æ–¹å¼å°±ä¸å¤ªä¸€æ ·ã€‚æˆ‘ä»¬è¦åšçš„å°±æ˜¯æ‰¾å‡ºè¿™ä¸ªè§„å¾‹ ï¼ˆé€’æ¨å…¬å¼ï¼‰ï¼Œç„¶åæ ¹æ®å¡«å†™å¥½çš„å­é—®é¢˜çš„è§£çš„è¡¨æ ¼ï¼Œè¿›ä¸€æ­¥æ‰©å¤§é—®é¢˜è§„æ¨¡ã€‚ æ‰€ä»¥å¦‚å‰é¢æ‰€è¯´ï¼ŒåŠ¨æ€è§„åˆ’ï¼šè‡ªé¡¶è€Œä¸‹é€’å½’ï¼Œè‡ªåº•è€Œä¸Šè¿­ä»£ã€‚","link":"/2018/12/19/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%951-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/"}],"tags":[{"name":"generation","slug":"generation","link":"/tags/generation/"},{"name":"CSAPP","slug":"CSAPP","link":"/tags/CSAPP/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"machine translation","slug":"machine-translation","link":"/tags/machine-translation/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"ai challenger","slug":"ai-challenger","link":"/tags/ai-challenger/"},{"name":"cs224d","slug":"cs224d","link":"/tags/cs224d/"},{"name":"sign language recognition","slug":"sign-language-recognition","link":"/tags/sign-language-recognition/"},{"name":"sentiment classification","slug":"sentiment-classification","link":"/tags/sentiment-classification/"},{"name":"Machine Translation","slug":"Machine-Translation","link":"/tags/Machine-Translation/"},{"name":"open set recognition","slug":"open-set-recognition","link":"/tags/open-set-recognition/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/tags/Tensorflow/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/tags/TensorFlow/"},{"name":"GANï¼ŒRL","slug":"GANï¼ŒRL","link":"/tags/GAN%EF%BC%8CRL/"},{"name":"æ–‡æœ¬åˆ†ç±»","slug":"æ–‡æœ¬åˆ†ç±»","link":"/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"MRC and QA","slug":"MRC-and-QA","link":"/tags/MRC-and-QA/"},{"name":"capsules","slug":"capsules","link":"/tags/capsules/"},{"name":"vision transformer","slug":"vision-transformer","link":"/tags/vision-transformer/"},{"name":"vision-language","slug":"vision-language","link":"/tags/vision-language/"},{"name":"æ•°æ®ç»“æ„ä¸ç®—æ³•","slug":"æ•°æ®ç»“æ„ä¸ç®—æ³•","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"generative models","slug":"generative-models","link":"/tags/generative-models/"},{"name":"ESA","slug":"ESA","link":"/tags/ESA/"},{"name":"language model","slug":"language-model","link":"/tags/language-model/"},{"name":"text matching","slug":"text-matching","link":"/tags/text-matching/"},{"name":"contrastive learning","slug":"contrastive-learning","link":"/tags/contrastive-learning/"},{"name":"data augmentation","slug":"data-augmentation","link":"/tags/data-augmentation/"},{"name":"transformer","slug":"transformer","link":"/tags/transformer/"},{"name":"sign language","slug":"sign-language","link":"/tags/sign-language/"},{"name":"sentence embedding","slug":"sentence-embedding","link":"/tags/sentence-embedding/"},{"name":"transfer learning","slug":"transfer-learning","link":"/tags/transfer-learning/"}],"categories":[{"name":"generation","slug":"generation","link":"/categories/generation/"},{"name":"CSAPP","slug":"CSAPP","link":"/categories/CSAPP/"},{"name":"interview","slug":"interview","link":"/categories/interview/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/categories/reinforcement-learning/"},{"name":"machine translation","slug":"machine-translation","link":"/categories/machine-translation/"},{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"DRL","slug":"DRL","link":"/categories/DRL/"},{"name":"cs224d","slug":"cs224d","link":"/categories/cs224d/"},{"name":"sign language recognition","slug":"sign-language-recognition","link":"/categories/sign-language-recognition/"},{"name":"è®ºæ–‡ç¬”è®°","slug":"è®ºæ–‡ç¬”è®°","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"pytorch","slug":"pytorch","link":"/categories/pytorch/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/categories/TensorFlow/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"GAN","slug":"GAN","link":"/categories/GAN/"},{"name":"GAN, RL","slug":"GAN-RL","link":"/categories/GAN-RL/"},{"name":"æ–‡æœ¬åˆ†ç±»","slug":"æ–‡æœ¬åˆ†ç±»","link":"/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"Machine Translation","slug":"è®ºæ–‡ç¬”è®°/Machine-Translation","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"},{"name":"ML","slug":"ML","link":"/categories/ML/"},{"name":"open set recognition","slug":"è®ºæ–‡ç¬”è®°/open-set-recognition","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"},{"name":"DL","slug":"è®ºæ–‡ç¬”è®°/DL","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"},{"name":"MRC and QA","slug":"è®ºæ–‡ç¬”è®°/MRC-and-QA","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"},{"name":"NLP","slug":"è®ºæ–‡ç¬”è®°/NLP","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"},{"name":"capsules","slug":"è®ºæ–‡ç¬”è®°/capsules","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"},{"name":"Transformer","slug":"è®ºæ–‡ç¬”è®°/Transformer","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"},{"name":"vision-language","slug":"è®ºæ–‡ç¬”è®°/vision-language","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"},{"name":"æ•°æ®ç»“æ„ä¸ç®—æ³•","slug":"æ•°æ®ç»“æ„ä¸ç®—æ³•","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"generative models","slug":"generative-models","link":"/categories/generative-models/"},{"name":"GAN","slug":"è®ºæ–‡ç¬”è®°/GAN","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"},{"name":"ESA","slug":"è®ºæ–‡ç¬”è®°/ESA","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"},{"name":"language model","slug":"è®ºæ–‡ç¬”è®°/language-model","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"},{"name":"text matching","slug":"è®ºæ–‡ç¬”è®°/text-matching","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"},{"name":"constrast learning","slug":"è®ºæ–‡ç¬”è®°/constrast-learning","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"},{"name":"data augmentation","slug":"è®ºæ–‡ç¬”è®°/data-augmentation","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"},{"name":"transformer","slug":"transformer","link":"/categories/transformer/"},{"name":"machine translation","slug":"è®ºæ–‡ç¬”è®°/machine-translation","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"},{"name":"computer vision","slug":"è®ºæ–‡ç¬”è®°/computer-vision","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"},{"name":"sentence embedding","slug":"è®ºæ–‡ç¬”è®°/sentence-embedding","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"},{"name":"dialogue system","slug":"è®ºæ–‡ç¬”è®°/dialogue-system","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"},{"name":"transfer learning","slug":"transfer-learning","link":"/categories/transfer-learning/"}]}