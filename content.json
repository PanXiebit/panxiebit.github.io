{"pages":[{"title":"","text":"**Pan Xie** *New main building E813, Beihang University, Beijing, 100191, China* Email: [ftdpanxie@gmail.com](ftdpanxie@gmail.com) Homepage: [panxiaoxie.cn](panxiaoxie.cn) Github: [https://github.com/PanXiebit](https://github.com/PanXiebit) Research Interest: machine translation, natural language generation","link":"/resume/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"leetcode","text":"Link: 数据结构和算法学习指南 (qq.com) 手把手刷链表题目 (学完一段时间之后，需要再回过头来再看一遍) 数据结构的存储方式 数组（顺序存储）：紧凑连续存储,可以随机访问，通过索引快速找到对应元素，而且相对节约存储空间。但正因为连续存储，内存空间必须一次性分配够，所以说数组如果要扩容，需要重新分配一块更大的空间，再把数据全部复制过去，时间复杂度 O(N)；而且你如果想在数组中间进行插入和删除，每次必须搬移后面的所有数据以保持连续，时间复杂度 O(N)。 链表（链式存储）：元素不连续，而是靠指针指向下一个元素的位置，所以不存在数组的扩容问题；如果知道某一元素的前驱和后驱，操作指针即可删除该元素或者插入新元素，时间复杂度 O(1)。但是正因为存储空间不连续，你无法根据一个索引算出对应元素的地址，所以不能随机访问；而且由于每个元素必须存储指向前后元素位置的指针，会消耗相对更多的储存空间。 刷题方法数据结构的基本存储方式就是链式和顺序两种，基本操作就是增删查改，遍历方式无非迭代和递归。 刷算法题建议从「树」分类开始刷，结合框架思维，把这几十道题刷完，对于树结构的理解应该就到位了。这时候去看回溯、动规、分治等算法专题，对思路的理解可能会更加深刻一些。 手把手刷链表题目 递归反转链表：如何拆解复杂问题 单链表的六大解题套路 Leetcode21 合并两个有序链表 虚拟头节点 Leetcode23 合并k个升序链表 关键在于如何在迭代过程中找到k个链表中最小的节点：可使用优先级队列（二叉堆），每次提取出最小的节点，也就是最小堆，并维护这个最小堆( O(NlogN) ) Leetcode19 删除链表的倒数第N个节点 双指针 p and p+N Leetcode876 单链表的中点 双指针 Leetcode141 判断链表是否包含环 双指针 slow and fast 如何找到环的起点 Leetcode160 判断两个链表是否相交","link":"/leetcode.html"}],"posts":[{"title":"C plus plus prime-类","text":"类定义类头、类体、类域 数据成员数据成员声明在类体中，可以是任何类型，eg. 指针，类 分为：非静态（nonstatic），静态（static）. 成员函数成员函数在类域之外是不可见的。通过 “.” 或 “-&gt;” 来引用。 注意区分全局域/全局函数，类域/成员函数。 成员访问信息隐藏（information hiding）：类成员的访问限制，通过访问限定符来实现的。 public 公有成员，提供给用户使用的 private 只能被成员函数和友元访问 protected 对派生类表现的像 public, 对其他程序表现的像 private. 友元关键字 find， 友元可以是一个名字空间函数、一个前面定义的类的一个成员函数、也可以是一个完整的类。 允许一个类授权其他的函数访问他的非公有成员，通常声明放在类头之后。 类声明和定义类声明，是只有类头，没有类体。无法确定类类型的大小，类成员也是未知的。但是可以声明指向该类类型的指针或引用。 类定义，是具有完整的类体。 什么时候用类声明？什么时候用类定义？ 类对象类定义不会分配存储区，只有定义了一个类的对象，才会分配。 类类型，即定义的一个类。通过它定义的对象是有生命期的，生命期根据它在哪个域中被声明的。 类对象可以被另一个对象初始化或赋值，拷贝一个类对象与拷贝它的成员函数等价。 当一个类对象被指定为函数实参或函数返回值时，它就被按值传递。我们可以把一个函数参数或返回值声明为一个类类型的指针或引用。（7.3节，7.4节） 回顾下指针： 123456789int i = 100;int *p;int* p = &amp;i;int *p = &amp;i; p 是指针变量，用来存储地址。注意数组名是 const 地址常量，代表第一个元素的地址。 指针和引用的区别： 指针：指针是一个变量，只不过这个变量存储的是一个地址，指向内存的一个存储单元；而引用跟原来的变量实质上是同一个东西，只不过是原变量的一个别名而已。如： 12345int a=1; int *p=&amp;a;int a=1; int &amp;b=a; 上面定义了一个整形变量和一个指针变量p，该指针变量指向a的存储单元，即p的值是a存储单元的地址. 而下面2句定义了一个整形变量a和这个整形a的引用b，事实上a和b是同一个东西，在内存占有同一个存储单元。 “sizeof引用”得到的是所指向的变量(对象)的大小，而”sizeof指针”得到的是指针本身的大小； 可以有const指针，但是没有const引用； 指针的值可以为空，但是引用的值不能为NULL，并且引用在定义的时候必须初始化； 指针的值在初始化后可以改变，即指向其它的存储单元，而引用在进行初始化后就不会再改变了 继续回到类对象作为函数参数时，用成员访问操作符来访问类对象的数据成员或成员函数。点操作符与类对象或引用连用; 箭头访问操作符与类对象的指针连用。 1234567891011121314151617# include &quot;Screen.h&quot;bool isEqual(Screen&amp; s1, Screen *s2){ ... s1.height ... s2-&gt;height ... ...} isEqual 是非成员函数，如果要直接引用 s1, s2 中的数据成员 height 是不可以的。所以必须借助于 Screen 中的公有成员函数。 123s2-&gt;height 等价于 (*s2).height 类成员函数一组操作的集合。 inline 和 非 inline 成员函数内联函数的作用：如果在程序中调用某个函数，不但要拷贝实参，保存机器的寄存器，程序还必须转向一个新的位置，这样会降低效率。 而内联函数就是解决这个问题的，在程序的调用节点上，“内联”的展开函数的操作，从而额外的执行开销被消除了。 类体内定义的成员函数自动的作为内联函数处理。 类体内声明，类体外定的函数，需要显示的加上关键字 inline. 同时类体外的定义需要用类名限定修饰(限定修饰符 :: ) 访问类成员成员函数的定义可以引用任何一个类成员，无论该成员是私有还是公有。 成员函数可以直接访问它所属类的成员，无需访问操作符。这实际上是通过 this 指针实现的。 私有与公有成员函数公有函数集定义了类的接口。私有成员函数为其他成员函数提供支持。 特殊的成员函数构造函数：管理类对象并处理初始化、赋值、内存管理、类型转换、析构等活动。每次定义一个类对象或 new 表达式分配一个类对象都会调用它。 构造函数的名字必须与类名相同。 const 和 volatile 成员函数只有被声明为 const 的成员函数才能被一个 const 对象调用。关键字 const 在函数体和参数表之间. 通常一个类如果想被广泛使用，其中不修改类数据成员的成员函数应该声明为 const 成员函数。 但是即使声明了，这个成员函数依然有可能修改数据成员，如果这个类含有指针。 比如： 12345private: char *_text; _text 不能被修改，但是它指向的字符却是可以被修改的。所以程序员这个时候就需要注意了。。 构造函数和析构函数即使不是 const 成员函数，也能被 const 对象调用。 volatile 跟 const 用法一样，它用来提示编译器该对象的值可能在编译器未被检测到的情况下被修改。因为，编译器不能武断的对引用这些对象的代码进行优化。 mutable 数据成员一旦一个对象被声明为 const，它的内容就不能修改。但是其中某些数据成员，比如索引，被修改之后并没有修改对象本身，这个时候就可以将该数据成员（也就是索引）声明为 mutable. 那样，即使 const 对象， const 成员函数修改了该数据成员，也不会有编译错误。 隐含的指针 this每个类成员函数都含有一个指向被调用对象的指针。但是一般不需要显示的写出来，如果写出来也是可以的。 何时使用指针呢？当连续使用成员函数时，比如 myVector.find().sort().insert() 时，对应的成员函数返回的值应该是被调用对象本身，也就是 *this. 还有一种情况， copy 函数： 123456789101112131415void Screen::copy(const Screen&amp; sobj){ if (this != &amp;sobj) { // 把 sobj 的值拷贝到 * this 中 }} 这里的 this 指针含有被调用对象的地址。如果 sobj 的地址 &amp;sobj 与 this 相同，那就不需要拷贝了。 注意这里 &amp; 用法：引用和取地址。 静态类成员静态数据成员对于非静态成员，每个类都有一个自己的拷贝。而静态成员对每个类类型只有一个拷贝。静态数据成员只有一份，由该类类型的所有对象共同访问。 不同于全局对象，它可以隐藏，并且不会与其他全局名字冲突。 关键字 static. 注意与 const 的区别，没有加 const 意味着是可以更新的。只需要更新一次，所有的类对象对应的值都会更新。 静态类成员的显示初始化，在类定义之外，用类名限定修饰。在静态数据成员的定义中也可以直接使用私有成员，这与在类成员函数中直接引用私有成员是一样的。 对于静态数据成员，除了通过类对象使用成员访问操作符访问之外，还可以直接使用类名加限定修饰符访问 123Account::_intersetRate 静态成员函数静态成员函数，只访问静态数据成员，而不访问任何其他数据成员。所以它们与哪个对象来调用这个函数无关。 在类体中声明时需要加关键字 static, 类体外不能指定关键字。并且不能设定为 const 和 volatile. 指向类成员的指针普通函数指针 7.9节成员函数指针","link":"/2018/12/12/Cplusplus-prime/"},{"title":"Deep Generative Models","text":"Catalog: Autoregressive Models (ARMs) Flow-based models (flows): RealNVP and IDFs (Integer Discrete Flows) Variational Auto-Encoders (VAEs): a plain VAE and various priors, a hierarchical VAE Hybrid modeling Energy-based Models Generative Adversarial Networks (GANs) Diffusion-based Deep Generative Models (DDGMs): a Gaussian forward diffusion Neural Compression with Deep Generative Modeling VAEs 以图像生成为例，想象一下这个场景，我们现在有一堆马🐎的图片，我们现在希望学习 $p(x)$，从而生成新的图片。那么我们可以问下自己，我们如何去画出马的图片呢？换句话说，我们把自己当作一个生成模型，我们如何做这件事呢？也许我们会先勾勒出一匹马的大致轮廓，它的大小和形状，然后添加马蹄，填充头部的细节，给它上色等等。最后，我们可以考虑背景。一般来说，我们可以说数据中有一些因素（例如轮廓、颜色、背景）对于生成对象（这里是马）至关重要。 一旦我们决定了这些因素，我们就可以通过添加细节来生成它们。 当我们画某物时，这或多或少是我们生成一幅画的过程。 我们现在用数学来表达这个生成过程。 也就是说，我们有我们感兴趣的高维对象 $x\\in \\mathcal{X}^D$（例如，对于图像，$\\mathcal{X}\\in {0,1,2,…,255}$）和一个低维潜在变量 $z\\in\\mathcal{Z}^M$（例如，$\\mathcal{Z}=\\mathbb{R}$)，我们可以称之为数据中的隐藏因素。 在数学上，我们可以将$\\mathcal{Z}^M$称为低维流形。 那么，生成过程可以表示为： $z\\sim p(z)$ (Figure1, In red) $x\\sim p(x|z)$(Figure1, In Blue) Figure 1. 潜在变量模型和生成过程的简图. 注意嵌入在高维空间(此处为3D)中的低维流形(此处为2D) 简单来说，我们首先采样𝐳（例如，我们想象我的马的大小、形状和颜色），然后创建具有所有必要细节的图像，即，我们从条件分布 $p(x|z)$中采样得到x。 由于许多各种外部因素，创建两次完全相同的图像几乎是不可能的。 潜在变量模型背后的idea是我们引入了潜在变量 $z$ ，并且联合分布被分解如下：$ 𝑝(𝐱,𝐳)=𝑝(𝐱|𝐳)𝑝(𝐳)$. 这个公式表达的就是上述的生成过程。 但是在训练时我们只能访问样本 $x$。 因此，根据概率推理，我们应该对未知的潜在变量，也就是𝐳 ，进行求和（sum out, 也叫边缘化 marginalize out）。最后，其边缘似然函数如下： $$p(\\mathbf{x}) = \\int p(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})\\ \\mathrm{d} \\mathbf{z} $$ 那么问题来了，如何计算这个积分呢？总的来说，这是个非常复杂的任务。有两个可能的解决方法：1. 直接处理这个积分 (Probabilistic PCA )；2. 利用近似的方法也来解决，也就是变分推断 (variational inference). Probabilistic PCA对于 $p(x|z)$是线性模型时，可以直接求解。但推导过程没看懂，先不写了。 Variational Inference for Non-linear Latent Variable Models模型和目标 让我们再看一次积分，并考虑我们无法准确计算积分的一般情况。 最简单的方法是使用蒙特卡罗近似：$$\\begin{align}p(\\mathbf{x}) &amp;= \\int p(\\mathbf{x} | \\mathbf{z})\\ p(\\mathbf{z})\\ \\mathrm{d} \\mathbf{z} \\\\ &amp;= E_{\\mathbf{z}\\sim p(\\mathbf{z})} \\left[ p(\\mathbf{x} | \\mathbf{z}) \\right] \\\\ &amp;\\approx \\frac{1}{K} \\sum_{k} p(\\mathbf{x} | \\mathbf{z}_{k}) \\end{align}$$其中，在最后一行我们采用近似的方法来模拟这个期望/积分函数。我们基于潜在变量的的先验概率来得到samples，$z_k\\sim p(z)$. 在当前计算机越来越快的情况下，这个方法是相对简单的，我们可以在短时间内采样出无数的点。然而，我们学过的统计学知识告诉我们，当潜在空间是$z\\in\\mathcal{Z}^M$多维，且$M$很大的时候，我们会陷入维度灾难。为了cover住样本空间，我们所需采样的样本是M的指数形式。如果我们采样的样本数不够，那么这个近似效果就不好。 我们当然可以采用一些更先进的蒙特卡洛方法，然而，它们始终会受到维度灾难的影响。另一个可选择的近似方法是变分推断variational inference (Jordan et al., 1999). 我们考虑一组由 $\\phi$ 参数化的变分分布，${q_{\\phi}(z)}$. 比如，我们假设$\\phi$为高斯分布， $\\phi={\\mu, \\sigma^2}$. 我们知道这些分布的形式，并假设它们将非零的概率分配给所有的潜在变量 $z\\in \\mathcal{Z}^{M}$. 然后，这个边缘概率分布可以近似推导如下：$$\\begin{align}\\ln p(x) &amp;= ln\\int p(x|z)p(z)dz \\\\&amp;= ln\\int \\dfrac{q_{\\phi}(z)}{q_{\\phi}(z)}p(x|z)p(z)dz \\\\&amp;= lnE_{z\\sim q_{\\phi}(z)}[\\dfrac{p(x|z)p(z)}{q_{\\phi}(z)}] \\\\&amp;\\ge E_{z\\sim q_{\\phi}(z)}ln[\\dfrac{p(x|z)p(z)}{q_{\\phi}(z)}] \\\\&amp;= E_{z\\sim q_{\\phi}(z)} [lnp(x|z)+lnp(z)-lnq_{\\phi}(z)] \\\\&amp;= E_{z\\sim q_{\\phi}(z)} [lnp(x|z)] - E_{z\\sim q_{\\phi}(z)}[lnq_{\\phi}(z) - lnp(z)]\\end{align}$$第4行使用了Jensen’s inequality. 上述推导过程，我们把 $q_{\\phi}(z)$ 换成 amortized variational posterior，也就是 $q_{\\phi}(z|x)$ 是不影响推导过程的，因此我们可以得到： $$lnp(z)\\ge E_{z\\sim q_{\\phi}(z|x)} [lnp(x|z)] - E_{z\\sim q_{\\phi}(z|x)}[lnq_{\\phi}(z|x) - lnp(z)]$$ amortized variational posterior 非常有用，因为我们可以利用神经网络得到这样一个模型，给定输入 $x$，然后输出对应分布的参数。在 (Kim et al., 2018) 中，作者使用了一种 semi-amortized variational inference. 最后，我们得到一个auto encoder-like模型，其包括一个encoder, $q_{\\phi}(z|x)$ 和 一个 decoder, $p(x|z)$. 我们用随机性来强调encoder和decoder其实就是概率分布，这与deterministic auto-encoder是有区别的（这里没太懂）。这个带有 amortized variational posterior的自编码模型就是变分自编码 Variational Auto-Encoder (Kingma &amp; Welling, 2013; Rezende et al., 2014). 其似然函数的下界就是 Evidence LOwer Bound (ELBO). ELBO的第一项是 reconstruction error,$E_{z\\sim q_{\\phi}(z|x)} [lnp(x|z)]$ ; 第二项是regularizer，$E_{z\\sim q_{\\phi}(z|x)}[lnq_{\\phi}(z|x) - lnp(z)]$,恰好就是KL散度，所以也可以成为KL item. 但在更复杂的模型中，这一项并不一定是 KL term，因此称作regularizer更通用一点。 回顾下：熵 $\\rightarrow$ 交叉熵 $\\rightarrow$ 相对熵/KL散度 信息量与概率成反比，当概率越大时，信息量越小；并且信息量为非负数。因此，定义信息量为 $log\\dfrac{1}{p}$. 熵是信息量的期望：$E_{x\\sim p(x)}ln\\dfrac{1}{p(x)}$ 交叉熵指的是：根据真实分布p来衡量预测分布q的度量。同样的我们也用信息量的期望来衡量这个交叉熵度量，我们希望交叉熵越小时，预测的q越准确，也就是越接近于1。类似地，交叉熵可以写成 $H(p, q) = E_{x\\sim p(x)}ln\\dfrac{1}{q(x)}$. 实际上，我们是无法知道真实分布p的，只能依据现有的样本统计得到。 1234567891011121314151617181920212223import torchimport torch.nn as nnimport torch.nn.functional as Fsize = 3input = torch.randn(2, size)target = torch.Tensor([0, 2]).long()# use loss functionloss_fn = nn.CrossEntropyLoss(reduction=&quot;mean&quot;)loss = loss_fn(input, target)# computer nll loss step by stepscore = torch.log_softmax(input, dim=1)my_nll = torch.sum(-score * F.one_hot(target, size)) / target.size(0)# use nll lossnll_loss_fn = nn.NLLLoss()nll_loss = nll_loss_fn(score, target)print(nll_loss == loss == my_nll) # True 相对熵： 根据Gibbs’ inequality上述例子中的 $H(p,q) &gt;= H(p)$ 恒成立。当且仅当q=p时，这个等号才成立。那么熵H(p,q)相比熵H(q)多出来的部分就是相对熵 ，也称为KL散度(Kullback–Leibler divergence，KLD). $$D(p||q)=H(p,q)-H(p)=\\sum_{x\\sim p(x)}[ln\\dfrac{1}{q(x)} - ln\\dfrac{1}{p(x)}]=E_{x\\sim p(x)}[ln\\dfrac{p(x)}{q(x)}]=\\int_{x}p(x)[ln\\dfrac{p(x)}{q(x)}]$$ A different perspective on the ELBO下面提供一种新的推导方法，个人觉得更能理解ELBO:$$\\begin{align}\\ln p(\\mathbf{x}) &amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}) \\right] \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln \\frac{p(\\mathbf{z}|\\mathbf{x}) p(\\mathbf{x})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln \\frac{p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{根据贝叶斯公式p(z|x)p(x)=p(x|z)p(z)}\\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln \\frac{p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{p(\\mathbf{z}|\\mathbf{x})} \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\right] \\quad \\text{这一步很关键, } 1 = \\frac{q_{\\phi(z|x)}}{q_{\\phi(z|x)}}, q_{\\phi(z|x)} 是用来近似真实后验概率的变分后验概率 \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\frac{p(\\mathbf{z})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) - \\ln \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{p(\\mathbf{z})} + \\ln \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{p(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{到这里能看出KL散度了}\\\\&amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\right] - KL\\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}) \\right] + KL \\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) |p(\\mathbf{z}|\\mathbf{x}) \\right] .\\end{align}$$前两项就是 ELBO，最后一项中 $p(z|x)$ 这个表示真实的后验概率 real posterior, $q_{\\phi}(z|x)$ 表示变分后验概率 variational posterior. 我们并不知道真实的后验概率，但是我们呢可以额跳过这一项，因为KL散度一定大于等于0. 去掉最后一项，我们得到ELBO，同时我们知道 ELBO和真实的对数似然之间的间隔是 $KL \\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) |p(\\mathbf{z}|\\mathbf{x}) \\right] $.$$\\begin{align}\\ln p(\\mathbf{x}) &amp;= E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\right] - KL\\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}) \\right] + KL \\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) |p(\\mathbf{z}|\\mathbf{x}) \\right] \\\\&amp;\\ge E_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\left[ \\ln p(\\mathbf{x}|\\mathbf{z}) \\right] - KL\\left[ q_{\\phi}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}) \\right]\\end{align}$$Beautiful! 但同样我们能看出一些问题，如果 $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ 和 $p(z|x)$ 距离很大，那么ELBO优化的再好，ELBO和真实的对数似然之间的差距依然很大。也就是说，如果我们采用很简单的后验概率，我们会得到一个不太好的VAE模型。 Figure 2. ELBO是对数似然的下界。ELBO最大时对应的 $\\hat {\\theta}$ 并不一定也能让对数似然最大。ELBO 越losser，这对模型参数的最大似然估计的偏差越大。 Components of VAEs 我们使用amortized variational posteriors ${q_{\\phi}(z|x)}_{\\phi}$ 来近似真实的后验分布 $p(z|x)$. 这个概率分布可以看作是 encoder. conditional likelihood p(x|z) 对应的概率分布可以看作是 decoder $p(z)$ 是对于潜在变量的边缘分布，也可以看作是 prior 模型 至此，还有两个问题需要解决： 如何参数化这些distributions? 如何计算这些期望呢？也就是积分。 Parameterization of distributions显然，我们用Neural Networks来表达上述两个distributions: encoder 和 decoder。在VAE框架下，大部分情况下，我们可以使用任何distributions. 但是，我们也必须满足对应的任务。 对于decoder分布 $p_{\\theta}(x|z)$ 显然不可能是正态分布，因为image的pixel values是离散的。一个可能的distribution可以是 categorical distribution: ​ $$p_{\\theta}(x|𝐳)=\\text{Categorical}(𝐱|\\theta(𝐳))$$ ​ 其中 $\\theta(z) = \\text{softmax}(NN(z))$. 这里把重构当作分类任务来做，当然也可以是回归任务。 对于潜在变量的分布，为了方便，通常 $z$ 可视为连续随机变量的向量，$z\\in \\mathbb{R}^M$. 因此，我们可以使用 Gaussians 来表示 variational posterior 和 prior.$$\\begin{align}q_{\\phi}(\\mathbf{z}|\\mathbf{x}) &amp;= \\mathcal{N}\\left(\\mathbf{z} | \\mu_{\\phi}(\\mathbf{x}), \\mathrm{diag}\\left[ \\sigma_{\\phi}^2(\\mathbf{x}) \\right] \\right) \\\\p(\\mathbf{z}) &amp;= \\mathcal{N}\\left(\\mathbf{z} | 0, \\mathbf{I} \\right)\\end{align}$$其中 $\\mu_{phi}(x), \\sigma_{\\phi}(x)$ 是神经网络的输出。在实际使用中，我们使用NN得到 $2M$ 的 values $\\in R^{1\\times 2M}$，其中 $R^{1\\times M}$ 表示 means，$R^{1\\times M}$ 表示 variances. Reparameterization trick到目前为止，我们学习了log-likelihood和ELBO。但是仍然有一个问题，我们用encoder $q_{\\phi}(z|x)$ 得到关于潜在变量的分布，我们该如何计算$E_{z\\sim q_{\\phi}(z|x)}(x|z)$这个积分呢？显然，$z\\sim q_{\\phi}(z|x)$这个采样过程是不可导的，我们可以采用MC-approximation,但是这样仍然有个问题，从这个变分后验sample得到的z，在ELBO的训练过程中，在计算关于$\\phi$的梯度时，梯度的方差特别大。 因此，另一个可能的方法是reparameterizing这个分布(Devroye, 1996). 具体地，我们可以将随机变量 $z$ 表示为具有简单分布的独立随机变量的原始变换（例如算术运算、对数等）的组合。换句话说，我们使用重参数技巧表达为确定性的变量:$$z = \\mu + \\sigma \\cdot \\epsilon$$其中 $\\epsilon \\sim \\mathcal{N}(\\epsilon|0,1)$ . 使用Reparameterization方法能够显著减小梯度的方法。Why？因为随机性来自独立的分布$p(\\epsilon)$，我们计算梯度是关于确定性函数（即神经网络），而不是随机的对象$z\\sim q_{\\phi}(z|x)$。 更棒的是，由于我们使用随机梯度下降来学习 VAE，因此在训练期间仅采样一次 $z$ 就足够了！ 综上，VAE框架主要包括： variational posterior $q_{\\phi}(z|x)$ using encoder sample z from $q_{\\phi}(z|x)$ and feed it to decoder, using reparameterization trick conditional likelihood $p_{\\theta}(x|z)$ using decoder reconstruction loss kl loss between variational posterior and prior $p_{\\theta}$. 其中 $q_{\\phi}(z|x)\\sim \\mathcal{N}(z|\\mu, \\sigma^2I)$ 多维高斯分布，$p_{\\theta}\\sim \\mathcal{N}(0,1)$ 是正态分布。 KL 散度的推导如下： 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class VariationalEncoder(nn.Module): def __init__(self, latent_dims): super(VariationalEncoder, self).__init__() self.linear1 = nn.Linear(784, 512) self.linear2 = nn.Linear(512, latent_dims) self.linear3 = nn.Linear(512, latent_dims) self.kl = 0 def forward(self, x): x = torch.flatten(x, start_dim=1) x = F.relu(self.linear1(x)) mu = self.linear2(x) sigma = torch.exp(self.linear3(x)) # reparameterization trick z = mu + sigma*torch.randn_like(sigma) self.kl = 0.5*(sigma**2 + mu**2 - torch.log(sigma) - 1).sum() return zclass Decoder(nn.Module): def __init__(self, latent_dims): super(Decoder, self).__init__() self.linear1 = nn.Linear(latent_dims, 512) self.linear2 = nn.Linear(512, 784) def forward(self, z): x_hat = F.relu(self.linear1(z)) x_hat = torch.sigmoid(self.linear2(x_hat)) return x_hat.reshape((-1, 1, 28, 28)) class VariationalAutoencoder(nn.Module): def __init__(self, latent_dims): super(VariationalAutoencoder, self).__init__() self.encoder = VariationalEncoder(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z = self.encoder(x) return self.decoder(z) def train(model, data, epochs=20): opt = torch.optim.Adam(model.parameters()) for epoch in range(epochs): print(f&quot;epoch: {epoch+1}/{epochs}&quot;) for x, y in data: x = x.to(device) # GPU opt.zero_grad() x_hat = model(x) loss = ((x - x_hat)**2).sum() + model.encoder.kl loss.backward() opt.step() return model latent_dims = 2vae = VariationalAutoencoder(latent_dims).to(device) # GPUvae = train(vae, data) More about VAEs! Estimation of the log-likelihood using importance weighting 我们前面提到过 ELBO 只是对数似然的下界，它不应该被用作对数似然的良好估计。(Burda et al., 2015; Rezende et al., 2014) 采用了一种 importance weighting procedure 方法。 Enhancing VAEs: Better encoders 意味着更好的后验概率, 一个很重要的方向是 conditional flow-based models (van den Berg et al., 2018; Hoogeboom et al., 2020; Kingma et al., 2016; Rezende &amp; Mohamed, 2015; Tomczak &amp; Welling, 2016; Tomczak &amp; Welling, 2017). Enhancing VAEs: Better decoders 可以使用不同的模型或者loss function来拟合原始的数据，比如pixel-CNN, transformer等。 Enhancing VAEs: Better priors 设定一个好的先验也是很重要的，能够减小与变分后验的gap。很多研究尝试解决这个问题，比如：using a multimodal prior mimicking the aggregated posterior (known as the VampPrior) (Tomczak &amp; Welling, 2018), or a flow-based prior (e.g., (Gatopoulos &amp; Tomczak, 2020)), an ARM-based prior (Chen et al., 2016) or using an idea of resampling (Bauer &amp; Mnih, 2019). VAEs for non-image data 不仅仅是图像数据，文本、序列数据等也可以。 Extending VAEs Here, we present the unsupervised version of VAEs. However, there is no restriction to that and we can introduce labels or other variables. In (Kingma et al., 2014) a semi-supervised VAE was proposed. This idea was further extended to the concept of fair representations (Louizos et al., 2015). In (Ilse et al., 2020), the authors proposed a specific latent representation that allows domain generalization in VAEs. In (Blundell et al., 2015) variational inference and the reparameterization trick were used for Bayesian Neural Nets. This paper is not necessarily introducing a VAE, but a VAE-like way of dealing with Bayesian neural nets. **Different latent spaces： ** in (Davidson et al., 2018; Davidson et al., 2019) a hyperspherical latent-space was used, and in (Mathieu et al., 2019) the hyperbolic latent space was utilized. **The posterior collapse： ** There were many ideas proposed to deal with the posterior collapse. For instance, (He et al., 2019) propose to update variational posteriors more often than the decoder. In (Dieng et al., 2019) a new architecture of the decoder is proposed by introducing skip connection to avoiding the posterior collapse. Various perspectives on the objective The core of the VAE is the ELBO. However, we can consider different objectives. For instance, (Dieng et al., 2017) propose an upper-bound to the log-likelihood that is based on the chi-square divergence (CUBO). In (Alemi et al., 2018) an information-theoretic perspective on the ELBO is presented. (Higgins et al., 2016) introduced the 𝛽β-VAE where the regularization term is weighted by a fudge factor 𝛽β. The objective does not correspond to the lowe-bound of the log-likelihood though. Deterministic Regularized Auto-Encoders: We can take look at the VAE and the objective, as mentioned before, and think of it as a regularized version of an auto-encoder with a stochastic encoder and a stochastic decoder. (Ghosh et al., 2020) “peeled off” VAEs from all stochasticity and indicated similarities between deterministic regularized auto-encoders and VAEs, and highlited potential issues with VAEs. Moreover, they brilliantly pointed out that even with a deterministic encoders, due to stochasticity of the empirical distribution, we can fit a model to the aggregated posterior. As a result, the deterministic (regularized) auto-encoder could be turned into a generative model by sampling from our model, 𝑝𝜆(𝐳)pλ(z), and then, deterministically, mapping 𝐳z to the space of observable 𝐱x. In my opinion, this direction should be further explored and an important question is whether we indeed need any stochasticity at all. Hierarchical VAEs Very recently, there are many VAEs with a deep, hierarchical structure of latent variables that achieved remarkable results! The most important ones are definitely BIVA (Maaløe et al., 2019), NVA (Vahdat &amp; Kautz, 2020), and very deep VAEs (Child, 2020). Another interesting perspective on a deep, hierarchical VAE was presented in (Gatopoulos &amp; Tomczak, 2020) where, additionally, a series of deterministic functions was used. Adversarial Auto-Encoders Another interesting perspective on VAEs is presented in (Makhzani et al., 2015). Since learning the aggregated posterior as the prior is an important component mentioned in some papers (e.g., (Tomczak &amp; Welling, 2018)), a different approach would be to train the prior with an adversarial loss. Further, (Makhzani et al., 2015) present various ideas how auto-encoders could benefit from adverarial learning. Prior in VAEs","link":"/2021/12/07/Deep-Generative-Models/"},{"title":"CSAPP-01.A tour of computer system","text":"CSAPP 第一章 1.1 信息就是位＋上下文 位（bit）由值０和１组成，一个字节有８位。 一个字长，对32位机器是4个字节，对64位机器是8个字节。 1.2 程序被翻译成其他不同的格式从源程序到可执行目标文件的过程分为4个阶段： 执行这4个阶段的程序（预处理器、编译器、汇编器和链接器）一起构成了编译系统。 hello.c（源程序，文本文件） –&gt; hello.i（添加了头文件的源程序，文本） –&gt; hello.s（汇编程序，文本） –&gt; hello.0（目标程序，二进制）–&gt; hello（可执行的目标程序，二进制） 1.3 为什么要了解编译系统是如何工作的 优化程序性能 为了在C程序中做出好的编码选择，需要去了解一起机器代码以及编译器将不同的C语句转化为机器代码的方式。 例如： 一个函数调用的开销有多大？ while循环比for循环更有效吗？ 指针引用比数组引用更有效吗？ 第3章会介绍编译器如何把不同的C语言结构转换成他们的机器语言的。第5章会学习简单的转换C语言代码。 理解链接时出现的错误 链接器报告无法解析一个引用，这是什么意思？ 静态变量和全局变量的区别是什么？ 不同的C文件中定义了两个相同的全局变量会发生什么？ 为什么有些链接错误直到运行时才会出现？ 第7章会解释这些。 避免安全漏洞 缓冲区溢出错误。第3章会描述堆栈原理和缓冲区溢出错误。 1.4 处理器读并解释存储在存储器中的指令1.4.1 系统硬件的组成 总线： 每次传送1个字。 I/O 设备： I/O总线和I/O设备之间传递信息。 主存： 一组动态随机存取存储器(DRAM)芯片组成。存储器是一个线性字节数组，每个字节都有其唯一的地址（即数组索引）。 处理器： 中央处理单元（CPU），解释（或执行）在主存中指令的引擎。处理器核心是一个字长的存储设备（或寄存器），称为程序计数器（PC）。在任何时刻，PC都指向主存中某条机器语言指令（即含有该条指令的地址）。 处理器是按照一个简单的指令执行模型来操作的，这个模型是由指令集结构决定的。PC读取指令，解释指令中的位，执行该指令的简单操作，然后更新PC，使其指向下一个指令，而这条指令并不一定与存储器中刚刚执行的指令相邻。 这样的操作是围绕主存、寄存器文件(register file)、算术逻辑单元（ALU）进行的。 CPU指令可能会要求如下操作： 加载：一个字节或一个字，从主存到寄存器 存储：一个字节或一个字，从寄存器到主存 操作：两个寄存器的内容复制到ALU，进行算术操作 跳转：从指令本身中抽取一个字，复制到PC中。 1.4.2 运行hello程序 图中黑色的线就是整个流程。shell外壳程序将字符读入寄存器，再存放到存储器中。一旦代码和数据加载到主存中，处理器就开始执行hello程序中的main程序中的机器语言指令。 利用存储器存取的技术，数据可以不通过处理器直接从磁盘到达主存。如下图： 然后从主存复制到寄存器文件中，再从寄存器复制到显示设备上。过程如下图： 机器指令最初是放在磁盘上的， 然后加载程序，复制到主存 处理器运行程序时，指令从主存到处理器 同样的，“hello word” 初始是在磁盘上的，然后复制到主存，最后从主存复制到显示器。 1.5 高速缓存至关重要可以发现这些复制减缓了程序的速度。 而读取速度： 存储器 &gt;&gt; 寄存器（存放几百字节） &gt;&gt; 主存（存放几十亿字节） &gt;&gt; 磁盘（可能比主存大1000倍，比如2T的硬盘，2G的内存.） 因此有了高速缓存存储器。 L1、L2高速缓存用一种静态随机访问存储器（SRAM）的硬件技术实现的。让高速缓存存放可能经常访问的数据时，可以大大提高程序的性能。 1.6 存储设备形成层次结构 存储器层次存储的主要思想是，速度更快的一层作为下一层的缓存区。 1.7 操作系统管理硬件 操作系统可以看做应用程序和硬件之间的一层软件。 1.7.1 进程一个CPU看上去像是在并发的执行多个进程，这是通过处理器在进程间切换实现的。操作系统实现这种交错执行的机制称为上下文切换。第8章会详细介绍。 1.7.2 线程一个进程实际上可以由多个称为线程的执行单元组成，每个线程都运行在进程的上下文中，并共享同样的代码和全局数据。多线程比多进程更容易共享数据，线程一般来说比进程更高效。 1.7.3 虚拟内存 程序代码和数据 堆：在运行时动态扩展和收缩。 共享库 栈：用户栈，编译器用它来实现函数调用。 内核虚拟内存：为内核保留的。 1.7.4 文件文件就是字节序列。每个I/O设备都可以看成是文件。系统的所有输入输出都是通过使用一小组称为Unix I/O的系统函数调用读写文件实现的。第10章会学习Unix I/O 1.8 系统之间利用网络通信 网络也是一种I/O设备。 这种客户端与服务器之间交互的类型在所有网络应用中是非常典型的。在第11章中，你将会学会如何构造网络应用程序，并创建一个简单的Web服务器。Exciting! 1.9 重要主题1.9.1 Amdahl定律系统运行时间 $T_{old}$, 系统某部分执行时间占比 $\\alpha(0&lt;\\alpha &lt;1)$, 该部分提升性能比例 $k(k&gt;1)$,则总的执行时间： $$T_{new}=(1-\\alpha)T_{old}+(\\alpha T_{old})/k = T_{old}[(1-\\alpha)+\\alpha/k]$$ $$S=T_{old}/T_{new} = \\dfrac{1}{(1-\\alpha)+\\alpha/k}&lt;k$$ 简单的数学运算，证明只提升系统某一部分的性能，对整体的加速明显小于这一部分提升的比例。也就是说，要加速整个系统，必须加速这个系统中的大部分的速度。 练习题 1.1 T_{old}=25 T_{new}=1500/150+(2500-1500)/100=20 加速比： T_{old}/T_{new}=1.25 1.9.2 并发（concurrency）和并行（parallelism） 线程级并发 多核处理器： 超线程，有时又称为同时多线程（simultaneous multi-threading），是一项允许一个CPU执行多个控制流的技术。第12章会更深入的讨论并发。 指令集并行 最近的处理器可以保持每个始终2~4条指令的执行效率。其实每条指令从开始到结束需要长得多的时间，大约20个或更多个周期。在第4章，会研究流水线(pipelining)的使用。在流水线中，将执行一条指令所需要的活动划分成不同的步骤，将处理器的硬件组织成一些列的阶段，每个阶段执行一个步骤。 处理器达到比一个周期一条指令更快的执行速率，就称之为超标量（super-scalar）处理器。 单指令、多数据并行 一条指令产生多个可以并行执行的操作，这种方式称为单指令、多数据，即 SIMD 并行。 1.9.3 计算机系统中抽象的重要性 文件是对 I/O 设备的抽象，虚拟内存是对程序存储器的抽象，而进程是对一个正在运行的程序的抽象。 虚拟机是对整个计算机的抽象。 1.10 小结","link":"/2018/06/08/CSAPP-01-A-tour-of-computer-system/"},{"title":"NLP算法-实习面试经验","text":"思必驰一面的大哥应该主要是做图像，并且偏工程方面的。基本上没问 NLP 相关的问题，主要问了些工程方面的问题和 CNN 相关的。 二面的老哥则主要问了简历上相关的 NLP 经验。 数据结构与算法 快速排序 单向有环链表怎么判断是否有环 深度学习 卷积核为 1 的 CNN 的主要作用是什么？ 权重初始化的方式有哪些？ Xavier 的推导。 手推 BP 神经网络。 NLP attention 熟悉吗？具体含义是什么？ BERT 模型有多少层？在机器阅读理解的任务上是怎么进行 fine-tune 的？ Transformer 中 multi-head 的意义是什么？ other SQL 熟悉吗？ C++ 中 新浪微博面试的大哥超级有亲和力，感觉人非常好。面对我渣如粪土的 coding 能力，依然表示理解。并且能跟你一起探讨算法的思路， 大佬先进行了自我介绍，表示他是在美国读的 phd，在微软工作了 10 年然后被新浪挖过来，部门主要任务是通过 NLP 算法的分析，提高广告的精准投放力度的。 聊天 你在简历上相关项目中，最 challenge 的一段经历是什么？ 都说深度学习是玄学调参，你觉得一个从业五年和一个从业一年的深度学习工程师有何区别？ 作为转行生，你觉得你以前的经验对现在有什么作用？ coding 球队抽签 有 N 只足球队，球队的强弱分别是 1&lt;2&lt;3&lt;….&lt;N，每次抽取两只球队。有如下两个条件： random 抽签，经过足够多的次数，每只球队都会被抽到 抽到强队的概率更大 找明星 在一个学校有 N 个人，其中可能有一位明星。找出这个明星，其中明星满足如下两个条件： everybody know him he donn’t know any one else. 网易有道聊天 为什么从机械转行到自然语言处理？ 简单介绍下在三星研究院的工作。详细说一下机器阅读理解的流程。 BERT 模型为什么好？从更高层面谈论下 BERT 模型提出的意义。 multi-head 的具体实现和作用。 coding单项链表是否是回文。","link":"/2019/01/23/NLP%E7%AE%97%E6%B3%95-%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C/"},{"title":"FAIR-无监督机器翻译","text":"无监督机器翻译的几篇paper: Word Translation without Parallel Data - ICLR’18 Unsupervised machine translation using monolingual corpora only, Lample, et al. ICLR 2018a Unsupervised neural machine translation, ICLR 2018 Phrase-based &amp; neural unsupervised machine translation. emnlp 2018b Cross-lingual Language Model Pretraining Neural word embedding as implicit matrix factorization Phrase-Based &amp; Neural Unsupervised Machine Translation对前两篇无监督机器翻译进行了一个总结： carefully initialize the MT system with an inferred bilingual dictionary. 通过双语字典对MT模型进行初始化。 leverage strong language models, via training the sequence-to-sequence system as a denoising autoencoder. 通过训练seq2eq模型来利用强大的语言模型作为降噪自编码。 turn the unsupervised problem into a supervised one by automatic generation of sentence pairs via back-translation.把无监督问题转换为有监督的问题，也就是通过back-translation自动生成语言对。 这篇论文的作者将上述方法做了个整合，得到的NMT 系统在无监督翻译上能达到 +10 BLEU, 并且应用到phrase-based MT上，达到了 +12 BLEU. 作者将无监督机器翻译抽象成上述过程。 B. 初始化 C. 语言模型 D. 迭代反向翻译。 Initialization前人的初始化方法： 利用意思相近的词、短语或是 subword bilingual dictionary dictionaries inferred in an unsupervised way. Lample et al. (2018) and Artetxe et al. (2018) 这种初始化的方式对于距离相距较远的语言可能效果不太好。比如中英？ 作者的初始化方法： 先对source和target language进行bpe处理(bpe的优势：减小词表大小，消除unknow word)，然后联合起来（而不是分开）训练word embedding. join the monolingual corpora apply BPE tokenization on the resulting corpus learn token embeddings (Mikolov et al., 2013) on the same corpus Language Modeling基于单语训练得到的语言模型，主要是通过 local substitutions and word reorderings 来提升翻译的质量（也就是 contextual information）。 作者的 language model training 基于 denosing autoencoder. C is a noise model with some words dropped and swapped. $P_{s→s}$ and $P_{t→t}$ are the composition of encoder and decoder both operating on the source and target sides, respectively. Back-translation: Iterative Back-translationDual Learning for Machine Translation $fr \\rightarrow \\hat{en} \\rightarrow fr$ fr 是 target language. en 是 source language. 先利用反向模型翻译得到 pesudo en sentence $\\hat{en}$. 然后将 $(\\hat{en}, fr)$ 作为翻译对进行有监督的学习。尽管 $\\hat{en}$ 会非常 noisy，但保证了target端是pure sentence，效果确实不错吧。 作者的 iteration BT 与上述方法一致： $u^{* }(y)=argmaxP_{t→s}(u|y)$, $u^{* }(y)$ 是 pesudo source sentence. $v^{* }(x)=argmaxP_{s→t}(v|x)$, $v^{* }(x)$ 是 pesudo target sentence. 作者在实验时，并没有对 $u\\rightarrow u^{* }, v\\rightarrow v^{* }$ 这个过程进行优化，因为这在实验中并没有提升。同时在训练时，作者是简单的将 $L^{back}$ 和 $L^{lm}$ 加起来进行优化。 为了避免模型混淆两个语言的向量空间，作者提供了一个解决方法，Sharing Latent Representations. 也就是 denosing aotoencoder 和 back-translation 可以用同一个 encoder-decoder 模型，不同语言之间共享 encoder 和 decoder. While sharing the encoder is critical to get the model to work, shar- ing the decoder simply induces useful regularization. 共享 encoder 对于无监督模型非常关键，而 decoder 的共享则主要是提供有效的正则化。这与 GNMT 模型不一样，不需要加上 tag 来指定翻译方向。 Result","link":"/2019/11/21/FAIR-%E6%97%A0%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"},{"title":"AI challenger 参会记录","text":"答辩听了观点型阅读理解和细粒度情感分类两个，相对来说后者更加干货满满，大佬云集的，基本上代表了国内 NLP 的四座大山，清/北/中科院/哈工大。造成前者干货较少的主要原因作为主持人的搜狗大佬也说了， BERT 的提出在阅读理解这样更加需要上下文理解任务的提升实在太多，使得选手的其它工作都黯然失色，导致大家的模型都趋向同一化。而 BERT 对于分类任务的提升就相对较少了，所以下午的答辩显得更加丰富，各种操作和 trick. 但也有选手说 BERT 作为单模型对这个分类任务依然能取得很不错的效果，所以 BERT 是真强啊 因为答辩的屏幕是真小，根本看不清楚。。所以记录会很零散，也许只是些关键词，后续还需要自行 google. 观点型阅读理解 取得好成绩的主要操作： 通过简单的正则匹配将三个观点转化为作为 “正/负/无法确定” 的三分类问题。训练集中 95% 的数据可以很准确的转化为这种形式，还有 5% 的是实体类问题，比如 “韩国/美国/无法确定”，有选手的做法是将 query 中对两个实体进行排序，比如韩国在前，美国在后。同样对应的 answer 就是 “韩国/美国/无法确定”. 将文本理解的问题，转换为分类问题之后，对整个模型的复杂度需求就降低太多了。但事实上，这是数据 bug … 模型关键词： BERT multiway attention + R-Net RCZoo 浙大大佬的：多层 LSTM 模型，浅层+主要+深层 三个 loss 优化。具体忘了拍照，以及真的看不清楚。。 基于 query 的 attention 还是基于 passage 的 attention 作为最终的 answer selection/matching. 说句不马后炮的话，这里面大部分我也都想到了啊，只是做与没做，以及用与没用 BERT 。。。 细粒度用户评论情感分析 seq2seq 选手这么做的原因是 他觉得各个 粒度 之间存在一定的关联，所以采用 decoder 的形式能有效的利用这些信息。很神奇的操作，是否真的有效朱小燕老师有问到，好像作者并没有做对照实验。 ELMo 提升最多 改进的注意力机制，其实就是 multi-head attention PRAUC 损失函数， 这个我好像在哪儿见过，我不记得了 大佬感觉可以发 paper 了。。 others其他的也很强，但没有 seq2seq 这么具有特殊性，所以可以一起说。 词嵌入部分微调，没太懂？ 哪一部分微调，以及非监督的情况下，如何保证微调的程度 F1 指标的优化，这个对于 unbalanced 数据看起来比 过/欠 采样有效。以及刘洋老师提到的可以基于 rainforce 对 F1 进行优化 附上刘洋老师照片一张，侧脸看起来真像李健啊，都是清华男神吧～ 伪朴素贝叶斯特征，PPT 里面说的很清楚～每次输入几个样本其提取的是局部特征，而伪朴素贝叶斯特征能体现一个词的全局特征。感觉很棒啊 数据增强方式： drop words 随机 mask shuffle words 打乱词序 组合增强策略 对抗训练 模型集成： 贪婪式模型选择 简单概率平均，最后采取了这种。。。anyway 根据验证集调整分类阈值，对当前的验证集当然会有较大提升。但是对于 测试集 可能出现过拟合，引入正则化和 Ensamble 策略。 $$b_i^j=\\text{argmax}_b[\\text{marco-}F_1(S^j[:,i]+b)-C|b]$$ 第 j 个情感要素第 i 类别上的偏置， C&gt;0 为正则系数。 还有些关键词，有些来不及拍照。。 BiSRU 未完待续。。","link":"/2018/12/19/AI-challenger-%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/"},{"title":"SCAPP-02-信息的表示和处理","text":"CSAPP 第二章 信息的表示和处理三种数字表示： 无符号(unsigned)编码：大于或等于零的数字 补码（two’s-complement）编码：表示有符号整数最常见的方式 浮点数(floating-point)编码： 溢出（overflow）:计算机的表示法是用有限数量的位来对一个数字编码。因此结果太大至无法表示时，某些运算就会溢出。 2.1 信息存储计算机使用8位的块，或者字节（byte）。作为最小的可寻址的内存单元，而不是单独的位。 机器级程序将内存视为一个非常大的数字来标识，称为它的内存。内存的每一个字节都由一个唯一的数字来标识，称为它的地址，所有可能地址的集合称为虚拟地址空间(virtual address space)。 接下来几章将讲述编译器和运行时系统是如何将存储器空间划分为更可管理的单元，来存放不同的程序对象，即程序数据、指令和控制信息。 2.1.1 十六进制表示法一个字节是8位： 对于二进制表示法，值域是 $00000000_2-11111111_2$ 对于十进制表示法，值域是 $0_{10}-255_{10}$ 对十六进制表示法，值域是 $00_{16}-FF_{16}$ 练习题 2.1 A. 将ox39A7F8转换为二进制. 0011 1001 1010 0111 1111 1000 B. 将二进制 1100 1001 0111 1011 转换为十六进制。 oxE97D 练习题 2.2 $2^n=2^{i+4j}$ 可以很容易写成十六进制就是 2^i后面跟着j个0 比如： $2^9$ -&gt; $2^{1+2x4}$ -&gt; ox200 $2^19$ -&gt; $2^{3+4x4}$ -&gt; ox80000 进制的转换 将十进制转换为16进制，其实可以理解为将十进制转换为十进制，就是除以10，余数分别是个位、百位…. 同样的道理，转换为16进制就是除以16 十六进制转换为10进制就更简单了，16的幂乘以每个十六进制数。 2.1.2 字数据大小对于一个字长为w位的机器，虚拟地址的范围为0~$2^w-1$，程序最多访问$2^w$个字节。 2.1.3 寻址和字节顺序在几乎所有的机器上，多字节对象都被存储为连续的字节序列，对象的地址为所使用字节中最小的地址。 假设变量x的类型为int， 位于地址0x100处，它的十六进制值为0x01234567. 十六进制的两位占一个字节，因此其地址范围 0x100~0x103. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include &lt;stdio.h&gt;typedef unsigned char *byte_pointer; // 字符指针// 使用 typedef 来命名数据类型void show_bytes(byte_pointer start, size_t len){ size_t i; for (i=0; i&lt;len; i++) printf(&quot; %.2x&quot;, start[i]); printf(&quot;\\n&quot;);}// size_t 表示size type，大小的类型，意味着它是sizeof运算符结果的类型// 在C语言中，可以用指针表示法来引用数组元素。引用start[i]表示我们想要读取以start指向的位置为起始的第i个位置处的字节。void show_int(int x){ show_bytes((byte_pointer)&amp;x, sizeof(int));}void show_float(float x){ show_bytes((byte_pointer)&amp;x, sizeof(float));}void show_pointer(void *x){ show_bytes((byte_pointer)&amp;x, sizeof(void *));}// C 和 C++ 中独有的操作， C的“取地址”运算符 &amp; 创建一个指针 &amp;x, 这个指针的类型取决于 x 的类型，因此这三个指针类型分别为 int*, float*, void**, 数据类型 void * 是一种特殊类型的指针，没有相关联的类型信息。// (byte_pointer)&amp;x 表示强制转换，无论 &amp;x 之前是什么类型，现在就是一个指向 unsigned char的指针。但这并不会改变真实的指针，只是编译器以新的数据类型来看待被指向的数据。void test_show_bytes(int val){ int ival = val; float fval = (float) ival; int * pval = &amp;ival; show_int(ival); show_float(fval); show_pointer(pval);}int main(){ test_show_bytes(12345); return 0;} 我的机器是Linux64，运行结果是这样的： 分析下为什么是这样: 12345，十六进制为 ox3039, 对于int数据采用的是4字节： 二进制数表示是: 0000 0000 0000 0000 0011 0000 0011 1001 在小端法机器的十六进制表示为: 39 30 00 00 在大端法机器的十六进制表示为: 00 00 30 39 以一个字节为单位，十六进制一个字节占两位，所以每两个数一个字节。二进制是每8个数一个字节。 为什么浮点数 12345.0 的十六进制是这样的呢？之后会讲到～ 练习题 2.5 十六进制数 ox87654321 小端法： 21 43 65 87 A. 小端法： 21 大端法：87 B. 小:21 43 大： 87 65 C. 小:21 43 65 大: 87 65 43 练习题 2.6 3510593 -&gt; ox00359141 -&gt; 0000 0000 0011 0101 1001 0001 0100 0001 3510593.0 -&gt; ox4A564504 -&gt; 0100 1010 0101 0110 0100 0101 0000 0100 最大匹配位数： 2.1.4 表示字符串每个字符对应一个ASCII码。总共有127个ascii码。 十进制数x的ASCII码正好是 0x3x, 比如要显示字符 0，其ASCII码是48，用十六进制就是0x30. ‘A’‘Z’的ASCII码十进制表示是 6590，‘a’‘z’十进制是 97122，十六进制是ox61~0x7A. null的ASCII码是 0，也就是0x00. 1234567891011121314151617181920212223void show_bytes(byte_pointer start, size_t len){ size_t i; for (i=0; i&lt;len; i++) printf(&quot; %.2x&quot;, start[i]); printf(&quot;\\n&quot;);}const char *s = &quot;abcdef&quot;;show_bytes((byte_pointer) s, strlen(s)); 运行结果： 61 62 63 64 65 66 2.1.5 表示代码 不同机器类型使用不同的且不兼容的指令和编码方式。 2.1.6 布尔代数简介0和1的研究～ 非、与、或、异或(表示P或者Q为真，但不同时为真时，P^Q为真) 布尔运算可拓展到位向量的运算： a=[0110], b=[1100], 那么 a&amp;b, a|b, a^b, ~b 分别为： 位向量可以用来表示有限集合。$[a_{w-1},…,a_1,a_0]$（注意是 $a_{w-1}$ 在左边，$a_0$ 在右边） 编码任何子集 $A\\in {0,1,2,…,w-1}$. 比如 a=[00101]就表示 A={0,2}. 其实可以把 a 看做多标签的 one-hot向量。。。 练习题 2.9 蓝色|绿色 = [001]|[010] = [011] = 蓝绿色 红色 ^ 红紫色 = [100]^[101] = [001] = 蓝色 2.1.7 C语言中的位级运算布尔位运算。 从十六进制转换为二进制，进行运算，在转换回十六进制。 练习题 2.10 任意一位向量 a，有 a ^ a = 0. 那么 a^a^b=b. 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;stdio.h&gt;void inplace_swap(int *x, int *y){ * y = * x ^ * y; * x = * x ^ * y; * y = * x ^ * y;}int main(){ int a=5, b=4; printf(&quot;%p, %p\\n&quot;,&amp;a, &amp;b); inplace_swap(&amp;a, &amp;b); printf(&quot;%p, %p\\n&quot;,&amp;a, &amp;b); printf(&quot;%i, %i\\n&quot;, a, b); return 0;} 运行结果： 12345670x7ffe90686210, 0x7ffe906862140x7ffe90686210, 0x7ffe906862144, 5 我们发现这里其实a和b的地址没有改变，是地址对应的值在进行异或运算。 5^4=1, 5^1=4, 4^1=5. 这就是二进制运算，也就是位运算。 6^5=3, 6^3=5, 5^3=6. 不过不要误会，跟加减法没关系，原理还是二进制 a^a=0. 所以能进行交换，就是因为 a^b^a=b. 结果是： step1 a a^b step2 a^(a^b)=b a^b step3 b b^(a^b)=a 练习题 2.11 将 &lt;= 改为 &lt; 即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;stdio.h&gt;void inplace_swap(int *x, int *y){ * y = * x ^ * y; * x = * x ^ * y; * y = * x ^ * y;}void reverse_array(int a[], int cnt){ int first, last; for (first = 0, last = cnt -1; first &lt; last; first++, last--) inplace_swap(&amp;a[first], &amp;a[last]);}int main(){ int a[] = {1,2,3,4,5}; reverse_array(a, 5); for (int i=0; i&lt;5; i++) printf(&quot;%i&quot;, a[i]); printf(&quot;\\n&quot;); return 0;} 2.1.8 C语言中的逻辑运算包括 ||, &amp;&amp; 和 !, 逻辑运算认为所有非零参数为TRUE，参数0 表示 FALSE. 要注意与位运算区分开来。 习题 2.15 if ((x^y)|(x^y)) return 1 else return 0 2.1.9 C语言中的移位运算x &lt;&lt; k: 表示向左移动k位，右边补0 x &gt;&gt; k: 有两种情况： 逻辑右移：向右移动k位，左边补0 算术右移：向右移动k位，左边补最高位的值。 可以看到一定是在一个字节内的，也就是以一个字节为整体。 对于有符号数： 在java中 x&gt;&gt;k 是算术右移， x&gt;&gt;&gt;&gt;k 是逻辑右移。 C语言中没有明确规定，但几乎所有的编译器采用算术右移。也就是补最高位的值。 对于无符号数，右移必须是逻辑右移。 移位运算符的优先级低于加法和减法。 2.2 整数表示整数操作术语： 2.2.1 整型数据类型32位机器： 64位机器： 跟机器相关的整型数据类型就只有 long，在64位中占8个字节。在32位中占4个字节。 可以稍微记一下这些数： 有符号字符， 1个字节。 范围 $-2^7=128 \\sim 2^7-1=127$ 无符号字符， 1个字节。 范围 $0 \\sim 2^8-1=255$ 有符号短整型，2个字节。范围 $-2^15=-32768 \\sim 2^15-1=32767$ 无符号短整型，2个字节。范围 $0 \\sim 2^16-1=65535$ 有符号整型， 4个字节。范围 $-2^31=-2 147 483 648 \\sim 2^31-1=2 147 483 647$ 无符号整型， 4个字节。范围 $0 \\sim 2^32-1=4 294 967 295$ 无符号的数值范围是有符号的两倍，是因为有符号数需要拿出一位来表示符号，所以就小了一倍了。比如：一个字符是一个字节，8位，无符号范围就是 $2^8-1=255$, 有符号就是 $2^7-1=127$. 然而后面介绍了补码，少一位编码是对的，但是负数的计算并不是这样哈哈哈，打脸了，并没有这么简单啦～～～ 这也解释了为啥负数是 -128， 正数是 127. C语言标准定义的每种数据类型必须表示的最小的取值范围。 2.2.2 无符号数的编码将二进制写成向量。 2.2.3 补码编码为了标书负数值，最常见的就是补码（two’s-complement）形式， 将字的最高有效位解释为负权（negative weight）. 最高位的权重是 $-2^{w-1}$,对该数是正是负取决定性作用。其为1时，为负；其为0时，为正。 所以w位补码能表示的范围： $$-2^{w-1} \\sim \\sum_{i=1}^{w-1}2^i=2^{w-1}-1$$ 重要的数字: UMax 表示最大无符号数，UMin没有写，就是0啦 TMax 表示最大有符号数，TMin 表示最小有符号数。 可以发现： 补码不对称，$|TMin| = |TMax| + 1$ 无符号最大值是有符号最大值的2倍大一点。比如一个字节8位为例，(2^8-1) = 2(2^7-1)+1. 练习题2.18 看不懂 2.2.4 有符号数和无符号数之间的转换强制转换保持位值，只是转换了其解释方式。 比如32为的无符号最大值 4294967295($UMax_32$)，与补码形式的-1的位模式是完全一样的。 有符号转无符号 $T2U_w$12345678910111213int main(){ unsigned u = 4294967295u; int tu = (int) u; printf(&quot;u=%u, tu=%d\\n&quot;, u, tu);} 运行结果： 123u=4294967295, tu=-1 补码和无符号码之间的关系，以16位为例： 有符号码： 12345的二进制： 0011 0000 0011 1001 -12345的二进制数： 1100 1111 1100 0111 53191的无符号码和-12345的二进制表示一样。 可以发现 $12345 + 53919 = 2^{16}$ 其实很容易推导出来, 可以得到如下关系： 从符号转无符号，如果是负数 u , 假设除了最高位的数表示为 y，那么： $-2^{w-1} + y = u$ 现在要求无符号： $2^{w-1}+y = u+ 2*2^{w-1}= 2^w + u$ 无符号转有符号 $U2T_w$ $TMax_{w} = 2^{w-1}-1$ 当 $u &gt; TMax_w$ 时，显然转换为有符号应该是负的。 无符号： $2^{w-1}+y = u$ 有符号： $-2^{w-1} +y = u-2*2^{w-1}=u-2^w$ 2.2.5 C语言中的有符号数和无符号数一般都默认为是有符号数，要创建一个无符号数，必须加上后缀 ‘u’ 或 ‘U’. 显示的强制转换： 隐式的强制转换： 输出 %d, %u, %x 分别表示十进制、无符号十进制和十六进制。所以在printf输出时，也会发生转换。 总之，记住输出的类型只是一种解释形式，底层的位值是不变的。 当执行运算时，一个运算数是无符号的，一个是有符号的。那么有符号的会隐式的转换为无符号进行计算。 比如32位机器中 -1 &gt; 0u. 这里的-1是 4294967295u 2.2.6 拓展一个数字的位表示 无符号拓展 $B2U_w(\\overrightarrow u)=B2U_{w’}=(\\overrightarrow u’)$ 在表示的开头添加 w’-w 个0 有符号拓展 $B2T_w(\\overrightarrow x) = B2T_{w’}(\\overrightarrow x’)$ 在表示开头添加 w’-w 个 $x_{w-1}$ 对于无符号很好理解，添加0后值不会改变。但对于有符号的负数，添加1之后，其值也是没有改变的。比如有符号 [101] 表示 -4+1=-3. 扩展一位 [1101]= -8+4+1= -3. 这是因为每扩展一位 $-2^w+2^{w-1}=-2^{w-1}$. 所以不变～～ 2.2.7 截断数字截断无符号数，其实就是取模运算$x=B2U_w(\\overrightarrow x), x’=B2U_k(\\overrightarrow x’)$ 则 x’=x mod $2^k$. 即对 2^k 取余。 截断有符号数$[x_{w-1},x_{w-2},…,x_0]$ 先按照无符号数截断，也就是对 $2^k$ 取余。然后在转换为有符号数。 2.2.8 关于有符号数与无符号数的建议练习题 2.25 出现错误是因为 length 是无符号数，当length=-1时，会转换为 2^31-1. 循环会无限循环下去。 将 unsigned lenght 修改为 int length 即可。 练习题2.26 strlen 返回的 size_t 是unsigned int， 所以 相减为-1之后会转换。 所以改为 int (strlen(s)- strlen(t)) &gt; 0; 无符号数有时候会很有用的。 2.3 整数运算2.3.1 无符号加法x， y是无符号数，0 &lt; x,y &lt; $2^w-1$. 如果溢出，说明 $2^{w} \\le x+y &lt; 2^{w+1}-2$. 需要 w+1 位才能表示出来。 s = x + y. 当溢出时，s = x + y - 2^w 溢出时，w+1位为1，丢弃它相当于从和中减去了 $2^w$。正常情况下，w+1 位为0，不会影响。 检测无符号溢出的条件： 如果溢出：$s = x + y -2^w$, 那么 $s = x+(y-2^w) &lt;x$, 同样 $s = y+(x-2^w)&lt;y$ 习题 2.27 1234567891011int uadd_ok(unsigned x, unsigned y){ unsigned s = x + y; return s &gt;= x;} 无符号数求反模数加法形成一种数据结构，称为阿贝尔群，也就是求 x 的逆元 $-^u_w x$. 什么意思呢，就是 这个逆元加上 x 对2^w 取模等于 0. 所谓模数加法，超过了模 $2^w$, 就是除以模 $2^w$ 的余数 练习题2.28 | x 十六进制 | x 十进制 | $-^u_4x$ 十进制 | $-^u_4x$十六进制 | | ———- | ——– | ————— | —————- | | 0 | 0 | 0 | 0 | | 5 | 5 | 11 | B | | 8 | 8 | 8 | 8 | | D | 13 | 3 | 3 | | F |15 | 1 | 1 | 2.3.2 补码加法 注意跟无符号数正溢出的区别，无符号数溢出之后是去掉 w+1 位，但是有符号数正溢出时，$x+y&lt;2^w$，w位还是要考虑的，只是变为负数了。所以是 $x+y-2*2^{w-1}$, 而不能认为是 $x+y-2^{w-1}$ 但是负溢出，就会溢出到 w+1 位了，那么去掉 w+1 位，就是 $x+y+ 2^w$ 了 推导的话，因为不论有符号还是无符号，都是相同的位级表示，二进制的加法也是进位同样的方式。 只是解释方式不一样而已。所以可以将有符号转换为无符号数进行 $+^u_w$ 的模 $2^w$ 的模数加法运算，然后在转换为有符号数即可。 补码溢出的条件 习题 2.30 补码溢出判断： 1234567891011121314151617int tadd_ok(int x, int y){ int sum = x+y; if ((x&gt;0 &amp;&amp; y&gt;0 &amp;&amp; s&lt;=0) || (x&lt;0 &amp;&amp;y&lt;0 &amp;&amp; s&gt;=0)) return 0; return 1;} 习题 2.31 123456789101112131415/* Determine whether arguments can be added without overflow *//* WARRING: This code is buggy */int tadd_ok(int x, int y){ int sum = x+y; return (sum-x == y) &amp;&amp; (sum-y ==x);} 无论正确与否，都是返回 1. 模数加法形成了一种数据结构，称为阿贝尔群(Abelian group). 因此代码中的判断，无论溢出与否，都是成立的，都返回 1； 具体推导如下： 如果 x+y 溢出， $sum = x+y-2^w$, 那么 $sum-x = y-2^w$. 因为 $y &lt; 2^{w-1}-1$, 则 $y-2^w&lt; -2^{w-1} -1$, 仍然负溢出。 所以 $sum-x= y-2^w+2^w = y$. 习题 2.32 x -y 不溢出时，返回1. 1234567891011/* Determine whether arguments can be subtracted without overflow *//* WARNNING: This code is buggy. */int tsub_ok(int x, int y){ return tadd_ok(x, -y);} x 和 y 取什么值是，会产生错误的结果？ 当 $y = TMin_w = -2^{w-1}$ 时， $-y = 2^{w-1}$ 原本应该是正数的，但已经溢出了，转换成了负数。 $-y = y$. 此时对于 tadd_ok(x, -y) 来说，当 x 为负时都认为溢出，返回 0，而 x 为非负时，都认为没有溢出返回 1。而情况恰恰相反，tsub_ok(x, TMin)，当 x 为 负数时，应认为没有溢出，返回 1，而 x 为非负时，应认为溢出返回 0. 2.3.3 补码的非 因为补码又负数，所以一般情况下，其逆元就是 -x， 而对于 $TMin_w$, $-TMin_w$ 会溢出，就不属于一般情况了。 练习题 2.33 | x 十六进制 | x 十进制 | $-^t_4x$ 十进制 | $-^t_4x$十六进制 | | ———- | ——– | ————— | —————- | | 0 | 0 | 0 | 0 | | 5 | 5 | -5 | -5 | | 8 | -8 | -8 | 8 | | D | -3 | 3 | 3 | | F | -1 | 1 | 1 | 第二种方法: 执行位级补码非，然后加1 对于无符号数或有符号数，非（或者逆元），他们的位模式是一样的。都是位模式取反+1， 对任意整数 非（逆元）-x 和 其位模式取反（位级补码非）加1 ~x+1 都是一样的。 2.3.4 无符号乘法$0\\le x,y \\le 2^w-1$, $0\\le x\\cdot y z^{2w}-2^{w-1}+1$ 需要2w 位来表示。将其截断为 w 位，等价于计算该值模 $2^w$. $$x*^u_w y = (x\\cdot y)mod\\ 2^w$$ 2.3.5 补码乘法补码乘法和无符号的乘法运算的位级表示是一样的。 对于 $TMin_w\\le x,y\\le TMax_w$ 有： $$x*^t_wy=U2T_w((x\\cdot y)mod\\ 2^w)$$ 原理是： $$T2B_w(x^t_wy)=U2B_w(x’ ^u_w y’)$$ x’,y’ 是x，y的无符号转换后的数。 证明略。 虽然完整的乘积的位级表示可能不同，但截断后是一样的。 练习题 2.34 | 模式 | x | y | $x\\cdot y$ | 截断后的 $x\\cdot y$ | | —— | ——– | ——– | ———– | ——————- | | 无符号 | [100]=4 | [101]=5 | [10100]=20 | [100]=4 | | 补码 | [100]=-4 | [101]=-3 | [1100] = 12 | [100]=-4 | | 无符号 | [010]=2 | [111]=7 | [1110]=14 | [110]=6 | | 补码 | [010]=2 | [111]=-1 | [110] =-2 | [110]=-2 | 练习题 2.35 12345678910111213141516171819/* Determine whether arguments can be multiplied without overflow */int tmult_ok(int x, int y){ int p = x*y; /* Either x is 0, or dividing p by x gives y * / return !x || p/x == y;} 见题 2.31，我们不能用减法来检验加法是否溢出，但这里可以用除法为检验乘法是否溢出。 证明 ？？ 练习题 2.36 1234567891011121314151617int tmul_ok(int x, int y){ /* Compute product without overflow*/ /* 这一行的强制类型转换至关重要。如果写成 int64_t p = x*y; * / /* 就会用 32 位值来计算乘积(可能溢出），再符号扩展到 64 位*/ int64_t p = (int64_t) x * y; /* See if casting to int preserves value*/ return (int)p == p; 2.3.6 乘以常数在大多数机器上，整数乘法指令相当慢，需要10个或者多个时钟周期。而加法、减法、位级运算和移位只需要1个时钟周期，因此编译通常把乘法转换为移位和加法的组合运算。 1.先考虑乘以 2 的幂 与2的幂的无符号乘法： x &lt;&lt; k 产生数值 $x*^u_w 2^k$ 与2的幂的补码乘法： 补码与无符号的位级操作等价，因此补码运算的2的幂的乘法也类似。 x &lt;&lt; k 产生数值 $x*^t_w 2^k$ 无论是补码还是无码，都可能溢出。即使溢出，通过移位和乘法的结果也是一致的，截断就好了。 对任意数的乘法可写成： $x14 = x(2^3+2^2+2^1)$ 或者 $x14 = x(2^4-2^1) = (x&lt;&lt;4)-(x&lt;&lt;1)$ 练习题 2.38 (a&lt;&lt;k)+b， 其中k可以为0,1,2,3, b可为0或a 123456789101112131415a 的倍数 LEA 指令2a (a&lt;&lt;1)+03a (a&lt;&lt;1)+a4a (a&lt;&lt;2)+05a (a&lt;&lt;2)+a8a (a&lt;&lt;3)+09a (a&lt;&lt;3)+a 乘法还可以这么看， x*K. 编译器将k的二进制表达为一组0和1交替的序列。 n表示第一段开始的n个连续的1，n-1表示第二段连续的1。 比如以8位为例，14 可以写成 [(0000)(111)(0)],从第3位开始到第1位为连续的1. 那么 x*14=(x&lt;&lt;3)+(x&lt;&lt;2)+(x&lt;&lt;1) 或者 x*14=(x&lt;&lt;4)-(x&lt;&lt;1) 很神奇。。。 练习题2.40 练习题2.41 编译器A、B两种形式选择哪一种： 选择操作次数少的。 2.3.7 除以2的幂除法更慢，需要30多个时钟周期。也可以采用移位运算实现。只不过是右移。无符号是逻辑右移（补0），补码是算术右移（补1） 定义向下取整 $\\lfloor a \\rfloor=a’. a’\\le a\\le a’+1$. 对于非负整数是满足除法的，但 $\\lfloor -3.14 \\rfloor = -4$ 是不满足的。 除以2的幂的无符号除法x&gt;&gt;k 等价于 $\\lfloor x/2^k\\rfloor$ 除以2的幂的补码除法算术右移，对与正数来说，最高有效位是0，与无符号的逻辑右移是一样的。 但对于负数来说，最高有效位是1，右移后效果如下图： $[x_{w-1},..,x_{w-1},x_{w-2},…,x_k]$ 是 $\\lfloor x/2^k\\rfloor$ 的补码表示，但是它不是向零舍入。需要使用“偏置”来修正这种不合适的舍入。 (x+(1&lt;&lt;k)-1)&gt;&gt;k 等价于 $x/2^k$ 向上取整。 偏置技术利用了如下属性： $\\lceil x/y \\rceil = \\lfloor (x+y-1)/y \\rfloor$ 推导： 假设 x = qy+r. $\\lfloor (x+y-1)/y\\rfloor=q+\\lfloor(r+y-1)/y\\rfloor$ 当r=0时，也就是能整除，$\\lfloor(r+y-1)/y\\rfloor=0$. 这也是要-1的原因，不然对于整除的时候就不符了。 当r&gt;0时，y&gt;r&gt;1,$\\lfloor(r+y-1)/y\\rfloor=1$, 结果相当于 q+1 所以，当 y=2^k 时， (x+(1&lt;&lt;k)-1)&gt;&gt;k = $\\lceil x/2^k\\rceil$ 总结： 对于使用算术右移的补码机器，C表达式： (x&lt;0 ? x+(1&lt;&lt;k)-1 : x) &gt;&gt; k 练习题2.42 写一个函数 div16，对于整数参数 x 返回 x/16 的值。要求不能使用除法、模运算、乘法、条件语句 (if 和 ? : )、比较运算符、循环等。假设 int 为 32 位，补码，算术右移。 1234567891011121314151617int div16(int x){ /* (x+bias) &gt;&gt; k 对于 x 为正数， bias=0 对于 x 为负数， bias=(1&lt;&lt;4)-1=15*/ int bias = (x&gt;&gt;31) &amp; 0xf; printf(&quot;%d&quot;, bias); return (x+bias) &gt;&gt; 4;} 练习题 2.43 下面的代码中，省略了常数 M 和 N 的定义： 1234567891011121314151617#define M /* Mystery number 1 */#define N /* Mystery number 2 */int arith(int x, int y){ int result = 0; result = x*M + y/N; return result;} 用某个 M 和 N 的值编译上面代码后，编译器将优化乘除操作。下面是产生的机器码翻译回 C 的结果： 1234567891011121314151617181920212223/* Translation of assembly code for arith */int optarith(int x, int y){ int t = x; x &lt;&lt;= 5; x -= t; /* x = x*2^5 -x，使用了形式B的优化：(x&lt;&lt;(n+1))-(x&lt;&lt;m)， 这里 n=4, m=0, 从而 M = [11111] = 31*/ if (y &lt; 0) y += 7; /* y + (1&lt;&lt;3)-1*/ y &gt;&gt;= 3; /* Arithmetic shift，从而 N 为 2 的 3 次方，即为 8*/ return x+y;} M=31, N=8 2.3.8 关于整数运算的最后思考整数运算实际上是一种模运算形式。 整数表示的有限字长限制了运算结果的取值范围，从而可能会溢出。无论是补码表示还是无符号数，它们的加减乘除操作，在位级上都完全一样或非常类似。 C 中的 unsigned 数据类型变量，在运算中会隐式地要求将其它参数先转换成 unsigned 数据类型再计算，从而会导致难以察觉的 BUG。 习题2.44 A. x=TMin_w 时为假， $x = -2^{w-1}-1 = -2^{w-1}-1+2^w=2^{w-1}-1 &gt;0$ B. 举反例就是两边都为假。左边 (x&amp;7)=7=[000…0111], 那么最后三位都为1. 右边 x&lt;&lt;29 &gt;0,那么倒数第三位为0. 所以不存在同时两边都为假的x。 C. 当 $x=2^31-1$, $x*x=2^62-2$,溢出，截断之后最高位为1，肯定小于0 2.4 浮点数2.4.1 二进制小数 小数的二进制表示法只能表示 $x\\times 2^y$ 的数，其他的只能近似表示。二进制表示的长度可以提高表示的精度。 例如 $\\dfrac{1}{5}$ 可以用十进制0.20精确表示，但并不能准确的用二进制表示。 $0.111…1_2$ 表示刚好小于0的数。用 $1.0-\\epsilon$ 表示 2.4.2 IEEE浮点表示 符号位 s 阶码（exponent）E的作用是对浮点数加权。 n为小数字段 $frac=f_{n-1}\\cdots f_1f_0$, 编码出来的也依赖与阶码段的值是否为0 单精度： s, exp, frac字段为 1,8,23 位 双精度： s,exp, frac字段为 1,11,52 位 1.规格化的值 exp的位模式不全为0，也不全为1。 阶码字段被解释为以偏置形式的有符号整数。 阶码的值 $E=e-Bias$, e是无符号数，为表示是 $e_{k-1}\\cdots e_1e_0$. Bias等于 $2^{k-1}-1$. 所以单精度范围： $e_{max}=2^8-2, e_{min}=1$ $E_{max}=2^8-2-2^7+1=127$ $E_{min}=1-(2^7-1)=-126$ 2.非规格化的值 阶码全为0， E=1-Bias. 3.特殊值 当阶码全为1时，小数域全为0，得到的值表示无穷。s=0时是正无穷，s=1是负无穷。 阶码全为1， 小数域非0，表示NaN. 2.4.3 数字示例","link":"/2018/06/09/CSAPP-02-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/"},{"title":"UCL-DRL-02-MDP","text":"Markov ProcessIntroduction to MDPs 马尔可夫决策过程： 环境完全可观测 当前状态能完整的描述这个过程 绝大多数 RL 问题都可以形式化为 MDPs: 优化控制问题 部分可观测问题可转化为 MDPs Bandits（老虎机问题） multi-armed bandits problems Markov property 一旦当前状态确定后，所有的历史信息都可以扔掉了。这个状态足够去预测 future. state transition matrix 状态转移矩阵定义了所有的从某一个状态到另一个状态的概率。所以每一行相加为 1. Markov chain 马尔可夫链可以看做是 一个 tuple(S, P) S 是有穷的状态的集合 P 是状态转移矩阵 $$P_{ss’}=P[S_{t+1}=s’|S_t=s]$$ example: 简单的马尔可夫链，没有 reward, action 等。 Markov reward process 在 Markov chain 基础上增加了 reward function. return 折扣因子 $\\gamma \\in [0,1]$ 越接近0， 意味着越短视 越接近1, 意味着有远见 Why discount 数值计算方便 避免无限循环 对未来的不确定性 如果reward是经济的，即刻的reward能获得更多的利益相比之后的reward 动物/人更倾向于即刻的奖励 有时候也会使用 undiscounted 马尔可夫奖励过程 value function因为 future 有很多中不确定性，所以需要采样. sample 得到的 $G_t$ 的期望，也就是 value $$v(s)=E[G_t|S_t=s]$$ 上图中 $v_1$ 应该是 $g_1$. $$G_1=R_2+\\gamma R_3+…+\\gamma^{T-2}R_T$$ 也就是计算从 t 时刻开始到结束，整个过程可能的奖励值。因为未来可能有很多中情况，所以需要 sample. 印象中，好像 alpha go 就是用的蒙特卡洛模拟。 Bellman Equation for MRPs 动态规划。。 这里容易理解错的一点就是： $R_s$ 实际上就是 $R_{t+1}$, s 表示 v(s) 中的当前状态。 $P_{ss’}$ 是状态转移的概率， 从 s 到 后继状态 s’ 的概率。 所以： $$\\gamma v(S_{t+1}|S_t=s) = \\gamma\\sum_{s’\\in S} P_{ss’}v(s’)$$ 转换成矩阵形式： 事实上: 系数矩阵 P 就是 状态转移矩阵，有 n 中状态，那就是 $n\\times n$ matrix. solving bellman problems 直接计算是 $O(n^3)$, 因为 n 个状态，P 的计算就是 $O(n^2)$ 对于很复杂的 MRPs，可以采用： dynamic programming monte-carlo evalution temporal-difference learning Markov Decision Process 想对于 Markov reward process 增加了 decision. $P_{ss’}^a$ 是状态转移矩阵，但是这里的状态之间的转移概率不再是确定好了的。而是取决于 当前状态，以及当前状态下的 action. $$P_{ss’}^a=P[S_{t+1=s’}|S_t=s, A_t=a]$$ 因此 R reward function： $$R_s^a=E[R_{t+1}|S_t=s,A_t=a]$$ 可以发现，与之前的区别是，state 到 state 之间是通过 action 决定的。比如 第一个 state 下，可以是 study or facebook. policy 这个决策就是想对于 Markov reward process 多出来的一部分，你需要自己去做决策。只不过在每一个状态下，其决策也是一个 distribution $$\\pi(a|s)=P[A_t=a|S_t=s]$$ 事实上 Markov decision process 也可以转换成 Markov reward process, 从状态 s 到 s’ 的概率： $$P_{s,s’}^{\\pi}=\\sum_{a\\in A}\\pi(a|s)P_{ss’}^a$$ value function value 是当前状态 s 下的 reward $G_t$ 的期望。 Bellman expectation Equation$v_{\\pi}$ 基于 $q_{\\pi}$ 的表示 state-value $v_{\\pi}$ 是 action-value $q_{\\pi}$ 基于 action 的期望。 $$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)q_{\\pi}(s, a)\\quad \\text{(1)}$$ $q_{\\pi}$ 基于 $v_{\\pi}$ 的表示 action-value： $$q_{\\pi}(s,a)=R_s^a+\\gamma\\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’)\\quad \\text{(2)}$$ 从状态 s 到 s’， 基于 statetransition probability matrix $P_{ss’}^a$. $v_{\\pi}$ 基于 $v_{\\pi}$ 的表示 将 （2）带入（1）可得： $$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)(R_s^a+\\gamma\\sum_{s’\\in S}P_{ss’}^av_{\\pi}(s’))\\quad \\text{(3)}$$ 这里得到的是 state-value 的表达式。第二个类和符号是 从状态 s 到 s’ 所有可能的后继状态 s’。第一个类和符号是 状态 s 下所有可能的 action. 这里的类和都是表示期望。 $q_{\\pi}$ 基于 $q_{\\pi}$ 的表示 将 （1） 带入 （2）可得： $$q_{\\pi}(s,a) = R_s^a+\\gamma\\sum_{s’\\in S}P_{ss’}^a\\sum_{a\\in A}\\pi(a’|s’)q_{\\pi}(s’,a’)$$ 这里得到的是 action-value 的表达式。第一个类和符号是在 action a 下，所有可能的后继状态 s’ 的类和。i第二个类和符号是 后继状态 s’ 下所有的 action 的类和。 example： Matrix form 类似于前面 Markov reward process 的表示，但是这里的状态转移矩阵变成了 $P^{\\pi}$, reward 变成了 $R^{\\pi}$. 个人理解： Markov decision process 与 reward process 的区别在于原本在一个 state s 到下一个 state s’ 的概率是给定了的。但是在 decision process，并不存在这个概率，而是取决于 action，而在当前 state s 下，每一个 action 的概率取决于 policy $\\pi$. optimal value function optimal policy Solving the Bellman Optimality Equation 最优 policy 就是找到 $v_{* }(s)$ 和 $q_{* }(s,a)$ 推导过程与前面类似，通过 bellman optimally equation 得到： $$v_*(s)=max_{a}q_*(s,a)$$ $$q_*(s,a)=R_s^a+\\gamma\\sum_{s’\\in S}P_{ss’}^av_*(s’)$$ $$v_*(s)={max}{a}(R_s^a+\\gamma\\sum{s’\\in S}P_{ss’}^av_*(s’))$$ $$q_*(s,a)=R_s^a+\\gamma\\sum_{s’\\in S}P_{ss’}^aq_*(s’,a’)$$ state-value function optimal action-value function optimal Extension to MDP","link":"/2019/01/14/UCL-DRL-02-MDP/"},{"title":"UCL-DRL-01-introduction","text":"About reinforcement learning强化学习的特性： 不同于有监督学习，没有 supervisor, 只有一个 reward signal. The reinforcement learning problemreward 强化学习就是基于 奖励假设(reward hypothesis). 决策的选择是为了获得更大的 future reward. environments statehistory and state state 是 history 的函数： $$S_t = f(H_t)$$ environment state 环境状态对于 agent 可以是可见的，也可以是不可见的。即使是可见的，也不一定是有用的信息。 Agent state 代理状态就是 agent 的中间表示。 information state 又称 Markov state. 看做是马尔可夫链，当前状态包含了过去所有的信息。那么之前的 history 就可以扔掉了。 Markov decision process 马尔可夫决策过程条件：环境 state 与 agent state 一样 Agent state = environment state = information state partially observable environments(POMDP) agent 并不能直接观察环境： 机器人具有摄像头的视觉信息，但不知道自己的绝对位置 trading agent 只能观察到当前价格 扑克牌选手只能看到公开的卡牌 agent state $\\ne$ environment state agent 必须重新构建自己的状态表示 $S_t^a$: 比如循环神经网络就是 POMDP $$S_t^a=tanh(s_{t-1}^aW_s+O_tW_o)$$ Inside An RL Agent在一个 agent 内部具体有什么呢?我们怎么去定义一个 agent: policy: agent’s behaviour function value function: how good is each state and/or action model: agent’s representation of the environment policy 策略 policy： 就是将 state 映射到 action. value function 如何设计 value function 来计算 future reward 感觉是个难度呀～ model 模型：用来预测环境如何变化，也就是模拟环境吧。比如 RNN 模型，就是用神经网络模拟序列变化。 Problems with Reinforcement LearningLearning and planning 学习和规划的区别 environment 是否 known agent 与 environment 是否有 interaction Exploration and Exploitation 探索与开发：之前在三星听在线学习的讲座时，通过 多臂老虎机 和 在线广告 讨论过这个问题 Exploration（探索） finds more information about the environment Exploitation（开发） exploits known information to maximise reward It is usually important to explore as well as exploit 这是一个需要权衡或博弈的问题。 prediction and control","link":"/2019/01/14/UCL-DRL-01-introduction/"},{"title":"UCL-DRL-03-planning by DP","text":"Planning by Dynamic ProgrammingIntroductionwhat is dynamic programming? 动态规划是一种方法/思想，将复杂问题分解成子问题，并解决子问题。 DP 是具有以下两种特质的问题常用的解决方法： 具有可优化的子结构 优化问题可以分解成子问题 重叠子问题 子问题重复出现很多次 子问题的solution可以被缓存和reuse 马尔可夫决策过程就具有这两种特质： Bellman 公式给出了迭代分解的方式 value funvtion 用来存储和再利用子solutions Bellman 公式的含义分为两部分，第一部分是目前的一步是最优的行为，第二部分是余下来的其他步骤也是最优的行为。 动态规划需要的输入： MDP $&lt;S,A,P,R,\\gamma&gt;$ and policy $\\pi$ or MDP $&lt;S,P^{\\pi},R^{\\pi},\\gamma&gt;$ 输出： value function $v_{\\pi}$ DP 适用的一些场景。比较熟悉的 sequence alignment, shortest path algorithms, viterbi algorithm. Policy evaluationIterative policy evaluation $v_{k+1}(s)=\\sum_{a\\in A}\\pi(a|s)(R_s^a+\\gamma\\sum_{s’\\in S}P^a_{ss’}v_k(s’))$ example 方块中任意一个状态到另一个状态的 reward 是 -1. 每迭代一步有 north, east, south, weat 4种选择。 k=0. 初始状态下，所有的方块累计的 reward 都是 0. 所以在这个状态下是 random policy. k=1, 迭代一步之后，除了 terminal squares, 其他的 reward 都是 1. 如果采用 greedy policy，就能确定部分路径了。 k=2, 在 random policy 策略下，我们来看下 1.75 怎么得到的： $$1+(1+1+1)/4=1.75$$ k=3, 在 random policy 策略下，我们来看 2.4, 2.9 怎么得到的： $$(1+2.7+3+3)/4=2.425$$ $$(2.7+3+3+3)/4=2.925$$ 这里的 k 是迭代的次数，并不是时间. policy iteration 迭代方式： policy evaluation policy improvement: greedy policy improvement $$v_{\\pi}(s)=E[R_{t+1}+\\gamma R_{t+2}+….|S_t=s]$$ improve the policy by acting greedily. 当前最优 policy: $${\\pi}^{‘}(s)=greedy(v_{\\pi})=argmax_{a\\in A}q_{\\pi}(s,a)$$ $q_{\\pi}(s,a)$ 是 action-value function. 这页ppt中最后的公式实际证明了：采用 greedy policy 至少能保证 $v_{\\pi^{‘}}\\ge v_{\\pi}(s)$. modified policy iteration value iteration 优化的定理： 任何优化策略都可以分解成两部分： 最优化的 action A 后继状态 S’ 下的最优化策略 policy example: summary of DP algorithms In-Place Dynamic Programming Prioritised Sweeping Real-Time Dynamic Programming Extensions to dynamic programming","link":"/2019/03/06/UCL-DRL-03-planning-by-DP/"},{"title":"UCL-DRL-04-policy gradient and Actor Critic Methods","text":"policy gradientbasicDeriving Policy Gradients and Implementing REINFORCE 这篇博客详细推导了 policy gradients 的过程，虽然公式很多，但其实还算简单。 最终的结论就是，从公式(1)推导为公式(2): $$J(\\theta)=\\mathbb{E}[\\sum_{t=0}^{T-1}r_{t+1}|\\pi_{\\theta}]=\\sum_{t=1}^{T-1}P(s_t, a_t|\\tau)r_{t+1}\\quad(1)$$ $$\\nabla_{\\theta}J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_{\\theta}log\\pi_{\\theta}(a_t|s_t)(\\sum_{t’=t+1}^T\\gamma^{t’-t-1}\\gamma_{t’})\\quad(2)$$ 其中 $Gt=\\sum_{t’=t+1}^T\\gamma^{t’-t-1}\\gamma_{t’}$ 公式(2)可简化为: $$\\nabla_{\\theta}J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_{\\theta}log\\pi_{\\theta}(a_t|s_t)G_t$$ 之后就是针对 policy gradient 的各种改进。接下来的公式参数是根据李宏毅老师课程来的。所以符号与上面的有差异。 add baseline$$\\nabla{\\overline R}{\\theta}=\\dfrac{1}{N}\\sum{n=1}^N\\sum_{t=1}^{T_n}(R(\\tau^n)-b)\\nabla logp_{\\theta}(a_t^n|s_t^n)$$ assign suitable credit$$\\nabla{\\overline R}{\\theta}=\\dfrac{1}{N}\\sum{n=1}^N\\sum_{t=1}^{T_n}(\\sum_{t’=t}^{T_n}r_{t’}^n-b)\\nabla logp_{\\theta}(a_t^n|s_t^n)$$ add discount factor advantage functionaction $a_t$ 的好是相对的，而不是绝对的。它可以是 state-dependent. on-policy and off-policyimportance sampling issue of importance sampling: 采用 importance sampling，会导致sample 得到的action的分布方差发生变化。因此要保证action的分布尽可能与 on-policy to off-policy PPO(proximal policy optimization)add constraint: $\\theta, \\theta’$ 并不是distribution，而是参数。那么这里的 $kl(\\theta, \\theta’)$ 到底是什么？ 这里的kl divergence 实际上是行为上的差距，也就是 action 分布的距离。那这个意思就是我们还是得用 $\\theta$ 去求出 action 的 distribution，但是我们不用去sample出样本了。可以继续需要用 $\\theta’$ sample出来的样本，但是需要给reward乘以系数 $\\dfrac{p_{\\theta}(a_t|s_t)}{p_{\\theta’}(a_t|s_t)}$. PPO2 因为 $kl(\\theta, \\theta’)$ 的计算还是蛮复杂的，所以用ppo2来代替。方法也是很直接，用clip的方式代替原来的正则项。 总结一下ppo： 首先它是off-policy的，为什么将on-policy转换成off-policy呢，因为on-policy速度太慢了。在policy iteration的时候先sample尽可能多的(state, action)pairs，然后计算对应的reward，再基于policy gradient来更新policy的参数 $\\theta$.这是一次迭代。再然后基于updated policy生成新的(state, action) pairs或者新的example吧，依次迭代。。。这个过程中，得到action的分布，然后sample得到尽可能多的actions，这一步是非常耗时的，而且每次迭代都需要重新生成样本。 off-policy改进的就是搞一个近似于当前policy $\\theta$ 的 $\\theta’$. 用这个 $\\theta’$ 去采样 $(state, action)$ 样本，这个 $\\theta’$ 并不是随着 policy $\\theta$ 的更新而更新的，所以它sample出来的样本可以用很久。 但是policy $\\theta$ 更新之后，其对应的 action 的分布也是变换的，这也是我们想要的。所以怎么减小 $\\theta’$ 和 $\\theta$ 对应的action分布的差异，就有了ppo公式的第一项，也就是 importance sampling. 但是importance sampling得到的action的分布存在偏差（方差不一致)，所以需要尽可能保证 $\\theta, \\theta’$ 采样得到的action分布尽可能接近。于是有了ppo,ppo2的方法。 Criticstate-value function$V_{\\pi}(s)$ 表示的是当到达某个状态 s 之后，如果接下来一直按着策略 $\\pi$ 来行动，能够获得的期望收益. action value function$Q_{\\pi}(s,a)$ 表示的是当达到某个状态 s 之后，如果强制采取行动 a ，接下来再按照策略 $\\pi$ 来行动，能获得的期望收益。 显然地，状态价值函数和行动价值函数之间有关系: $$V_{\\pi}(s)=\\sum_a\\pi(a|s)Q_{\\pi}(s,a)$$ MC v.s. TD sample同样的结果，采用 MC 和 TD最终计算的结果也是不一样的。 MC考虑的是，当前state $s_a$ 对未来的 state $s_b$ 可能也是有影响的. 所以MC实际上是要计算到整个游戏（episode）结束。","link":"/2019/06/22/UCL-DRL-04-policy-gradient-and-Actor-Critic-Methods/"},{"title":"chapter10-词性标注","text":"词性分类 clossed class, open Classes 标记集 Tagset HMM tagging: 生成模型 $p(t_i|w_i)=p(w_i|t_i)p(t_i|t_{i-1})$ 有viterbi算法和greedy算法～ MEMM: 判别模型 $p(t_i|w_i,t_{i-1},…f;w)$ 也有Viterbi和greedy两种算法，更新参数w～ 双向模型 CRF，chapter20讲 前言： 词性标注（parts-of-speech）又称 (pos, word classes, or syntactic categories) 分为8类： noun, verb, pronoun, preposition, adverb, conjunction, participle, and article.（名词，动词，代词，介词，副词，连词，分词和文章。） 词性标注在寻找命名实体（named entities）和其他一些 信息提取(information extraction) 的工作中是很重要的特征。词性标注也会影响形态词缀(morphological affixes)，从而影响词干(stemming)的信息检索;在语音识别中对语音的生成也有很重要的作用，比如CONtent是名词，而conTENT是形容词。 (Mostly) English Word Classes传统上，词类根据形态和语法功能来分类： distributional properties 分布特征:单词出现在相似的环境中； morphological properties 形态特征：单词的词缀具有相似的功能。 词类可分为两大类：封闭类 closed class types 和 开放类 open class type. 封闭类：prepositions 介词，function words， like，of，it,…. 一般都很短，而且频率高;开放类：nouns, verbs,adjectives, and adverbs noun 名词: 专有名词(Proper nouns)和普通名词（common nouns）。专有名词一般不受冠词限制，且一地个字母要大写。普通名词又分为可数名词（count nouns）和不可数名词（mass nouns）. 当某种东西能在概念上按照同质来分组时，就使用物质名词，这样的词是不可数的。（Mass nouns are used when something is conceptualized as a homogeneous group）. verb 动词： 用来表示动作或过程。动词有若干形态，(non-third-person-sg非第三人称单数 (eat), third-person-sg第三人称单数 (eats), 进行时progressive (eating), 过去分词past participle (eaten)). adjectives 形容词： 描述性质和质量的单词。 adverbs 副词： 无论是从语义上还是形态上，都比较杂。通常用来修饰动词，也可以用来修饰其他副词或是动词短语。方位副词和地点副词 Directional adverbs or locative adverbs， 程度副词 degree adverbs (extremely, very, somewhat); 方式副词 manner adverbs (slowly, slinkily, delicately); 时间副词 temporal adverbs (yesterday, Monday). 封闭类： preposition 介词: 出现在名词短语之前，从语义上讲，他们是表示关系的。 particle 小品词： 与介词或副词相似，经常和动词结合，形成动词短语。 determiner 限定词: 其中包括 article 冠词，a,an,the. 其他的限定词，比如 this,that conjunction 连词： 连接两个短语、分句或句子。 pronoun 代词： 简短地援引某些名词短语、实体或事件的一种形式。 auxiliary 助动词： 包括系动词be,两个动词do, have,以及情态动词。 英语中还有很多 叹词(oh,ah,hey,man,alas),否定词(no,not),礼貌标志词(please,thank you). 是否把这些词放在一起,取决于标记的目的。 The Penn Treebank Part-of-Speech Tagset 标记集 Peen Treebank标记集是Brown语料库原有的87个标记集中挑选出来的。还有两个比较大的标记集 C5,C7. 词性标注词性标注(pos tagging)与计算机语言的 tokenization （词形还原？分词） 过程是一致的，但词性标注具有更多的歧义性。在Brown语料库中，只有11.5%的英语词型(word type)是具有歧义的,40%以上的词例(word token)有歧义的。 chapter2中几个容易混淆的概念： 词型 word type 不包括重复词， 词例 word token 包括重复词。 lemma 是词意，am is are是同一个单词be Wordform 是词的形状。 词性标注是歧义消解（disambiguation）的一个重要方面，大多数的标注算法分为两类：基于规则的标注算法(rule-based tagger)，一类是随机标注算法(stochastic tagger). HMM Part-of-Speech Tagging基于隐马尔可夫的词性标注，语料库是观察序列，part-of-speech是隐藏状态,对于Peen Treebank标记集有45中隐藏状态。因为训练数据是有人工标注的，所以我们的目标是，根据已知的观察序列和对应的隐藏状态，可以根据极大似然估计或者相对频率计算出状态转移矩阵和发射矩阵的参数～ 所以词性标注的问题和预测问题是一样的～可以使用viterbi算法，对带标注的序列进行标注～ The basic equation of HMM TaggingHMM decoding:求概率最大的tagging序列 $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)$$ 使用bayes公式： $$\\hat t_1^n=argmax_{t_1^n}\\dfrac{P(w_1^n|t_1^n)P(t_1^n)}{P(w_1^n)}$$ 目的是让观察序列的似然概率最大化～因此观察序列对于任何隐藏序列都是一样的～故可以直接去掉分母： $$\\hat t_1^n=argmax_{t_1^n}P(w_1^n|t_1^n)P(t_1^n)$$ 根据概率图模型中，一个word的生成只取决于其对应的tag，而与其他word和word无关，也就是条件独立可得： $$P(w_1^n|t_1^n)\\approx \\prod_{i=1}^nP(w_i|t_i)$$ 根据bigram假设： $$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1})$$ 联立可得： $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)\\approx argmaxmax_{t_1^n}\\prod_{i=1}^nP(w_i|t_i)P(t_i|t_{i-1})\\tag{10.8}$$ Estimating probabilities 概率估计在HMM tagging中，概率估计是直接通过对训练语料库中进行计数得到的。 状态转移概率 transition probabilities: $P(t_i|t_{i-1})$,shape=(45,45) $$P(t_i|t_{i-1})=\\dfrac{C(t_{i-1},t_i)}{C(t_{i-1})}$$ 发射概率 emission probabilities: $P(w_i|t_i)$,shape=(45, V) $$P(w_i|t_i)=\\dfrac{C(t_i,w_i)}{C(t_i)}$$ Working through an example举个栗子 其中状态转移概率和发射概率依据已经标记好的语料库WSJ corpus计算得到，其对应的状态转移矩阵A和发射矩阵B： 那么对应的可能的状态序列如下图，加粗的黑色路径是概率最大的状态序列。 Viterbi算法先回顾下隐马尔科夫模型中的viterbi算法： 这是在给定模型 $\\lambda=(A,B)$ 的情况下，寻找状态序列使得观察序列的似然概率最大～ 矩阵 viterbi [N+2,T], 第一行和第二行是 states 0 和 $q_F$ 从state 1 开始： $$v_t(j)=\\max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t)$$ 可以看到，行代表的是[start, NNP, MD,…, DT, end]^T,总共 N+2 中状态。 列代表的是时间步 [1,2,3…,t],因为t=0时刻，肯定是start状态，没有emission. 从第一列 t=1 到 第二列 t=2，每一步都是前一步的N种状态转移到当前状态的max. Extending the HMM Algorithm to Trigrams$$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1})$$ 改为： $$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1},t_{t-2})$$ 每一步的计算复杂度，从N变成 $N^2$. 本书上写的，当前 state-of-art 的HMM 标注算法是 A statistical part-of-speech tagger. In ANLP 2000, Seattle. 论文作者让标注者在每句话结尾处加上 end-of-sequence marker for $t_{n+1}$. 这里的n表示序列长度。 $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)\\approx argmaxmax_{t_1^n}[\\prod_{i=1}^nP(w_i|t_i)P(t_i|t_{i-1},t_{i-2})]P(t_{n+1}|t_n)$$ 其中，转移概率可以通过语料库计数得到： $$P(t_i|t_{i-1},t_{i-2})=\\dfrac{C(t_{i-2},t_{i-1},t_i)}{C(t_{i-2},t_{i-1})}$$ 但是在测试集中，很可能遇到状态序列（也就是tag序列） $t_{i-2},t_{i-1},t_i$ 在训练集中从未出现过，也就是其概率为zeros，这样就无法对测试集中的序列进行标注了。也就是语言模型中提到的zeros情况～比如图10.7中，某个时间步没有可选择的隐藏状态tag 同样，我们也可以使用插值法～ 其中 $\\lambda$ 的计算可以用 deleted interpolation 不太理解。。。 Unknown WordsSamuelsson (1993) and Brants (2000)根据形态（morphology）来判断unknown word的可能状态. 比如 -s 通常是复数名词NNS, -ed通常是过去时态VBN, -able通常是形容词 JJ…. $$P(t_i|l_{n-i+1}…l_n)$$ 这个概率也是可以通过语料库中相对频率计算得到～ 他们使用尽量短的后缀（shorter and shorter suffixes）应用回退smoothing方法来估计unknow word的可能状态，但要尽量避免其状态为 closed class,比如介词 prepositions,可以将unknow word的状态tag选择从频率 $\\le 10$ 中选择，或者只从open classes 中选择可能的tag. Brants(2000) 还额外使用了首字母的特征信息，将公式(10.21)可改写为： $$P(t_i,c_i|t_{i-1},c_{i-1},t_{i-2},c_{i-2})$$ 这样语料库中标记集就包括首字母大写和小写两种版本，其对应的标记集tagset就增大为2倍。 state-of-art HMM tagging也就是论文Brants(2000) 的准确率达到 96.7% 在使用Penn Treebank标记集。 Maximum Entropy Markov Models最大熵隐马尔可夫模型，是依据logistic回归的，所以它是判别模型 discriminative sequence model, 而HMM是生成模型 generative sequence model. sequence of words: W = $w_1^n$ sequence of tags: T = $t_1^n$ 那么HMM模型的P(T|W)是依据bayes规则和最大似然P(w|T)得到的： 而在最大熵隐马尔科夫模型，直接计算后验概率 posterior P(T|W). $$\\hat T = argmax_TP(T|W) = argmax_T\\prod_iP(t_i|w_i,t_i-1)$$ 对比HMM和MEMM，HMM计算是在tag的条件下观察序列似然最大， MEMM计算是在观察序列的条件下tag序列似然最大～显然在tag已经标注好的训练集里，判别模型是完全可行的，也许会是更好的～ Features in a MEMM在HMM中，我们得到下一个tag的概率只依赖与前一个或两个tag，以及当前的word，如果想考虑更多的特征，比如首字母capitalization，后缀suffix，那样计算复杂度会增加很多。 相比之下，MEMM可以考虑更多的特征： MEMM可以依赖的特征可以是 word，neighboring words, previous tags, and various tags, and various combinations. 可使用 features templates 来表示： 对于之前的例子： Janet/NNP will/MD back/VB the/DT bill/NN 当 $w_i$ 是 back 时，对应的特征模板 features templates,也就是 know-words feature: 除此之外，还有当前词 $w_i$ 的拼写和形状特征，这些特征对于处理 unknow word 很有必要～ 那么根据上述规则，单词 well-dressed 的 word shape 特征就是： 这样一来，特征就很多很多了，通常需要进行一定的cutoff. 其中 $w_{i-l}^{i+l}$ 表示考虑当前词前后 l 个单词， $t_{i-k}^{i-1}$ 表示考虑前k个tags. Decoding and Training MEMMs 训练MEMMs在MEMMs中，每一步都是一个local classifer，然后make a hard decision,选择概率最大的词。。。以此类推。因此，这是贪心greedy算法～ 虽然这样使用greedy的方法，得到的准确率也还不错～ Viterbi算法原始Viterbi： $$v_t(j) = max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t)$$ HMM tagging: $$v_t(j) = max_{i=1}^Nv_{t-1}P(s_j|s_i)P(o_t|s_j)$$ MEMM: $$v_t(j) = max_{i=1}^Nv_{t-1}P(s_j|s_i,o_t)$$ 书上对如何训练参数一笔带过了，我自己总结如下： 初始化转移矩阵和发射矩阵参数，设为w 根据Viterbi算法填写矩阵 $N\\times T$, 也就是每一个网格用概率 $P(t_i|w_{i-1}^{i+l},t_{i-k}^{i-1})$, 并保留 backpointers 比较tag路径与真实路径，然后反向传播，更新参数权重w, 注意有正则化L1,L2 要更深入的理解MEMM，需要和HMM tagging对比更容易理解～～ HMM是生成模型，它是在求已知 $t_i$ 条件下生成 $w_i$ 的概率 $P(w_i|t_i)$,这根据语料库中的频率是可以计算得到的；然后后验概率 $P(t_i|t_{i-1})$ 也是可以通过频率计算得到的～ 知道似然概率和发射概率后根据bayes公式就可以预测了P(T|W)～ MEMM是判别模型，它是直接求 P(T|W),显然是tag生成word，而不是word生成tag，因此无法直接通过频率，但我们将它分解为每一个时间步计算 $P(t_i|w_i,t_{i-1},…)（各种特征），每一个特征有对应的权重w,然后根据最大熵原理，也就是公式（10.29）所示，从前一个tag $t_{i-1}$ 有N中状态，到下一个tag $t_i$, 是max的过程，但整个过程不是greedy的（这需要好好理解，其实也就是Viterbi一样的。。），然后根据backpoints得到的tag序列与真实tag序列对比，不断更新权重参数w！ Bidirectionality这里举了个例子来阐述双向的重要性。 will/NN to/TO fight/VB 通常 to 都是接在名词NN后面，而不会是情态动词 MD 后面，这里的will也应该是 NN。 但是在 $P(t_{will}|&lt; s &gt;)$， $t_{will}$ 更倾向于是情态动词 MD，而且 $P(TO|t_{will},to)$ 的概率接近于1,无论 $t_{will}$ 是啥，所以这时候 $t_{will}$ 就会错误的标注为 MD. 这样的错误叫 label bias or observation bias, 所以我们需要双向～ 条件随机场 Conditional Random Field or CRF 就是这样的～ 任何sequence model都可以是双向的。比如，对于tagging，可以在第一遍 left to right, 而从二遍开始就可以双向了～ SVMTool system 就是这样的，不过它在每一个时间步使用的不是最大熵分类器，而是SVM分类器，然后用Viterbi算法或者是greedy应用到整个sequence model～ Part-of-Speech Tagging for Other Languages中文的分词可以和标注一起进行～ 中文的未登录词问题～ 总结：","link":"/2018/04/17/chapter10-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"title":"chapter12-句法分析","text":"ambiguous and disambiguation PCFGs 如何从语料库中得到PCFGs，极大似然估计，也就是计算频率 Chomsky 范式 SKY，一种bottom-up的dynamic programming算法～ 前言： 为什么要做句法分析呢？主要是避免歧义。在 grammar checking，semantic analysis，question answering， information extraction 中都有很重要的作用。 如何从Treebanks中找到某一个sentence的tree呢？这个章节中会介绍一种动态规划的算法 Cocke-Kasami-Younger(CKY). 歧义 Ambiguity在词性标注中我们遇到了 part-of-speech ambiguity and part-of-speech disambiguation, 在句法分析中也会遇到类似的问题 structure ambiguity. 举个栗子： 先有这样的一个简单的 Treebank $L_1$. 对于同样的一句话，可以从上述Treebank中生成两种 parse tree. I shot an elephant in my pajamas. 显然右边的parse tree才是我们想要的，但左边的也是满足 $L_1$ 的语法规则的。 结构歧义通常分为两种：附着歧义(attachment ambiguity),并列歧义(coordination ambiguity。 事实上，在自然语言处理中，会有很多语法正确，但是语义上不合理的parse。因此我们需要句法消歧 syntactic disambiguation.一个有效的句法消歧算法需要很多语义semantic和语境contextual知识, 但幸运的是，统计的方法就可以很好的解决～ Probabilistic Context-Free Grammars (PCFGs)Speech and language Processing 这本书中关于PCFG内容实在是有点多，而且纯英文的难度看起来真的有点大。。。于是找到Columbia University，Michael Collins 的NLP课程，相比之下内容少了很多，而且老师讲的也很清楚～所以这部分以Michael Collins的NLP讲义为主。 context-free grammars上下文无关语法CFG可定义为4元祖 $G=(N,\\Sigma, R, S)$ (left-most) Derivations给定一个上下文无关语法，一个left-most derivation是一个序列 $s_1…s_n$,其中： $s_1 = S$, the start symbol $s_n\\in \\Sigma^*$, $\\Sigma^*$ 表示任何从 $\\Sigma$ 中得到的words组成 $s_i$, 表示找到 $s_{i-1}$ 中left-most的非终极符号X，并用 $\\beta$ 代替它，并且 $X\\rightarrow \\beta \\in R$ Probabilistic Context-Free Grammars (PCFGs)用 $T_G$ 表示对应与上下文无关语法G所有的left-most 推导的parse tree. 如果 $s\\in \\Sigma^* $, 那么 $T_G(s)$ 表示： $$t:t\\in T_G, yield(t)=s$$ $T_G(s)$ 表示在语法G下，s对应的所有parse tree的集合。 如果一个sentence是有歧义ambiguous的,那么 $|T_G(s)|\\ge 1$ 如果一个sentence是符合语法G的grammatical, 那么 $|T_G(s)|\\ge 0$ 给一个可能的推导，也就是parse tree一个概率,p(t),那么对于任意 $t\\in T_G$: $$p(t)\\ge 0$$ 并且： $$\\sum_{t\\in T_G}p(t)=1$$ 咋一看，这似乎很复杂。每一个parse tree就很复杂，然后这样的parse tree可能是infinite.那么如何定义概率分布p(t)呢？ 知道了p(t)，我们也就知道了一个给定的sentence对应的最有可能的parse tree.也就是： $$arg\\ max_{t\\in T_G(s)p(t)}$$ 有了这个我们就能很好的消除语言中存在的ambiguous了～～～ 那么问题来了： How do we define the function p(t)? How do we learn the parameters of our model of p(t) from training examples? For a given sentence s, how do we find the most likely tree, namely $$arg\\ max_{t\\in T_G(s)p(t)}?$$ definition of PCFGs一个PCFG包括： 上下文无关语法 $G=(N,\\Sigma,S,R)$ 参数 $$q(\\alpha \\rightarrow \\beta)$$ 那么对于 $t\\in T_G$包含rules $\\alpha_2\\rightarrow \\beta_1,\\alpha_2\\rightarrow \\beta_2,…,\\alpha_n\\rightarrow \\beta_n$，则有： $$p(t)=\\prod_{i=1}^nq(\\alpha_i\\rightarrow \\beta_i)$$ 那么对于所有的非终极符号 $X\\in N$: $$\\sum_{\\alpha\\rightarrow \\beta \\in R:\\alpha=X}q(\\alpha\\rightarrow \\beta)=1$$ 也就是从一个非终极符号展开的概率之和为1. 对于一个parse tree的概率，举个例子： 根据直觉，可以将parse tree的生成看做随机过程 stochastical process，其过程如下： Deriving a PCFG from a CorpusHaving defined PCFGs, the next question is the following: how do we derive a PCFG from a corpus? We will assume a set of training data, which is simply a set of parse trees $t_1,t_2,…,t_m$. $yield(t_i)$ is the i’th sentence in the corpus. Each parse tree t i is a sequence of context-free rules: we assume that every parse tree in our corpus has the same symbol, S, at its root. We can then define a PCFG (N, Σ, S, R, q) as follows: 有点疑惑： 一个语料库对应一个PCFG 一个语料库中所有的parse tree的start symbol是相同的S $t_1,t_2,…,t_m$ 具体有多少我们并不知道，也不需要知道吧。。但是有多少sentences是知道的。 比如figure5中显示的那样～ Chomsky Normal Form collins 教授举的例子： VP -&gt; VT NP PP 0.2 转换成 Chomsky Normal Form: VP -&gt; VT-NP 0.2 VP-NT -&gt; VT NP 1 总结下这个过程就是： Parsing using the CKY Algorithm在前面最小编辑距离，Viterbi，Froward算法中，都是先填写表格，表格中包含有所有的子问题。而在句法分析也是类似的，子问题是展示所有成分的parse tree，也可以展示在表格中，但它是三维的，因为不仅仅包含句子长度这一维度，还有对应的短语结构. 对于一个sentence $s=x_1\\cdots x_n$,其中 $x_i$ 表示句子中第i个词。也就是找出概率最大的parse tree $$arg\\ max_{t\\in T_G}p(t)$$ CKY算法是一个动态规划算法～ 具体思路： 给sentence标上index fill the table $(n+1)\\times (n+1)$， 每个 cell 对应一个三维的量 $\\pi[i,j,X]$,$X\\in N, 1\\le i\\le j\\le n$, 其中non-terminal总个数有V个，那么 table 的维度是 $(n+1)\\times (n+1)\\times V$ sentence其中的一部分 $x_i\\cdots x_j$，并且对应的root为非终极符号X. $$\\pi(i,j,X) = max_{t\\in T(i,j,X)}p(t)$$ 那么我们要求的sentence对应的parse tree： $$\\pi(1,n,S) = arg\\ max_{t\\in T_G(s)}$$ 用递归的方法填写table 这里是一种 bottom-up,从下到上的方法，只需要矩阵的上三角部分～ Initialization 初始值， 也就是主对角线的值 $$\\pi(i,i,X) =\\begin{cases} q(X\\rightarrow x_i), &amp; \\text{if $X \\rightarrow x_i \\in$ R} \\ 0, &amp; \\text{otherwise} \\end{cases} $$ 递归 recursive $$\\pi(i,j,X)=max_{X\\rightarrow Y Z \\in R,s\\in {i…(j-1)}}(q(X\\rightarrow Y Z)\\times \\pi(i,s,Y)\\times \\pi(s+1,j,Z))\\tag{1}$$ 伪代码： 填表的过程： The algorithm fills in the $\\pi$ values bottom-up: first the $\\pi(i, i, X)$ values, using the base case in the recursion; then the values for $\\pi(i, j, X)$ such that j = i + 1; then the values for $\\pi(i, j, X)$ such that j = i + 2; and so on. Justification for the Algorithm 考虑 $\\pi(3,8,VP)$ 在语法中 $VP\\rightarrow Y Z$ 只有两种情况 $VP \\rightarrow Vt\\ NP\\ and\\ VP\\rightarrow VP\\ PP$，那么 $\\pi(3,3)$ 这个cell有两个维度，同样的 $\\pi(3,4),…,\\pi(8,8)$ 都有两个维度. 总的计算复杂度是 $O(n^3|N|^3)$,怎么来的呢？ table中选择cell[i,j],复杂度 $n^2$ 对于每个cell[i,j]要考虑 N 个non-terminal,所以是 $n^2N$ 然后 $X\\rightarrow Y Z$ Y Z都有 N 种情况，所以 $n^2N^3$ 然后 $s\\in {i…(j-1)}$ 所以是 $O(n^3|N|^3)$ 整个回顾下，就是我们先从下到上 bottom-up 填写三维table[i,j,V] ，然后直接计算 $\\pi(1,n,S)$， 同样的类似于HMM中需要backpoints. weaknesses of PCFGs Lack of sensitivity to lexical information Lack of sensitivity to structural frequencies Lack of sensitivity to lexical information 在PCFGs中 $NNP\\rightarrow IBM$ 是一个独立的过程，并没有考虑到其他的words，这在NLP中显然是不合理的。 附着歧义(attachment ambiguity）: 可以看到对于同一句话的两种parse tree,它们的区别只有图中黑色加粗的rule，那么比较两个parse tree那个更好仅仅取决于 $q(NP\\rightarrow NP PP) q(VP\\rightarrow VP PP)$ 的大小。 边上的举例 dumped sacks into bin 没太懂，准确率是怎么来的。。。 Lack of sensitivity to structural frequencies close attachment: 通过两个例子，说明PCFGs的劣势，failed to capture the structural preferences. 两个parse tree的rules完全相同，那么对应的概率p(t)也一样。PCFGs failed to display a preference for one parse tree or the other and in particular it completely ignores the lexical information. 参考： Natural Language Processing，Columbia University，Michael Collins youtobe,Natural Language Processing Speech and language Processing","link":"/2018/04/20/chapter12-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"title":"chapter13 Lexicalized PCFG","text":"Lexicalized PCFGs Lexicalization of a treebank： 给treebank添加head Lexicalized probabilistic context-free Grammars： Lexicalized PCFGs中的参数，也就是rules的概率 Parsing with lexicalized PCFGs：使用动态规划CKY对测试集中的sentences寻找最优parse tree Parameter estimation in lexicalized probabilistic context-free grammars： 通过训练集，也就是语料库corpus得到Lexicalized PCFG的参数 Accuracy of lexicalized PCFGs：测试集的准确率计算 相关研究 Lexicalized PCFGs 词汇化PCFGsLexicalization of a treebank 给每个rules添加一个head，前面也介绍过～不过么看懂，这里又一次讲到了。 关于怎么找到这个head，一个rule中最中心的部分～a core idea in syntax the central sub-constituent of each rule the sementic predicate in each rule 在语料库中是没有标注出head的，那么需要一些规则来人为选定head。 以NP为例，right-most 以VP为例， left-most 添加head之后有啥用呢？ make the PCFG more sensitive to lexical information. propagate lexical items bottom-up throught these threes using head annotations where each non-terminal receives its head from its head child. Lexicalized probabilistic context-free Grammars Chomsky Normal Form 突然发现自己不看字母也能听懂 Michael Collins 的英语了～发音真的好听而且清晰！！还有自己截图右下角都有个预览的框框。。因为鼠标截图时不得不回碰到youtobe预览的进度条。。。 Lexicalized context-free grammars in chomsky normal form 跟regular CFG很相似，但多了head words Parameters in a Lexicalized PCFG 在PCFG中，是 $S\\rightarrow$ 在 Lexicalized PCFG中， 是 $S(saw)\\rightarrow$ 因此参数多了很多很多，比如 $S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$, $S(saw)\\rightarrow_2 NP(women)\\ VP(saw)$ 这都有对应的概率～we heve every possible lexicalized rule, 同时smoothing技术在这会直接用到～ Parsing with lexicalized PCFGs 对于一个rule grammar，其non-terminal展开是 $|\\Sigma|^2\\times |N|^3$ 对于一个长度为n的sentence，使用dynamic programming算法需要的计算复杂度为 $O(n^3|\\Sigma|^2|N|^3)$,但是 词典 $|\\Sigma|$ 可能会巨大！！！ 但可以这么简化，就是不用考虑整个词典 $|\\Sigma|$, 只需要考虑在sentence出现过的词，也就是 n 个。 那么总的计算复杂度为 $O(n^5|N|^3)$. 棒啊！～ Parameter estimation in lexicalized probabilistic context-free grammars 为什么 Lexicalized PCFGs 要好于 PCFG？ 使用prepositional phrase ambiguity举例说明: rules: 7个non-terminal: $$S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$$ $$NP(man)\\rightarrow_2 DT(the)\\ NN(man)$$ $$VP(saw)\\rightarrow_1 VP(saw)\\ PP(with)\\tag{~}$$ $$VP(saw)\\rightarrow_1 Vt(saw)\\ NP(dog)\\tag{~}$$ $$NP(dog)\\rightarrow_2 DT(the)\\ NN(dog)$$ $$PP(with)\\rightarrow_1 IN(with)\\ NP(telescope)$$ $$NP(telescope)\\rightarrow_2 DT(with)\\ NN(telescope)$$ 9个terminal： $$\\cdots$$ 7个non-terminal: $$S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$$ $$NP(man)\\rightarrow_2 DT(the)\\ NN(man)$$ $$VP(saw)\\rightarrow_1 VP(saw)\\ PP(dog)\\tag{~}$$ $$NP(dog)\\rightarrow_1 NP(dog)\\ PP(with)\\tag{~}$$ $$NP(dog)\\rightarrow_2 DT(the)\\ NN(dog)$$ $$PP(with)\\rightarrow_1 IN(with)\\ NP(telescope)$$ $$NP(telescope)\\rightarrow_2 DT(with)\\ NN(telescope)$$ 9个terminal： $$\\cdots$$ 两者的区别在于（～）的概率大小。如果没有lexicalized,parse tree的选择就仅仅只取决于短语结构，而完全忽视了语义的信息。 如何计算参数 $q(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))$ 是条件概率，可以看做是已知S,saw,从CFG语法中选出 $S\\rightarrow_2 NP\\ VP$, 并且从NP的 word 中选出 man 的概率. $$q(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))=q(S\\rightarrow_2 NP\\ VP|S,saw) \\times q(man|S\\rightarrow_2NP\\ VP,saw)$$ 用极大似然估计可得： 第一项： $q(S\\rightarrow_2 NP\\ VP|S,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP\\ VP)}{count(S(saw))}$ 第二项： $q(man|S\\rightarrow_2NP\\ VP,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))}{count(S(saw)\\rightarrow_2 NP\\ VP,saw)}$ 用线性插值法估计参数: $$\\lambda_1+\\lambda_2=1$$ $$q_{ML}(S\\rightarrow_2NP\\ VP|S,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP\\ VP)}{count(S(saw))}$$ $$q_{ML}(S\\rightarrow_2NP\\ VP|S) = \\dfrac{count(S\\rightarrow_2 NP\\ VP)}{count(S)}$$ 依然还有很多需要解决的问题： deal with rules with more child incorporate parts of speech(useful in smoothing) encode preferences for close attachment further reading: MIchael Collins.2003. Head-Driven Statistical Models for Natural Language Parsing. Accuracy of lexicalized PCFGsgold standard and parsing output: Result介绍了一些相关的研究～ reference: MIchael Collins, Natural Language Processing","link":"/2018/04/22/chapter13-Lexicalized-PCFG/"},{"title":"chapter2:正则表达式、文本标准化和编辑距离","text":"各种正则表达式，析取，组合和优先关系 文本标准化：各种预处理方法 编辑距离：动态规划 前言： 早期的自然语言处理工具ELIZA采用的方法是pattern matching. 而对于文本模式(text pattern)的描述，有一个很重要的工具：正则表达式(regular expression). 对文本处理任务的统称，就是文本标准化(text normalization)。其中有： tokenizing: 分词？ lemmatization: 词形还原。 确定表面不一样的词是否是同一个词根，针对时态比较复杂的语言非常必要。 stemming:词干提取。 针对前缀后缀，一种简单的lemmatization sentence segmantation: 句子分割. 用时态或者标点符号 最后提到编辑距离(edit distance):一种算法，在自然语言处理和语音识别中都很常用。 正则表达式 regular expression 用斜线(slashes)来分隔正则表达式，斜线不是正则表达式的一部分。 正则表达式的区分大小写的，可以用方括号(square braces)来解决这个问题。 对所有的数字(digits)可以用/[1234567890]/,但对于所有的字母这样就不太方便了，可用连字符(dash)(-)来表示一个范围(range). 脱字符(caret)(^) 用脱字符表示否定，或者仅仅表示它自身。放在方括号的第一个位置时才有效。 用问号?表示前一个字符是可选的。 问号除了表示可选外，还有贪婪和非贪婪的区别。/.* ?/ 和 /.* / 用 * 表示前一个字符的零个或多个；用 + 表示前一个字符的一个或多个 举个栗子： baa!(至少两个a) baaa! baaaaaa! 用 /baaa*/ 或 /baa+/ 可匹配上面这种形式。 单位数的价格可用 /[0-9]/，一个整数（字符串）的正则表达式：/[0-9][0-9]* / 或者 /[0-9]+/ 通配符(wildcard) /./ /./表示匹配任何字符，那么和星号一起使用，就可以表示任何字符串了。/.* / 锚号(anchor)是一种把正则表达式锚在字符串中的特定位置，最普通的锚号是脱字符^和美元符$. 脱字符 ^ 与行的开始相匹配。/^ The/ 表示单词只出现在一行的开始，这样脱字符就有三种用法了。 美元符 \\$ 表示一行的结尾./^ The dog\\.\\$/表示这一行只有The dog. 其中点号前面必须加反斜杠，因为我们要让它表示点号，而不是通配符。 还有两个锚号： \\b表示词界， \\B表示非词界 非数字、下划线或字母，可以看做词界。 Disjunction,Grouping, and Precedence 析取，组合和优先关系 析取算符(Disjunction operator)(|),正则表达式 /cat|dog/ 表示字符串是dog或者cat,对于后缀guppy和guppies,可以写作 /gupp(y|ies)/ 圆括号算符“()”.我们知道 * 只能表示前一个字符的重复，但如果要重复一个字符串呢，那就得用括号了。比如 /(column [0-9]+_*)*/ 就表示column后面跟一个数字和任意数目空格的重复～ 运算符的优先级 正式因为 * 的优先级高于序列，所以 /the* / 表示与theeeee匹配，而不是与thethe匹配。 还有正则表达式的匹配是贪心的(greedy).比如 /[a-z]* / 可以匹配零个或多个字母，所以结果是尽可能长的符号串。 怎么让它不贪心呢（non-greedy）？ 可以这么写 /[a-z]* ?/ 或 /[a-z]+?/会匹配尽可能少的符号串。 一个简单的栗子要用正则表达式找到the /the/ 并不能找到the位于句子开头的情况The /[tT]he/ 当the嵌入在其他单词之间时theology，也是不对的 /\\b[tT]he\\b/ 加入词界后也不包括the_或者the25了，但如果我们也想找到这种情况中的the呢？那就说明，在the两侧不能出现字母。 /[^a-zA-Z][tT]he[^a-zA-Z]/ 这样仍然有问题，这意味着前面必须有个非字母符。所以应该这样： /(^|[^a-zA-Z])[tT]he[^a-zA-Z]/ 更多算符 通用字符集的别名(aliases) 用于计数符的算符 需要加反斜杠的特殊算符 正则表达式中的替换(substitution)、存储器(capture group)和ELIZAs/regexp1/regexp2/ 表示用第二个正则表达式替换第一个的内容 s/colour/color/ s/([0-9]+)/&lt;\\1&gt;/ 其中 \\1表示参照第一个模式中的内容，也就是括号的内容，然后加上&lt;&gt;后对它进行替换。实际上就是找到这样的，加上&lt;&gt; /the (.* )er they were, the \\1er they will be/ 可以匹配 The bigger they were, the bigger they will be 但不能匹配 bigger they were, the faster they will be. 括号中用于存储的模式叫做 capture group，而用于存储的数字存储器叫做 寄存器(register). 这样一来圆括号就有了两种含义了，可以用来优先级的运算符，也可以用来capture group. 所以必须加以区别，用 ?: 来表示 non-capturing group. (?: pattern ) 举个栗子: /(?:some|a few) (people|cats) like some \\1/ 可以用来匹配 some cats like some people，而不能匹配 some people like some a few. 因为\\1 表示的是(people|cats)这个括号中的内容。 ELIZA： 这可真是“人工”智能啊。。。hahha Lookahead assertions最后，有时候我们需要预测未来look ahead：在文本中向前看，看看有些模式是否匹配，但不会推进匹配游标(match cursor)，以便我们可以处理模式。 不推进匹配游标是什么意思？ lookahead assertions 使用(?=pattern)和(?!pattern). The operator (?= pattern) is true if pattern occurs, but is zero-width. 负向预测： /(ˆ?!Volcano)[A-Za-z]+/ 表示 这个不太理解，到regex.com上试了下： Words and Corpora在我们对word进行处理时，我们需要确定怎么样才算一个word. 语料库： written texts from different genres (newspaper, fiction, non-fiction, academic, etc.), Brown University in 1963–64 (Kučera and Francis,1967). telephone conversations between strangers，(Godfrey et al., 1992). disfluencies, fragment, filled pauses举个栗子： I do uh main- mainly business data processing 对于语句中出现的不流利的地方 (disfluencies). main- 称为片段 (fragment), 像uh和um这样的称为 fillers or filled pauses 我们在处理文本的时候是否需要保留这些不流利的地方呢，这取决于我们的应用。 Disfluencies like uh or um are actually helpful in speech recognition in predicting the upcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech recognition they are treated as regular words. Because people use different disfluencies they can also be a cue to speaker identification. capitalized tokens or uncapitalized tokensthey 和 They 是否需要当做同一个单词处理。 我们知道在 part-of-speech or named-entity tagging 中首字母大写是很有用的特征，这需要保留下来。 lemma词意 and wordform 一句话中的WORD可以用两种不同的标准来区分。一种是Lemma，一种是wordform。 wordform就是词的形状，而lemma则是词意。比如 am is are ，都是一个lemma，但是3个wordform。在阿拉伯语中，需要将lemmatization，可能因为他们同一个词意，能用的词太多了吧，我记得看哪个视频的时候说过骆驼，有四十多种。。。对于英语的话，wordform就够了。 word type 词型 and word token 词例倘若以wordform 词形的形式来界定一个词，那么一句话中WORD的数目还可以用两种不同的标准来区分。Type是相同的词都算一个，Token是每个词出现几次都算。所以 “no no no …. it is not possible” 这样的一句话，Type 有5个，Token 有7个。Token包括重复词。 其中 Tokens N 和 types |V| 有这样的关系： $$|V|=kN^{\\beta}$$ $\\beta$ 取决于语料库的大小(size)和类型(genre).当语料库至少有上图中的大小时， $\\beta$ 的值的大小为0.67到0.75之间。 Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words. 另外一种是以lemmas来界定一个词，而不是wordform. 文本标准化 Text Normalization在进行自然语言处理之前，都需要对文本进行标准化处理。 Segmenting/tokenizing words from running text 分词 Normalizing word formats 单词格式归一化 Segmenting sentences in running text. 句子分割 Unix tools for crude tokenization and normalization介绍了一个Linux命令 tr 可用来统计词频 但这个统计非常简单粗暴，去掉了所有的标点符号和数字 Word Tokenization and Normalization介绍了标点符号在很多地方的用途: Ph.D,m.p.h… 时间(09/04/18)..等等 email, urls clitic contractions by apostrophes. 用’号表示的缩写 what’re,we’re 根据应用不同，tokenize也会不同，比如New York通常也会标记为一个词。在 name entity detection 中Tokenization会很有用。 tokenize standard: Penn Treebank tokenization standard 由Linguistic Data Consortium(LDC)发布。 case folding: everything is mapped to lower case. 在语音识别和信息检索中会比较常用。 但是在sentiment anal- ysis and other text classification tasks, information extraction, and machine transla- tion 中大小写是很有用的，因此通常不会使用case folding. 下一章中的有限状态自动机 finite state automata 就是用基于正则表达式判别算法编译而成的。 中文词分割：maximum matching/MaxMatch 最大匹配算法一种贪心算法，需要一个字典(dictionary/wordlist)进行匹配. 伪代码： 代码参考：http://www.cnblogs.com/by-dream/p/6429615.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;//宏，计算数组个数#define GET_ARRAY_LEN(array, len){len=sizeof(array)/sizeof(array[0]);}string dict[] = {&quot;计算&quot;,&quot;计算语言学&quot;,&quot;课程&quot;,&quot;有&quot;,&quot;意思&quot;};//是否为词表中的词或词表中的前缀bool inDict(string str){ bool res = false; int i; int len = 0; GET_ARRAY_LEN(dict, len); for (i=0; i&lt;len; i++){ if (str == dict[i].substr(0, str.length())) { res = true; } } return res;}int main(){ string sentence = &quot;计算语言学课程有意思&quot;; string word = &quot;-&quot;; int wordlen = word.length(); // 1 int i; string s1 = &quot;&quot;; for (i=0; (unsigned)i&lt;sentence.length(); i += wordlen) { string tmp = s1 + sentence.substr(i, wordlen); //每次增加一个词 if (inDict(tmp)) { s1 = s1 + sentence.substr(i, wordlen); } else // 如果不在词表中，先打印出之前的结果，然后从下一个词开始 { cout &lt;&lt; &quot;分词结果：&quot; &lt;&lt; s1 &lt;&lt; endl; s1 = sentence.substr(i, wordlen); } } cout &lt;&lt; &quot;分词结果：&quot; &lt;&lt; s1 &lt;&lt; endl;} 如果词表足够大的话，就可以对更多的句子进行分词了。 我们用一个指标来量化分词器的准确率，称为 word error rate. 怎么计算word error rate:通过计算最小编辑距离 We compare our output segmentation with a perfect hand-segmented (‘gold’) sentence, seeing how many words differ. The word error rate is then the normalized minimum edit distance in words between our output and the gold: the number of word insertions插入, deletions删除, and substitutions替换 divided by the length of the gold sentence in words. 作者还提到最准确的中文分词算法是通过监督学习训练的统计 sequence models, 在chapter 10中会讲到。 Lemmatization and Stemming 词形还原和词干提取Lemmatization： 词形还原，am, is，are有共同的词元(Lemma)：be 举例说明： He is reading detective stories. –&gt; He be read detective story. 那么lemmatization是怎么实现的呢？ The most sophisticated methods for lemmatization involve complete morphological parsing(形态解析) of the word. morphological parsing会在chapter3中讲到。 Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes(语素). 语素包括两类： stems：词干 affixes: 词缀 关于词形还原的工具：blog，词形还原工具对比 Python: NLTK Python: Pattern Python: TextBlob Tree Tagger The Porter Stemmer通常我们用finite-state transducers 来处理 morphological parser,但我们有时候也会使用简单粗暴的去掉词缀的方法 stemming. 这里作者就介绍了一种这样的算法 Poster algorithm. 算法的原理主要是基于一些规则 cascade. Sentence Segmentation 句子分割主要是用标点符号啦～ 比较unambiguous的标点符号有：Question marks and exclamation points 而Periods就比较ambiguous了。 具体的句子分割算法垢面chapter会讲到 Minimum Edit DIstance 最小编辑距离用来表示两个句子之间的相似性。 deletion 删除： cost 1 insertion 插入： cost 1 substitution 替换： cost 2 The Minimum Edit Distance Algorithm一种动态规划的算法。 dynamic programming,Bellman, R. (1957). Dynamic Programming. Princeton University Press. that apply a table-driven method to solve problems by combining solutions to sub-problems. source string X[1…i…n] target string Y[1…j…m] 用D(i,j)来定义X中前i个字符到Y中前j个字符的编辑距离，那么X到Y的编辑距离就是D(n,m) 计算D[i,j],也就是递推有三种方式： 定义cost: 初始情况： D(i,0) = i，也就是 source substring of length i but an empty target string D(o,j) = j，也就是 With a target substring of length j but an empty source 那么伪代码： 1234567891011121314151617181920212223242526272829303132333435# 创建矩阵[n+1,m+1]D = np.zeros(n+1, m+1)# 1. Initialization:D[0,0] = 0for each row i for i to n: D[i,0] = D[i-1] + 1for each column j from 1 to m: D[0,j] = D[0,i-1] + 1# 2. Recurrence:for each row i from 1 to n: for each column j from 1 to m: D[i,j] = min(D[i-1,j]+1, D[i-1,j]+1, D[i−1, j−1]+2)# 3. Termination:return D[n,m] 我们知道了最小编辑距离是多少，但是我们还想知道最小编辑距离对应的两个字符串对齐方式 alignment.据说alignment在语音识别和机器翻译中很有用～ 最小编辑距离和viterbi算法、前向算法很相似。 最小编辑距离：递推一步有三种选择方式，然后取最小值。每一步中三种方式的权重weight也是有意义的。 Viterbi算法：递推一步有N个路径，然后取max，可以看做最小编辑距离的拓展，权重在这里就是概率。 前向算法：递推每一步有N个路径，然后取sum. 其中最小编辑距离和Viterbi算法有 backtrace. 同样的，在前向递推的过程中填表： 填表的过程就是从D(0,0)开始，每进入一个 boldfaced cell(除了第0行和第0列)都有三种选择，然后选择最小的。 计算 alignment path，分为两步骤： 在算法计算的过程中，存储后指针backpointer backtrace：从最后一行最后一列的cell开始，沿着指针，每一步都是最小的。 总结： 介绍了各种正则表达式 用 - 表示range 脱字符 ^ 的三种用法：自身，方括号中的否定，与行开头匹配 问号 ? 表示前一个字符是可选的 表示前一个字符零个或多个， + 表示前一个字符一个或多个 . 表示通配符，匹配任意一个字符，/.* /匹配任意长度字符，且贪心的 锚号 ^ 和 \\$ 匹配行开头和结尾 锚号 \\b和\\B 词界和非词界 析取，组合和优先关系 主要是析取算符|和圆括号()的用法，以及运算符优先级 替换和寄存器 s/regexp1/regexp2/ \\1 基于正则表达式的分词和文本标准化 用于词干提取stemming的简单粗暴的算法 Porter algorithm 用于描述字符串相似度的算法，最小编辑距离 leetcode上有个编辑距离的题目：https://leetcode.com/problems/edit-distance/description/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class Solution {public: int minDistance(string word1, string word2) { int n = word1.length(); int m = word2.length(); int a[n+1][m+1]; a[0][0] = 0; for (int i=1; i&lt;=n; i++){ a[i][0] = a[i-1][0] + 1; } for (int j=1; j&lt;=m; j++){ a[0][j] = a[0][j-1] + 1; } for (int i=1; i&lt;=n; i++){ for (int j=1; j&lt;=m; j++){ if (word1[i-1] != word2[j-1]){ int tmp = min(a[i-1][j-1] + 1, a[i-1][j] + 1); a[i][j] = min(tmp, a[i][j-1] + 1); } else { a[i][j] = a[i-1][j-1]; } } } return a[n][m]; }};","link":"/2018/04/09/chapter2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E3%80%81%E6%96%87%E6%9C%AC%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"title":"chapter14 dependency Parsing1","text":"CS224d-Lecture6:Dependency Parsing Dependency ParsingUnlabled Dependency Parses root is a special root symbol Each dependency is a pair (h, m) where h is the index of a head word, m is the index of a modifier word. In the figures, we represent a dependency (h, m) by a directed edge from h to m Dependencies in the above example are (0, 2), (2, 1), (2, 4), and (4, 3). (We take 0 to be the root symbol.) Conditions on Dependency Structures 从root到任何一个word都有一条直接路径 no crossing (比如右下角的笔记就不行～) 对于 “John saw Mary” 有5中dependency parse 有crossing的结构叫 non-projective structure dependency parsing resource: Conll 2007 McDonald dependency banks 一个 treebank 通过 lexicalization 可以转换成 dependency bank.也就是一个lexicalizated PCFG可以转换为一个 dependency bank. efficiency of dependency parsing dynamic programming - Jason Eisner very efficiencyat Parsing very useful representations CS224什么是dependency structure describing the structure of a sentence by taking each word and saying what it’s a dependent on.So, if it’s a word that kind of modifies or is an argument of another word that you’re saying, it’s a dependent of that word. ambiguity: PP attachments attachment ambiguities:A key parsing decision is how we ‘attach’ vairous constituents PPs 介词短语, adverbial or participial phrase 副词和分词短语, infinitives 不定式, coordinations 并列关系 Catalan numbers: $C_n=(2n)!/[n+1]!n!$ ??需要在查资料!! 人工标注也就太太麻烦了～ dependency Grammar and Dependency structureuniversal dependency the arrow connects a head (governor, superior, regent) with a dependent (modifier, inferior, subordinate) usually, dependencies from a tree(connected, acyclic 非周期的, single-head) Question: compare CFGs and PCFGs and do they, dependency grammars look strongly lexicalized,they’re between words and does that makes it harder to generalize? dependency conditioning preferences关于如何确定dependency? 有以下几点perferences？ dependency parsing 这里需要注意一点：是否有交叉 crossing(non-projective). 在图中的例子中，是有crossing的。 但是dependency tree的定义是 no crossing。 methods of dependency Parsing 第4种方法是目前最主流的方法。 paper presentation: Improving distributional similarity with lessions learned from word embeddings, Omer Levy, Yoav Goldberg, Ido Dagan shift reduced parsing？？？ Greedy transition-based parsing The parser: a stack $\\sigma$， written with top to the right which starts with the ROOT symbol a buffer $beta$, written with the top to the left which starts with the input sentence s set of dependency arcs A which starts off empty a set of actions $Left-Arc_r$ $\\sigma|w_i|w_j,\\beta,A \\rightarrow \\sigma|w_j, \\beta, A\\bigcup{r(w_j,w_i)}$ 表示stack $\\sigma$ 中的两个元素 $w_i\\leftarrow w_j$ MaltParser Question: dependency parsing的准确率是否会出现waterfall般的下降，buz one decision will prevent sone other decisions. it’s not bad. dependency parse evaluation suffers much less badly from waterfall effects than CFG parsing when which is worse in that respect. feature representation不太懂这部分。。。 evaluation of dependency parsing why train a neural dependency parser? Indicator Features Revisited. our approach: learn a dense and compact feature representation. a neural dependency parser A Fast and Accurate Dependency Parser using Neural Networks 清华的本科生简直神一样的存在。。。 distributed representations Model Architecture Non-linearities between layers: Why they are needed Google announcement of Parsey McParseface, SyntaxNet","link":"/2018/04/23/chapter14-dependency-Parsing/"},{"title":"UCB-cs294-policy gradient","text":"Policy Gradient","link":"/2019/08/12/UCB-cs294-policy-gradient/"},{"title":"chapter11-上下文无关语法CFG","text":"上下文无关语法 CFG，又叫短语结构语法 英语语法中的各种规则： 句子级的结构，分4种 名词短语，中心词，以及围绕中心词的前后修饰语 动词短语 并列关系 Treebanks：经过剖析后的语料库，包含了句法syntatic信息和语义semantic信息 Treebanks as Grammars Heads and Head Finding 语法等价与范式 Lexicalized Grammars 词汇语法 前言： 上下文无关语法 context-free gramma. 上下文无关文法是许多自然语言句法形式化模型的支柱. Constituency 组成性何为组成性（constituency）：groups of words behaving as a single units, or constituents. 以名词短语(noun phrase)为例: 一个明显的证据是名词短语通常在动词 verb 之前。 Context-Free Grammars 上下文无关语法上下文无关语法 Context-Free Grammar, or CFG, 又叫短语结构语法 Phrase-Structure Grammars, 其形式化方法等价于 Backus-Naur Form, or BNF. 一个上下文无关语法，由规则 rules 或 产生式 productions ,以及单词和符号的一个词表 lexicon. 举例： 一个名词短语（NP or noun phrase）可以由一个专有名词(ProperNoun)组成或者是一个限定词(Det)后接一个名词性成分（Nominal）,而一个名词性成分又可以是一个或多个名词。 parse tree 剖析树 NP是树的根，也就是 CFG 定义的形式语言的初始符号 start symbol, 用 S 表示，通常可以解释为 “句子”，所以由 S 推导出来的符号串的集合就是句子的集合。 a, fight 这样的单词是终极符号 terminal symbol, 词表是引入终极符号的规则的集合 表示终极符号的聚类或概括的符号称为非终极符号 non-ternimal symbol,比如箭头左边的 NP.Det, Noun…. 用 “|” 表示非终极符号的展开方式： 语法规则，用 $L_0$ 表示～ 剖析树也可以用更为简洁的方式表示： Formal Definition of Context-Free Grammar 上下文无关语法的形式定义诸如 $L_0$ 的CFG定义了一个形式语言，在chapter2 中讲过，形式语言是符号串的集合。使用形式语言来模拟自然语言的语法称为 “生成语法（generative grammar）”. 一个上下文无关语法有四个参数（也称为四元组，4-tuple）： 非终极符号的集合 N 终极符号的集合 $\\Sigma$ 生成式的集合 R， 每个生成式的形式为 $A\\rightarrow \\beta$,其中 A 是非终极符号， $\\alpha$ 是由符号串的无限集 $(\\Sigma \\bigcup N)* $ 的符号构成的符号串 把单词的符号串映射到剖析树的问题称为 Syntactic parsing 句法分析. 在chapter12中会讲到～ Some Grammar Rules for English本书中关于英语短语结构只是点到为止，需要了解更多可以看 [Huddleston, R. and Pullum, G. K. (2002). The Cambridge Grammar of the English Language. Cambridge University Press.] Sentence-Level Constructions 句子级的结构除了上文中介绍的陈述句，还有4中比较常见和重要的结构：declaratives 陈述式结构, imperatives 命令式结构, yes-no questions yes-no疑问式结构, and wh-questions 疑问式结构. imperatives 命令式结构 动词短语开头，没有主语。 yes-no questions yes-no疑问式结构 助动词开头，后面跟一个主语NP，再跟一个VP. wh-questions 疑问式结构 比较复杂，有两种情况： wh主语疑问式（wh-subject-question）结构，主语疑问式结构与陈述句结构相同 wh非主语疑问式（wh-non-subject-question）结构，wh短语不是句子的主语 类似wh-non-subject-question 的结构我们称为 long-distance dependencies,因为 wh-NP 远离在语义上和它相关的谓词 have. The Noun Phrase 名词短语关于名词短语主要有： pronouns代词， proper nouns专有名词，和NP $\\rightarrow det\\ Nominal$. 接下来的部分主要介绍最后一种结构 NP $\\rightarrow det\\ Nominal$. 在名词短语中，包括一个中心词 head, 围绕中心词的有 前修饰语(prehead modifier) 和 后修饰语(post-head modifier). The Determiner 限定词，出现在中心词之前 The Nominal $$Nominal \\rightarrow Noun$$ Before the Head Noun: cardinal numbers 基数词, ordinal numbers 序数词, quantifiers 数量修饰语, and adjectives after the Head Noun: postmodifiers 主要分为三类： 介词短语 非限定从句 关系从句 介词短语 prepositional phrases 非限定从句 non-finite postmodifiers，有三种： 动名词gerundive (-ing), -ed, and infinitive forms. （1）动名词 gerundive (-ing) 列举一些例子： 其对应的形式语言： （2）infinitives不定式 and -ed形式 关系从句(postnominal relative clause), 准确的说是 限制性关系从句(restrictive relative clause), 通常用关系代词 relative pronoun 开头. before the Noun Phrase: 在NPs之前的词，通常叫 predeterminers, 最常见的就是 all. 复杂的名词短语的剖析树举例 The Verb Phrase 动词短语比较简单的动词短语的结构： 除此之外还有更复杂的，在动词后嵌入完整的句子,这样的成分叫做句子补语 sentential complements 在VP后潜在的成分可能是另一个VP。动词可以与不同类型的补语相容，但不是每个动词都与每个动词短语相容。比如 及物动词(transitive) 和 非及物动词(intransitive) 之分。 将动词 次范畴化(subcategorize), 也就是按照 NP或其他补语 再分类。动词的这些可能的补语的集合称为该动词的 次范畴化框架（subcategorize frame）。 Coordination 并列关系conjunctions: and, or and but. NP $\\rightarrow$ NP and NP Nominal $\\rightarrow$ Nominal and Nominal VP $\\rightarrow$ VP and VP S $\\rightarrow$ S and S Treebanks何为Treebanks: a corpus where every sentence in the collection is paired with a corresponding parse， Such a syntactically annotated corpus is called a treebank 一个语料库中所有的sentences都有其对应的剖析树。 Penn Treebank project (whose POS tagset we introduced in Chapter 10) has produced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal corpora of English, as well as treebanks in Arabic and Chinese. Penn Treebank 是一个treebank，其语料库来自于 Brown等。。 Treebanks as Grammars Viewed as a large grammar in this way, the Penn Treebank III Wall Street Journal corpus, which contains about 1 million words, also has about 1 million non-lexical rule tokens, consisting of about 17,500 distinct rule types. Heads and Head Finding每一个组成成分constituent，也就是树中的一个节点都有一个词汇头部 （lexical head）. 为什么需要head？ 暂时还不太懂，学过后面的内容应该就能理解了吧～ 举个栗子： 每个非终极non-terminal符号，也就是非叶节点都有一个head. 那么怎么找每个节点对应的head呢？ Instead of specifying head rules in the grammar itself, heads are identified dynamically in the context of trees for specific sentences. In other words, once a sentence is parsed, the resulting tree is walked to decorate each node with the appropriate head. 也就是说，没有具体的规则，根据具体的句子动态的定义对应的head，这些规则也都是 hand-written rules. 以NP中的head为例： Grammar Equivalence and Normal Form 语法等价与范式 强等价 strong equivalence 和 弱等价 weak equivalence 如果两个语法生成相同的符号串集合，而且对每个句子都指派相同的短语结构(只改变终极符号)，这样的两个语法是强等价 如果生成的符号串集合相同，但是不给每个句子指派相同的短语结构，就是弱等价，其实就是对同一个句子，理解不同～ 举个栗子： 一个上下文无关语法CFG是自由的，并且它的每个生成式的形式都是 $A\\rightarrow B C$ 或 $A \\rightarrow a$, 也就是说，每个规则 rule的右边要么是两个非终极符号，要么是一个终极符号，那么这个CFG就是 Chomsky normal form. 任何一个CFG都可以写成弱等价的Chomsky范式语法。 举个栗子： VBD是动词的过去式， Lexicalized Grammars 词汇语法可以看到，CFG过分强调短语结构，而忽视了词汇的作用，也就是单词的语义。但这样的短语结构都太复杂和笨重(cumbersome)了，而且 语法冗余，难以管理和脆弱（redundant,hard to manage, and brittle）。为了解决这样的问题，需要更好的利用lexicon。 接下来介绍其中的一种方法： **Combinatory Categorial Grammar，CCG ***,考虑到 句法 syntactic 和 语义 semantic 的高度 词汇化lexicalized 的方法. 下一章会详细讲到词汇化～ 总结 参考： Speech and language Processing，Chapter11 Natural Language Processing, Michael Collins, Columbia University","link":"/2018/04/19/chapter11-%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95/"},{"title":"chapter27-Question Answering","text":"Speech and language Processing, chapter27:Question Answering Question Answering System什么是 Question Answering?主要分为两类： 多文档的智能问答系统 针对一系列文档提出的问题 答案可能出现多次，也可能没有出现 主要应用在于互联网搜索引擎，文本资料库的搜索，比如新闻档案、医学文献、科学文章等 单个文档的智能问答 Question Type Simple (factoid) questions (most commercial systems): 简单的问题，可以用简单的事实回答，答案简短通常是一个 named entity Who wrote the Declaration of Independence? What is the average age of the onset of autism? Where is Apple Computer based? Complex (narrative) questions: 稍微复杂的叙述问题，答案略长 What do scholars think about Jefferson’s position on dealing with pirates? What is a Hajj? In children with an acute febrile illness, what is the efficacy of single medication therapy with acetaminophen or ibuprofen in reducing fever? Complex (opinion) questions: 复杂的问题，通常是关于观点／意见 Was the Gore/Bush election fair? 问答系统分类现代智能问答系统的主要有两个范式(two major modern paradigms of question answering)以及混合方法， 都是关注于 **事实性的回答(factoid question)**。: IR-based question answering 基于信息检索的智能问答 knowledge-based question answering 基于知识库的智能问答 Hybrid approaches (IBM Watson) 对比下三者，具体了解之后可以再会过头来看看： IR-based Factoid question answeringIR-based factoid AQ 的流程图，包括三个阶段：问题处理(question processing), 篇章检索(passage retrieval and ranking), 和 答案处理(answer processing). Question Processing Answer Type Detection: 分析 question，决定 answer type Query Formulation: 形成合适的查询语句进行检索 Passagge Retrieval 通过检索得到 top N documents 把 documents 拆分称合适的单位(unit/passage) Answer Processing 得到候选的 answer 进行排序，选出最佳 answer Question Processing两个任务： answer type detection what kind of entity (person, place) is the answer? 形成合适的 query what is the query to the IR system Answer type: the kind of entity the answer consists of (person, location, time, etc.) Query: the keywords that should be used for the IR system to use in searching for documents Focus: the string of words in the question that are likely to be replaced by the answer in any answer string found 在这个阶段需要做的事： 举个栗子： Answering Type Detection (Question classification)通常而言，我们可以把它当作一个机器学习的分类问题。 定义类别 注释训练数据，给数据打上分类标签 训练分类器，所用特征可以包括 hand-written rules. 定义分类类别前人已经提供了一些 answer type 的层次型分类结构，如 answer type Taxonomy(from Li &amp; Roth). Two-layered Taxonomy 6 coarse classes:（coarse-grained tag） ABBEVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC_VALUE 50 fine classes:（fine-grained tags） HUMAN: group, individual, title, description ENTITY: animal, body, color, currency… LOCATION: city, country, mountain… 提取特征将 answer type 看作一个监督学习任务。question中可以用来分类的特征： words in the questions, the part-of-speech of each word named entities in the questions answer type word or question headword: 通常 wh-word 之后的第一个 NP 可以用来作为特征 semantic information about the words，WordNet synset ID 分类方法总结下可以用来分类的方法： hand-written rules machine learning hybrids 其中 rules 包括： regular expression-baesd rules $\\text{who {is | was | are | were} PERSON}$ PERSON（YEAR-YEAR） Other rules use the question headword headword of first noun phrase after wh‐word 当然也可以把上述某些分类方法当作特征一起进行训练。就分类效果而言，PERSON, LOCATION, TIME 这类的问题类型有更高的准确率，REASON，DESCRIPTION 这类的问题更难识别。 Query Formulation根据 question 产生一个 keyword list，作为 IR 系统的输入 query。可能的流程是去除 stopwords，丢掉 question word(where, when, etc.)，找 noun phrases，根据 tfidf 判断 keywords 的去留等等。 如果 keywords 太少，还可以通过 query expansion 来增加 query terms。 keyword selection algorithm: select all non-stop words in quotations select all NNP words in recognized named entities select all complex nominals with theor adjectival modifiers select all other complex nominals select all nouns with their adjetival modifiers select all other nouns select all verbs select all adverbs select the QFW word(skipped in all previous steps) select all other words 最终得到的 query: Passage Retrieval有了 query，我们进行检索，会得到 top N 的文档，然而文档并不是得到 answer 的最好的单位，下一步我们需要从文档抽取 potential answer passages，来方便后面的 answer processing。passage 可以是 sections, paragraphs, sentence，具体情况具体分析。 1.rank the documents by relevance 2.extract a set of potential answer passages from the retrieved set of documents. the passage could be sections, paragraphs, sentences. 3.passage retrieval: run a named entity or answer type classification on the retrieved passages. 然后对剩下来的 passages 进行排序，通过监督学习进行分类，有以下一些列特征： 对于基于 web 的 QA 系统，我们可以依靠网页搜索来做 passage extraction, 简单的说，可以把网页搜索产生的 snippets 作为 passages。 总结下 passage retrieval: Answer Processing现在，我们已经有了 answer type，也选出了相关的 passages，就可以进一步缩小 candidate answer 的范围。 answer-extraction task 主要有两种方法 answer-type pattern extraction基于正则化匹配的机制。如果 answer types 是 HUMAN 或者 DISTANCE-QUANTITY. 可以直接通过模式匹配得到： 有时候光用 pattern-extraction 方法是不够的，一方面我们不能创造规则，另一方面 passage 里也可能有多个 potential answer。另外，对于没有特定定命名实体类型的答案，我们可以使用正则表达式(人工编写或自动学习)。 真“人工”智能！！！ 前两天北理工交流会，有个自动化所的博士讲的论文就是用神经网络来生成很自然的回答，赞！ N-gram tiling/redundancy-based approachN-gram tiling 又被称为 redundancy-based approach(Brill et al. 2002, Lin 2007)，基于网页搜索产生的 snippet，进行 ngram 的挖掘，具体步骤如下： N-gram mining 提取每个片段中的 unigram, bigram, and trigram, 并赋予权重 N-gram filtering 根据 ngram 和预测的 answer type 间的匹配程度给 ngram 计算分数 N-gram tiling 将重叠的 ngram 连接成更长的答案 **standard greedy method** &gt; 1. start with the highest-scoring candidate and try to tile each other candidate with this candidate 2. add the best-scoring concatenation to the set of candidates 3. remove the lower-scoring candidate 4. continue until a single answer is built 其中怎么求这个 weigth， 也就是 ngram 和 answer type 的匹配度： Ranking Candidate AnswersIn other cases we use machine learning to combine many rich features about which phrase is the answer 可能用到的 feature: 总结 IR-based question answering factoid question answering answer type detection query formulation passage retrieval passage ranking answer extration web-based factoid question answering Knowledge-based Question Answering当大量的信息以结构化的形式存储时， 通过语义分析(semantic parsers) 将 query 映射成一个 logical form. 未完待续。。 reference: Question Answering 28.Question Answering NLP 笔记 - Question Answering System","link":"/2018/06/27/chapter27-Question-Answering/"},{"title":"chapter3-有限状态自动机","text":"有限状态机 FSA 确定性的识别器 DFSA 确定性的识别器 NFSA：深度优先搜索 or 广度优先搜索 形式语言 有限状态自动机 finite-state automation, FSA前面一章节中的正则表达式只是一种用于文本搜索的方便的元语言，它是描述有限状态机的一种方法。 任何正则表达式又可以用有限状态机来实现（除了使用存储特性的那些正则表达式）。比如上一章中用于描述羊的语言的正则表达式： /baa+!/ 就可以表示为这样的有限自动机： FSA可以用来识别（或接受）符号串。接受方式如下：把输入想象成一个长长的带子(tape),带子上的一个单元格(cell)可以写一个符号： 怎么理解这个输入带子和自动机呢？就是当输入带上的字母和自动机中离开当前状态的弧相匹配时，就穿过这个弧，进入到自动机中的下一个状态和带子上的下一个符号。比如上图中初始状态 $q_0$ 只有遇到b时才会匹配成功而进入下一个状态和输入符号，所以上图中的自动机不能接受输入。对图2.10中 $q_3$ 状态，他可以匹配 a或!. 我们可以用状态转移表(state-transition table)来表示自动机。状态转移表可以表示初始状态、接收状态和符号在状态之间的转移情况。 一个有限自动机可以用下面5个参数来定义： 其中Q表示N种状态的集合，$\\Sigma$ 表示有限的输入符号字母表， $q_0$是初始状态， $F\\in Q$ 是终极状态。 $\\delta(q,i)$ 是转移矩阵，其中 $q\\in Q,i\\in \\Sigma$. $\\delta(q,i)$ 返回一个新的状态，$q’\\in Q$，则 $\\delta(q,i)$ 是从 $Q \\times \\Sigma$ 到 $Q$ 的一个关系. 在HMM中，状态转移矩阵是N*N的，跟这里有区别。 DFSA可以用状态转移表来识别字符串，就是把一个字符串输入进去，然后用自动机来判别是accept还是reject。这种算法是“确定性的是识别器”，deterministic算法，简称 D-RECOGNIZE. 伪代码： 在图2.13中，状态之间的转移矩阵是 transition-table[current-state, tape[index]]. current-state表示行，tape[index]表示列。其中index = index + 1. 如果将状态转移矩阵中的空状态称为失败状态(fail state)或者吸收状态(sink state).那么对任何状态和任何输入，自动机都能找到可以转移的地方。 NFSA存在某个状态，难以判断接下来怎么走的自动机称为非确定的FSA(或NFSA).如下图： 还有一种NFSA,用到了 $\\epsilon-$ 转移的新弧。如果到达了状态3，那么可以进入带有标记!的弧，也可以不看带子上的符号，直接转移到状态2. 那么其对应的状态转移表如下图： 用NFSA来接受符号串: 有三种方法： 回退 前瞻 并行 这里我们采用回退法： 代码阅读： function ND-RECOGNIZE(tape, machine) return accept or reject 创建进程表(agenda): 其中的元素是current-search-state, 是由自动机的一个结点（状态）和带子上的一个位置组合而成的。 agenda = {(Initial state of machine, beginning of tape)} current-search-state = NEXT(agenda) 主回路：用来确定输入带子上的全部内容是否都被自动机全部识别了，这可以用ACCEPT-STATE?函数实现，如果当前搜索状态既包括一个接受的机器状态,也包含一个子结尾的指针，那么就返回accept.这一步类似于 D-RECOGNIZE. 如果不行，那就调用 GENERATE-NEW-STATE 来生成一系列可能的下一个状态。 1234567891011121314151617if ACCEPT-STATE?(current-search-state) return ture : return ACCEPTelse agenda = agenda and GENERATE-NEW-STATE(current-search-state) //这一步不太理解？？？if agenda is empty: return rejectelse current-search-state = NEXT(agenda) NEXT函数从进程表返回一个为探测过的状态。那么怎么定义函数NEXT，这是一个值得研究的问题。 进程表用stack实现，那就是深度优先搜索(depth-first search)或后进先出(Last In First Out, LIFO). 进程表用queue实现，那就是广度优先搜索(breadth-first search)或先进先出(First In First Out, FIFO). ACCEPT-STATE? 函数用来确定输入带子上的全部内容是否都被自动机成功的识别了。 123456789101112131415function ACCEPT-STATE? (search-state) returns ture of false current-node = the node search-state is in index = the point on the tape search-state is looking at if index is at the end of the tape and current-node is an accept state of machine: return ture else return false GENERATE-NEW-STATE函数用与选取一种为访问过的路径 需要理解的是：在NEXT(agenda)中,agenda采用的是stack或queue, 那么意味着在只有一种选择的结点处，走过就删掉了，只有有两种选择的地方，另外一种会储存在agenda中。所以current-search-state = NEXT(agenda)生成的是有两种选择处的未访问过的路径。那么GENERATE-NEW-STATE函数就是生成这样的一个路径，然后在ACCEPT-STATE?中判断。 识别就是搜索： 把符号串输入当做输入带，然后用自动机来识别。其实也可以看作是搜索的过程。 深度优先搜索： 广度优先搜索： 形式语言形式语言是早期处理自然语言处理技术中，当时几乎是唯一的方法，可以用于描述自然语言的语法规律，能最大限度的逼近自然语言，并且很容易生成语言内容。 形式语言和自动机之间存在的对应关系，使其天生就容易被计算机处理。 虽然现在自然语言处理主要是基于统计模型来处理的，但形式语言仍然会出现在很多论文中。 图论 无向图 G = (V, E), V 表示定点， E表示无向边。 有向图 G = (V, E), V 是定点，E 是有向边 连通图和回路 文法，形式语法在用计算机系统,来判断一个句子是否是某语言的合法句子时，从句子和语言的结构特征上着手是非常重要的。一般可以通过这个句子是否能有给定语言对应的文法产生来作出判断，如果能，他就是合法的，否则，他就是非法的。对一类语言，可以在字母表上按照一定的规则，根据语言的结构特点，定义一个文法。用文法作为相应语言的有穷描述不仅可以描述出语言的结构特性，而且还可以产生出这个语言的所有句子。","link":"/2018/04/11/chapter3-%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"title":"chapter4:语言模型和N元语法","text":"N-gram: Markov chain是其中一个特例 验证N-gram模型： 困惑度 预处理：泛化和zeros情况 smoothing：用来处理zeros～ 困惑度和交叉熵 前言: 通过实例，简单的介绍了assign probabilities to sequences of words 的重要性，以及在这些方面的应用： speech recognition, handwriting recognition, spelling correction, augmentative communication. 给句子或是词语赋予概率的模型就是语言模型(language models,LM)，这个chapter主要介绍一个简单的语言模型N-grams. 2-gram(bigram), 3-gram(trigram). N-grams不论是整个句子的概率还是一个序列中预测下一个单词的概率，都要使用概率模型。 计算一个完整的句子的概率 $P(w_1,w_2,…,w_n)?$ ，我们用 $w_1…w_n 或 w_1^n$ 来表示N个单词组成的句子。可使用链式法则计算概率： $$P(w_1…w_n) = P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)…P(w_n|w_1^{n-1})$$ $$P(w_1…w_n) = \\prod_{k=1}^nP(w_k|w_1^{k-1})$$ 但是 $P(w_n|w_1^{n-1})$ 很难计算,因为： 这样我们引入了N-gram模型，就是考虑预测单词之前的少数单词，而不是之前的所有词。比如 bigram 就是2-gram模型，生成下一个单词只依赖于前一个单词： $$P(w_n|w_1^{n-1}) = P(w_n|w_{n-1})$$ 也就是 Markov假设～针对语言来说可能不太合理吧，毕竟语言一两个词包含的信息并不多，但对于很多其他的，比如天气，只依赖于前两天是很靠谱的对吧～ 那么怎么估计bigram和N-gram的概率，一个直接的方法是 极大似然估计(maximum likelihood estimation, MLE). $$P(w_n|w_{n-1}) = \\dfrac{C(w_{n-1}w_n)}{C(w_{n-1})}$$ 其中我们需要对每个句子加上特殊的begin-symbol&lt; s &gt; 和 end-symbol &lt; /s &gt;. 对于一般的N元语法，其参数估计为： $$P(w_n|w_{n-N+1}^{n-1}) = \\dfrac{C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}$$ 这里前面N-1个单词同时出现的频度来除整个序列出现的频度，这个比值称为相对频度(relative frequency).在最大似然估计中，相对频度是概率估计的一种方法，即在给定模型M的情况下，计算得到的模型参数能使得训练集T的似然度P(T|M)最大。 Some pratical issues: 通常使用 trigran，4-gram,甚至是5-gram效果要比bigram更好。但这需要更大的语料库。 由于相对概率都是大于0小于1的,概率相乘之后会更小，可能会导致数值下溢(numerical underflow)。因此采用对数概率(log probabilities). Evaluating Language Models外在评估, extrinsic evaluation . end-to-end evaluation， 应用到实际，然后去评估模型的好坏，但这太麻烦了。 内在评估, metric: intrinsic evaluation 内在评估: : training set or training corpus. development test set or devset: fresh test set often: 80% training, 10% development, 10% test. Perplexity 这里简单的提了一下perplexity(困惑度)：perplexity is a normalized version of the probability of the test set，可以理解为 weight average barnching factor. barnching factor是指预测下一个词出现的概率。 在后面结合信息熵还会再次讲到。这里可以先简单的理解为： 测试集的概率 $P(w_1w_2…w_N)$ 越大，也就是准确率越高，其困惑度就越小。 同时也可以看到，N-gram中N越大，模型提取的信息越多，其困惑度就越小。 说到这儿，N元语法到底从句子中提取到了什么信息？ 总结下来，还是一个概率性的问题。什么词更容易出现在什么词身后，eat后面更容易出现名词和形容词，这是句法(syntactic)信息;在对话中，I 更容易出现在句首； 还有文化相关的，人们looking for Chinses food的概率比english food大。 Generalization and Zeros The N-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a given training corpus. Another implication is that N-grams do a better and better job of modeling the training corpus as we increase the value of N. the value of N 随着N的增加，外在评估效果也越来越好。但在4-gram中 it cannot be but so.这个是直接从 king henry 中得到的，原因是因为在莎士比亚文集中，it cannot be but下面可持续的单词只有5个(that, I, he, thou, so). 所以，语料库相对4-gram太小了。 在回顾以下整个过程： 在training data中，对语料库中的句子加上&lt;\\s&gt;（不需要加 &lt; s &gt;）,这里并没有训练，只是计算了对应的unigram, bigram, trigram的概率. 然后在test data上进行验证，来判断哪个模型好，那个坏？这是内在评估 然后外在评估就是每一次random一个词，在这个基础上根据概率random下一个词，直到生成&lt;\\s&gt;. the corpus不同的语料库训练出来的模型，生成得到的序列会相差很远。怎么解决这个问题呢？ How should we deal with this problem when we build N-gram models? One way is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish. To build a language model for translating legal documents, we need a training corpus of legal documents. To build a language model for a question-answering system, we need a training corpus of questions. zeros 这里是个大坑测试集中出现了训练集中没有出现的词，或者二元组，这里不是,可能offer在其他地方出现过，但在denied the后面出现的次数为0，那么它的概率就是0，然后困惑度perplexity就无穷大了，这显然是不合理的。如下图中所示。 这是个很重要的问题！后面smoothing算法都是在解决这个问题！ Unknow Wordsclosed vocabulary:测试集中的词也都是来自于词库的(lexicon),比如语音识别和机器翻译，并不会出现unknown词。 但我们也会需要处理一些我们没有见过的词 **unknow or out of vocabulary(OOV)**， 我们通过给 open vocabulary 增加一个 pseudo-word . 解决这个问题有两种常见的方法： 第一种就是把问题转换为 closed vocabulary，也就是提前建立一一个词汇表 话说我不是太理解这个方法，应该是只出现在test data中吧，那么在train data中的概率就是0啊，那么得到的模型的概率也是0啊，那么在test时，首先它肯定不会生成,而且碰到时，下一个词怎么生成。。。。 第二种方法还好理解一点，就是在train data中把出现字数较少的词当做，那么这个时候就像普通的词一样也是有它的概率的。但有一个缺点： it takes some time to be convinced. Smoothing 平滑主要是针对前面那个坑的，就是zeros的情况。不是，但是这个二元组或者是三元组在test data中出现了，在train data中从来没有出现过。 解决办法就是：shave off a bit of probability mass from some more frequent events and give it to the events we’ve never seen. This modification is called smoothing or discounting. 有以下这些方法：**add-1 smoothing, add-k smoothing, Stupid backoff, and Kneser-Ney smoothing.** Laplace Smoothing 拉普拉斯平滑事实上，关于前面一部分极大似然估计和laplace smoothing这一段没看太懂。但是拉普拉斯平滑整体还是能够理解的。 对于bigram: 下图分别是伯克利餐馆对话的语料库(V=1446)以及加1平滑后的bigram： 可以看到，分子是所有的零和非零项都加1，所以对于分母C(w_{n-1})来说，也就是上面二维数组中某一行都加1，也就是增加了一个词表的数量V。理论上来讲是可以的，但总感觉有点问题是吧？ $$P(want|I) = \\dfrac{C(want|I)}{C(I)}\\le \\dfrac{C(want|I)+1}{C(I)+V}$$ 显然这个数变小了！因为未出现过的trigram占了一定的概率空间，除此之外，p(to|i),p(chinese|i)…这些概率都认为相等也是否合理呢？ 这个博客blog中提到了我的疑问： 如此一来，训练语料中未出现的n-Gram的概率不再为 0，而是一个大于 0 的较小的概率值。Add-one 平滑算法确实解决了我们的问题，但显然它也并不完美。由于训练语料中未出现n-Gram数量太多，平滑后，所有未出现的n-Gram占据了整个概率分布中的一个很大的比例。因此，在NLP中，Add-one给训练语料中没有出现过的 n-Gram 分配了太多的概率空间。此外，认为所有未出现的n-Gram概率相等是否合理其实也值得商榷。而且，对于出现在训练语料中的那些n-Gram，都增加同样的频度值，这是否欠妥，我们并不能给出一个明确的答案。 Add-k smoothing$$P_{add-k}(w_i|w_{i-n+1}\\cdots w_{i-1}) = \\frac{C(w_{i-n+1}\\cdots w_i)+k}{C(w_{i-n+1}\\cdots w_{i-1})+k|V|}$$ 通常，add-k算法的效果会比Add-one好，但是显然它不能完全解决问题。至少在实践中，k 必须人为给定，而这个值到底该取多少却莫衷一是。 Backoff and Interpolation 回退和插值回退通常我们会认为高阶模型更加可靠，前面的例子也表明，当能够获知更多历史信息时，其实就获得了当前推测的更多约束，这样就更容易得出正确的结论。所以在高阶模型可靠时，尽可能的使用高阶模型。但是有时候高级模型的计数结果可能为0，这时我们就转而使用低阶模型来避免稀疏数据的问题。 回退的三元语法： $$\\hat P(w_i|w_{i-2}w_{i-1}) =\\begin{cases}P(w_i|w_{i-2}w_{i-1})&amp;,if\\quad C(w_{i-2}w_{i-1})&gt;0\\ \\alpha_1P(w_i|w_{i-1})&amp;,if\\quad C(w_{i-1})&gt;0\\ \\alpha_2P(w_i)&amp;, otherwise \\end{cases}$$ 一般性的回退模型,又叫 Katz backoff ： $$P_BO(w_n|w_{n-N+1}^{n-1}) =\\begin{cases} \\tilde P(w_n|w_{n-N+1}^{n-1})&amp;, if\\quad C(w_{n-N+1 \\cdots w_{n-1} }&gt;0)\\ \\theta(P(w_n|w_{n-N+1}^{n-1}))\\alpha P_BO(w_n|w_{n-N+2}^{n-1})&amp;,otherwise\\end{cases}$$ 其中： $$\\theta(x) =\\begin{cases}1&amp;,if\\quad x=0\\ 0&amp;,otherwise\\end{cases}$$ 试想一下：为什么这里要用 $\\alpha$ ,没有会怎么样？ 如果没有 $\\alpha$ 值，等式就不是一个概率了，因为三元语法是根据相对频度来计算的，原本为0的概率，回退到一个低阶模型后，相当于把多余的概率量加到等式中来，这样一来，单词的总概率就将大于1. 因此，所有的回退模型都要打折（discounting）. 其中 $\\tilde P$ 用于给最大似然估计MLE的概率打折,也就是直接从计数计算出来的旧的相对频度节省概率量。$\\alpha$ 用于保证低阶N元语法概率量之和恰好等于前面 $\\tilde p$ 省下来的概率量。 插值插值和回退的思想其实非常相像。设想对于一个trigram的模型，我们要统计语料库中 “like chinese food” 出现的次数，结果发现它没出现过，则计数为0。在回退策略中，将会试着用低阶gram来进行替代，也就是用 “chinese food” 出现的次数来替代。 在使用插值算法时，我们把不同阶别的n-Gram模型线形加权组合后再来使用。简单线性插值（Simple Linear Interpolation） 可以用下面的公式来定义： $$\\hat P(w_n|w_{n-1}w_{n-1})=\\lambda_1P(w_n|w_{n-2}w_{n-1})+\\lambda_2P(w_n|w_{n-1})+\\lambda_3P(w_n)$$ 其中: $\\sum_i\\lambda_i=1$ $\\lambda_i$ 可以根据试验凭经验设定，也可以通过应用某些算法确定，例如EM算法。 在简单单线形插值法中，权值 $\\lambda_i$ 是常量。显然，它的问题在于不管高阶模型的估计是否可靠（毕竟有些时候高阶的Gram计数可能并无为 0），低阶模型均以同样的权重被加入模型，这并不合理。一个可以想到的解决办法是让 λi 成为历史的函数。也就是 条件插值（conditional interpolation） 则有: $$\\hat P(w_n|w_{n-1}w_{n-1})=\\lambda_1(w_{n-2}w_{n-1})P(w_n|w_{n-2}w_{n-1})+\\lambda_2(w_{n-2}w_{n-1})P(w_n|w_{n-1})+\\lambda_3(w_{n-2}w_{n-1})P(w_n)$$ 可使用EM算法来训练 $\\lambda$ 的值，使得从主语料库中分出来的语料库的似然度最大。具体怎么训练的。。看文献吧，(Jelinek and Mercer, 1980). absolute discounting想想之前的Add-one，以及Add-k算法。我们的策略，本质上说其实是将一些频繁出现的 N-Gram 的概率匀出了一部分，分给那些没有出现的 N-Gram 上。因为所有可能性的概率之和等于1，所以我们只能在各种可能的情况之间相互腾挪这些概率。 既然我们打算把经常出现的一些N-Gram的概率分一些出来，其实也就等同于将它们出现的次数减去（discount）一部分，那到底该discount多少呢？Church &amp; Gale (1991) 设计了一种非常巧妙的方法。首先他们在一个 留存语料库（held-out corpus） 考察那些在训练集中出现了4次的bigrams出现的次数。具体来说，他们首先在一个有2200万词的留存语料库中检索出所有出现了4次的bigrams （例如： “chinese food”，“good boy”，“want to”等），然后再从一个同样有2200万词的训练集中，分别统计这些bigrams出现的次数（例如：C(“chinese food”)=4，C(“good boy”)=3，C(“want to”)=3）。最终，平均下来，他们发现：在第一个2200万词的语料中出现4次的bigrams，在第二个2200万词的语料中出现了3.23次。下面这张表给出了 c 从0到9取值时（也就是出现了 c 次），统计的bigrams在留存集和训练集中出现的次数。 其实你应该已经发现其中的规律了。除了计数为0和为1的bigram之外，held-out set中的平均计数值，都大约相当于training set中的计数值减去0.75。 基于上面这个实验结果所诱发的直觉，Absolute discounting 会从每一个计数中减去一个（绝对的）数值 d。这样做的道理就在于，对于出现次数比较多的计数我们其实已经得到了一个相对比较好的估计，那么当我们从这个计数值中减去一个较小的数值 d 后应该影响不大。上面的实验结果暗示在实践中，我们通常会对从2到9的计数进行处理。 $$P_{AbsDiscount}(w_i|w_{i-1})=\\frac{C(w_{i-1}w_i)-d}{C(w_{i-1})}+\\lambda(w_{i-1})P(w_i)$$ 好好理解下： 就是通过 held out set 的对比得到的直觉后，我们在train data中，将所有的 $C(w_{i-1}w_i)$ 减去一个数d，分母不变，概率肯定变小了，但只是减去一个很小的数，影响并不大。然后加上 $\\lambda(w_{i-1})P(w_i)$，这样一来，原本bigram中为0的概率就不是0了，因为unigram肯定不为0嘛～但后面这一项怎么求，是接下来一部分的内容 从上一篇文章中，我们已经知道，对于bigram model而言，$P(w_i|w_{i−1})=C(w_{i−1}w_i)/C(w_{i−1})$，所以上述方程等号右侧第一项即表示经过 discounting 调整的概率值，而第二项则相对于一个带权重 λ 的 unigram 的插值项。通常，你可以把 d 值就设为 0.75，或者你也可以为计数为 1 的 bigram 设立一个单独的等于 0.5 的 d 值（这个经验值从上面的表中也能看出来）。 Kneser-Ney discountingKneser-Ney discounting是在absolute discounting的基础上，对低阶gram进行一些处理～ 这个博客对这部分讲的非常清楚：https://blog.csdn.net/baimafujinji/article/details/51297802 ，我就直接贴过来了。。。 如果我们使用bigram和unigram的插值模型来预测下面这句话的下一个词： I can’t see without my reading _______. 我们的直觉是 glasses，但是如果语料库中出现 Kong 的频率非常高，因为 Hong Kong 是高频词。所以采用unigram模型， Kong 具有更高的权重，所以最终计算机会选择 Kong 这个词（而非glasses）填入上面的空格，尽管这个结果看起来相当不合理。这其实就暗示我们应该调整一下策略，最好仅当前一个词是 Hong 时，我们才给 Kong 赋一个较高的权值，否则尽管在语料库中 Kong 也是高频词，但我们并不打算单独使用它。 如果说 P(w) 衡量了 w 这个词出现的可能性，那么我们现在想创造一个新的 unigram 模型，叫做 $P_{continuation}$ ，它的意思是将 w 这个词作为一个新的接续的可能性。注意这其实暗示我们要考虑前面一个词（即历史）的影响。或者说，为了评估 $P_{continuation}$ （注意这是一个 unigram 模型），我们其实需要考察使用了 w 这个词来生成的不同 bigram 的数量。注意这里说使用了 w 这个词来生成的不同类型 bigram 的数量，是指当前词为 w ，而前面一个词不同时，就产生了不同的类型。例如：w = “food”, 那么不同的 bigram 类型就可能包括 “chinese food”，“english food”，“japanese food”等。每一个 bigram 类型，当我们第一次遇到时，就视为一个新的接续（novel continuation）。 也就是说 $P_continuation$ 应该同所有新的接续（novel continuation）构成的集合之势（cardinality）成比例。所以，可知: $$P_{continuation}(w_i)\\propto \\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert$$ 如果你对此尚有困惑，我再来解释一下上面这个公式的意思。当前词是 $w_i$，例如“food”，由此构成的不同类型的 bigram 即为 $w_{i−1}w_i$，其中 $w_{i−1}$ 表示前一个词（preceding word）。显然，所有由 $w_{i−1}w_i$ 构成的集合的势，其实就取决于出现在 $w_i$ 之前的不同的 $w_{i−1}$ 的数量。 然后，为了把上面这个数变成一个概率，我们需要将其除以一个值，这个值就是所有 bigram 类型的数量，即 $\\lvert {(w_{j-1},w_j):C(w_{j-1}w_j)&gt;0}\\rvert$,这里大于0的意思就是“出现过”。于是有: $$P_{continuation}(w_i)=\\frac{\\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert}{\\lvert{(w_{j-1},w_j):C(w_{j-1}w_j)&gt;0}\\rvert}$$ 也可以写成这种形式： $$P_{continuation}(w_i)=\\frac{\\lvert { w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert} {\\sum_{w’_ {i-1}} \\lvert{w’_ {i-1}:C(w’_ {i-1}w’_ i)&gt;0}\\rvert}$$ 其实要理解这个很简单，绝对值里面的看成一个字典 dict,出现 $w_{i-1}w_i$ 了一次它的数量就是1，以 $P_continuation(food)$ 为例，语料库中出现了 chinese food, japan food, english food,那么分子就是计算不同 $w_{i-1}$ 的个数，其大小就是3。 而分母呢，就是所有的bigram，放到dict中，相当于去重，然后它的总数就是分母的值。即所有不同的 bigram 的数量就等于出现在单词 $w’_ i$ 前面的所有不同的词 $w’_ {i−1}$ 的数量，这个计算复杂度应该就是V吧，遍历整个词表即可。 如此一来，一个仅出现在 Hong 后面的高频词 Kong 只能获得一个较低的接续概率（continuation probability）。由此，再结合前面给出的Absolute Discounting 的概率计算公式，就可以得出 Interpolated Kneser-Ney Smoothing 的公式，即 $$P_{KN}(w_i|w_{i-1})=\\frac{max(C(w_{i-1}w_i)-d,0)}{C(w_{i-1})}+\\lambda(w_{i-1})P_{continuation}(w_i)\\tag{4.33}$$ 其中，$max(C(w_{i−1}w_i)−d,0)$ 的意思是要保证最后的计数在减去一个 d 之后不会变成一个负数。其次，我们将原来的 $P(w_i)$ 替换成了 $P_continuation(w_i)$。此外，**$\\lambda$ 是一个正则化常量，用于分配之前discount的概率值**（也就是从高频词中减去的准备分给那些未出现的低频词的概率值）： $$\\lambda(w_{i-1})=\\frac{d}{C(w_{i-1})}\\cdot \\lvert {w:C(w_{i-1},w)&gt;0}\\rvert\\tag{4.34}$$ 以 $P_{KN}(Kong|Hong)$ 为例，（4.33）式中第一项是对bigram (Kong|Hong)打了折，那么后面一项应该在此基础上补回来一点。 （4.34）第一项 $\\dfrac{d}{C(w_{i-1})}$ 是折扣归一化（normalized discount）；第二项 $\\lvert {w:C(w_{i-1},w)&gt;0}\\rvert$ 是打折的次数。 不太好理解，看原文章： 将上述公式泛化，可用递归表示为： $$P_{KN}(w_i|w_{i-n+1}\\cdots w_{i-1})=\\frac{max(0,C_{KN}(w_{i-n+1} \\cdots w_i) - d)}{C_{KN}(w_{i-n+1}\\cdots w_{i-1})} +\\lambda(w_{i-n+1}\\cdots w_{i-1})\\cdot P_{KN}(w_i|w_{i-n+2}\\cdots w_{i-1})$$ 其中 $C_{KN}$ 取决于用什么插值方式，在（4.33）式中只采用了bigram，如果采用trigram，bigram，unigram的联合插值，那么对于最高阶的 trigram 在计数时并不需要使用接续计数（采用普通计数即可），而其他低阶，即 bigram 和 unigram 则需要使用接续计数。这是因为在 unigram 中，我们遇到了一个 Kong，我们可以参考它的 bigram，同理在 bigram，我还可以再参考它的 trigram，但是如果我们的插值组合中最高阶就是 trigram，那么现在没有 4-gram来给我们做接续计数。用公式表示即为： $$C_{KN}(\\cdot)=\\begin{cases}count(\\cdot) &amp;,for\\ the\\ highest\\ order\\ continuationcount(\\cdot) &amp;,for\\ all\\ other\\ lower\\ orders \\end{cases}$$ 为什么低阶的gram要采用continuation呢？当trigram的概率为0时，使用bigram会增大概率量，所以都要打折，所以对应的bigram也得用continuation才好对吧～ 我们前面提到Kneser-Ney Smoothing 是当前一个标准的、广泛采用的、先进的平滑算法。这里我们所说的先进的平滑算法，其实是包含了其他以 Kneser-Ney 为基础改进、衍生而来的算法。其中，效果最好的Kneser-Ney Smoothing 算法是由Chen &amp; Goodman（1998）提出的 modified Kneser-Ney Smoothing 算法，它对 discount d使用不同的值 $d_1,d_2,d_3$ 分别对应于unigram, bigram, trigram等等。很多NLP的开发包和算法库中提供有原始的Kneser-Ney Smoothing（也就是我们前面介绍的），以及modified Kneser-Ney Smoothing 算法的实现。 The Web and Stupid Backoff对于特别大的语言模型，效率考虑很重要。 它不是将每个单词存储为一个字符串，而是通常在内存中将其表示为64位散列号，并将单词本身存储起来。通常只使用4-8位（而不是8字节的浮点数）对概率进行量化，并将N-gram存储在反向尝试中。 N-gram也可以通过修剪来缩小，例如只存储计数大于某个阈值的N-gram（例如用于Google N-gram发布的计数阈值40）或使用熵修剪不太重要的N-grams（Stolcke，1998）。 stupid Backoff 没有discounting，没那么多复杂的方法，就是简单的回退到上一阶，所以不是一个概率分布，概率加起来大于1了。如果回退到unigram, $S(w)=\\dfrac{count(w)}{N}$,并且 $\\lambda$ 使用的0.4. Advanced: Perplexity’s Relation to Entropy熵 entropy困惑度是来自信息论中的交叉熵cross-entroy的概念。 前面我的一篇博客从信息论的角度提到了交叉熵：blog 对于一个随机变量 $X=(X_1…X_n)$，它的分布是p(x),那么这个随机变量的熵就是： $$H(X)=-\\sum p(x)logp(x)$$ 对数底数可以为任何数。$-logp(x)$ 是香农定义的信息量，如果我们假设底数为2，那么信息量 $-log_2p(x)$ 表示描述X随机变量取X_1时所需的编码长度，p(X=X_1)概率越大，所需的编码长度越小。那么熵H(X)就表示描述随机变量X编码长度的期望了。 作者这里用一个例子描述了熵这个概念： 文章中对概率最大的 Horse1,其信息量 $-log_2(1/2)=1$,其用二进制编码就是0， Horse2 其信息量就是 $-log_2(1/4)=2$,其用二进制编码就是 10…. 可以看到概率越大，所需的编码长度越小，也就是信息量越小。可以极端点想，对于一个随机事件，如果其分布是(1,0,0,0)和(1/4,1/4,1/4,1/4)，同样发生10次，可能前者的分布就判断出来了，但后者需要更多。那么用来描述这个随机变量的不确定性的度量，就是熵了，也就是信息量的期望。 总结下：所谓熵，就是信息量 -logp(x) 的期望。 信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。 这里引入到语言模型中，对于一个sequence可以看做一个随机变量，$W={ w_0,w_1,…,w_n }$, 对于一个有限长度为n的sequence，其熵为： $$H(w_1,w_2,…,w_n)=-log_{w_1^n\\in L}p(w_1^n)logp(w_1^n)$$ entropy rate, 也可以看作是per-word entropy: $$\\dfrac{1}{n}H(w_1,w_2,…,w_n)=-\\dfrac{1}{n}log_{w_1^n\\in L}p(w_1^n)logp(w_1^n)$$ 交叉熵 cross-entropy但我们在考虑language的熵时，需要认为sequence是无限长度的。可以把sequences生成的过程看成是一个随机过程stochastic process L, 那么L的熵： $$H(L)=-lim_{n\\to \\infty}\\dfrac{1}{n}H(w_1,w_2,…,w_n)=-lim_{n\\to \\infty}\\sum_{W\\in L}p(w_1,w_2,…,w_n)logp(w_1,w_2,…,w_n)$$ cross-entropy: 但我们不知道生成数据的真实分布p时，可以用分布m来表示： $$H(p,m)=lim_{n\\to \\infty}-\\dfrac{1}{n}\\sum_{W\\in L}p(w_1,w_2,…,w_n)logm(w_1,w_2,…,w_n)$$ 对于静态随机过程(stationary ergodic process),静态假设在HMM中有讲到：下一个词对上一个词的依赖的概率，不会随时间的改变而改变。那么可以认为 $p(w_1,w_2,…,w_n)$ 是一个定值。 $$H(p,m)=lim_{n\\to \\infty}-\\dfrac{1}{n}\\sum_{W\\in L}logm(w_1,w_2,…,w_n)\\tag{4.47}$$ 又有：$$H(p)\\le H(p,m)$$ 因此我们可以用m分布来估计p分布。 困惑度和交叉熵N-gram模型 $M=P(w_i|w_{i-N+1} \\cdots w_{i-1})$ 生成序列sequences W, 根据公式(4.47): $$H(W) = -\\dfrac{1}{N}logP(w_1w_2 \\cdots w_N)$$ 基于模型 $M=P(w_i|w_{i-N+1} \\cdots w_{i-1})$ 的困惑度perplexity可定义为交叉熵的指数形式： 似乎也是有点巧啊，交叉熵和困惑度都可以用来表征p分布是否接近真实分布～厉害了 总结 ###参考： [1] Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin, 3rd. Chapter 4 [2] 自然语言处理中N-Gram模型的Smoothing算法","link":"/2018/04/12/chapter4-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8CN%E5%85%83%E8%AF%AD%E6%B3%95/"},{"title":"chapter9 隐马尔可夫模型","text":"chapter6: 隐马尔科夫模型 standford tutorial, 可以说是看过的关于隐马尔科夫最好的教程了。要是看英文原版能和看中文一样easy该多好，可惜文化差异的情况下，即使是单词都认识，理解起来也会有点困难。对此，只能自己重新总结一下吧～ 可以说，斯坦福从未让人失望过，太赞了！ 也是无意中在google上看到这篇文章，才发现了这么好的一本书, Speech and language processing. 前言： 隐马尔可夫模型马尔可夫模型的后裔，它是一个序列模型(sequence model). 对于一个序列模型，它的工作是给序列中的每一个小单元分配标签label或是类别class.其中包括：part-of-speech tagging, named entity tagging, and speech recognition. 马尔可夫链 Markov chains马尔可夫链和隐马尔科夫模型都是有限自动机(finite automation)的拓展。可以将他们看作是有权重的有限自动机(weighted finite automation),包括一些列状态和状态之间的转移关系，其中从一个状态到另一个状态的转移弧线是有权重的。在马尔可夫链中，这样的权重代表着概率，且从一个节点出来的概率之和为1. 上图中不仅包括了状态之间的转移概率，还包括了start和end两种特定的状态。 | states | hot1 | cold2 | warm3 | | —— | ——– | ——– | ——– | | hot1 | $a_{11}$ | $a_{12}$ | $a_{13}$ | | cold2 | $a_{21}$ | $a_{22}$ | $a_{23}$ | | warm3 | $a_{31}$ | $a_{32}$ | $a_{33}$ | start0: {$a_{01},a_{02},a_{03}$} end4: {$a_{14},a_{24},a_{34}$} 根据图9.1，马尔可夫链可以看做是一个概率图模型，其中包括以下几部分： Q代表状态集合，A是状态转移矩阵， $a_{ij}$ 表示从状态 $q_i$ 到状态 $q_j$ 的概率,那么有 $\\sum_{j=1}^na_{ij}=1,i=1,2…N$ $q_0$ 和 $q_F$是初始状态和终止状态。 做两个重要的假设： 以简化模型 (1) The Limited horiqon assumption 齐次假设 对于t时刻的状态，只取决于之前的一个状态。 $$ Markov Assumption: P(q_i|q_1…q_{i-1})=P(q_i|q_{i-1})$$ 也就是一阶马尔科夫链(a first-order Markov)。 图中的 $x_i$ 表示可观测时的 $q_i$ (2) Stationary process assumption 静态过程假设 the conditional distribution over next state given current state does not change over time. 对于状态之间的条件概率不会随时间的变化而改变，也就是状态转移矩阵只有一个～ $$P(q_t|q_{t-1})=P(q_2|q_1);t \\in 2…T$$ 在很多其他的论文中，用 $\\pi$ 来表示初始状态 都是一样的，不过这篇教程中用第一种表示方法。 隐马尔可夫模型为了更形象的描述隐马尔可夫模型，文章举了这样的一个栗子： Imagine that you are a climatologist in the year 2799 studying the history of global warming. You cannot find any records of the weather in Baltimore, Maryland, for the summer of 2007, but you do find Jason Eisner’s diary, which lists how many ice creams Jason ate every day that summer. Our goal is to use these observations to estimate the temperature every day. We’ll simplify this weather task by assuming there are only two kinds of days: cold (C) and hot (H). So the Eisner task is as follows: Given a sequence of observations O, each observation an integer corresponding to the number of ice creams eaten on a given day, figure out the correct ‘hidden’ sequence Q of weather states (H or C) which caused Jason to eat the ice cream. 总结下就是，观察到的序列是每天吃的冰淇淋数目2,3,1,2,3,….，从而判断每天的天气是hot or cold这两种状态中的哪一种。 定义一个隐马尔可夫模型，有以下组成部分： 两个假设 (1)一阶马尔科夫模型： $$ Markov Assumption: P(q_i|q_1…q_{i-1})=P(q_i|q_{i-1})$$ (2)条件独立，在状态 $q_i$ 的条件下，观测 $o_i$ 只取决于 $q_i$， 而与其他的状态和观测值够无关 $$Output Indepence: P(o_i|q_1…q_i,…,q_T,o_1,…,o_i,…,o_T)=P(o_i|q_i)$$ 对ice cream task.问题的描述如下图： 注意到，图中所有的概率都不为零，这种HMM模型叫做 fully connected 或是 ergodic HMM. 但并不是所有状态都可以互相转移的，比如 left-to-right HMM,又称Bakis HMMs,通常用于语音处理。 左图表示Bakis HMM，右图是ergodic HMM. 关于隐马尔可夫模型的三个问题： 问题1：计算似然概率。 前向/后向算法 问题2：解码问题，又称预测问题，已知模型参数和观测序列，求最有可能的状态序列 维特比算法 问题3：学习问题，已知观测序列，估计模型参数使得该模型下观测序列的概率最大。 极大似然估计，Baum-Welch, EM算法 概率计算：前向算法状态已知的话，是监督学习以图9.3为例，观测序列为（3,1,3）假如我们知道隐藏状态是（hot, hot, cold）的话，在此基础上计算似然概率就很简单了。 也就是说，已知观测序列 $O=o_1,o_2,…,o_T$. 且隐藏状态序列已知 $Q=q_0,q_1,…,q_T$,那么似然概率为： $$P(O|Q)=\\prod_{i=1}^TP(o_i|q_i)$$ 状态无法观测，无监督学习但大多数情况下，状态是不知道的，因为我们需要取计算出现观测序列 (3,1,3) 的所有可能的隐藏状态序列。 观测序列 $O=o_1,o_2,…,o_T$，假定存在一个特定的隐藏状态序列 $Q=q_0,q_1,…,q_T$，那么联合概率分布： $$P(O,Q)=P(O|Q)\\times P(Q)=\\prod_{i=1}^Tp(o_i|q_i)\\times \\prod_{i=1}^TP(q_i|q_{i-1})\\tag{9.10}$$ 所以隐马尔科夫是一个双重随机过程。 那么观察序列为（3,1,1）和状态序列为（hot, hot, cold）的联合概率为： 这样我们知道了怎么求一个特定的隐藏序列和观测序列的联合概率，那么所有可能隐藏序列的类和就是观测序列的总似然概率了。 $$P(O)=\\sum_QP(O,Q)=\\sum_QP(O|Q)P(Q)\\tag{9.12}$$ 对冰淇淋的例子，如果观测序列是（3,1,3），那么似然概率为： 如果有N中状态的话，对于长度为T的序列，其计算复杂度就是 $O(N^T)$ 这就太大了，所以得寻求更简单的解法。 forward algorithm前向算法是一种动态规划算法，其计算复杂度是 $O(N^2T)$ The forward algorithm is a kind of dynamic programming algorithm, that is, an algorithm that uses a table to store intermediate values as it builds up the probability of the observation sequence. The forward algorithm computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a single forward trellis. 之所以高效的原因，是它将所有的路径都隐式的折叠到一个前向网格中。。 前向网格(forward trellis)中的每一个单元(cell) $\\alpha_t(j)$ 表示给定模型 $\\lambda$，t时刻观测序列为 $o_1,o_2,…,o_t$,状态为j的概率。 $$\\alpha_t(j)=P(o_1,o_2,…,o_t,q_t=j|\\lambda)\\tag{9.13}$$ 其中， $\\alpha_t(j)$ 的计算是叠加所有可能的路径。 $$\\alpha_t(j)=[\\sum_{i=1}^N\\alpha_{t-1}a_{ij}]b_j(o_t)$$ 表示t时刻状态为j,从t-1时刻到t时刻，有N条路径，叠加～ 以图9.7中的第二时间步和状态2为例， $$\\alpha_2(1)=\\alpha_1(1)×P(H|H)×P(1|H) + α_1(2)×P(H|C)×P(1|H)$$ 下图描述了前向网格中计算一个cell的步骤： 整个似然概率的计算过程，伪代码： 真的讲的太好了太清楚了！！！感动哭。。。说真的，要不中国的大学都改成英文教学吧。。看着那些翻译过来的书籍都头疼。。这么好的英文教学材料，为什么翻译过来之后就那么难理解了。。感觉很多老师可能自己懂了，但是讲出来的课或是写出来的书，完全就是应付任务的吧。。 好吧，回到主题。伪代码中： 概率矩阵 forward [N+2,T] 是包括了初始状态start和结束状态end,那么 forward[s,t]表示 $\\alpha_t(s)$ initialization step: $$\\alpha_1(j)=a_{0j}b_j(o_1),1\\le j \\le N$$ 伪代码中：forward[s,1] &lt;– $a_{0,s}* b_s(o_1)$ 是从状态0到t=1时刻的状态s Recursion (since states 0 anf F are non-emittinf): $$\\alpha_t(j)=[\\sum_{i=1}^N\\alpha_{t-1}(i)a_{ij}]b_j(o_t)$$ 在伪代码中有两个循环，分别是对时间步2到T，以及每个时间步，其中的状态从1到N 1234567for each time step from 2 to T do: for each state s from 1 to N do: forward[s,t] = sum_{s'=1}^N forward[s', t-1] * a_{s',s} * b_s{o_t} $forward[s,t] = sum_{s’=1}^N forward[s’, t-1] * a_{s’,s}$ 可以发现，概率矩阵forward中的每一个值表示的是t时刻状态为s的概率，也就是 $\\alpha_t(s)$ Termination: $$P(O|\\lambda)=\\alpha_T(q_F)=\\sum_{i=1}^N\\alpha_T(i)a_{iF}$$ 伪代码中： forward[$q_F$,T] = $\\sum_{s=1}^N$ forward[s,T] * $a_{s,q_F}$ return forward[$q_F$,T] T时刻状态为 $q_F$的概率。感觉应该是T+1时刻吧。。。？？？ 预测问题：维特比算法Decoding: The Viterbi Algorithm 解码问题（预测问题）：给定HMM模型 $\\lambda=(A,B)$ 和观察序列 $O=o_1,o_2,…,o_T$, 找出概率最大的隐藏状态序列 $Q=q_1q_2…q_T$ 在前向算法中，我们知道了怎么计算特定隐藏状态序列和观测序列的联合概率，也就是公式（9.13），然后找出其中概率最大的序列就可以了对吧？但是我们知道状态序列有 $N^2$ 个，这样计算就太复杂了。于是，有了 Viterbi algorithm. 维特比算法是一种动态规划算法，类似于最小化编辑距离。 上图展示了，冰淇淋例子中，HMM模型参数已知，观测序列为（3,1,3）的情况下，计算最大隐藏状态序列的过程。Viterbi网格中每一个cell为 $v_T(j)$ 表示t时刻，观察序列为 $o_1,o_2,…,o_t$， 隐藏状态为j，前t-1的状态序列为 $q_1q_2…q_{t-1}$ 的概率。 $$v_t(j)=P(q_0q_1…q_{t-1},o_1,o_2,…,o_t,q_t=j|\\lambda)$$ 换句话说，t-1时刻的状态序列是这样 $q_1q_2…q_{t-1}$，有N个这样的序列（因为t-1时刻的状态有N个），然后计算出这N个序列中到t时刻状态为j的概率，找出其中最大值，就是从开始到t时刻状态为j的最大概率序列。 $$v_t(j)=max_{i=1}^N v_{t-1}(i)a_{ij}b_j(o_t)$$ 对应到图（9.10）中，以 $v_2(2)$为例， $$v_2(2)=max(v_1(1)* a_{12}* b_2(1),v_1(2)* a_{22}* b_2(1))$$ $$v_2(2)=max(v_1(1)* P(H|C)* P(1|H), v_1(2)* P(H|H)* P(1|H))$$ 维特比算法的整个过程，伪代码如下： 我们发现Viterbi算法跟前向算法非常相似，除了前向算法是计算的sum，而Viterbi是计算的max，同时我们也发现，Viterbi相比前向算法多了一个部分：backpointers. 原因是因为前向算法只需要计算出最后的似然概率，但Viterbi不仅要计算出最大的概率，还要得到对应的状态序列。因此，在类似于前向算法计算概率的同时，记录下路径，并在最后backtracing最大概率的路径。 The Viterbi backtrace. initialization: $$v_1(j)=a_{0j}b_j(o_1)\\tag{9.20}$$ $$b_{t1}(j)=0\\tag{9.21}$$ 初始状态只有一个节点start，可确定为0 Recursion(recall that states 0 and $q_F$ are non-emitting): $$v_t(j)=max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t);1\\le j \\le N, 1\\le t\\le T\\tag{9.22}$$ $$b_{t_t}(j)=argmax_{i=1}^Nv_{t-1}(i)a_{ij}b_i(o_t);1\\le j \\le N, 1\\le t\\le T\\tag{9.23}$$ $b_{t_t}(j)表示t时刻状态为j的所有N个路径 $(q_1,q_2,…,q_{t-1})$$ 概率最大的路径的第k-1个节点。 以图9.12为例，对于t=2，状态为1的节点，其max()中有2项，分别是 $v_1(1)* a_{11}* b_1(1)$ 和 $v_1(2)* a_{21}* b_1(1)$,其中较大的项的节点就是 $max_{i=1}^Nv_{t-1}(i)a_{ij}b_i(o_t)$, 但t时刻也有N个节点，所以也需要记录，因此用arg Termination: The best score: $$P*=v_T(q_F)=max_{i=1}^Nv_T(i)* a_{iF}\\tag{9.24}$$ 计算出T时刻到end状态的最大概率，也就是所有路径中的最大概率。 The start of backtrace: $$q_T*=b_{t_T}(q_F)=argmax_{i=1}^Nv_T(i)* a_{iF}\\tag{9.25}$$ 表示T时刻中N个路径中概率最大的结点。 其实Viterbi算法的路径数量和前向算法的路径数量是一模一样的，只是一个是max，一个是sum，因此也可以参考图9.8. 学习问题：HMM Training: The Forward-Backward AlgorithmLearning: Given an observation sequence O and the set of possible states in the HMM, learn the HMM parameters A and B. 第三个问题，给定观测序列 $O=(o_1,o_2,…,o_T)$, 估计模型参数使得在该HMM模型下，观测序列的概率 $P(O|\\lambda)$ 概率最大，即用极大似然估计的方法估计参数。 先考虑马尔可夫链马尔可夫链其状态是可观察的，可以看作是退化的隐马尔可夫模型。即没有发射概率(emmision probablities) B.因此，我们需要学习的参数只有状态转移矩阵（probability matrix）A. 其中 $a_ij$ 表示从状态i转移到状态j的概率，可以用大数定律来计算。 $C(i\\rightarrow)$ 表示观察到的序列中从状态i转移到状态j的数量。然后除以所有从状态i转移的总数量。 $$a_{ij}=\\dfrac{C(i\\rightarrow j)}{\\sum_{q\\in Q}C(i\\rightarrow q)}\\tag{9.26}$$ 显然分母不包括最后 T 时刻出现状态 i，因为end不属于Q. 隐马尔可夫模型： Baum-Welch算法 The Baum-Welch algorithm uses two neat intuitions to solve this problem. The first idea is to iteratively estimate the counts. We will start with an estimate for the transition and observation probabilities and then use these estimated probabilities to derive better and better probabilities. The second idea is that we get our estimated probabilities by computing the forward probability for an observation and then dividing that probability mass among all the different paths that contributed to this forward probability. 其实就是EM算法～ 在此之前，先了解一下后向算法，可以看做反向的前向算法。 后向算法对应的后向概率（Backward probability）$\\beta_t(i)$ 表示给定hmm模型 $\\lambda$, 在 t 时刻状态为 i 的条件下，t+1 时刻的观测序列为 $o_{t+1},o_{t+2},…,o_T$的概率. $$\\beta_t(i)=P(o_{t+1},o_{t+2},…,0_T|q_t=i,\\lambda)\\tag{9.27}$$ initialization: $$\\beta_T(i)=a_{iF},1\\le i \\le N$$ 在李航老师的《统计学习导论》这本书上 $\\beta_T(i)=1$. $\\beta_T(i)$ 的定义表示在T时刻状态为i，观测到序列为 $o_{T+1}$, 这东西不存在，可以看做是end吧，所以从T到end其概率应该是 $a_{iF}$,但是在李航老师的书上只有初始状态的概率 $\\pi=a_{01}$,而没有 $a_{iF}$. 感觉跟具体在什么场景下有关系。。 Recursion(again since stetes 0 and $q_F$ are non-emitting) $$\\beta_t(i)=\\sum_{j=1}^N\\beta_{t+1}(j)a_{ij}b_j(o_{t+1}),1\\le j \\le N, 1\\le t\\le T$$ 根据定义很好理解。$\\beta_{t+1}(j)$ 表示给定hmm模型 $\\lambda$, 在t+1时刻状态为j的条件下，t+1时刻的观测序列为 $o_{t+2},o_{t+3},…,o_T$的概率.那么就可以得到 $\\beta_t(i)$ 了。 Termination: $$P(O|\\lambda)=\\alpha_T(q_F)=\\beta_1(q_0)=\\sum_{j=1}^N\\beta_1(j)a_{0j}b_j(o_1)$$ 在初始模型参数下，用大数定律估计新的模型参数，也就是极大似然估计根据公式(9.26)我们可以知道，状态ｉ到ｊ的概率： $$\\hat a_{ij}=\\dfrac{expected\\ number\\ of\\ transitions\\ from\\ state\\ i \\ to\\ state\\ j}{expected\\ number\\ of\\ transitions\\ from\\ state\\ i}$$ 然而怎么计算这些numerator？试想，如果我们知道特定时刻t，从状态i转移到j的概率，那么就能计算所有时刻t的从i转移到j的数量。 定义 $\\zeta_t$ 表示在给定模型参数和观察序列条件下，t时刻状态为i，t+1时刻状态为j的概率。 $$\\zeta_t(i,j)=P(q_t=i,q_{t+1}=j|O,\\lambda)\\tag{9.32}$$ 但是模型参数我们不知道呀，也是我们需要学习得到的。 这样我们先计算一个和 $\\zeta_t$ 相似的概率。 $$not-quite-\\zeta_t(i,j)=P(q_t=i,q_{t+1}=j,O|\\lambda)\\tag{9.33}$$ $\\alpha_t(i)$ 和 $\\beta_t(j)$ 是前向/后向算法中的定义。我们先看下前向算法计算的条件： 也就是problem1中的条件，给定 $\\lambda$ 和 观察序列，求 $P(O|\\lambda)$ 我们可以用 $\\alpha_t(i)$ 和 $\\beta_t(j)$ ，来表示 $\\zeta_t$, 是因为我们在计算 $\\zeta_t$ 时是先假定有这个一个模型参数，比如初始参数～ 那么： $$not-quite-\\zeta_t=\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)\\tag{9.34}$$ 根据bayes公式： $$P(X|Y,Z)=\\dfrac{P(X,Y,Z)}{P(Y,Z)}=\\dfrac{P(X,Y,Z)}{P(Z)P(Y|Z)}=\\dfrac{P(X,Y|Z)}{P(Y|Z)}\\tag{9.35}$$ 对应起来就是： $$P(q_t=i,q_{t+1}=j|O,\\lambda)=\\dfrac{P(q_t=i,q_{t+1}=j,O|\\lambda)}{P(O|\\lambda)}=\\dfrac{not-quite-\\zeta_t}{P(O|\\lambda)}\\tag{9.36}$$ 其中： $$P(O|\\lambda)=\\alpha_T(q_F)=\\beta_1(q_0)=\\sum_{j=1}^N\\alpha_t(j)\\beta_t(j)\\tag{9.37}$$ 这一步最后面一个式子的理解可以看做是前向算法和后向算法在时刻t相遇。 看到这里会发现，李航老师书中179页，公式25-26的推导就有点逻辑不通了。 因此，现在就可以推导出 $\\zeta_t$： $$\\zeta_t{i,j}=\\dfrac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\alpha_T(q_F)}\\tag{9.37}$$ $\\zeta_t$ 表示的是某一个时刻t，那么对于参数 $a_{ij}$ 的估计就是所有的时刻中i到j的总数除以i到k(k=1,2…N)的总数 $$\\hat a_{ij}=\\dfrac{\\sum_{t=1}^{T-1}\\zeta_t(i,j)}{\\sum_{t=1}^{T-1}\\sum_{k=1}^N\\zeta_t(i,k)}\\tag{9.38}$$ 同样的道理，我们可以推理得到发射矩阵B的参数估计 $b_j(v_k)$ 状态为j，观察得到 $v_k,v_k\\in V$的概率： 在t时刻状态为j的概率，定义为 $\\gamma_t(j)$ $$\\gamma_t(j)=P(q_t=j|O,\\lambda)\\tag{9.40}$$ 同样的道理： $$\\gamma_t(j)=\\dfrac{P(q_t=j,O|\\lambda)}{P(O|\\lambda)}\\tag{9.41}$$ 同公式（9.37）一样，前向后向算法在t时刻相遇： $$\\gamma_t(j)=\\dfrac{\\alpha_t(j)\\beta_t(j)}{P(O|\\lambda)}$$ 然后求整个时间段内的j到 $v_k$的总数，除以状态为j的总数。 $$\\hat b_j(v_k)=\\dfrac{\\sum_{t=1,o_t=v_k}^T}{\\sum_{t=1}^T}\\gamma_t(j)$$ 仔细回顾以下这个过程，在初始模型参数和观测序列的条件下，根据大数定律对模型参数进行更新。其实就是在初始模型参数下，根据极大似然估计求得观测序列的极大似然估计，然后在似然概率最大的条件下求得相应的模型参数。 可以看到这里直接用大数定律和李航老师书上，使用极大似然估计，然后求导得到的公式是一样的。 EM算法 E step: 根据初始模型参数或是M step得到的模型参数，得到后验概率。 M step: 根据E step中得到的概率，估计出新的模型参数。这里直接用大数定律得到的，其实其本质原理就是极大似然估计，也就是求出使得概率最大的模型参数。 那么迭代条件呢？什么情况下终止？在GMM中有log函数，这里呢。。 这里应该就是 $P(O|\\lambda)$ 吧，在前向算法中有计算到在模型参数和观察序列条件下的极大似然估计。 根据GMM和HMM对使用EM算法进行参数估计的一点想法：所以EM算法中的E step并不是求期望，而是在对模型参数进行估计时，在初始模型或previous模型的情况下，求得基于观测序列或是训练样本的用极大似然估计或是大数定律求得后验概率。 然后M STEP就是让这个概率最大的条件下更新模型参数。 总结： 在回顾下隐马尔可夫模型的三个问题： 第一个问题，计算概率 已知模型参数 $\\lambda$ 和观测序列 $O$，求在该模型下，出现观测序列的概率。 使用前向算法，一个动态回归的算法，把求长度为T的概率转换为t到t+1的概率sum 这一问题其实主要是为后面两个问题铺垫的，因为一般的场景都是状态未知，更不可能知道模型参数了。 预测问题，又称解码问题 已知模型参数 $\\lambda$ 和观测序列 $O$, 求概率最大的状态序列。 使用Viterbi算法，类似于前向算法，不过每一步不是sum，而是max，并且需要回溯backpointers 这个问题的应用场景就比较广了。 学习问题：模型参数估计 已知观测序列 $O$，估计模型参数 $\\lambda$, 使得观测序列的概率 $P(O|\\lambda)$ 最大。 使用Baum-Welch(极大似然估计)或forward-backward(大数定律)算法，并使用EM算法迭代，对参数进行估计，","link":"/2018/04/06/chapter9-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-standford/"},{"title":"chapter8-神经网络和自然语言模型","text":"Embeddings 这部分简单的看一下吧～毕竟已经很熟了。。。 chapter 8 - neural networks chapter 15 - semantic representations for words called embeddings chapter 25 - sequence-to-sequence (seq2seq model), applied to language generation: machine translation, conversation agents and summarization. Neural Language Models相比前面chapter4介绍的paradigm(the smoothed N-grams),依据神经网络的语言模型有很多优势～ 不需要smoothing？ 能够依赖更长的历史依据 handle much longer histories 泛化能力更强 除此之外，还是生成模型的基础～ Embeddings词向量： 为什么要用向量表示词？ Vectors turn out to be a really powerful representation for words, because a distributed representation allows words that have similar meanings, or similar grammatical properties, to have similar vectors. 相近意思和相似语法性质的词，具有相似的向量。 用神经网络模型训练词向量，这里是用的4-gram，生成下一个词取决于前三个词： input layer: 1x|V| embedding vector matrix: dx|V| projection layer: 1x3d hidden layer: W.shape (dhx3d) -&gt; 1xdh output layer: U.shape (|V|xdh) -&gt; 1x|V| softmax : $P(w_t=i|w_{t-1},w_{t-2},w_{t-3})$","link":"/2018/04/17/chapter8-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"title":"chapter6-朴素贝叶斯和情感分类","text":"Naive bayes 模型 p(x|y) 训练：求根据极大似然估计（频率代替概率） p(y|x),p(x)，无参估计 优化：各种预处理和特征提取 验证模型： Precision, Recall, F-measure 对于多分类的处理 交叉验证 比较分类器：统计显著性测试 前言：文本分类(text categorization) 的应用： 情感分析（sentiment analysis）:the extraction of sentiment 垃圾邮件检测（spam detection） 作者归属（author attribution） 主题分类（subject category classification） 除了文本分类外，分类在NLPL领域还有很多其他应用： 句号消歧（period disambiguation） 单词标记（word tokenization） part-of-speech tagger 命名实体标注 named-entity tagging 本章节深入介绍 multinomial naive Bayes, 下一章将 nultinomial logistic regression, 又叫 maximum entropy. 同时，简单介绍了一下生成算法和判别算法的区别：可参考前面的机器学习-生成模型到高斯判别分析再到GMM和EM算法 进一步的理解下： 判别算法： $p(y|x;\\theta)$ 根据交叉熵 $H(\\hat y, y)$ 等损失函数，使用梯度下降等来优化参数 $theta$. 生成算法:p(x|y)， 可以分为两种，有标签的和无标签的 有标签的是监督学习，以高斯判别分析GDA和本章节中的Naive Bayes为例，就是根据 $p(y|x) = \\dfrac{p(x|y)p(y)}{p(x)}$,在有标签的情况下，可以根据MLE（用相对频度代替概率）来计算出 likelihood $p(x|y)$ 和先验概率prior $p(y)$,然后带入预测样本数据，得到对于每一个分类p(x|y=0,1,…)的概率，然后 $argmax_yp(x|y)$ 就是预测数据的类别。 无标签的是无监督学习，高斯混合模型GMM为例，需要引入隐藏变量z，计算 $p(x,z)=p(x|z)p(z)$,假设p(x|z)是多维高斯分布，然后使用EM算法对高斯分布的参数，以及先验概率p(z)进行估计~ 但因为类标签是不存在的，所以只能对样本数据进行聚类，而无法给对预测数据进行分类。 Naive Bayes Classifiers 朴素贝叶斯分类器样本数据： $(d_1,c_1),…,(d_N,c_N),\\quad c\\in C$ 贝叶斯分类器是一个概率分类器： $$\\hat c = argmax_{c\\in C}P(c|d)$$ Bayesian Inference: $$p(x|y) = \\dfrac{p(y|x)p(x)}{p(y)}$$ 则有： $$\\hat c = argmax_{c\\in C}P(c|d)=argmax_{c\\in C}\\dfrac{P(d|c)P(c)}{P(d)}$$ 可以直接去掉P(d),因为我们要计算对于每一类 $c\\in C$的概率 $\\dfrac{P(d|c)P(c)}{P(d)}$, 而P(d)对于任何类别都是不变的。而我们要求的是最后可能的class，其对应的P(d)都一样，所以可以直接去掉。 $$\\hat c = argmax_{c\\in C}P(c|d)=argmax_{c\\in C}P(d|c)P(c)$$ 其中P(d|c)是样本数据的 似然概率likelihood, P(c)是 先验概率prior,可以写成特征的形式： $$\\hat c = argmax_{c\\in C}P(f_1,f_2,…,f_n|c)p(c)$$ 显然 $P(f_1,f_2,…,f_n|d)$ 是很难计算的，以语言模型为例，你不可能考虑所有可能的词的组合，因此朴素贝叶斯分类器做了两个假设以简化模型： bag of words assumption：不考虑词的顺序，也就是说 love 这个词不管出现在 1st,20th等，它对这个sequence所属类别的影响是一样的。 naive bayes assumption:所有特征之间相互独立 $$P(f_1,f_2,…,f_n|c)=P(f_1|c)cdot P(f_2|c)\\cdots P(f_n|c)$$ 因此有： $$c_{NB}=argmax_{c\\in C}P(c)\\prod_{f\\in F}P(f|c)$$ 将朴素贝叶斯分类器应用于文本分类，我们需要考虑词的位置，这里其实也没有考虑词的顺序嘛。。 $$positions \\leftarrow all\\ word\\ position\\ in\\ test\\ document$$ $$c_{NB}=argmax_{c\\in C}P(c)\\prod_{i\\in position}P(w_i|c)$$ 为避免数值下溢，在对数域进行计算： $$c_{NB}=argmax_{c\\in C}logP(c)+\\sum_{i\\in position}logP(w_i|c)$$ Training the Naive Bayes Classifier分别计算概率P(c), $P(f_i|c)$ $$\\hat P(c)=\\dfrac{N_c}{N_{doc}}$$ 这里 $N_c$ 表示c类中所有词的总数，$N_{doc}$ 表示所有词的总数。 $$\\hat P(w_i|c)=\\dfrac{count(w_i,c)}{\\sum_{w\\in V}count(w,c)}$$ 分母表示c类中所有词的总数。词典V表示所有类别的词的总数，不仅仅是c类别的。 有个问题: 如果词“fantastic”，从未出现在 positive类别中，那么： 这是不合理的，所以采用 add-one smoothing: 这里要想清楚为啥是V，而不是c类中词的个数？要保证 $\\sum_iP(w_i|c)=1$ 参考习题： 对于 unknown words: 直接删除。。 对于 stop words: 也就是 the, a,..,这样高频词，可以删掉。也可以不管，效果差不多～ 算法流程 总结下： 1234567891011121314151617181920212223for each class in C: 计算 logprior[c] for each word in V: 计算 word在当前class下的likelihood loglikelihood[word,c]for each class in C: sum[c] = logprior[c] for each position in testdoc: if word in V: sum[c] += loglikehood[word,c] return argmax(sum[c]) Optimizing for Sentiment Analysisbinary multinominal naive Bayes or binary NB. 将每一个sequence或是documents中的重复词的数量变为1. 比如 great 出现在两个documents中，最后一个有2个great，那么positive中great总数为2，有点类似于归一化。任何一个词的数量不会超过document的数量。 deal with negation遇到logical negation(n’t, not, no, never)后，前置前缀 NOT_ 给每一个单词，直到遇到标点符号。 sentiment lexicons如果没有足够多的已经标注好的数据，可以使用预注释好的情感词。四个非常流行的词库： General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). 以 MPQA 为例，6885 words, 2718 positive and 4912 negative 话说怎么使用。。 Evaluation: Precision, Recall, F-measure 对于数据不均衡是，使用accuracy是不准确了。 举个栗子： a million tweet: 999,900条不是关于pie的，只有100条是关于pie的 对于一个stupid分类器，他认为所有的tweet都是跟pie无关的，那么它的准确率是99.99%！但这个分类器显然不是我们想要的，因而accuracy不是一个好的metric，当它目标是rare，或是complete unbalanced. 引入另外两个指标： 精度 precision: 是分类系统预测出来的positive的中真实positive的比例 召回 call： 是真实positive中分类系统也预测为positive的比例。 上面的例子中，目的是要找到tweet中和pie相关的一类，那么其召回率就是 0/10 = 0， 其precision是 0/0 Ｆ-measure $$F_{\\beta}=\\dfrac{(\\beta^2+1)PR}{\\beta^2P+R}$$ 当 $\\beta&gt;1$时，Recall的比重更大;当 $\\beta&lt;1$时，precision的比重更大。使用最多的是 $\\beta=1$,也就是 $F_{\\beta=1}, F_1$. $\\beta$ 的的取值取决于实际应用。 $$F_1 = \\dfrac{2PR}{P+R}$$ Ｆ-measure 是 precision 和 recall 的 **加权调和平均值(weighted harmonic mean)**。 调和平均值是倒数的算术平均值的倒数。 为什么要使用调和平均值呢？因为它是一个更 保守的度量(conservative metric). 相比直接计算 P 和 R 的平均值， F-measure的值更看重两者中的较小值。 More than two classes多分类有两种情况： 一种是一个document对应多标签，**（any-of or multi-label classification）** ：解决方法是对每一个类别使用二分类，比如对于类别c，可分类 positive labeled c 和 negative not labeled c. 另外一种是一个document只对应一个标签，但总的类别数大于2. (one-of or multinomial classification) confusion metrix: 将三个分类分开来看，pooled表示汇总 Test sets and Cross-validation 测试和交叉验证10折交叉验证： Statistical Significance Testing 统计显著性测试比较两个分类器A和B的好坏。 假设检验对于两个分类器 classifier A and B. 我们需要知道A的性能一定比B好吗？还是只是在特定的数据集上表现比B好？ null hypothesis: A is not really better than B，A 和 B在指标F-measure的差距是 $\\delta(x)$ 随机变量X是测试集的集合。 接受域： $H_0: P(\\delta (X)&gt; \\delta (x)|H_0)$ 当p-value(X)&lt;0.05 or 0.01,时我们拒绝假设 $H_0$ bootstrap test在NLP中通常不使用传统的统计方法，因为metrics并不是正态分布(normal distribution),违反了测试的假设。 对于Bootstrap知乎上有个比较清楚的答案：https://zhuanlan.zhihu.com/p/24851814 本质上，Bootstrap方法，是将一次的估计过程，重复上千次上万次，从而便得到了得到上千个甚至上万个的估计值，于是利用这不止一个的估计值，我们就可以估计待估计值的均值以外的其他统计量：比如标准差、中位数等。 应用在这里是因为但样本数据较少时，一次样本估计无法准确的计算出两个分类器A和B的差值，因而采用bootstrap. 参考： Speech and Language Processing ,3rd, Chapter 6 知乎：https://zhuanlan.zhihu.com/p/24851814","link":"/2018/04/14/chapter6-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%92%8C%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"title":"chapter7-logistic回归","text":"logistic regression 模型 p(y|x,w) 针对语言模型的特征处理 $f_i(c,x)$ 训练模型 正则化 特征选择：信息增益 分类器选择：bias-variance 前言: 分类算法：multinomial logistic regression, 当应用到NLP时，又称 maximum entropy, MaxEnt. 再一次说到了生成模型和判别模型。上一章已经说了了，就不写了～ logistic regression$$\\hat y=argmax_yP(y|x)$$ 这句话可以说解释的很清楚了～ 然鹅，能够直接计算出对应的概率P(y|x)吗？像这样： $$P(y|x)?=\\sum_{i=1}^Nw_if_i$$ $$?=w\\cdot f$$ 显然，这计算出来的并不是一个合理的概率，因为 $\\sum_{i=1}^N$ 的范围是 $-\\infty\\ to\\ \\infty$. 怎么解决这个问题呢？就是让得到的概率在0-1之间。 对于二分类：$y\\in {0,1}$可以使用sigmoid函数： $$\\sigma(z)=\\dfrac{1}{1+e^{-z}}$$ 将z压缩到0-1范围内。 则有： $$\\hat y = \\dfrac{1}{1+e^{-w^Tx}}$$ $$p(y=1|x) = \\dfrac{1}{1+e^{-w^Tx}}$$ $$p(y=0|x) = \\dfrac{1}{1+e^{w^Tx}}$$ 根据cross-entroy函数： $$L = -p(x)logq(x)$$ 对于单个样本有： $真实分布p(x):(y,1-y),对应的预测分布(\\hat y, 1-\\hat y)$ 带入可得： $$L(\\hat y, y)=-ylog(\\hat y)-(1-y)log(1-\\hat y)$$ 对于多分类softmax分类器： $$p(c|x)=\\dfrac{exp(\\sum_{i=1}^Nw_if_i(c,x))}{\\sum_{c’\\in C}exp(\\sum_{i=1}^Nw_if_i(c’,x))}$$ 其中 $f_i(c,x)$ 就是表示在给定样本条件下类别c对应的输入数据处理后的特征i。 怎么理解 $w_if_i(c,x)$ 呢？how to convince myself~ 假设单个样本 $X:$ (3072,1) 总共有10个类别 $c\\in C$ (10,) 则对应的权重： $W$ (10, 3072) 其实可以认为权重W的每一行对应一个分类器 $w_i$，也就是特征提取器。$f_i(c,x)$ 应该就是对不同类别的输入数据进行特征处理吧。 在图像处理中，可能并不需要提前对输入数据进行处理，但在NLP中先对输入数据进行特征工程是很重要的。 Features in Multinomial Logistic Regression假设document x含有词great,其class是 +($f_1$).则对应的 $f_1(c,x)$ $w_1(x)$ 表示great作为 class + 的权重。 Classification in Multinomial Logistic Regression这是一个简单的二分类positive or negative，其实也可以看起来跟图像是一样的，比如这里有4个词，也就是4个特征, $x = (1,1,1,1)^T$ $w_+=(0,0,0,1.9)$,$w__ =(0.7,0.9,-0.8)$ 在预测属于哪一类是，可以简化计算： Learning Logistic Regression怎么计算权重呢？ 训练样本数据通过 条件极大似然估计（conditional maximum likelihood estimation.） 对于单个样本 $(x^{(j)},y^{(j)})$ ，优化权重： $$\\hat w = argamx_w logP(y^{(j)}|x^{(j)})$$ 那么对于整个样本集： $$\\hat w = argamx_w \\sum_jlogP(y^{(j)}|x^{(j)})$$ 通过优化似然概率 $L(w)$ 来学习得到参数w $$L(w) = \\sum_jlogP(y^{(j)}|x^{(j)})\\tag{7.12}$$ 这里和我们前面讲过的softmax回归有点区别，softmax是先求出 $\\hat y$,然后与真实分布y进行比较，最小化差异；而multinomial logistic regression 是直接将真实分布y和观测数据x联合在一起最大化p(y|x). 同样也是一个凸优化问题（convex optimization problem），通过采用随机梯度上升～ $L’(w)关于权重求导$ 正则化当模型对训练数据过拟合（overfitting）时，给公式（7.12）增加正则化项，用来惩罚权重较大的项。 L2正则化Euclidean distance L1正则化Manhattan distance L1正则化和L2正则化都可以通过贝叶斯来解释～ L1正则化可以看作是权重满足Laplace分布. L2正则化可以看做是权重满足均值为0的高斯分布 以L2正则化为例，$w_j$服从高斯分布 然后假设均值 $\\mu=0$，$2\\sigma^2=1$,在对数域对w求导可得： 这与公式（7.17）一致～ 知乎上有一篇文章很好的解释了L1正则化与L2正则化 Feature Selection 特征选择对于生成模型如 naive bayes 无法使用正则化，因此需要 feature selection 如何进行特征选择，就是通过一些metric对特征进行排序，选择重要的特征。 information gain 这部分参考宗成庆老师的《统计自然语言处理》 信息增益（IG）法依据某项特征 $w_i$ 为整个分类所能提供的信息量的多少来衡量该特征项的重要程度。某个特征的信息增益指的是有该特征和没有该特征时，为整个分类所能提供的信息量的差别。其中，信息量的多少由熵来衡量。 因此信息增益即不考虑任何特征时文档的熵和考虑该特征后文档的熵的插值： $P(c_i)$ 表示训练样本中 $c_i$ 类文档的概率。 $P(w)$ 表示训练样本中包含特征w的文档占总文档数的概率。假设某一个文档中有两个词 ‘great’,那么需要将它数量变为1，这在chapter6中有讲到。 $P(c_i|w)$ 表示文档中包含特征w且类别为 $c_i$ 的概率。 在李航老师的《统计学习方法》中，决策树这一章中也有讲到使用信息增益来进行特征选择。 $$H(C|w) = P(w)\\sum_{i=1}^CP(c_i|w)logP(c_i|w)$$ 表示在特征w条件下对训练样本进行分类的不确定性，也就是条件熵的期望。 公式（7.23）中第一项是经验熵，就是对训练数据集进行不确定性的度量。第二项是经验条件熵，也就是在特征w给定的条件下对训练数据集进行分类的不确定性。 Choosing a classifier and features显然logistic回归要比naive bayes要好，因为naive bayes中假设特征 $f_1,f_2,…,f$ 相互独立，如果特征 $f_1 和 f_2$ 具有一定的相关性，那么naive bayes就overestimate这个特征。而logistic相比之下对具有相关性的特征的处理鲁棒性要强很多，如果 $f_1,f_2$ 完全正相关，那么他们的权重都会赋值减少为原来的 1/2. The overly strong conditional independence assumptions of Naive Bayes mean that if two features are in fact correlated naive Bayes will multiply them both in as if they were independent, overestimating the evidence. Logistic regression is much more robust to correlated features; if two features f 1 and f 2 are perfectly correlated, regression will simply assign half the weight to w 1 and half to w 2 . 当特征具有很强的相关性时，logistic的准确率要高于Naive bayes。但当数据集较小时，naive bayes的准确率要高于logistic和SVM，而且naive bayes更容易训练。 bias-variance tradeoff 偏差bias 较高: 欠拟合 underfitting 方差variance 较高： 过拟合 overfitting 如何选择各种分类器classifier: low bias : SVM with polynomial or RBF kernels, downweighting or removing features low variance: naive bayes, add more features feature interactions :特征工程很重要。 常见的分类器有：Support Vector Machines (SVMs) with polynomial or RBF kernels, and random forests. 总结 参考： Speech and language Processing，Chapter7 知乎：L1正则化与L2正则化 宗成庆，《统计自然语言处理》，第13章 李航，《统计学习方法》，第5章","link":"/2018/04/16/chapter7-logistic%E5%9B%9E%E5%BD%92/"},{"title":"cs224d-lecture1-词向量表示","text":"Word Vectors Skip-gram Continuous Bag of words(CBOW) Negative Sampling Hierarchical SoftmaxM Word2Vec 1. How to represent words?With word vectors, we can quite easily encode this ability in the vectors themselves (using distance measures such as Jaccard, Cosine, Eu-clidean, etc). 2. Word Vectorsencode word tokens into some vector(N-dimensional space, N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfre using speech. one-hot vector: V is the size of vocabulary. each word is a completely independent entity. 3. Iteration Based Methods - Word2Vec 2 algorithms: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary. language model Unigram model : $$P(w_1,w_2,…,w_n) = \\prod_{i=1}^nP(w_i)$$ bigram model: $$P(w_1,w_2,…,w_n) = \\prod_{i=1}^nP(w_i|w_{i-1})$$ 4. Continuous bag of words model(CBOW)predict center word from the context. $$ \\prod_{c=1}^{n}P(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$ negative log likelihood: $$J(\\theta)= -\\sum_{c=1}^{n}logP(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$ the words of context to generate the center word is dependent: $$J(\\theta) = \\dfrac{1}{n}\\sum_{c=1}^T\\sum_{-m\\le j\\le m}logp(w_c|w_{c+j})$$ how to present this probability??? To one sentence: $$ \\begin{align} minimize J &amp;= -logP(w_c|w_{c-m},..,w_{c-1},w_{c+1},…,w_{c+m})\\ &amp;= -log P(u_c|\\hat v)\\tag{1}\\ &amp;= -log \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{2}\\ &amp;= -u_c^T\\hat v + log\\sum_{j=1}^{|V|}exp(u_j^T\\hat v) \\end{align} $$ important: from word to vector the (1) to (2), using the softmax to present the probability $$P(u_c|\\hat v) = \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{* }$$ 其实word2vec可以理解为两个word，他们的上下文越相似，那么他们俩的词向量表示也就越相似.比如 he 和 she 大多数情况下他们的语境，也就是上下文出现的单词v都是很接近的，那么同样与这些词内积得到的概率就会差不多～说到底，也是个频率统计的方法，只不过用了无监督学习这个方式来得到distribution vector了～从这个角度理解就很合理了。错误的理解是 u 和v 出现在同一个窗口，他们的内积的概率就越大，这无法解释任何东西。 4.1 We can use an simple neural networt to train this matrix weightsinput: $x^{(c)}\\in R^{|V|\\times 1}$, the input one-hot vector of context labels: $y^{(c)}\\in R^{|V|\\times 1}$, the one hot vector of the known center word. parameters: $w_i$: word i from vocabulary V $V \\in R^{n\\times |V|}$ input word matrix $v_i$:i-th column of $V$, the input vector representation of word $w_i$ $U\\in R^{|V|\\times n}$: output word matrix $u_i$: i-th row of $U$, the output vector representation of word $w_i$ n is an arbitrary size which defines the size of our embedding space there are some differences with the figure….$W_1^{n\\times |V|}$, $W_2^{|V|\\times n}$ input : $x_1.shape = (|V|, 1)$, $x_2.shape = (|V|, 1)$,…,$x_{2m}.shape = (|V|, 1)$ $W_1$ : input matrix V, $W_1.shape = (n, |V|)$ each column is the representation of $w_i$ hidden layer: $\\hat v = \\dfrac{V.dot(x_1)+…+V.dot(x_{2m})}{2m}$, $\\hat v.shape = (n,1)$ $W_2$ : output matrix U, $W_2.shape = (|V|, n)$ each row is the representation of $w_i$ score: $u.shape = (|V|, 1)$ output: $\\hat y = softmax(u)$, $\\hat y.shape=(|V|,1)$ cross entropy: $$H(\\hat y, y) = -\\sum_{j=1}^{|V|}y_jlog(\\hat y_j)$$ Because y is the one hot vector, and i is the index whose value is 1. $$H(\\hat y, y) = -y_ilog(\\hat y_i)$$ look at the paper word2vec Parameter Learning Explained, it is very cautious, very wonderful!!! The symbols are different from the above. 4.2 one word context inference 4.3 one word context backpropagation 4.4 multi-words context 5. Skip-gram$$ \\begin{align} minimize J &amp;=-logP(w_{c-m},…,w_{c-1},w_{c+1},..,w_{c+m}|w_c)\\ &amp;=-log\\prod_{j=0,j\\neq m }^{2m}P(w_{c-m+j}|w_c)\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}logP(w_{c-m+j}|w_c)\\tag{3}\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}log\\dfrac{exp(u_{c-m+j}^Tv_c)}{\\sum_{k=1}^{|V|}exp(u_{k}^Tv_c)}\\tag{4}\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}u_{c-m+j}^Tv_c+2m\\ log\\sum_{k=1}^{|V|}exp(u_{k}^Tv_c) \\end{align} $$ important: from word to vector the (1) to (2), using the softmax to present the probability $$P(u_{c-m+j}|w_c) = \\dfrac{exp(u_{c-m+j}^Tv_c)}{\\sum_{j=1}^{|V|}exp(u_j^Tv_c)}\\tag{* }$$ V is the input matrix, U is the output matrix 5.1 We can use the simple neural networks to train matrix weights 5.2 inference and backpropagation Skip-gram treats each context word equally: the models computes the probability for each word of appearing in the context independently of its distance to the center word. 6. Optimizing Computational Efficiency6.1 Hirarchical Softmax 6.2 Negative Samplingloss function: $$E = -log\\sigma(v’^T_{w_O}h)-\\sum_{w_j\\in W_{neg}}log\\sigma(-v’^T_{w_j}h)$$ in the CBOW, $h=\\dfrac{1}{C}\\sum_{c=1}^Cv_{w_c^T}$ in the skip-gram, $h=v_{w_I}^T$ how to choose the K negative samples? As described in (Mikolov et al., 2013b), word2vec uses a unigram distribution raised to the 3/4th power for the best quality of results. 关于负采样的原理的理解： Intuitive explanation of Noise Contrastive Estimation (NCE) loss? The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead. For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of kk randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors. This is important: instead of predicting the next word (the “standard” training technique), the optimized classifier simply predicts whether a pair of words is good or bad. reference: word2vec Parameter Learning Explained word2vec原理推导与代码分析","link":"/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"},{"title":"cs224d-lecture11 再看GRU和NMT","text":"主要内容： GRU进一步理解 GRU和LSTM对比 LSTM的进一步理解 训练RNN的一些tips Ensemble MT Evaluation 生成词使用softmax导致的计算量过大的问题 presentation GRU 进一步理解shortcut connection adaptive shortcut connection 使用update gate 自适应的增加shortcut connection prune unnecessary connections adaptively 使用reset gate自适应的修剪不必要的连接。 突然想到个问题，为什么神经网络具有自适应性？我个人的理解是，神经网络是一个参数学习和拟合的过程，在梯度下降的过程中，模型得到优化使其具有自适应性。 question1:how you select the readable subset based on this reset gate? $$r_t=\\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})\\tag{reset gate}$$ $$\\tilde h_t=tanh(Wx_t+r_t\\circ Uh_{t-1})\\tag{new memory}$$ the reset gate decides which parts of the hidden state to read to update the hidden state. So, the reset gate calculates which parts to read based on the current input and the previous hidden state. So it’s gonna say, okay, I wanna pay a lot of attention to dimensions 7 and 52. And so, those are the ones and a little to others. And so those are the ones that will be being read here and used in the calculation of the new candidate update, which is then sort of mixed together with carrying on what you had before. 对此，Christopher老头儿还举了个例子，在隐藏状态中动词保存在47-52 dimensions,当遇到新的verb是，隐藏状态的这部分维度就会得到更新。看到这，真想试试打印出 $r_t$ 看看它随时间步的变化情况。。 question2:how you select the writable subset based on this update gate? $$u_t=\\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})\\tag{update gate}$$ $$h_t=(1-u_t)\\circ \\tilde h_t+u_t\\circ h_{t-1} \\tag{Hidden state}$$ some of the hidden state we’re just gonna carry on from the past. We’re only now going to edit part of the register. And saying part of the register, I guess is a lying and simplifying a bit, because really, you’ve got this vector of real numbers and some said the part of the register is 70% updating this dimension and 20% updating this dimension that values could be one or zero but normally they won’t be. So I choose the writable subset And then it’s that part of it that I’m then updating with my new candidate update which is then written back, adding on to it. And so both of those concepts in the gating, the one gate is selecting what to read for your candidate update. And the other gate is saying, which parts of the hidden state to overwrite? 感觉意思是，update gate主要是为了控制生成当前时间步的隐藏状态 $h_t$，如果更新门的值都是1，那就以为着保存所有的以前的信息。 question3:how does these gates avoid gradient vanishing? $$h_t=f(h_{t-1},x_t)=u_t \\circ \\tilde h_t + (1-u_t)\\circ h_{t-1}$$ the secret is this plus sign. 在回过头来看一下梯度消失的那个公式： $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}\\dfrac{\\partial h_t}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}$$ 举个例子，我们要求t=3时刻的损失函数对 $W_{hh}$ 的导数，那么： $$\\begin{align} \\dfrac{\\partial E_3}{\\partial W} &amp;=\\sum_{k=1}^3\\dfrac{\\partial E_3}{\\partial y_3}\\dfrac{\\partial y_3}{\\partial h_3}\\dfrac{\\partial h_3}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}\\ &amp;=\\dfrac{\\partial E_3}{\\partial y_3}\\dfrac{\\partial y_3}{\\partial h_3}(\\dfrac{\\partial h_3}{\\partial W}+\\dfrac{\\partial h_3}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial W}+\\dfrac{\\partial h_3}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial h_1}\\dfrac{\\partial h_1}{\\partial W}) \\end{align}$$ 可以看到很早之前的隐藏状态 $h_1$ 随着时间的增长，对当前时刻的影响越来越小。而在GRU中，当update gate $u_t=0$ 时，$h_3=h_2$,这说明之前的隐藏状态存储的信息能有效的传递下来， that’s why it can carry information for a very long time.。 question4:how long does a GRU actually end up remembering for? Answer: I kind of think order of magnitude the kind number you want in your head is 100 steps. So they don’t remember forever I think that’s something people also get wrong. question5:Does GRU train faster than lstm? Answer: LSTMs have a slight edge on speed. No huge difference. GRU和LSTM的区别 question6:LSTMs中为什么 $h_t=o_t\\circ tanh(c_t)$ 中要用到tanh？ TA Richard的解释是，对于 new memory cell $\\tilde c = f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$ 这是一个线性的layer，加上tanh非线性因素，能让lstm更powerful. LSTM 直观图解 可以说是很清楚了～～不过这里有点区别,将 $h_{t-1}$ 和 $x_t$ concat在一起了，比如三个gate: $$i_t = \\sigma (W_i[h_{t-1},x_t]+b_i)\\tag{input/update gate}$$ $$o_t = \\sigma (W_o[h_{t-1},x_t]+b_o)\\tag{output gate}$$ $$f_t = \\sigma (W_f[h_{t-1},x_t]+b_f)\\tag{forget gate}$$ 而更新的 new memory cell $\\tilde c_t$: $$\\tilde c_t=\\tanh(W_c[h_{t-1}, x_t]+b_c)$$ 最终的记忆细胞状态 $c_t$: $$c_t= f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$$ 最终的隐藏状态 $h_t$: $$h_t=o_t\\circ tanh(c_t)$$ LSTM的核心，类似于resnet: 用加和，也就是图中的plus sign，原本的rnn的仅有matrix multiply，使得网络具有 long dependency. 训练rnn的一些经验 第7点，万万不能dropout horizontally,那样会丢失很多信息。 Ensemble 之前看到在cnn里面，dropout可以看做是很多模型的集成，不知道rnn是否也可以。 MT Evaluation关于机器翻译的模型验证 notoriously tricky and subjective task，臭名昭著的棘手以及非常具有主观性。 BLEU: a Method for Automatic Evaluation of Machine Translation 原理： n-gram matches $p_n$ = # matched n-grams / # n-grams in candidate translation 其实就是 precision $p_n$ 表示 n-gram 的precision score. 并且，使用 $w_n=1/2^n$ 作为对应的权重。 brevity penalty：短译句容易得高分，因此需要给予惩罚 $$BP=\\begin{cases} 1, &amp; \\text{if c &gt; r}\\ e^{1-r/c}, &amp; \\text{if c $\\le r$} \\end{cases}$$ BLEU: $$BLEU=BP\\cdot exp(\\sum_{n=1}^Nw_nlogp_n)$$ 在对数域： $$log BLEU=min(1-\\dfrac{r}{c},0)+\\sum_{n=1}^Nw_nlogp_n$$ 又是softmax的问题 在每个时间步，从隐藏状态到词表，使用softmax这一步非常消耗计算力。 解决方法： Not GPU-friendly! 不知道为啥。。感觉需要好好了解下GPU 原论文： On Using Very Large Target Vocabulary for Neural Machine Translation Word and character-based models??? presentation","link":"/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/"},{"title":"cs224d-lecture14 Tree-RNN and Constituency Parsing","text":"主要内容： 语言的语义解释 如果将短语结构映射到向量空间中：利用语义的合成性 对比 RNN 和 CNN Recursive neural networks Parsing a sentence with an RNN 使用tree-rnn 进行分类： assignment3 情感分类 语言的语义解释–并不只是词向量 词向量只是词语级别的向量，人们可以用更大颗粒度的文本来表达自己的意思，而不仅仅是词袋中的某个单词。 比如: the country of my birth, the place where I was born Question: how can we represent the meaning of longer phrases? Answer: By mapping them into the same vector space. How should we map phrases into a vector space?利用语义的合成性： use principle of Compositionality the meanings of its words the rules that combine them 其实想想RNN也是将一个sentence或者是phrase压缩到一个向量中去。后面会介绍它们的区别。 通过同时学习句法树和复合性向量表示，就可以得到短语的向量表示了。 句法树： 句法树结构和向量表示Learn Structure and Representation： 问题是：我们真的需要学习这种树结构吗？Do we really need to learn this structure? 从两个角度来说明这个问题，一是对比recursive 和 rnn， 而是从语言的本质来解释。 Recursive vs. RNN Richard mentioned that the recurrent models are really sort of capturing representations of whole prefixes and you’re not getting any representations of smaller units than that. 两者都是递归神经网络，只不过前者在空间上递归，后者在时间上递归。中文有时会把后者翻译为“循环神经网络”，但这明显混淆了等级，令人误解。 它们各有各的优缺点，Recursive neural net需要分析器来得到句法树，而Recurrent neural net只能捕捉“前缀”“上文”无法捕捉更小的单位。 但人们还是更倾向于用后者，LSTM之类。因为训练Recursive neural net之前，你需要句法树；句法树是一个离散的决策结果，无法连续地影响损失函数，也就无法简单地利用反向传播训练Recursive neural net。另外，复杂的结构也导致Recursive neural net不易在GPU上优化。 语言本质是递归的吗？ 在认知科学上虽然有些争议，因为一般一个句子是有长度限制的，人们几乎从不说300个词以上的句子。但是递归是描述语言的最佳方式，比如 [The man from [the company that you spoke with about [the project] yesterday]] 这里面一个名词短语套一个名词短语，一级级下去。从实用的角度讲 1、通过递归地描述句子（句法树），可以有效地消歧： 2、便于指代相消等任务。 3、便于利用语法树结构（基于短语的机器翻译） 从 RNNs 到 CNNsRNN只会为满足语法的短语计算向量，而CNN为每个可能的短语计算向量。从语言学和认知科学的角度来讲，CNN并不合理。甚至recurrent neural network也比tree model和CNN更合理。 两者的关系可以这样想象，RNN将CNN捕捉的不是短语的部分删除了： 得到： So the sort of picture is that for the CNN, you’re sort of making a representation of every pair of words, every triple of words, every four words. Where as the tree recursive neural network is saying well some of those representations don’t correspond to a phrase and so we’re gonna delete them out. So that for the convolultional neural network, you have a representation for every bigram. So you have a representation for there speak and trigram there speak slowly. Whereas for the recursive neural network, you only have representations for the sort of semantically meaningful phrases like people there and speaks slowly going together to give a representation for the whole sentence. Recursive Neural Networks for Structure Prediction 输入： 两个子节点的向量表示 输出： 两个子节点合并后的新节点语义表示，以及新节点成立的分值 Recursive Neural Network Definition可以同时得到句法树和向量表示的一种任务。通过socre来得到句法树。 顺便提一下assignment3: 在 assignment3 是这样的tree-RNN $$h=relu([h^{(1)}{left},h^{(1)}{right}]W+b^{(1)})$$ $$\\hat y = softmax(h^{(1)}U+b^{(s)})$$ $L\\in R^{|V|\\times d},W^{(1)}\\in R^{2d\\times d}, b^{(1)}\\in R^{1\\times d}, U\\in R^{(d\\times 5)}, b^{(s)}\\in R^{1\\times 5}$ 在assignment3中用tree-rnn进行情感分析，是已经通过句法分析得到了句法树的，所以只需要从根节点开始，递归找到子节点，并计算出对应的向量表示，并归一化softmax，然后与每个节点（包括叶节点）真实标签对比，计算得到损失值。然后用梯度下降优化得到模型参数。 那么这里的参数怎么理解？在传统的rnn中 $W_{hh}$ 可以看做是隐藏状态转移矩阵，这里呢？？？关于 $W_{hh}$ 的理解，可以看知乎 HMM和RNN是什么关系？功效上两者有冲突重叠？ Parsing a sentence with an RNNgreedily incrementally building up parse structure. 计算任意两个单词合并的得分（虽然下图是相邻两个，但我觉得那只是绘图方便；就算是我第一次写的玩具级别的依存句法分析器，也是任意两个单词之间计算）： 然后贪心地选择得分最大的一对合并： 重复这一过程:计算任意两个节点，合并得分最大的一对 直到得到根节点： 模型中只有一个合成函数，使用同一个权值矩阵W处理NP、VP、PP……这明显是不合理的。 Max-Margin Framework-Details损失函数使用最大间隔 再回顾一下多分类支持向量机损失 Multiclass Support Vector Machine Loss 我的cs231n笔记 对于单个节点，可以看做单个样本损失 $$L_i=\\sum_{j\\ne y_j}^N max(0, s_j-(s_{y_i}-\\Delta))$$ 其中 $s_{y_j}$ 表示真实标签对应的值，非真实分类的得分不能超过 $s_{y_j}-\\Delta$，凡是超过的都会对 $L_i$ 产生影响。比这个值就没事～ 对于整个sentence： $$J=\\sum_imax(0, s(x_i,y_j)-max_{y\\in A(x_i)}(s(x_i,y)+\\Delta(y,y_i)))$$ $\\Delta$ 表示对所有非正确分类的惩罚 max 表示贪心搜索得到的syntactic tree的得分 有时候也可用beam search 使用 tree-RNN 进行分类任务这里的rnn指的是 递归recursive neural networks.空间结构上的递归，而以前学的RNN也是递归，不过是时间序列上的递归。 以assignment3中的情感分类任务为例，进行前向、反向传播推导。 由于前向传播时每个节点的信号来自所有子节点，所以梯度也来自所有子节点。并且前向传播时父节点的信号是利用子节点信号的拼接计算的，所以梯度需要针对子节点的信号计算： 这个问题其实在TensorFlow那一课已经讲过了，图计算：前向传播信号流入某节点，反向传播误差就得从某节点分流到所有源节点。树只是图的一个特例： Richard Socher 的代码比如softmax之类的可真熟练～ 123456789101112131415161718192021222324252627def forwardProp(self, node): # Recursive ... # This is node's hidden activation node.h = np.dot(self.W, np.hstack([node.left.h, node.right.h])) + self.b # [1,d] # Relu node.h[node.h&lt;0] = 0 # Softmax node.score = np.dot(self.Ws, node.h) + self.bs # [1, 5] node.score -= np.max(node.probs) node.probs = np.exp(node.score)/np.sum(np.exp(node.score)) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def backProp(self.node, error=None): # softmax`grad deltas = node.probs deltas[node.label] -= 1.0 self.dWs = np.outer(deltas, node.h) # Compute the outer product of two vectors. 训练时一个batch只有一个sentence，所有h是向量。 self.dbs += deltas # 对隐藏状态求导 dh = np.dot(self.Ws.T, deltas) # Add deltas from above if error is not None: dh += error # f'(z) now: # relu 反向传播 dh *= (node.h != 0) # Updata word vector if leaf node: # 如果是叶节点，h 就是 L，词向量. if node.isLeaf: self.dL[node.word] += deltas return # Recursively backProp # 如果当前节点不是叶节点，那么需要更新权重W和b，同时将error if not node.isLeaf: self.dW += np.outer(deltas, np.hstack([node.left.h, node.right.h])) self.db += deltas # Error signal to children dh = np.dot(self.W.T, dh) # 就是公式 h=relu([h^{(1)}_{left},h^{(1)}_{right}]W+b^{(1)}) # 递归计算左子节点，node.lef用来计算左子节点自身的损失， dh[:self.hiddenDim]用来计算父节点传递下来的损失 self.backProp(node.left, dh[:self.hiddenDim]) self.backProp(node.right, dh[self.hiddenDim:]) ok！完全弄懂了吧？！～ Syntactically-Untied RNN Version3 Presentation[Deep reinforcement learning for dialogue generation] reference CS224n笔记14 Tree RNN与短语句法分析 Recursive Neural Networks Can Learn Logical Semantics","link":"/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/"},{"title":"cs224d lecture16 dynamic Memory network","text":"主要内容： paper:Ask Me Anything: Dynamic Memory Networks for Natural Language Processing 是否所有NLP任务都可视作QA？ 在old-school NLP系统中，必须手工整理一个“知识库”；然后在这个知识库上做规则推断。这节课介绍的DMN完全不同于这种小作坊，它能够直接从问答语料中学习所有必要的知识表达。 DMN还可以在问答中做情感分析、词性标注和机器翻译。 所以构建一个joint model用于通用QA成为终极目标。 要实现这个目标，有两个障碍。 没有任何已有研究探讨过如何让单个模型学会这么多的任务。每种任务都有独特的特点，适合不同的神经网络来做： 第二个障碍 Fully joint multitask learning（同一个decoder/classifier，不仅仅共享词向量，而应该共享全部参数）非常困难。 有些不成功的研究发现，只能在低层（词向量）共享参数、如果任务之间没有直接关联则会顾此失彼。 感觉就是迁移学习无法应用于 nlp 上，同时应到到两个不同的任务上，然后对相同的参数进行训练，往往会得到很差的效果。 So if you’re trying to train two tasks together in one model, say you just have two softmaxes on the same Hidden state of your LSTMs. It turns to actually get worse in many cases, too. Dynamic Memory Networks今天介绍的DMN仅仅解决了第一个问题。虽然有些超参数还是得因任务而异，但总算是个通用的架构了。 回答特别难的问题 你无法记住全文，但看了问题之后，只要带着问题扫几眼原文，你就能找出答案。 这种现象启发了DMN。 Dynamic Memory Networks 先来看big picture（接下来会对每个模块单独讲解）： 主要有以下几个module： semantic memory module: 词向量 input module： 使用GRU或LSTM对原文进行encoder，每一个word对应一个hidden vector，同question mudule 共享 GRU 的权重参数 Question Module 和 Episodoc memory Module： 计算出一个Question Vector q，根据q应用attention机制，回顾input的不同时刻。根据attention强度的不同，忽略了一些input，而注意到另一些input。这些input进入Episodic Memory Module，注意到问题是关于足球位置的，那么所有与足球及位置的input被送入该模块。该模块每个隐藏状态输入Answer module，softmax得到答案序列。 attention 的过程实际上是一个 transitive reasoning 传递关系的过程： 比如上图中，问题是关于football的语义 question vector q，那么带着 q 回顾一遍原文，找到 John put down the football， 得到Episodic memory modelu的输出 $m^1$, 然后带着q 和 $m_1$ 再回顾一次原文，然后又找到 John moved to the bedroom 和 John went to the hallway， 计算得到 $m^2$. 具体怎么实现看接下来具体模块的讲解。 input Module 输入模块接受 $T_I$ 个输入单词，输出 $T_C$ 个“事实”的表示。如果输出是一系列词语，那么有 $T_C=T_I$；如果输出是一系列句子，那么约定 $T_C$ 表示句子的数量，$T_I$ 表示句子中单词的数量。我们使用简单的GRU读入句子，得到隐藏状态 $h_t=GRU(x_t,h_{t−1})$，其中 $x_t=L[w_t]$，L是embedding matrix，$w_t$ 是时刻 t 的词语。 事实上，还可以将这个Uni-GRU升级为Bi-GRU： Question Module Episodic Memory Mudule $$h_i^t=g_i^tGRU(s_i,h_{i-1}^t)+(1-g_i^t)h_{i-1}^t$$ 其中: $g_i^t$ is just a single scalar number. Should I pay attention to this sentence.也相当于一个gate机制，当 $g_i^t=0$ 时表示与 $s_i$ 无关。 上标t表示 $t^{th}$ time that we went over the entire input. 如何计算 $g_i^t$ ，也就是怎么判断当前迭代与input中的每个sentence是否相关。 相当简单和直接： sentence similarity: element-wise product or subtraction of sentence vector. 计算sentence相似性： $$z_i^t=[s_i\\circ q; s_i\\circ c^{t-1}; |s_i-q|; |s_i-m^{t-1}|]$$ 一个双层neural network: $$Z_i^t = W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)}$$ softmax计算当前迭代次数下每个sentence所占的比重： $$g_i^t=\\dfrac{exp(Z_i^t)}{\\sum_{k=1}^{M_i}exp(Z_i^t)}$$ Answer Module 相关工作有很多已有工作做了类似研究： Sequence to Sequence (Sutskever et al. 2014) Neural Turing Machines (Graves et al. 2014) Teaching Machines to Read and Comprehend (Hermann et al. 2015) Learning to Transduce with Unbounded Memory (Grefenstette 2015) Structured Memory for Neural Turing Machines (Wei Zhang 2015) Memory Networks (Weston et al. 2015) End to end memory networks (Sukhbaatar et al. 2015) 同 MemNet 对比 相同点 都有input, scoring, attention and response模块 不同点 MemNets对于input representations 使用词袋，然后有一些embedding去encode位置， DMN 使用 GRU MemNets迭代运行attention和response 这些不同点都是由于MemNets是个非sequence模型造成的。而DMN是个血统纯正的neural sequence model，天然适合序列标注等任务，比MemNets应用范围更广。 DMN的sequence能力来自GRU，虽然一开始用的是LSTM，后来发现GRU也能达到相同的效果，而且参数更少。（这回答了GRU和LSTM那节课有个学生的问题：哪个计算复杂度更低。Manning当时回答应该是一样的，还不太相信Richard的答案。说明在工程上，还是做实验的一线博士更有经验） Evaluation 这是一个自动生成的QA语料库，里面都是一些简单的问答。部分NLP学者很厌恶机器生成的语料，但如果连机器生成的语料都无法解决，何谈解决真实的复杂问题。 情感分析 依然拿到最高分数。 此时问题永远是相同的，其向量是固定的。 遗憾的是，对于不同的任务，超参数依然必须不同才能拿到最佳结果。 Episodes数量： 其中task3是三段论，理论只需要3个pass，但模型依然需要5个。考虑到这是个end to end训练，没有监督信号指示那些fact是重要的，所以这个表现还挺好。情感分析的NA是因为，计算复杂度实在太高了。分数已经在降低，所以干脆没跑。 情感分析的一些例子 VQA vision question answeringEverthing is Question Answering, 这也太酷了吧～ input module 不太一样，这里是通过CNN提取特征，将图像中的每一块区域用向量表示，然后作为GRU的输入。由于卷积特征并不是序列的，所以输入模块的输出特征只是所有时刻隐藏状态向量的拼接。 显然这需要很好的数据集啊。。 这就真的很吊了～！ 最高分。。 总结 参考 hankcs的博客CS224n笔记16 DMN与问答系统","link":"/2018/05/21/cs224d-lecture16-dynamic-neural-network/"},{"title":"cs224d-lecture10 机器翻译和注意力机制","text":"主要内容： Seq2Seq 基础模型 seq2seq encoder and decoder Attention Mechanisms: 介绍了三种attention Bahdanau et al. NMT model: 重点是怎么计算 context vector $c_i$ 前言对于NER标注，是通过previous words预测下一个word.还有一类NLP任务，是针对sequential output or outputs that are sequences of potentially varying length. 比如： Translation: taking a sentence in one language as input and outputting the same sentence in another language Conversation: taking a statement or question as input and responding to it. Summarization: taking a large body of text as input and outputting a summary of it. 这一节内容讲的就是根据一个基于深度学习的框架sequence-tosequence模型，用来处理序列生成的问题。 序列生成的历史方法: word-based system: 无法capture句子中的词序 phrase-based system: 无法解决长时间依赖的问题 Seq2Seq模型：can generate arbitrary output sequences after seeing the entire input. They can even focus in on specific parts of the input automatically to help generate a useful translation. sequence-to-sequence Basics Sutskever et al. 2014, “Sequence to Sequence Learning with Neural Networks” Seq2Seq-encoder encoder的目的就是将input sentence读入到模型中，并生成一个固定维度 context vector C. 显然，就一个任意长度的句子的信息压缩到一个固定维度的向量中，这是很困难的。所以encoder通常使用 stacked LSTMs. 通常会将sentence翻转作为输入，以机器翻译为例，翻转后输入的最后一个词对应的翻译，也就是output的第一个词。 明显感觉效果会不太好对吧，所以也就有了后来的attention 举个例子： input sentence：”what is your name” 那么得到的context vector 就是 a vector space representation of the notion of asking someone for their name. Seq2Seq-decoder decoder的目的是生成sentence，在最上面一层LSTM上接着softmax用来生成当前时间步的output词。然后用这个词作为下一个时间步的input word. 一旦得到output sentence,通过最小化交叉熵损失函数，来训练encoder和decoder中的参数。 Bidirectional RNNs Attention MechanismMotivation在seq2seq模型中，使用单一的 context vector：different parts of an input have different levels of significance. Moreover, different parts of the output may even consider different parts of the input “important.” 也就是说输入sentence中，每个词并不是具有同样的重要程度的。比如 “the ball is on the field”,显然”ball” “on” “field”比较重要。 而且，在输出的某一部分也可能更看中input中的某一部分。通常output中的前几个词主要却取决于input中的前几个词，output中的后几个词主要取决于input中的后几个词。 那么Attention mechanisms采用的方法是： providing the decoder network with a look at the entire input sequence at every decoding step; the decoder can then decide what input words are important at any point in time. 在decoder时采用注意力机制，确定在任何时刻生成词时输入序列中每个词的权重。 Bahdanau et al. NMT model原论文： Bahdanau et al. Neural Machine Translation by Jointly Learning to Align and Translate Decoder: General description decoder中生成下一个词的条件概率： $$P(y_i|y_1,…,y_{i-1},X)=g(y_{i-1},s_i,c_i)$$ 其中，当前时间步的隐藏状态 $s_i$ 表示为： $$s_i=f(s_{i-1},y_{i-1},c_i)$$ 也就是： i时刻生成此 $y_i$ 取决于 上一个生成词 $y_{i-1}$ (在生成序列时上一个时间步的输出是下一个时间步的输入) 和 i-1时刻的隐藏状态 $s_{i-1}$ 以及context vector对应的值 $c_i$. 重点是怎么计算每个时间步的context vector $c_i$： 在标准的seq2seq模型中，context vector只有一个，但在attention模型中，每个时间步都有单独的context vector $c_i$,它依赖于输入序列中的所有annotation $(h_1; · ·· ; h_{T_x})$,并赋予他们一定的权重。也就是： $$c_i=\\sum_{j=1}^{T_x}\\alpha_{ij}h_j$$ 其中i表示输出序列的第i时刻，j表示输入序列的第j个word的annotation. 其中对于每个输入词的annotation即 $h_j$ 的权重 $a_{ij}$ 是这么计算的： $$\\alpha_{ij}=\\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$$ 其中： $$e_{ij}=a(s_{i-1},h_j)$$ 是对其模型(alignment model),$s_{i-1}$ 表示输出序列的隐藏状态， $h_j$ 表示输入序列的隐藏状态，所以用来计算输入sentence中第j个位置和输出序列中第i个位置匹配的得分(score).这个得分是基于decoder中的前一个时刻的隐藏状态 $s_{i-1}$, 和输入序列中的第j个annotation $h_j$。 a是任意函数，且得到的值是R。比如可以是一个单层的全连接神经网络，得到了序列 $e_{i,1},…,e_{i,n}$， 然后使用softmax得到 $\\alpha_i=(\\alpha_{i,1},…,\\alpha_{i,n})$. 疑问：知道了 $e_{ij}$ 的意义，但不太明白怎么计算。。还没看论文，猜想既然a是任意函数，那么应该就是用神经网络来表示了。 总结下来： Let $\\alpha_{ij}$ be a probability that the target word yi is aligned to, or translated from, a source word $x_j$. Then, the i-th context vector $c_i$ is the expected annotation over all the annotations with probabilities $\\alpha_{ij}$. 所以 $\\alpha_{ij}$ 表示的就是从词 $x_j$ translate to (or align to) 词 $y_i$ 的概率。也就是说 output中第i个词 $y_i$ 可能由 input中的任意一个词对齐而来的，也不一定就是翻译，就是 翻译 $y_i$ 的时候，input中每一个对它的影响程度，也就是这个权重值 $\\alpha_{ij}$. 而这个概率 $\\alpha_{ij}$ 以及其 associated energy $e_{ij}$ 反映了 $s_{i-1}$ 和 $h_j$ 对生成下一个word的重要性。 疑问：在训练的时候可以通过反向传播，得到参数 $c_i$，但是这个权重也只能用于当前的序列吧。。测试的时候，这些训练的参数还能用么？ Encoder: bidirectional RNN for annotation sequences在encoder时，将输入序列编码为annotation $(h_1,h_2,…,h_{T_x})$. 为了既考虑preceding words，又考虑 following words，采用双向RNN（BiRNN）. forward RNN $\\overrightarrow f$ reads input sentence (from $x_1$ to $x_{T_x}$): $$(\\overrightarrow h_1,…,\\overrightarrow h_{T_x})$$ backward RNN $\\overleftarrow f$ reads input sentence in the reverse order (from $x_{T_x}$ to $x_1$): $$(\\overleftarrow h_1,…,\\overleftarrow h_{T_x})$$ annotation for $x_j$: $$h_j=[\\overrightarrow {h_{T_j}^T},\\overleftarrow {h_{T_j}^T}]$$ Connection with translation alignment在训练的decoder过程中，可以得到这样的一个alignment table, a table mapping words in the source to corresponding words in the target sentence.使用attention score $\\alpha_{i,j}$ 填充这个表格。 这就解决了之前的疑惑了，在测试的时候，context vector的权重 $\\alpha_i$ 直接通过查表得到～ Huong et al. NMT model原论文： Effective Approaches to Attentionbased Neural Machine Translation by Minh-Thang Luong, Hieu Pham an Christopher D. Manning Global attentionencoder 隐藏状态序列： $h_1,…,h_n$ ，n表示序列长度 decoder 隐藏状态序列： $\\overline h_1,…,\\overline h_n$ 对于每一个decoder中的隐藏状态 $\\overline h_i$，计算其基于所有encoder隐藏状态的attention vector $c_i$. $$ score(h_i^T\\overline h_j)=\\begin{cases} h_i^T\\overline h_j \\ h_i^TW\\overline h_j \\quad &amp; \\text{$\\in R$}\\ W[h_i,\\overline h_j] \\end{cases} $$ 类似于Bahdanau et al. NMT model中的 $e_{ij}$, 同样的需要得到的权重 $\\alpha_{ij}$ 是概率，也就是encoder中的 $h_i$ 与 decoder中的 $h_j$ 匹配的概率, attention vector $\\alpha_{i,j}$： $$\\alpha_{i,j}=\\dfrac{exp(score(h_i^T\\overline h_j))}{\\sum_{k=1}^nexp(score(h_k^T\\overline h_j))}$$ 那么context vector: $$c_i=\\sum_{j=1}^n \\alpha_{i,j}h_j$$ 那么使用context vector和隐藏状态 $\\overline h_i$ 生成新的decoder中第i时间步的新的 vector $$\\tilde h_i=f[\\overline h_i,c_i]$$ Local Attentionthe model predicts an aligned position in the input sequence. Then, it computes a context vector using a window centered on this position. The computational cost of this attention step is constant and does not explode with the length of the sentence. window怎么选？？ Christopher 好像说用强化学习。。 Google’s new NMT6 Johnson et el. 2016, “Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation” The new multilingual model not only improved their translation performance, it also enabled “zero-shot translation,” in which we can translate between two languages for which we have no translation training data. For instance, if we only had examples of Japanese-English translations and Korean-English translations, Google’s team found that the multilingual NMT system trained on this data could actually generate reasonable Japanese-Korean translations. The powerful implication of this finding is that part of the decoding process is not language-specific, and the model is in fact maintaining an internal representation of the input/output sentences independent of the actual languages involved. More advanced papers using attention Show, Attend and Tell: Neural Image Caption Generation with Visual Attention by Kelvin Xu, Jimmy Lei Ba,Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel and Yoshua Bengio. This paper learns words/image alignment. Modeling Coverage for Neural Machine Translation by Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu and Hang Li. Their model uses a coverage vector that takes into account the attention history to help future attention. Incorporating Structural Alignment Biases into an Attentional Neural Translation Model by Cohn, Hoang, Vymolova, Yao, Dyer, Haffari. This paper improves the attention by incorporating other traditional linguistic ideas. Sequence model decoders使用统计的方法找到最合适的sequence $\\hat s*$： $$\\hat s* = argmax_{\\hat s}(P(\\hat s|s))$$ Exhaustive search: NP问题 Ancestral sampling $$x_t \\sim P(x_t|x_1,..,x_n)$$ Greedy search $$x_t=argmax_{\\tilde x_t}P(\\tilde x_t|x_1,…,x_n)$$ 如果其中一步错了，对接下来的序列影响很大。 Beam search： the idea is to maintain K candidates at each time step. PresentationGoogle’s Multilingual Neural Machine Translation System: Enabling zero-short Translation","link":"/2018/05/08/cs224d-lecture10-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"title":"cs224d-lecture2-词向量的高级表示","text":"what do word2vec capture? window based coocurrence Matrix GloVe Intrinsic evaluation What do word2vec capture? Go through each word of the whole corpus predict surrounding words of each word word2vec captures coocurrence of words one at a time. Why not capture coocurrence counts directly? word2vec将窗口视作训练单位，每个窗口或者几个窗口都要进行一次参数更新。要知道，很多词串出现的频次是很高的。能不能遍历一遍语料，迅速得到结果呢？ 早在word2vec之前，就已经出现了很多得到词向量的方法，这些方法是基于统计共现矩阵的方法。如果在窗口级别上统计词性和语义共现，可以得到相似的词。如果在文档级别上统计，则会得到相似的文档（潜在语义分析LSA,Latent Semantic Analysis）。 Window based Co-occurrence Matrix 基于窗口的共现矩阵 Solution: Low dimensional vectors SVD的问题： 计算复杂度高：对n×m的矩阵是O(mn2) 不方便处理新词或新文档 与其他DL模型训练套路不同 Count based VS direct prediction 这些基于计数的方法在中小规模语料训练很快，有效地利用了统计信息。但用途受限于捕捉词语相似度，也无法拓展到大规模语料。 而NNLM, HLBL, RNN, Skip-gram/CBOW这类进行预测的模型必须遍历所有的窗口训练，也无法有效利用单词的全局统计信息。但它们显著地提高了上级NLP任务，其捕捉的不仅限于词语相似度。 2. Combining the beat of both words: GloveGloVe: Global Vectors for Word Representation Glove的原理： Using global statistics to predict the probability of word j appearing in the context of word i with a least squares objective. 即利用了词频统计的作用，又利用了word2vec中出现在同一个窗口的两个词的概率，用词向量做内积来表示。 在word2vec中 $$P(u_c|\\hat v) = \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{* }$$ 共现矩阵 Co-occurrence Matrix基于频率统计概率～ word $w_j$ 出现在中心词 $w_i$ 的上下文中的概率： $$P_{ij}=P(w_j|w_i)=\\dfrac{X_{ij}}{X_i}=\\dfrac{count(w_i,w_j)}{count(w_i)}$$ Least Squares Objective依据word2vec skip-gram中的原理，$w_j$ 出现在 $w_i$ 的上下文中的概率： $$Q_{ij}=P(w_j|w_i) = \\dfrac{exp(u_j^Tv_i)}{\\sum_{j=1}^{|V|}exp(u_j^Tv_i)}$$ 根据极大似然估计的负数形式： $$J=-\\sum_{i\\in corpus}\\sum_{j\\in context(i)}Q_{ij}$$ 对于共现词 $w_i, w_j$ 同时出现多次，因此可以将上式简化为： $$J=-\\sum_{i=1}^V\\sum_{j=1}^VX_{ij}Q_{ij}$$ 我们知道 $Q_{ij}$ 是表示经过softmax归一化之后的概率，需要遍历整个corpus，这会导致很大的计算量。因此，我们用最小二乘法来作为目标函数： $$\\hat J = \\sum_{i=1}^V\\sum_{j=1}^VX_i(\\hat P_{ij}-\\hat Q_{ij})$$ 其中 $\\hat P_{ij}=X_{ij}$ 和 $\\hat Q_{ij}=exp(u_j^Tv_i)$ 都表示未归一化分布。这里其实做了一个近似～～～将softmax中的很难计算的量 $\\sum_{j=1}^Vexp(u_j^Tv_i)$ 近似成了 $X_i$ 通常语料库较大的情况下 $X_{ij}$ 都会很大，这会使得优化变得困难。一个有效的方法是最小化它们俩的对数形式： $$\\begin{align} \\hat J&amp;=\\sum_{i=1}^V\\sum_{j=1}^V(log(\\hat P)_ {ij}-log(\\hat Q)_ {ij})\\ &amp;=\\sum_{i=1}^V\\sum_{j=1}^VX_i(u_j^Tv_i-logX_{ij})^2 \\end{align}$$ Glove的优点： 训练迅速：也需要遍历整个语料库，但是计算每一个词的概率时并不需要像word2vec那样消耗softmax那么大的计算量 scalable to huge corpora 可拓展性 对于较小的语料库和向量也有很好的性能 3. Intrinsic evaluation3.1 Word Vector Analogies 语义 semantic information： 句法结构 syntactic structure: 3.2 Intrinsic Evaluation Tuning Example: Analogy Evaluations需要调节的超参数： 词向量的维度 dimension of word vectors 语料库的大小 corpus size 语料库的种类 corpus source/type 上下文窗口大小 context window size 上下文对称性 context symmetry 3.4 Further Reading: Dealing With AmbiguityImproving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al, 2012) 4. Extrinsic Taskssentiment, named-entity recognition(NER), given a context and a central word, than classify the central word to be one of many classes. 4.1 retraining word VectorsIf we retrain word vectors using the extrinsic task, we need to ensure that the training set is large enough to cover most words from the vocabulary. presentationLinear algebraic structure of word senses with applications to polysemy reference: CS224n笔记3 高级词向量表示","link":"/2018/04/30/cs224d-lecture2-%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E9%AB%98%E7%BA%A7%E8%A1%A8%E7%A4%BA/"},{"title":"cs224d-lecture3 基于Window的分类与神经网络","text":"分类问题 window classification 如何自己开始一个项目 分类问题sentiment, named-entity recognition(NER)都可以看作是分类问题。给定一个词的词向量，预测其所属的类。这与传统的监督学习都是一样的～ $${x^{(i)},y^{(i)}}_1^N$$ 其中 $x^{(i)}$ 是一个 d-维向量，$y^(i)$ 是一个 C-维one-hot向量，N是总数。 softmaxsoftmax: $$p(y_i=1|x)=\\dfrac{exp(W_jx)}{\\sum_{c=1}^Cexp(W_cx)}$$ softmax与交叉熵损失训练时可以直接最小化正确类别的概率的负对数： $$-log(\\dfrac{exp(W_kx)}{\\sum_{c=1}^Cexp(W_cx)})$$ 其实这个损失函数等效于交叉熵 $H(\\hat y, y)=-logy_ilog(\\hat y)$,其中y是one-hot向量。 对于N个数据点 $$-\\sum_{i = 1}^N\\log \\bigg(\\frac{\\exp(W_{k{(i)}\\cdot}x^{(i)})}{\\sum_{c=1}^C\\exp(W_{c\\cdot}x^{(i)})}\\bigg)$$ 加上正则化： $$-\\sum_{i = 1}^N\\log \\bigg(\\frac{\\exp(W_{k{(i)}\\cdot}x^{(i)})}{\\sum_{c=1}^C\\exp(W_{c\\cdot}x^{(i)})}\\bigg) + \\lambda \\sum_{k=1}^{C\\cdot d + |V|\\cdot d} \\theta_k^2$$ 在传统的机器学习中，我们只需要训练权重参数即可。但在这里我们还可以以重新训练词向量中的权重参数。那么需要训练的参数数量：Nxd+dxV retrain embedding但当你的训练集很小时，可能会使词向量失去泛化效果。 This is because Word2Vec or GloVe produce semantically related words to be located in the same part of the word space. When we retrain these words over a small set of the vocabulary, these words are shifted in the word space and as a result, the performance over the final task could actually reduce. Window classification通过监督学习来对一个single word进行分类显然是不符合自然语言的特性的。因为一个word具有多义性和多词性。需要结合上下文来判断。 用$X^{i}_{window}$ 代替单个词作为输入 $W_i$ cs224d-lecture4 反向传播和项目指导Project QA: A neural network for Factoid Question Answering over Paragraph sentiment: http://nlp.standford.edu/sentiment/ 接下来都是围绕着课程项目的指导与建议，就不啰嗦了。简单写写一些体会： 不要想着一上来就发明个新模型搞个大新闻 也不要浪费大部分时间在爬虫上面，本末倒置 把旧模型用于新领域\\新数据也是不错的项目 先要按部就班地熟悉数据、熟悉评测标准、实现基线方法 再根据基线方法的不足之处思考深度学习如何能带来改进 再实现一个已有的较为前沿的模型 观察该模型犯的错误，思考如何改进 这时才能没准就福至心灵发明一个新方法","link":"/2018/04/30/cs224d-lecture3-Word-Window%E5%88%86%E7%B1%BB%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"cs224d-lecture13 卷积神经网络","text":"主要内容： 为什么要使用CNN single layer connection pooling: max-pooling Multiple-Filters: multiple n-grams Multiple-Channels: 两个词向量 static, dynamic lassification after one CNN layer 为什么要使用CNN？CNN模型在计算机视觉与语音识别方面取得了卓越的成就. 在 NLP 也是可以的. 卷积具有局部特征提取的功能, 所以可用 CNN 来提取句子中类似 n-gram 的关键信息. Single layer connectionConvolution Neural Networks for Sentence Classification 对于单个词，其词向量：$x_i\\in R^k$, k维词向量 n个词concatenate在一起表示为：$x_{1:n}=x_1\\bigoplus x_2 \\bigoplus…\\bigoplus x_n$ 卷积过滤器filter: $w\\in R^{hk}$ h表示过滤器的尺寸，也就是覆盖多少个词. 上图figure13表示： k=2,n=5,h=3 需要注意的是filter是一个长度为 hk 的 vector，与input的每一个时间步做一次卷积（加权和）得到一个实数: $$c_i=f(W^Tx_{i:i|h-1}+b)$$ 那么输出向量：$c=[c_1,c_2,…,c_{n-h+1}]\\in R^{n-h+1}$ 如果h=3，那么最后两个词 my birth 就不够卷积，可以如图figure14所示采用 h-1 zero-vector padding. poolingmax-pooling: $$\\hat c=max{c}, c\\in R$$ filter可以看做和图像中的filter一样，一个图像特征提取器，那么在文本中，一个filter也可以看做是一个n-gram特征提取器，比如一个用来表示positive bigram,那么这个filter和句子中同样表示positive bigram做卷积的话，其值就会很大～那么使用max-pooling就是找到这个值 当然使用min-pooling也是可以的.但更多的时候我们选择用 relu作为激活函数，那么使用min-pooling的话，就会出现更多的 0. Multiple-FiltersWe can use multiple bi-gram filters because each filter will learn to recognize a different kind of bi-gram. Even more generally, we are not restricted to using just bi-grams, we can also have filters using tri-grams, quad-grams and even higher lengths. Each filter has an associated max-pool layer. Thus, our final output from the CNN layers will be a vector having length equal to the number of filters. 同时使用好几个bigram，用以获取不同的pattern. 除了bigram，还会使用trigram,unigram等等。 Multiple-Channels我们有时候会需要在特定的场景下更新词向量，也就是也训练词向量参数，这样能够适应更特殊的任务。但如果在test中出现了train中没有出现的词unseen word， 那么这个词的词向量还会保持初始词向量的值（Glove等）。但是与这个词语义相关的词的词向量却发生了变化，这样就造成了相似的词的的词向量相差较大。这显然是不合理的。 所以我们使用两个词向量，one ’static’ (no gradient flow into them) and one ’dynamic’, which are updated via SGD. Backprop into only one set, keep other “static” Both channels are added to $c_i$ before max-pooling. Classification after one CNN layer First one convolution, followed by one max-pooling To obtain final feature vector: $z=[\\hat c_1,…,\\hat c_m]$ 假设有m个过滤器filter Simple final softmax layer $y=softmax(W^{(S)}z+b)$ Convolution Neural Networks for Sentence Classification 论文中的模型： 第一层： two word-vector channels $n\\times k\\times 2$ 第二层： m个filter得到的m列feature maps，由于有多种filter尺寸，如果没有zero-padding的话，那么得到的feature maps长度是不一致的。 第三层：max-pooling $z=[\\hat c_1,…,\\hat c_m]$ 第四层：fully connectioned layer with dropout and softmax output. Tricks: Dropout$$y=softmax(W^{(S)}(r\\circ z)+b)$$ 针对dropout，在train和test时，处理方式是不一样的： 模型参数选择问题 CNN更容易实现在GPU上的并行处理。 CNN的变种以及应用 Narrow vs Wide Narrow就是没有zero-padding: 那么output长度就是 $n-h+1$ Wide就是前后两端都有h-1 zero-padding: 那么output长度就是： $[n+2\\times (h-1)]- h+1=n+h-1$ k-max pooling 相比max-pooling， k-max pooling是选出最大的k个值 模型对比总结 PresentationCharacter-Aware neural language models,Yon Kim at al.","link":"/2018/05/14/cs224d-lecture13-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"docker学习和使用","text":"docker 安装docker tensorflowdocker 镜像迁移有时候需要将镜像从一台服务器移动到另外一台服务器。可使用 nc 进行大文件传输 docker 文件映射1234567sudo docker run -itd -p 8888:8888 -v container_path:/usr/home/wuwei7/tmp_data image_id /bin/bashsudo docker exec -it container_id /bin/bash -v 参数表示容器内文件与宿主机器文件映射 docker 驻守状态1234567sudo docker run -itd -p 8888:8888 -v container_path:/usr/home/wuwei7/tmp_data image_id /bin/bashsudo docker exec -it container_id /bin/bash -itd 表示驻守状态 exec 退出之后容器依然在运行 docker 常用命令123456789101112131415sudo docker ps 查看正在运行的容器sudo docker images 查看镜像sudo docker rm -rf Container_ID 删除正在运行的容器sudo docker rmi -f Image_ID 删除镜像，-f 强制删除在容器中运行的镜像","link":"/2019/02/21/docker%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BD%BF%E7%94%A8/"},{"title":"ctc loss and decoder","text":"想要详细的了解ctc loss，建议直接转至大佬的博客 $\\rightarrow$ https://xiaodu.io/ctc-explained/. 我个人写这个笔记的目的在于最近要对ctc loss进行魔改时，发现之前的细节都忘了。所以按照自己已有的基础上重新整理了一遍。除此之外，大佬的博客里面并没有代码解析。这里有ctc loss 和 ctc decode的python代码实现，所以想要对ctc loss进行魔改的，可以再过一遍我这篇文章~Why ctc loss, ctc loss vs cross entropy 现实中有很多任务都可以看作是序列到序列的对齐训练。主要可以分为两类： NLP领域常见的机器翻译和对话。对于这类任务，在训练阶段，我们通常使用cross-entropy + teacher forcing来训练模型。这类任务的特点是源序列和目标序列没有严格的对齐关系。他们本质上可以看作是 conditional language model. 也就是目标序列作为条件语言模型，更看重连贯性、流利度，其次是与源序列的对应关系（所以他们会有多样性研究）。 识别领域常见的语音识别，OCR，手语识别。对于这类任务，我们则主要使用ctc loss作为损失函数来训练模型。这类任务的特点是源序列和目标序列有严格的对齐关系。对于语音或手语，目标序列有语言模型的特点，但是更看重与源序列的准确的对应关系。第二类任务其实也可以用cross entropy来训练，但是往往效果不太好。我们可以发现，对于第二类任务，最理想的情况是将源序列先进行分割，这样单独的对某一个音节，手语或者字符进行识别，准确率就会很高了。但是现实是，源序列更多的是未分割的情况。针对这类任务，[Alex Graves, 2006] 提出了Connectionist Temporal Classification. 使用ctc进行训练有两个要求： 源序列长度 &gt;&gt; 目标序列长度 源序列的order与目标序列的order一致，且存在顺序对齐的关系 ctc training如何计算 ctc loss, 这篇博客CTC Algorithm Explained Part 1写的非常非常赞(以下简称为ctc explain blog)，细节看这个就好了。 这里简单的概括下思想。 给定源序列 $X={x_1,x_2,..,x_T}$ 和目标序列 $Y={y_1,..y_N}$ . 其中 $T&gt;&gt;N$.根据极大似然估计原理，我们的目标是找到使得 P(Y|X;W) 最大化的W。这里 X,Y 存在多对一的关系。我们假设存在这样的路径 $\\pi={\\pi_1, …,\\pi_T}$, $\\pi_i\\in |V’=V+blank|$ 与源序列一一对应（V是词表）。并且存在这样的映射关系 $\\beta(\\pi) = Y$ , 其映射法则就是去重和去掉blank（这个映射法则有其物理意义：就是一个音节或手语动作会包含多帧，以及存在中间停顿或无意义帧等情况.） 因此，优化的目标模型可以转换为：$P(Y|X)=\\sum_{\\pi\\in \\beta^{-1}(Y)}P(\\pi|X;W)$ 所以我们的目标现在转换成了最大化满足 $\\pi\\in \\beta^{-1}(Y)$ 的所有路径的概率。现在问题就转变成如何找到 Y 对应的所有路径。这是一个动态规划的问题。Graves 根据HMM的前向后向算法，利用动态规划的方法来求解。根据目标序列Y和X的帧数构建一个表格： 纵轴是将目标序列扩展为前后都有blank的序列 $l’=(-, l_1, -, l_2,…,- ,l_N, -)$ 。如果 T=N 时，那么Y和X就是一一对应了。X的序列越长，这个表格的搜索空间越大，存在的可能的路径就越多。 如何找到所有的合法路径，先定义路径规则，然后找到递归条件便能通过动态规划的方法解决，具体细节参见ctc explain blog. 路径规则： 转换只能往右下方向，其他方向不允许 相同的字符之间至少有一个空格 非空字符不能被跳过（不然最终就不是apple了 起点必须从前两个字符开始 重点必须落在结尾两个字符 这里以前向算法为例来解释： 其中的符号：x表示输入序列，z表示输出序列，s表示纵轴的节点(2T+1个)初始条件， t=1时刻只能是 blank 或 $l^{‘}_ {2}$ $\\alpha_1(1)=y_{-}^1$ 表示 t=1 时刻为blank的概率. $\\alpha_1(2)=y_{l_2’}^1$ 表示 t=1 时刻为s中的第二个节点 s_2 的概率，也就是输出序列的第一个节点 $\\alpha_1(s)=0, \\forall s&gt;2$ 表示t=1时刻其他节点概率为0 $\\alpha_t(s)=0, \\forall s &lt; |l’|-2(T-t)-1$ 对于任何时刻都有部分节点是完全不可能的 t=T 时刻，只有最后两个节点可行。 t=0 时刻，对于节点 $s&lt;|l’|-2T -1(|l’|=2N+1)$ . 其概率为 0. 如果输入序列的长度 T=N（与label等长），则 $|l’|=2T+1$ . 一般情况下 T&gt;&gt;N，s&lt;0 0&lt;t&lt;T 时刻，以特例 T=N 为例， $s&lt;2N+1-2N+2t -1 \\rightarrow s&lt;2t$ . 也就是 s&lt;2t 的节点概率都为0. 前向递推公式，如果 t 时刻为 s 节点，那么 t-1 时刻可能的节点与 s 节点是否为 blank 有关。 如果s节点为blank. 不能跳过非空字符，所以 $\\alpha_t$ 仅依赖 $\\alpha_{t-1}(s)$, $\\alpha_{t-1}(s-1)$ , 不依赖于 $\\alpha_{t-1}(s-2)$ 如果 s = s-2. 相同字符之间必须有空格。公式同上。 不属于上述两种情况， $\\alpha_t$ 也能依赖于 $\\alpha_{t-1}(s-2)$ 最终通过公式计算loss, 通过迭代的方法计算T时刻最后两个节点的概率:$-ln(p(l|x)) = -ln(\\alpha_T(|l’|) + \\alpha_T(|l’|-1))$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232代码解析import numpy as npfrom six.moves import xrangeimport numpy as npimport editDistance as edimport heapq as hqfrom six.moves import xrangedef ctc_loss(params, seq, blank=0, is_prob=True): &quot;&quot;&quot; params: [vocab_size, T], logits.softmax(-1). T 是输入序列的长度，vocab_size是词表大小。 seq: [seq_len] 输出序列的长度。 CTC loss function. params - n x m matrix of n-D probability distributions over m frames. seq - sequence of phone id's for given example. is_prob - whether params have already passed through a softmax Returns objective and gradient. &quot;&quot;&quot; seqLen = seq.shape[0] # Length of label sequence (# phones) numphones = params.shape[0] # Number of labels L = 2 * seqLen + 1 # Length of label sequence with blanks, 拓展后的 l'. T = params.shape[1] # Length of utterance (time) # 建立表格 l' x T. alphas = np.zeros((L, T)) # 前向概率 betas = np.zeros((L, T)) # 后向概率 # 这里dp的map： # 横轴为 2*seq_len+1, 也就是 ground truth label中每个token前后插入 blank # 纵轴是 T frames # logits 转换为概率 if not is_prob: # if not probs, params is logits without softmax. params = params - np.max(params, axis=0) params = np.exp(params) params = params / np.sum(params, axis=0) # Initialize alphas and forward pass # 初始条件：T=0时，只能为 blank 或 seq[0] alphas[0, 0] = params[blank, 0] alphas[1, 0] = params[seq[0], 0] # T=0， alpha[:, 0] 其他的全部为 0 c = np.sum(alphas[:, 0]) alphas[:, 0] = alphas[:, 0] / c # 这里 T=0 时刻所有可能节点的概率要归一化 llForward = np.log(c) # 转换为log域 for t in xrange(1, T): # 第一个循环： 计算每个时刻所有可能节点的概率和 start = max(0, L - 2 * (T - t)) # 对于时刻 t, 其可能的节点.与公式2一致。 end = min(2 * t + 2, L) # 对于时刻 t，最大节点范围不可能超过 2t+2 for s in xrange(start, L): l = (s - 1) / 2 # blank，节点s在偶数位置，意味着s为blank if s % 2 == 0: if s == 0: # 初始位置，单独讨论 alphas[s, t] = alphas[s, t - 1] * params[blank, t] else: alphas[s, t] = (alphas[s, t - 1] + alphas[s - 1, t - 1]) * params[blank, t] # s为奇数，非空 # l = (s-1/2) 就是 s 所对应的 lable 中的字符。 # ((s-2)-1)/2 = (s-1)/2-1 = l-1 就是 s-2 对应的lable中的字符 elif s == 1 or seq[l] == seq[l - 1]: alphas[s, t] = (alphas[s, t - 1] + alphas[s - 1, t - 1]) * params[seq[l], t] else: alphas[s, t] = (alphas[s, t - 1] + alphas[s - 1, t - 1] + alphas[s - 2, t - 1]) \\ * params[seq[l], t] # normalize at current time (prevent underflow) c = np.sum(alphas[start:end, t]) alphas[start:end, t] = alphas[start:end, t] / c llForward += np.log(c) return llForwardctc_beam_search 解码def ctc_beam_search_decode(probs, beam_size=5, blank=0): &quot;&quot;&quot; :param probs: The output probabilities (e.g. post-softmax) for each time step. Should be an array of shape (time x output dim). :param beam: :param blank: :return: &quot;&quot;&quot; # T表示时间，S表示词表大小 T, S = probs.shape # 求概率的对数 probs = np.log(probs) # Elements in the beam are (prefix, (p_blank, p_no_blank)) # Initialize the beam with the empty sequence, a probability of # 1 for ending in blank and zero for ending in non-blank # (in log space). # 每次总是保留beam_size条路径 beam = [(tuple(), ((0.0, NEG_INF), tuple()))] for t in range(T): # Loop over time # A default dictionary to store the next step candidates. next_beam = make_new_beam() for s in range(S): # Loop over vocab # print(s) p = probs[t, s] # t时刻，符号为s的概率 # The variables p_b and p_nb are respectively the # probabilities for the prefix given that it ends in a # blank and does not end in a blank at this time step. for prefix, ((p_b, p_nb), prefix_p) in beam: # Loop over beam # p_b表示前缀最后一个是blank的概率，p_nb是前缀最后一个非blank的概率 # If we propose a blank the prefix doesn't change. # Only the probability of ending in blank gets updated. if s == blank: # 增加的字母是blank # 先取出对应prefix的两个概率，然后更后缀为blank的概率n_p_b (n_p_b, n_p_nb), _ = next_beam[prefix] # -inf, -inf n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p) # 更新后缀为blank的概率 next_beam[prefix] = ((n_p_b, n_p_nb), prefix_p) # s=blank， prefix不更新，因为blank要去掉的。 # print(next_beam[prefix]) continue # Extend the prefix by the new character s and add it to # the beam. Only the probability of not ending in blank # gets updated. end_t = prefix[-1] if prefix else None n_prefix = prefix + (s,) # 更新 prefix, 它是一个tuple n_prefix_p = prefix_p + (p,) # 先取出对应 n_prefix 的两个概率, 这个是更新了blank概率之后的 new 概率 (n_p_b, n_p_nb), _ = next_beam[n_prefix] # -inf, -inf if s != end_t: # 如果s不和上一个不重复，则更新非空格的概率 n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p) else: # 如果s和上一个重复，也要更新非空格的概率 # We don't include the previous probability of not ending # in blank (p_nb) if s is repeated at the end. The CTC # algorithm merges characters not separated by a blank. n_p_nb = logsumexp(n_p_nb, p_b + p) # If s is repeated at the end we also update the unchanged # prefix. This is the merging case. if s == end_t: (n_p_b, n_p_nb), n_prefix_p = next_beam[prefix] n_p_nb = logsumexp(n_p_nb, p_nb + p) # 如果是s=end_t，则prefix不更新 next_beam[prefix] = ((n_p_b, n_p_nb), n_prefix_p) else: # *NB* this would be a good place to include an LM score. next_beam[n_prefix] = ((n_p_b, n_p_nb), n_prefix_p) # print(t, next_beam.keys()) # Sort and trim the beam before moving on to the # next time-step. # 根据概率进行排序，每次保留概率最高的beam_size条路径 beam = sorted(next_beam.items(), key=lambda x: logsumexp(*x[1][0]), reverse=True) beam = beam[:beam_size] # best = beam[0] # return best[0], -logsumexp(*best[1][0]), best[1][1] pred_lens = [len(beam[i][0]) for i in range(beam_size)] max_len = max(pred_lens) pred_seq, scores, pred_pobs = np.zeros((beam_size, max_len), dtype=np.int32), \\ [], np.zeros((beam_size, max_len)) for bs in range(beam_size): pred_seq[bs][:pred_lens[bs]] = beam[bs][0] scores.append(-logsumexp(*beam[bs][1][0])) pred_pobs[bs][:pred_lens[bs]] = np.exp(beam[bs][1][1]) return pred_seq, scores, pred_pobs# 因为代码中为了避免数据下溢，都采用的是对数概率，所以看起来比较繁琐def logsumexp(*args): &quot;&quot;&quot; Stable log sum exp. &quot;&quot;&quot; if all(a == NEG_INF for a in args): return NEG_INF a_max = max(args) lsp = math.log(sum(math.exp(a - a_max) for a in args)) # 概率相加再取log，为避免数值下溢 return a_max + lsp# 创建一个新的beamdef make_new_beam(): fn = lambda: ((NEG_INF, NEG_INF), tuple()) return collections.defaultdict(fn)if __name__ == &quot;__main__&quot;: import ctcdecode, time np.random.seed(3) seq_len = 50 output_dim = 20 probs = np.random.rand(seq_len, output_dim) # probs = np.random.rand(time, output_dim) # probs = np.random.rand(time, output_dim) probs = probs / np.sum(probs, axis=1, keepdims=True) start_time = time.time() labels, score, labels_p = MPGenerate.ctc_beam_search_decode(probs, beam_size=5, blank=0) print(&quot;labels:&quot;, labels[0], len(labels[0])) print(&quot;labels_p: &quot;, labels_p[0], len(labels_p[0])) print(&quot;Score {:.3f}&quot;.format(score[0])) print(&quot;First method time: &quot;, time.time() - start_time) dec_logits = torch.FloatTensor(probs).unsqueeze(0) len_video = torch.LongTensor([seq_len]) decoder_vocab = [chr(x) for x in range(20000, 20000 + output_dim)] second_time = time.time() decoder = ctcdecode.CTCBeamDecoder(decoder_vocab, beam_width=5, blank_id=0, num_processes=10) pred_seq, scores, _, out_seq_len = decoder.decode(dec_logits, len_video) # pred_seq: [batch, beam, length] # out_seq_len: [batch, beam] print(pred_seq[0, 0, :][:out_seq_len[0, 0]]) print(out_seq_len[0, 0]) print(&quot;Score {:.3f}&quot;.format(scores[0, 0])) print(&quot;Second method time: &quot;, time.time() - second_time) References Sequence Modeling With CTC, Awni HannunCTC算法详解之训练篇 CTC Algorithm Explained Part 2：Decoding the Network（CTC算法详解之解码篇","link":"/2020/09/13/ctc-loss-and-decoder/"},{"title":"cs224d-lecture9 机器翻译","text":"主要内容： RNN Translation Model GRU LSTM Towards a Better Language Modeling RNN Translation Model encoder: $$h_t=\\phi(h_{t-1},x_t)=f(W^{(hh)}h_{t-1}+W^{(hx)}x_t)$$ encoder: $$h_t=\\phi(h_{t-1})=f(W^{(hh)}h_{t-1})$$ $$y_t=softmax(W^{(S)}h_t)$$ corss entropy function: $$max_{\\theta}\\dfrac{1}{N}\\sum_{n=1}^Nlogp_{\\theta}(y^{(n)}|x^{(n)})$$ rnn的几点扩展1.在encoder和decoder中，$W^{(hh)}$ 是不一样的 2.计算decoder中的隐藏神经元时，可以不仅仅只使用上一个隐藏层的信息，而是使用三种input来获取更多的信息 The previous hidden state (standard) Last hidden layer of the encoder (c = hT) Previous predicted output word, $y^{t−1}$ $$h_t=\\phi(h_{t-1},c,y_{t-1})$$ 3.使用deep rnn: 这需要更大的语料库 4.使用bi-directional encoder 5.颠倒词序进行训练 rnn 到底做了什么？we never gave this model an explicit grammar for the source language, or the target language, right? It’s essentially trying, in some really deep, clever, continuous function, general function approximation kind of way, just correlation, basically, right? And it doesn’t have to know the grammar, but as long as you’re consistent and you just reverse every sequence, the same way. It’s still grammatical if you read it from the other side. And the model reads it from potentially both sides, and so on. RNN的缺点我们知道在传统的神经网络传递中$a^{} = g(W_{a}\\cdot[a{},x{}] + b_a)$, 很容易造成梯度消失，并且神经网络不擅长处理长期依赖的问题。以语言模型为例，即序列很难反向传播到比较靠前的部分，也就难以调整序列前面的计算。 GRU(gated recurrent units)原论文：https://arxiv.org/pdf/1406.1078v3.pdf 这个图画的算是很好了的吧。。但是还是复杂了一点，必须对着公式才能看懂。可以看简化图： $$r_t=\\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})\\tag{reset gate}$$ $$u_t=\\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})\\tag{update gate}$$ $$\\tilde h_t=tanh(Wx_t+r_t\\circ Uh_{t-1})\\tag{new memory}$$ $$h_t=(1-u_t)\\circ \\tilde h_t+u_t\\circ h_{t-1} \\tag{Hidden state}$$ 主要就是两个gate： 重置门r：决定了如何将新的输入信息与前面的记忆相结合。所以它的作用对象是 $\\tilde h_t$ 也就是new memory cell. 更新门u：定义了前面记忆保存到当前时间步的量。所以它的作用对象是 $h_t$.也就是当前memory cell保存 $h_{t-1}$ 和 $\\tilde h_t$ 多少信息量。 GRU 背后的原理： 如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。 如果重置门设为0，那么将忽视之前的隐藏状态，这意味着模型可以丢掉之前的信息，当它们与未来的信息不相关时。 更新门u控制着过去的状态对现在的影响。If z close to 1, then we can copy information in that units through many time steps!这意味着 Less vanishing gradient! Units with short-term dependencies often have reset gate very active. 这几句总结可以说是道出了GRU的精髓了！ 仍需要理解的几个问题： 激活函数为什么是tanh ,sigmoid，并不能像概率图模型那样，用数学来解释，就是很玄学吧。。 这篇文章写的不错～经典必读：门控循环单元（GRU）的基本概念与原理 LSTM 这个图有点抽象。 这个图是来自Ng的课，将图中的 $a^{&lt; t &gt;}$ 换成 $h_t$ 就可以了～ 三个gate以及新的记忆细胞，三个sigmoid和一个tanh $$i_t = \\sigma(W^{(i)}x_t+U^{(i)}h_{t-1})\\tag{Input\\update gate}$$ $$f_t = \\sigma(W^{(f)}x_t+U^{(f)}h_{t-1})\\tag{forget gate}$$ $$o_t = \\sigma(W^{(o)}x_t+U^{(o)}h_{t-1})\\tag{Output/Exposure gate}$$ $$\\tilde c_t = tanh(W^{(c)}x_t+U^{(c)}h_{t-1})\\tag{New memory cell}$$ 输入门和遗忘门作用于新的记忆细胞得到最终的记忆细胞: $$c_t=f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$$ 输出门作用于新的记忆细胞得到最终的隐藏状态： $$h_t=o_t\\circ tanh(c_t)$$ 这里要理解每个gate的目的到底是啥？虽然很难用数学来解释，但是从intuitive上来理解下还是可以的～ New memory cell: 在GRU中也存在，但是是有区别的，这里是通过input word $x_t$ 和 过去的隐藏状态 $h_{t-1}$ 得到的。GRU中虽然也是，但直接使用了reset gate Input gate: 也叫更新门，因为新的记忆细胞 $\\tilde c_t$ 生成时并未考虑current word是否有保留的意义，因此 $i_t$ 作用于 $\\tilde c_t$. Forget gate: 跟输入门是同样的道理，新的记忆细胞 $\\tilde c_t$ 生成时并未考虑past memory cell是否有保留的意义，因此 $f_i$ 作用于 $c_{t-1}$ Final memory generation: 综合考虑了遗忘门作用后的 $c_{t-1}$ 和 输入门作用后的 $\\tilde c_t$ Output/Exposure Gate: 在GRU中不存在，这里是用来区分最终的记忆细胞 $c_t$ 和 最终的隐藏状态 $h_t$ 的。因为记忆细胞中包含有很多对于隐藏状态来说不必要的信息。 使用LSTM的机器翻译效果很神奇！！！ PresentationTowards a Better Language Modeling. Better inputs: word $\\rightarrow$ subword $\\rightarrow$ char Better regularization/Processing Better Model","link":"/2018/05/07/cs224d-lecture9-machine-translation/"},{"title":"cs224d-lecture8-RNN","text":"主要内容： 语言模型 Language model 循环神经网络 recurrent neural network 梯度消失和梯度爆炸问题的原因以及解决方法 双向rnn， deep bi-RNNs 关于依存分析的presentation 语言模型 Language Model语言模型是计算一系列词以特定序列出现的概率。传统的语言模型是基于频率，计算在前n个词的条件下生成下一个词 $w_i$ 的概率。 $$P(w_1,…,w_m)=\\prod_{i=1}^{i=m}P(w_i|w_1,…,w_i-1)\\approx\\prod_{i=1}^{i=m}P(w_i|w_{i-n},…,w_{i-1})$$ 其中： $$P(w_2|w_1)=\\dfrac{count(w_1,w_2)}{count(w_1)}$$ $$P(w_3|w_1,w_2)=\\dfrac{count(w_1,w_2,w_3)}{count(w_1,w_2)}$$ 但基于概率的语言模型并不能捕捉到一些语义信息。 For instance, consider a case where an article discusses the history of Spain and France and somewhere later in the text, it reads “The two countries went on a battle”; clearly the information presented in this sentence alone is not sufficient to identify the name of the two countries. 于是出现了第一个神经网络的语言模型， learning a distributed representation of words $$\\hat y=softmax(W^{(2)}tanh(w^{(1)}x+b^{(1)})+w^{(3)}x+b^{(3)})$$ W^{(1)} 应用于词向量（solid green arrows） W^{(2)} 应用于隐藏层 W^{(3)} 应用于词向量（dashed green arrows） 但如果要记忆更多的词，必须要增大windows size n，这会造成计算量太大而无法计算。 循环神经网络 Recurrent Neural Network language model $$h_t = \\sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$ 其中+表示concatenate还是直接相加？通过作业实现，是相加～ shapes: $h_0\\in R^{D_h}$ is some initialization vector for the hidden layer at time step 0, $x\\in R^{d}$ is the column vector for L at index [t] at time step t $W^{hh}\\in R^{D_h\\times D_h}$ $W^{hx}\\in R^{D_h\\times d}$ $W^{(S))}\\in R^{|V|\\times D_h}$ 当前时间步的输出： $\\hat y \\in R^{|V|}$ 通过softmax得到的在词表V上的概率分布。 那么当前时间步的损失值： $$J^{(t)}(\\theta) = -\\sum_{j=1}^{|V|}y_{t,j}log\\hat y_{t,j}$$ $y_{t,j}$ 表示当前时间步的actual word,是 one-hot vector. 在训练模型时，$\\hat y_t$ 用来计算当前时间步的损失值,从而训练参数。而在测试集中时，也就是生成sentence时，用来作为下一个时间步的输入。 那么对整个sentence的预测的损失值： $$J=\\dfrac{1}{T}\\sum_{t=1}^T(\\theta)=-\\dfrac{1}{T}\\sum_{t=1}^T\\sum_{j=1}^{|V|}y_{t,j}\\times log(\\hat y_{t,j})$$ 困惑度： $$Perplexity=2^J$$ 梯度消失和梯度爆炸问题Training RNNs is incredibly hard! Buz of gradient vanishing and explosion problems. 这篇文章对rnn中梯度消失的问题说的比较清楚，RNN梯度消失和爆炸的原因 这里将rnn简化了,原本应该是： $$h_t=\\sigma (Wf(h_{t-1})+W^{(hx)}x_{|t|})$$ $$\\hat y = softmax(W^{(S)}f(h_t))$$ 这里就按照简化的来推导吧，t时间步的损失值对 $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}\\dfrac{\\partial h_t}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}$$ 其实主要是这个式子的问题 $\\dfrac{\\partial h_t}{\\partial h_k}$, $h_t$ 是W 和 $h_t-1$ 的函数， $h_{t-1}$ 又是 W 和 $h_{t-2}$ 的函数….. 也就是说 $h_t$ 是之前所有时刻 $h_k$ 的函数，而 $h_k$ 也是权重 W 的函数 $$\\dfrac{\\partial h_t}{\\partial h_k} = \\prod_{j=k+1}^k\\dfrac{\\partial h_j}{\\partial h_{j-1}}=\\prod_{j=k+1}^tW^T\\times diag[f’(j_{j-1})]$$ 其中 $h\\in R^{D_h}$, 因此其导数 $\\partial h_j/\\partial h_{j-1}$ 是一个 $D_h \\times D_h$ 的雅克比矩阵。 所以有： $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}(\\prod_{j=k+1}^t\\dfrac{\\partial h_j}{\\partial h_{j-1}})\\dfrac{\\partial h_k}{\\partial W}$$ 定义 $\\beta$ 为范式的下界，那么 $||\\dfrac{\\partial h_j}{\\partial h_{j-1}}||$ 很容易变得很小或很大。 解决梯度爆炸或消失的一些tricks梯度裁剪 gradient clipping对于gradient exploding，有个很简单的trick:gradient clipping 可以动手实践下，也许对梯度会有更深的理解～ 实线Solid lines表示 standard gradient descent trajectories 虚线Dashed lines表示 gradients rescaled to fixed size 将error看作是很多维参数空间的函数,如果是二维的话，那error surface就是一个曲面。在曲面上高曲率的地方(high curvature walls)，其梯度也就很大。 详细的还是看文献吧On the difficulty of training Recurrent Neural Networks, Pascanu 对于梯度消失 vanishing gradients 参数初始化 Initialization relus, Rectified Relus 很难理解为啥用relu能很好的解决梯度消失的问题，的确relu的梯度为1，但它的非线性也太简单了吧。。。所以得看看原论文 A Simple Way to Initialize Recurrent Networks of Rectified Linear Units softmax计算量太大的问题对于每个时间步，从隐藏层到输出 $W^{(S)} \\in R^{D_h, V}$ ,如果词表很大的话，这个矩阵也就很大了～ 序列模型的一些其他任务Classify each word into: NER Entity level sentiment in context opinion expression extraction Opinion Mining with Deep Recurrent Neural Networks 双向 RNNs 其实跟rnn没有太多变化，有两个隐藏层，并且隐藏层的递归分别是从语料库的两个不同的方向。 Deep bidirectional RNNs $$\\overrightarrow {h_t^{(i)}}=f(\\overrightarrow{W^{(i)}}h_t^{(i-1)}+\\overrightarrow{V^{(i)}}h_{t-1}^{(i)}+\\overrightarrow{b^{(i)}})$$ 其中 $h_t^{(i-1)}$ 表示上一层隐藏层的输入， $h_{t-1}^{(i)}$ 表示当前隐藏层层的上一个时间步的输入。 $$\\overleftarrow {h_t^{(i)}}=f(\\overleftarrow{W^{(i)}}h_t^{(i-1)}+\\overleftarrow{V^{(i)}}h_{t-1}^{(i)}+\\overleftarrow{b^{(i)}})$$ 需要训练的参数有：$\\overrightarrow{W^{(i)}},\\overleftarrow{W^{(i)}}$ $\\overrightarrow{V^{(i)}},\\overleftarrow{V^{(i)}}$ $$\\hat y_t=g(Uh_t+c)=g(U[\\overrightarrow{h_t^{(L)}};\\overleftarrow{h_t^{(L)}}]+c)$$ data evalution PresentationStructured Training for Neural Network Transition-Based Parsing, David Weiss, Chris Alberti, Michael Collins, Slav Petrov 表示根本听不懂，只知道使用deeplearning做依存分析。。用state-of-art的SyntaxNet和前人几篇有影响力的进行了对比～","link":"/2018/05/04/cs224d-lecture8-RNN/"},{"title":"论文笔记 Deep Transition Architecture","text":"paper 1paper: Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers, 2018 WASSA@EMNLP 将 self-attention 和 LSTM，CNN 进行对比. 因为单纯只是为了比较三者作为一个 building block 的性能，所以保证公平性的情况下，只用了 1 layer 和 2-layer. 整篇文章并没有看到其他的创新性，不知道是不是看错了。。所以这样也能发 EMNLP? 但是看了还是有点收获，复习了一下 multi-head self-attention. 以及文中提到了三种 position encoding: Sinusoidal Position Encoding Learned Position Encoding Relative Position Representations Sinusoidal 是在 Transformer 中使用的, 好处在于即使是测试集中出现 sentence 的长度比训练集中所有的 sentence 都要长，也能计算其 position encoding. Relative 是效果最好的,作者和 Tansformer 的作者是一样的，值得一看。Self-attention with relative position representations For this method, the self-attention mechanism is modified to explicitly learn the relative positional information between every two sequence positions. As a result, the input sequence is modeled as a labeled, directed, fully-connected graph, where the labels represent positional information. A tunable parameter k is also introduced that limits the maximum distance considered between two sequence positions. [Shaw et al., 2018] hypothesized that this will allow the model to generalize to longer sequences at test time. paper 2DTMT: A Novel Deep Transition Architecture for Neural Machine Translation AAAI 2019 腾讯 WeChat AI 的一篇机器翻译的 paper，同样也是 encoder-decoder, 但是改进了 Deep transition RNN. 先了解何为 deep transition RNN， 注意与 deep stacked architectures 的区别。 也就是在传统的 RNN 迭代之前，先对 state 进行 transition。而这篇 paper 改进的就是这种 transition 的方式，目的就是为了减轻其中因为非线性操作带来的梯度消失的问题。 模型结构图： encoder transition multi-head attention query transition decoder transition 其中提出了 GRU 以及其变体 T-GRU 和 L-GRU. GRU回顾下 GRU $$h_t = (1-z_t)\\odot h_{t-1} + z_t\\odot \\tilde h_t$$ 其中： candidate state: $$\\tilde h_t = tanh(W_{xh}x_t + r_t\\odot (W_{hh}h_{t-1}))$$ reset gate: $$r_t = \\sigma(W_{xr}x_t+W_{hr}h_{t-1})$$ update gate: $$z_t=\\sigma(W_{xz}x_t+W_{hz}h_{t-1})$$ T-GRU (transition GRU)对 GRU 做了简化，因为只针对于 state 的变化，去掉了 $x_t$ 的输入. $$h_t = (1-z_t)\\odot h_{t-1} + z_t\\odot \\tilde h_t$$ candidate state: $$\\tilde h_t = tanh(r_t\\odot (W_{hh}h_{t-1}))$$ reset gate: $$r_t = \\sigma(W_{hr}h_{t-1})$$ update gate: $$z_t=\\sigma(W_{hz}h_{t-1})$$ L-GRU( Linear Transformation enhanced GRU)$$h_t = (1-z_t)\\odot h_{t-1} + z_t\\odot \\tilde h_t$$ candidate state: $$\\tilde h_t = tanh(W_{xh}x_t + r_t\\odot (W_{hh}h_{t-1}))+ l_t\\odot H(x_t)$$ 增加了一个基于 x_t 的线性的变换 H(x_t), 并由 $l_t$ 控制信息量的多少。 $$H(x_t)=W_xx_t$$ $$l_t=\\sigma(W_{xl}x_t+W_{hl}h_{t-1})$$ 这个模块相对于 $W_{xh}x_t$ 多了一个线性控制器，其实他与第二个模块 $r_t\\odot (W_{hh}h_{t-1}))$ 倒是对称的，只不过 $h_{t-1}$ 换成了 $x_t$. 所以我可不可以再加个这样的？ $\\tilde h_t = tanh(W_{xh}x_t +W_{hh2}h_{t-1} +l_t\\odot W_{xh2}x_t+ r_t\\odot (W_{hh}h_{t-1}))$ DNMTDecoder$L_s$ 表示 encoder transition 的深度 depth. $j$ 表示 current time step. $$\\overrightarrow h_{j,0}=L-GRU(x_j, \\overrightarrow h_{j-1,L_s})$$ $$\\overrightarrow h_{j,k}=T-GRU(\\overrightarrow h_{j, k-1}),\\text{ for } 1\\le k\\le L_s$$ 很好理解，就是 k=0 transition 的第一步需要输入 $x_t$，使用 L-GRU. transition 的剩余步骤使用 T-GRU. 双向 bi-direction GRU: $C=[\\overrightarrow h_{j, L_s}, \\overleftarrow h_{j, L_s}]$ Decoder query transition: depth $L_q$ decoder transition: depth $L_d$ 对于 query transition: 输入是上一步的输出 $y_{t-1}$ 和 上一步的隐藏状态 $s_{t-1}$. 然后得到 $S_{t, L_q}$ 与 encoder 进行交互, multi-head attention: 得到 attention vector $c_t$ 之后，$c_t$ 类似于输入， $s_{t,L_q}$ 是隐藏状态，进一步进行 transition state 转换。 得到 $s_{t, L_q+L_d+1}$ 就是当前时间步的最终隐藏状态，通过映射到词表空间，即可预测当前时间步的词。 Tricks 还是有很多可以借鉴的地方的～","link":"/2018/12/21/paper-reading-12-21/"},{"title":"类和方法","text":"Python的OOP模型主要思想：在一堆对象中查找属性，并为函数定一个特殊的第一个参数。 1. 类代码编写基础1.1 类和实例化python面向对象中有两种对象：类对象和实例对象。 类对象提供默认行为，实例对象是程序处理的实际对象：各自都有自己的命名空间。 在class语句内，任何赋值语句都会产生类属性： 123456789101112131415161718192021class ShareDate: spam = 42 # 不是在__init__函数中，有点类似于C++的静态成员变量x = ShareDate()y = ShareDate()print(x.spam) # 42print(y.spam) # 42ShareDate.spam = 89print(x.spam) # 89print(y.spam) # 89 1.2 类方法类方法：与普通的def函数不同的是，类中的方法第一个参数是self，即引用正处理的实例对象。self参数同C++的this指针很相似。 1.3 类继承，重载除了继承和重载，在子类中还可以重新定制构造函数，这也是很常见的。在1.8中会通过完整的例子来讲定制构造函数。 123456789101112131415161718192021222324252627class FirstClass: def setdata(self, value): self.data = value def display(self): print(self.data)class SecondClass(FirstClass): # 类继承 def display(self): # 类函数的重载 print('current value = &quot;%s&quot; '%self.data) z = SecondClass()z.setdata(42)z.display() # current value = &quot;42&quot; 1.4 类是模块的属性类名称总是存在与模块文件中的，类是模块对象的属性。类和模块都是命名空间，但类对应于语句（而不是整个文件），而且支持多个实例、继承以及运算符重载这些OOP概念。 需理解_init_.py文件。 1.5 运算符重载，类可以截获Python运算符实际上，“运算符重载”只是意味着在类方法中拦截内置的操作……当类的实例出现在内置操作中，Python自动调用你的方法，并且你的方法的返回值变成了相应操作的结果。以下是对重载的关键概念的复习： 运算符重载让类拦截常规的Python运算。 类可重载所有Python表达式运算符 类可以重载打印、函数调用、属性点号运算等内置运算 重载使类实例的行为像内置类型。 重载是通过特殊名称的类方法来实现的。 举个栗子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class ThirdClass(SecondClass): def __init__(self, value): self.data = value def __add__(self, other): return ThirdClass(self.data + other) def __str__(self): print(&quot;__str__ is called&quot;) return &quot;[ThirdClass: %s]&quot; % self.data def mul(self, other): self.data *= othera = ThirdClass(&quot;abc&quot;)a.display()print(a) # 自动调用__str__b = a + &quot;xyz&quot; # 自动调用__add__b.display()a.mul(3) # 显示结果与python内置类型的行为不同a.display()# 运行结果：current value = &quot;abc&quot;__str__ is called[ThirdClass: abc]current value = &quot;abcxyz&quot;current value = &quot;abcabcabc&quot; 意思就是，当你的类出现在内置函数比如+，print这样的表达式中，就会被_add_,__str__这样的类方法拦截。但如果在你的类中，没有定义这样的方法，就会很有可能报错了～那么，为什么会拦截呢？还需要看底层代码～猜想应该是对自定义的类执行+操作时，就会自动从类树中寻找__add__方法吧～ | 方法 | 重载 | 调用 | | —————| ———| ————————–| | _init_ | 构造函数 | 对象建立：X = Class（args）| |_del_ |析构函数| X对象收回| |_add_ |运算符 + | 如果没有__iadd__， X + Y, X += Y |_or_ |运算符（位OR）| 如果没有_ior_ |_repr_, _str_| 打印、转换| print(X), repr(X), str(X) |_call_ |函数调用| X(*args, **kargs) |_getattr_| 点号运算| X.undefined |_setattr_| 属性赋值语句| X.any = value |_delattr_| 属性删除| del X.any |_getattribute_| 属性获取 |X.any |_getitem_| 索引运算|X[key],X[i:j]，没__iter__时的for循环和其他迭代器 |_setitem_ |索引赋值语句| X[key] = value, X[i:j] = sequence |_delitem_|索引和分片删除| del X[key], del X[i:j] |_len_| 长度 |len(X), 如果没有__bool__， 真值测试 |_bool_| 布尔测试| bool(X), 真测试（在Python 2.6中叫做__nonzero__） |_lt_, _gt_,_lt_,_ge_,_eq_, _ne|特定比较| X&lt;Y, X&gt;Y, X&lt;=Y, X&gt;=Y, X == Y, X != Y(或者在Python 2.6中只有_cmp) |_radd_| 右侧加法| Other + X |_iadd| 实地（增强的）加法| X += Y(or else _add) |_iter_, _next_ |迭代环境| I = iter(X), next(I); for loops, in if no _contains_, all comprehensions, map(F, X), 其他(__next__在Python2.6中成为next) |_contains_ |成员关系测试| item in X(任何可迭代的) |_index_| 整数值 |hex(X), bin(X), oct(X), O[X], O[X:]（替代Python 2中的_oct_,__hex__） |_enter_, _exit_ |环境管理器| with obj as var: |_get_, _set_,_delete_|描述符属性| X.attr, X.attr = value, del X.attr |_new_ |创建 |在__init__之前创建对象 所有重载方法的名称前后都有两个下划线，以便把同类中定义的变量名区别开来。特殊方法名称和表达式或运算的映射关系，是由Python语言预先定义好的（在标准语言手册中有说明）。例如名称，__add__按照Python语言的定义，无论__add__方法的代码实际在做些什么，总是对应到了表达式 + 。 如果没有定义运算符重载方法的话，它可能继承自超类，就像任何其他的方法一样。运算符重载方法也都是可选的……如果没有编写或继承一个方法，你的类直接不支持这些运算，并且试图使用它们会引发一个异常。一些内置操作，比如打印，有默认的重载方法（继承自Python 3.x中隐含的object类），但是，如果没有给出相应的运算符重载方法的话，大多数内置函数会对类实例失败。 也就是虽然自定义的类里面没有重载类方法__str__或是_add_,但Python3所有自定义的类都是继承了object类的，含有默认的_add_,_str_,但大部分会对实例失败。 还是举个之前的那个栗子，这次我们不定义_add_,_str_: 123456789101112131415161718192021class ThirdClass(SecondClass): def __init__(self, value): self.data = valueprint(dir(ThirdClass))print(&quot;***************&quot;)a = ThirdClass(&quot;abc&quot;)print(a)b = a + &quot;xyz&quot;b.display() 运行结果： 123456789101112131415161718192021222324252627282930313233['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'display', 'setdata']***************&lt;__main__.ThirdClass object at 0x7f30d8472da0&gt;---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-40-8717227036a8&gt; in &lt;module&gt;() 7 a = ThirdClass(&quot;abc&quot;) 8 print(a)----&gt; 9 b = a + &quot;xyz&quot; 10 b.display()TypeError: unsupported operand type(s) for +: 'ThirdClass' and 'str' 我们可以看到，ThirdClass类本身继承了object的很多内置方法，包括__str__，只是显示的与我们之前自定义的__str__不一样而已。而__add__就会出现异常了。 1.6 类和字典的关系python的类模型相当动态。类和实例只是命名空间，属性是通过赋值语句动态建立的。 模块的命名空间实际上是以字典的形式出现的，类和实例对象也是如此。可用__dict__来显示这一点，属性点号运算其实就是字典内的索引运算，而属性继承其实就是搜索链接的字典。 1.7 测试脚本文件代码1234567if __name__ == &quot;__main__&quot;: # self-test pass 只有在当前脚本文件下运行时，上述if语句条件才为真。因此，这样就可以在文件底部运行测试语句，而不会在导入文件的时候运行。 1.8 定制构造函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Person(): def __init__(self, name, job=None, pay=0): self.name = name self.job = job self.pay = pay def LastName(self): return self.name.spilt()[-1] def giveRaise(self, persent): self.pay = int(self.pay * (1 + persent)) def __str__(self): return self.__class__.__name__ + ':{0} {1}'.format(self.name, self.pay)class Manager(Person): def __init__(self, name, pay): # 定制构造函数 Person.__init__(self, name, 'mgr', pay) # 必须手动调用超类 def giveRaise(self, persent, bonus=0.1): Person.giveRaise(self, persent + bonus)if __name__ == &quot;__main__&quot;: tom = Manager('Tom Jones', 5000) print(tom) print(tom.__class__.__name__) print(Manager.__bases__) print(tom.__dict__.keys()) print(getattr(tom, 'name') == tom.name) # 虽然类和字典类似，但是不能写tom['name']# 运行结果：Manager:Tom Jones 5000Manager(&lt;class '__main__.Person'&gt;,)dict_keys(['name', 'job', 'pay'])True 在初始化中self.name=name这里看起来有些多余，但实际上name, job在__init__函数中只是本地变量但self.job是实例中的一个属性，这两个是不同的变量，只是恰好名字一样__init__函数没什么奇妙之处，只是在产生一个实例时，会自动调用，并且有特殊的第一个参数 这其中用到了特殊的类属性， _class_ _name_ _bases_ _dict_ 1.9 把对象存储到数据库中 pickle 任意Python对象和字符串之间的序列化 dbm 实现一个可通过键访问的文件按系统，以存储字符串 shelve 使用另两个模块把python对象存储到文件中 123456789101112131415161718192021222324252627282930313233343536373839404142434445bob = Person('bob smith')sue = Person('Sue Jones', job='dev', pay=10000)tom = Manager('Tom Jones', pay=5000)import shelvedb = shelve.open('Persondb')for object in (bob, sue, tom): db[object.name] = objectdb.close()db = shelve.open('Persondb')print(db)print(len(db))print(list(db.keys()))print(db['bob smith'])#运行结果：&lt;shelve.DbfilenameShelf object at 0x7f915bdce588&gt;3['bob smith', 'Sue Jones', 'Tom Jones']Person:bob smith 0 1.10 抽象超类对于抽象基类，需要子类来填充。当行为无法预测，非得等到更为具体的子类编写时才知道，通常可用这种方式把类通用化。OOP软件框架也使用这种方式作为客户端定义、可定制的运算的实现方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from abc import ABCMeta, abstractmethodclass Super(metaclass=ABCMeta): def delegate(self): self.action() @abstractmethod def action(self): passclass Sub1(Super): # 没有定义action函数，就会报错 passclass Sub2(Super): def action(self): print('spam')X2 = Sub2()X2.delegate()X1 = Sub1()X1.delegate()# 运行结果spamTypeError: Can't instantiate abstract class Sub1 with abstract methods action 1.11 Python命名空间直接看代码，看懂了也就了解了Python的命名空间了～ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# manyname.pyX = 11def f(): print(X)def g(): X = 22 print(X)class C: X = 33 def m(self): X = 44 self.X = 55print(X) # 11 module,模块属性f() # 11 globalg() # 22 local,函数内的本地变量print(X) # 11 模块属性没变obj = C()print(obj.X) # 33 类属性obj.m()print(obj.X) # 55 实例属性print(C.X) # 33 1.12 super()超类首先要弄清楚为什么要使用super()。是因为我们想要调用父类中已经被子类覆盖的方法。最常见的比如 init()函数，在子类中必然会覆盖父类的初始化方法，这样父类中很多属性就没有了。 举个栗子： 12345678910111213141516171819202122232425class A: def __init__(self, batch_size, vocab_size): self._batch_size = batch_size self._vocab_size = vocab_size print('A.spam')class B(A): def __init__(self): print('B.spam')b = B()print(b._batch_size) 运行结果: 1234567891011B.spamTraceback (most recent call last): File &quot;/home/panxie/Documents/NLP实战/text classification/06-memory-networks/test.py&quot;, line 25, in &lt;module&gt; print(b._batch_size)AttributeError: 'B' object has no attribute '_batch_size' 结果报错：子类中不存在_batch_size这个属性。所以必须在子类的 init()函数中调用父类的 init()函数。 也就是这样： 123456789101112131415161718192021222324252627class A: def __init__(self, batch_size, vocab_size): self._batch_size = batch_size self._vocab_size = vocab_size print('A.spam')class B(A): def __init__(self): super(B, self).__init__(8, 1000) print('B.spam')b = B()print(b._batch_size) 运行结果： 1234567A.spamB.spam8 那我们可不可以不用super()呢，直接调用A类的方法也行呀，比如： 123456789101112131415161718192021222324252627class A: def __init__(self, batch_size, vocab_size): self._batch_size = batch_size self._vocab_size = vocab_size print('A.spam')class B(A): def __init__(self): A.__init__(self, 8, 10) print('B.spam')b = B()print(b._batch_size) 运行结果跟上面一样，貌似也没毛病。但是在多继承的时候，父类会重复调用什么的，比如 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Base: def __init__(self): print('Base.__init__')class A(Base): def __init__(self): Base.__init__(self) print('A.__init__')class B(Base): def __init__(self): Base.__init__(self) print('B.__init__')class C(A,B): def __init__(self): A.__init__(self) B.__init__(self) print('C.__init__')c = C()print(C.__mro__) 运行结果： 12345678910111213Base.__init__A.__init__Base.__init__B.__init__C.__init__(&lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.B'&gt;, &lt;class '__main__.Base'&gt;, &lt;class 'object'&gt;) 可以发现基类被重复调用了两次。这显然是不好的。但是换成 super() 每个类的 init()方法保证只会调用一次。具体原理可以参考 python-cookbook","link":"/2018/03/25/python-%E7%B1%BB%E5%92%8C%E6%96%B9%E6%B3%95/"},{"title":"论文笔记-baseline for OOD","text":"paper: A baseline for detecting misclassified and out-of-distribution examples Motivation文章开头先说到，通过 softmax 预测得到的各个类别的概率分布 (prediction probability) 和 置信度之间的对应关系并不是很直接 (have a poor direct correspondence to confidence)。这是因为 softmax 的计算使用了指数函数(fast-growing exponential function),这就会导致一个很小的额外附加输入，都会使得接下来的输出分布(output distribution) 发生改变。事实上，之前也有人做了相当一部分实验，证明了一个高斯随机噪声加入到一个 MNIST 图像之后，会让这个图像获得 91% 的预测概率。 尽管如此，然而，这篇文章作者依旧认为 错误的类别 或 OOD(incorrect and out-of-distribution) 更倾向于具有较低的预测概率，相对于正确的样本。因此，获得关于正确的或者 in-sample 样本的预测概率的统计，通常来说足够去检测出 错误或不正常(error or abnormal),即使单独来看预测概率可能会有误导。也就是从统计的角度，softmax 得到的概率分布还是可信的。 这篇文章在很多任务上进行了实验，并不都是 SOTA, 所以只是提供了一个新的方法用来验证一个神经网络能否有效的区分出 abnormal，作为 baseline method. 除了这个 baseline method,作者还制定了标准的任务和指标，用来评价对 错误和OOD 的自动检测 . 所以看这篇文章的目的就是 baseline mathod 是什么？也就是怎么去评价一个模型自动区分出 OOD 的能力。 作者给出的标准任务和数据 Baseline method这篇文章的主要解决的两个问题： error and success prediction: 能否正确的对一个样本分类 in- and out-of-distribution detection： 能否正确的检测出 OOD 通常来说，OOD 和 in sample 来说，样本数量差异会很大，比如疾病检测，未见过的罕见疾病 OOD 就很少; 又比如对一个猫狗分类器，他在测试时，OOD 就很大。所以对于这种数据不均衡的问题，accuracy 已经无法满足这类问题了。 metric1: AUROC作者使用了 AUROC(Area Under the Receiver Operating Characteristic curve). 其实这个在前面的笔记中有详细介绍过，这里再复习遍～ 机器学习-常用指标总结 ROC 曲线是依赖于阈值的性能验证指标 (metric which is a threshold-independent performance evalution). 因为在这类不均衡问题中，我们关注的是 positive label. 所以我们关注的指标是 真正类率 TPR, 负正类率 FPR. TPR, 真正类率(true positive rate ,TPR),： 如果一个实例是正类并且也被 预测成正类，即为真正类（True positive），真正类率是指分类器所识别出来的 正实例占所有正实例的比例。就是正类的 Recall 吧～ TPR = TP / (TP + FN) FPR, 负正类率： 分类器错认为正类的负实例占所有负实例的比例，FPR = FP / (FP + TN) 还是不太明白为啥 TPR + FPR = 1？？？？ metric2: AUPRArea Under the Precision-Recall curve (AUPR) The PR curve plots the precision (tp=(tp+fp)) and recall (tp=(tp + fn)) against each other. 对于 PR 曲线，选择哪个类别作为 positive 类，非常重要。","link":"/2018/12/24/paper-reading-12-24/"},{"title":"pytorch-Tensor","text":"Tensor从接口的角度来讲，对tensor的操作可分为两类： torch.function，如torch.save等。 另一类是tensor.function，如tensor.view等。 而从存储的角度来讲，对tensor的操作又可分为两类： 不会修改自身的数据，如 a.add(b)， 加法的结果会返回一个新的tensor。 会修改自身的数据，如 a.add_(b)， 加法的结果仍存储在a中，a被修改了。 表3-1: 常见新建tensor的方法 |函数|功能| |:—:|:—:| |Tensor(*sizes)|基础构造函数| |ones(*sizes)|全1Tensor| |zeros(*sizes)|全0Tensor| |eye(*sizes)|对角线为1，其他为0| |arange(s,e,step|从s到e，步长为step| |linspace(s,e,steps)|从s到e，均匀切分成steps份| |rand/randn(*sizes)|均匀/标准分布| |normal(mean,std)/uniform(from,to)|正态分布/均匀分布| |randperm(m)|随机排列| 其中使用Tensor函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor. b.tolist() 把 tensor 转为 list b.numel() b 中元素总数，等价于 b.nelement() torch.Tensor(b.size()) 创建和 b 一样的 tensor 除了tensor.size()，还可以利用tensor.shape直接查看tensor的形状，tensor.shape等价于tensor.size() 1234567891011121314151617181920212223# 用list的数据创建tensorb = torch.Tensor([[1,2,3],[4,5,6]])print(b)print(b.tolist())print(b.numel())# 创建一个和b形状一样的tensorc = torch.Tensor(b.size())print(c)# 创建一个元素为2和3的tensord = torch.Tensor((2, 3))print(d) tensor([[ 1., 2., 3.], [ 4., 5., 6.]]) [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]] 6 tensor(1.00000e-15 * [[-3.4942, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]]) tensor([ 2., 3.]) 常用Tensor操作view, squeeze, unsqueeze, resize 通过tensor.view方法可以调整tensor的形状，但必须保证调整前后元素总数一致。view不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候squeeze和unsqueeze两个函数就派上用场了。 tensorflow 里面是 tf.expand_dim 和 tf.squeeze. resize是另一种可用来调整size的方法，但与view不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。 12345a = torch.arange(0, 6)a.view(2, 3) tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 12345b = a.view(-1, 3) # 当某一维为-1的时候，会自动计算它的大小b tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 123b.unsqueeze(1) # 注意形状，在第1维（下标从0开始）上增加“１” tensor([[[ 0., 1., 2.]], [[ 3., 4., 5.]]]) 123b.unsqueeze(-2) # -2表示倒数第二个维度 tensor([[[ 0., 1., 2.]], [[ 3., 4., 5.]]]) 12345c = b.view(1, 1, 1, 2, 3)c.squeeze(0) # 压缩第0维的“１” tensor([[[[ 0., 1., 2.], [ 3., 4., 5.]]]]) 123c.squeeze() # 把所有维度为“1”的压缩 tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 12345a[1] = 100b # a修改，b作为view之后的，也会跟着修改 tensor([[ 0., 100., 2.], [ 3., 4., 5.]]) 12345b.resize_(1, 3)b tensor([[ 0., 100., 2.]]) 12345b.resize_(3, 3) # 旧的数据依旧保存着，多出的大小会分配新空间b tensor([[ 0.0000, 100.0000, 2.0000], [ 3.0000, 4.0000, 5.0000], [ -0.0000, 0.0000, 0.0000]]) 索引操作Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。 其它常用的选择函数如表3-2所示。 表3-2常用的选择函数 函数|功能| :—:|:—:| index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列 masked_select(input, mask)|例子如上，a[a&gt;0]，使用ByteTensor进行选取 non_zero(input)|非0元素的下标 gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样 gather是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下： 12345out[i][j] = input[index[i][j]][j] # dim=0out[i][j] = input[i][index[i][j]] # dim=1 三维tensor的gather操作同理，下面举几个例子。 index_select(input, dim, index) 指定维度上选取某些行和列, 返回的是某行和某列1234567a = torch.randn(3, 4)print(a)print(a[0,1]) # 第 0 行， 第 1 列 tensor([[ 0.5948, -0.5760, 1.3726, -0.9664], [ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) tensor(-0.5760) 返回行的四种方式123print(a[torch.LongTensor([1,2])]) # 第 0 行 和 第 1 行 tensor([[ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) 12345index = torch.LongTensor([1,2])a.index_select(dim=0, index=index) tensor([[ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) 123a[1:3] # 只能是连续的行 tensor([[ 0.5705, 1.0374, -1.1780, 0.0635], [-0.1195, 0.6657, 0.9583, -1.8952]]) 123print(a[torch.LongTensor([[1],[2]])]) # 还是第 0 行 和 第 1 行 tensor([[[ 0.5705, 1.0374, -1.1780, 0.0635]], [[-0.1195, 0.6657, 0.9583, -1.8952]]]) 返回列的两种方式123a.index_select(dim=1, index=index) tensor([[-0.5760, 1.3726], [ 1.0374, -1.1780], [ 0.6657, 0.9583]]) 123a[:, 1:3] # 连续的列 tensor([[-0.5760, 1.3726], [ 1.0374, -1.1780], [ 0.6657, 0.9583]]) masked_selected(input, mask) 使用 ByteTensor 进行选取mask is ByteTensor, 类似于 a[a&gt;1] 123456789a = torch.randn(3, 4)print(a)print(a[a&gt;0])a.masked_select(a&gt;0) tensor([[ 0.3464, 1.4499, 0.7417, -1.9551], [-0.0042, -0.0141, 1.2861, 0.0691], [ 0.5843, 1.6635, -1.2771, -1.4623]]) tensor([ 0.3464, 1.4499, 0.7417, 1.2861, 0.0691, 0.5843, 1.6635]) tensor([ 0.3464, 1.4499, 0.7417, 1.2861, 0.0691, 0.5843, 1.6635]) 123a&gt;0 # 返回一个 ByteTensor tensor([[ 1, 1, 1, 0], [ 0, 0, 1, 1], [ 1, 1, 0, 0]], dtype=torch.uint8) 12345b = torch.ByteTensor(3,4)b tensor([[ 80, 235, 127, 167], [ 199, 85, 0, 0], [ 0, 0, 0, 0]], dtype=torch.uint8) 123a[b] tensor([ 0.3464, 1.4499, 0.7417, -1.9551, -0.0042, -0.0141]) gather(input, dim, index) 根据 index 在 dim 维度上选取数据，输出 size 与 index 一样.123456789a = torch.arange(0, 20).view(4,5)print(a)index = torch.LongTensor([[0,1,2,1,3]])print(index, index.shape) tensor([[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.], [ 10., 11., 12., 13., 14.], [ 15., 16., 17., 18., 19.]]) tensor([[ 0, 1, 2, 1, 3]]) torch.Size([1, 5]) 123a.gather(dim=0, index=index) tensor([[ 0., 6., 12., 8., 19.]]) 所以 gather 就是 index 与 input 中某一个维度一致，比如这里 input.size()=[4,5]. 那么 dim=0, index.size()=[1,5]. 然后在每列对应的 index 选取对应的数据。最后输出 size 与 index 一致。 12345index2 = torch.LongTensor([[1],[2],[3],[4]])print(index2.shape) torch.Size([4, 1]) 123a.gather(dim=1, index=index2) tensor([[ 1.], [ 7.], [ 13.], [ 19.]]) list 转换成 one-hot 向量1234567891011### list 转换成 one-hot 向量label = [1, 2, 3, 4, 5]label = torch.LongTensor(label).view(-1, 1)one_hot = torch.zeros(5, 10).scatter_(dim=1, index=label, value=1)one_hot tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]) Tensor 类型Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过t.set_default_tensor_type 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有1000*1000*1000=10^9个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。 ^2: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste 表3-3: tensor数据类型 数据类型| CPU tensor |GPU tensor| :—:|:—:|:–:| 32-bit 浮点| torch.FloatTensor |torch.cuda.FloatTensor 64-bit 浮点| torch.DoubleTensor| torch.cuda.DoubleTensor 16-bit 半精度浮点| N/A |torch.cuda.HalfTensor 8-bit 无符号整形(0~255)| torch.ByteTensor| torch.cuda.ByteTensor 8-bit 有符号整形(-128~127)| torch.CharTensor |torch.cuda.CharTensor 16-bit 有符号整形 | torch.ShortTensor| torch.cuda.ShortTensor 32-bit 有符号整形 |torch.IntTensor |torch.cuda.IntTensor 64-bit 有符号整形 |torch.LongTensor |torch.cuda.LongTensor 各数据类型之间可以互相转换，type(new_type)是通用的做法，同时还有float、long、half等快捷方法。CPU tensor与GPU tensor之间的互相转换通过tensor.cuda和tensor.cpu方法实现。Tensor还有一个new方法，用法与t.Tensor一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。 torch.set_sefault_tensor_type(‘torch.IntTensor) 逐元素操作这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。 表3-4: 常见的逐元素操作 |函数|功能| |:–:|:–:| |abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..| |cos/sin/asin/atan2/cosh..|相关三角函数| |ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分| |clamp(input, min, max)|超过min和max部分截断| |sigmod/tanh..|激活函数 对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如a ** 2 等价于torch.pow(a,2), a * 2等价于torch.mul(a,2)。 其中clamp(x, min, max)的输出满足以下公式： $$ y_i = \\begin{cases} min, &amp; \\text{if } x_i \\lt min \\ x_i, &amp; \\text{if } min \\le x_i \\le max \\ max, &amp; \\text{if } x_i \\gt max\\ \\end{cases} $$ clamp常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。 123456789import torcha = torch.arange(0,6).view(2,3)print(a)torch.clamp(a, min=3, max=5) tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) tensor([[ 3., 3., 3.], [ 3., 4., 5.]]) 归并操作此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法sum，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。 表3-5: 常用归并操作 |函数|功能| |:—:|:—:| |mean/sum/median/mode|均值/和/中位数/众数| |norm/dist|范数/距离| |std/var|标准差/方差| |cumsum/cumprod|累加/累乘| 以上大多数函数都有一个参数 **dim**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式： 假设输入的形状是(m, n, k) 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k) 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k) 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n) size中是否有”1”，取决于参数keepdim，keepdim=True会保留维度1。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如cumsum。 12345a = torch.arange(0, 6).view(2,3)a tensor([[ 0., 1., 2.], [ 3., 4., 5.]]) 123a.norm(dim=0, p=1), a.norm(dim=0, p=2), a.norm(dim=0, p=3) (tensor([ 3., 5., 7.]), tensor([ 3.0000, 4.1231, 5.3852]), tensor([ 3.0000, 4.0207, 5.1045])) 123torch.norm?? $||x||{p} = \\sqrt[p]{x{1}^{p} + x_{2}^{p} + \\ldots + x_{N}^{p}}$ torch.dist??dist(input, other, p=2) -&gt; Tensor Returns the p-norm of (:attr:input - :attr:other) 123torch.dist(torch.ones(4), torch.zeros(4), 2) tensor(2.) 123torch.var(torch.randn(10,3), dim=0) tensor([ 0.7617, 1.0060, 1.6778]) 比较比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。 表3-6: 常用比较函数 |函数|功能| |:–:|:–:| |gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等| |topk|最大的k个数| |sort|排序| |max/min|比较两个tensor最大最小值| 表中第一行的比较操作已经实现了运算符重载，因此可以使用a&gt;=b、a&gt;b、a!=b、a==b，其返回结果是一个ByteTensor，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况： t.max(tensor)：返回tensor中最大的一个数 t.max(tensor,dim)：指定维上最大的数，返回tensor和下标 t.max(tensor1, tensor2): 比较两个tensor相比较大的元素 至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。 max/min sort topk 12345a = torch.rand(3,4)a tensor([[ 0.1845, 0.4101, 0.1470, 0.0083], [ 0.7520, 0.8871, 0.9494, 0.2504], [ 0.3879, 0.4554, 0.4080, 0.1703]]) 123torch.max(a, dim=0) (tensor([ 0.7326, 0.6784, 0.9791, 0.9011]), tensor([ 1, 2, 1, 1])) 123a.sort(dim=0) (tensor([[ 0.1424, 0.5681, 0.1833, 0.1654], [ 0.4556, 0.6418, 0.3242, 0.5120], [ 0.7326, 0.6784, 0.9791, 0.9011]]), tensor([[ 2, 0, 0, 2], [ 0, 1, 2, 0], [ 1, 2, 1, 1]])) 123a.topk(k=2, dim=0) (tensor([[ 0.7326, 0.6784, 0.9791, 0.9011], [ 0.4556, 0.6418, 0.3242, 0.5120]]), tensor([[ 1, 2, 1, 1], [ 0, 1, 2, 0]])) 线性代数PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。 表3-7: 常用的线性代数函数 |函数|功能| |:—:|:—:| |trace|对角线元素之和(矩阵的迹)| |diag|对角线元素| |triu/tril|矩阵的上三角/下三角，可指定偏移量| |mm/bmm|矩阵乘法，batch的矩阵乘法| |addmm/addbmm/addmv/addr/badbmm..|矩阵运算 |t|转置| |dot/cross|内积/外积 |inverse|求逆矩阵 |svd|奇异值分解 具体使用说明请参见官方文档^3，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的.contiguous方法将其转为连续。 1234567b.contiguous(), b.size()b.contiguous().is_contiguous()print(a.matmul(b.contiguous())) tensor([[ 0.8260, 1.3392, 0.5944], [ 1.3392, 2.7192, 1.0062], [ 0.5944, 1.0062, 0.6130]]) 123456789b = a.t()print(a.size(), b.shape)print(a.mm(b))b.is_contiguous() torch.Size([3, 4]) torch.Size([4, 3]) tensor([[ 0.8260, 1.3392, 0.5944], [ 1.3392, 2.7192, 1.0062], [ 0.5944, 1.0062, 0.6130]]) False 123b, b.diag() (tensor([[ 0.4556, 0.7326, 0.1424], [ 0.5681, 0.6418, 0.6784], [ 0.1833, 0.9791, 0.3242], [ 0.5120, 0.9011, 0.1654]]), tensor([ 0.4556, 0.6418, 0.3242])) 12345a = torch.randn(5,5)a.triu(1) tensor([[ 0.0000, 1.5959, -0.2253, 0.2349, -0.5151], [ 0.0000, 0.0000, -0.0366, -0.0867, 0.2737], [ 0.0000, 0.0000, 0.0000, 0.9904, -1.4889], [ 0.0000, 0.0000, 0.0000, 0.0000, -1.1053], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) Tensor和NumpyTensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。 注意： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。 123456789import numpy as npa = np.ones([2,3])print(a.dtype)a float64 array([[1., 1., 1.], [1., 1., 1.]]) 1234567b = torch.Tensor(a)print(b.type())b torch.FloatTensor tensor([[ 1., 100., 1.], [ 1., 1., 1.]]) 123torch.from_numpy?? 1234567c = torch.from_numpy(a)print(c.type())c torch.DoubleTensor tensor([[ 1., 1., 1.], [ 1., 1., 1.]], dtype=torch.float64) 12345a[0,1] = 100b # b与a不通向内存，所以即使a改变了，b也不变 tensor([[ 1., 1., 1.], [ 1., 1., 1.]]) 123c # c 与 a 共享内存 tensor([[ 1., 100., 1.], [ 1., 1., 1.]], dtype=torch.float64) BroadCast广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。 Numpy的广播法则定义如下： 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状 PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错： unsqueeze或者view：为数据某一维的形状补1，实现法则1 expand或者expand_as，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。 注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。 12345a = torch.ones(3, 2)b = torch.zeros(2, 3, 1) 12345678910111213# 自动广播法则# 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，# 即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,# 第二步: a和b在第一维和第三维形状不一样，其中一个为1 ，# 可以利用广播法则扩展，两个形状都变成了（2，3，2）a+b tensor([[[ 1., 1.], [ 1., 1.], [ 1., 1.]], [[ 1., 1.], [ 1., 1.], [ 1., 1.]]]) 123a.unsqueeze(0).expand(2,3,2) + b.expand(2,3,2) tensor([[[ 1., 1.], [ 1., 1.], [ 1., 1.]], [[ 1., 1.], [ 1., 1.], [ 1., 1.]]]) 12345# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存e = a.unsqueeze(0).expand(10000000000000, 3,2) 内部结构tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。 一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。 12345a = torch.arange(0,6)a tensor([ 0., 1., 2., 3., 4., 5.]) 123a.storage() 0.0 1.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 12345b = a.view(2, 3)b.storage() 0.0 1.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 1234567# 一个对象的id值可以看作它在内存中的地址# storage的内存地址一样，即是同一个storageid(b.storage()) == id(a.storage()) True 12345c = torch.arange(0, 6)c.storage() 0.0 1.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 1234567# 一个对象的id值可以看作它在内存中的地址# storage的内存地址一样，即是同一个storageid(c.storage()) == id(a.storage()) True 1234567# a改变，b也随之改变，因为他们共享storage, 但是 c 没有改变啊，很神奇a[1] = 100b, c (tensor([[ 0., 100., 2.], [ 3., 4., 5.]]), tensor([ 0., 1., 2., 3., 4., 5.])) 1234567# 一个对象的id值可以看作它在内存中的地址# storage的内存地址一样，即是同一个storageid(c[1].storage()), id(c.storage()) (139719200619016, 139719200619016) 1234567c = a[2:]print(c)c.storage() tensor([ 2., 3., 4., 5.]) 0.0 100.0 2.0 3.0 4.0 5.0 [torch.FloatStorage of size 6] 12345c.data_ptr(), a.data_ptr() # data_ptr返回tensor首元素的内存地址# 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float) (94551854283064, 94551854283056) 12345c[0]=-100 # c[0]的内存地址对应 a[2] 的内存地址a tensor([ 0., 100., -100., 3., 4., 5.]) 1234567d = torch.Tensor(c.storage())d[0] = 6666b tensor([[ 6666., 100., -100.], [ 3., 4., 5.]]) 12345# 下面４个tensor共享storageid(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage()) True 123a.storage_offset(), c.storage_offset(), d.storage_offset() (0, 2, 0) 12345e = b[::2, ::2] # 隔2行/列取一个元素id(e.storage()) == id(a.storage()) True 123e.is_contiguous() False 可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。 此外有些操作会导致tensor不连续，这时需调用tensor.contiguous方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。 另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。 持久化Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的pickle模块，在load时还可将GPU tensor映射到CPU或其它GPU上。 123456789101112131415161718192021if torch.cuda.is_available(): a = a.cuda() # 把a转为GPU1上的tensor, torch.save(a,'a.pth') # 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上) b = torch.load('a.pth') # 加载为c, 存储于CPU c = torch.load('a.pth', map_location=lambda storage, loc: storage) # 加载为d, 存储于GPU0上 d = torch.load('a.pth', map_location={'cuda:1':'cuda:0'}) 12345a = torch.load(&quot;a.pth&quot;)print(a) tensor([ 6666., 100., -100., 3., 4., 5.], device='cuda:0') 向量化向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是for循环。在科学计算程序中应当极力避免使用Python原生的for循环。 1234567891011def for_loop_add(x, y): result = [] for i,j in zip(x, y): result.append(i + j) return torch.Tensor(result) 123456789x = torch.zeros(100)y = torch.ones(100)%timeit -n 10 for_loop_add(x, y)%timeit -n 10 x + y 351 µs ± 9.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) The slowest run took 16.46 times longer than the fastest. This could mean that an intermediate result is being cached. 4.24 µs ± 7.12 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) 可见二者有超过40倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯。 此外还有以下几点需要注意： 大多数torch.function都有一个参数out，这时候产生的结果将保存在out指定tensor之中。 torch.set_num_threads可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。 torch.set_printoptions可以用来设置打印tensor时的数值精度和格式。 下面举例说明。 1234567a = torch.randn(2,3)torch.set_printoptions(precision=10)a tensor([[-0.3306640089, -0.0507176071, -0.4223535955], [-0.8678948879, -0.0437202156, 0.0183448847]]) 线性回归线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数： $$ loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2 $$ 然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。 123456789import torch as t%matplotlib inlinefrom matplotlib import pyplot as pltfrom IPython import display 12345678910111213141516171819202122232425# 设置随机数种子，保证在不同电脑上运行时下面的输出一致t.manual_seed(1000)def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1) * 20 y = x * 2 + (1 + t.randn(batch_size, 1))*3 return x, y# 来看看产生的x-y分布x, y = get_fake_data()plt.scatter(x.squeeze().numpy(), y.squeeze().numpy()) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# 随机初始化参数w = torch.randn(1,1)b = torch.zeros(1,1)lr = 0.001 # 学习率for epoch in range(20000): x, y = get_fake_data(batch_size=8) # forward y_pred = x.mm(w) + b.expand_as(y) loss = 0.5 * (y_pred - y) ** 2 loss = loss.sum() # backward: 手动计算梯度 dloss = 1 dy_pred = dloss * (y_pred - y) dw = x.t().contiguous().mm(dy_pred) db = dy_pred.sum() # 更新参数 w.sub_(lr * dw) b.sub_(lr * db) if epoch % 1000 == 0: print(&quot;epoch:{}, loss:{}&quot;.format(epoch, loss)) # 画图 display.clear_output(wait=True) x = torch.arange(0, 20).view(-1, 1) # [20, 1] y = x.mm(w) + b.expand_as(x) # predicted data plt.plot(x.numpy(), y.numpy()) x2, y2 = get_fake_data(batch_size=20) # true data plt.scatter(x2.numpy(), y2.numpy()) plt.xlim(0,20) plt.ylim(0,41) plt.show() plt.pause(0.5)print(w.squeeze()[0], b.squeeze()[0]) tensor(2.0264241695) tensor(2.9323694706)","link":"/2018/12/01/pytorch-book-1-Tensor/"},{"title":"rejection系列1-overview","text":"关于 open set recognition 的一片综述。 paper: Recent Advances in Open Set Recognition: A Survey Motivation In real-world recognition/classification tasks, limited by various objective factors, it is usually difficult to collect training samples to exhaust all classes when training a recognizer or classifier. A more realistic scenario is open set recognition (OSR), where incomplete knowledge of the world exists at training time, and unknown classes can be submitted to an algorithm during testing, requiring the classifiers not only to accurately classify the seen classes, but also to effectively deal with the unseen ones. 现实中对于分类任务，不可能在训练集中穷尽所有类别。更实际的情况是 open set recognition (OSR). 在训练阶段包含的是不完整的 knowledge of world. 在测试阶段会出现 unknown 类别。这需要分类器不仅能准确的识别在训练阶段已经见到过的类别，也能有效的处理没有见过的类别, 比如 rejection 或者归类为 unknown. This paper provides a comprehensive survey of existing open set recognition techniques covering various aspects ranging from related definitions, representations of models, datasets, experiment setup and evaluation metrics. Furthermore, we briefly analyze the relationships between OSR and its related tasks including zero-shot, one-shot (few-shot) recognition/learning techniques, classification with reject option, and so forth. Additionally, we also overview the open world recognition which can be seen as a natural extension of OSR. Importantly, we highlight the limitations of existing approaches and point out some promising subsequent research directions in this field. 这篇综述覆盖了相关的定义、模型、实验以及验证指标。更多地，还分析了 与OSR 相关的任务 zero-shot, one-shot 识别，以及 rejection. 额外地，还概述了 open world recognition 可以看作 OSR 的扩展。更重要的是，作者说明了当前一些方法的限制，并指出了未来研究的一些方向。 Introduction a more realistic scenario is usually open and non-stationary such as driverless, fault/medical diagnosis, etc., where unseen situations can emerge unexpectedly, which drastically weakens the robustness of these existing methods. 更现实的场景是 开放的和非静态 的。 To meet this challenge, several related research directions actually have been explored including lifelong learning [1], [2], transfer learning [3]–[5], domain adaptation [6], zero-shot [7]–[9], one-shot (few-shot) [10]–[16] recognition/learning and open set recognition/classification [17]–[19], and so forth. 涉及到的领域： lifelong learning, transfer learning, domain adaption, zero-shot, one-shot, open set recogntion. recognition should consider four basic categories of classes as follows: known known: train/dev 中有标签的样本，包括正负类别，并且有相关的语义信息。 known unknown: train/dev 中有标签的样本，负类，没有相关的语义信息。 unknown known: test 中没有出现在 train 中的样本，但是有相关的语义信息。比如，train 中有猫，然后 test 中有另外一种猫科动物，那么动物这个样本是有意义的吧？？？ unknown unknown: test 中没有出现在 train 中的样本，并且没有任何相关的语义信息。 Unlike the traditional classification, zero-shot learning (ZSL) can identify unseen classes which have no available observations in training. However, the available semantic/attribute information shared among all classes including seen and unseen ones are needed. zero-shot 是针对 unknown known, 也就是包含了语义信息。 The ZSL mainly focuses on the recognition of the unknown known classes defined above. Actually, such a setting is rather restrictive and impractical, since we usually know nothing about the testing samples which may come from known known classes or not. unknown known 这种设定很有限，并且不切实际。因为我们很难知道 test 中的样本是否是包含了语义信息，无法判断是 unknown known or unknown unknown. comparision between open set recognition and traditional classification Via these decision boundaries, samples from an unknown unknown class are labeled as ”unknown” or rejected rather than misclassified as known known classes. Basic notation and related definition经验风险函数： $L(x, y, f(x)) \\ge 0$ 是 loss function. P(x,y) 是对应样本 (x, y) 的概率，通常这个联合分布的概率我们是不知道的，因为我们无法确定自然界中样本空间(label space)到底是个什么分布。 [李航，机器学习] 中关于风险函数的定义： 损失函数度量一次模型预测的好坏，风险函数度量平均意义下模型预测的好坏。 以前看不懂这一部分，现在只想说： Perfect! Therefore, traditional recognition/classification approaches minimize the empirical risk instead of the ideal risk RI by using other knowledge, such as assuming that the label space is at least locally smooth and regularizing the empirical risk minimization. 传统的分类方法是根据其他的外部知识来最小化经验风险，比如 label space 是光滑的，然后使用正则化最小经验风险（也就是上面所说的结构风险函数）。 Note that traditional recognition problem is usually performed under the closed set assumption. When the assumption switches to open environment/set scenario with the open space, other things should be added since intuitively there is some risk in labeling sample in the open space as any known known classes. This gives such an insight for OSR that we do know something else: we do know where known known classes exist, and we know that in open space we do not have a good basis for assigning labels for the unknown unknown classes. 传统的识别是假设在固定的样本空间下(known known). 当转换到开放式场景下，我们很敏感的意识到需要给 label space 加点 risk。。我们知道 known known classes 是存在的，我们也知道我们并没有这样一个 basis 去给 unknown unknown 打标签。 open space risk这部分的内容主要引自这篇 paper: 17. Toward Open Set Recognition 这篇 paper 是把 class of interest 当作一个类，然后所有的 unknown/known 当作很多 classes, 也就是 1-vs-set. To improve the overall open set recognition error, our 1-vs-set formulation balances the unknown classes by obtaining a core margin around the decision boundary A from the base SVM, specializing the resulting half-space by adding another plane $\\Omega$ and then generalizing or specializing the two planes (shown in Fig. 2) to optimize empirical and open space risk. This process uses the open set training data and the risk model to define a new “open set margin.” The second plane $\\Omega$ allows the 1-vs-set machine to avoid the overgeneralization that would misclassify the raccoon in Fig. 2. The overall optimization can also adjust the original margin with respect to A to reduce open space risk, which can avoid negatives such as the owl. 使用了两个超平面，去分隔 Negatives/positivecs/unknown. While we do not know the joint distribution $P(x, y)$ in, one way to look at the open space risk is as a weak assumption: Far from the known data the Principle of Indifference [8] suggests that if there is no known reason to assign a probability, alternatives should be given equal probability. In our case, this means that at all points in open space, all labels (both known and unknown) are equally likely, and risk should be computed accordingly. However, we cannot have constant value probabilities over infinite spaces—the distribution must be integrable and integrate to 1. We must formalize open space differently (e.g., by ensuring the problem is well posed and then assuming the probability is proportional to relative Lebesgue measure [9]). Thus, we can consider the measure of the open space to the full space, and define our risk penalty proportional to such a ratio. 无法知道联合分布 $P(x, y)$, 作者假设所有的样本概率是相等的，但是向量空间中样本总数是不确定的，所以作者定义一个比例来描述 在 open space 中出现 unknown 的危险惩罚系数。 where open space risk is considered to be the fraction (in terms of Lebesgue measure) of positively labeled open space compared to the overall measure of positively labeled space (which includes the space near the positive examples). open space risk 是开放空间中 positive label 的总数与总体空间中 positive label 的总体度量。 不太懂。。问题还是不知道怎么度量？ unknown 的类别能确定？？？ opennessopenness，用来表征数据集的开放程度： $C_{TR}$ 是训练集中的类别数，越大，开放程度越小。 $C_{TE}$ 测试集中的类别数。 The Open Set Recognition Problem our goal is to balance the risk of the unknown in open space with the empirical (known) risk. In this sense, we formally define the open set recognition problem as follows: 我们的目的是平衡 the risk of unknown 出现在基于 known classes 计算的到的 open space 的 empirical risk。 怎么理解呢？就是传统的风险函数都是只考虑了经验风险，也就是完全基于训练数据的。但是在 open space 里面，我们还要测试时会出现的 unknown，所以在 风险函数的设置的同时，就要考虑到 unknown 的存在。也就是前面的 open space risk. a categorization of OSR techniques问题的关键在于如何将 公式（4）open space risk 合并到模型中去。然后大佬们提出各式各样的模型，主要分为 discriminative model and generative models. 更进一步，可以分为：five categories (Table II): Traditional ML-based Deep Network-based Adversarial Learning-based EVT-based Dirichlet Process-based OSR models Deep Neural Network-based OSR Models大佬们的杰作，感觉都挺新的，新坑？ 提出了 OpenMax,使用 deep networks, 还是用 softmax 损失函数来最小化 交叉熵 cross entropy loss. 然后在网络的倒数第二层（softmax 的前一层？）得到每一个正分类的 mean activate vector(MAV). 然后是根据 Weibull districution 去 redistribution 以及重新分类等等接下来的操作还是看相应 的 paper 吧。 the OpenMax effectively addressed the challenge of the recognition for fooling/rubbish and unrelated open set images. However, as discussed in [71], the OpenMax fails to recognize the adversarial images which are visually indistinguishable from training samples but are designed to make deep networks produce high confidence but incorrect answers [96], [98]. OpenMax 有效的解决了 不相关的 open set images 的问题，但是却无法有效区分对抗生成样本。 Actually, the authors in [72] have indicated that the OpenMax is susceptible to the adversarial generation techniques directly working on deep representations. Therefore, the adversarial samples are still a serious challenge for open set recognition. Furthermore, using the distance from MAV, the cross entropy loss function in OpenMax does not directly incentivize projecting class samples around the MAV. In addition to that, the distance function used in testing is not used in training, possibly resulting in inaccurate measurement in that space [73]. To address this limitation, Hassen and Chan [73] learned a neural network based representation for open set recognition, which is similar in spirit to the Fisher Discriminant, where samples from the same class are closed to each other while the ones from different classes are further apart, leading to larger space among known known classes for unknown unknown classes’ samples to occupy. 交叉熵并不能有效的将类别映射到相应的 MAV 中，因为在测试集中的 distence function 跟在 training set 里面是不一样的，这会导致不准确的判别。基于此，[73]提出了 Fisher 判别，从同一个类别中采样，使得unknown unknown 和 known known 的间距很大。 OpenMax to text classification Deep Open classifier tWiSARD hidden unknown unknown classes Adversarial Learning-based OSR Models Note that the main challenge for open set recognition is the incomplete class knowledge existing in training, leading to the open space risk when classifiers encounter unknown unknown classes during testing. Fortunately, the adversarial learning technique can account for open space to some extent by adversarially generating the unknown unknown class data according to the known known class knowledge, which undoubtedly provides another way to tackle the challenging multiclass OSR problem. open set recognition 最大的挑战是 training 中不完整的 knowledge， 在 testing 中遇到 unknown unknown 导致 open space risk. 而对抗训练网络在某种程度上根据 known known 生成 unknown unknown，提供了另外一种方式解决 OSR 问题。 EVT-based OSR Models As a powerful tool to increase the classification performance, the statistical Extreme Value Theory (EVT) has recently achieved great success due to the fact that EVT can effectively model the tails of the distribution of distances between training observations using the asymptotic theory[100]. 不是很懂这个理论，给出几篇 paper 吧 Remark: As mentioned above, almost all existing OSR methods adopt the threshold-based classification scheme, where recognizers in decision either reject or categorize the input samples to some known known class using empirically set threshold. Thus the threshold plays a key role. However, the selection for it usually depends on the knowledge of known known classes, inevitably incurring risks due to lacking available information from unknown unknown classes [57]. This indicates the threshold-based OSR methods still face serious challenges. 基于 known known 得到的 threshold 因为缺乏 unknown unknown 的信息，不可避免的会造成 risk, 这也是基于 threshold 这类方法所面临的困难。 Dirichlet Process-based OSR Models (生成模型) Dirichlet process (DP) [104]–[108] considered as a distribution over distributions is a stochastic process, which has been widely applied in clustering and density estimation problems as a nonparametric prior defined over the number of mixture components. Furthermore, this model does not overly depend on training samples and can achieve adaptive change as the data changes, making it naturally adapt to the open set recognition scenario. In fact, researchers have begun the related research Dirichlet 过程作为一种基于混合模型的非参数方法广泛用于聚类，参数估计。这种模型不需要依赖于 training，可以随着 dataset 的变化而自适应的变化，这使得它能有效的适用于 open set 的场景。 对生成模型不是很熟。。 Remark: Instead of addressing the OSR problem from the discriminative model perspective, CD-OSR actually reconsiders this problem from the generative model perspective due to the use of HDP, which provides another research direction for open set recognition. Furthermore, the collective decision strategy for OSR is also worth further exploring since it not only takes the correlations among the testing samples into account but also provides a possibility for new class discovery, whereas single-sample decision strategy2 adopted by other existing OSR methods can not do such a work since it can not directly tell whether the single rejected sample is an outlier or from new class. Beyond open set Recognition关于 open set recognition 如果仅仅考虑静态的 set，意义不是很大。以及，只对 unknown unknown 进行 rejection 也是不够的。为此，有人提出 open world recognition. open world recognition (OWR), where a recognition system should perform four tasks: detecting unknown unknown classes choosing which samples to label for addition to the model labelling those samples updating the classifier Remark: As a natural extension of OSR, the OWR faces more serious challenges which require it not only to have the ability to handle the OSR task, but also to have minimal downtime, even to continuously learn, which seems to have the flavor of lifelong learning to some extent. Besides, although some progress regarding the OWR has been made, there is still a long way to go. 终身学习。。666 Dataset and evalution metricsdataset https://dx.doi.org/10.6084/m9.figshare.1097614 https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/multi-class.html Experiment Setup: In open set recognition, most existing experiments are carried out on a variety of recastes multi-class benchmark datasets. Specifically, taking the Usps dataset as an example, when it is used for OSR problem, one can randomly choose S distinct labels as the known known classes, and vary openness by adding a subset of the remaining labels. 可以增加减少类别数来改变 openness. Evaluation Metrics for Open Set Recognition TP： true positive FP: false positive TN: true negative FN: false negative TU: true unknown FU: false unknown accuracy对于 closed set : $$\\text{accuracy}=\\dfrac{TP+TN}{TP+TN+FP+FN}$$ 对于 open set: $$\\text{accuracy}_O=\\dfrac{(TP+TN)+TU}{(TP+TN+FP+FN)+(TU+FU)}$$ 对于不均衡情况，accuracy 并不能客观的评价模型好坏。比如在testing 中，unknown unknown 样本数量很多,那么如果分类器把所有的类别都判为 unknown unknown，它的准确率依旧很高。 于是，有人提出了 normalized accuracy(NA). $0\\le \\lambda \\le 1$ 是正则化常数。 F-measureF1: $$F1=\\dfrac{2\\text{precision} \\text{recall}}{\\text{precision}+\\text{recall}}$$ $$precision=\\dfrac{TP}{TP+FP}$$ 精度： 预测得到的 positive 中真正是 positive 的概率。 $$recall=\\dfrac{TP}{TP+FN}$$ 召回： 所有真正 positive 的样本被预测为 positive 的概率。 在 open set 场景下，F1 值无法考虑 unknown unknown. Instead, the computations of Precision and Recall in it are only for available known known classes. Additionally, the work [67] has indicated that although the computations of Precision and Recall are only for available known known classes, the FN and FP also consider the false unknown unknown classes and false known known classes by taking into account the false negative and the false positive, and we refer the reader to [67] for more details. 事实上，在 FP 和 FN 中可能也包括 false unknown unknown, 这就有问题了是吧。。 详细参考这篇 paper Nearest neighbors distance ratio open-set classifier Note that the Precision contains the macro-Precision and micro-Precision while Recall includes the macro-Recall and micro-Recall, which leads to the corresponding macro-F-measure and micro-F-measure. Nevertheless, whether it is macro-F-measure or micro-F-measure, the higher their values, the better the performance of the corresponding OSR model. Youden’s index for OSR$$J= \\text{Recall}+S-1$$ 其中 S 是真负类率： $S=\\dfrac{TN}{TN+FP}$ future research directionsAbout modeling 大部分工作都是基于判别模型来做的，只有少部分是基于生成模型，也许生成模型会更有探索空间。 OSR 的主要挑战是传统的分类器是在 closed-set 场景下获得的，一旦 unknown unknown class 落入这个空间，将永远无法被正确的分类。 modeling known known classes如果得到的 known known class 没有被过拟合，那么这样的分类器就能有效的区分出 unknown unknown. 所以聚类和分类算法的结合会是不错的方向。关于 clustering 和 classification 的 unified learning framework: 这两篇 paper 依旧是在 closed-set 下做的，所以需要你去尝试。。。 modeling unknown unknown classes似乎在只有 known known classes 的情况下是很难去学习 unknown unknown 的类的性质的。但是可以通过对抗学习来生成 unknown unknown 也是不错的方向。 顺便作者还提了下 transductive leanring，以及基于 Dirichlet process 的自适应行，CD-OSR、Dirichlet processed-based OSR 也是值得探索的。 About rejecting大部分的工作都是 reject unknown unknown classes，而没有后续的工作了。只有少量的 [66][67]进行了后续的工作，比如 new classes discovery. About the decision所有的 OSR 模型都是用来识别单个样本的，但是一个决策的决定并没有考虑样本之间的相关性。所以 collective decision 不仅在 testing 时考虑相关性，同时还能发现 new classes. Open set + ‘sth’As open set scenario is a more practical assumption for the real-world classification/recognition tasks, it can naturally be combined with various fields involving classification/recognition such as semi-supervised learning, domain adaptation, active learning, multi-task learning, multi-label learning, multi-view learning, and so forth. For example, [124]–[126] recently introduced this scenario into domain adaptation, while [127] explored the open set classification in active learning field. Therefore, many interesting works are worth looking forward to. 看起来是个不错的方向。。 Generalized Open Set Recognition利用 side-information,比如 unknown unknwon 和 known known 会有共同的语义信息(semantic/attribute information). Appending semantic/attribute information In fact, a lot of semantic/attibute information is shared between the known known and the unknown unknown classes. Therefore, we can fully utilize this kind of information to ’cognize’ the unknown unknown classes, or at least to provide a rough semantic/attribute description for the corresponding unknown unknown classes instead of simply rejecting them. 利用语义信息去意识到 unknown unknwon，而不是简单的 reject. 但是要注意区分 open set recognition 和 ZSL(zero-shot learning) 的区别： The $\\text{side-information}^1$ in ZSL denotes the semantic/attribute information shared among all classes including known known and unknown known classes. where the $\\text{side-information}^4$ denotes the available semantic/attribute information only for known known classes 感觉这个 side-information 的界限很难确定啊？Generalized Open Set Recognition 的这个范围似乎很难实现， 怎么可能出现在 training 中的 semantice information 完全不出现在 unknown unknown 中呢。。 还有一些相似的工作： Using other available side-information **The main reason for open space risk is that the traditional classifiers trained under closed set scenario usually divide over-occupied space for known known classes, thus inevitably resulting in misclassifications once the unknown unknown class samples fall into the space divided for some known known class.** From this perspective, the open space risk will be reduced as the space divided for those known known classes decreases by using other side-information like universum [135], [136] to shrink their regions as much as possible. 虽然感觉很扯淡。。但是还是有人做啊，不过关于 open space risk 的定义可以在看一遍。。 Relative Open Set Recognition感觉这个还挺有意思的。疾病的诊断，所有的样本空间都可以区分为 sick or no sick, 所以仅仅是判断有没有病，那么这是个 closed set 问题。但是如果我们要进一步判断疾病的类型，那么有可能出现 unseen disease in training. Knowledge Integration for Open Set Recognition In fact, the incomplete knowledge of the world is universal, especially for the single individuals: something you know does not mean I also know. how to integrate the classifiers trained on each sub-knowledge set to further reduce the open space risk will be an interesting yet challenging topic in the future work, especially for such a situation: we can only obtain the classifiers trained on corresponding sub-knowledge sets, yet these sub-knowledge sets are not available due to the privacy protection of data. 利用知识库来减小 open space risk。 似乎这个看起来比较靠谱，因为 unknown 范围确实很难定义，如果给个外部知识库给你，把跟知识库相关的 unknown 识别出来，就很棒了吧 相关的一些开源工具和代码：","link":"/2018/12/09/rejection%E7%B3%BB%E5%88%971-overview-1/"},{"title":"rejection系列3 OpenMax","text":"paper: Towards Open Set Deep Networks. CVPR Motivationclosed set recognition 天然的特性使得它必须选择一个类别作为预测对象。但是实际场景下， recognition system 必须学会 reject unknown/unseen classes 在 testing 阶段。 于是乎，作者提出了一个新的 model layer, OpenMax, 能够估计一个样本输入是来自于 unknown class 的概率。 A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. 所以关键词是 meta-recohnition, activation pattern/vector. Introduction很多工作是基于 threshold 来找出 unknown 的，他们认为 unknwon 通过 softmax 会得到 low probability/confidence. 但是实际上很多 “fooling” “rubbish” 也会拥有 high probability/confidence scores. 比如通过对抗学习得到的 adversarial images. 作者在后面也提到了， threshold 实际上拒绝的不是 unknown, 而是 uncertain predictions. OpenMax incorporates likelihood of the recognition system failure. This likelihood is used to estimate the probability for a given input belonging to an unknown class. For this estimation, we adapt the concept of Meta-Recognition[22, 32, 9] to deep networks. We use the scores from the penultimate layer of deep networks (the fully connected layer before SoftMax, e.g., FC8) to estimate if the input is “far” from known training data. We call scores in that layer the activation vector(AV). 关于 OpenMax 如果实现的简单总结，回过头在看。 A key insight in our opening deep networks is noting that “open space risk” should be measured in feature space rather than in pixel space. 一个重要的观点是，在 open deep networks 里面， open space risk 应该是从特征空间 feature space 的角度出发的， 而不是 pixel space. 也就是神经网络判断是不是 unknown, 应该是从 feature 的角度来看的。 We show that an extreme-value meta-recognition inspired distance normalization process on the overall activation patterns of the penultimate network layer provides a rejection probability for OpenMax normalization for unknown images, fooling images and even for many adversarial images. Open set deep networks Building on the concepts of open space risk, we seek to choose a layer (feature space) in which we can build a compact abating probability model that can be thresholded to limit open space risk. 基于 open space risk 的概念，提出了 compact abating probability model 能限制 open space risk. multi-classes meta-recognition作者先简单介绍了一下前人的工作: . Prior work on meta-recognition used the final system scores, analyzed their distribution based on Extreme Value Theory (EVT) and found these distributions follow Weibull distribution. 感觉看懂这部分先要理解极值理论(Extreme value theory). from wikipedia: It seeks to assess, from a given ordered sample of a given random variable, the probability of events that are more extreme than any previously observed. 它试图从给定随机变量的给定有序样本中评估比先前观察到的任何事件更极端的事件的概率. 然后是 极值分布 的一种 Weibull distribution 所以 Weibull distribution 就是从整个分布中取最极端的例子 sampling top-n score，然后的到的分布。 将极值理论运用到视觉特征的提取中。具体的我也不太清楚了。。这也是前人的研究。作者也并没有采取这种方法。 We take the approach that the network values from penultimate layer (hereafter the Activation Vector (AV)), are not an independent per-class score estimate, but rather they provide a distribution of what classes are “related.” 作者采用的方法是 倒数第二层，也就是 (Activation Vector) 提供不同 classes 之间的相关性分布，而不是每一个类对应的独立的分布。 interpretation of activation vectorOpenMax","link":"/2018/12/11/rejection%E7%B3%BB%E5%88%973-OpenMax/"},{"title":"Tensorflow Attention API 源码阅读1","text":"这节内容是详细了解下 tensorflow 关于 attention 的api，由于封装的太好，之前使用过发现报错了很难去找出哪儿 bug，所以用 eager execution 来看具体细节。 按照官方教程 Seq2seq Library (contrib) 这里的流程逐步深入。 This library is composed of two primary components: New attention wrappers for tf.contrib.rnn.RNNCell objects. A new object-oriented dynamic decoding framework. 主要包括两个部分，一个是新的基于 attention 的 RNNCell 对象，一个面向对象的动态解码框架。 AttentionAttention wrappers are RNNCell objects that wrap other RNNCell objects and implement attention. The form of attention is determined by a subclass of tf.contrib.seq2seq.AttentionMechanism. These subclasses describe the form of attention (e.g. additive vs. multiplicative) to use when creating the wrapper. An instance of an AttentionMechanism is constructed with a memory tensor, from which lookup keys and values tensors are created. attenion wrapper 也是 RNNCell 对象，父类是 tf.contrib.seq2seq.AttentionMechanism,然后其子类是针对不同 attention 形式（additive vs. multiplicative）的实现。AttentionMechanism 的构造是在 memory 的基础上，memory 也就是 attention 过程中的 keys values. Attention Mechanismsattention 的提出来自于： paper: Neural Machine Translation by Jointly Learning to Align and Translate paper:Effective Approaches to Attention-based Neural Machine Translation encoder 采用单层或多层的单向或双向的 rnn 得到source sentence 的隐藏状态表示 $H=[h_1,…,h_T]$。 decoder 的 t 时间步的隐藏状态为 $s_t$, 在 decoder 阶段也是 rnn，其中隐藏状态的更新为： $s_i=f(s_{i-1},y_{i-1},c_i)$ 其中 $s_{i-1}$ 是上一个隐藏状态，$y_{i-1}$ 是上一时间步的输出，$c_i$ 是当前时间步的 attention vector. 那么现在就是怎么计算当前时间步的 $c_i$. 当前时间步的 $e_t=a(s_{i-1}, h_j)$, 这是对齐模型，也就是计算上一个隐藏状态 $s_{i-1}$ 与 encoder 中每一个 hidden 的 match 程度，计算这个 score 有很多中方式，其中最常见的，也是 tf api 中使用的两种 BahdanauAttention 和 LuongAttention. $$\\text{BahdanauAttention:}\\quad e_{ij}=v_a^Ttanh(W^as_{i-1}+U_ah_j)$$ $$\\text{LuongAttention:}\\quad e_{ij}=h_j^TW^as_i$$ 然后对得到的对齐 score 使用 softmax 得到相应的概率: $$a_{ij}=\\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$$ softmax 实际上相比上面的公式有点区别，就是 $exp(e^{ij}-max(e^{ik}))$ 防止数值溢出。 将得到的 $s_{i-1}$ 与 encoder 中的 $h_j$ 计算得到的概率与 $h_j$ 做加权和得到当前时间步的 attention vector $c_i$ 在然后使用 $c_{i-1},s_{i-1},y_{i-1}$ 更新decoder 中的隐藏状态，循环下去。。。 根据当前的隐藏状态 $s_i$ 计算得到当前时间步的输出 $y_t$ $$y_t=Ws_{i}+b$$ 先看父类 tf.contrib.seq2seq.AttentionMechanism源码： 12345678910111213141516171819class AttentionMechanism(object): @property def alignments_size(self): raise NotImplementedError @property def state_size(self): raise NotImplementedError 两个属性： alignments_size 和 state_size 分别对应 sequence 的长度，所以这个 alignment_size 是表示 mask 之后的长度吧？接下来看源码。 state_size 表示隐藏层的状态。显然这里的 attention 也是一个时间步内的计算。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261class _BaseAttentionMechanism(AttentionMechanism): &quot;&quot;&quot;A base AttentionMechanism class providing common functionality. Common functionality includes: 1. Storing the query and memory layers. 2. Preprocessing and storing the memory. &quot;&quot;&quot; def __init__(self, query_layer, memory, probability_fn, memory_sequence_length=None, memory_layer=None, check_inner_dims_defined=True, score_mask_value=None, name=None): &quot;&quot;&quot;Construct base AttentionMechanism class. Args: query_layer: Callable. Instance of `tf.layers.Layer`. The layer's depth must match the depth of `memory_layer`. If `query_layer` is not provided, the shape of `query` must match that of `memory_layer`. memory: The memory to query; usually the output of an RNN encoder. This tensor should be shaped `[batch_size, max_time, ...]`. probability_fn: A `callable`. Converts the score and previous alignments to probabilities. Its signature should be: `probabilities = probability_fn(score, state)`. memory_sequence_length (optional): Sequence lengths for the batch entries in memory. If provided, the memory tensor rows are masked with zeros for values past the respective sequence lengths. memory_layer: Instance of `tf.layers.Layer` (may be None). The layer's depth must match the depth of `query_layer`. If `memory_layer` is not provided, the shape of `memory` must match that of `query_layer`. check_inner_dims_defined: Python boolean. If `True`, the `memory` argument's shape is checked to ensure all but the two outermost dimensions are fully defined. score_mask_value: (optional): The mask value for score before passing into `probability_fn`. The default is -inf. Only used if `memory_sequence_length` is not None. name: Name to use when creating ops. &quot;&quot;&quot; if (query_layer is not None and not isinstance(query_layer, layers_base.Layer)): raise TypeError( &quot;query_layer is not a Layer: %s&quot; % type(query_layer).__name__) if (memory_layer is not None and not isinstance(memory_layer, layers_base.Layer)): raise TypeError( &quot;memory_layer is not a Layer: %s&quot; % type(memory_layer).__name__) self._query_layer = query_layer self._memory_layer = memory_layer self.dtype = memory_layer.dtype if not callable(probability_fn): raise TypeError(&quot;probability_fn must be callable, saw type: %s&quot; % type(probability_fn).__name__) if score_mask_value is None: score_mask_value = dtypes.as_dtype( self._memory_layer.dtype).as_numpy_dtype(-np.inf) self._probability_fn = lambda score, prev: ( # pylint:disable=g-long-lambda probability_fn( _maybe_mask_score(score, memory_sequence_length, score_mask_value), prev)) with ops.name_scope( name, &quot;BaseAttentionMechanismInit&quot;, nest.flatten(memory)): self._values = _prepare_memory( memory, memory_sequence_length, check_inner_dims_defined=check_inner_dims_defined) self._keys = ( self.memory_layer(self._values) if self.memory_layer # pylint: disable=not-callable else self._values) self._batch_size = ( self._keys.shape[0].value or array_ops.shape(self._keys)[0]) self._alignments_size = (self._keys.shape[1].value or array_ops.shape(self._keys)[1]) @property def memory_layer(self): return self._memory_layer @property def query_layer(self): return self._query_layer @property def values(self): return self._values @property def keys(self): return self._keys @property def batch_size(self): return self._batch_size @property def alignments_size(self): return self._alignments_size @property def state_size(self): return self._alignments_size def initial_alignments(self, batch_size, dtype): &quot;&quot;&quot;Creates the initial alignment values for the `AttentionWrapper` class. This is important for AttentionMechanisms that use the previous alignment to calculate the alignment at the next time step (e.g. monotonic attention). The default behavior is to return a tensor of all zeros. Args: batch_size: `int32` scalar, the batch_size. dtype: The `dtype`. Returns: A `dtype` tensor shaped `[batch_size, alignments_size]` (`alignments_size` is the values' `max_time`). &quot;&quot;&quot; max_time = self._alignments_size return _zero_state_tensors(max_time, batch_size, dtype) def initial_state(self, batch_size, dtype): &quot;&quot;&quot;Creates the initial state values for the `AttentionWrapper` class. This is important for AttentionMechanisms that use the previous alignment to calculate the alignment at the next time step (e.g. monotonic attention). The default behavior is to return the same output as initial_alignments. Args: batch_size: `int32` scalar, the batch_size. dtype: The `dtype`. Returns: A structure of all-zero tensors with shapes as described by `state_size`. &quot;&quot;&quot; return self.initial_alignments(batch_size, dtype) 这个类 _BaseAttentionMechanism 是最基本的 attention 类了。可以看到 self._keys 和 self._values 的计算方式都是需要考虑 memory_sequence_length 这个参数的。 有这几个属性： values: 其计算使用了 _prepare_memory 函数对应的是把输入序列 memory 的超过对应实际长度的部分的值变为 0 keys： self._keys = self.memory_layer(self._values) 是在得到了 values 之后进行全链接的值，其shape=[batch, max_times, num_units] state_size 和 alignment_size 是一样的，都是 max_times self._probability_fn(score, prev) 使用了 _maybe_mask_score 这个函数计算得到 score 之后并 mask 的概率，然后还要利用 prev state? _maybe_mask_score源码： 123456789101112131415161718192021def _maybe_mask_score(score, memory_sequence_length, score_mask_value): if memory_sequence_length is None: return score message = (&quot;All values in memory_sequence_length must greater than zero.&quot;) with ops.control_dependencies( [check_ops.assert_positive(memory_sequence_length, message=message)]): score_mask = array_ops.sequence_mask( memory_sequence_length, maxlen=array_ops.shape(score)[1]) score_mask_values = score_mask_value * array_ops.ones_like(score) return array_ops.where(score_mask, score, score_mask_values) 12345score = tf.random_uniform(shape=[2,10])tf.shape(score).numpy() array([ 2, 10], dtype=int32) 12345678910111213141516171819score = tf.random_uniform(shape=[2,10])memeory_sequence_len = [5,8]score_mask_value = -100000000score_mask = tf.sequence_mask(lengths=memeory_sequence_len, maxlen=tf.shape(score)[1])print(&quot;true or false: %s\\n&quot; %score_mask)score_mask_values = score_mask_value * tf.ones_like(score)print(&quot;-inf: %s\\n&quot;%score_mask_values)ans = tf.where(score_mask, score, score_mask_values)print(ans) true or false: tf.Tensor( [[ True True True True True False False False False False] [ True True True True True True True True False False]], shape=(2, 10), dtype=bool) -inf: tf.Tensor( [[-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08] [-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08]], shape=(2, 10), dtype=float32) tf.Tensor( [[ 2.3987615e-01 4.9896538e-01 7.2822869e-01 4.7516704e-02 1.6099060e-01 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08] [ 3.5503960e-01 2.5502288e-01 8.1264114e-01 4.3110681e-01 1.1858845e-01 2.5748730e-02 4.8437893e-01 2.8339624e-02 -1.0000000e+08 -1.0000000e+08]], shape=(2, 10), dtype=float32) _prepare_memory\\12345self._keys = _prepare_memory(memory, memory_sequence_length,check_inner_dims_defined=check_inner_dims_defined) 其中 _prepare_memory 这个函数,也就是怎么计算 mask 的，其计算如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107def _prepare_memory(memory, memory_sequence_length, check_inner_dims_defined): &quot;&quot;&quot;Convert to tensor and possibly mask `memory`. Args: memory: `Tensor`, shaped `[batch_size, max_time, ...]`. memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`. check_inner_dims_defined: Python boolean. If `True`, the `memory` argument's shape is checked to ensure all but the two outermost dimensions are fully defined. Returns: A (possibly masked), checked, new `memory`. Raises: ValueError: If `check_inner_dims_defined` is `True` and not `memory.shape[2:].is_fully_defined()`. &quot;&quot;&quot; memory = nest.map_structure( lambda m: ops.convert_to_tensor(m, name=&quot;memory&quot;), memory) if memory_sequence_length is not None: memory_sequence_length = ops.convert_to_tensor( memory_sequence_length, name=&quot;memory_sequence_length&quot;) if check_inner_dims_defined: def _check_dims(m): if not m.get_shape()[2:].is_fully_defined(): raise ValueError(&quot;Expected memory %s to have fully defined inner dims, &quot; &quot;but saw shape: %s&quot; % (m.name, m.get_shape())) nest.map_structure(_check_dims, memory) if memory_sequence_length is None: seq_len_mask = None else: seq_len_mask = array_ops.sequence_mask( memory_sequence_length, maxlen=array_ops.shape(nest.flatten(memory)[0])[1], dtype=nest.flatten(memory)[0].dtype) seq_len_batch_size = ( memory_sequence_length.shape[0].value or array_ops.shape(memory_sequence_length)[0]) def _maybe_mask(m, seq_len_mask): rank = m.get_shape().ndims rank = rank if rank is not None else array_ops.rank(m) extra_ones = array_ops.ones(rank - 2, dtype=dtypes.int32) m_batch_size = m.shape[0].value or array_ops.shape(m)[0] if memory_sequence_length is not None: message = (&quot;memory_sequence_length and memory tensor batch sizes do not &quot; &quot;match.&quot;) with ops.control_dependencies([ check_ops.assert_equal( seq_len_batch_size, m_batch_size, message=message)]): seq_len_mask = array_ops.reshape( seq_len_mask, array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0)) return m * seq_len_mask else: return m return nest.map_structure(lambda m: _maybe_mask(m, seq_len_mask), memory) _prepare_memory 其实很简单，就是根据 batch 中每个样本的实际长度，将超出部分设置为 0 tf.contrib.seq2seq.BahdanauAttention这里涉及到了两篇 paper: [Neural Machine Translation by Jointly Learning to Align and Translate.” ICLR 2015. ](https://arxiv.org/abs/1409.0473) [Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.”](https://arxiv.org/abs/1602.07868) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class BahdanauAttention(_BaseAttentionMechanism): &quot;&quot;&quot;Implements Bahdanau-style (additive) attention. This attention has two forms. The first is Bahdanau attention, as described in: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot; ICLR 2015. https://arxiv.org/abs/1409.0473 The second is the normalized form. This form is inspired by the weight normalization article: Tim Salimans, Diederik P. Kingma. &quot;Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.&quot; https://arxiv.org/abs/1602.07868 To enable the second form, construct the object with parameter `normalize=True`. &quot;&quot;&quot; def __init__(self, num_units, memory, memory_sequence_length=None, normalize=False, probability_fn=None, score_mask_value=None, dtype=None, name=&quot;BahdanauAttention&quot;): &quot;&quot;&quot;Construct the Attention mechanism. Args: num_units: The depth of the query mechanism. memory: The memory to query; usually the output of an RNN encoder. This tensor should be shaped `[batch_size, max_time, ...]`. memory_sequence_length (optional): Sequence lengths for the batch entries in memory. If provided, the memory tensor rows are masked with zeros for values past the respective sequence lengths. normalize: Python boolean. Whether to normalize the energy term. probability_fn: (optional) A `callable`. Converts the score to probabilities. The default is @{tf.nn.softmax}. Other options include @{tf.contrib.seq2seq.hardmax} and @{tf.contrib.sparsemax.sparsemax}. Its signature should be: `probabilities = probability_fn(score)`. score_mask_value: (optional): The mask value for score before passing into `probability_fn`. The default is -inf. Only used if `memory_sequence_length` is not None. dtype: The data type for the query and memory layers of the attention mechanism. name: Name to use when creating ops. &quot;&quot;&quot; num_units 是query mechanism 的维度. 它可以既不是 query 的维度,也可以不是 memory 的维度对吧? query 的维度要和 memory(也就是 keys/values) 的维度一致吗?是不需要的.在 BahdanauAttention 的实现中比较好理解,两个全链接最后的维度一致即可相加.但是在 LuongAttention 中矩阵矩阵相乘时需要注意维度变化. memory_sequence_length: 这个参数很重要, mask 消除 padding 的影响. score_mask_value: 上一个参数存在时,这个参数才会使用,默认为 -inf. 继续看源码的实现: 12345678910111213141516171819202122232425262728293031323334353637if probability_fn is None: probability_fn = nn_ops.softmaxif dtype is None: dtype = dtypes.float32wrapped_probability_fn = lambda score, _: probability_fn(score)super(BahdanauAttention, self).__init__( query_layer=layers_core.Dense( num_units, name=&quot;query_layer&quot;, use_bias=False, dtype=dtype), memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name)self._num_units = num_unitsself._normalize = normalizeself._name = name 现在理解了 _BaseAttentionMechanism 这个类中 query_layer 和 memory_layer 的意义了. score_mask_value 沿用父类中的计算方式. 继续看 call 函数,也就是 attention 的计算方式 123456789101112131415161718192021222324252627282930313233343536373839def __call__(self, query, state): &quot;&quot;&quot;Score the query based on the keys and values. Args: query: Tensor of dtype matching `self.values` and shape `[batch_size, query_depth]`. state: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). Returns: alignments: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). &quot;&quot;&quot; with variable_scope.variable_scope(None, &quot;bahdanau_attention&quot;, [query]): processed_query = self.query_layer(query) if self.query_layer else query score = _bahdanau_score(processed_query, self._keys, self._normalize) alignments = self._probability_fn(score, state) next_state = alignments return alignments, next_state 然后看怎么计算的 score. score = _bahdanau_score(processed_query, self._keys, self._normalize) 其中 processed_query 和 self._keys 都是通过全链接层后得到的, [batch, alignments_size, num_units] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def _bahdanau_score(processed_query, keys, normalize): &quot;&quot;&quot;Implements Bahdanau-style (additive) scoring function. &quot;&quot;&quot; dtype = processed_query.dtype # Get the number of hidden units from the trailing dimension of keys num_units = keys.shape[2].value or array_ops.shape(keys)[2] # Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting. processed_query = array_ops.expand_dims(processed_query, 1) v = variable_scope.get_variable( &quot;attention_v&quot;, [num_units], dtype=dtype) if normalize: # Scalar used in weight normalization g = variable_scope.get_variable( &quot;attention_g&quot;, dtype=dtype, initializer=init_ops.constant_initializer(math.sqrt((1. / num_units))), shape=()) # Bias added prior to the nonlinearity b = variable_scope.get_variable( &quot;attention_b&quot;, [num_units], dtype=dtype, initializer=init_ops.zeros_initializer()) # normed_v = g * v / ||v|| normed_v = g * v * math_ops.rsqrt( math_ops.reduce_sum(math_ops.square(v))) return math_ops.reduce_sum( normed_v * math_ops.tanh(keys + processed_query + b), [2]) else: return math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [2]) 源码中计算 score 的最后一步不是全链接，而是这样的： 12345v = tf.get_variable(&quot;attention_v&quot;, [num_units])score = tf.reduce_sum(v * tanh(keys + processed_query), [2]) 1234567891011121314151617181920212223import tensorflow as tfimport numpy as npimport tensorflow.contrib.eager as tfetf.enable_eager_execution()print(tfe.executing_eagerly())memory = tf.ones(shape=[1, 10, 5]) # batch=1, max_sequence_len=10, embed_size=5memory_sequence_len = [5] # 有效长度为 5attention_mechnism = tf.contrib.seq2seq.BahdanauAttention(num_units=32, memory=memory, memory_sequence_length=memory_sequence_len) True 123print(attention_mechnism.state_size, attention_mechnism.alignments_size) 10 10 123memory &lt;tf.Tensor: id=3, shape=(1, 10, 5), dtype=float32, numpy= array([[[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]], dtype=float32)&gt; 123attention_mechnism.values # 可以发现 values 就是把 memory 中超过memory_sequence_length 的部分变为 0 &lt;tf.Tensor: id=30, shape=(1, 10, 5), dtype=float32, numpy= array([[[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]], dtype=float32)&gt; 12345print(attention_mechnism.keys.shape) # 经过了全链接之后的attention_mechnism.keys.numpy()[0,1,:] (1, 10, 32) array([ 0.09100786, 0.18448338, -0.7751561 , 0.00775184, 0.467805 , 0.9172474 , 0.57645243, -0.3915946 , -0.22213435, 0.76866853, 0.3591721 , 0.8922573 , 0.15866229, 0.6033571 , 0.51816225, 0.3820553 , -0.39130217, 0.04532939, -0.02089322, 0.6878175 , -0.28697258, 0.59283376, -0.37825382, -0.5865691 , 0.17466056, -0.5915747 , 0.6070496 , -0.18531135, -0.821724 , 1.2838829 , 0.15700272, -0.2608306 ], dtype=float32) 123print(attention_mechnism.query_layer, attention_mechnism.memory_layer) &lt;tensorflow.python.layers.core.Dense object at 0x7fa0464da908&gt; &lt;tensorflow.python.layers.core.Dense object at 0x7fa0464dab38&gt; 123456789# 利用 call 函数来计算下一个 state 和 attention vectorquery = tf.ones(shape=[1, 8]) # query_depth = 10state_h0 = attention_mechnism.initial_alignments(batch_size=1, dtype=tf.float32)attention_vector = attention_mechnism(query=query, state=state_h0) 123print(attention_vector) (&lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;, &lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;) tf.contrib.seq2seq.LuongAttentionpaper: Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class LuongAttention(_BaseAttentionMechanism): &quot;&quot;&quot;Implements Luong-style (multiplicative) attention scoring. &quot;&quot;&quot; def __init__(self, num_units, memory, memory_sequence_length=None, scale=False, probability_fn=None, score_mask_value=None, dtype=None, name=&quot;LuongAttention&quot;): if probability_fn is None: probability_fn = nn_ops.softmax if dtype is None: dtype = dtypes.float32 wrapped_probability_fn = lambda score, _: probability_fn(score) super(LuongAttention, self).__init__( query_layer=None, memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name) self._num_units = num_units self._scale = scale self._name = name 可以发现 query 没有经过 query_layer 的处理，也就是没有全链接。但是 memory 还是要用全链接处理的，得到 [batch, max_times, num_units] 再看使用 call 函数计算对其概率 alignment 和 next_state. 12345678910111213141516171819202122232425262728293031323334353637def __call__(self, query, state): &quot;&quot;&quot;Score the query based on the keys and values. Args: query: Tensor of dtype matching `self.values` and shape `[batch_size, query_depth]`. state: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). Returns: alignments: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). &quot;&quot;&quot; with variable_scope.variable_scope(None, &quot;luong_attention&quot;, [query]): score = _luong_score(query, self._keys, self._scale) alignments = self._probability_fn(score, state) next_state = alignments return alignments, next_state 接下来看怎么计算的 score 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def _luong_score(query, keys, scale): &quot;&quot;&quot;Implements Luong-style (multiplicative) scoring function. Args: query: Tensor, shape `[batch_size, num_units]` to compare to keys. keys: Processed memory, shape `[batch_size, max_time, num_units]`. scale: Whether to apply a scale to the score function. Returns: A `[batch_size, max_time]` tensor of unnormalized score values. Raises: ValueError: If `key` and `query` depths do not match. &quot;&quot;&quot; depth = query.get_shape()[-1] key_units = keys.get_shape()[-1] if depth != key_units: raise ValueError( &quot;Incompatible or unknown inner dimensions between query and keys. &quot; &quot;Query (%s) has units: %s. Keys (%s) have units: %s. &quot; &quot;Perhaps you need to set num_units to the keys' dimension (%s)?&quot; % (query, depth, keys, key_units, key_units)) dtype = query.dtype # Reshape from [batch_size, depth] to [batch_size, 1, depth] # for matmul. query = array_ops.expand_dims(query, 1) # Inner product along the query units dimension. # matmul shapes: query is [batch_size, 1, depth] and # keys is [batch_size, max_time, depth]. # the inner product is asked to **transpose keys' inner shape** to get a # batched matmul on: # [batch_size, 1, depth] . [batch_size, depth, max_time] # resulting in an output shape of: # [batch_size, 1, max_time]. # we then squeeze out the center singleton dimension. score = math_ops.matmul(query, keys, transpose_b=True) score = array_ops.squeeze(score, [1]) if scale: # Scalar used in weight scaling g = variable_scope.get_variable( &quot;attention_g&quot;, dtype=dtype, initializer=init_ops.ones_initializer, shape=()) score = g * score return score 通过源码可以发现 LuongAttention 调用 call 函数时，其 query 的维度必须是 num_units. 而 BahdanauAttention 并不需要。 其是计算 score 的方式如下： 123456789101112131415161718192021222324252627282930313233343536373839batch_size = 2query_depth = num_units = 32memory_depth = 15max_times = 10embed_size = 5scale = Truequery = tf.random_normal(shape=[batch_size, num_units])# memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])# values = self._prepaer_memory(memory)# keys = memory_layer(values)values = tf.random_normal(shape=[batch_size, max_times, memory_depth])keys = tf.layers.dense(inputs=values, units=num_units) # [batch, max_times, num_units]query = tf.expand_dims(query, axis=1) # [batch, 1, num_units]score = tf.matmul(query, keys, transpose_b=True) # [batch, 1, max_times]score = tf.squeeze(score, axis=1) # [batch, max_times]print(score.shape) (2, 10) 12345678910111213141516171819202122232425### 完整的过一遍memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])memory_sequence_len = [5,8]query_len = 5query = tf.random_normal(shape=[batch_size, num_units])state = tf.zeros(shape=[batch_size, max_times])attention_mechnism = tf.contrib.seq2seq.LuongAttention(num_units=num_units, memory=memory, memory_sequence_length=memory_sequence_len)attention_vector = attention_mechnism(query, state)attention_vector[0], attention_vector[1] # attention_vector 和 state (&lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy= array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02, 6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06, 3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04, 0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;, &lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy= array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02, 6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06, 3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04, 0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;) 123tf.reduce_sum(attention_vector[0][1]).numpy() 1.0 这只是针对单个 query 的情况，但实际上 query 一般是这样的 [batch, query_len, num_units]，那怎么办呢？ 总结最后总结一下再看一遍两个 attention 初始化的差异 123456789101112131415161718192021super(BahdanauAttention, self).__init__( query_layer=layers_core.Dense( num_units, name=&quot;query_layer&quot;, use_bias=False, dtype=dtype), memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name) 12345678910111213141516171819super(LuongAttention, self).__init__( query_layer=None, memory_layer=layers_core.Dense( num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length=memory_sequence_length, score_mask_value=score_mask_value, name=name) 作为一个类对象时，AttentionMechanism，BahdanauAttention，LuongAttention它们具有如下属性： query_layer: 在 BahdanauAttention 中一般是 tf.layer.dense 的实例对象，其维度是 num_units. 所以 BahdanauAttention 中 query 的维度可以是任意值。而 LuongAttention 中 query_layer 为 None，所以 query 的维度只能是 num_units. memory_layer: 在两个 attention 中都是一样的，tf.layer.dense,且维度为 num_units. alignments_size: 对齐size，是 memory 的 max_times. batch_size: 批量大小 values: 是经过 mask 处理后的 memory. [batch, max_times, embed_size] keys: 是经过 memory_layer 全链接处理后的。 [batch, max_times, num_units]. state_size: 等于 alignment_size. 然后是对应的方法： init: 初始化类实例，里面的参数： num_units: 在 Bahdanau 中这个参数其实是个中间值，将 query 和 keys 转化为这个维度，叠加，但最后还是要在这个维度上 reduce_sum． 但是在 LuongAttention 中它必须和 query 的维度一致，然后和 memory_layer 处理后的 memory 做矩阵相乘。 memory: [batch, max_times, embed_size] normalize: 是佛有归一化 probability_fn: tf.nn.softmax，tf.contrib.seq2seq.hardmax， tf.contrib.sparsemax.sparsemax memory_sequence_length： 没有经过 padding 时 memory 的长度。其维度应该是 [1, batch_size] call(query, state) 调用该实例 query: [batch_size, query_length]. 在 LuongAttention 中 query_length 必须等于 num_units. state: [batch_size, alignments_size]. 一直不太理解 state 有啥用？在源码中是用来计算 alignments 的： 123456789101112131415161718192021222324252627282930313233alignments = self._probability_fn(score, state)self._probability_fn = lambda score, prev: ( # pylint:disable=g-long-lambda probability_fn( _maybe_mask_score(score, memory_sequence_length, score_mask_value), prev))# 其中 score 是可能需要 mask 的. probability_fn 是 tf.nn.softmax. 所以呢？？？？不需要 prev 啊？# 然后发现确实不需要啊。。。一步步往上找probability_fn=wrapped_probability_fnwrapped_probability_fn = lambda score, _: probability_fn(score) initial_alignments(batch_size, dtype) 初始化对齐 Args: batch_size: int32 scalar, the batch_size. dtype: The dtype. Returns: A dtype tensor shaped [batch_size, alignments_size] initial_state(batch_size, dtype)： Creates the initial state values for the AttentionWrapper class. batch_size: int32.","link":"/2018/09/01/tensorflow-Attention-API/"},{"title":"从0开始GAN-1-from-GAN-to-WGAN","text":"From GAN to WGANReference: From GAN to WGAN 令人拍案叫绝的Wasserstein GAN 听李宏毅老师讲段子之 GAN 这是一篇 copy + translate + understand 的学习笔记. NLP 选手总是会听说 GAN 不适合自然语言处理这类任务，但学了才发现，emmmm，真香。。 不管是否适合，但真的好玩！ 纵所周知，GAN非常难训练，在训练时总是会面临训练不稳定，以及难以收敛的情况。这里，作者尝试通过阐述GAN背后的数学原理，来解释为什么GAN不好训练，并且介绍了GAN的另一个版本来更好的解决这些训练难题。 Kullback–Leibler and Jensen–Shannon Divergence在学习GAN之前，先回顾一下如何衡量两个概率分布相似度的标准。 KL (Kullback–Leibler) divergence如何通俗的解释交叉熵与相对熵? 熵: 信息量可表示为 $log\\dfrac{1}{p}$，其可理解为概率为p的随机事件所包含的信息量。比如“太阳明天早上在东边升起”，这个概率p=1，那么其所包含的信息就为0了，意思就是这不是句屁话嘛。。所以信息量与概率p成反比。至于为什么就是 $log\\dfrac{1}{p}$ 这种形式，为啥不是 $1/p$，这需要去问香农了。。 而熵则是 信息量的期望，也可以理解为 随机性的度量。随机性越大，熵越大。 交叉熵 两个概率分布p和q，p为真实分布，q为非真实分布。按照真实分布来衡量识别一个样本或者是判断随机事件的准确性的度量，就是熵，也就是信息量的期望 $H(p)=\\sum_ip(i) * log\\dfrac{1}{p(i)}$,但是事实是，我们无法得知这个真实的分布，只能通过统计来预测这个分布，也就是用非真实分布q去衡量这个熵，$H(p,q)=\\sum_ip(i) * log\\dfrac{1}{q(i)}$, 注意这里的概率是真实分布 p(i). H(p,q)就是我们的“交叉熵”。 当用来预测的非真实分布q越接近真实分布，其随机性越小，准确率也就越高。 相对熵/KL散度 根据Gibbs’ inequality上述例子中的 $H(p,q) &gt;= H(p)$ 恒成立。当且仅当q=p时，这个等号才成立。那么熵H(p,q)相比熵H(q)多出来的部分就是相对熵 $D(p||q)=H(p,q)-H(p)=\\sum_ip(i)* log\\dfrac{p(i)}{q(i)}$，也称为KL散度(Kullback–Leibler divergence，KLD). 从机器学习的角度去思考，我们预测得到的非真实分布就是q，当模型越好时，q与p越接近，也就是模型的准确度越高，随机性越小，所以交叉熵/相对熵也就越小。反过来，就可以通过交叉熵/相对熵来训练我们所需的模型了～ 所以： $$D_{KL}(p||q)=H(p,q)-H(p)=\\sum_ip(i)* log\\dfrac{p(i)}{q(i)}=\\int_x{p(x)}log\\dfrac{p(x)}{q(x)}dx$$ 但是，这里有个问题，p和q并不是完全对称的。显然当p(x)为0，q(x)为非零值时，q(x)的影响就不存在了。反过来呢，q不可能为零。所以当两个概率完全相等时，用KL散度来衡量两个概率的相似度就会存在问题了。 Jensen–Shannon DivergenceJS散度的范围是[0,1],并且是完全对称的。 $$D_{JS}(p | q) = \\frac{1}{2} D_{KL}(p | \\frac{p + q}{2}) + \\frac{1}{2} D_{KL}(q | \\frac{p + q}{2})$$ p是均值为 0，方差为 1 的正态分布，q是均值为 1，方差为 1 的正态分布。两者的均值的分布是 m=(p+q)/2.可以看到 $D_{Kl}$ 是非对称的，而 $D_{JS}$ 是对称的。 [How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? ](https://arxiv.org/pdf/1511.05101.pdf)认为 GAN 能成功的很大一部分原因是用JS散度代替了传统的基于极大似然估计的KL散度。 Generative Adversarial Network (GAN)GAN 包含两个模型： Discrimator D: 判别器 D 用来估计来自 $p_r$ 或 $p_g$ 的样本是真实样本的概率 Generator G: 给定随机输入变量 z（随机 z 带来了多样性, $z\\sim p_z$），输出得到合成的样本。G 的训练是通过捕捉真实样本的分布，从而生成尽可能真实的样本 ($G(z)=x\\sim p_g$)，换句话说，就是欺骗判别器 D 使得生成的样本获得较高的概率。 $p_z$ noise，可以是正态分布，也可以是均匀分布 $p_g$ 通过 sample 生成器生成的样本得到的分布 $p_r$ 通过 sanple 真实样本的 database 得到的分布 整个训练过程是迭代进行的： 固定生成器的参数，训练判别器 固定判别器参数，训练优化器 iteration… 何时停止，以及如何判断何时停止，这也是 GAN 需要解决的问题。 Generator实际上，可以把神经网络 G 看作是用来定义一个分布，$p_g$, 使得这个分布尽可能的接近真实样本的图像在高维空间中的分布 $p_r$. 所以对于生成器的目标函数是 $G^* =\\argmax_{G}Div(P_g,p_r)$ 但是问题在于，如何去评判两个 distributin 的接近程度呢，也就是 $Div(p_g,p_{data})$ 怎么计算？ DiscriminatorGAN 牛逼的地方就是用另一个神经网络来判断这两个 distribution. 所以可以看作是一个二分类问题了。 当两个分布很接近的时候，判别器就很难去区分来自于 $p_g$ 和 $p_r$ 的样本。 所以对于判别器，其目标是尽可能的去区分出 $p_g$ 和 $p_r$，当计算出的 divergence 越大时，D 越好 $D^* =\\argmax_{D}Div(D,G)$. 所以，G 和 D 两个模型在训练中是相互博弈的过程。G 尽可能的去欺骗 D，而 D 则尽可能的不被欺骗。这是一个有趣的zero-sum游戏。 loss function从极大似然估计的角度来分析根据极大似然估计，二分类判别器 D 的输入样本集 ${(x_1, y_1),(x_2,y_2),…,(x_N, y_N)}$ 的概率最大，而输入到判别器 D 的样本可能来自 real data, $x\\sim p_r(x)$，也可能来自生成器 G, $x\\sim p_g(x)$. 其中对应的 label: $$ y= \\begin{cases} 1, &amp; \\text {$x\\sim p_r(x)$} \\ 0, &amp; \\text{$x\\sim p_g(x)$} \\end{cases} $$ 似然函数（样本集的概率最大）: $$L(\\theta)=\\prod_iD(y_i=1|x_i)^{y_i}(1-D(y_i=1|x_i))^{(1-y_i)}$$ 对于 $x\\sim p_r$, $y_i=1$,所以 $$logL=\\sum_{x\\sim p_r} logD(x)$$ 对于 $x\\sim p_g$, $y_i=0$, 可以得到： $$logL=\\sum_{x\\sim p_g}log(1-D(x))$$ 所以对于判别器D, 在生成器G固定参数时最优的判别器 D 就是最大化下面这个目标函数： $$E_{x\\sim p_r(x)}[logD(x)]+E_{x\\sim p_g}[log(1-D(x)]\\qquad\\text{(1)}$$ 事实上，我们发现，这个目标函数跟 logistic regression 是一样的。。。 从熵的角度来分析我们通过最大化 $E_{x\\sim p_r(x)}[logD(x)]$ 来保证判别器 D 在 real data $p_r$上的准确率。与此同时，G 生成得到的 fake 样本，G(z), $z\\sim p_z(z)$，判别器D期望对于 fake 样本的概率 D(G(z)) 越接近于 0 越好，也就是最大化 $E_{z\\sim p_z(z)}[log(1-D(G(z)))]$. 对于生成器，其目的就是让判别器D在 fake 样本上得到的概率更大，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是 $$E_{z\\sim p_z}[log(1-D(G(z))]=E_{x\\sim p_g}[log(1-D(x)]\\qquad\\text{(2)}$$ $$E_{z\\sim p_z}[-logD(G(z)]=E_{x\\sim p_g}[-logD(x)]\\qquad\\text{(3)}$$ 这个直观上也很好理解~固定了判别器 G，然后让 $E_{x\\sim p_g}[logD(x)]$ 尽可能大，也就是 $E_{x\\sim p_g}[log(1-D(x)]$ 或者 $E_{x\\sim p_g}[-logD(x)]$ 尽可能小。 然后把两者（1）和 （2）合并起来（它们有共同的第二项），D和G正在进行的就是一个 minimax game，而我们所需优化的loss function就是： $$% &lt;![CDATA[ \\begin{aligned} \\min_G \\max_D L(D, G) &amp; = \\mathbb{E}{x \\sim p{r}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))] \\ &amp; = \\mathbb{E}{x \\sim p{r}(x)} [\\log D(x)] + \\mathbb{E}_{x \\sim p_g(x)} [\\log(1 - D(x)] \\quad (1.5) \\end{aligned} %]]&gt;$$ 对于生成器，需要最小化这个目标函数。对于判别器，需要最大化这个函数。 如何求关于判别器 D 的最优解定义好了 loss function，接下来推导对于 D 的最优解. 上式可以写成积分函数： $$L(G,D)=\\int_x(p_r(x)log(D(x))+p_g(x)log(1-D(x)))dx\\quad (4)$$ 对于判别器 D，我们要求最大化上述目标函数。假设 D(x) 可以模拟任何函数（事实上 neural network 也是可以的），那么最大化上述函数等同于最大化积分内的函数。 也就是 given $\\forall$ x ，求解其最优的判别器 D*. $$p_r(x)log(D(x))+p_g(x)log(1-D(x))$$ 为了简化计算，假设 $$\\tilde x=D(x), A=p_r(x), B=p_g(x)$$ 对积分内部求导（这里可以忽略积分，因为x是采样任何可能的值）： $$% &lt;![CDATA[ \\begin{aligned} f(\\tilde{x}) &amp; = A log\\tilde{x} + B log(1-\\tilde{x}) \\ \\frac{d f(\\tilde{x})}{d \\tilde{x}} &amp; = A \\frac{1}{ln10} \\frac{1}{\\tilde{x}} - B \\frac{1}{ln10} \\frac{1}{1 - \\tilde{x}} \\ &amp; = \\frac{1}{ln10} (\\frac{A}{\\tilde{x}} - \\frac{B}{1-\\tilde{x}}) \\ &amp; = \\frac{1}{ln10} \\frac{A - (A + B)\\tilde{x}}{\\tilde{x} (1 - \\tilde{x})} \\ \\end{aligned} %]]&gt;$$ 然后，令 $\\dfrac{df(\\tilde x)}{d\\tilde x}=0$,可以得到D(x)的最优解： $D^* (x) = \\tilde{x}^* = \\frac{A}{A + B} = \\frac{p_{r}(x)}{p_{r}(x) + p_g(x)} \\in [0, 1]\\qquad\\text{(5)}$ 这个结果从直观上很容易理解，就是看一个样本x来自真实分布和生成分布的可能性的相对比例。如果 $P_r(x) = 0$ 且 $P_g(x) \\neq 0$，最优判别器就应该非常自信地给出概率0；如果 $P_r(x) = P_g(x)$，说明该样本是真是假的可能性刚好一半一半，此时最优判别器也应该给出概率0.5。 如何得到生成器 G 的最优解，也就是全局最优解记住我们的目标是训练得到一个生成器，使得其生成的 $P_g$ 分布能尽可能的接近于 $P_r$. 所以当判别器最优时，最小化目标函数就能得到 G*，将 (4) 带入 (5) 式可以得到： $$\\begin{aligned} L(G, D^* ) &amp;= \\int_x \\bigg( p_{r}(x) \\log(D^* (x)) + p_g (x) \\log(1 - D^* (x)) \\bigg) dx \\ &amp;= \\int_x (p_r(x)log\\dfrac{p_r}{p_r+p_g} + p_g(x)log\\dfrac{p_g}{p_r+p_g})dx \\ &amp;= \\int_x(p_xlog\\dfrac{\\dfrac{1}{2}p_r}{\\dfrac{1}{2}(p_r+p_g)} + p_glog\\dfrac{\\dfrac{1}{2}p_g}{\\dfrac{1}{2}(p_r+p_g)})dx\\quad\\text{上下同时乘以$\\dfrac{1}{2}$}\\ &amp;= -2log2 + \\int_xp_r(x)log\\dfrac{p_r(x)}{\\dfrac{1}{2}(p_r(x)+p_g)}dx + \\int_xp_g(x)log\\dfrac{p_g(x)}{\\dfrac{1}{2}(p_r(x)+p_g)}dx \\quad\\text{(6)}\\ &amp;= -2log2 + D_{KL}(p_r||\\dfrac{p_r+p_g}{2}) + D_{KL}(p_g||\\dfrac{p_r+p_g}{2})\\quad\\text{带入 KL 散度公式} \\ &amp;= -2log2 + D_{JS}(p_{r} | p_g)\\quad\\text{带入 JS 散度公式} \\end{aligned}$$ 我们突然发现，诶，卧槽，厉害了。given D* 的条件下，当生成器 G 最优时，通过推导发现，最小化目标函数等同于最小化 $p_r$ 和 $p_g$ 的 JS 散度。所以啊，通过理论证明，让两个分布更接近的话，使用 JS 散度明显要比我们传统上使用的 KL 散度要合理呀~ 所以如何判别两个分布的 Divergence, 通过推导告诉我们，JS 散度更好~ 整个算法流程： 这个为什么 D 训练时是多次, 而 G 训练时只需要一次呢？ 固定判别器为 $D^* $，通过梯度下降训练 $G_0 \\rightarrow G_1$, 这里 $G_0$ 和 $G_1$ 不能差距太大. 因为如果 G 变化太大，那么对应的 JS divergence 变化可能就如上图所示，会突然变得很大，而不是我们所预想的减小了。 Problems in GANs理论上，满足 JS 散度越小，两个分布越接近是可以的。但是要使得 JS 散度越来越小这个有点难度，因为图像是高维空间里面的低维 mainfold. 这也是接下来要讲的问题。 尽管 GAN 在图像生成上取得了很大的成功，但是其训练并不容易，过程很慢并且不稳定。令人拍案叫绝的Wasserstein GAN 将原始 GAN 的问题主要分成两部分。 判别器的问题：D 越好，生成器梯度消失越严重。 生成器的问题：最小化生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足。 判别器的问题：判别器越好，生成器梯度消失越严重对于前面说到的 JS 散度上。我们会希望如果两个分布之间越接近它们的JS散度越小，我们通过优化JS散度就能将 $p_g$ “拉向” $p_r$，最终以假乱真。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分（要么是 $x\\sim p_r$, 要么是 $x\\sim p_g$），或者它们重叠的部分可忽略（等会儿解释什么叫可忽略），它们的JS散度是多少呢？ $p_1(x)=0且p_2(x)=0$ $p_1(x)\\ne0且p_2(x)\\ne0$ $p_1(x)=0且p_2(x)\\ne 0$ $p_1(x)\\ne 0且p_2(x)=0$ 第一种对计算JS散度无贡献 第二种情况由于重叠部分可忽略,（$p_r和 p_g$ 都是高维空间中的低维流形），所以贡献也为0. 第三种情况，带入公式（6）倒数第三步的的后两项，JS 的散度计算可以得到其值为 log2. 第四种情况同理。 换句话说，无论跟是远在天边，还是近在眼前，只要它们俩没有一点重叠或者重叠部分可忽略，JS散度就固定是常数，而这对于梯度下降方法意味着——梯度为0！此时对于最优判别器来说，生成器肯定是得不到一丁点梯度信息的；即使对于接近最优的判别器来说，生成器也有很大机会面临梯度消失的问题。 但是 $P_r$ 与 $P_g$ 不重叠或重叠部分可忽略的可能性有多大？不严谨的答案是：非常大。比较严谨的答案是：当 $P_r$ 与 $P_g$ 的支撑集（support）是高维空间中的低维流形（manifold）时，$P_r$ 与 $P_g$ 重叠部分测度（measure）为0的概率为1。 也就是接下来要说的: low dimensional support 和 gradient vanishing. Low dimensional supports有两个数学概念： Manifold: A topological space that locally resembles Euclidean space near each point. Precisely, when this Euclidean space is of dimension n, the manifold is referred as n-manifold. 拓扑空间，在每个点附近局部类似于欧几里德空间。 确切地说，当该欧几里德空间具有n维时，该流形被称为n-流形。 Support: In mathematics, the support of a real-valued function f is the subset of the domain containing those elements which are not mapped to zero. 在数学中，实值函数f的支持是包含那些未映射到零的元素的域的子集 “Towards principled methods for training generative adversarial networks”. 这篇非常理论的论文讨论了对于 $p_r$ 和 $p_g$ 的support是处于低维的空间，并且这导致了GAN的训练中的不稳定性。 真实样本空间具有高度的人工特征，因为它的主题一旦确定，其包含的对象也就固定了。比如dog应该有two ears和a tail.一个Skyscraper应该有straight和tall的身体。这些限制使得图像不具备高维空间的形式。 同样的 $p_g$ 也是在低维流形空间。当给定初始的噪声输入变量为100维，生成器将其作为输入生成较大的图像 $64\\times 64$，对于输出的分布 4096 pixels已经被100维随机的向量定义了，所以它也很难去填满整个高维空间。 “撑不满”就会导致真实分布与生成分布难以“碰到面”，这很容易在二维空间中理解：一方面，二维平面中随机取两条曲线，它们之间刚好存在重叠线段的概率为0；另一方面，虽然它们很大可能会存在交叉点，但是相比于两条曲线而言，交叉点比曲线低一个维度，长度（测度）为0，可忽略。三维空间中也是类似的，随机取两个曲面，它们之间最多就是比较有可能存在交叉线，但是交叉线比曲面低一个维度，面积（测度）是0，可忽略。从低维空间拓展到高维空间，就有了如下逻辑：因为一开始生成器随机初始化，所以几乎不可能与有什么关联，所以它们的支撑集之间的重叠部分要么不存在，要么就比和的最小维度还要低至少一个维度，故而测度为0。所谓“重叠部分测度为0”，就是上文所言“不重叠或者重叠部分可忽略”的意思。 因为 $p_r$ 和 $p_g$ 都是处于低维流形，他们很大可能性是不相交的。当他们具备不相交的特性时，我们就很容易找到一个完美的判别器来准确的100%区分fake样本和真实样本。 左侧图是两条线在三维空间。右侧是两个平面在三维空间。通过维度的对比来表明相交的可能性。 Vanishing gradient当判别器非常完美的时候，$D(x)=1,\\forall x\\in p_r$, $D(x)=0, \\forall x\\in p_g$. $$L(G,D)=\\int_x(p_r(x)log(D(x))+p_g(x)log(1-D(x)))$$ 带入这个公式可以发现，loss function L 会降为0，在迭代过程中，梯度也就无法更新。下图证明了，当判别器越好的时候，梯度消失越快。 WGAN前作Figure 2。先分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器重新随机初始化从头开始训练，对于第一种形式的生成器loss产生的梯度可以打印出其尺度的变化曲线，可以看到随着判别器的训练，生成器的梯度均迅速衰减。注意y轴是对数坐标轴。 生成器的问题：最小化生成器loss函数，会等价于最小化一个不合理的距离衡量这样会导致两个问题：一是梯度不稳定，二是 mode collapse/dropping 即多样性不足。 前面说到 Goodfellow 给了两个 generator 的 loss function，也就是公式 （2）和（3）. Goodfellow 换成第二种 loss function 的理由如上图，因为在训练生成器 G 是，D(x) 肯定是很小的，所以观察上图可以看到 log(1-D(x)) 在 D(x) 偏小的区域梯度很小，所以导致训练很慢。 但是，换成第二种 loss 会导致 mode collapse. 接下来通过公式推导证明这俩问题。 通过公式（1.5）和公式（6）可以得到在 $D^* $ 的条件下： $$ \\mathbb{E}{x \\sim p{r}(x)} [\\log D^* (x)] + \\mathbb{E}{x \\sim p_g(x)} [\\log(1 - D^* (x)]=-2log2 + D{JS}(p_{r} | p_g)\\quad\\text{(7)}$$ 我们在算一个 KL 散度: $$% &lt;![CDATA[ \\begin{aligned} KL(p_g||p_r) &amp;=\\mathbb{E}_{x\\sim p_g}log(\\dfrac{p_g(x)}{p_r(x)})\\ &amp;=\\mathbb{E}_{x\\sim p_g}[log\\dfrac{p_g(x)/(p_g(x)+p_r(x))}{p_r(x)/p_g(x)+p_r(x)}]\\ &amp;=\\mathbb{E}_{x\\sim p_g}[log\\dfrac{1-D^* (x)}{D^* (x)}]\\ &amp;=\\mathbb{E}{x\\sim p_g}log[1-D^* (x)]-\\mathbb{E}{x\\sim p_g}logD^* (x)\\quad\\text{(8)} \\end{aligned} %]]&gt;$$ 将公式（7）和 （8）带入到第二种 loss（3）中可以得到： $$\\begin{aligned} \\mathbb{E}_{x\\sim p_g}logD^* (x) &amp;=KL(p_g||p_r)-\\mathbb{E}_{x\\sim p_g}log[1-D^* (x)]\\ &amp;=KL(p_g||p_r)-D_{JS}(p_{r} | p_g)+2log2-\\mathbb{E}{x \\sim p{r}(x)}\\quad\\text{(7)} \\end{aligned}$$ 上式后两项与 G 无关，所以最小化 loss（3）等价于最小化: $$KL(p_g||p_r)-D_{JS}(p_{r} | p_g)$$ 这个等价最小化目标存在两个严重的问题。第一是它同时要最小化生成分布与真实分布的KL散度，却又要最大化两者的JS散度，一个要拉近，一个却要推远！这在直观上非常荒谬，在数值上则会导致梯度不稳定，这是后面那个JS散度项的毛病。 第二，即便是前面那个正常的KL散度项也有毛病。因为KL散度不是一个对称的衡量，KL(P_g || P_r)与KL(P_r || P_g)是有差别的。以前者为例: 当 $P_g(x)\\rightarrow 0$ 而 $P_r(x)\\rightarrow 1$ 时，$P_g(x) \\log \\frac{P_g(x)}{P_r(x)} \\rightarrow 0$，对 $KL(P_g || P_r)$ 贡献趋近0 当 $P_g(x)\\rightarrow 1$ 而 $P_r(x)\\rightarrow 0$ 时，$P_g(x) \\log \\frac{P_g(x)}{P_r(x)} \\rightarrow +\\infty$，对 $KL(P_g || P_r)$ 贡献趋近正无穷 换言之，KL(P_g || P_r)对于上面两种错误的惩罚是不一样的，第一种错误对应的是“生成器没能生成真实的样本”，惩罚微小；第二种错误对应的是“生成器生成了不真实的样本” ，惩罚巨大。第一种错误对应的是缺乏多样性，第二种错误对应的是缺乏准确性。这一放一打之下，生成器宁可多生成一些重复但是很“安全”的样本，也不愿意去生成多样性的样本，因为那样一不小心就会产生第二种错误，得不偿失。这种现象就是大家常说的collapse mode。 第一部分小结：在原始GAN的（近似）最优判别器下，第一种生成器loss面临梯度消失问题，第二种生成器loss面临优化目标荒谬、梯度不稳定、对多样性与准确性惩罚不平衡导致mode collapse这几个问题。 这位老哥讲的太好了。。直接 copy 了。。 Mode collapsemode collapse: 重复生成一张图片 mode dropping: G 在迭代时只能生成一类图片。 还有一个问题：Lack of a proper evaluation metricGAN 没有一个好的目标函数来描述训练过程。没有好的验证指标，就好比在黑暗中work. 没有信号来提示该在什么时候停止，也没有好的指标来评价多种模型的好坏。 Improving GAN Training Improve Techniques for Training GANs Towards principled methods for training generative adversarial networks Wasserstein GAN (WGAN)Wasserstein Distance 是一种测量两个分布距离的方式。可以类比成 earth mover’s distance. 为了简单的理解，可以把两个分布看作是离散的。这里看作是两堆土，Wasserstein distance 就是计算如何移动最少量的土使得两个分布一致。 所以相比 JS 散度，从分类任务变成了回归任务。即使两个分布完全无交集，也是存在 divergence 更大或者更小的问题，所以也就不存在梯度为零的情况了。 但是问题来了，怎么计算 Wasserstein distance 呢？ step1: $p_1 \\underrightarrow{2} p_2$, 使得 $p_1和Q_1$ match. step2: $p_2 \\underrightarrow{2} p_3$, 使得 $p_2和Q_2$ match. step3: $Q_3 \\underrightarrow{1} Q_4$, 使得 $p_3和Q_3$ match. 所以总的 W=5. 对于连续分布，Wasserstein distance： $$W(p_r, p_g) = \\inf_{\\gamma \\sim \\Pi(p_r, p_g)} \\mathbb{E}_{(x, y) \\sim \\gamma}[| x-y |]$$ $\\Pi(p_r, p_g)$ 是所有可能的联合分布. 其中一种联合分布 $\\gamma \\sim \\Pi(p_r, p_g)$ 表示一种 move plan, 就比如上图中的示例。 其中,对于任何一个联合分布 $\\gamma$,其边缘分布分别是 $p_g(x)=\\sum_x\\gamma(x,y)$, $p_r(y)=\\sum_y\\gamma(x,y)$. 在此分布下的移动距离是 $||x-y||$. 那么当前联合分布下的 cost 是 $\\gamma(x, y) \\cdot | x-y |$. 其期望就是： $$\\sum_{x, y} \\gamma(x, y) | x-y |= \\mathbb{E}_{x, y \\sim \\gamma} | x-y |$$ 而我们需要求的是所有可能的联合分布中的下界, 就定义为 Wasserstein distance. Why Wasserstein is better than JS or KL divergence?Wasserstein 距离的优势在于，即使两个分布没有交集，也能平滑的表示两个分布之间的散度。 $$\\forall (x, y) \\in P, x = 0 \\text{ and } y \\sim U(0, 1)\\ \\forall (x, y) \\in Q, x = \\theta, 0 \\leq \\theta \\leq 1 \\text{ and } y \\sim U(0, 1)\\$$ 当 $\\theta\\ne 0$ 时，分别计算 KL,JS，WS 散度： $$% &lt;![CDATA[ \\begin{aligned} D_{KL}(P | Q) &amp;= \\sum_{x=0, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{0} = +\\infty \\ D_{KL}(Q | P) &amp;= \\sum_{x=\\theta, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{0} = +\\infty \\ D_{JS}(P, Q) &amp;= \\frac{1}{2}(\\sum_{x=0, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{1/2} + \\sum_{x=0, y \\sim U(0, 1)} 1 \\cdot \\log\\frac{1}{1/2}) = \\log 2\\ W(P, Q) &amp;= |\\theta| \\end{aligned} %]]&gt;$$ 当 $\\theta = 0$ 时，分别计算 KL,JS，WS 散度： $$% &lt;![CDATA[ \\begin{aligned} D_{KL}(P | Q) &amp;= D_{KL}(Q | P) = D_{JS}(P, Q) = 0\\ W(P, Q) &amp;= 0 = \\lvert \\theta \\rvert \\end{aligned} %]]&gt;$$ 当两个分布没有交集时，KL 散度的值是 inifity. JS 散度则存在不连续的问题，对于这个例子而言，当 $\\theta=0$ 时，JS 散度不可微。只有 WS 距离是可微的，这对于梯度下降而言是非常友好的。 Use Wasserstein distance as GAN loss function但是要穷尽两个联合分布的所有情况来计算 $\\inf_{\\gamma \\sim \\Pi(p_r, p_g)}$ 是不可能的。WGAN 的作者给出了一个聪明的转换，可以把公式 (8) 写成： $$W(p_g,p_r)=\\dfrac{1}{K}sup_{|f|_ L \\le K}\\mathbb{E}{x\\sim p_r}[f(x)]-\\mathbb{E}{x\\sim p_g}[f(x)]\\quad\\text{(9)}$$ 这里的意思就是用 f 函数来表示上面说到的任何可能的联合分布。所以 $\\mathbb{E}{x\\sim p_r}[f(x)]-\\mathbb{E}{x\\sim p_g}[f(x)]$ 等效于 $\\mathbb{E}_{x, y \\sim \\gamma} | x-y |$. 然后我们又知道神经网络足够强大，所以用神经网络 D 来代替 f，来表示上面说到的任何可能的联合分布。但是 D 必须像前面提到的 Wasserstein distance 那样足够光滑。这样一来， Wasserstein distance 就转变成了我们想要的 loss function. $$V(G,D)=\\max_{D\\sim \\text{1-Lipschitz}}{\\mathbb{E}{x\\sim p_r}[D(x)]-\\mathbb{E}{x\\sim p_g}[D(x)]}\\quad\\text{(10)}$$ 通过采样来计算 V(G,D),我们希望 这里用一个例子来说明为什么 D 要足够光滑： 我们看到要让 V(G,D) 在 real example 上增大，在 fake example 上减小， D 完全可以做到像上图中红色箭头那样。所以这样下来，D 无法收敛。 怎么保证一个神经网络足够光滑, $D\\sim \\text{1-Lipschitz}$，貌似听起来很难，毕竟涉及到那么多的参数。 Lipschitz continuity: 这里首先需要介绍一个概念——Lipschitz连续。它其实就是在一个连续函数 f 上面额外施加了一个限制，要求存在一个常数 $K\\geq 0$ 使得定义域内的任意两个元素 $x_1$ 和 $x_2$ 都满足 $$|f(x_1) - f(x_2)| \\leq K |x_1 - x_2|$$ 此时称函数 f的Lipschitz常数为K。实际上就是 f 函数的导函数的值不能超过 K. Lipschitz 连续条件限制了一个连续函数的最大局部变动幅度。 在 WGAN 这篇论文中，作者采用了一种特别简单的方法，Weight Clipping. 对于任何参数，使其在 [-c,c] 范围内，就可以保证 K 不会特别大。。 到此，我们就能把 Wasserstein distance 应用到了 GAN 上，也就是 WGAN. 相比传统的 GAN，其区别在于： 判别器 D 的任务不是分类，而是回归。所以去掉最后一层 sigmoid. 生成器和判别器的 loss 不取 log，原因是 Wasserstein distance 就是这样呀~ 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行 总的流程就是： $n_{\\text{critic}}$ 表示判别器的训练迭代次数，生成器在一次完整的迭代中只训练一次。 对于判别器的loss就是公式（10） $$\\mathbb{E}{x\\sim p_g}[D(x)]-\\mathbb{E}{x\\sim p_r}[D(x)]\\quad(11)$$ 上图中与这个是相反的，所以上述流程中使用的是梯度上升。如果用公式（11）还是应该是梯度下降。 生成器的loss是第二项 $$-\\mathbb{E}_{x\\sim p_g}[D(x)]\\quad(12)$$ 其中 $-\\mathbb{E}{x\\sim p_g}[D(x)]+\\mathbb{E}{x\\sim p_r}[D(x)]$ 可以指示训练进程，其数值越小，表示真实分布与生成分布的Wasserstein距离越小，GAN训练得越好。 improved WGANimproved WGAN 主要是改进 weight clipping 这一略显粗糙的方式。取代它的是增加一个正则化项，来约束参数的变化。 $$|\\nabla_xD(x)|\\le 1$$ 类似于 SVM loss： $$-\\lambda\\mathbb{E}{x\\sim p{penalty}}[max(0, |\\nabla_xD(x)|- 1)]$$ 其中 $x\\sim p_{penalty}$ 这部分表示的是 $p_r和p_g$ 连线上的样本采样。 这里顺便把 $max(0, |\\nabla_xD(x)|- 1)$ 改进成了 $(|\\nabla_xD(x)|- 1)^2$ 以此来惩罚梯度太小的项。 “Simply penalizing overly large gradients also works in theory, but experimentally we found that this approach converged faster and to better optima.” Spectrum NormSpectral Normalization → Keep gradient norm smaller than 1 everywhere [Miyato, et al., ICLR, 2018] 但其实前面说到的 $(|\\nabla_xD(x)|- 1)^2$ 这一正则惩罚项依然是存在问题的。因为任意 sample $p_r 和 p_g$ 中的两点，然后拉进他们俩，实际上并不太合理，因为与 $p_g$ 最接近的 $p_r$ 中的一点并不就是采样到的这个.","link":"/2019/05/18/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-from-GAN-to-WGAN/"},{"title":"从0开始GAN-4-ScratchGAN","text":"Training Language GANs from Scratch发现一个问题，目前看到language gans的相关paper大部分是Google，DeepMind的paper. 感觉是个深不见底的坑，弱渣哭了。。。 Motivation我们知道language GAN非常难训练，主要是因为gradient estimation, optimization instability, and mode collapse等原因，这导致很多NLPer选择先基于maximum likelihood对模型进行预训练，然后在用language GAN进行fine-tune.作者认为这种 fine-tune 给模型带来的benefit并不clear，甚至会带来不好的效果。 关于mode collapse，李宏毅老师讲过，在对话生成时，模型总是倾向于生成“我不知道”，”我知道了”这样通用的没有太多sense的回复，其实就是属于mode collapse. 类似于图像领域，既要生成鼻子，又要生成嘴巴，但是模型会倾向于生成一个居中的distribution来模拟这两个distribution。 关于gradient estimator，是因为对于离散的数据，其gradients的方差会很大。 [13-16]就是先使用ML预训练模型，然后在此基础上adversarial fine-tune.[17-18]则说明了 “that the best-performing GANs tend to stay close to the solution given by maximum-likelihood training”. 所以作者为了证明language GAN真的能work，就from scratch训练了一个language GAN, 对，没有预训练。作者认为从头训练好language GAN的核心技术是 large batch sizes, dense rewards and discriminator regularization. 本文的贡献： 从头训练一个language GAN能达到基于ML方法的unconditional text generation. 证明 large batch sizes, dense rewards and discriminator regularization 对于训练language GAN的重要性。 作者对文本生成模型的evaluation提出了一些性的拓展，能充分挖掘生成的language更多的特性。比如： BLEU and Self-BLEU [19] capture basic local consistency. The Frechet Distance metric [17] captures global consistency and semantic information. Language and Reverse Language model scores [18] across various softmax temperatures to capture the diversity-quality trade-off. Nearest neighbor analysis in embedding and data space provide evidence that our model is not trivially overfitting. Generative Models of Text生成模型的本质就是对unknown data distribution进行建模，也就是学习模型 p(x|y) 的参数。在传统的机器学习里面，我们认为模型 p(x|y) 的分布就是多维高斯正态分布，然后用EM算法去学习得到参数。在基于neural network的自然语言处理领域，对于 $x=[x_1,..,x_T]$， $p_{\\theta}(x_t|x_1,…,x_{t-1})$ 也可以看作是学习这样一个distribution，只不过模型的参数不是高斯正态分布这么简单，而是基于network来模拟的。同样序列特性使得其非常适合使用自回归模型进行建模: $$p_{\\theta}=\\prod_{t=1}^Tp_{\\theta}(x_t|x_1,…,x_{t-1})$$ Maximum Likelihood一旦模型建立好了，接下来就是训练模型。最常用的方法就是使用极大似然估计 maximum likelihood estimation(MLE). $$\\argmax_{\\theta}\\mathbb{E}{p^* (x)}logp{\\theta}(x)$$ 关于 maximum likelihood 是否是最优解，这篇paper有讨论[9]。 Generative Adversarial Networks 前面seqgan也说过自回归模型中 $p_{\\theta}=\\prod_{t=1}^Tp_{\\theta}(x_t|x_1,…,x_{t-1})$的过程有个sample的操作，这是不可导的。针对这个问题，有三种解决方法： 高方差，无偏估计的 reinforce[28]. 基于大数定律的条件下，去sample更多的example，来模拟 $p(y_t|s_t)$ 的分布，然后基于policy gradient去优化这个distribution，这使得速度很慢。 低方差，有偏估计的 gumbel-softmax trick[29-30]. other continuous relaxations[11]. Learning Signals对于generator的训练，作者采用了基于 REINFORCE 的方法: 其中同 MaliGAN[15] 一样，设置 $R(x)=\\dfrac{p^* (x)}{p_{\\theta}(x)}$, 这样等效于 MLE 估计。 基于MLE eatimator的梯度更新可以看作是reinforce的一个spacial case.区别在于language gans的reward是可以学习的，也就是discriminator是不断更新的。可学习的discriminator的效果已经被证明过了[34]. 如果learned reward能够提供相比MLE loss更光滑的信号，那么discriminator就能提供更多有意义的signal，甚至training data没有cover的distribution. 同时，discriminator是可以ensemble的，使用更多的domain knowledge.这样能学习到更多的信息。 Training Language GANs from Scratch作者通过实验验证，要训好一个language gans，所需要的是： a recurrent discriminator used to provide dense rewards at each time step large batches for variance reduction discriminator regularization dense rewards判别器能够判别generated sentence和real sentence，但是对于不完整的句子，就没办法去判断。这就造成，如果generated sentence很容易就被判断为fake，那么在fix discriminator训练generator时，生成器无法获得有意义的信号，也就是梯度为0吧。 为了避免这种情况，作者采用了 MaskGAN[32] 的方法: maskGAN maskGAN是一种 actor-critic 方法，利用类似于完形填空的形式，只需要生成被挖去的词，就能对整个sentence进行判别，并计算reward，这样得到的reward相比sentence中的每一个词都是生成的，其variance会小很多。 具体做法是： 生成器是 seq2seq 的形式，输入sequence $x=(x_1,…,x_T)$. 通过 binary mask $m=(m_1,…,m_T)$ 得到 $m(x)$. 根据 m(x) 来生成得到完整的 generated examples $\\hat x=(\\hat x_1, \\hat x_2,…,\\hat x_T)$. 这里生成的时候参考上图，如果当前time-step被mask了，则需要用到上一个time-step生成的词，如果没有被mask，就直接使用当前词，类似于teacher-forcing. 判别器就是计算每一个词为真的概率，注意这里判别器的输入也有 m(x)，其原因是让模型更好的识别生成的sentence中，哪一个是之前被mask了的。 $$D_{\\phi}(\\tilde x_t|\\tilde x_{0:T}, m(x)) = P(\\tilde x_t=x_t^{real}|\\tilde x_{0:T}, m(x))$$ reward 的计算： $$r_t=logD_{\\phi}(\\tilde x_t|\\tilde x_{0:T}, m(x))$$ Large Batch Sizes for Variance Reductionreference: [9] How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv [12-16] [17-18] [32] Maskgan: Better text generation via filling in the ____ [34]","link":"/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-4-ScratchGAN/"},{"title":"从0开始GAN-2-sequence generation by GAN","text":"paper list Generating Sentences from a Continuous Space GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution Categorical Reparameterization with Gum-bel-Softmax Deep Reinforcement Learning for Dialogue Generation Generative Adversarial Networks 李宏毅老师讲seqGAN Role of RL in Text Generation by GAN(强化学习在生成对抗网络文本生成中扮演的角色) 好玩的文本生成 Conditional Generative Adversarial Nets Generative Adversarial Text to Image Synthesis Adversarial Learning for Neural Dialogue Generation How (not) to train your generative model: Scheduled sampling, likelihood, adversary? Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation 为什么GAN不适合文本生成前面学过了GAN很自然的就会想到将GAN引入到文本生成中来，比如对话可以看作是conditional GAN, 但实际上却并不如想象中那样简单，原因是GAN只适用于连续数据的生成，对离散数据效果不佳。 Role of RL in Text Generation by GAN(强化学习在生成对抗网络文本生成中扮演的角色) 这里面从两方面讲的很清楚: sampling：从生成得到的softmax probability到one-hot向量，从而查询出对应index的词，这一步称为“sampling”，显然是不可微的。 去掉sampling,将softmax probability和one-hot vector作为discriminator的输入，如果是discriminator是一个二分类器的话，判别器D很容易“作弊”，它根本不用去判断生成分布是否与真实分布更加接近，它只需要识别出给到的分布是不是除了一项是 1 ，其余都是 0 就可以了。因此，我们也可以想到用WGAN来解决这个问题。Improved Training of Wasserstein GANs也给出了文本生成的实验，效果当然是好了很多，不至于直接崩了。 但是WGAN为什么没那么好呢？将一个softmax probability强行拉倒一个one-hot vector真的可行吗？ Gumbel-softmax，模拟Sampling的softmaxRL in text generationreinforcement learningreinforcement learning 和监督学习、非监督学习一起构成机器学习的三大范式。 Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) RL所适用的环境是一个典型的马尔科夫决策过程(Markov decision process,MDP)。所以强化学习实际上也可以看作是一种动态规划的方法。不过与传统的dynamic programming方法不同的是，RL不会假设MDP的精确数学模型的知识。我的理解是，在很多DP问题中，状态转移矩阵是已知的，但是RL所处理的问题，从一个状态到另一个状态，不是根据已有的知识，而是取决于当前action带来的reward以及未来的reward,所以这也就涉及到了 exploration 和 exploitation 的平衡问题。 Markov decision process 包括：GANs-in-NLP/Reinforcement_learning_diagram.png 环境以及agent状态的集合 S; agent能采取的动作的集合 $A$ 状态之间转换的规则 $P_a(s,s’)=Pr(s_{t+1}=s’|s_t=s,a_t=a)$ 规定转换之后的即时奖励 $R_a(s,s’)$ 描述主体能够观察到什么的规则(这是啥玩意？？) policy将从头到尾所有的动作连在一起就称为一个“策略”或“策略路径” $pi$ ，强化学习的目标就是找出能够获得最多奖励的最优策略. $\\pi: A\\times S \\rightarrow [0,1]$ $\\pi(a,s)=Pr(a_t=a|s_t=s)$ state-value function状态-值函数 $V_{\\pi}(s)$ 定义在当前状态 s下，按照策略 $\\pi$ 接下来能获得的 reward.也就是说，given state s，当前以及未来的reward期望. $$V_{\\pi}(s)=E[R]=E[\\sum_{t=0}^{\\infty}\\gamma^tr_t|s_0=s]$$ 其中 $\\gamma^t$ 是折扣因子，因为还是当前利益最重要嘛，所以未来的reward要打个折。 $$R=\\sum_{t=0}^{\\infty}\\gamma^tr_t$$ value functionvalue funcion 和 state-value function 的区别是后者给定了一个 state. 而value function是计算给定任意初始状态，得到的reward. $$V^{\\pi}=E[R|s,\\pi]$$ 所以最优的 policy 实际上就是 value function 的期望最大。$\\rho^{\\pi}=E[V^{\\pi}(S)]$， 其中状态S是从一个分布 $\\mu$ 随机采样得到的。 尽管 state-value 足够定义最优 policy，再定义一个 action-value 也是很有用的。 given state s, action a, policy $\\pi$, action-value: $$Q^{\\pi}(s,a)=E[R|s,a,\\pi]$$ 个人理解，在强化学习的应用场景中，很多时候是由 action 来确定下一个 state 的。所以 action-value 这个function会更实用吧。比如 text generation，sample当前词就是 action，然后才有下一个时刻的 state. Monte Carlo methodsTemporal difference methodsRL应用到对话场景下Deep Reinforcement Learning for Dialogue Generation 对话生成任务本身非常符合强化学习的运行机理（让人类满意，拿奖励）。 输入句子是 h,模型返回的response是 x，其从人类得到的奖励是 $R(h,x)$. 基于RL的目标函数就是最大化对话的期望奖励。上图中 $p_{\\theta}(x,h)$ 表示在 $\\theta$ 参数下，一组对话 $(x,h)$ 出现的概率。$P(h)$ 表示出现句子 h 的概率。 最大化奖励期望： $$公式(1)$$ 上式中 $h\\sim P(h)$ 可以看作是均匀分布，所以 $E_{h\\sim P(h)}\\approx \\dfrac{1}{N}$. 其中 $E_{x\\sim P_{\\theta}(x|h)}$ 的计算无法考虑所有的对话，所以通过采样 $(h^1,x^1), (h^2,x^2), .., (h^N,x^N)$ 来计算。 然后问题来了，我们需要优化的参数 $\\theta$ 不见了，这怎么对 $\\theta$ 进行求导呢？可以采用强化学习中常用的 policy gradient 进行变形： $$\\dfrac{dlog(f(x))}{dx}=\\dfrac{1}{f(x)}\\dfrac{df(x)}{dx}$$ 适当变形后，对 $\\theta$ 进行求导： $$公式(2)$$ 这样一来，梯度优化的重心就转化到了生成对话的概率上来，也就是说，通过对参数 $\\theta$ 进行更新，奖励会使模型趋于将优质对话的出现概率提高，而惩罚则会让模型趋于将劣质对话的出现概率降低。 自AlphaGo使得强化学习猛然进入大众视野以来，大部分对于强化学习的理论研究都将游戏作为主要实验平台，这一点不无道理，强化学习理论上的推导看似逻辑通顺，但其最大的弱点在于，基于人工评判的奖励 Reward 的获得，让实验人员守在电脑前对模型吐出来的结果不停地打分看来是不现实的，游戏系统恰恰能会给出正确客观的打分（输/赢 或 游戏Score）。基于RL的对话生成同样会面对这个问题，研究人员采用了类似AlphaGo的实现方式（AI棋手对弈）——同时运行两个机器人，让它们自己互相对话，同时，使用预训练（pre-trained）好的“打分器”给出每组对话的奖励得分 R(a^i, x^i) ，关于这个预训练的“打分器” R ，可以根据实际的应用和需求自己DIY。 SeqGANseqGAN对前面仅基于RL的对话生成进行了改进，也就是前面用pre-trained的打分器（或者是人类），用GAN中的判别器进行了代替。 这里问题在于生成得到的response x输入到判别器时，这个过程涉及到了sampling的操作，所以固定discriminator来更新generator时，梯度无法回流。 这就需要RL的出现了。 总结一下RL在这里面的作用：这里的discriminator得到的是reward。我们fix住判别器D来优化生成器 $\\theta$ 的过程就变成了：生成器不再是原来的sample一个词，作为下一个time step的输入，因为这不可导。而是把当前time step作为一个state，然后采取action，这个action当然也是在词表中选一个词(用Monte Carlo Search). 以前是通过最大化似然概率（最小化交叉熵）来优化生成器，现在是寻找最优的 policy（最大化奖励期望）来优化生成器。而采用policy gradient可以将reward期望写成 $\\theta$ 的连续函数，然后就可以根据最大化reward期望来优化 $\\theta$,也就是梯度上升。 有了前面的基础再重新阅读seqGAN这篇paper. motivation传统的GAN在序列生成的能力有限主要是两个原因： 无法处理离散的数据（前面已经讲过了） 判别器D只能对完整的序列进行评价（原因是判别器就是基于完整的句子或dialogue进行训练的）。但是在序列生成的过程中，在生成部分序列的时候，对当前部分序列的评价也是很重要的。 传统的基于 RNN/attention 的序列生成模型也存在 exposure bias 的问题，也就是训练阶段和inference阶段不一致的问题。在训练阶段是teacher forcing，而在infer阶段，下一个词的预测仅仅依赖于当前的隐藏状态（attention-based会有attention vector）. Bengio 的弟弟，另一个 Bengio 提出了 scheduled sampling 的方法，但这依然未能完全解决这个问题。 为此，作者提出基于RL的seqGAN。对序列生成的问题进行建模，把序列生成问题看作是马尔可夫决策过程(Data generation as sequential decision making)，从而转换成基于RL的寻找最优policy的问题，有效的解决了上述三个问题。 Sequence Generative Adversarial Nets这里先介绍一些数学符号： 我们的目的是训练得到一个生成模型 $G_{\\theta}$，使其能生成得到这样的一个序列 $Y_{1:T}=(y_1,…,y_t,…,y_T)$. 其中 $y_t\\sim V$. V是候选词表。用RL来描述序列生成的过程就是： 当前时间步 t 的状态 state s: $(y_1,…,y_{t-1})$ action a 是选择下一个 token $y_t$. policy也就是生成模型 $G_{\\theta}(y_t|Y_{1:t-1})$ 状态的转移取决于 action a. 比如状态转移的概率 $\\sigma_{s,s’}^a=1$，也就是在当前状态 $s=Y_{1:t-1}$ 情况下，下一个状态是 $s’$ 的概率为1，那么下一个状态是 $s’=Y_{1:t}$,对应的action也就是 $a=y_t$. 首先我们需要训练一个判别模型 $D_{\\phi}(Y_{1:T})$, 通过判断输入来自 real or fake 进行训练。而生成器的训练需要借助于判别器D的输出，也就是 reward. SeqGAN via Policy Gradient如果不考虑中间每一个时间步的奖励，也就是只考虑整个sentence的reward, 那么基于生成模型（policy）$G_{\\theta}(y_t|Y_{1:t-1})$ 的最大奖励期望的函数是: $$J(\\theta)=E[R_T|s_0,\\theta]=\\sum_{y\\sim V}G_{\\theta}(y|s_0)\\cdot Q_{D_{\\phi}}^{G_{\\theta}}(s_0,y)$$ 其中 $R_T$ 是对整个sentence的奖励, $G_{\\theta}(y|s_0)$ 是 given $s_0$,生成 $y$ 的概率，$Q_{D_{\\phi}}^{G_{\\theta}}(s_0,y )$ 是 action-value 函数，也就是 given $s_0$ 和 policy $G_{\\theta}$ 后采取的 action 是 $y$ 时对应的 reward. 在这篇论文里面，reward 就是判别器判断生成的sentence为real的概率。 $$Q_{D_{\\phi}}^{G_{\\theta}}(a=y_T,s=Y_{1:T-1})=D_{\\phi}(Y_{1:T})$$ 但是对于序列生成问题，不能仅仅考虑完整的句子的reward，还要考虑到每一个 time step. 但是在每一个time step也不能贪心的只考虑当前最大的reward，还要考虑到未来的情况. 作者提出基于 Monte Carlo search 的方法。 Monte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement. Monte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy ${\\displaystyle \\pi }$, the goal is to compute the function values ${\\displaystyle Q^{\\pi }(s,a)}$ (or a good approximation to them) for all state-action pairs ${\\displaystyle (s,a)}$. Assuming (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair ${\\displaystyle (s,a)}$ can be computed by averaging the sampled returns that originated from ${\\displaystyle (s,a)}$ over time. Given sufficient time, this procedure can thus construct a precise estimate ${\\displaystyle Q}$ of the action-value function ${\\displaystyle Q^{\\pi }}$. This finishes the description of the policy evaluation step. policy iteration分为两个步骤，policy evaluation和policy improvement.蒙特卡洛被用在policy evaluation step中，给定一个静态的，判别型的policy $\\pi$，其目标是计算 具体来说，在当前状态 $s=Y_{1:t}$ 下，基于一个 roll-out policy $G_{\\beta}$ 生成剩下的 T-t 个tokens，这个过程重复 N 次. $${Y_{1:T}^1,…,Y_{1:T}^N}=MC^{G_{\\beta}}(Y_{1:t;N})$$ 式子左边是 N 个完整的sentence。 对于 roll-out policy $G_{\\beta}$ 作者在这篇 paper 中采用的与生成模型一样的 $G_{\\theta}$. 如果追求速度的话，可以选择更简单的策略。 这样基于 Monte Carlo method 就能计算每一个 time step 的能考虑到 future 的reward. $$Q_{D_{\\phi}}^{G_{\\theta}}(s=Y_{1:t-1}, a=y_t)= \\begin{cases} \\dfrac{1}{N}\\sum_{n=1}^ND_{\\phi}(Y_{1:T}^n),Y_{1:T}^n \\sim MC^{G_{\\beta}}(Y_{1:t;N}), \\quad \\text{for t &lt; T}\\ D_{\\phi}(Y_{1:t}),\\quad\\text{for t = T} \\end{cases}\\quad (4)$$ 公式还是比较好理解的。所以事实上判别器 $D_{\\phi}$ 依旧是只能判断完整的sentence，但是在每一个 time step 可以借助于 roll-out policy 来得到完整的sentence，进而对当前 action 进行评分，计算得到 $a=y_t$ 的reward。 知道了如何计算reward，就可以利用最大化这个奖励期望来优化我们的生成器（policy $G_{\\theta}$）.对 $\\theta$ 求导: $$\\nabla J(\\theta)=\\sum_{t=1}^T\\mathbb{E}{Y{1:t-1}\\sim G_{\\theta}}[\\sum_{y_t\\sim V}\\nabla_{\\theta}G_{\\theta}({y_t|Y_{1:t-1}})\\cdot Q_{D_{\\phi}}^{G_{\\theta}}(Y_{1:t-1},y_t)]\\quad\\text{公式(3)}$$ 公式（3）与前面李弘毅老师讲的公式（2）是一致的，只不过这里考虑的中间 reward.上式中 $E_{Y_{1:t-1}\\sim G_{\\theta}}[\\cdot]$ 等同于前面提到的 $E_{x\\sim P_{\\theta}(x|h)}$ 都是通过sample 来计算的。同样 reward 的计算式 $Q_{D_{\\phi}}^{G_{\\theta}}(Y_{1:t-1},y_t)$ 也是不包含生成器的参数 $\\theta$ 的。 上述公式中 $\\sum_{y_t\\sim V}\\sim G_{\\theta}(y_t|Y_{1:t-1})$ 然后基于梯度上升来优化参数 $\\theta$. $$\\theta \\leftarrow \\theta + \\alpha_h\\nabla J(\\theta)\\quad(8)$$ 作者建议使用 Adam 或 RMSprop 优化算法。 除了生成器的优化，这里的判别器D是动态的。这样相比传统基于pre-train的判别器会更叼吧。优化判别器的目标函数是： $$\\min_{\\phi}-\\mathbb{E}{Y\\sim p{data}}[logD_{\\phi}(Y)]-\\mathbb{E}{Y\\sim G{\\theta}}[log(1-D_{\\phi}(Y))]\\quad(5)$$ 具体的算法步骤是： And to reduce the vari- ability of the estimation, we use different sets of negative samples combined with positive ones, which is similar to bootstrapping (Quinlan 1996) The Generative Model for Sequences作者使用基于 LSTM 的生成器G。 $$h_t=g(h_{t-1},x_t)$$ $$p(y_t|x_1,…,x_t)=z(h_t)=softmax(c+Vh_t)$$ The Discriminative Model for Sequences作者使用基于 CNN 的判别器，用来预测一个sentence为real的概率。 一些细节 + 一些延伸到目前为止，基本理解了seqGAN的大部分细节，需要看看源码消化下。 接下来会有更多的细节和改进可先参考：Role of RL in Text Generation by GAN(强化学习在生成对抗网络文本生成中扮演的角色) seagan 代码学习TensorArray 和 基于lstm的MDP模拟文本生成这也是seqgan的核心，用Monte Carlo search代替sampling来选择next token.在看具体代码之前先了解下 tensorarray. TensorArray Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays This class is meant to be used with dynamic iteration primitives such as while_loop and map_fn. It supports gradient back-propagation via special “flow” control flow dependencies. 一个封装了动态大小、per-time-step 写入一次的 tensor数组的类。在序列生成中，序列的长度通常是不定的，所以会需要使用动态tensorarray. 类初始化12345678910111213141516171819202122232425def __init__(self, dtype, size=None, dynamic_size=None, clear_after_read=None, tensor_array_name=None, handle=None, flow=None, infer_shape=True, element_shape=None, colocate_with_first_write_call=True, name=None): size: int32 scalar Tensor, 动态数组的大小 dynamic_size: Python bool, 是否可以增长，默认false 方法 stack 1234567def stack(self, name=None): &quot;&quot;&quot;Return the values in the TensorArray as a stacked `Tensor`. &quot;&quot;&quot; 将动态数组 stack 起来，得到最终的 tensor. concat 1234567def concat(self, name=None): &quot;&quot;&quot;Return the values in the TensorArray as a concatenated `Tensor`. &quot;&quot;&quot; 将动态数组 concat 起来，得到最终的 tensor. read 123456789def read(self, index, name=None): &quot;&quot;&quot;Read the value at location `index` in the TensorArray. 读过一次之后会清0. 不能读第二次。但可以再次写入之后。 &quot;&quot;&quot; write 1234567891011def write(self, index, value, name=None): &quot;&quot;&quot;Write `value` into index `index` of the TensorArray. &quot;&quot;&quot; - index: int32 scalar with the index to write to. - value: tf.Tensor gather unstack split scatter tf.while_loop1234567891011121314151617181920212223242526272829def while_loop_v2(cond, body, loop_vars, shape_invariants=None, parallel_iterations=10, back_prop=True, swap_memory=False, maximum_iterations=None, name=None):&quot;&quot;&quot;Repeat `body` while the condition `cond` is true.&quot;&quot;&quot;- cond: callable, return boolean scalar tensor. 参数个数必须和 loop_vars 一致。 - body: vallable. 循环执行体，参数个数必须和 loop_vars 一致.- loop_vars: 循环变量，tuple, namedtuple or list of numpy array. example:123456789101112131415161718192021222324252627282930313233343536373839matrix = tf.random.normal(shape=[5, 1], dtype=tf.float32)sequence_length = 5gen_o = tf.TensorArray(dtype=tf.float32, size=sequence_length, dynamic_size=False, infer_shape=True)init_state = (0, gen_o)condition = lambda i, _: i &lt; sequence_lengthbody = lambda i, gen_o : (i+1, gen_o.write(i, matrix[i] * 2))n, gen_o = tf.while_loop(condition, body, init_state)gen_o_stack = gen_o.stack()gen_o_concat = gen_o.concat()用 LSTM 模拟马尔科夫决策过程print(gen_o) # TensorArray objectprint(gen_o_stack) # tf.Tensor(), [5,]print(gen_o_concat) # tf.Tensor(), [5,1]print(gen_o.read(3)) # -0.22972003, tf.Tensor 读过一次就被清0了print(gen_o.write(3, tf.constant([0.22], dtype=tf.float32))) # TensorArray objectprint(gen_o.concat()) # tf.Tensor([-2.568663 0.09471891 1.2042408 0.22 0.2832177 ], shape=(5,), dtype=float32)print(gen_o.read(3)) # tf.Tensor([0.22], shape=(1,), dtype=float32)print(gen_o.read(3)) # Could not read index 3 twice because it was cleared after a previous read 用 LSTM 模拟马尔科夫决策过程 current time t state: $(y_1,…,y_t)$. 但是马尔科夫决策过程的原理告诉我们一旦当前状态确定后，所有的历史信息都可以扔掉了。这个状态足够去预测 future. 所以在LSTM里面就是隐藏状态 $h_{t-1}$. 以及当前可观测信息 $x_t$. action a: 选择 next token $y_t$. policy: $G_{\\theta}(y_t|Y_{1:t-1})$. 也就是生成next token的策略。下面代码的方法 $o_t \\rightarrow log(softmax(o_t))$. 然后基于这个 log-prob 的分布进行 sample. 问题是这个过程不可导呀？ generator这是生成器生成sample的过程，初始状态是 $h_0$. g_recurrence 就是step-by-step的过程，next_token是通过tf.multinomial采样得到的，其采样的distribution是 log_prob [tf.log(tf.nn.softmax(o_t))]。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131class Generator(tf.keras.Model): ... self.h0 = tf.zeros([self.batch_size, self.hidden_dim]) self.h0 = tf.stack([self.h0, self.h0]) # define variables self.g_embeddings = tf.Variable(self.init_matrix([self.vocab_size, self.emb_dim])) self.g_params.append(self.g_embeddings) self.g_recurrent_unit = self.create_recurrent_unit(self.g_params) # maps h_{t-1} to h_t for generator self.g_output_unit = self.create_output_unit(self.g_params) # maps h_t to o_t (output token logits) def _unsuper_generate(self): &quot;&quot;&quot; unsupervised generate. using in rollout policy. :return: 生成得到的 token index &quot;&quot;&quot; &quot;&quot;&quot; :param input_x: [batch, seq_len] :param rewards: [batch, seq_len] :return: &quot;&quot;&quot; gen_o = tf.TensorArray(dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True) gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length, dynamic_size=False, infer_shape=True) def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x): h_t = self.g_recurrent_unit(x_t, h_tm1) # hidden_memory_tuple o_t = self.g_output_unit(h_t) # [batch, vocab] , logits not prob log_prob = tf.log(tf.nn.softmax(o_t)) #tf.logging.info(&quot;unsupervised generated log_prob:{}&quot;.format(log_prob[0])) next_token = tf.cast(tf.reshape(tf.multinomial(logits=log_prob, num_samples=1), [self.batch_size]), tf.int32) x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token) # [batch, emb_dim] gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.vocab_size, 1.0, 0.0), tf.nn.softmax(o_t)), 1)) # [batch_size] , prob gen_x = gen_x.write(i, next_token) # indices, batch_size return i + 1, x_tp1, h_t, gen_o, gen_x _, _, _, def _super_generate(self, input_x): &quot;&quot;&quot; supervised generate. :param input_x: :return: 生成得到的是 probability [batch * seq_len, vocab_size] &quot;&quot;&quot; with tf.device(&quot;/cpu:0&quot;): self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, input_x), perm=[1, 0, 2]) # [seq_len, batch_size, emb_dim] # supervised pretraining for generator g_predictions = tf.TensorArray( dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True) ta_emb_x = tf.TensorArray( dtype=tf.float32, size=self.sequence_length) ta_emb_x = ta_emb_x.unstack(self.processed_x) self.gen_o, self.gen_x = tf.while_loop( cond=lambda i, _1, _2, _3, _4: i &lt; self.sequence_length, body=_g_recurrence, loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x)) self.gen_x = self.gen_x.stack() # [seq_length, batch_size] self.gen_x = tf.transpose(self.gen_x, perm=[1, 0]) # [batch_size, seq_length] return self.gen_x 所以是通过monte carlo的形式生成fake sample，作为discriminator的输入吗？那这个过程也不可导呀。其实不是这样的。我们再看对抗学习中更新generator的代码: 123456789101112131415161718192021222324252627282930313233343536373839def gen_reward_train_step(x_batch, rewards): with tf.GradientTape() as tape: g_loss = generator._get_generate_loss(x_batch, rewards) g_gradients, _ = tf.clip_by_global_norm( tape.gradient(g_loss, generator.trainable_variables), clip_norm=5.0) g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables)) return g_losstf.logging.info(&quot;------------------ 6. start Adversarial Training...--------------------------&quot;)for total_batch in range(TOTAL_BATCH): # fix discriminator, and train the generator for one step for it in range(1): samples = generator._unsuper_generate() #tf.logging.info(&quot;unsuper generated samples:{}&quot;.format(samples[0])) rewards = rollout.get_reward(samples, rollout_num=2, discriminator=discriminator) # 基于 monte carlo 采样16，计算并累计 reward. #tf.logging.info(&quot;reward:{}&quot;.format(rewards[0])) gen_reward_train_step(samples, rewards) # update generator. # Update roll-out parameters rollout.update_params() # update roll-out policy. 这儿采用的是 generator._get_generate_loss， 所以它对generator的参数都是可导的吗？ 我们再看这个生成器中这个function的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class Generator(tf.keras.Model): ... def _super_generate(self, input_x): &quot;&quot;&quot; supervised generate. :param input_x: :return: 生成得到的是 probability [batch * seq_len, vocab_size] &quot;&quot;&quot; with tf.device(&quot;/cpu:0&quot;): self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, input_x), perm=[1, 0, 2]) # [seq_len, batch_size, emb_dim] # supervised pretraining for generator g_predictions = tf.TensorArray( dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True) ta_emb_x = tf.TensorArray( dtype=tf.float32, size=self.sequence_length) ta_emb_x = ta_emb_x.unstack(self.processed_x) def _pretrain_recurrence(i, x_t, h_tm1, g_predictions): h_t = self.g_recurrent_unit(x_t, h_tm1) o_t = self.g_output_unit(h_t) g_predictions = g_predictions.write(i, tf.nn.softmax(o_t)) # [batch, vocab_size] x_tp1 = ta_emb_x.read(i) # supervised learning, teaching forcing. return i + 1, x_tp1, h_t, g_predictions _, _, _, self.g_predictions = tf.while_loop( cond=lambda i, _1, _2, _3: i &lt; self.sequence_length, body=_pretrain_recurrence, loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, g_predictions)) self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1, 0, 2]) # [batch_size, seq_length, vocab_size] self.g_predictions = tf.clip_by_value( tf.reshape(self.g_predictions, [-1, self.vocab_size]), 1e-20, 1.0) # [batch_size*seq_length, vocab_size] return self.g_predictions # [batch_size*seq_length, vocab_size] def _get_generate_loss(self, input_x, rewards): &quot;&quot;&quot; :param input_x: [batch, seq_len] :param rewards: [batch, seq_len] :return: &quot;&quot;&quot; self.g_predictions = self._super_generate(input_x) real_target = tf.one_hot( tf.to_int32(tf.reshape(input_x, [-1])), depth=self.vocab_size, on_value=1.0, off_value=0.0) # [batch_size * seq_length, vocab_size] self.pretrain_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_target, logits=self.g_predictions) # [batch * seq_length] self.g_loss = tf.reduce_mean(self.pretrain_loss * tf.reshape(rewards, [-1])) # scalar return self.g_loss 所以seqgan的作者是怎么做的呢，利用 generator._unsuper_generate先生成fake sample，然后再利用 generator._super_generate 得到 g_predictions, 将fake sample作为 real_target 与 g_predictions 做交叉熵求出 pretrain_loss，然后乘以每一个token对应的rewards得到最终的loss. 这个过程是可导的。 通常情况下Monte carlo方法在里面的作用其实就是 collect data. collecting data的过程用到了policy,然后基于reward对policy进行求导。 但是seqgan的作者在代码中呈现的是另一种trick. 先用generator生成fake样本，然后用rollout policy对该样本进行打分reward.这里并不是直接对reward求导，而是把fake样本作为target进行MLE训练，得到pretrain_loss，reward作为权重乘以pretrain_loss作为最终的损失函数。 roll-policy这个过程比较容易理解，对于给定的 given_num,小于 given_num 的直接 copy，但是 $h_t$ 的计算依旧。大于 given_num 的token采用 generate._unsuper_generate. 疑问看了代码总觉得代码写得与论文有出入。 基于policy gradient来更新policy(generator)，按照公式应该是直接对rewards求导才对吧。基于Monte carlo采样的过程可以看作是sample不同的样本，是一种近似模拟 $o_t$ 分布的方法，是不要求可导的。","link":"/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/"},{"title":"从0开始GAN-8-RL in NMT","text":"related papers: A Study of Reinforcement Learning for Neural Machine Translation Bilingual-GAN: A Step Towards Parallel Text Generation [On the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://arxiv.org/pdf/1907.01752.pdf) 暂时有个idea，根据那篇paper, beyond bleu 提出的metric来作为优化指标。好处是，对于每一个token生成的时候，不需要接着生成完整的句子就能得到有效的reward（这点需要用实验来验证）。这样对于每个句子中的token都会有对应的rewards,最好可以给每个rewards一个折扣因子，越靠前的系数越小，越靠后的系数越大。 RL in NMTpaper: [On the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://arxiv.org/pdf/1907.01752.pdf) Motivation","link":"/2019/10/11/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-8-RL-in-NMT/"},{"title":"从0开始GAN-9-metric for NLG","text":"related papers: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Why We Need New Evaluation Metrics for NLG Beyond BLEU: Training Neural Machine Translation with Semantic Similarity Better Rewards Yield Better Summaries: Learning to Summarise Without References RUSE: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation ROUGE: A Package for Automatic Evaluation of Summaries Chin-Yew MoverScoreMotivation评价指标对于模型的训练或选择至关重要，现阶段对于文本生成的模型（机器翻译，摘要生产，图像标题生成）大都采用的hard match的方式，比如 BLEU, ROUGE. 这些都是简单的基于共现词的统计，这种metric仅仅只考虑了表面的形式，无法覆盖同意却不同词的表达，所以他们并不太好（over correction），不具备评价文本相似性的能力。 MoverScore It is particularly important for a metric to not only capture the amount of shared content between two texts, i.e., intersect(A,B), as is the case with many semantic textual similarity measures 根据语义相似度来计算距离。 计算 MoverScore 的一些符号： sentence：$x=(x_1,x_2,…,x_m)$ $x^n$ 表示 x 中的 n-gram. $f_{x^n}$ 表示x中每一个 n-gram 的权重。如果 n=(size of sentence), 那么 $f_{x^n}=1$ n-gram 之间的距离： $$C_{ij}=d(x_i^n,y_j^n)$$ 表示 x 中第 $i^{th}$ 个 n-gram 与 y 中第 $j^{th}$ 个 n-gram 的距离。 那么两个句子中所有 n-gram 的距离 Word Mover’s Distance (WMD)： $$WMD(x^n,y^n):=min_{F\\in R^{|x^n|\\times |y^n|}}&lt;C,F&gt;$$ $&lt;&gt;$ 表示加权求和。计算出两个 n-gram 序列的推土机距离与传统的推土机距离不太一样的地方是，这里每个 n-gram 还有权重。 那么如何计算两个 n-gram 的距离 $d(x_i^n,y_j^n)$ 呢, 作者采用的是 Euclidean distance： $$d(x_i^n,y_j^n)=||E(x_i^n)-E(y^n_j)||_ {2}$$ $E$ 是n-gram 的向量表示，比如 $x_i^n=(x_i,..,x_{i+n-1})$ 是 x 中第 i 个 n-gram. $$E{(x_i^n)}=\\sum_{k=i}^{i+n-1}idf{(x_k)}\\cdot E{(x_k)}$$ n-gram 的权重计算： $$f_{x_i^n}=\\dfrac{1}{Z}\\sum_{k=i}^{i+n-1}idf{(x_k)}$$ Z 是归一化常数，也就是总和吧。 当 n&gt;(size of sentence) 时，$WMD(x^n,y^n)$ 变成计算两个完整的句子的距离： $$SMD(x^n,y^n)=||E(x_1^{l_x})-E(y_1^{l_y})||$$ 其中 $l_x,l_y$ 表示两个sentence 的长度。 Contextualized Representations如何得到一个 word/n-gram 的向量表示，基于预训练的模型来得到 contextualized 表示是一个开放性的问题，Elmo和BERT都是多层结构，不同的layer包含了不同的含义。作者这里提到了两种方法，并最终采用了前者： the concatenation of power means a routing mechanism for aggregation power means: $$h_i(p)=(\\dfrac{z_{i,1}^p+…+z_{i,L}^p}{L})^{1/p}$$ L 表示预训练模型的层数，p=1是数值平均，p=0时是调和平均。 $$E(x_i)=h_i^{p_1}\\oplus …. \\oplus h_i^{p_k}$$ $\\oplus$ 表示 concatenation. 作者设置 p=1,K=3. 也就是一个词的向量表示由三个向量表示 $h$ 拼接而成,而每个h又是不同层的数值平均。 result对于这种提出新指标的问题，一直很疑惑怎么去 evaluation。好像只能通过人工去评价了对吧？ 这是机器翻译的结果。 WMD-1/2+BERT+MNLI+PMeans：表示 1-gram 的word mover distences + 在NMLI语料上训练的BERT + PMeans 的融合方式。 根据 NMT system 得到 translations，然后与 references 计算对应的指标。然后根据指标与human evalation相似度进行对比，越接近人类评价的，这个指标就越好。","link":"/2019/10/31/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-9-metric-for-NLG/"},{"title":"pytorch-损失函数","text":"pytorch loss function. Cross Entropy简单来说，交叉熵是用来衡量在给定的真实分布 $p_k$ 下，使用非真实分布 $q_k$ 所指定的策略 f(x) 消除系统的不确定性所需要付出的努力的大小。交叉熵的越低说明这个策略越好，我们总是 minimize 交叉熵，因为交叉熵越小，就证明算法所产生的策略越接近最优策略，也就间接证明我们的算法所计算出的非真实分布越接近真实分布。交叉熵损失函数从信息论的角度来说，其实来自于 KL 散度，只不过最后推导的新式等价于交叉熵的计算公式： 从信息论的视角来理解： 信息量/信息熵（熵）/交叉熵/条件熵 信息量： 一个事件的信息量就是这个时间发生的概率的负对数，概率越大，所带来的信息就越少嘛。至于为什么是负对数，就要问香农了。。起码要满足$P(X)=1$时信息量为0，且始终大于0 $$-\\log P(X)$$ 信息熵， 也就是熵，是随机变量不确定性的度量，依赖于事件X的概率分布。即信息熵是信息量的期望。即求离散分布列的期望～～ $$H(p) = -\\sum_{i=1}^np_i\\log p_i$$ 交叉熵： 回归到分类问题来，我们通过score function得到一个结果（10，1），通过softmax函数压缩成0到1的概率分布，我们称为 $q_i=\\dfrac{e^{f_{y_i}}}{\\sum_je^{f_j}}$ 吧， $$H(p,q) = -\\sum_{i=1}^np_i\\log q_i$$ 这就是我们所说的交叉熵，通过 Gibbs’ inequality 知道：$H(p,q)&gt;=H(p)$ 恒成立，当且仅当 $q_i$ 分布和 $p_i$ 相同时，两者相等。 相对熵： 跟交叉熵是同样的概念，$D(p||q)=H(p,q)-H(p)=-\\sum_{i=1}^np(i)\\log {\\dfrac{q(i)}{p(i)}}$，又称为KL散度，表征两个函数或概率分布的差异性，差异越大则相对熵越大. 最大似然估计、Negative Log Liklihood(NLL)、KL散度与Cross Entropy其实是等价的，都可以进行互相推导，当然MSE也可以用Cross Entropy进行推导出（详见Deep Learning Book P132）。 BCELossCreates a criterion that measures the Binary Cross Entropy between the target and the output 用于二分类的损失函数，也就是 logistic 回归的损失函数。 对于二分类，我们只需要预测出正分类的概率 p，对应的 (1-p) 则是负分类的概率。其中 p 可使用 sigmoid 函数得到。 $$sigmoid(x) = \\dfrac{1}{1+e^{(-x)}}$$ 对应的损失函数可通过极大似然估计推导得到： 假设有 n 个独立的训练样本 ${(x_1,y_1), …,(x_n, y_n)}$ y 是真实标签，$y\\in {0,1}$, 那么对于每一个样本的概率为： $$P(x_i, y_i)=P(y_i=1|x_i)^{y_i}P(y_i=0|x_i)^{1-y_i}$$ $$=P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}$$ 取负对数即可得： $$-y_iP(y_i=1|x_i)-(1-y_i)(1-P(y_i=1|x_i))$$ 不难看出，这与常见的 softmax 多分类的 loss 计算是一致的。 12345678910111213141516171819202122232425262728293031class BCELoss(_WeightedLoss): def __init__(self, weight=None, size_average=None, reduce=None, reduction='elementwise_mean'): &quot;&quot;&quot; - weight: 手动调整权重，不太明白有啥用，用到在看吧 - size_average, reduce 弃用，直接看 reduction 即可 - reduction： &quot;elementwise_mean&quot;|&quot;sum&quot;|&quot;none&quot;，看名字就知道啥意思了 &quot;&quot;&quot; super(BCELoss, self).__init__(weight, size_average, reduce, reduction)def forward(self, input, target): return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction) &quot;&quot;&quot; - input: 预测概率，任意 shape, 但是值必须在 0-1 之间 - target: 真实概率， shape 与 input 相同 &quot;&quot;&quot; $$loss(p,t)=−\\dfrac{1}{N}\\sum_{i=1}^{N}=\\dfrac{1}{N}[t_i∗log(p_i)+(1−t_i)∗log(1−p_i)]$$ example: 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchimport torch.nn as nnloss = nn.BCELoss(reduction=&quot;elementwise_mean&quot;)input = torch.randn(5)target = torch.ones(5)loss = loss(torch.sigmoid(input), target)my_loss = torch.mean(-target * torch.log(torch.sigmoid(input)) - (1-target) * torch.log((1-torch.sigmoid(input))))# test weight parameterloss1 = F.binary_cross_entropy(torch.sigmoid(input), target, reduction=&quot;none&quot;, weight=torch.Tensor([0,0,0,0,1]))loss2 = F.binary_cross_entropy(torch.sigmoid(input), target, weight=torch.Tensor([0,0,0,0,1]))print(my_loss, loss)print(loss1, loss2*5)# tensor(0.7590) tensor(0.7590)# tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3104]) tensor(0.3104) 通常使用 sigmoid 函数时，我们预测得到正分类的概率，然后需要人为设置 threshold 来判断概率达到 threshold 才是正分类，有点类似于 hingle loss 哦。 torch.nn.CrossEntropyLossThis criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. 多分类交叉熵损失函数，可以看作是 binary_cross_entropy 的拓展。计算过程可以分为两步，log_softmax() 和 nn.NLLloss() It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. 在不均衡数据集中，参数 weight 会很有用。 1234567891011121314151617181920212223class CrossEntropyLoss(_WeightedLoss): def __init__(): &quot;&quot;&quot; - weights: 给每一个类别一个权重。 - reduction: &quot;elementwise_mean&quot;|&quot;sum&quot;|&quot;none&quot;. &quot;&quot;&quot; def forward(): &quot;&quot;&quot; - input: [batch, C] or [batch, C, d_1, d_2, ..., d_k] - target: [batch], 0 &lt;= targte[i] &lt;= C-1, or [batch, d_1, d_2, ..., d_k], K &gt;= 2. &quot;&quot;&quot; example: 12345678910111213141516171819202122232425262728293031323334353637383940414243input = torch.randn(2, 3)target = torch.Tensor([0, 2]).long()# use loss functionloss_fn = nn.CrossEntropyLoss()loss = loss_fn(input, target)# compute loss step by stepscore = torch.log_softmax(input, dim=1)score1 = torch.log(F.softmax(input, dim=1))print(score)print(score1)# use nll lossnll_loss_fn = nn.NLLLoss()nll_loss = nll_loss_fn(score, target)# computer nll loss step by stepmy_nll = torch.mean(-score[0][0] - score[1][2])print(nll_loss, loss, my_nll) 1234567891011tensor([[-0.8413, -0.7365, -2.4073], [-0.4626, -2.0660, -1.4120]])tensor([[-0.8413, -0.7365, -2.4073], [-0.4626, -2.0660, -1.4120]])tensor(1.1266) tensor(1.1266) tensor(1.1266) torch.nn.NLLlossThe negative log likelihood loss. It is useful to train a classification problem with C class. input 是已经通过 log_softmax 层的输入。loss 是对应样本中真实标签对应的值的负数。 12345678910111213class NLLLoss(_WeightedLoss): def __init__(): &quot;&quot;&quot; 参数设置跟 CrossEntropyLoss 基本一致。 &quot;&quot;&quot; NLLloss $$\\ell(x, y) = L = {l_1,\\dots,l_N}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}{c \\not= \\text{ignore_index}}$$ example： 12345678910111213141516171819202122232425262728293031loss = nn.NLLLoss()# input is of size N x C = 3 x 5input = torch.randn(3, 5, requires_grad=True)# each element in target has to have 0 &lt;= value &lt; Ctarget = torch.tensor([1, 0, 4])output = loss(torch.log_softmax(input, dim=1), target)score = torch.log_softmax(input, dim=1)output2 = (-score[0, 1]-score[1, 0]-score[2, 4])/3output.backward()# output2.backward()print(output, output2)# tensor(1.5658, grad_fn=&lt;NllLossBackward&gt;) tensor(1.5658, grad_fn=&lt;DivBackward0&gt;) MultiMarginLoss$loss = \\dfrac{1}{N}\\sum_{j\\ne y_i}^{N}max(0,s_j - s_{y_i}+\\Delta)$ $s_{yi}$ 表示其真实标签对应的值，那么其他非真实分类的结果凡是大于 $s_{yi}−\\Delta$ 这个值的，都对最后的结果 $loss$ 产生影响，比这个值小的就没事～ 显然想对于 softmax 损失函数来说，softmax 考虑到了所有的错分类，而 marginloss 只考虑概率较大的错分类。 1234567891011121314151617181920212223242526272829303132333435class MultiMarginLoss(_WeightedLoss): def __init__(self, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='elementwise_mean'): &quot;&quot;&quot; - p (int, optional): Has a default value of `1`. `1` and `2` are the only supported values - margin (float, optional): Has a default value of `1`. &quot;&quot;&quot; super(MultiMarginLoss, self).__init__(weight, size_average, reduce, reduction) if p != 1 and p != 2: raise ValueError(&quot;only p == 1 and p == 2 supported&quot;) assert weight is None or weight.dim() == 1 self.p = p self.margin = margin def forward(self, input, target): return F.multi_margin_loss(input, target, p=self.p, margin=self.margin, weight=self.weight, reduction=self.reduction) example: 1234567891011121314151617loss = nn.MultiMarginLoss()input = torch.FloatTensor([[0, 3, 1], [0, 4, 2], [1, 5, 2], [3, 5, 1]])target = torch.ones(4).long()out = loss(input, target)print(out) # 显然应该是 0,因为负分类与真实标签的 socre 差值都大于等于 1.# tensor(0.) nn.L1loss$$L1(\\hat{y}, y)=\\dfrac{1}{m}\\sum|\\hat{y}_i−y_i|$$ nn.MSEloss$$L2(\\hat{y}, y)=\\dfrac{1}{m}\\sum|\\hat{y}_i−y_i|^2$$ 1234567891011121314151617181920212223loss = nn.L1Loss()loss2 = nn.MSELoss()input = torch.FloatTensor([1,2,3])target = torch.FloatTensor([1,2,9])output = loss(input, target)output2 = loss2(input, target)print(output, output2)# tensor(2.) tensor(12.)","link":"/2018/12/07/pytorch-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"title":"文本分类系列0：NLTK学习和特征工程","text":"计算语言：简单的统计123from nltk.book import * *** Introductory Examples for the NLTK Book *** Loading text1, ..., text9 and sent1, ..., sent9 Type the name of the text or sentence to view it. Type: 'texts()' or 'sents()' to list the materials. text1: Moby Dick by Herman Melville 1851 text2: Sense and Sensibility by Jane Austen 1811 text3: The Book of Genesis text4: Inaugural Address Corpus text5: Chat Corpus text6: Monty Python and the Holy Grail text7: Wall Street Journal text8: Personals Corpus text9: The Man Who Was Thursday by G . K . Chesterton 1908 找出text1,《白鲸记》中的词monstrous，以及其上下文 123text1.concordance(&quot;monstrous&quot;, width=40, lines=10) Displaying 10 of 11 matches: was of a most monstrous size . ... Thi Touching that monstrous bulk of the wh enish array of monstrous clubs and spea wondered what monstrous cannibal and s e flood ; most monstrous and most mount Moby Dick as a monstrous fable , or sti PTER 55 Of the Monstrous Pictures of Wh exion with the monstrous pictures of wh ose still more monstrous stories of the ed out of this monstrous cabinet there 找出text1中与monstrous具有相同语境的词。比如monstrous的上下文 the __ pictures, the __ size. 同样在text1中与monstrous类似的上下文的词。很好奇这个是怎么实现的？ 1234567891011def similar(self, word, num=20): &quot;&quot;&quot; Distributional similarity: find other words which appear in the same contexts as the specified word; list most similar words first. &quot;&quot;&quot; 123text1.similar(&quot;monstrous&quot;) true contemptible christian abundant few part mean careful puzzled mystifying passing curious loving wise doleful gamesome singular delightfully perilous fearless 共用两个或两个以上词汇的上下文，如monstrous和very 123text2.common_contexts([&quot;monstrous&quot;, &quot;very&quot;]) a_pretty am_glad a_lucky is_pretty be_glad 自动检测出现在文本中的特定词，并显示同一上下文中出现的其他词。text4是《就职演说语料》， 12345if __name__ == &quot;__main__&quot;: text4.dispersion_plot([&quot;citizens&quot;, &quot;liberty&quot;, &quot;freedom&quot;]) &lt;matplotlib.figure.Figure at 0x7f3794818588&gt; 如果不使用 if name==”main“ 的话会报错``` ‘NoneType’ object has no attribute ‘show’ 12345678910111213```pythonfdist1 = FreqDist(text1)vocabulary1 = list(fdist1.keys()) # keys() 返回key值组成的listprint(vocabulary1[:10]) ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.'] 需要加list，不然回报错，“TypeError: 'dict_keys' object is not subscriptable” dict.keys() returns an iteratable but not indexable object. The most simple (but not so efficient) solution would be: 1234567# 同样的道理这里也需要加list，因为生成的&lt;class 'dict_items'&gt;z在python3中是迭代器print(type(fdist1.items()))print(list(fdist1.items())[:10]) &lt;class 'dict_items'&gt; [('[', 3), ('Moby', 84), ('Dick', 84), ('by', 1137), ('Herman', 1), ('Melville', 1), ('1851', 3), (']', 1), ('ETYMOLOGY', 1), ('.', 6862)] 123456789# dict.items() 实际上是将dict转换为可迭代对象list，list的对象是 ('[', 3), ('Moby', 84), ('Dick', 84), ('by', 1137)这样的# 这下总能记住dict按照value排序了吧。。。尴尬，以前居然没弄懂？？fdist_sorted = sorted(fdist1.items(), key=lambda item:item[1], reverse=True)print(fdist_sorted[:10]) [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)] 1234567# 这个就是按照key排序。fdist_sorted2 = sorted(fdist1.keys(), reverse=True)print(fdist_sorted2[:10]) ['zoology', 'zones', 'zoned', 'zone', 'zodiac', 'zig', 'zephyr', 'zeal', 'zay', 'zag'] 123fdist1.plot(20, cumulative=True) 可以看到高频词大都是无用的停用词 12345678910111213# 低频词 fdist.hapaxes() 出现次数为1的词print(len(fdist1.hapaxes()))for i in fdist1.hapaxes(): if fdist1[i] is not 1: print(&quot;hh&quot;) 9002 可以看到低频词也很多，而且大都也是很无用的词。 词语搭配123list(bigrams(['more', 'is', 'sad', 'than', 'done'])) [('more', 'is'), ('is', 'sad'), ('sad', 'than'), ('than', 'done')] 123text4.collocations(window_size=4) United States; fellow citizens; four years; years ago; men women; Federal Government; General Government; self government; Vice President; American people; every citizen; within limits; Old World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice; one another; Declaration Independence; protect defend 文本4是就职演说语料，可以看到n-grams能够很好的展现出文本的特性，说明n-grams是不错的特征。 collections()源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def collocations(self, num=20, window_size=2): &quot;&quot;&quot; Print collocations derived from the text, ignoring stopwords. :seealso: find_collocations :param num: The maximum number of collocations to print. :type num: int :param window_size: The number of tokens spanned by a collocation (default=2) :type window_size: int &quot;&quot;&quot; if not ('_collocations' in self.__dict__ and self._num == num and self._window_size == window_size): self._num = num self._window_size = window_size #print(&quot;Building collocations list&quot;) from nltk.corpus import stopwords ignored_words = stopwords.words('english') finder = BigramCollocationFinder.from_words(self.tokens, window_size) finder.apply_freq_filter(2) finder.apply_word_filter(lambda w: len(w) &lt; 3 or w.lower() in ignored_words) bigram_measures = BigramAssocMeasures() self._collocations = finder.nbest(bigram_measures.likelihood_ratio, num) colloc_strings = [w1+' '+w2 for w1, w2 in self._collocations] print(tokenwrap(colloc_strings, separator=&quot;; &quot;)) 自动理解自然语言 词义消歧 Ambiguity 关于词义消歧的理解可以看之前的笔记chapter12-句法分析 指代消解 anaphora resolution 自动问答 机器翻译 人机对话系统 获得文本语料和词汇资源布朗语料库123from nltk.corpus import brown 有以下这些类别的文本 123print(brown.categories()) ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction'] 123456789import nltknews_text = brown.words(categories=&quot;news&quot;)fdist_news = nltk.FreqDist([w.lower() for w in news_text])print(len(fdist_news)) 13112 标注文本语料库经过了标注的语料库，有词性标注、命名实体、句法结构、语义角色等。 分类和标注词汇1234567text = nltk.word_tokenize(&quot;and now for something completely differences!&quot;)print(text)print(nltk.pos_tag(text)) ['and', 'now', 'for', 'something', 'completely', 'differences', '!'] [('and', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('differences', 'VBZ'), ('!', '.')] 词性标注NLTK中采用的方法可参考：A Good Part-of-Speech Tagger in about 200 Lines of Python 对于一些同形同音异义词，通过词性标注能消除歧义.很多文本转语音系统通常需要进行词性标注，因为不同意思发音会不太一样。 123456789text1 = nltk.word_tokenize(&quot;They refuse to permit us tpo obtain the refuse permit&quot;)print(nltk.pos_tag(text1))text2 = nltk.word_tokenize(&quot;They refuse to permit us to obtain the refuse permit&quot;)print(nltk.pos_tag(text2)) [('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('tpo', 'VB'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')] [('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')] 获取已经标注好的语料库123print(nltk.corpus.brown.tagged_words()) [('The', 'AT'), ('Fulton', 'NP-TL'), ...] 12345print(nltk.corpus.treebank.tagged_words())print(nltk.corpus.treebank.tagged_sents()[0]) [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...] [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')] 查看brown语料库中新闻类最常见的词性 1234567brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)tag_fd.keys() dict_keys(['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']) 文本分类朴素贝叶斯分类选取特征，将名字的最后一个字母作为特征. 返回的字典称为特征集 1234567def gender_features(word): return {'last_letter':word[-1]}gender_features('Shrek') {'last_letter': 'k'} 定义一个特征提取器 123456789from nltk.corpus import namesimport randomnames = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]) 12345print(nltk.corpus.names.words('male.txt')[:10])print(names[:10]) ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim'] [('Aamir', 'male'), ('Aaron', 'male'), ('Abbey', 'male'), ('Abbie', 'male'), ('Abbot', 'male'), ('Abbott', 'male'), ('Abby', 'male'), ('Abdel', 'male'), ('Abdul', 'male'), ('Abdulkarim', 'male')] 使用特征提取器处理names数据，并把数据集分为训练集和测试集 12345# 二分类features = [(gender_features(n), g) for (n, g) in names] 123train_set, test_set = features[500:], features[:500] 123print(train_set[:10]) [({'last_letter': 'n'}, 'male'), ({'last_letter': 'e'}, 'male'), ({'last_letter': 'e'}, 'male'), ({'last_letter': 'b'}, 'male'), ({'last_letter': 'b'}, 'male'), ({'last_letter': 'e'}, 'male'), ({'last_letter': 'y'}, 'male'), ({'last_letter': 'y'}, 'male'), ({'last_letter': 't'}, 'male'), ({'last_letter': 'e'}, 'male')] 123classifier = nltk.NaiveBayesClassifier.train(train_set) 12345# 预测一个未出现的名字classifier.classify(gender_features('Pan')) 'male' 12345# 测试集上的准确率print(nltk.classify.accuracy(classifier, test_set)) 0.602 123classifier.show_most_informative_features(5) Most Informative Features last_letter = 'a' female : male = 35.5 : 1.0 last_letter = 'k' male : female = 34.1 : 1.0 last_letter = 'f' male : female = 15.9 : 1.0 last_letter = 'p' male : female = 13.5 : 1.0 last_letter = 'v' male : female = 12.7 : 1.0 构建包含所有实例特征的单独list会占用大量内存，所有应该把这些特征集成起来。 定义一个特征提取器包含多个特征12345678910111213141516171819# 添加多个特征from nltk.classify import apply_featuresdef gender_features2(word): features = {} features['firstletter'] = word[0].lower() features['lastletter'] = word[-1].lower() for letter in 'abcdefghijklmnopqrstuvwxyz': features[&quot;count(%s)&quot;%letter] = word.lower().count(letter) return features 12345print(gender_features2('xiepan'))print(len(gender_features2('xiepan'))) # 有28个特征， 2+26=28 {'firstletter': 'x', 'lastletter': 'n', 'count(a)': 1, 'count(b)': 0, 'count(c)': 0, 'count(d)': 0, 'count(e)': 1, 'count(f)': 0, 'count(g)': 0, 'count(h)': 0, 'count(i)': 1, 'count(j)': 0, 'count(k)': 0, 'count(l)': 0, 'count(m)': 0, 'count(n)': 1, 'count(o)': 0, 'count(p)': 1, 'count(q)': 0, 'count(r)': 0, 'count(s)': 0, 'count(t)': 0, 'count(u)': 0, 'count(v)': 0, 'count(w)': 0, 'count(x)': 1, 'count(y)': 0, 'count(z)': 0} 28 1234567# 对每个样本进行特征处理features = [(gender_features(n), g) for (n,g) in names]print(len(features)) 7944 123456789# 训练集，开发集和测试集train_set = features[1500:]dev_set = apply_features(gender_features2, names[500:1500])test_set = apply_features(gender_features2, names[:500]) 123456789classifier = nltk.NaiveBayesClassifier.train(train_set)print(nltk.classify.accuracy(classifier, dev_set))print(nltk.classify.accuracy(classifier, test_set))print(nltk.classify.accuracy(classifier, train_set)) ## 明显过拟合了～ 0.007 0.008 0.883302296710118 文档分类1234567from nltk.corpus import movie_reviewsdocuments = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category) ] 123movie_reviews.categories() ['neg', 'pos'] 123456789neg_docu = movie_reviews.fileids('neg')print(len(neg_docu)) # neg类别的文档数 1000print(len(documents)) # 总的文档数 1000len(movie_reviews.words(neg_docu[0])) # 第一个文件中单词数 879 1000 2000 879 123random.shuffle(documents) 文档分类的特征提取器所谓特征提取器实际上就是将文档原本的内容用认为选定的特征来表示。然后用分类器找出这些特征和对应类标签的映射关系。 那么什么样的特征才是好的特征，这就是特征工程了吧。 文本分类概述文本分类，顾名思义，就是根据文本内容本身将文本归为不同的类别，通常是有监督学习的任务。根据文本内容的长短，有做句子、段落或者文章的分类；文本的长短不同可能会导致文本可抽取的特征上的略微差异，但是总体上来说，文本分类的核心都是如何从文本中抽取出能够体现文本特点的关键特征，抓取特征到类别之间的映射。 所以，特征工程就显得非常重要，特征找的好，分类效果也会大幅提高（当然前提是标注数据质量和数量也要合适，数据的好坏决定效果的下限，特征工程决定效果的上限）。 也许会有人问最近的深度学习技术能够避免我们构造特征这件事，为什么还需要特征工程？深度学习并不是万能的，在NLP领域深度学习技术取得的效果有限（毕竟语言是高阶抽象的信息，深度学习在图像、语音这些低阶具体的信息处理上更适合，因为在低阶具体的信息上构造特征是一件费力的事情），并不是否认深度学习在NLP领域上取得的成绩，工业界现在通用的做法都是会把深度学习模型作为系统的一个子模块（也是一维特征），和一些传统的基于统计的自然语言技术的特征，还有一些针对具体任务本身专门设计的特征，一起作为一个或多个模型（也称Ensemble，即模型集成）的输入，最终构成一个文本处理系统。 特征工程那么，对于文本分类任务而言，工业界常用到的特征有哪些呢？下面用一张图以概括： 我主要将这些特征分为四个层次，由下往上，特征由抽象到具体，粒度从细到粗。我们希望能够从不同的角度和纬度来设计特征，以捕捉这些特征和类别之间的关系。下面详细介绍这四个层次上常用到的特征表示。","link":"/2018/05/25/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%970%EF%BC%9ANLTK%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"title":"从0开始GAN-7-IRGAN","text":"MotivationIRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models 信息检索的方法主要分为两个流派，生成式检s索模型(generative retrieval model)和判别式检索模型(discriminative retrieval model)。 生成式检索模型 ($q \\rightarrow d$)：认为query和检索所需要的document之间有一个潜在的随机生成的过程。也就是给定一个 query，然后生成相应的 document. 判别式检索模型 ($q + d \\rightarrow r$)：把query和document作为联合feature，计算其相关性relevancy. 然后基于 relevancy 对 document 进行排序。其中关于 ranking a list of documents 有三种范式：pointwise, pairwise, listwise. 作者将上述两种模型与GAN相结合，利用GAN的对抗性的思想去提升两类模型。 判别式检索模型 $p_{\\phi}(r|q,d)$ 作为判别器，maximize 来自真实 labeled 的数据。它提供信息来指导生成器的训练，这种信息不同于传统的 log-likelihood. 判别式检索模型 $p_{\\theta}(d|q,r)$ 是生成器，生成generated sample来迷惑判别器，minimize 对应的目标函数。 Model ArchitectureA Minimax Retrieval Frameworka set of queries ${q_1,…,q_N}$, a set of documents ${d_1,…,d_M}$. 其中 给定一个 query 都有对应的相关度较高的 document 也就是真实的数据 $true_{(q,d)}$，其数据量是远小于总的document数量 M 的. The underlying true relevance distribution can be expressed as conditional probability $p_{true} (d|q, r)$, which depicts the (user’s) relevance preference distribution over the candidate documents with respect to her submitted query. 这样真实的相关性 (q,d) 存在潜在的相关性条件分布 $p_{true}(d|q,r)$. Generative retrieval model $p_{\\theta}(d|q,r)$: 生成器的目的就是去尽可能的模拟真实的相关性分布 $p_{ture}(d|q,r)$, 从而尽可能生成相似度高的 document. Discriminative retrieval model $f_{\\phi}(q,d)$：是一个二分类分类器。 其中判别器具体的计算 $f_{\\phi}(d,q)$ 与IR task有关。后续会详细介绍。 Overall Objective 目标函数： 最小化来自生成器 $p_{\\theta}$ 的 sample 的概率，最大化来自true data $p_{true}$ 的 sample 的概率. Optimising Discriminative Retrieval优化判别器： Optimising Generative Retrieval这篇paper中生成器不是token by token的生成新的ducoment，而是从given documents中选择最相关的document. 对于生成器的优化，最小化目标函数（1）： 上述公式从第一步到第二步有点小变化，简单推导下即可。 这里sample得到d的过程是离散的。怎么理解呢，可以类比文本的生成（尽管此表的分布是连续的，但是从中选一个token，然后作为判别器的输入，这个过程是不可导的）。同样，这里是从一系列documents中sample一个作为判别器的输入，这个过程是离散的，且不可导。所以作者采用了policy gradient的方法来解决这个问题。 公式(4)中对生成器的优化可以看作是 maximize $J^G(q_n)$. 使用policy gradient优化的推导如下： 这里的policy是 $p_{\\theta}(d|q_n,r)$ 就是我们需要优化的生成式检索模型，对应的action是给定environment q的情况下sample得到 document. 判别器得到的log-prob就是reward. 为了减小REINFORCE方法中variance，作者采用了advantage-function，也就是减去baseline，其中baseline是均值： 整个IRGAN的训练过程的伪代码： 上图中的公式(22)就是公式(5). 整个过程理解起来还是蛮简单的。 还有个问题为解决的是，前面提到对于不同的 IR 任务，判别器 $f_{\\phi}(q,d)$ 的方式是不一样的。 pairwise case Furthermore, ifwe use graded relevance scales (indicating a varying degree of match between each document and the corresponding query) rather than binary relevance, the training data could also be represented naturally as ordered document pairs. 此外，如果我们使用分级相关性比例（指示每个文档与相应查询之间的不同匹配程度）而不是二元相关性，则训练数据也可以自然地表示为有序文档对. 也就是不仅仅根据query和document之间是否相似这样的二元文档对，而是利用有序文档对（这在IR中其实更为常见），作为判别器的输入，这样能获取更多的信息。 这个时候的labeled document是 $R_n={&lt;d_i,d_j&gt;|d_i &gt; d_j}$, 其中 $d_i &gt; d_j$ 意味着 $d_i$ 比 $d_j$ 的相关性更高。 使用pairwise discriminator对应的目标函数： 其中 $o = &lt;d_u,d_v&gt;, o’=&lt;d_u’,d_v’&gt;$. 在实际的操作中，选择一对document $&lt;d_i,d_j&gt;$. 然后选择相似度较低的 $d_j$ 与生成器得到的 $d_k$ 组成新的pairs $&lt;d_k, d_j&gt;$，作为判别器的输入。这样的目的就是认为 $d_k$ 的相似度高于 $d_j$ 的情况下，让 $d_k$ 尽可能的去与 $d_i$ 相似。 在前面介绍了生成器 $p_{\\theta}(d|q,r)$ 实际上就是 softmax，看公式(2). 对于pairwise的形式,$d_j$ 也作为生成器的输入之一，对应的生成器是另一种 softmax: 其中 $g_{\\theta}(q,d)$ is a task-specific real-valued function reflecting the chance of d being generated from q.","link":"/2019/08/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-7-IRGAN/"},{"title":"从0开始GAN-6-pretraining for NLG","text":"related papers BERT BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model GPT/GPT-2.0 MASS: Masked Sequence to Sequence Pre-training for Language Generation Unified Language Model Pre-training for Natural Language Understanding and Generation Pretraining for Conditional Generation with Pseudo Self Attention Transformer-XL XLNet Defending Against Neural Fake News ERNIE WWM SpanBERT cross-lingual word embeddingA survey of cross-lingual word embedding models, Ruder et al.2017 Word translation without parallel data. Conneau et al.2017 Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Artetxe et al.2018 contextual word embeddingELMo Word2vec Glove GPT ULMFT: Universal language model fine-tuning for text classificatio Cross-lingual language model pretraining Polyglot contextual representations im- prove crosslingual transfer pre-trained for NMTTowards Making the Most of BERT in Neural Machine Translation Unsupervised Pretraining for Sequence to Sequence Learning When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation? Cross-lingual Language Model Pretraining XLMpaper: Cross-lingual Language Model Pretraining 作者提出了两种方法来学习 cross-lingual 语言模型。其中一种是仅基于 monolingual data, 另一种是基于平行语料。在 cross-lingal 相关的任务上都有很大的提升。比如 XNLI，unsupervised machine translation, 以及 supervised machine tranlsation. 现有的在NLP领域的发展主要是围绕英文进行的，一些start-of-the-art或者NLP任务的benchmarks都是以英文为基础的。其他的一些语言受限于语料的问题，发展相对缓慢。近期随着cross-lingual sentence representation的发展，消除English-centric bias,并且构建一个通用的cross-lingual encoder来讲任何语言的sentence编码到共享的embedding空间成为可能。 Shared sub-word vocabulary 使用 bpe,并且不同的language共享词表. this greatly improves the alignment of embedding spaces across languages that share either the same alphabet or anchor tokens such as digits (Smith et al., 2017) or proper nouns. 共享词表能显著提升那些具有相同字母表或者anchor token(数字或专有名词)的语言之间的向量空间的对齐。 作者先从不同语言的monolingual data中筛选出部分data，然后学习bpe splits. Sentences are sampled according to a multinomial distribution with probabilities ${q_i}_{i=1…N}$, where: 其中 $n_i$ 表示第 i 中语言中sentence的总数。 $\\sum_{k=1}^nn_k$ 表示N种语言所有的sentence的总数。$p_i$ 则表示第 i 中语言sample的概率。设定 $\\alpha=0.5$，这样能增加 low-resource 的比例，从而减轻 bias to high-resource language. 作者总共提出了三种 language model. 接下来一一介绍： Causal Language Modeling (CLM) $p(w_t|w_1,…,w_{t-1},\\theta)$ 也就是普通的 aotu-regressive 语言模型。 Character-Level Language Modeling with Deeper Self-Attention 这篇paper使用的self-attention, 我们知道self-attention 不像rnn那样具有hidden state的概率，这篇paper把上一个batch作为下一个batch的context，有点类似于 transformer-XL,但是这对于cross-lingual不太适合，所以这里的 CLM 与传统的language model完全一致。 Masked Language Modeling (MLM) 与 BERT 中MLM的区别： Differences between our approach and the MLM of Devlin et al. (2018) include the use of text streams of an arbitrary number of sentences (truncated at 256 tokens) instead of pairs of sentences. 文本 stream 是任意数量的sentences，而不是pairs.（这里的pairs in BERT应该指的是 next sentence prediction.） 在筛选用来mask的词时，为了处理 rare word 和 frequent word(punctuation or stop words) 的不均衡问题: tokens in a text stream are sampled according to a multinomial distribution, whose weights are proportional to the square root of their invert frequencies. Translation Language Modeling (TLM) 在预测一个 masked english word 的同时，不仅可以attend english context，也可以 attend franch translation. Cross-lingual Language Models 如何使用这三种语言模型，CLM 和 MLM 在单语上进行训练。 TLM 在平行语料上训练。TLM 在使用时是联合 MLM 一起训练的，迭代优化两个目标函数。 In this work, we consider cross-lingual language model pretraining with either CLM, MLM, or MLM used in combination with TLM. For the CLM and MLM objectives, we train the model with batches of 64 streams of continuous sentences composed of 256 tokens. At each iteration, a batch is composed of sentences coming from the same language, which is sampled from the distribution ${q_i}_{i=1…N}$ above, with α = 0.7. When TLM is used in combination with MLM, we alternate between these two objectives, and sample the language pairs with a similar approach. CTNMTpaper: [Towards Making the Most of BERT in Neural Machine Translation Jiacheng](https://arxiv.org/abs/1908.05672) ByteDance 的一篇paper. 前人的研究中我们发现，BERT pretrian 对NMT几乎没有提升。作者认为其原因是，NMT 相对其他linguistic的任务，训练的steps会多很多，比如NMT一般是10万step，而 POS tagging只需要几百步。这使得在训练过程中，参数的更新太多导致 catastrophic forgetting problem. 也就是 BERT 训练得到的knowledge并不能给NMT代来提升。 于是乎，作者认为只是大家没有好好利用BERT而已，像我们这样搞, BERT还是能对NMT有提升的。然后提出了三种techniques: Asymptotic Distillation 渐近蒸馏，主要是用来解决 catastrophic forgetting 这一问题的，和这篇paper “Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation“ 相似，也是采用的 Elastic Weight Consolidation(EWC) 的方法，对weight采用MSE的约束。 $$L_{kd}=-||\\hat h^{lm}-h_l||^2_2$$ $$L=\\alpha\\cdot L_{nmt}+(1-\\alpha)\\cdot L_{kd}$$ 其中 $L_{kd}$ 是正则化项，$\\hat h^{lm}$ 是经过BERT编码之后的 sentence embedding. $h_l$ 则是NMT的encoded之后的 sentence embedding. 作者都使用的最后一层的表示。在后续实验中，作者也测试了不同层的表示进行约束。 Dynamic Switch 动态开关。 就是GRU中的gate机制。 $$g = \\sigma(Wh^{lm} + Uh^{nmt} + b)$$ $$h=g\\odot h^{lm}+(1-g)\\odot h^{nmt}$$ Rate-scheduled learning slanted triangular learning, 斜三角学习率。最开始提出是在 ULMFT: Universal language model fine-tuning for text classificatio 这篇论文中。 $$\\theta_t=\\theta_{t-1}-\\eta\\nabla_{\\theta}L(\\theta)$$ 对 NMT 和 LM 对应的参数使用不同的学习率，但是都采用这种scheduled学习率. Result","link":"/2019/06/30/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-6-pretraining-for-NLG/"},{"title":"文本分类系列1-fasttext","text":"在Facebook fasttext github主页中，关于fasttext的使用包括两个方面，词向量表示学习以及文本分类。 词向量表示学习：Enriching Word Vectors with Subword Information 文本分类：Bag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov Paper Reading1这篇文章是用来进行文本分类的: Bag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov 这个模型跟word2vec 中的CBOw模型极其相似，区别在于将中心词换成文本标签。那么输入层是文本中单词经过嵌入曾之后的词向量构成的的n-gram，然后求平均操作得到一个文本sentence的向量，也就是隐藏层h，然后再经过一个输出层映射到所有类别中，论文里面还详细论述了如何使用n-gram feature考虑单词的顺序关系，以及如何使用Hierarchical softmax机制加速softmax函数的计算速度。模型的原理图如下所示： 目标函数： $$\\dfrac{1}{N}\\sum_{n=1}^Ny_nlog(f(BAx_n))$$ N表示文本数量，训练时就是Batch size吧？ $x_n$ 表示第n个文本的 normalized bag of features $y_n$ 表示第n个文本的类标签 A is the look up table over n-gram. 类似于attention中的权重吧 B is the weight matrix 隐藏层到输出层的计算复杂度是 $O(hk)$. h是隐藏层的维度，k是总的类别数。经过hierarchical softmax处理后，复杂度为 $O(hlog_2k)$ 这种模型的优点在于简单，无论训练还是预测的速度都很快，比其他深度学习模型高了几个量级 缺点是模型过于简单，准确度较低。 paper reading2这篇文章是在word2vec的基础上拓展了，用来学习词向量表示 Enriching Word Vectors with Subword Information AbstractPopular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. 之前的模型在用离散的向量表示单词时都忽略了单词的形态。 In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. 这篇文章提出了一个skipgram模型,其中每一个单词表示为组成这个单词的字袋模型 a bag of character n-grams. 一个单词的词向量表示为这些 n-grams表示的总和。 Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information. We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach. 这篇文章可以看作是word2vec的拓展，主要是针对一些形态特别复杂的语言。 word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。 Related workMorphological word representations针对形态词表示已有的工作： 传统的用单词的形态特征来表示单词： [Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored neural language models. In Proc. NAACL] introduced factored neural language models. 因式分解模型 words are represented as sets of features. These features might include morphological information Schütze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing the four-grams representations. 这篇文正的工作跟本文的方法是比较接近的。 Character level features for NLP NLP的字符特征字符级别的研究工作最近很多了。一类是基于RNN的，另一类是基于CNN的。 General Model这一部分是对word2vec中跳字模型的回顾。Skip-gram predicts the distribution (probability) of context words from a center word. giving a sequence of words $w_1, w_2,…,w_T$ 那么skipgram 模型就是最大化对数似然函数： $$\\sum_{t=1}^T\\sum_{c\\in C_t}logp(w_c|w_t)$$ we are given a scoring function s which maps pairs of (word, context) to scores in R. $$p(w_c|w_t)=\\dfrac{e^{s(w_t,w_c)}}{\\sum_{j=1}^We^{s(w_t,j)}}$$ The problem of predicting context words can instead be framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. For the word at position t we consider all context words as positive examples and sample negatives at random from the dictionary. 这篇文章采用负采样的方法。与原本的softmax或者是hierarchical softmax不一样的是，负采样中预测一个上下文的单词context $w_c$ 是把它看做一个独立的二分类，存在或者是不存在。 因此选择一个上下文 position c, using binary logistic loss we obtain the following negative log-likelihood:: $$log(1+e^{-s(w_t,w_c)})+\\sum_{n\\in N_{t,c}}log(1+e^{s(w_t,n)})$$ 其实跟word2vec中是一样的就是 $-log\\sigma(s(w_t,w_c))=-log\\dfrac{1}{1+e^{-s(w_t,w_c)}}=log(1+e^{-s(w_t,w_c)})$ 那么对于整个Sequence，设定 $l: x\\rightarrow log(1+e^{-x})$ 那么： $$\\sum_{t=1}^T[\\sum_{c\\in C_t}l(s(w_t,w_c))+\\sum_{n\\in N_{t,c}}l(-s(w_t,n))]$$ $N_{t,c}$ is a set of negative examples sampled from the vocabulary. 怎么选负采样呢？ 每个单词都被给予一个等于它频率的权重（单词出现的数目）的3/4次方。选择某个单词的概率就是它的权重除以所有单词权重之和。 $$p(w_i)=\\dfrac{f(w_i)^{3/4}}{\\sum_{j=0}^W(f(w_j)^{3/4})}$$ Then the score can be computed as the scalar product between word and context vectors as: $$s(w_t,w_c) = u_{w_t}^Tv_{w_v}$$ Subword modelBy using a distinct vector representation for each word, the skipgram model ignores the internal structure of words. In this section, we propose a different scoring function s, in order to take into account this information. 单词的离散词向量表示是忽略了单词内部的结构信息的，也就是其字母组成。 给每个单词左右加上 &lt; 和 &gt;，用来区分前缀和后缀。对于单词 where 来说，用 character trigram 表示： 用 $z_g$ 表示n-gram g 的向量表示。那么 scoring function: $$s(w,c)=\\sum_{g\\in G_w}z_g^Tv_c$$ 如果词表很大的话，其对应的 n-gram 也会非常多吧，为了限制占用的内存，we use a hashing function that maps n-grams to integers in 1 to K. We hash character sequences using the Fowler-Noll-Vo hashing function (specifically the FNV-1a variant).1 We set $K = 2.10^6$ below. 代码实现需要注意的问题 代码实现中对于sentence的向量表示，是unigram的平均值，如果要让效果更好，可以添加bigram, trigram等。 tf.train.exponential_decay tf.nn.nce_loss","link":"/2018/05/23/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%971-fasttext/"},{"title":"Tensorflow RNN API 源码阅读","text":"在三星研究院实习一段时间发现在公司写代码和在学校还是有差别的。一是在公司要追求效率，会使用很多官方封装好的api，而在学校的时候因为要去理解内部原理，更多的是在造轮子，导致对很多 api 不是很熟悉。但实际上官方api不仅在速度，以及全面性上都比自己写的还是好很多的。二是，在公司对代码的复用率要求比较高，模型跑到哪一个版本了，对应的参数都要留下来，随时可以跑起来，而不是重新训练，这对模型、参数的保存要求很重要。以及在测试集上的性能指标都要在代码上很完整，而不是仅仅看看 loss 和 accuracy 就可以的。 这节内容主要是详细过一遍 tensorflow 里面的 rnn api，根据RNN and Cells (contrib)这里的顺序逐步深入研究 先回顾一下 RNN/LSTM/GRU:参考之前 cs224d 的笔记 cs224d-lecture9 机器翻译 cs224d-lecture8-RNN 发现有些小错误，但不影响自己复习。。 basic rnn： $$h_t = \\sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$ 先看 tf.contrib.rnn.RNNCellhttps://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/ops/rnn_cell_impl.py#L170) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251@tf_export(&quot;nn.rnn_cell.RNNCell&quot;)class RNNCell(base_layer.Layer): &quot;&quot;&quot;Abstract object representing an RNN cell. Every `RNNCell` must have the properties below and implement `call` with the signature `(output, next_state) = call(input, state)`. RNNCell 是一个抽象的父类，之后更复杂的 RNN/LSTM/GRU 都是重新实现 call 函数，也就是更新隐藏状态 的方式改变了。 The optional third input argument, `scope`, is allowed for backwards compatibility purposes; but should be left off for new subclasses. scope 这个参数管理变量，在反向传播中变量是否可训练。 This definition of cell differs from the definition used in the literature. In the literature, 'cell' refers to an object with a single scalar output. This definition refers to a horizontal array of such units. 这里的 cell 的概念和一些论文中是不一样的。在论文中，cell 表示一个神经元，也就是单个值。而这里表示的是 一组神经元，比如隐藏状态[batch, num_units]. An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs. This operation results in an output matrix with `self.output_size` columns. If `self.state_size` is an integer, this operation also results in a new state matrix with `self.state_size` columns. If `self.state_size` is a (possibly nested tuple of) TensorShape object(s), then it should return a matching structure of Tensors having shape `[batch_size].concatenate(s)` for each `s` in `self.batch_size`. rnn cell 的输入是一个状态 state 和 input 矩阵，参数有 self.output_size 和 self.state_size. 分别表示输出层和隐藏层的维度。其中 state_size 可能是 tuple，这个之后在看。 &quot;&quot;&quot; def __call__(self, inputs, state, scope=None): &quot;&quot;&quot;Run this RNN cell on inputs, starting from the given state. Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: if `self.state_size` is an integer, this should be a `2-D Tensor` with shape `[batch_size, self.state_size]`. Otherwise, if `self.state_size` is a tuple of integers, this should be a tuple with shapes `[batch_size, s] for s in self.state_size`. scope: VariableScope for the created subgraph; defaults to class name. Returns: A pair containing: - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`. - New state: Either a single `2-D` tensor, or a tuple of tensors matching the arity and shapes of `state`. &quot;&quot;&quot; if scope is not None: with vs.variable_scope(scope, custom_getter=self._rnn_get_variable) as scope: return super(RNNCell, self).__call__(inputs, state, scope=scope) else: scope_attrname = &quot;rnncell_scope&quot; scope = getattr(self, scope_attrname, None) if scope is None: scope = vs.variable_scope(vs.get_variable_scope(), custom_getter=self._rnn_get_variable) setattr(self, scope_attrname, scope) with scope: return super(RNNCell, self).__call__(inputs, state) def _rnn_get_variable(self, getter, *args, **kwargs): variable = getter(*args, **kwargs) if context.executing_eagerly(): trainable = variable._trainable # pylint: disable=protected-access else: trainable = ( variable in tf_variables.trainable_variables() or (isinstance(variable, tf_variables.PartitionedVariable) and list(variable)[0] in tf_variables.trainable_variables())) if trainable and variable not in self._trainable_weights: self._trainable_weights.append(variable) elif not trainable and variable not in self._non_trainable_weights: self._non_trainable_weights.append(variable) return variable @property def state_size(self): &quot;&quot;&quot;size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. &quot;&quot;&quot; raise NotImplementedError(&quot;Abstract method&quot;) @property def output_size(self): &quot;&quot;&quot;Integer or TensorShape: size of outputs produced by this cell.&quot;&quot;&quot; raise NotImplementedError(&quot;Abstract method&quot;) def build(self, _): # This tells the parent Layer object that it's OK to call # self.add_variable() inside the call() method. pass def zero_state(self, batch_size, dtype): &quot;&quot;&quot;Return zero-filled state tensor(s). Args: batch_size: int, float, or unit Tensor representing the batch size. dtype: the data type to use for the state. Returns: If `state_size` is an int or TensorShape, then the return value is a `N-D` tensor of shape `[batch_size, state_size]` filled with zeros. If `state_size` is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of `2-D` tensors with the shapes `[batch_size, s]` for each s in `state_size`. &quot;&quot;&quot; # Try to use the last cached zero_state. This is done to avoid recreating # zeros, especially when eager execution is enabled. state_size = self.state_size is_eager = context.executing_eagerly() if is_eager and hasattr(self, &quot;_last_zero_state&quot;): (last_state_size, last_batch_size, last_dtype, last_output) = getattr(self, &quot;_last_zero_state&quot;) if (last_batch_size == batch_size and last_dtype == dtype and last_state_size == state_size): return last_output with ops.name_scope(type(self).__name__ + &quot;ZeroState&quot;, values=[batch_size]): output = _zero_state_tensors(state_size, batch_size, dtype) if is_eager: self._last_zero_state = (state_size, batch_size, dtype, output) return output 两个属性 output_size, state_size 分别表示输出层的维度和隐藏层的维度。call 函数用来表示计算下一个时间步的隐藏状态和输出，zero_state 函数用来初始化初始状态全为 0, 这里 state_size 有两种情况，一种是 int 或 tensorshape,那么 [batch, state_size]. 如果是多层嵌套 rnn, 那么初始状态 [batch, s] for s in state_size 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class LayerRNNCell(RNNCell): &quot;&quot;&quot;Subclass of RNNCells that act like proper `tf.Layer` objects. def __call__(self, inputs, state, scope=None, *args, **kwargs): &quot;&quot;&quot;Run this RNN cell on inputs, starting from the given state. Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: if `self.state_size` is an integer, this should be a `2-D Tensor` with shape `[batch_size, self.state_size]`. Otherwise, if `self.state_size` is a tuple of integers, this should be a tuple with shapes `[batch_size, s] for s in self.state_size`. scope: optional cell scope. *args: Additional positional arguments. **kwargs: Additional keyword arguments. Returns: A pair containing: - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`. - New state: Either a single `2-D` tensor, or a tuple of tensors matching the arity and shapes of `state`. &quot;&quot;&quot; # Bypass RNNCell's variable capturing semantics for LayerRNNCell. # Instead, it is up to subclasses to provide a proper build # method. See the class docstring for more details. return base_layer.Layer.__call__(self, inputs, state, scope=scope, *args, **kwargs) 再看 Core RNN Cells tf.contrib.rnn.BasicRNNCell tf.contrib.rnn.BasicLSTMCell tf.contrib.rnn.GRUCell tf.contrib.rnn.LSTMCell tf.contrib.rnn.LayerNormBasicLSTMCell tf.contrib.rnn.BasicRNNCell直接扒源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129@tf_export(&quot;nn.rnn_cell.BasicRNNCell&quot;)class BasicRNNCell(LayerRNNCell): &quot;&quot;&quot;The most basic RNN cell. Args: num_units: int, The number of units in the RNN cell. activation: Nonlinearity to use. Default: `tanh`. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. name: String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. dtype: Default dtype of the layer (default of `None` means use the type of the first input). Required when `build` is called before `call`. &quot;&quot;&quot; def __init__(self, num_units, activation=None, reuse=None, name=None, dtype=None): super(BasicRNNCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) # Inputs must be 2-dimensional. self.input_spec = base_layer.InputSpec(ndim=2) self._num_units = num_units self._activation = activation or math_ops.tanh @property def state_size(self): return self._num_units @property def output_size(self): return self._num_units def build(self, inputs_shape): if inputs_shape[1].value is None: raise ValueError(&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot; % inputs_shape) input_depth = inputs_shape[1].value self._kernel = self.add_variable( _WEIGHTS_VARIABLE_NAME, shape=[input_depth + self._num_units, self._num_units]) self._bias = self.add_variable( _BIAS_VARIABLE_NAME, shape=[self._num_units], initializer=init_ops.zeros_initializer(dtype=self.dtype)) self.built = True def call(self, inputs, state): &quot;&quot;&quot;Most basic RNN: output = new_state = act(W * input + U * state + B).&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) output = self._activation(gate_inputs) return output, output 可以发现 state_size = output_size = num_units, 以及输出就是下一个隐藏状态 output = new_state = act(W * input + U * state + B) = act(W[input, state] + b) 其中 self._kernel 表示 W, 其维度是 [input_depth + num_units, num_units] 1234567891011import tensorflow as tfimport tensorflow.contrib.eager as tfetf.enable_eager_execution()print(tfe.executing_eagerly()) True 12345cell = tf.contrib.rnn.BasicRNNCell(num_units=128, activation=None)print(cell.state_size, cell.output_size) 128 128 123456789inputs = tf.random_normal(shape=[32, 100], dtype=tf.float32)h0 = cell.zero_state(batch_size=32, dtype=tf.float32)output, state = cell(inputs=inputs, state=h0)print(output.shape, state.shape) (32, 128) (32, 128) tf.contrib.rnn.BasicLSTMCell先回顾下 LSTM: 自己试着手敲公式～ 看着图还是简单，不看图是否也可以呢？ 三个gate：遗忘门，输入/更新门，输出门 $$f_t=\\sigma(W^{f}x_t + U^{f}h_{t-1})$$ $$i_t=\\sigma(W^{i}x_t + U^{i}h_{t-1})$$ $$o_t=\\sigma(W^{o}x_t + U^{o}h_{t-1})$$ new memory cell: $$\\hat c=tanh(W^cx_t + U^ch_{t-1})$$ 输入门作用于新的记忆细胞,遗忘门作用于上一个记忆细胞，并得到最终的记忆细胞: $$c_t=f_t\\circ c_{t-1} + i_t\\circ\\hat c$$ 用新的memory cell 和输出门得到新的隐藏状态： $$h_t = tanh(o_t\\circ c_t)$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245class BasicLSTMCell(LayerRNNCell): &quot;&quot;&quot;Basic LSTM recurrent network cell. The implementation is based on: http://arxiv.org/abs/1409.2329. We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training. It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline. For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell} that follows. &quot;&quot;&quot; def __init__(self, num_units, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None, dtype=None): &quot;&quot;&quot;Initialize the basic LSTM cell. Args: num_units: int, The number of units in the LSTM cell. forget_bias: float, The bias added to forget gates (see above). Must set to `0.0` manually when restoring from CudnnLSTM-trained checkpoints. state_is_tuple: If True, accepted and returned states are 2-tuples of the `c_state` and `m_state`. If False, they are concatenated along the column axis. The latter behavior will soon be deprecated. activation: Activation function of the inner states. Default: `tanh`. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. name: String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. dtype: Default dtype of the layer (default of `None` means use the type of the first input). Required when `build` is called before `call`. When restoring from CudnnLSTM-trained checkpoints, must use `CudnnCompatibleLSTMCell` instead. &quot;&quot;&quot; super(BasicLSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) if not state_is_tuple: logging.warn(&quot;%s: Using a concatenated state is slower and will soon be &quot; &quot;deprecated. Use state_is_tuple=True.&quot;, self) # Inputs must be 2-dimensional. self.input_spec = base_layer.InputSpec(ndim=2) self._num_units = num_units self._forget_bias = forget_bias self._state_is_tuple = state_is_tuple self._activation = activation or math_ops.tanh @property def state_size(self): return (LSTMStateTuple(self._num_units, self._num_units) if self._state_is_tuple else 2 * self._num_units) @property def output_size(self): return self._num_units def build(self, inputs_shape): if inputs_shape[1].value is None: raise ValueError(&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot; % inputs_shape) input_depth = inputs_shape[1].value h_depth = self._num_units self._kernel = self.add_variable( _WEIGHTS_VARIABLE_NAME, shape=[input_depth + h_depth, 4 * self._num_units]) self._bias = self.add_variable( _BIAS_VARIABLE_NAME, shape=[4 * self._num_units], initializer=init_ops.zeros_initializer(dtype=self.dtype)) self.built = True def call(self, inputs, state): &quot;&quot;&quot;Long short-term memory cell (LSTM). Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: An `LSTMStateTuple` of state tensors, each shaped `[batch_size, num_units]`, if `state_is_tuple` has been set to `True`. Otherwise, a `Tensor` shaped `[batch_size, 2 * num_units]`. Returns: A pair containing the new hidden state, and the new state (either a `LSTMStateTuple` or a concatenated state, depending on `state_is_tuple`). &quot;&quot;&quot; sigmoid = math_ops.sigmoid one = constant_op.constant(1, dtype=dtypes.int32) # Parameters of gates are concatenated into one multiply for efficiency. if self._state_is_tuple: c, h = state else: c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one) gate_inputs = math_ops.matmul( array_ops.concat([inputs, h], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) # i = input_gate, j = new_input, f = forget_gate, o = output_gate i, j, f, o = array_ops.split( value=gate_inputs, num_or_size_splits=4, axis=one) forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype) # Note that using `add` and `multiply` instead of `+` and `*` gives a # performance improvement. So using those at the cost of readability. add = math_ops.add multiply = math_ops.multiply new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))), multiply(sigmoid(i), self._activation(j))) new_h = multiply(self._activation(new_c), sigmoid(o)) if self._state_is_tuple: new_state = LSTMStateTuple(new_c, new_h) else: new_state = array_ops.concat([new_c, new_h], 1) return new_h, new_state 阅读源码可以发现具体实现与上面的公式还是有点差别的。 先 concat[input, h], 然后 gate_input = matmul(concat[input, h], self._kernel)+self._bias,多了偏置项,这里的矩阵维度 [input_depth + h_depth,4*num_units]. 然后 i,j,f,o = split(gate_input, 4, axis=1). 其中 j 表示 new memory cell. 然后计算 new_c，其中 i,f,o 对应的激活函数确定是 sigmoid,因为其范围只能在(0,1)之间。但是 j 的激活函数self._activation 可以选择，默认是 tanh. 与公式的差别之二在于 self._forget_bias.遗忘门在激活函数 $\\sigma$ 之前加了偏置，目的是避免在训练初期丢失太多信息。 要注意 state 的形式，取决于参数 self._state_is_tuple. 其中 c,h=state，表示 $c_{t-1},h_{t-1}$ 123lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=128, forget_bias=1.0, state_is_tuple=True) WARNING:tensorflow:From &lt;ipython-input-9-3f4ca183c5d7&gt;:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version. Instructions for updating: This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell'). 提示要更新了，那就换成最新的吧 123lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=128, forget_bias=1.0, state_is_tuple=True) 123lstm_cell.output_size, lstm_cell.state_size (128, LSTMStateTuple(c=128, h=128)) 123h0 = lstm_cell.zero_state(batch_size=30, dtype=tf.float32) 我们发现 lstm 的状态 state 是一个tuple,分别对应 c_t 和 h_t. 123456789class LSTMStateTuple(_LSTMStateTuple): &quot;&quot;&quot;Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state. Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state and `h` is the output. 这里的解释感觉是有点问题的，c is the hidden state and h is the output. 看源码 12345678910111213141516171819new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))), multiply(sigmoid(i), self._activation(j)))new_h = multiply(self._activation(new_c), sigmoid(o))if self._state_is_tuple: new_state = LSTMStateTuple(new_c, new_h)else: new_state = array_ops.concat([new_c, new_h], 1)return new_h, new_state 发现 c 表示的就是 new memory cell, 而 h 表示的是最后的隐藏状态。 1234567# 计算下一步的 output 和 stateinputs = tf.random_normal(shape=[30, 100], dtype=tf.float32)output, state = lstm_cell(inputs, h0) 123output.shape, state[0].shape, state[1].shape (TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(128)])) 123state.c, state.h # c 和 h 的值是不一样的 (&lt;tf.Tensor: id=108, shape=(30, 128), dtype=float32, numpy= array([[ 0.08166471, 0.14020835, 0.07970127, ..., -0.1540019 , 0.38848224, -0.0842322 ], [-0.03643086, -0.20558938, 0.1503458 , ..., 0.01846285, 0.15610473, 0.04408235], [-0.0933667 , 0.03454542, -0.09073547, ..., -0.12701994, -0.34669587, 0.09373946], ..., [-0.00752909, 0.22412673, -0.270195 , ..., 0.09341058, -0.20986181, -0.18622127], [ 0.18778914, 0.37687936, -0.24727295, ..., -0.06409463, 0.00218048, 0.5940756 ], [ 0.04073388, -0.08431841, 0.35944715, ..., 0.14135318, 0.08472287, -0.11058106]], dtype=float32)&gt;, &lt;tf.Tensor: id=111, shape=(30, 128), dtype=float32, numpy= array([[ 0.04490132, 0.07412361, 0.03662094, ..., -0.07611651, 0.17290959, -0.0277745 ], [-0.02212535, -0.13554382, 0.08272093, ..., 0.00918258, 0.0861209 , 0.02614526], [-0.05723168, 0.01372226, -0.02919216, ..., -0.06374882, -0.1918035 , 0.03912015], ..., [-0.00377504, 0.15181372, -0.14555399, ..., 0.06073361, -0.09804281, -0.07492835], [ 0.10244624, 0.17440473, -0.09896267, ..., -0.03794969, 0.00123257, 0.21985768], [ 0.01832823, -0.03795732, 0.1654894 , ..., 0.05827027, 0.02769112, -0.05957894]], dtype=float32)&gt;) tf.nn.rnn_cell.GRUCell先回顾下 GRU. 手敲 GRU 公式： $$r_t=\\sigma(W^rx_t + U^rh_{t-1})$$ $$z_t=\\sigma(W^zx_t + U^zh_{t-1})$$ $$\\tilde h_t = tanh(Wx_t + r_t\\circ h_{t-1})$$ $$h_t=(1-z_t)\\circ\\tilde h_t + z_t\\circ h_{t-1}$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197@tf_export(&quot;nn.rnn_cell.GRUCell&quot;)class GRUCell(LayerRNNCell): &quot;&quot;&quot;Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078). Args: num_units: int, The number of units in the GRU cell. activation: Nonlinearity to use. Default: `tanh`. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. kernel_initializer: (optional) The initializer to use for the weight and projection matrices. bias_initializer: (optional) The initializer to use for the bias. name: String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. dtype: Default dtype of the layer (default of `None` means use the type of the first input). Required when `build` is called before `call`. &quot;&quot;&quot; def __init__(self, num_units, activation=None, reuse=None, kernel_initializer=None, bias_initializer=None, name=None, dtype=None): super(GRUCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) # Inputs must be 2-dimensional. self.input_spec = base_layer.InputSpec(ndim=2) self._num_units = num_units self._activation = activation or math_ops.tanh self._kernel_initializer = kernel_initializer self._bias_initializer = bias_initializer @property def state_size(self): return self._num_units @property def output_size(self): return self._num_units def build(self, inputs_shape): if inputs_shape[1].value is None: raise ValueError(&quot;Expected inputs.shape[-1] to be known, saw shape: %s&quot; % inputs_shape) input_depth = inputs_shape[1].value self._gate_kernel = self.add_variable( &quot;gates/%s&quot; % _WEIGHTS_VARIABLE_NAME, shape=[input_depth + self._num_units, 2 * self._num_units], initializer=self._kernel_initializer) self._gate_bias = self.add_variable( &quot;gates/%s&quot; % _BIAS_VARIABLE_NAME, shape=[2 * self._num_units], initializer=( self._bias_initializer if self._bias_initializer is not None else init_ops.constant_initializer(1.0, dtype=self.dtype))) self._candidate_kernel = self.add_variable( &quot;candidate/%s&quot; % _WEIGHTS_VARIABLE_NAME, shape=[input_depth + self._num_units, self._num_units], initializer=self._kernel_initializer) self._candidate_bias = self.add_variable( &quot;candidate/%s&quot; % _BIAS_VARIABLE_NAME, shape=[self._num_units], initializer=( self._bias_initializer if self._bias_initializer is not None else init_ops.zeros_initializer(dtype=self.dtype))) self.built = True def call(self, inputs, state): &quot;&quot;&quot;Gated recurrent unit (GRU) with nunits cells.&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._gate_kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias) value = math_ops.sigmoid(gate_inputs) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state candidate = math_ops.matmul( array_ops.concat([inputs, r_state], 1), self._candidate_kernel) candidate = nn_ops.bias_add(candidate, self._candidate_bias) c = self._activation(candidate) new_h = u * state + (1 - u) * c return new_h, new_h_LSTMStateTuple = collections.namedtuple(&quot;LSTMStateTuple&quot;, (&quot;c&quot;, &quot;h&quot;)) 仔细阅读源码发现在 $sigma$ 计算 gate，以及 tanh 计算 candidate 之前都有偏置项，不过公式中都没写出来。而且在不设置 bias 的初始值时，默认的 GRU 中 gate_bias 的初始值是 1, 而 LSTM 中 gate_bias 的初始值是 0. 123gru_cell = tf.nn.rnn_cell.GRUCell(num_units=128) 123gru_cell.state_size, gru_cell.output_size (128, 128) 123h0 = gru_cell.zero_state(batch_size=30, dtype=tf.float32) 1234567inputs = tf.random_normal(shape=[30, 100], dtype=tf.float32)output, state = gru_cell(inputs, h0)output.shape, state.shape (TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(128)])) 出现个很神奇的现象，如果我写成这样： 12345output, state = gru_cell.call(inputs, h0) # 会报错的，gru_cell 没有 self._gate_kernel 这个属性，很神奇， # 不过这里先运行上面那行代码，所以没有出现报错 tf.nn.rnn_cell.LSTMCell, tf.contrib.rnn.LSTMCell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@tf_export(&quot;nn.rnn_cell.LSTMCell&quot;)class LSTMCell(LayerRNNCell): &quot;&quot;&quot;Long short-term memory unit (LSTM) recurrent network cell. The default non-peephole implementation is based on: http://www.bioinf.jku.at/publications/older/2604.pdf S. Hochreiter and J. Schmidhuber. &quot;Long Short-Term Memory&quot;. Neural Computation, 9(8):1735-1780, 1997. The peephole implementation is based on: https://research.google.com/pubs/archive/43905.pdf Hasim Sak, Andrew Senior, and Francoise Beaufays. &quot;Long short-term memory recurrent neural network architectures for large scale acoustic modeling.&quot; INTERSPEECH, 2014. The class uses optional peep-hole connections, optional cell clipping, and an optional projection layer. &quot;&quot;&quot; def __init__(self, num_units, use_peepholes=False, cell_clip=None, initializer=None, num_proj=None, proj_clip=None, num_unit_shards=None, num_proj_shards=None, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None, dtype=None): &quot;&quot;&quot;Initialize the parameters for an LSTM cell. &quot;&quot;&quot; super(LSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype) 相比 BasicLSTMCell 多了这四个参数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071Args: use_peepholes: bool, set True to enable diagonal/peephole connections. cell_clip: (optional) A float value, if provided the cell state is clipped by this value prior to the cell output activation. num_proj: (optional) int, The output dimensionality for the projection matrices. If None, no projection is performed. proj_clip: (optional) A float value. If `num_proj &gt; 0` and `proj_clip` is provided, then the projected values are clipped elementwise to within `[-proj_clip, proj_clip]`.````其中 cell_clip 很好理解，就是限制隐藏状态的大小，也就是 output 和 state 的大小。 而 num_proj 呢？```python if num_proj: self._state_size = ( LSTMStateTuple(num_units, num_proj) if state_is_tuple else num_units + num_proj) self._output_size = num_proj else: self._state_size = ( LSTMStateTuple(num_units, num_units) if state_is_tuple else 2 * num_units) self._output_size = num_units ````通过源码可以发现，如果有 num_proj 那么 state 还要加一个全链接， state_size = num_units + num_proj. 而 proj_clip 是限制这个全链接的输出的。BacisLSTMCell 和 LSTMCell 区别还在于后者增加了 peephole 和 cell_clip![](https://img-blog.csdn.net/20171201095120010?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWJjbGhxMjAwNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)输入输出的shape，以及状态都是一样的，只不过内部计算方式更加复杂了。对于 peephole 是值得计算 gate 时，考虑到了 $c_{t-1}$ 和 $c_t$.```pythoncell = tf.nn.rnn_cell.LSTMCell(num_units=64,cell_clip=0.000000001, num_proj=128, proj_clip=0.001) 123cell.state_size, cell.output_size (LSTMStateTuple(c=64, h=128), 128) 发现 state_size 中的 h 维度发生了变化，相当于在每一个时间步得到的 state.h 之后再添加一个全链接。 在 decoder 中可以将 num_proj 设置为词表的大小，那么输出就是对应时间步的词表的分布。在此基础上在 softmax 就可以吧？ 但是下一个时间步的输入的隐藏状态 $h_{t-1}$ 岂不是维度为词表大小。。。感觉最好还是不用这个参数吧 12345h0 = cell.zero_state(batch_size=30, dtype=tf.float32)h0.c.shape, h0.h.shape (TensorShape([Dimension(30), Dimension(64)]), TensorShape([Dimension(30), Dimension(128)])) 123inputs = tf.ones(shape=[30,50]) 123output, state = cell(inputs=inputs, state=h0) 123output.shape, state.c.shape, state.h.shape (TensorShape([Dimension(30), Dimension(128)]), TensorShape([Dimension(30), Dimension(64)]), TensorShape([Dimension(30), Dimension(128)])) 封装了 RNN 的其他组件Core RNN Cell wrappers (RNNCells that wrap other RNNCells) tf.contrib.rnn.MultiRNNCell tf.contrib.rnn.LSTMBlockWrapper tf.contrib.rnn.DropoutWrapper tf.contrib.rnn.EmbeddingWrapper tf.contrib.rnn.InputProjectionWrapper tf.contrib.rnn.OutputProjectionWrapper tf.contrib.rnn.DeviceWrapper tf.contrib.rnn.ResidualWrapper 主要看 tf.contrib.rnn.MultiRNNCell 和 tf.contrib.rnn.DropoutWrapper吧，其他的封装的太好了也不好，用的其实也少。 tf.contrib.rnn.MultiRNNCell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165class MultiRNNCell(RNNCell): &quot;&quot;&quot; RNN cell composed sequentially of multiple simple cells. &quot;&quot;&quot; def __init__(self, cells, state_is_tuple=True): &quot;&quot;&quot;Create a RNN cell composed sequentially of a number of RNNCells. Args: cells: list of RNNCells that will be composed in this order. state_is_tuple: If True, accepted and returned states are n-tuples, where `n = len(cells)`. If False, the states are all concatenated along the column axis. This latter behavior will soon be deprecated. Raises: ValueError: if cells is empty (not allowed), or at least one of the cells returns a state tuple but the flag `state_is_tuple` is `False`. &quot;&quot;&quot; super(MultiRNNCell, self).__init__() if not cells: raise ValueError(&quot;Must specify at least one cell for MultiRNNCell.&quot;) if not nest.is_sequence(cells): raise TypeError( &quot;cells must be a list or tuple, but saw: %s.&quot; % cells) self._cells = cells for cell_number, cell in enumerate(self._cells): # Add Checkpointable dependencies on these cells so their variables get # saved with this object when using object-based saving. if isinstance(cell, checkpointable.CheckpointableBase): # TODO(allenl): Track down non-Checkpointable callers. self._track_checkpointable(cell, name=&quot;cell-%d&quot; % (cell_number,)) self._state_is_tuple = state_is_tuple if not state_is_tuple: if any(nest.is_sequence(c.state_size) for c in self._cells): raise ValueError(&quot;Some cells return tuples of states, but the flag &quot; &quot;state_is_tuple is not set. State sizes are: %s&quot; % str([c.state_size for c in self._cells])) @property def state_size(self): if self._state_is_tuple: return tuple(cell.state_size for cell in self._cells) else: return sum([cell.state_size for cell in self._cells]) @property def output_size(self): return self._cells[-1].output_size def zero_state(self, batch_size, dtype): with ops.name_scope(type(self).__name__ + &quot;ZeroState&quot;, values=[batch_size]): if self._state_is_tuple: return tuple(cell.zero_state(batch_size, dtype) for cell in self._cells) else: # We know here that state_size of each cell is not a tuple and # presumably does not contain TensorArrays or anything else fancy return super(MultiRNNCell, self).zero_state(batch_size, dtype) def call(self, inputs, state): &quot;&quot;&quot;Run this multi-layer cell on inputs, starting from state.&quot;&quot;&quot; cur_state_pos = 0 cur_inp = inputs new_states = [] for i, cell in enumerate(self._cells): with vs.variable_scope(&quot;cell_%d&quot; % i): if self._state_is_tuple: if not nest.is_sequence(state): raise ValueError( &quot;Expected state to be a tuple of length %d, but received: %s&quot; % (len(self.state_size), state)) cur_state = state[i] else: cur_state = array_ops.slice(state, [0, cur_state_pos], [-1, cell.state_size]) cur_state_pos += cell.state_size cur_inp, new_state = cell(cur_inp, cur_state) new_states.append(new_state) new_states = (tuple(new_states) if self._state_is_tuple else array_ops.concat(new_states, 1)) return cur_inp, new_states 参数 cell 是元素为 RNNCell对象 的 list 或 tuple. 其实还是只是计算一个时间步的 state 这里先不考虑双向，只考虑 deep. 也就是使用 MultiRNNCell 1234567num_units = [64, 128]stack_rnns = [tf.nn.rnn_cell.BasicLSTMCell(num_units=i) for i in num_units]stack_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(stack_rnns) 123h0 = [cell.zero_state(batch_size=32, dtype=tf.float32) for cell in stack_rnn] 12345inputs = tf.random_normal(shape=[32, 100], dtype=tf.float32)output, state = stack_rnn_cell(inputs=inputs, state=h0) 123output.shape TensorShape([Dimension(32), Dimension(128)]) 123state[0].c.shape, state[0].h.shape (TensorShape([Dimension(32), Dimension(64)]), TensorShape([Dimension(32), Dimension(64)])) 123state[1].c.shape, state[1].h.shape (TensorShape([Dimension(32), Dimension(128)]), TensorShape([Dimension(32), Dimension(128)])) 源码中的一部分： 12345cur_inp, new_state = cell(cur_inp, cur_state)new_states.append(new_state) 其中从源码中也可以发现把每一层的 state 都储存起来了，而 output 要作为下一层的输入，最后得到的 output 是最后一层的输出。 tf.contrib.rnn.DropoutWrapper参考paper: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141@tf_export(&quot;nn.rnn_cell.DropoutWrapper&quot;)class DropoutWrapper(RNNCell): &quot;&quot;&quot;Operator adding dropout to inputs and outputs of the given cell.&quot;&quot;&quot; def __init__(self, cell, input_keep_prob=1.0, output_keep_prob=1.0, state_keep_prob=1.0, variational_recurrent=False, input_size=None, dtype=None, seed=None, dropout_state_filter_visitor=None): &quot;&quot;&quot;Create a cell with added input, state, and/or output dropout. If `variational_recurrent` is set to `True` (**NOT** the default behavior), then the same dropout mask is applied at every step, as described in: Y. Gal, Z Ghahramani. &quot;A Theoretically Grounded Application of Dropout in Recurrent Neural Networks&quot;. https://arxiv.org/abs/1512.05287 如果参数 variational_recurrent 设置为 True，那么 dropout 在每一个时间步都会执行 dropout， Otherwise a different dropout mask is applied at every time step. Note, by default (unless a custom `dropout_state_filter` is provided), the memory state (`c` component of any `LSTMStateTuple`) passing through a `DropoutWrapper` is never modified. This behavior is described in the above article. Args: cell: an RNNCell, a projection to output_size is added to it. input_keep_prob: unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input dropout will be added. output_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. state_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the outgoing states of the cell. **Note** the state components to which dropout is applied when `state_keep_prob` is in `(0, 1)` are also determined by the argument `dropout_state_filter_visitor` (e.g. by default dropout is never applied to the `c` component of an `LSTMStateTuple`). 上面三个参数分别表示 input，output，state 是否 dropout，以及 dropout 率。 variational_recurrent: Python bool. If `True`, then the same dropout pattern is applied across all time steps per run call. If this parameter is set, `input_size` **must** be provided. 这个参数如果为 True，那么每一个时间步都需要 dropout. input_size: (optional) (possibly nested tuple of) `TensorShape` objects containing the depth(s) of the input tensors expected to be passed in to the `DropoutWrapper`. Required and used **iff** `variational_recurrent = True` and `input_keep_prob &lt; 1`. dtype: (optional) The `dtype` of the input, state, and output tensors. Required and used **iff** `variational_recurrent = True`. seed: (optional) integer, the randomness seed. dropout_state_filter_visitor: (optional), default: (see below). Function that takes any hierarchical level of the state and returns a scalar or depth=1 structure of Python booleans describing which terms in the state should be dropped out. In addition, if the function returns `True`, dropout is applied across this sublevel. If the function returns `False`, dropout is not applied across this entire sublevel. Default behavior: perform dropout on all terms except the memory (`c`) state of `LSTMCellState` objects, and don't try to apply dropout to `TensorArray` objects: Raises: TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided but not `callable`. ValueError: if any of the keep_probs are not between 0 and 1. &quot;&quot;&quot; 123456789cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.LSTMCell(num_units=128), input_keep_prob=1.0, output_keep_prob=1.0, state_keep_prob=1.0) 123cell.state_size, cell.output_size (LSTMStateTuple(c=128, h=128), 128) 123456789# 多层 rnnfrom tensorflow.nn.rnn_cell import *NUM_UNITS = [32,64, 128]rnn = MultiRNNCell([DropoutWrapper(LSTMCell(num_units=n), output_keep_prob=0.8) for n in NUM_UNITS]) 123rnn.output_size, rnn.state_size (128, (LSTMStateTuple(c=32, h=32), LSTMStateTuple(c=64, h=64), LSTMStateTuple(c=128, h=128))) tf.nn.dynamic_rnn最后前面说了这么多 class，他们都只是一种计算当前时间步的 output 和 state 的方式，但是 rnn 处理的都是序列，所以怎么将这些 cell 对象封装到序列 rnn 中 12345678910111213def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None): &quot;&quot;&quot;Creates a recurrent neural network specified by RNNCell `cell`. Performs fully dynamic unrolling of `inputs`. &quot;&quot;&quot; 12345rnn_layers = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=n)) for n in [32, 64]]cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers) 123inputs = tf.random_normal(shape=[30, 10, 100]) 123initial_state = cell.zero_state(batch_size=30, dtype=tf.float32) 123output, state = tf.nn.dynamic_rnn(cell,inputs=inputs, initial_state=initial_state, dtype=tf.float32) 123output.shape TensorShape([Dimension(30), Dimension(10), Dimension(64)]) 123cell.state_size (LSTMStateTuple(c=32, h=32), LSTMStateTuple(c=64, h=64)) 所以目前为止，暂时ojbk了～～ 接下来就是在 attention 封装 rnn 了","link":"/2018/09/01/tensorflow-rnn-api-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"title":"文本分类系列2-textCNN","text":"paper reading主要框架和使用CNN进行文本分类的意图参考paper: Convolutional Neural Networks for Sentence Classification 可参考cs224d中的课堂笔记，这堂课就是讲的这篇paper： cs224d-lecture13 卷积神经网络 TextCNN详细过程： 第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了,然后在图像中图像的表示是[length, width, channel],这里将文本的表示[sequence_len, embed_size, 1]。可以看到下面代码中： 1234567embeded_words = tf.nn.embedding_lookup(self.embedding, self.input_x) # [None, sentence_len, embed_size]# three channels similar to the image. using the tf.nn.conv2dself.sentence_embedding_expanded = tf.expand_dims(embeded_words, axis=-1) # [None, sentence_len, embed_size, 1] 然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。上图中的filter有3个，分别为： filter:[2, 5, 2] ==&gt; feature map:[6,1,2] filter:[3, 5, 2] ==&gt; feature map:[5,1,2] filter:[4, 5, 2] ==&gt; feature map:[4,1,2] 第三维表示channels，卷积后得到两个feature maps. 第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成scale，这里每个fiter_size的channel为2，所以输入的pooling.shape=[batch_size,1,1,2], 然后concat为一个flatten向量。 最后接一层全连接的 softmax 层，输出每个类别的概率。 特征：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。 通道（Channels）：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。下面代码中的通道为1. 一维卷积（conv-1d）：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。 Pooling层：利用CNN解决文本分类问题的文章还是很多的，比如这篇 A Convolutional Neural Network for Modelling Sentences 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子： “ 我觉得这个地方景色还不错，但是人也实在太多了 ” 虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。 代码实现参数设置和模型具体实现参考paper: A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification 设计一个模型需要考虑的： input word vector representations 输入的词向量表示 filter region size(s); 卷积核的大小 the number of feature maps; 特征图的通道数 the activation function 激活函数 the pooling strategy 池化的方式 regularization terms (dropout/l2) 正则化项（dropout/l2） 需要注意的问题一个单本对应单个标签和多个标签的区别？关于多标签分类，应该看看周志华老师的这篇文章A Review on Multi-Label Learning Algorithms, 知乎上还有其他资料多标签（multi-label）数据的学习问题，常用的分类器或者分类策略有哪些？ 本文代码中的方法： 真实值labels的输入：单个标签的真实值是 input_y.shape=[batch_size], 多个标签的真实值是 input_y_multilabels.shape=[batch_size, label_size] 12345self.input_y = tf.placeholder(dtype=tf.int32, shape=[None], name='input_y')self.input_y_multilabels = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name=&quot;input_y_multilabels&quot;) 损失函数的选择： 评价指标的区别：","link":"/2018/05/30/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%972-textCNN/"},{"title":"文本分类系列4-textRCNN","text":"paper readingpaper: Recurrent Convolutional Neural Networks for Text Classification Introduction先对之前的研究进行一番批判(0.0). 传统的文本分类方法都是基于特征工程 feature representation，主要包括： 词袋模型 bag-of-words(BOW)model，用于提取unigram, bigram, n-grams的特征。 常见的特征选择的方法：frequency, MI (Cover and Thomas 2012), pLSA (Cai and Hofmann 2003), LDA (Hingmire et al. 2013)，用于选择具有更好的判别效果的特征。 其原理就是去噪声来提高分类效果。比如去掉停用词，使用信息增益，互信息，或者L1正则化来获取有用的特征。但传统的特征表示的方法通常忽视了上下文信息和词序信息。 Richard Socher 提出的 Recursive Neural Network RecusiveNN 通过语言的tree结构来获取句子的语义信息。但是分类的准确率太依赖文本的树结构。在文本分类之前建立一个树结构需要的计算复杂度就是 $O(n^2)$ （n是句子的长度）。所以对于很长的句子并不适用。 循环神经网络 Recurrent Neural Network 计算复杂度是 $O(n)$，优点是能够很好的捕获长文本的语义信息，但是在rnn模型中，later words are more dominatant than earlier words. 但是如果对与某一个文本的分类，出现在之前的word影响更大的话，RNN的表现就不会很好。 为解决RNN这个问题，可以将CNN这个没有偏见的模型引入到NLP的工作中来，CNN能公平的对待句子中的每一个短语。 To tackle the bias problem, the Convolutional Neural Network (CNN), an unbiased model is introduced to NLP tasks, which can fairly determine discriminative phrases in a text with a max-pooling layer. 但是呢，通过前面的学习我们知道CNN的filter是固定尺寸的（fixed window），如果尺寸太短，会丢失很多信息，如果尺寸过长，计算复杂度又太大。所以作者提出个问题：能不能通过基于窗口的神经网络（CNN）学到更多的上下文信息，更好的表示文本的语义信息呢？ Therefore, it raises a question: can we learn more contextual information than conventional window-based neural networks and represent the semantic of texts more precisely for text classification. 于是，这篇论文提出了 Recurrent Concolution Neural Network(RCNN). Model $$c_l{(w_i)} = f(W^{(l)}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))$$ $$c_r{(w_i)} = f(W^{(r)}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))$$ 这两个公式类似于双向RNN，将 $c_l(w_i)$ 看作前一个时刻的隐藏状态 $h_{t-1}$, $c_l(w_{i-1})$ 就是 t-2 时刻的隐藏状态 $h_{t-2}$… 所以这就是个双向RNN…. 然后比较有创新的是，作者将隐藏状态 $h_{t-1}$ 和 $\\tilde h_{t+1}$ ($\\tilde h$ 表示反向), 以及当前word的词向量堆在一起，作为当前词以及获取了上下文信息的向量表示。 $$x_i = [c_l(w_i);e(w_i);c_r(w_i)]$$ 然后是一个全连接层，这个可以看做textCNN中的卷积层,只是filter_size=1： $$y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})$$ 接着是最大池化层： $$y^{(3)} = max_{i=1}^ny_i^{(2)}$$ 然后是全连接层+softmax： $$y^{(4)} = W^{(4)}y^{(3)}+b^{(4)}$$ $$p_i=\\dfrac{exp(y_i^{(4)})}{\\sum_{k=1}^nexp(y_k^{(4)})}$$ 感觉就是双向rnn呀，只不过之前的方法是用最后一个隐藏层的输出作为整个sentence的向量表示，但这篇论文是用每一个时刻的向量表示(叠加了上下时刻的隐藏状态)，通过卷积层、maxpool后得到的向量来表示整个sentence. 确实是解决了RNN过于重视句子中靠后的词的问题，但是RNN训练慢的问题还是没有解决呀。但是在这里 brightmart/text_classification 中textCNN 和 RCNN的训练时间居然是一样的。why？ Results and Discussion代码实现需要注意的问题： tf.nn.rnn_cell.DropoutWrapper tf.nn.bidirectional_dynamic_rnn tf.einsum 损失函数的对比 tf.nn.softmax_cross_entropy_with_logits 词向量是否需要正则化 tensorflow.contrib.layers.python.layers import optimize_loss 和 tf.train.AdamOptimizer(learning_rate).minimize(self.loss, self.global_steps) 的区别","link":"/2018/06/01/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%974-textRCNN/"},{"title":"文本分类系列5-Hierarchical Attention Networks","text":"Hierarchical Attention Networks for Document Classification paper reading主要原理： the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since **documents have a hierarchical structure (words form sentences, sentences form a document)**, we likewise construct a document representation by first building representations of sentences and then aggregating those into a document representation. Second, it is observed that different words and sentences in a documents are differentially informative. 对于一个document含有这样的层次结构，document由sentences组成，sentence由words组成。 the importance of words and sentences are highly context dependent, i.e. the same word or sentence may be differentially important in different context (x3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015) — one at the word level and one at the sentence level — that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document. words和sentences都是高度上下文依赖的，同一个词或sentence在不同的上下文中，其表现的重要性会有差别。因此，这篇论文中使用了两个attention机制，来表示结合了上下文信息的词或句子的重要程度。（这里结合的上下文的词或句子，就是经过RNN处理后的隐藏状态）。 Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014) attention不仅有好的效果，而且能够可视化的看见哪些词或句子对哪一类document的分类影响大。 本文的创新点在于，考虑了ducument中sentence这一层次结构，因为对于一个document的分类，可能前面几句话都是废话，而最后一句话来了一个转折，对document的分类起决定性作用。而之前的研究，只考虑了document中的词。 Model Architecture GRU-based sequence encoderreset gate: controls how much the past state contributes to the candidate state. $$r_t=\\sigma(W_rx_t+U_rh_{t-1}+b_r)$$ candidate state: $$\\tilde h_t=tanh(W_hx_t+r_t\\circ (U_hh_{t-1})+b_h)$$ update gate: decides how much past information is kept and how much new information is added. $$z_t=\\sigma(W_zx_t+U_zh_{t-1}+b_z)$$ new state: a linear interpolation between the previous state $h_{t−1}$ and the current new state $\\tilde h_t$ computed with new sequence information. $$h_t=(1-z_t)\\circ h_{t-1}+z_t\\circ \\tilde h_t$$ Hierarchical AttentionWord Encoder$$x_{it}=W_ew_{it}, t\\in [1, T]$$ $$\\overrightarrow h_{it}=\\overrightarrow {GRU}(x_{it}),t\\in[1,T]$$ $$\\overleftarrow h_{it}=\\overleftarrow {GRU}(x_{it}),t\\in [T,1]$$ $$h_{it} = [\\overrightarrow h_{it},\\overleftarrow h_{it}]$$ i means the $i^{th}$ sentence in the document, and t means the $t^{th}$ word in the sentence. Word AttentionNot all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Attention机制说到底就是给予sentence中每个结合了上下文信息的词一个权重。关键在于这个权重怎么确定？ $$u_{it}=tanh(W_wh_{it}+b_w)$$ $$\\alpha_{it}=\\dfrac{exp(u_{it}^Tu_w)}{\\sum_t^Texp(u_{it}^Tu_w)}$$ $$s_i=\\sum_t^T\\alpha_{it}h_{it}$$ 这里首先是将 $h_{it}$ 通过一个全连接层得到 hidden representation $u_{it}$,然后计算 $u_{it}$ 与 $u_w$ 的相似性。并通过softmax归一化得到每个词与 $u_w$ 相似的概率。越相似的话，这个词所占比重越大，对整个sentence的向量表示影响越大。 那么关键是这个 $u_w$ 怎么表示？ The context vector $u_w$ can be seen as a high level representation of a fixed query “what is the informative word” over the words like that used in memory networks (Sukhbaatar et al., 2015, End-to-end memory networks.; Kumar et al., 2015, Ask me anything: Dynamic memory networks for natural language processing.). The word context vector $u_w$ is randomly initialized and jointly learned during the training process. Sentence Encoder$$\\overrightarrow h_{i}=\\overrightarrow {GRU}(s_{i}),t\\in[1,L]$$ $$\\overleftarrow h_{i}=\\overleftarrow {GRU}(s_{i}),t\\in [L,1]$$ $$H_i=[\\overrightarrow h_{i}, \\overleftarrow h_{i}]$$ hi summarizes the neighbor sentences around sentence i but still focus on sentence i. Sentence Attention$$u_i=tanh(W_sH_i+b_s)$$ $$\\alpha_i=\\dfrac{exp(u_i^Tu_s)}{\\sum_i^Lexp(u_i^Tu_s)}$$ $$v = \\sum_i^L\\alpha_ih_i$$ 同样的 $u_s$ 表示： a sentence level context vector $u_s$ Document ClassificationThe document vector v is a high level representation of the document and can be used as features for document classification: $$p=softmax(W_cv+b_c)$$ 代码实现需要注意的问题 如果使用tensorboard可视化 变量范围的问题 Context dependent attention weightsVisualization of attention","link":"/2018/06/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks/"},{"title":"代码实现高斯混合模型","text":"sklearn源码阅读，用em算法计算高斯混合模型GMM 代码实现高斯混合模型参考这篇博客Regularized Gaussian Covariance Estimation非常值得一读，同事这篇博客很深入的讲了协方差怎么求的问题，在前文中我也有提到～但我解释的很low。。 代码直接就看sklearn里面的源码吧～网上很多不靠谱。。。 github源码 类初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445class GaussianMixture(BaseMixture): &quot;&quot;&quot; Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a Gaussian mixture distribution. 对混合的高斯分布进行参数估计～ &quot;&quot;&quot; def __init__(self, n_components=1, covariance_type='full', tol=1e-3, reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10): super(GaussianMixture, self).__init__( n_components=n_components, tol=tol, reg_covar=reg_covar, max_iter=max_iter, n_init=n_init, init_params=init_params, random_state=random_state, warm_start=warm_start, verbose=verbose, verbose_interval=verbose_interval) # 主要是针对3个要学习的参数的初始化 self.covariance_type = covariance_type # 协方差矩阵形式 self.weights_init = weights_init # 多项式分布，每一类的概率 self.means_init = means_init # 均值 (n_components, n_features) self.precisions_init = precisions_init # 协方差 先看初始化构造函数，参数是真的多。。。 n_components=1: The number of mixture components.表示混合类别的个数，也就是混合高斯分布的个数 covariance_type=’full’: 协方差矩阵的类型。{‘full’, ‘tied’, ‘diag’, ‘spherical’} 分别对应完全协方差矩阵（元素都不为零），相同的完全协方差矩阵（HMM会用到），对角协方差矩阵（非对角为零，对角不为零），球面协方差矩阵（非对角为零，对角完全相同，球面特性），默认‘full’ 完全协方差矩阵 tol=1e-3 收敛阈值，EM iterations will stop when the lower bound average gain is below this threshold.也就是当下界的平均增益小于阈值时，em迭代就停止。这里的下界指的是公式 （3）中的下界凸函数。我们知道em算法分两步，e step是期望，也就是不等式相等，m setp是最大化， 也就是下界凸函数最大化。这里的阈值平均增益就是指凸函数的最大化过程中的增益。 reg_covar=1e-6： Non-negative regularization added to the diagonal of covariance.Allows to assure that the covariance matrices are all positive. 非负正则化添加到协方差矩阵对角线上，保证协方差矩阵都是正定的。 max_iter=100: em算法的最大迭代次数 n_init: int, defaults to 1.初始化的次数 init_params: {‘kmeans’, ‘random’}, defaults to ‘kmeans’. The method used to initialize the weights, the means and the precisionsself. Must be one of:: - 'kmeans' : responsibilities are initialized using kmeans. - 'random' : responsibilities are initialized randomly. - 这里对应的初始化，是指的隐藏变量z的分类所占比例，也就是weight_init，kmeans表示“hard”guess， {0, 1} or {1, . . . , k}) random应该就是”soft”guess吧。 weights_init : shape (n_components, ), optional The user-provided initial weights, defaults to None. If it None, weights are initialized using the init_params method. 先验权重初始化，对应的就是隐藏变量有n_components类，而每一类所占的比例，也就是多项式分布的初始化～对应$\\phi_i$ means_init : array-like, shape (n_components, n_features), optional. The user-provided initial means, defaults to None, If it None, means are initialized using the init_params method.混合高斯分布的均值初始化，注意shape=(n_components, n_features),有n_components这样的多维高斯分布，每个高斯分布有n_features维度 precisions_init : The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the ‘init_params’ method.The shape depends on ‘covariance_type’:: (n_components,) if ‘spherical’, (n_features, n_features) if ‘tied’, (n_components, n_features) if ‘diag’, (n_components, n_features, n_features) if ‘full’ 用来初始化高斯分布中的协方差矩阵，协方差矩阵代表的是n_features维向量中每一维特征与其他维度特征的关系，对于一个高斯分布来说是n_featuresn_features，n_components个混合也就是’full’。其中要学习的参数个数是(n_features+1) n_features/2.具体关于协方差矩阵参考前面那篇博客 random_state : int, RandomState instance or None, optional (default=None) 随机数生成器 warm_start : bool, default to False.If ‘warm_start’ is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. 若为True，则fit（）调用会以上一次fit（）的结果作为初始化参数，适合相同问题多次fit的情况，能加速收敛，默认为False。 verbose : int, default to 0. Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step. 使能迭代信息显示，默认为0，可以为1或者大于1（显示的信息不同） verbose_interval: 与13挂钩，若使能迭代信息显示，设置多少次迭代后显示信息，默认10次。 E step就是求$w_j^i$ 12345678910111213141516171819202122232425def _e_step(self, X): &quot;&quot;&quot;E step. Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- log_prob_norm : log_responsibility : 后验概率，样本i是j类的概率w_j^{i} &quot;&quot;&quot; log_prob_norm, log_resp = self._estimate_log_prob_resp(X) return np.mean(log_prob_norm), log_resp 那么如何求$w_j^{i}$呢？ $$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)=\\dfrac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}{\\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\\mu,\\Sigma)p(z^{(i)}=l;\\phi)}$$ 要注意的是，为了计算方便，有以下几点： 因为分子分母中设计到正态分布，即指数形式，故先计算其log形式。然后带入到M step中取回指数形式即可。 对于协方差矩阵，如果n_features很大的话，计算其逆矩阵和行列式就很复杂，因此可以先计算其precision矩阵，然后进行cholesky分解，以便优化计算。 先计算分子对数形式，两个对数相加：$$logp(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)+logp(z^{(i)}=j;\\phi)$$ 1234567891011121314151617181920212223# 计算P(x|z)p(z)的对数形式def _estimate_weighted_log_prob(self, X): &quot;&quot;&quot;Estimate the weighted log-probabilities, log P(X | Z) + log weights. Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- weighted_log_prob : array, shape (n_samples, n_component) &quot;&quot;&quot; return self._estimate_log_prob(X) + self._estimate_log_weights() 其中前者是高斯分布概率的对数,根据均值，协方差矩阵的cholesky分解可求得。 1234567def _estimate_log_prob(self, X): return _estimate_log_gaussian_prob( X, self.means_, self.precisions_cholesky_, self.covariance_type) 这个函数，_estimate_log_gaussian_prob根据高斯分布的参数计算概率，涉及到协方差矩阵，要优化计算，很复杂，放在最后说。先把整个流程走完。 后者是每一类高斯分布所占的权重，也就是$\\phi_j$ 1234567def _estimate_log_weights(self): # 刚开始是初始值，后面随着m step而更新 return np.log(self.weights_) 再计算$w_j^i$1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def _estimate_log_prob_resp(self, X): &quot;&quot;&quot;Estimate log probabilities and responsibilities for each sample. Compute the log probabilities, weighted log probabilities per component and responsibilities for each sample in X with respect to the current state of the model. Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- log_prob_norm : array, shape (n_samples,) log p(X) log_responsibilities : array, shape (n_samples, n_components) logarithm of the responsibilities &quot;&quot;&quot; # 计算分子log P(X | Z) + log weights. weighted_log_prob = self._estimate_weighted_log_prob(X) # 计算分母log P(x) log_prob_norm = logsumexp(weighted_log_prob, axis=1) with np.errstate(under='ignore'): # 忽略下溢，计算log(w_J^j)，也就是两个对数相减 log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis] return log_prob_norm, log_resp M setp123456789101112131415161718192021222324252627282930313233343536373839def _m_step(self, X, log_resp): &quot;&quot;&quot;M step. Parameters ---------- X : array-like, shape (n_samples, n_features) log_resp : array-like, shape (n_samples, n_components) Logarithm of the posterior probabilities (or responsibilities) of the point of each sample in X. &quot;&quot;&quot; n_samples, _ = X.shape # 根据E step中求得的log_resp,更新权重，均值和协方差 self.weights_, self.means_, self.covariances_ = ( _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar, self.covariance_type)) # 更新类别权重phi_j self.weights_ /= n_samples # 更新协方差矩阵的精度矩阵 self.precisions_cholesky_ = _compute_precision_cholesky( self.covariances_, self.covariance_type) 具体怎么求，就是根据前面推导的公式了。根据前面的公式分别求对应的估计参数： $$\\Sigma_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T}{\\sum_{i=1}^mw_j^{(i)}}$$ 协方差矩阵：以‘full’为例1234567891011121314151617181920212223242526272829def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar): &quot;&quot;&quot;Estimate the full covariance matrices. resp:表示E step中猜测的w_j^{i} &quot;&quot;&quot; n_components, n_features = means.shape # 协方差矩阵 covariances = np.empty((n_components, n_features, n_features)) for k in range(n_components): diff = X - means[k] covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k] # 正则化，flat表示展开成一维，然后每隔n_features取一个元素，单个协方差矩阵shape是 # [n_features, n_features],所以就是对角线元素加上reg_covar covariances[k].flat[::n_features + 1] += reg_covar return covariances 然后是正态分布的参数估计$u_j, \\phi_j$$$\\phi_j:=\\frac{1}{m}\\sum_{i=1}^mw_j^{(i)}$$ $$\\mu_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\\sum_{i=1}^mw_j^{(i)}}$$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type): &quot;&quot;&quot;Estimate the Gaussian distribution parameters. Parameters ---------- X : 样本数据 (n_samples, n_features) resp : Estep猜测的样本i是j类的概率w_i^{j}, shape (n_samples, n_components) reg_covar : 对角线正则化项 covariance_type : {'full', 'tied', 'diag', 'spherical'} Returns ------- nk : 当前类别下的样本和 (n_components,) 也就是\\sum_i^{m}(w_j^{i}) means : k个n维正态分布的均值, shape (n_components, n_features) covariances : 协方差矩阵 &quot;&quot;&quot; # 因为要做分母，避免为0 nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps means = np.dot(resp.T, X) / nk[:, np.newaxis] covariances = {&quot;full&quot;: _estimate_gaussian_covariances_full, &quot;tied&quot;: _estimate_gaussian_covariances_tied, &quot;diag&quot;: _estimate_gaussian_covariances_diag, &quot;spherical&quot;: _estimate_gaussian_covariances_spherical }[covariance_type](resp, X, nk, means, reg_covar) return nk, means, covariances 迭代收敛，重复以上过程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161def fit(self, X, y=None): &quot;&quot;&quot;Estimate model parameters with the EM algorithm. The method fit the model `n_init` times and set the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for `max_iter` times until the change of likelihood or lower bound is less than `tol`, otherwise, a `ConvergenceWarning` is raised. 迭代终止条件： 迭代次数ｎ_init，极大似然函数或下界函数的增益小于`tol` Parameters ---------- X : array-like, shape (n_samples, n_features) Returns ------- self &quot;&quot;&quot; X = _check_X(X, self.n_components) self._check_initial_parameters(X) # if we enable warm_start, we will have a unique initialisation do_init = not(self.warm_start and hasattr(self, 'converged_')) n_init = self.n_init if do_init else 1 max_lower_bound = -np.infty self.converged_ = False random_state = check_random_state(self.random_state) n_samples, _ = X.shape # 初始化次数 for init in range(n_init): self._print_verbose_msg_init_beg(init) # 先初始化参数 if do_init: self._initialize_parameters(X, random_state) self.lower_bound_ = -np.infty # 迭代次数 for n_iter in range(self.max_iter): prev_lower_bound = self.lower_bound_ # E step求出后验概率w_j^i或是Q分布 log_prob_norm, log_resp = self._e_step(X) # Ｍ step更新参数 self._m_step(X, log_resp) # 求出下界函数的最大值 self.lower_bound_ = self._compute_lower_bound( log_resp, log_prob_norm) # 下界函数的增益 change = self.lower_bound_ - prev_lower_bound self._print_verbose_msg_iter_end(n_iter, change) # 比较下界函数增益与ｔｏｌ if abs(change) &lt; self.tol: self.converged_ = True break self._print_verbose_msg_init_end(self.lower_bound_) # if self.lower_bound_ &gt; max_lower_bound: max_lower_bound = self.lower_bound_ best_params = self._get_parameters() best_n_iter = n_iter if not self.converged_: warnings.warn('Initialization %d did not converge. ' 'Try different init parameters, ' 'or increase max_iter, tol ' 'or check for degenerate data.' % (init + 1), ConvergenceWarning) self._set_parameters(best_params) self.n_iter_ = best_n_iter return self E step中p(x|z=j)根据高斯分布的参数计算概率,优化的计算方法。 先计算协方差矩阵的precision矩阵，并进行cholesky分解 Precision matrix 协方差矩阵的逆矩阵：https://www.statlect.com/glossary/precision-matrix 然后根据精度矩阵的cholesky分解形式,这样可以优化矩阵运算 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def _compute_precision_cholesky(covariances, covariance_type): &quot;&quot;&quot;Compute the Cholesky decomposition of the precisions. Parameters ---------- covariances : array-like The covariance matrix of the current components. The shape depends of the covariance_type. covariance_type : {'full', 'tied', 'diag', 'spherical'} The type of precision matrices. Returns ------- precisions_cholesky : array-like The cholesky decomposition of sample precisions of the current components. The shape depends of the covariance_type. &quot;&quot;&quot; if covariance_type in 'full': n_components, n_features, _ = covariances.shape precisions_chol = np.empty((n_components, n_features, n_features)) for k, covariance in enumerate(covariances): try: cov_chol = linalg.cholesky(covariance, lower=True) except linalg.LinAlgError: raise ValueError(estimate_precision_error_message) precisions_chol[k] = linalg.solve_triangular(cov_chol, np.eye(n_features), lower=True).T elif covariance_type == 'tied': _, n_features = covariances.shape try: cov_chol = linalg.cholesky(covariances, lower=True) except linalg.LinAlgError: raise ValueError(estimate_precision_error_message) precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features), lower=True).T else: if np.any(np.less_equal(covariances, 0.0)): raise ValueError(estimate_precision_error_message) precisions_chol = 1. / np.sqrt(covariances) return precisions_chol 计算cholesky分解的行列式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Gaussian mixture probability estimators# 根据cholesky分解计算行列式的logdef _compute_log_det_cholesky(matrix_chol, covariance_type, n_features): &quot;&quot;&quot;Compute the log-det of the cholesky decomposition of matrices. Parameters ---------- matrix_chol : 协方差矩阵的cholesky分解 covariance_type : {'full', 'tied', 'diag', 'spherical'} n_features : int Number of features. Returns ------- log_det_precision_chol : array-like, shape (n_components,) The determinant of the precision matrix for each component. &quot;&quot;&quot; if covariance_type == 'full': n_components, _, _ = matrix_chol.shape log_det_chol = (np.sum(np.log( matrix_chol.reshape( n_components, -1)[:, ::n_features + 1]), 1)) elif covariance_type == 'tied': log_det_chol = (np.sum(np.log(np.diag(matrix_chol)))) elif covariance_type == 'diag': log_det_chol = (np.sum(np.log(matrix_chol), axis=1)) else: log_det_chol = n_features * (np.log(matrix_chol)) return log_det_chol 计算分子: $logp(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)$123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type): &quot;&quot;&quot;Estimate the log Gaussian probability. Parameters ---------- X : 样本数据(n_samples, n_features) means : k个n维正态分布的均值(n_components, n_features) precisions_chol : 精度矩阵的Cholesky分解 covariance_type : {'full', 'tied', 'diag', 'spherical'} Returns ------- log_prob : (n_samples, n_components) &quot;&quot;&quot; n_samples, n_features = X.shape n_components, _ = means.shape # det(precision_chol) is half of det(precision) log_det = _compute_log_det_cholesky( precisions_chol, covariance_type, n_features) if covariance_type == 'full': log_prob = np.empty((n_samples, n_components)) for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)): y = np.dot(X, prec_chol) - np.dot(mu, prec_chol) log_prob[:, k] = np.sum(np.square(y), axis=1) elif covariance_type == 'tied': pass elif covariance_type == 'diag': pass elif covariance_type == 'spherical': pass return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det sklearn中实例Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid. We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data. On the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only the first two dimensions are shown here, and thus some points are separated in other dimensions. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231import matplotlib as mplimport matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasetsfrom sklearn.mixture import GaussianMixturefrom sklearn.model_selection import StratifiedKFoldprint(__doc__)colors = ['navy', 'turquoise', 'darkorange']iris = datasets.load_iris() # data, target# Break up the dataset into non-overlapping training (75%) and testing# (25%) sets.skf = StratifiedKFold(n_splits=4)# Only take the first fold.train_index, test_index = next(iter(skf.split(iris.data, iris.target)))X_train = iris.data[train_index] # (111, 4)y_train = iris.target[train_index] # (111,)X_test = iris.data[test_index] # (39, 4)y_test = iris.target[test_index] # (39,)n_classes = len(np.unique(y_train))# Try GMMs using different types of covariances. 根据协方差矩阵，有4中不同的GMM模型estimators = dict((cov_type, GaussianMixture(n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0)) for cov_type in ['spherical', 'diag', 'tied', 'full'])n_estimators = len(estimators) # 4# figsize表示图像的尺寸（width, height in inches）plt.figure(figsize=(3 * 5 // 2, 6))# 图像之间的间距plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05, left=.01, right=.99)# 椭圆def make_ellipses(gmm, ax): for n, color in enumerate(colors): if gmm.covariance_type == 'full': covariances = gmm.covariances_[n][:2, :2] elif gmm.covariance_type == 'tied': covariances = gmm.covariances_[:2, :2] elif gmm.covariance_type == 'diag': covariances = np.diag(gmm.covariances_[n][:2]) elif gmm.covariance_type == 'spherical': covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n] # 参数估计得到的混合二维高斯分布，将其用椭圆表示出来～ v, w = np.linalg.eigh(covariances) # 返回协方差矩阵的特征值和列向量由特征矩阵构成的矩阵 u = w[0] / np.linalg.norm(w[0]) # order=None 表示 Frobenius norm，2-norm angle = np.arctan2(u[1], u[0]) angle = 180 * angle / np.pi # 转换为角度 v = 2. * np.sqrt(2.) * np.sqrt(v) ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1], 180 + angle, color=color) ell.set_clip_box(ax.bbox) ell.set_alpha(0.5) ax.add_artist(ell) # 增加文字for index, (name, estimator) in enumerate(estimators.items()): # Since we have class labels for the training data, we can # initialize the GMM parameters in a supervised manner. # 这里因为有类标签，所以直接用真实均值来初始化GMM的均值。在无标签或者标签较少的情况下，则需要随机初始化 estimator.means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)]) # Train the other parameters using the EM algorithm. # 用em算法来估计其他参数 estimator.fit(X_train) # 画椭圆 h = plt.subplot(2, n_estimators // 2, index + 1) make_ellipses(estimator, h) for n, color in enumerate(colors): data = iris.data[iris.target == n] # 不同的种类数据用不同的点表示 plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]) # 用xx表示测试集 for n, color in enumerate(colors): data = X_test[y_test == n] plt.scatter(data[:, 0], data[:, 1], marker='x', color=color) # 训练集的准确率 y_train_pred = estimator.predict(X_train) # 预测是选取概率最大的一类 # 当无标签时是没有办法计算准确率的。但是这里有标签， # y_train_pred返回的是概率最大的索引， y_train的元素是[0,1,2,3]中的一个 # 因此可以求得准确率 print(y_train_pred[:5]) print(y_train[:5]) train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100 plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy, transform=h.transAxes) y_test_pred = estimator.predict(X_test) test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100 plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy, transform=h.transAxes) plt.xticks(()) plt.yticks(()) plt.title(name)plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))plt.show()","link":"/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"title":"机器学习-不均衡数据问题","text":"对于机器学习中不均衡数据的问题是很常见的，之前在三星实习也遇到过，但当时对指标性能要求并不高，差不多也就行了。。但这次参加 kaggle 比赛，0.1% 就有很大差距了，所以还是要好好搞搞 imbalanced data problem. 很多时候，数据不均衡不仅仅数据收集和整理的问题，而是数据本身预期的就是这样，比如表征欺诈性交易（characterize fraudulent transactions），大多数交易都是不具有欺诈性的,只有极少数的 具有欺诈性的（Fraud）, 或者 kaggle 比赛中的 insincere-questions, 大多数问题都是正常的，只有极少数的 insincere-questions. More data显然，这是最直接的。在实际中，也是最可行的。但打比赛时，数据是给定的，就需要用到 resampling. Metrics改变评价指标来选择更优的模型。 accurayc 显然这个对不均衡数据是不合适的 Precision: (tp/(tp+fp)) Recall: (tp/(tp+fn)) F1 Score (or F-score): precision 和 recall 的一种权衡 Kappa: Cohen’s kappa ROC 曲线：这些在之前的笔记中都有介绍 机器学习-常用指标总结 PRAUC 损失函数？ ai challenger 比赛中看到的 Resampling 过采样 over-sampling 欠采样 under-sampling 实践中可以两种都尝试，并且能做集成增强。 一些采样策略： 数据量很大（上万或者十万？），建议欠采样，数据量较小，建议过采样 可以尝试随机采样，也可以尝试分层采样（划分好） 尝试不同的比例，并不一定要最后是 1:1 Generate Synthetic SamplesSMOTE(Synthetic Minority Over-sampling Technique): 不是简单的 copy, 而是选取两个或更多的相似的样本（根据距离），然后随机扰动一个样本的属性（某一维特征吧），扰动值在它与相邻样本的差异之间。 paper: SMOTE: Synthetic Minority Over-sampling Technique python tool: UnbalancedDataset Try Penalized Models在训练过程中，给类别较少的一类增加惩罚项，通常是正则化．这有利于模型能注重 minority class. 对类和权重的惩罚对不同的算法不太一样，所有有专门的版本的算法：penalized-SVM 和 penalized-LDA. 也有通用的惩罚模型，适用于不同的分类器 CostSensitiveClassifier 惩罚模型提供了另外一种方法来 “balance” 模型，但设置惩罚矩阵是很复杂的，需要很多尝试。 Try a Different Perspective对于之前说过的欺诈性检测和kaggle insincere 问题的发现，从另外一个角度看，也能看做是异常检测（anomaly detection）和 变异检测（change detection）. 类似于 open set recognition 或 out-of-distribution 问题，样本数极少的那个类别 minior class 可以看做是 outliers class. Anomaly detection is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by it’s sequence of system calls. The events are rare and when compared to normal operation. Change detection is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.","link":"/2018/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%8D%E5%9D%87%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98/"},{"title":"机器学习-中文文本预处理","text":"中文文本挖掘预处理特点参考：https://www.cnblogs.com/pinard/p/6744056.html 首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在文本挖掘的分词原理中，我们已经讲到了中文的分词原理，这里就不多说。 第二，中文的编码不是utf8，而是unicode。这样会导致在分词的时候，和英文相比，我们要处理编码的问题。 这两点构成了中文分词相比英文分词的一些不同点，后面我们也会重点讲述这部分的处理。当然，英文分词也有自己的烦恼，这个我们在以后再讲。了解了中文预处理的一些特点后，我们就言归正传，通过实践总结下中文文本挖掘预处理流程。 数据集收集在文本挖掘之前，我们需要得到文本数据，文本数据的获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。 对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“机器学习”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。 对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用beautifulsoup。但是我们我们需要某些特殊的语料数据，比如上面提到的“机器学习”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成。这个我一般使用ache。 ache允许我们用关键字或者一个分类算法来过滤出我们需要的主题语料，比较强大。 除去数据中非文本部分这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用beautifulsoup来去除。去除掉这些非文本的内容后，我们就可以进行真正的文本预处理了。 处理中文编码问题由于Python2不支持unicode的处理，因此我们使用Python2做中文文本预处理时需要遵循的原则是，存储数据都用utf8，读出来进行中文相关处理时，使用GBK之类的中文编码，在下面一节的分词时，我们再用例子说明这个问题。 中文分词常用的中文分词软件有很多，个人比较推荐结巴分词。安装也很简单，比如基于Python的，用”pip install jieba”就可以完成。下面我们就用例子来看看如何中文分词。 首先我们准备了两段文本，这两段文本在两个文件中。两段文本的内容分别是nlp_test0.txt和nlp_test2.txt： 1234567891011import jiebawith open(&quot;./nlp_test1.txt&quot;) as f: document = f.read() # 如果是python2，则需要用 decode(&quot;GBK&quot;) document_cut = jieba.cut(document)document_cut &lt;generator object Tokenizer.cut at 0x7f6a84cf09e8&gt; 12345result = &quot; &quot;.join(document_cut)result Building prefix dict from the default dictionary ... Loading model from cache /tmp/jieba.cache Loading model cost 0.438 seconds. Prefix dict has been built succesfully. ' 沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \\n \\n 沙 瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王 大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王 大路 就 在 京州帝 豪园 买 了 三套 别墅 ， 可是 李达 康和易 学习 都 不要 ， 这些 房子 都 在 王 大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。' 12345with open(&quot;./nlp_test2.txt&quot;, &quot;w&quot;) as f2: f2.write(result) 可以发现对于一些人名和地名，jieba处理的不好，不过我们可以帮jieba加入词汇如下： 123456789jieba.suggest_freq('沙瑞金', True)jieba.suggest_freq('易学习', True)jieba.suggest_freq('王大路', True)jieba.suggest_freq('京州', True) 3 所以在很多 NLP 任务中先做命令实体识别的意义就在这里对吧? 123456789101112131415with open(&quot;./nlp_test1.txt&quot;, &quot;r&quot;) as f1: text = f1.read() text_cut = jieba.cut(text) # list result = &quot; &quot;.join(text_cut) print(result) with open(&quot;./nlp_test2.txt&quot;, &quot;w&quot;) as f2: f2.write(result) 沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。 引入停用词在上面我们解析的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个，下载地址在这。当然也有其他版本的停用词表，不过这个1208词版是我常用的。 在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。 123456789stpword_path = &quot;stop_words.txt&quot;with open(stpword_path, encoding=&quot;gbk&quot;) as f: stpword_content = f.read() stpword_list = stpword_content.splitlines() 123print(stpword_list[:100]) [',', '?', '、', '。', '“', '”', '《', '》', '！', '，', '：', '；', '？', '人民', '末##末', '啊', '阿', '哎', '哎呀', '哎哟', '唉', '俺', '俺们', '按', '按照', '吧', '吧哒', '把', '罢了', '被', '本', '本着', '比', '比方', '比如', '鄙人', '彼', '彼此', '边', '别', '别的', '别说', '并', '并且', '不比', '不成', '不单', '不但', '不独', '不管', '不光', '不过', '不仅', '不拘', '不论', '不怕', '不然', '不如', '不特', '不惟', '不问', '不只', '朝', '朝着', '趁', '趁着', '乘', '冲', '除', '除此之外', '除非', '除了', '此', '此间', '此外', '从', '从而', '打', '待', '但', '但是', '当', '当着', '到', '得', '的', '的话', '等', '等等', '地', '第', '叮咚', '对', '对于', '多', '多少', '而', '而况', '而且', '而是'] 特征处理现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在文本挖掘预处理之向量化与Hash Trick中，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在文本挖掘预处理之TF-IDF中，我们也讲到了TF-IDF特征处理的方法。这里我们就用scikit-learn的TfidfVectorizer类来进行TF-IDF特征处理。 向量化与 Hash Trick词袋模型在讲向量化与Hash Trick之前，我们先说说词袋模型(Bag of Words,简称BoW)。词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。 词袋模型首先会进行分词，在分词之后，通过统计每个词在文本中出现的次数，我们就可以得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就是我们常说的向量化。向量化完毕后一般也会使用TF-IDF进行特征的权重修正，再将特征进行标准化。 再进行一些其他的特征工程后，就可以将数据带入机器学习算法进行分类聚类了。 总结下词袋模型的三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。 词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。 词袋模型之向量化在词袋模型的统计词频这一步，我们会得到该文本中所有词的词频，有了词频，我们就可以用词向量表示这个文本。这里我们举一个例子，例子直接用scikit-learn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化，代码如下： 12345from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer() 1234567891011corpus=[&quot;I come to China to travel&quot;, &quot;This is a car polupar in China&quot;, &quot;I love tea and Apple &quot;, &quot;The work is to write some papers in science&quot;]print(vectorizer.fit_transform(corpus)) (0, 16) 1 (0, 3) 1 (0, 15) 2 (0, 4) 1 (1, 5) 1 (1, 9) 1 (1, 2) 1 (1, 6) 1 (1, 14) 1 (1, 3) 1 (2, 1) 1 (2, 0) 1 (2, 12) 1 (2, 7) 1 (3, 10) 1 (3, 8) 1 (3, 11) 1 (3, 18) 1 (3, 17) 1 (3, 13) 1 (3, 5) 1 (3, 6) 1 (3, 15) 1 可以看出4个文本的词频已经统计出，在输出中，左边的括号中的第一个数字是文本的序号，第2个数字是词的序号，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。 我们可以进一步看看每个文本的词向量特征和各个特征代表的词，代码如下： 123print(vectorizer.fit_transform(corpus).toarray()) [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] 123print(vectorizer.get_feature_names()) ['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write'] 也就是先统计整个文本corpus, 去掉停用词，剩下的词就是向量的维度。然后统计每一行文字出现的词频，得到相应的向量。显然词表是按照字母顺序排序的。 可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词”I”在英文中是停用词，不参加词频的统计。 由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。 这里有个疑问？ 向量化之后的维度是根据自己的数据集来定，为什么不就是词表大小呢。这里是根据自己的数据集来的，但我们对测试集分类时，会出现 UNK 词吧，但是这个词其实在词表中是有的。那么在训练集中如果加上这个维度，其实也没有太大意义，因为在训练集中这个维度上所有的值都为0. 将文本做了词频统计后，我们一般会通过TF-IDF进行词特征值修订，这部分我们后面再讲。 向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，达到100万+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们怎么办呢？第一反应是我们要进行特征的降维，说的没错！而Hash Trick就是非常常用的文本特征降维方法。 Hash Trick在大规模的文本处理中，由于特征的维度对应分词词汇表的大小，所以维度可能非常恐怖，此时需要进行降维，不能直接用我们上一节的向量化方法。而最常用的文本降维方法是Hash Trick。说到Hash，一点也不神秘，学过数据结构的同学都知道。这里的Hash意义也类似。 在Hash Trick里，我们会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于我们的词汇表的特征维度，因此可以看成是降维。具体的方法是，对应任意一个特征名，我们会用Hash函数找到对应哈希表的位置，然后将该特征名对应的词频统计值累加到该哈希表位置。如果用数学语言表示,假如哈希函数h使第i个特征哈希到位置j,即 $h(i)=j$,则第i个原始特征的词频数值 $\\phi(i)$ 将累加到哈希后的第j个特征的词频数值 $\\hat \\phi(i)$上，即： $$\\hat \\phi(i)=\\sum_{i\\in J;h(i)=j}\\phi(i)$$ 其中 J 是原始特征的维度。 但是上面的方法有一个问题，有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这个问题，出现了hash Trick的变种signed hash trick,此时除了哈希函数h,我们多了一个一个哈希函数： $$\\xi:N\\rightarrow \\pm1$$ 此时我们有 $$\\hat \\phi(j)=\\sum_{i\\in J;h(i)=j}\\phi(i)\\xi(i)$$ 这样做的好处是，哈希后的特征仍然是一个无偏的估计，不会导致某些哈希位置的值过大。 当然，大家会有疑惑，这种方法来处理特征，哈希后的特征是否能够很好的代表哈希前的特征呢？从实际应用中说，由于文本特征的高稀疏性，这么做是可行的。如果大家对理论上为何这种方法有效，建议参考论文：Feature hashing for large scale multitask learning.这里就不多说了。 在scikit-learn的HashingVectorizer类中，实现了基于signed hash trick的算法，这里我们就用HashingVectorizer来实践一下Hash Trick，为了简单，我们使用上面的19维词汇表，并哈希降维到6维。当然在实际应用中，19维的数据根本不需要Hash Trick，这里只是做一个演示，代码如下： 1234567from sklearn.feature_extraction.text import HashingVectorizervectorizer2 = HashingVectorizer(n_features=6, norm=None)print(vectorizer2.fit_transform(corpus)) (0, 1) 2.0 (0, 2) -1.0 (0, 4) 1.0 (0, 5) -1.0 (1, 0) 1.0 (1, 1) 1.0 (1, 2) -1.0 (1, 5) -1.0 (2, 0) 2.0 (2, 5) -2.0 (3, 0) 0.0 (3, 1) 4.0 (3, 2) -1.0 (3, 3) 1.0 (3, 5) -1.0 大家可以看到结果里面有负数，这是因为我们的哈希函数ξ可以哈希到1或者-1导致的。 和PCA类似，Hash Trick降维后的特征我们已经不知道它代表的特征名字和意义。此时我们不能像上一节向量化时候可以知道每一列的意义，所以Hash Trick的解释性不强。 向量化与 Hash Track 小结这里我们对向量化与它的特例Hash Trick做一个总结。在特征预处理的时候，我们什么时候用一般意义的向量化，什么时候用Hash Trick呢？标准也很简单。 一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。 而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。因此，实际工作中我使用的都是特征向量化。 向量化与Hash Trick就介绍到这里，下一篇我们讨论TF-IDF。 文本向量化特征的不足在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，比如在文本挖掘预处理之向量化与Hash Trick这篇文章中，我们将下面4个短文本做了词频统计： 123456789corpus=[&quot;I come to China to travel&quot;, &quot;This is a car polupar in China&quot;, &quot;I love tea and Apple &quot;, &quot;The work is to write some papers in science&quot;] 不考虑停用词，处理后得到的词向量如下： 123456789[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] 如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现”come”,”China”和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的”China”和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。 TF-IDF概述TF-IDF是Term Frequency - Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。 前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们讲到几乎所有文本都会出现的”to”其词频虽然高，但是重要性却应该比词频低的”China”和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。 概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。 上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下： $$IDF(x)=\\dfrac{N}{N(x)}$$ 其中，N代表语料库中文本的总数，而 $N(x)$ 代表语料库中包含词x的文本总数。为什么IDF的基本公式应该是是上面这样的而不是像 $N/N(x)$ 这样的形式呢？这就涉及到信息论相关的一些知识了。感兴趣的朋友建议阅读吴军博士的《数学之美》第11章。 上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为： $$IDF(x)=log\\dfrac{N+1}{N(x)+1}+1$$ 有了IDF的定义，我们就可以计算某一个词的TF-IDF值了： $$\\text{TF-IDF(x)}=TF(x)*IDF(x)$$ 其中TF(x)指词x在当前文本中的词频。 用scikit-learn进行TF-IDF预处理在scikit-learn中，有两种方法进行TF-IDF的预处理。 第一种方法是在用CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理。第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理。 首先我们来看第一种方法，CountVectorizer+TfidfTransformer的组合，代码如下： 123456789101112131415161718192021222324252627from sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizercorpus = [&quot;I come to China to travel&quot;, &quot;This is a car polupar in China&quot;, &quot;I love tea and Apple &quot;, &quot;The work is to write some papers in science&quot;]vectorizer = CountVectorizer()transformer = TfidfTransformer()tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))print(tfidf) (0, 4) 0.4424621378947393 (0, 15) 0.697684463383976 (0, 3) 0.348842231691988 (0, 16) 0.4424621378947393 (1, 3) 0.3574550433419527 (1, 14) 0.45338639737285463 (1, 6) 0.3574550433419527 (1, 2) 0.45338639737285463 (1, 9) 0.45338639737285463 (1, 5) 0.3574550433419527 (2, 7) 0.5 (2, 12) 0.5 (2, 0) 0.5 (2, 1) 0.5 (3, 15) 0.2811316284405006 (3, 6) 0.2811316284405006 (3, 5) 0.2811316284405006 (3, 13) 0.3565798233381452 (3, 17) 0.3565798233381452 (3, 18) 0.3565798233381452 (3, 11) 0.3565798233381452 (3, 8) 0.3565798233381452 (3, 10) 0.3565798233381452 123456789from sklearn.feature_extraction.text import TfidfVectorizertfidf2 = TfidfVectorizer()re = tfidf2.fit_transform(corpus)print(re) (0, 4) 0.4424621378947393 (0, 15) 0.697684463383976 (0, 3) 0.348842231691988 (0, 16) 0.4424621378947393 (1, 3) 0.3574550433419527 (1, 14) 0.45338639737285463 (1, 6) 0.3574550433419527 (1, 2) 0.45338639737285463 (1, 9) 0.45338639737285463 (1, 5) 0.3574550433419527 (2, 7) 0.5 (2, 12) 0.5 (2, 0) 0.5 (2, 1) 0.5 (3, 15) 0.2811316284405006 (3, 6) 0.2811316284405006 (3, 5) 0.2811316284405006 (3, 13) 0.3565798233381452 (3, 17) 0.3565798233381452 (3, 18) 0.3565798233381452 (3, 11) 0.3565798233381452 (3, 8) 0.3565798233381452 (3, 10) 0.3565798233381452 输出的各个文本各个词的TF-IDF值和第一种的输出完全相同。大家可以自己去验证一下。 由于第二种方法比较的简洁，因此在实际应用中推荐使用，一步到位完成向量化，TF-IDF与标准化。 TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。 当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想 还的好好理解下 TF-IDF 是怎么实现的！ 建立分析模型有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。比如我们上面的两段文本，就可以是两个训练样本了。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而 主题模型 是自然语言处理比较特殊的一块，这个我们后面再单独讲。 中文文本挖掘预处理总结上面我们对中文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如我们涉及到词上下文关系的一些需求，此时不能使用词袋模型。而有时候我们对于特征的处理有自己的特殊需求，因此这个流程仅供自然语言处理入门者参考。","link":"/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"},{"title":"文本分类系列3-TextRNN","text":"paper reading尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。 RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，Recurrent Neural Network for Text Classification with Multi-Task Learning文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。 关于解决RNN无法并行化，计算效率低的问题Factorization tricks for LSTM networks We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is “matrix factorization by design” of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.","link":"/2018/05/31/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%973-TextRNN/"},{"title":"机器学习-常用指标总结","text":"参考链接 http://www.cnblogs.com/maybe2030/p/5375175.html#_label0 http://alexkong.net/2013/06/introduction-to-auc-and-roc/ 精确率 Precision, 召回率 Recall 和 F1 值 对于数据不均衡时，使用accuracy是不准确的。 举个栗子： a million tweet: 999,900条不是关于pie的，只有100条是关于pie的 对于一个stupid分类器，他认为所有的tweet都是跟pie无关的，那么它的准确率是99.99%！但这个分类器显然不是我们想要的，因而accuracy不是一个好的metric，当它目标是rare，或是complete unbalanced. 引入另外两个指标： 精度 precision: 是检索出来的条目（比如：文档、网页等）有多少是准确的。精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率。 召回 call： 召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率. 需要先确定一个正分类： 这里需要检索出来的是与 pie 无关的，也就是与 pie 无关的是正分类。那么精度就是分类器检测出来的正分类中 gold pos 所占的比例，那么 precision = 0/(0+0) 这里需要检索的是与 pie 无关的，检索出来的 true pos 占样本中 gold pos 的比例，那么 recall = 0/(100+0) = 0. 总结下来，precision 和 recall 都是以 true pos 作为分子，precision 是以分类器预测出来的 pos(true pos + false neg) 作为分母，所以是差准率. recall 则是以总的 gold pos(true pos + false neg) 作为分母，所以是查全率。 当然希望检索结果Precision越高越好，同时Recall也越高越好，但事实上这两者在某些情况下有矛盾的。比如极端情况下，我们只搜索出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就会很低。因此在不同的场合中需要自己判断希望Precision比较高或是Recall比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。 Ｆ-measure $$F_{\\beta}=\\dfrac{(\\beta^2+1)PR}{\\beta^2P+R}$$ 当 $\\beta&gt;1$时，Recall的比重更大;当 $\\beta&lt;1$时，precision的比重更大。使用最多的是 $\\beta=1$,也就是 $F_{\\beta=1}, F_1$. $\\beta$ 的的取值取决于实际应用。 $$F_1 = \\dfrac{2PR}{P+R}$$ Ｆ-measure 是 precision 和 recall 的 **加权调和平均值(weighted harmonic mean)**。 调和平均值是倒数的算术平均值的倒数。 为什么要使用调和平均值呢？因为它是一个更 保守的度量(conservative metric). 相比直接计算 P 和 R 的平均值， F-measure的值更看重两者中的较小值。 ROC曲线和AUC考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。 TPR, 真正类率(true positive rate ,TPR),： 如果一个实例是正类并且也被 预测成正类，即为真正类（True positive），真正类率是指分类器所识别出来的 正实例占所有正实例的比例。就是 Recall 吧～ TPR = TP / (TP + FN) FPR, 负正类率： 分类器错认为正类的负实例占所有负实例的比例，FPR = FP / (FP + TN) TNR， 真负类率： 分类器认为负类的负实例占所有负实例的比例，也就是负类的 Recall 吧～ TNR = TN /(FP + TN) = 1 - FPR 为什么要引入 ROC 曲线 Motivation1：在一个二分类模型中，对于所得到的连续结果，假设已确定一个阀值，比如说 0.6，大于这个值的实例划归为正类，小于这个值则划到负类中。如果减小阀值，减到0.5，固然能识别出更多的正类，也就是提高了识别出的正例占所有正例 的比类，即TPR,但同时也将更多的负实例当作了正实例，即提高了FPR。为了形象化这一变化，引入ROC，ROC曲线可以用于评价一个分类器。 Motivation2：在类不平衡的情况下,如正样本90个,负样本10个,直接把所有样本分类为正样本,得到识别率为90%。但这显然是没有意义的。单纯根据Precision和Recall来衡量算法的优劣已经不能表征这种病态问题。 ROC 曲线前面已经说道，对于数据不均衡的情况下，precision 和 recall 不足以表征这类问题，就比如上面的例子中，把找出不关于 pie 的 tweet看作是正类，那么它的 精度和召回率 都很高。所以引入 ROC 因为我们要关注的是正类，所以关注指标是 真正类率 TPR 和 负正类率 FPR，真正类率越高越好，负正类率越低越好。但显然这两者之间是矛盾的，与分类器的阈值有关，ROC 就是用来表征阈值与 TPR 和 FPR 之间的关系曲线。 ROC（Receiver Operating Characteristic）翻译为“接受者操作曲线”。曲线由两个变量1-specificity 和 Sensitivity绘制. 1-specificity=FPR，即负正类率。Sensitivity即是真正类率，TPR(True positive rate),反映了正类覆盖程度。 为了更好地理解ROC曲线，我们使用具体的实例来说明： 如在医学诊断中,判断有病的样本。那么尽量把有病的揪出来是主要任务,也就是第一个指标TPR,要越高越好。而把没病的样本误诊为有病的,也就是第二个指标FPR,要越低越好。 不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感,稍微的小症状都判断为有病,那么他的第一个指标应该会很高,但是第二个指标也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么第一个指标达到1,第二个指标也为1。 我们以FPR为横轴,TPR为纵轴,得到如下ROC空间。 参考链接中 ROC 曲线的解释: http://www.cnblogs.com/maybe2030/p/5375175.html#_label0 上面的图表示，分类器对于 pos 和 neg 的分别是有个阈值的，阈值很高，也就是 A 处，显然它的 TPR 不会很高，阈值越低，TPR 越高。 但是阈值很低的话，预测为正类的负实例也就越多， FPR 也会越高。 曲线距离左上角越近,证明分类器效果越好。我们用一个标量 AUC 来量化这个分类效果。 怎么得到 ROC 曲线我们知道对一个二值分类器，其预测出来的正类的 score 是一个概率。当一个样本为正类的概率大于 threshold时，我们判别它为正类。所以不同的 threshold，其对应的 TPR 和 FPR 的值也就不一样，这样就得到了 ROC 曲线。 详细可参考：ROC和AUC介绍以及如何计算AUC 非常清楚！！！ AUCAUC值为ROC曲线所覆盖的区域面积,显然,AUC越大,分类器分类效果越好。 AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。 0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。 AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。 AUC的物理意义：假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。 怎么计算 AUCAUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。 代码实现： sklearn.metrics.roc_auc_score 置信度和置信区间这里好像跟置信度和置信区间没啥关系。。。但是了解下也没事 置信区间再来理解置信度，首先要理解 95%置信区间 和 置信度。 知乎：95%置信区间 我的理解就是，首先总体均值是固定的，但只有上帝知道。我们就要用样本去估计总体均值。一次次抽样会得到很多样本均值，但是我们无法判断哪个均值最好，最接近总体均值。于是，我们构造区间来看这个区间是否含有总体均值。 怎么构造区间： 通过一次次抽样得到的样本均值： $$M=\\dfrac{X_1 + X_2+…+X_n}{n}$$ 根据大数定律： $$M\\sim N(\\mu,\\dfrac{\\sigma^2}{n})$$ 通过查表 标准正太分布表可知，这样可以计算出置信区间，（如果方差为止，则用样本方差代替） 我们以 $1.96\\dfrac{\\sigma }{\\sqrt{n}}$ 为半径做区间，就构造出了 $95%$ 置信区间。按这样去构造的100个区间，其中大约会有95个会包含 $\\mu$ ： 那么，只有一个问题了，我们不知道、并且永远都不会知道真实的 $\\mu$ 是多少: 我们就只有用 $\\hat{\\mu }$ 来代替 $\\mu$ ： $$P(\\hat \\mu-1.96\\dfrac{\\sigma}{n}\\le M\\le\\hat \\mu+1.96\\dfrac{\\sigma}{n}) = 0.95$$ 这样可以得到置信区间了。如果抽样100次，对应也就有100个置信区间，那么其中含有总体均值的概率约为 95%. 置信度样本数目不变的情况下，做一百次试验，有95个置信区间包含了总体真值。置信度为95%","link":"/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/"},{"title":"深度学习-Batch Normalization","text":"Paper Reading paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Motivation Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift. 神经网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，这使得不得不降低学习率、小心地初始化，并且使得那些具有易饱和非线性激活函数的网络训练臭名昭著。作者将分布发生变化称之为 internal covariate shift。 stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate and the initial parameter values. The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper. 在深度学习中我们采用SGD取得了非常好的效果，SGD简单有效，但是它对超参数非常敏感，尤其是学习率和初始化参数。 The change in the distributions of layers’ inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience covariate shift (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. 因为学习的过程中每一层需要去连续的适应每一层输入的分布，所以输入分布发生变化时，会产生一些问题。这里作者引用了 covariate shift 和 domain adaptation 这两个概念。 Therefore, the input distribution properties that aid the network generalization – such as having the same distribution between the training and test data – apply to training the sub-network as well.As such it is advantageous for the distribution of x to remain fixed over time. 有助于网络泛化的输入分布属性：例如在训练和测试数据之间具有相同的分布，也适用于训练子网络 Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the subnetwork, as well. 固定输入分布对该子网络其他部分的网络的训练会产生积极的影响。 总结下为什么要使用 BN： 在训练的过程中，因为前一层的参数改变，将会导致后一层的输入的分布不断地发生改变，这就需要降低学习速率同时要注意参数的初始化，也使具有饱和非线性（saturating nonlinearity）结构的模型非常难训练（所谓的饱和就是指函数的值域是个有限值，即当函数自变量趋向无穷时，函数值不趋向无穷）。深度神经网络之所以复杂是因为它每一层的输出都会受到之前层的影响，因此一个小小的参数改变都会对网络产生巨大的改变。作者将这种现象称为internal covariate shift，提出了对每个输入层进行规范化来解决。在文中，作者提到使用BN可以在训练的过程中使用较高的学习速率，可以比较随意的对参数进行初始化，同时BN也起到了一种正则化的作用，在某种程度上可以取代dropout的作用。 考虑一个以sigmoid为激活函数的神经层： $z=g(Wu+b)$ 其中 u 是输入， g 是 sigmoid 激活函数 $g(x)=\\dfrac{1}{1+exp(x)}$，当 |x| 增加时，$g’(x)$ 趋近于0, 这意味着 $x=Wu+b$ 的所有维度，除了绝对值较小的维度，其他的流向输入 u 的梯度都会消失,也就是进入非线性的饱和区域，这会降低模型训练速度。 在实际应用中，对于非线性饱和的情况，已经有很有对应策略： ReLU 初始化 Xavier initialization. 用一个较小的学习速率进行学习 If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate. 如果保证非线性输入的分布稳定，优化器也就不会陷于饱和区域了，训练也会加速。 We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as Internal Covariate Shift. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. 作者把这种输入分布的变化叫做内部协方差偏移。并提出了 Batch Normalization,通过固定输入的均值和方差。 Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes. BN 除了能解决 internal covariate shift 的问题，还能够降低梯度对学习率，初始化参数设置的依赖。这使得我们可以使用较大的学习率，正则化模型，降低对 dropout 的需求，最后还保证网络能够使用具有饱和性的非线性激活函数。 Towards Reducing Internal Covariate Shiftwhitening 白化操作 It has been long known (LeCun et al., 1998b; Wiesler &amp; Ney, 2011) that the network training converges faster if its inputs are whitened – i.e., linearly transformed to have zero means and unit variances, and decorrelated. 使用白化 whitening 有助于模型收敛，白化是线性变化，转化为均值为0,方差为1,并且去相关性。 However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step. 如果将白化与基于梯度下降的优化混合在一起，那么在执行梯度下降的过程中会受到标准化的参数更新的影响，这样会减弱甚至抵消梯度下降的产生的影响。 作者举了这样一个例子： 考虑一个输入 u 和一个可学习的参数 b 相加作为一个 layer. 通过减去均值进行标准化 $\\hat x=x-E[x]$, 其中 x=u+b. 则前向传播的过程： $x=u+b \\rightarrow \\hat x = x-E[x] \\rightarrow loss$ 反向传播对参数 b 求导（不考虑 b 和 E[x] 的相关性）： $\\dfrac{\\partial l}{\\partial b}=\\dfrac{\\partial l}{\\partial \\hat x}\\dfrac{\\partial \\hat x}{\\partial b} = \\dfrac{\\partial l}{\\partial \\hat x}$ 那么 $\\Delta b = -\\dfrac{\\partial l}{\\partial \\hat x}$, 则对于参数 b 的更新： $b \\leftarrow \\Delta b + b$. 那么经过了标准化、梯度下降更新参数之后： $u+(b+\\Delta b)-E[u+(b+\\Delta b)]=u+b-E[u+b]$ 这意味着这个 layer 的输出没有变化，损失 $\\dfrac{\\partial l}{\\partial \\hat x}也没有变化$, 那么随着训练的进行，**b会无限的增长???**，而loss不变。 This problem can get worse if the normalization not only centers but also scales the activations. We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step. 如果规范化不仅中心处理(即减去均值)，而且还对激活值进行缩放，问题会变得更严重。通过实验发现， 当归一化参数在梯度下降步骤之外进行，模型会爆炸。 进行白化操作，并且在优化时考虑标准化的问题 The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. To address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution.Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters Θ. 之所以会产生以上的问题，主要是梯度优化的过程中没有考虑到标准化操作的进行(不好实现)。为了解决这一问题，作者提出我们需要保证网络产生的激活总是有相同的分布。这样做允许损失值关于模型参数的梯度考虑到标准化。 再一次考虑 x 是一个 layer 的输入，看作一个向量，$\\chi$ 是整个训练集，则标准化： $\\hat x = Norm(x, \\chi)$ 这时标准化的参数不仅取决于当前的输入x，还和整个训练集 $\\chi$ 有关，当x来自其它层的输出时，那么上式就会和前面层的网络参数 $\\theta$ 有关，反向传播时需要计算: $$\\frac{\\partial{Norm(x,\\chi)}}{\\partial{x}}\\text{ and }\\frac{\\partial{Norm(x,\\chi)}}{\\partial{\\chi}}$$ 如果忽略上边第二项就会出现之前说到的问题。但是直接在这一架构下进行白话操作很非常的费时，代价很大。主要是需要计算协方差矩阵，进行归一化，以及反向传播时也需要进行相关的计算。因此这就需要寻找一种新的方法，既可以达到类似的效果，又不需要在每个参数更新后分析整个训练集。 Normalization via Mini-Batch Statistics对比于白化的两个简化 Since the full whitening of each layer’s inputs is costly, we make two necessary simplifications. The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have zero mean and unit variance. 既然白化操作这么费时费力，作者考虑两点必要的简化。第一点，对输入特征的每一维 $x=(x^{(1)},…,x^{(d)})$ 进行去均值和单位方差的处理。 $$\\hat x^{(k)} = \\dfrac{x^{(k)}-E[x^{(k)}]}{\\sqrt {Var[x^{(k)}]}}$$ where the expectation and variance are computed over the training data set. 其中均值和方差是基于整个训练集计算得到的。 Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. 但是如果仅是简单的对每一层的输入进行标准化可能会对该层的表达造成能力改变。比如对一个sigmoid激活函数的输入标准化会将输入固定在线性区域。 为了解决这一问题，作者提出了这样的改变,引入一对参数 $\\gamma^{(k)}$, $\\beta^{(k)}$ 来对归一化之后的值进行缩放和平移。 $$y^{(k)} = \\gamma^{(k)}\\hat x^{(k)} + \\beta^{(k)}$$ $\\gamma^{(k)}$, $\\beta^{(k)}$ 是可学习的参数，用来回复经过标准化之后的网络的表达能力。如果 $\\gamma^{(k)}=\\sqrt {Var[x^{(k)}]}$, $\\beta^{(k)}=E[x^{(k)}]$ In the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations. However, this is impractical when using stochastic optimization. Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation. 在batch中使用整个训练集的均值和方差是不切实际的，因此，作者提出了 第二个简化，用 mini-batch 来估计均值和方差。 Note that the use of mini-batches is enabled by computation of per-dimension variances rather than joint covariances; in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices. 注意到 mini-batches 是计算每一维的方差，而不是联合协方差。使用协方差就需要对模型进行正则化，mini-batches 的大小往往小于需要白化的激活值的数量,会得到 奇异协方差矩阵(singular vorariance matrices)???. BN 核心流程batch size m, 我们关注其中某一个维度 $x^{k}$, k 表示第k维特征。那么对于 batch 中该维特征的 m 个值： $$B={x_{1,…,m}}$$ 经过线性转换： $$BN_{\\gamma, \\beta}:x_{1,..,m}\\rightarrow y_{1,..,m}$$ 对于输入的 mini-batch 的一个维度，计算均值和方差 标准化（注意 epsilon 避免0错误） 使用两个参数进行平移和缩放 这里有点疑惑：为什么在第三步已经完成标准化的情况下还要进行4操作，后来发现其实作者在前文已经说了。首先 $\\hat x$ 是标准化后的输出，但是如果仅以此为输出，其输出就被限定为了标准正态分布，这样很可能会限制原始网络能表达的信息，前文已用sigmoid函数进行了举例说明。因为 $\\gamma, \\beta$ 这两个参数是可以学习的，所以的标准化后的”恢复”程度将在训练的过程中由网络自主决定。 利用链式法则，求损失函数对参数 $\\gamma, \\beta$ 求导： Thus, BN transform is a differentiable transformation that introduces normalized activations into the network. This ensures that as the model is training, layers can continue learning on input distributions that exhibit less internal covariate shift, thus accelerating the training. BN 是可微的，保证模型可训练，网络可以学习得到输入的分布，来减小 internal covarite shift, 从而加速训练。 Training and Inference with Batch-Normalized Networks The normalization of activations that depends on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterministically. For this, once the network has been trained, we use the normalization $\\hat x = \\dfrac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}$ using the population, rather than mini-batch, statistics. 在训练阶段和推理(inference)阶段不一样，这里的推理阶段指的就是测试阶段，在测试阶段使用总体的均值，而不是 mini-batch 的均值。 Using moving averages instead, we can track the accuracy of a model as it trains. Since the means and variances are fixed during inference, the normalization is simply a linear transform applied to each activation. Batch-Normalized Convolutional Networks 第1-5步是算法1的流程，对每一维标准化，得到 $N_{BN}^{tr}$ 6-7步优化训练参数 $\\theta \\bigcup {\\gamma^{k}, \\beta^{k}}$，在测试阶段参数是固定的 8-12步骤是将训练阶段的统计信息转化为训练集整体的统计信息。因为完成训练后在预测阶段，我们使用的是模型存储的整体的统计信息。这里涉及到通过样本均值和方差估计总体的均值和方差的无偏估计，样本均值是等于总体均值的无偏估计的，而样本均值不等于总体均值的无偏估计。具体可看知乎上的解答 https://www.zhihu.com/question/20099757 Batch Normalization enables higher learning rates In traditional deep networks, too high a learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. 学习率过大容易发生梯度消失和梯度爆炸，从而陷入局部最小值。 By normalizing activations throughout the network, it prevents small changes in layer parameters from amplifying as the data propagates through a deep network. 通过规范化整个网络中的激活，可以防止层参数的微小变化在数据通过深层网络传播时放大。 Batch Normalization also makes training more resilient to the parameter scale. Normally, large learning rates may increase the scale of layer parameters, which then amplify the gradient during backpropagation and lead to the model explosion. However, with Batch Normalization, backpropagation through a layer is unaffected by the scale of its parameters. BN 能让训练时的参数更有弹性。通常，学习率过大会增大网络参数，在反向传播中导致梯度过大而发生梯度爆炸。而 BN 使得网络不受参数的大小的影响。 正则化除了可以更快地训练网络，BN层还有对模型起到正则化的作用。因为当训练一个BN网络的时候，对于一个给定的样本，它还可以”看到”一个batch中其他的情况，这样网络对于一个给定的样本输入每次就可以产生一个不确定的输出(因为标准化的过程和batch中其他的样本均有关联)，作者通过实验证明这对减少模型的过拟合具有作用。 代码实现tensorflow 已经封装好了 BN 层，可以直接通过 tf.contrib.layers.batch_norm() 调用，如果你想知道函数背后的具体实现方法，加深对BN层的理解，可以参考这篇文章Implementing Batch Normalization in Tensorflow。 reference: paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) 关于Batch Normalization的一些阅读理解","link":"/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/"},{"title":"机器学习-生成模型到高斯判别分析再到GMM和EM算法","text":"生成模型-高斯判别分析GDA- 高斯混合模型GMM- EM算法 在学习生成模型之前，先学习了解下密度估计和高斯混合模型。 生成学习算法(cs229,Ng)生成算法和判别算法的区别举个栗子： 我们要区分elephants(y=1)和dogs(y=0) 对判别模型（discriminative），以logistic回归为例： logistic回归模型：$p(y|x;\\theta)，h_{\\theta}=g(\\theta^Tx)$,对应的模型其中g是sigmoid函数。通过logistic回归，我们找到一条决策边界decision boundary，能够区分elephants和dogs. 这个学习的过程就是找到表征这个决策过程的参数 $\\theta$. 生成模型（generative）： 同样的我们也是要通过给定的特征x来判别其对应的类别y。但我们换个思路，就是先求p(x|y),也就是通过y来分析对应x满足的一个概率模型p(x|y)。然后在反过来看特征x，以二分类为例，p(x|y=0)和p(x|y=1)哪个概率大，那么x就属于哪一类。 模型：p(x|y)，在给定了样本所属的类的条件下，对样本特征建立概率模型。 p(x|y=1)是elephants的分类特征模型 p(x|y=0)是dogs的分类特征模型 然后通过p(x|y)来判断特征x所属的类别，根据贝叶斯公式： $$p(y=1|x) = \\dfrac{p(x|y=1)p(x)}{p(x)}$$ 在给定了x的情况下p(x)是个定值，p(y)是先验分布，那么计算方法如下： $$arg\\max_yp(y|x) = arg\\max_{y}\\dfrac{p(x|y)p(y)}{p(x)}= arg\\max_{y}p(x|y)p(y)$$ 总结下就是： 生成模型：一般是学习一个代表目标的模型，然后通过它去搜索图像区域，然后最小化重构误差。类似于生成模型描述一个目标，然后就是模式匹配了，在图像中找到和这个模型最匹配的区域，就是目标了。 判别模型：以分类问题为例，然后找到目标和背景的决策边界。它不管目标是怎么描述的，那只要知道目标和背景的差别在哪，然后你给一个图像，它看它处于边界的那一边，就归为哪一类。 由生成模型可以得到判别模型，但由判别模型得不到生成模型。 然鹅，生成模型p(x|y)怎么得到呢？不慌，我们先了解下多维正态分布～ 多维正态分布(the multivariate nirmal distribution) 关于一维正态分布怎么推导出多维正态分布的概率密度函数，可参考知乎:多维高斯分布是如何由一维发展而来的？ 首先一维正态分布: $p(x) = \\dfrac{1}{\\sqrt{2\\pi}}exp(\\dfrac{-x^2}{2})$ 二维标准正态分布，就是两个独立的一维标准正态分布随机变量的联合分布： $p(x,y) = p(x)p(y)=\\dfrac{1}{2\\pi}exp(-\\dfrac{x^2+y^2}{2})$ 把两个随机变量组合成一个随机向量：$v=[x\\quad y]^T$ $p(v)=\\dfrac{1}{2\\pi}exp(-\\dfrac{1}{2}v^Tv)\\quad$ 显然x,y相互独立的话，就是上面的二维标准正态分布公式～ 然后从标准正态分布推广到一般正态分布，通过一个线性变化：$v=A(x-\\mu)$ $p(x)=\\dfrac{|A|}{2\\pi}exp[-\\dfrac{1}{2}(x-\\mu)^TA^TA(x-\\mu)]$ 注意前面的系数多了一个|A|（A的行列式）。 可以证明这个分布的均值为$\\mu$，协方差为$(A^TA)^{-1}$。记$\\Sigma = (A^TA)^{-1}$，那就有 $$p(\\mathbf{x}) = \\frac{1}{2\\pi|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (\\mathbf{x} - \\mu) ^T \\Sigma^{-1} (\\mathbf{x} - \\mu) \\right]$$ 推广到n维： $$p(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (\\mathbf{x} - \\mu) ^T \\Sigma^{-1} (\\mathbf{x} - \\mu) \\right]$$ 需要注意的是：这里的二维、n维到底指的是什么？ 以飞机检测的数据点为例，假设它由heat和time决定，那么这就是个二维正态分布，数据点的生成所处的位置由其概率决定，也就是$p(\\mathbf{x})$ 如果这个数据有n个特征，那么其分布就是n维正态分布。 之前一直理解的是，n维正态分布是两个向量巴拉巴拉。。好像一直没搞懂。。 再顺便了解下协方差矩阵吧～ 关于协方差矩阵，参考blog对多维随机变量$X=[X_1,X_2,…,X_n]^T$，我们往往需要计算各维度之间的协方差，这样协方差就组成了一个n×n的矩阵，称为协方差矩阵。协方差矩阵是一个对角矩阵，对角线上的元素是各维度上随机变量的方差,非对角线元素是维度之间的协方差。 我们定义协方差为$\\Sigma$, 矩阵内的元素$\\Sigma_{ij}$为: $$\\Sigma_{ij} = cov(X_i,X_j) = E[(X_i - E(X_i)) (X_j - E(X_j))]$$ 则协方差矩阵为: $$\\Sigma = E[(X-E(X)) (X-E(X))^T] = \\left[ \\begin{array}{cccc} cov(X_1,X_1) &amp; cov(X_1,X_2) &amp; \\cdots &amp; cov(X_1,X_n) \\ cov(X_2,X_1) &amp; cov(X_2,X_2) &amp; \\cdots &amp;cov(X_2,X_n) \\ \\vdots &amp; \\vdots&amp; \\vdots &amp; \\vdots \\ cov(X_n,X_1) &amp; cov(X_n,X_2,)&amp;\\cdots&amp; cov(X_n,X_n) \\end{array} \\right]$$如果X~$N(\\mu,\\Sigma)$,则$Cov(X)=\\Sigma$ 可以这么理解协方差，对于n维随机变量X，第一维是体重$X_1$，第二维是颜值$X_2$，显然这两个维度是有一定联系的，就用$cov(X_1,X_2)$来表征，这个值越小，代表他们越相似。协方差怎么求，假设有m个样本，那么所有的样本的第一维就构成$X_1$…不要把$X_1$和样本搞混淆了。 了解了多维正态分布和协方差，我们再回到生成模型p(x|y)。。其实我们就是假设对于n维特征，p(x|y)是n维正态分布～怎么理解呢，下面就说！ 高斯判别分析模型The Gaussian Discriminant Analysis model高斯判别模型就是：假设p(x|y)是一个多维正态分布，为什么可以这么假设呢？因为对于给定y的条件下对应的特征x都是用来描述这一类y的，比如特征是n维的，第一维描述身高，一般都是满足正态分布的吧，第二维描述体重，也可认为是正态分布吧～ 则生成模型： y ~ Bernoulli($\\phi)$ 伯努利分布，又称两点分布，0-1分布 x|y=0 ~ $N(u_0,\\Sigma)$ x|y=1 ~ $N(u_1,\\Sigma)$ 这里可以看作是一个二分类，y=0和y=1,可以看作是伯努利分布，则$p(y)=\\phi^y(1-\\phi)^{1-y}$，要学的参数之一: $\\phi=p(y=1)$，试想如果是多分类呢，那么要学习的参数就有$\\phi_1,\\phi_2,….\\phi_k$ 其中类别对应的特征x|y=0,x|y=1服从正态分布。怎么理解呢？就是既然你们都是一类人，那么你们的身高啊，体重啊等等应该满足正态分布。。有几维特征就满足几维正态分布 这里x是n维特征，身高，体重，颜值…balabala，所以x|y=0满足n维正态分布～x|y=1也是啦，只不过对于不同的类，对应n维特征的均值不一样，奇怪为什么协方差矩阵是一样的？？这里是将它特殊化了，后面会讲的一般性的em算法就不是这样的了 每个分类对应的n维特征的分布显然不是独立的，比如体重和颜值还是有关系的吧～他们的协方差，方差就统统都在$\\Sigma$协方差矩阵里面了 $$p(x|y=0) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_0) ^T \\Sigma^{-1} (x - \\mu_0) \\right]$$ $$p(x|y=1) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_1) ^T \\Sigma^{-1} (x - \\mu_1) \\right]$$ 这样，模型中我们要学习的参数有$\\phi,\\Sigma, \\mu_0,\\mu_1$，对于训练数据，就是观测到的数据x,y，既然他们出现了，那么他们的联合概率，也就是似然函数$\\prod_{i=1}^mp(x,y)$就要最大～其对数似然log-likelihood： $$\\begin{equation}\\begin{aligned} L(\\phi,\\Sigma, \\mu_0,\\mu_1) &amp;= log\\prod_{i=1}^mp(x^{(i)},y^{(i)};\\phi,\\Sigma, \\mu_0,\\mu_1$) \\ &amp;= log\\prod_{i=1}^mp(x^{(i)}|y^{(i)};\\phi,\\Sigma, \\mu_0,\\mu_1$) p(y^{(i)};\\phi)\\ \\end{aligned}\\end{equation}\\label{eq2}$$ 其中$p(y^{(i)};\\phi)$是已知的，也就是先验概率(class priors)，$p(x^{(i)}|y^{(i)})$就是上面推导的～代入后，分别对参数求导即可： 在回过头来看这些公式， $\\phi$很好理解，就是样本中正分类的概率。 $\\mu_0$就是负分类中x对应的均值 $\\mu_1$就是正分类中x对应的均值 $\\Sigma$就是$(x-\\mu_1)$和$x-\\mu_2$的协方差矩阵 然后通过p(x|y=0),p(x|y=1)即可对需要预测的x求出对应的概率，然后做出判别了。这样看来，如果直接对x|y=1,和x|y=0做出了正态分布的猜测，就可以直接写出来了。只不过，我们用极大似然估计重新推导了一遍。 高斯混合模型GMMGMM前面GDA是有标签的，也算是有监督学习。而在没有标签的情况下呢，就是无监督学习了，虽然我们无法给出x所属的类叫啥，但是我们可以判断出哪些x是同一类，以及样本中总共有多少类（虽然这个类数嘛。。类似于k-means的类数，可根据交叉验证选择）。 其实和GDA非常相似，不过这里没有了类标签，只有一堆样本特征，${x^{(1)},x^{(2)},…,x^{(m)}}$, 我们不知道这些样本属于几个类别，也不知道有哪些类了。但虽然不知道，我们确定他们是存在的，只是看不见而已。我们可以假设存在k类，${z^{(1)},z^{(2)},…,z^{(k)}}$,看不见的，我们就叫它们隐藏随机变量(latent random variable)， 这样一来，就训练样本就可以用这样的联合分概率模型表示了，$p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})$ 同GDA不一样的是，这里是多分类，可假定$z^{(i)}\\sim Multinomial(\\phi)$，多项式分布（二项分布的拓展～），那么$p(z^{(i)})=\\phi_j$ 同GDA相同的是，对于每一个类别，其对应的样本满足n维正态分布，也就是：$x^{(i)}|z^{(i)}=j\\sim N(\\mu_j,\\Sigma_j)$,但注意哦，这里每个高斯分布使用了不同的协方差矩阵$\\Sigma_j$ $$p(x|z^{(1)}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_0|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_0) ^T \\Sigma_0^{-1} (x - \\mu_0) \\right]$$ $$p(x|z^{(2)}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_1|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_1) ^T \\Sigma_1^{-1} (x - \\mu_1) \\right]$$ $$….$$ $$p(x|z^{(k)}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_k|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - \\mu_k) ^T \\Sigma_k^{-1} (x - \\mu_k) \\right]$$ 然后带入到训练样本的对数似然（log-likelihood）: $$L(\\phi,\\mu,\\Sigma)=\\sum_{i=1}^{m}logp(x^{(i)};\\phi,\\mu,\\Sigma)$$ $$L(\\phi,\\mu,\\Sigma)=\\sum_{i=1}^{m}log\\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)};\\mu,\\Sigma) p(z^{(i)};\\phi)\\$$ 这里需要注意下标：对于类别有k类，第一个求和符号是对第i个样本在k个类别上的联合概率，第二个求和符号是m个样本的联合概率。 我们可以注意到，如果我们知道$z^{(i)}$,那么这个似然函数求极大值就很容易了，类似于高斯判别分析，这里的$z^{(i)}$相当于标签，分别对参数求导可得： 其中的参数: $1{z^{(i)}=j}$表示第i个样本为j类时，这个值就为１，那么$\\phi_j=\\frac{1}{m}\\sum_{i=1}^m1{z^{(i)}=j}$表示样本中类别为j的概率 其中$p(z^{(i)};\\phi)$是根据伯努利分布得到的，在GDA中$p(y|\\phi)$是已知的频率概率。 So $z^{(i)}$ 到底有多少个分类？每个类别的概率是多少？譬如上式中 $\\sum_{i=1}^{m}1{z^{(i)}=j}$ 这个没法求对吧～它是隐藏变量！所以还是按照这个方法是求不出来的～ 这个时候EM算法就登场了～～～ 用EM算法求解GMM模型上面也提到了，如果$z^({i})$是已知的话，那么$\\phi_j=\\frac{1}{m}\\sum_{i=1}^m1{z^{(i)}=j}$表示类别j的概率$p(z^{(i)}=j)$也就已知了，但是呢？我们不知道。。所以我们要猜测$p(z^{(i)}=j)$这个值，也就是EM算法的第一步： Repeat until convergence 迭代直到收敛:{ (E-step):for each i,j,set: $w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)$ $w_j^{(i)}$什么意思呢?就是对于i样本,它是j类的后验概率。在GDA里面，x_i的类别是确定的，在GMM里面呢？不知道它的类别，所以只能假设k类都有可能，它是j类别的概率就是$w_j^{(i)}$，它仅仅取决于$\\phi_j$,而在GMM里面，它取决于$\\phi_j,\\mu_j,\\Sigma_j$，实际上$w_j^{(i)}$的值，就包含了两个我们在GMM所做的假设，多项式分布和正态分布。 The values $w_j$ calculated in the E-step represent our “soft” guesses for the values of $z^{(i)}$ . The term “soft” refers to our guesses being probabilities and taking values in [0, 1]; in contrast, a “hard” guess is one that represents a single best guess (such as taking values in {0, 1} or {1, . . . , k}). 硬猜测是k均值聚类，GMM是软猜测。 这样一来，参数更新就可以这样写了，也就是EM算法的第二步： (M-step) Updata the parameters: 然后对似然函数求导，后面会详细介绍 $$\\phi_j:=\\frac{1}{m}\\sum_{i=1}^mw_j^{(i)}$$ $$\\mu_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\\sum_{i=1}^mw_j^{(i)}}$$ $$\\Sigma_j:=\\dfrac{\\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T}{\\sum_{i=1}^mw_j^{(i)}}$$ ｝ 训练过程的理解可参考blog $w_j^{(i)}表示第i个样本为ｊ类别的概率，而\\phi_j$表示m个样本中j类别的概率，$\\mu_j,\\Sigma_j$分别表示j类别对应的n维高斯分布的期望和协方差矩阵 所以，求出$w_j^{(i)}$，一切就都解决了吧？对于后验概率$p(z^{(i)}=j|x^{(i)})$可以根据Bayes公式： $$p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)=\\dfrac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}{\\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\\mu,\\Sigma)p(z^{(i)}=l;\\phi)}$$ 其中先验概率$p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)$可以根据高斯分布的密度函数来求，$z^{(i)}=j\\sim N(\\mu_j,\\Sigma_j)$ 类先验(class priors)$p(z^{(i)}=j;\\phi)$可以取决于多项式分布中j类的概率$\\phi_j$ The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments $c^(i)$, we instead have the “soft” assignments $w_j$ . Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea. EM算法使我们联想起了k-means,区别在于k-means的聚类是通过欧氏距离c(i)来定义的，而EM是通过$w_j$probabilities来分类的。同k-means一样，这里的EM算法也是局部优化，因此最好采用不同的方式初始化～ convergence?我们知道k-means一定是收敛的，虽然结果不一定是全局最优解，但它总能达到一个最优解。但是EM算法呢，也是收敛的。 The EM algorithm前面我们讲的是基于高斯混合模型的EM算法，但一定所有的类别都是高斯分布吗？还有卡方分布，泊松分布等等呢，接下来我们就将讨论EM算法的一般性。 在学习一般性的EM算法前，先了解下Jensen’s inequality Jensen’s inequality如果函数$f$，其二阶导恒大与等于0 $(f^{‘’}\\ge 0)$，则它是凸函数f(convec function)。 如果凸函数的输入是向量vector-valued inputs，那么它的海森矩阵(hessian)H是半正定的。Jensen’s 不等式： Let f be a convex function, and let X be a random variable. Then: $$E[f (X)] ≥ f (EX).$$ Moreover, if f is strictly convex, then $E[f (X)] = f (EX)$ holds true if and only if $X = E[X]$ with probability 1 (i.e., if X is a constant). 举个栗子来解释jensen不等式： 假设输入随机变量X是一维的哈，然后Ｘ取a,b的概率都是0.5,那么 $$EX=(a+b)/2,f(EX)=f(\\dfrac{a+b}{2})$$,$$E[f(X)]=\\dfrac{f(a)+f(b)}{2}$$ 因为是凸函数，所以 $f(EX)\\le E[f(X)]$ 同理，如果是凹函数(concave function),那么不等式方向相反$f(EX)\\ge E[f(X)]$。后面EM算法里面就要用到log(X)，log(x)就是个典型的凹函数～ The EM algorithm首先，问题是：我们要基于给定的m个训练样本${x^{(1)},x^{(2)},…,x^{(m)}}$来进行密度估计～ 像前面一样，创建一个参数模型p(x,z)来最大化训练样本的对数似然： $$L(\\theta)=\\sum_{i=1}^mlogp(x;\\theta)$$ $$L(\\theta)=\\sum_{i=1}^mlog\\sum_zp(x,z;\\theta)$$ 一般性就是把前面特殊化的假设去掉，没有了正态分布和多项式分布。 可以看到，$z^{(i)}$是隐藏的随机变量(latent random variable),关于参数$\\theta$的最大似然估计就很难计算了。 解释下公式中的推导： 这里是针对样本i来说，对于样本i，它可能是$z^1,z^2,…,z^k$都有可能，但他们的probability之和为１，也就是 $\\sum_zQ_i(z)=1$ (2)到(3)的推导：可以将 $\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}$ 看做随机变量Ｘ,那么（２）式中的后半部分 $log\\sum_{z^{(i)})}[\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}]$就是log(EX)了，$logx$是一个凹函数，则其大于$E[log(x)]$ EM迭代过程(重点): （1）根据上式可以看做$L(\\theta)\\ge J(Q,\\theta)$.两边都是关于$\\theta$的函数，那么将$\\theta$固定，调整Q在一定条件下能使等式成立。 （2）然后固定Q,调整$\\theta^t$到$\\theta^{t+1}$找到下界函数的最大值$J(Q,\\theta^{t+1})$.显然在当前Q的条件下，$L(\\theta^{t+1})\\ne J(Q,\\theta^{t+1})$,那么根据Jensen不等式，$L(\\theta_{t+1})&gt;J(Q,\\theta^{t+1})=L(\\theta^{t})$,也就是说找到了使得对数似然L更大的$\\theta$.这不就是我们的目的吗？！ 然后迭代循环(1)(2)步骤，直到在调整$\\theta$时，下界函数$J(Q,\\theta)$不在增加，即小于某个阈值。 看下Ng画的图： 任意初始化$\\theta$和Q,然后找下界函数和$l(\\theta)$交接的点，这就是EM算法的第一步： 我们要让不等式相等,即Jensen’s inequality中的随机变量取值是一个常量，看(2)式： $$\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c$$ 对左边分子分母同时对z求类和： $$\\dfrac{\\sum_zp(x^{(i)},z^{(i)};\\theta)}{\\sum_zQ_i(z^{(i)})}=c$$ 根据$\\sum_zQ_i(z)=1$： $$\\sum_zp(x^{(i)},z^{(i)};\\theta)=c$$ 带回去可得： $$Q_i(z^{(i)})=\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{\\sum_zp(x^{(i)},z^{(i)};\\theta)}$$ $$Q_i(z^{(i)})=\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{p(x^{(i)};\\theta)}$$ $$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\\theta)$$ EM总结下来： Repeat until convergence { (E-step) For each i,找到下界函数, set: $$Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\\theta)$$ (M-step)找到下界凹函数的最大值,也就是(3)式 Set: $$\\theta:=arg\\max_{\\theta}\\sum_i^m\\sum_{z^{(i)}}^kQ_i(z^{(i)})log\\dfrac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}$$ } 要理解的是： EM算法只是一种计算方式，对于上式中的$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\\theta)$我们还是要根据假设来求得，比如GMM中的多类高斯分布。然后带回到对数似然中，通过求导得到参数估计。我们费尽心机证明EM算法收敛，只是为了去证明这样去求似然函数的极大值是可行的，然后应用到类似于GMM，HMM中。 training and will converge?首先说是否收敛，答案是肯定收敛的。。懒得输公式了。。直接贴图吧，这个比较好理解： 上面写这么多，其实就是证明$L(\\theta_{t+1})&gt;L(\\theta_t)$. Mixture of Gaussians revisited我们知道了em算法是一种计算方式，用来解决含有隐变量似然对数很难求的问题，那么我们把它运用到GMM中。 E step: $w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)$ $$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\\phi,\\mu,\\Sigma)=\\dfrac{p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)p(z^{(i)}=j;\\phi)}{\\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\\mu,\\Sigma)p(z^{(i)}=l;\\phi)}$$ 其中先验概率$p(x^{(i)}|z^{(i)}=j;\\mu,\\Sigma)$可以根据高斯分布的密度函数来求，$z^{(i)}=j\\sim N(\\mu_j,\\Sigma_j)$ 类先验(class priors)$p(z^{(i)}=j;\\phi)$可以取决于多项式分布中j类的概率$\\phi_j$ 这样我们就完成了对$w_j^{(i)}$的soft ‘guess’，也就是E step. M step: 然后对参数求导： 详细推导过程，参考cs229-notes8 我们在整体回顾一下整个过程，所谓的E step就是找到$Q_i(z^{j}),w_i^j$（在一定假设下是可以通过bayes公式求得的），使得下界函数与log函数相等，也就是Jensen取等号时。然后是M step就是在Q的条件下找到下界函数最大值，也就是对参数求导，导数为0的地方。 然后在根据求得的参数，再求Q，再带入求导。。。迭代直到收敛。","link":"/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/"},{"title":"深度学习-优化算法","text":"paper: An overview of gradient descent optimization algorithms Gradient descent variantsBatch gradient descentcomputes the gradient of the cost function to the parameters $\\theta$ for the entire training dataset. $$\\theta= \\theta - \\delta_{\\theta}J(\\theta)$$ 1234567for i in range ( nb_epochs ): params_grad = evaluate_gradient ( loss_function , data , params ) params = params - learning_rate * params_grad Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces. Stochastic gradient descentStochastic gradient descent (SGD) in contrast performs a parameter update for each training example x(i) and label y(i): $$\\theta= \\theta - \\delta_{\\theta}J(\\theta; x^{(i)}; y^{(i)})$$ 1234567891011for i in range ( nb_epochs ): np. random . shuffle ( data ) for example in data : params_grad = evaluate_gradient ( loss_function , example , params ) params = params - learning_rate * params_grad Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. 批梯度下降的计算过于冗余，它在每一次参数更新之前的计算过程中会计算很多相似的样本。随机梯度下降则是每一次参数更新计算一个样本，因此更新速度会很快，并且可以在线学习。但是用于更新的梯度的方差会很大，导致 loss 曲线波动很大。 While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. 批梯度下降收敛到的最小值与相应的参数关系很大（也就是说跟权重的初始化会有很大影响）。而 SGD 由于loss波动很大，更有效的跳出局部最优区域，从而获得更好的局部最优值。但另一方面，这也会使得 SGD 难以收敛。实验表明，缓慢的降低学习率， SGD 和 BatchGD 能获得同样的局部最优解。 Mini-batch gradient descentMini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples. $$\\theta= \\theta - \\delta_{\\theta}J(\\theta; x^{(i+n)}; y^{(i+n)})$$ 1234567891011for i in range ( nb_epochs ): np. random . shuffle ( data ) for batch in get_batches (data , batch_size =50): params_grad = evaluate_gradient ( loss_function , batch , params ) params = params - learning_rate * params_grad reduces the variance of the parameter updates, which can lead to more stable convergence; 减小参数更新的方差，使得收敛更稳定。 can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient mini-batch very efficient. 能非常好的利用矩阵优化的方式来加速计算，这在各种深度学习框架里面都很常见。 Challenges Choosing a proper learning rate. 选择合适的学习率。 Learning rate schedules. try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics. 学习率计划。在训练过程中调整学习率，譬如退火，预先定义好的计划，当一个 epoch 结束后，目标函数（loss） 减小的值低于某个阈值时，可以调整学习率。 the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features. 对所有的参数使用相同的学习率。如果你的数据是稀疏的，并且不同的特征的频率有很大的不同，这个时候我们并不希望对所有的参数使用相同的学习率，而是对更罕见的特征执行更大的学习率。 Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. [5] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions. 对于非凸损失函数的优化问题，需要避免陷入其众多的次优局部极小值。Dauphin et al. [5] 则认为， 相比局部极小值，鞍点的是更难解决的问题。鞍点是一个维度上升，一个维度下降。详细的关于鞍点以及 SGD 如何逃离鞍点可参考：知乎：如何逃离鞍点 . Gradient descent optimization algorithmsMomentum [17] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Figure 2b. It does this by padding a fraction $gamma$ of the update vector of the past time step to the current update vector. Momentumpaper: [Neural networks : the official journal of the International Neural Network Society]() without Momentum: $$\\theta += -lr * \\nabla_{\\theta}J(\\theta)$$ with Momentum: $$v_t=\\gamma v_{t-1}+\\eta \\nabla_{\\theta}J(\\theta)$$ $$\\theta=\\theta-v_t$$ 动量梯度下降的理解： The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. 如上图中垂直方向的梯度方向是一致的，那么它的动量会累积，并在这个方向的速度越来越大。而在某个水平方向，其梯度方向总是变化，那么它的速度会减小，也就是在这个方向的波动幅度会得到抑制。 其实就是把梯度看做加速度，参数的更新量看做速度。速度表示一个step更新的大小。加速度总是朝着一个方向，速度必然越来越快。加速度方向总是变化，速度就会相对较小。 $\\gamma$ 看做摩擦系数， 通常设置为 0.9。$\\eta$ 是学习率。 Nesterov accelerate gradient(NAG)paper: [Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2).]() We would like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again. Nesterov accelerated gradient (NAG) [14] is a way to give our momentum term this kind of prescience. 如果采用 momentum，在接近目标函数最优值时，由于速度在垂直方向是一直增加的，所以速度会很大，这个时候就会越过最小值，然后还得绕回来，增加了训练时间。所以我们需要参数的更新具有先见之明，知道在接近最优解时，降低参数更新的速度大小。 $$v_t=\\gamma v_{t-1}+\\eta \\nabla_{\\theta}J(\\theta-\\gamma v_{t-1})$$ $$\\theta=\\theta-v_t$$ 在 momentum 中，我们用速度 $\\gamma v_{t-1}$ 来更新参数。 事实上在接近局部最优解时，目标函数对于 $\\theta$ 的梯度会越来越小，甚至接近于 0. 也就是说，尽管速度在增加，但是速度增加的程度越来越小。我们可以通过速度增加的程度来判断是否要接近局部最优解了。$\\nabla_{\\theta}J(\\theta-\\gamma v_{t-1})$ 就表示速度变化的程度，代替一直为正的 $\\nabla_{\\theta}J(\\theta)$，在接近局部最优解时，这个值应该是负的，相应的参数更新的速度也会减小. 在代码实现时，对于 $J(\\theta-\\gamma v_{t-1})$ 的梯度计算不是很方便，可以令： $$\\phi = \\theta-\\gamma v_{t-1}$$ 然后进行计算，具体可参考 tensorflow 或 pytorch 中代码。 Adagradpaper: [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization]() Adagrad [8] is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. 对于不同的参数，自适应的调整对应的梯度大小。对低频参数或特征，使其更新的梯度较大，对高频的参数或特征，使其更新的梯度较小。比如在训练 Glove 词向量时，低频词在某一步迭代中可能并没有参与 loss 的计算，所以更新的会相对较慢，所以需要人为的增大它的梯度。 不同的时间步 t,不同的参数 i 对应的梯度： $$g_{t,i}=\\nabla_{\\theta_t}J(\\theta_t,i)$$ $$\\theta_{t+1,i}=\\theta_{t,i}-\\eta \\cdot g_{t,i}$$ $$\\theta_{t+1,i}=\\theta_{t,i}-\\dfrac{\\eta}{\\sqrt G_{t,ii}+\\epsilon} g_{t,i}$$ $G_{t,ii}$ 是对角矩阵，对角元素是对应的梯度大小。 12345cache += dx**2x += -lr * dx/(np.sqrt(cache) + 1e-7) RMSpropGeoff Hinton Lecture 6e Adagrad 中随着 cache 的累积，最后的梯度会变为 0，RMSprop 在此基础上进行了改进，给了 cache 一个衰减率，相当于值考虑了最近时刻的梯度值，而很早之前的梯度值经过衰减后影响很小。 $$E[g^2]_ t=0.9E[g^2]_ {t-1}+0.1g^2_t$$ $$\\theta_{t+1}=\\theta_t-\\dfrac{\\eta}{E[g^2]_ t+\\epsilon}g_t$$ 12345cache = decay_rate*cache + (1-decay_rate)*dx**2x += -lr * dx/(np.sqrt(cache) + 1e-7) 使用指数衰减的形式来保存 cache 能有效的节省内存，只需要记录当前的梯度值即可，而不用保存所有的梯度值。 Adam(Adaptive Moment Estimation)Adam: a Method for Stochastic Optimization. In addition to storing an exponentially decaying average of past squared gradients $v_t$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum: similar like momentum: $$m_t=\\beta_1m_{t-1}+(1-\\beta_1)g_t$$ similar like autograd/RMSprop: $$v_t=\\beta_2v_{t-1}+(1-\\beta_2)g_t^2$$ $m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$ are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. β1 and β2 are close to 1). They counteract these biases by computing bias-corrected first and second moment estimates: $$\\hat m_t=\\dfrac{m_t}{1-\\beta^t_1}$$ $$\\hat v_t=\\dfrac{v_t}{1-\\beta^t_2}$$ They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule: $$\\theta_{t+1}=\\theta_t-\\dfrac{\\eta}{\\sqrt{\\hat a}+ \\epsilon}{\\hat m_t}$$ $m_t$ 是类似于 Momentum 中参数更新量，是梯度的函数. $\\beta_1$ 是摩擦系数，一般设为 0.9. $v_t$ 是类似于 RMSprop 中的 cache，用来自适应的改变不同参数的梯度大小。 $\\beta_2$ 是 cache 的衰减系数，一般设为 0.999. AdaMaxAdam: a Method for Stochastic Optimization. 在 Adam 中, 用来归一化梯度的因子 $v_t$ 与过去的梯度(包含在 $v_{t-1}$ 中)以及当前的梯度 $|g_t|^2$ 的 l2 范式成反比。 $$v_t=\\beta_2v_{t-1}+(1-\\beta_2)g_t^2$$ 可以将其泛化到 $l_p$ 范式。同样的 $\\beta_2$ 变为 $\\beta_2^p$. Norms for large p values generally become numerically unstable, which is why $l_1$ and $l_2$ norms are most common in practice. However, $l_{\\infty}$ also generally exhibits stable behavior. For this reason, the authors propose AdaMax [10] and show that $v_t$ with $l_{\\infty}$ converges to the following more stable value. To avoid confusion with Adam, we use ut to denote the infinity norm-constrained $v_t$: $$\\mu_t=\\beta_2^{\\infty}v_{t-1}+(1-\\beta_2^{\\infty})|g_t|^{\\infty}$$ $$=max(\\beta_2\\cdot v_{t-1}, |g_t|)$$ 然后用 $\\mu_t$ 代替 Adam 中的 $\\sqrt(v_t)+\\epsilon$: $$\\theta_{t+1}=\\theta_t-\\dfrac{\\eta}{\\mu_t}{\\hat m_t}$$ Note that as $\\mu_t$ relies on the max operation, it is not as suggestible to bias towards zero as $m_t$ and $v_t$ in Adam, which is why we do not need to compute a bias correction for ut. Good default values are again: $$\\eta = 0.002, \\beta_1 = 0.9, \\beta_2 = 0.999.$$ Visualization of algorithms we see the path they took on the contours of a loss surface (the Beale function). All started at the same point and took different paths to reach the minimum. Note that Adagrad, Adadelta, and RMSprop headed off immediately in the right direction and converged similarly fast, while Momentum and NAG were led off-track, evoking the image of a ball rolling down the hill. NAG, however, was able to correct its course sooner due to its increased responsiveness by looking ahead and headed to the minimum. 如果目标函数是 Beale 这种类型的函数，自适应优化算法能更直接的收敛到最小值。而 Momentum 和 NAG 则偏离了轨道，就像球从山上滚下一样，刹不住车。但是 NAG 因为对未来具有一定的预见性，所以能更早的纠正从而提高其响应能力。 shows the behaviour of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope, which pose a difficulty for SGD as we mentioned before. Notice here that SGD, Momentum, and NAG find it difficulty to break symmetry, although the latter two eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope, with Adadelta leading the charge. 各种优化算法鞍点的表现。 Momentum, SGD, NAG 很难打破平衡，而自适应性的算法 Adadelta, RMSprop, Adadelta 能很快的逃离鞍点。 examplemodel123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.nn as nnimport torch.optim as optimclass TestNet(nn.Module): def __init__(self): super(TestNet, self).__init__() self.linear1 = nn.Linear(10, 5) self.linear2 = nn.Linear(5, 1) self.loss = nn.BCELoss() def forward(self, x, label): &quot;&quot;&quot; x: [batch, 10] label: [batch] &quot;&quot;&quot; out = self.linear2(self.linear1(x)).squeeze() loss = self.loss(out, label) return out, loss 12345model = TestNet()model TestNet( (linear1): Linear(in_features=10, out_features=5, bias=True) (linear2): Linear(in_features=5, out_features=1, bias=True) (loss): BCELoss() ) 123list(model.named_parameters()) [('linear1.weight', Parameter containing: tensor([[ 0.2901, -0.0022, -0.1515, -0.1064, -0.0475, -0.0324, 0.0404, 0.0266, -0.2358, -0.0433], [-0.1588, -0.1917, 0.0995, 0.0651, -0.2948, -0.1830, 0.2356, 0.1060, 0.2172, -0.0367], [-0.0173, 0.2129, 0.3123, 0.0663, 0.2633, -0.2838, 0.3019, -0.2087, -0.0886, 0.0515], [ 0.1641, -0.2123, -0.0759, 0.1198, 0.0408, -0.0212, 0.3117, -0.2534, -0.1196, -0.3154], [ 0.2187, 0.1547, -0.0653, -0.2246, -0.0137, 0.2676, 0.1777, 0.0536, -0.3124, 0.2147]], requires_grad=True)), ('linear1.bias', Parameter containing: tensor([ 0.1216, 0.2846, -0.2002, -0.1236, 0.2806], requires_grad=True)), ('linear2.weight', Parameter containing: tensor([[-0.1652, 0.3056, 0.0749, -0.3633, 0.0692]], requires_grad=True)), ('linear2.bias', Parameter containing: tensor([0.0450], requires_grad=True))] add model parameters to optimizer123456789import torch.optim as optim# parameters = model.parameters()parameters_filters = filter(lambda p: p.requires_grad, model.parameters()) 12345678910111213optimizer = optim.Adam( params=parameters_filters, lr=0.001, betas=(0.8, 0.999), eps=1e-8, weight_decay=3e-7) 123optimizer.state_dict &lt;bound method Optimizer.state_dict of Adam ( Parameter Group 0 amsgrad: False betas: (0.8, 0.999) eps: 1e-08 lr: 0.001 weight_decay: 3e-07 )&gt; 不同的模块设置不同的参数12345parameters = [{&quot;params&quot;: model.linear1.parameters()}, {&quot;params&quot;:model.linear2.parameters(), &quot;lr&quot;: 3e-4}] 12345678910111213optimizer2 = optim.Adam( params=parameters, lr=0.001, betas=(0.8, 0.999), eps=1e-8, weight_decay=3e-7) 123optimizer2.state_dict &lt;bound method Optimizer.state_dict of Adam ( Parameter Group 0 amsgrad: False betas: (0.8, 0.999) eps: 1e-08 lr: 0.001 weight_decay: 3e-07 Parameter Group 1 amsgrad: False betas: (0.8, 0.999) eps: 1e-08 lr: 0.0003 weight_decay: 3e-07 )&gt; zero_grad在进行反向传播之前，如果不需要梯度累加的话，必须要用zero_grad()清空梯度。具体的方法是遍历self.param_groups中全部参数，根据grad属性做清除。 123456789101112131415def zero_grad(self): r&quot;&quot;&quot;Clears the gradients of all optimized :class:`torch.Tensor` s.&quot;&quot;&quot; for group in self.param_groups: for p in group['params']: if p.grad is not None: p.grad.detach_() p.grad.zero_() 12345group_parameters = [{&quot;params&quot;: model.linear1.parameters()}, {&quot;params&quot;:model.linear2.parameters(), &quot;lr&quot;: 3e-4}] 123456789x = torch.randn(2, 10)label = torch.Tensor([1,0])out, loss = model(x, label)loss.backward() 123optimizer2.zero_grad() 1234567891011for group in group_parameters: for p in group[&quot;params&quot;]: if p.grad is not None: p.grad.detach_() p.grad.zero_() 这里并没有使用 backward() 所以暂时不存在梯度。 在反向传播 backward() 计算出梯度之后，就可以调用step()实现参数更新。不过在 Optimizer 类中，step()函数内部是空的，并且用raise NotImplementError 来作为提醒。后面会根据具体的优化器来分析step()的实现思路。 辅助类lr_schedulerlr_scheduler用于在训练过程中根据轮次灵活调控学习率。调整学习率的方法有很多种，但是其使用方法是大致相同的：用一个Schedule把原始Optimizer装饰上，然后再输入一些相关参数，然后用这个Schedule做step()。 1234567# lambda1 = lambda epoch: epoch // 30lambda1 = lambda epoch: 0.95 ** epochscheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1) 123scheduler.step() warm up scheduler12345678910111213141516171819202122232425262728293031323334353637import mathparameters = filter(lambda p: p.requires_grad, model.parameters())lr_warm_up_num = 1000optimizer = optim.Adam( params=parameters, lr=0.001, betas=(0.8, 0.999), eps=1e-8, weight_decay=3e-7)cr = 1.0 / math.log(lr_warm_up_num)scheduler = optim.lr_scheduler.LambdaLR( optimizer, lr_lambda=lambda ee: cr * math.log(ee + 1) if ee &lt; lr_warm_up_num else 1)","link":"/2018/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"title":"机器学习-过拟合","text":"过拟合的原理以及解决方法。 Overfitting过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。 具体表现就是最终模型在训练集上效果好；在测试集上效果差。模型泛化能力弱。 为什么要解决过拟合为什么要解决过拟合现象？这是因为我们拟合的模型一般是用来预测未知的结果（不在训练集内），过拟合虽然在训练集上效果好，但是在实际使用时（测试集）效果差。同时，在很多问题上，我们无法穷尽所有状态，不可能将所有情况都包含在训练集上。所以，必须要解决过拟合问题。 为什么在机器学习中比较常见？这是因为机器学习算法为了满足尽可能复杂的任务，其模型的拟合能力一般远远高于问题复杂度，也就是说，机器学习算法有「拟合出正确规则的前提下，进一步拟合噪声」的能力。 而传统的函数拟合问题（如机器人系统辨识），一般都是通过经验、物理、数学等推导出一个含参模型，模型复杂度确定了，只需要调整个别参数即可。模型「无多余能力」拟合噪声。 解决方法获取更多数据这是解决过拟合最有效的方法，只要给足够多的数据，让模型「看见」尽可能多的「例外情况」，它就会不断修正自己，从而得到更好的结果： 如何获取更多数据，可以有以下几个方法： 从数据源头获取更多数据：这个是容易想到的，例如物体分类，我就再多拍几张照片好了；但是，在很多情况下，大幅增加数据本身就不容易；另外，我们不清楚获取多少数据才算够； 根据当前数据集估计数据分布参数，使用该分布产生更多数据：这个一般不用，因为估计分布参数的过程也会代入抽样误差。 数据增强（Data Augmentation）：通过一定规则扩充数据。如在物体分类问题里，物体在图像中的位置、姿态、尺度，整体图片明暗度等都不会影响分类结果。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充； 使用合适的模型前面说了，过拟合主要是有两个原因造成的：数据太少+模型太复杂。所以，我们可以通过使用合适复杂度的模型来防止过拟合问题，让其足够拟合真正的规则，同时又不至于拟合太多抽样误差。 （PS：如果能通过物理、数学建模，确定模型复杂度，这是最好的方法，这也就是为什么深度学习这么火的现在，我还坚持说初学者要学掌握传统的建模方法。） 对于神经网络而言，我们可以从以下四个方面来限制网络能力： 网络结构 Architecture这个很好理解，减少网络的层数、神经元个数等均可以限制网络的拟合能力； 训练时间 Early stopping对于每个神经元而言，其激活函数在不同区间的性能是不同的： 当网络权值较小时，神经元的激活函数工作在线性区，此时神经元的拟合能力较弱（类似线性神经元）。 有了上述共识之后，我们就可以解释为什么限制训练时间（early stopping）有用：因为我们在初始化网络的时候一般都是初始为较小的权值。训练时间越长，部分网络权值可能越大。如果我们在合适时间停止训练，就可以将网络的能力限制在一定范围内。 限制权值 Weight-decay，也叫正则化（regularization）原理同上，但是这类方法直接将权值的大小加入到 Cost 里，在训练的时候限制权值变大。以 L2 regularization为例： 训练过程需要降低整体的 Cost，这时候，一方面能降低实际输出与样本之间的误差 ，也能降低权值大小。 增加噪声 Noise给网络加噪声也有很多方法： 在输入中加噪声：噪声会随着网络传播，按照权值的平方放大，并传播到输出层，对误差 Cost 产生影响。推导直接看 Hinton 的 PPT 吧： 在输入中加高斯噪声，会在输出中生成 的干扰项。训练时，减小误差，同时也会对噪声产生的干扰项进行惩罚，达到减小权值的平方的目的，达到与 L2 regularization 类似的效果（对比公式）。 在权值上加噪声在初始化网络的时候，用0均值的高斯分布作为初始化。Alex Graves 的手写识别 RNN 就是用了这个方法 Graves, Alex, et al. “A novel connectionist system for unconstrained handwriting recognition.” IEEE transactions on pattern analysis and machine intelligence 31.5 (2009): 855-868. It may work better, especially in recurrent networks (Hinton) 对网络的响应加噪声如在前向传播过程中，让默写神经元的输出变为 binary 或 random。显然，这种有点乱来的做法会打乱网络的训练过程，让训练更慢，但据 Hinton 说，在测试集上效果会有显著提升 （But it does significantly better on the test set!）。 结合多种模型简而言之，训练多个模型，以每个模型的平均输出作为结果。 从 N 个模型里随机选择一个作为输出的期望误差 ，会比所有模型的平均输出的误差 大（我不知道公式里的圆括号为什么显示不了）： 大概基于这个原理，就可以有很多方法了： Bagging简单理解，就是分段函数的概念：用不同的模型拟合不同部分的训练集。以随机森林（Rand Forests）为例，就是训练了一堆互不关联的决策树。但由于训练神经网络本身就需要耗费较多自由，所以一般不单独使用神经网络做Bagging。 Boosting既然训练复杂神经网络比较慢，那我们就可以只使用简单的神经网络（层数、神经元数限制等）。通过训练一系列简单的神经网络，加权平均其输出。 Dropout 在训练时，每次随机（如50%概率）忽略隐层的某些节点；这样，我们相当于随机从2^H个模型中采样选择模型；同时，由于每个网络只见过一个训练数据（每次都是随机的新网络），所以类似 bagging 的做法，这就是我为什么将它分类到「结合多种模型」中； 此外，而不同模型之间权值共享（共同使用这 H 个神经元的连接权值），相当于一种权值正则方法，实际效果比 L2 regularization 更好。 贝叶斯方法总结","link":"/2018/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%BF%87%E6%8B%9F%E5%90%88/"},{"title":"机器学习中的一些 tricks","text":"L2正则化的数学原理 L2正则化：To avoid parameters from exploding or becoming highly correlated, it is helpful to augment our cost function with a Gaussian prior: this tends to push parameter weights closer to zero, without constraining their direction, and often leads to classifiers with better generalization ability. If we maximize log-likelihood (as with the cross-entropy loss, above), then the Gaussian prior becomes a quadratic term 1 (L2 regularization): $$J_{reg}(\\theta)=\\dfrac{\\lambda}{2}[\\sum_{i,j}{W_1}{i,j}^2+\\sum{i’j’}{W_2}_{i,j}^2]$$ 可以证明： $$W_{ij} ∼ N (0; 1=λ)$$ 从两种角度理解正则化：知乎 RNN为什么容易出现梯度消失和梯度爆炸问题relu为啥能有效的解决梯度消失的问题很难理解为啥用relu能很好的解决梯度消失的问题，的确relu的梯度为1，但这也太简单了吧。。。所以得看看原论文 A Simple Way to Initialize Recurrent Networks of Rectified Linear Units","link":"/2018/04/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%AD%A3%E5%88%99%E5%8C%96/"},{"title":"深度学习-Dropout","text":"dropout的数学原理。 Dropout随机失活（Dropout）是一个简单又极其有效的正则化方法。该方法由Srivastava在论文Dropout: A Simple Way to Prevent Neural Networks from Overfitting中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。 在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。 关于dropout的理解:知乎上的回答 python代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&quot;&quot;&quot; 普通版随机失活: 不推荐实现 (看下面笔记) &quot;&quot;&quot;p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): &quot;&quot;&quot; X中是输入数据 &quot;&quot;&quot; # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &lt; p # 第一个随机失活遮罩,rand() [0,1)的随机数 H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &lt; p # 第二个随机失活遮罩 H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略)def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p out = np.dot(W3, H2) + b3 在上面的代码中，train_step函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据X创建一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩U1和U2加入进去。 注意：在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以p，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以p=0.5为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元x的输出，那么进行随机失活的时候，该神经元的输出就是px+(1-p)0，这是有1-p的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整x\\to px来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。 反向随机失活它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;&quot;&quot;反向随机失活: 推荐实现方式.在训练的时候drop和调整数值范围，测试时不做任何事.&quot;&quot;&quot;p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # 第一个随机失活遮罩. 注意/p! H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = (np.random.rand(*H2.shape) &lt; p) / p # 第二个随机失活遮罩. 注意/p! H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # 反向传播:计算梯度... (略) # 进行参数更新... (略)def predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了 H2 = np.maximum(0, np.dot(W2, H1) + b2) out = np.dot(W3, H2) + b3 在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献： Dropout paper by Srivastava et al. 2014. Dropout Training as Adaptive Regularization：“我们认为：在使用费希尔信息矩阵（fisher information matrix）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”","link":"/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Dropout/"},{"title":"算法-排序算法","text":"剑指offer 第二章-面试需要的基础知识 冒泡排序遍历数组，将最大的移到最后。逐渐缩小数组范围。 具体步骤： 第一个 for 循环 P=N-1：将最大的数放到最后面 第一趟冒泡，从第一个数开始逐个向后比较，将最大的数向后移动。 第二个 for 循环 P=N-2：将次大的书放到倒数第二个位置上 第二趟冒泡，从第一个数开始逐个向后比较，比较到第二个位置为止 复杂度分析： 最好情况,数组本身是顺序的，那么外循环只需要执行一次就会发现 flag 标识没变 T=O(N), 最差情况，数组本身是倒序的，那么外循环会执行 N-1 次，总的计算量是 (N-1 + N-2 +…+ 1), 则时间复杂度是 T=O(N^2) 12345678910111213141516171819202122232425262728293031323334353637void Bubble_sort(int num[], int N){ for ( int P=N-1; P&gt;0; P--) { int flag = 0; for (int i=0; i&lt;P; i++) { if(num[i] &gt; num[i+1]) { int temp = num[i]; num[i] = num[i+1]; num[i+1] = temp; flag = 1; } } if (flag == 0) break; // 如果flag 没有发生变化，证明上一次的遍历没有发生交换，说明剩余的数组已经排序好了，就不需要缩小数组范围，继续比较了。 }} 插入排序插入排序是增加增大比较范围。 从数组中先取一个数; 再取第二个数，与前面的进行比较，如果比前面的小，则交换位置。 再取第三个数，与前面的进行比较，直到找到比它大的数，插入。 依次循环。。 最好情况，本身就是个正序，外循环还是会执行完 N-1 次，但是内循环对应的只需要只执行一次，就会发现增加的数位置正确。所以总的复杂度还是 O(N) 最差情况，本身是个倒序的, 外循环会执行 N-1次， 内循环对应次数，所以总的是 O(N^2). 123456789101112131415161718192021222324252627282930313233343536373839void InsertOrder(int num[], int N){ for(int i=1; i&lt;N; i++) { for(int j=i; j&gt;0; j--) { int flag = 0; if (num[j] &lt; num[j-1]) { int temp = num[j]; num[j] = num[j-1]; num[j-1] = temp; flag = 1; } if(flag == 0) break; // 这里的 flag 是放在第二个循环里面的，因为找到了位置，就停下来了，后面的排好序的肯定比他大，就不用比较了 } }} 对比冒泡排序和插入排序 冒泡排序是从数组末尾到头逐渐有序，插入排序是从头到尾逐渐有序。 flag的位置不同。冒泡排序的外循环不一定能执行完，但内循环都会执行对应的次数。插入排序外循环一定会执行完，但内循环不一定。 堆排序通过选择排序引入堆排序 选择排序：遍历选择最小的元素，放入数组里面。然后在无序序列里面再遍历选择最小元素，循环。。。可见时间复杂度最坏情况：O(N2) 上图可以看出 for 循环要执行 N 次，总的复杂度取决于寻找最小元。看到最小元，自然想到最小堆。最小堆的根节点就是最小值。 堆排序算法1：针对选择排序的改进，怎么样找到最小元？可以将原序列调整成最小堆(时间复杂度是线性的 O(N))，然后依次弹出根结点 O(NlogN)。再将弹出的序列复制到原序列里面 O(N)。可见时间复杂度为：T(N)=O(N)+O(Nlog(N))+O(N)=O(Nlog(N)) 但是原数组占有O(N)内存，调整成最小堆后弹出的序列存储也需要O(N)内存，因此，空间复杂度上需要额外的O(N) 堆排序算法2先调整成最大堆O(N),再将根结点与最后一个元素交换位置O(logN)，然后再将前N-1个元素调整成最大堆，依次循环。。最后直接就是排序的结果，而不用重新占用内存空间。时间复杂度：O(N)+O(Nlog(N))…但陈姥姥给的是下面这样…so？ 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//堆排序/*因为堆排序是完全二叉树，用数组表示的，所以可以把原始序列看成树，然后进行调整*/void HeapSort(int num[], int N) { /*build max heap*/ int i,temp; for (i = 1; i &lt; N; i++) { //从数组下标为1的元素开始，类似于最大堆的插入一样，依次与父节点比较 temp = num[i];// 因为i一直在变化，因此把他保存 for (; num[i] &gt; num[(i-1)/2]; i = (i - 1) / 2) num[i] = num[(i - 1) / 2]; //把比num[i]小的父节点往下一层移 num[i] = temp; } /*delete max and swaps the max with the last one of the unoder sequence*/ int j,k, tempp; for (j = N - 1; j &gt; 0; j--) { tempp = num[0]; num[0] = num[j]; num[j] = tempp; for (k = 1; k &lt; j; k++) { //从数组下标为1的元素开始，类似于最大堆的插入一样，依次与父节点比较 tempp = num[k];// 因为i一直在变化，因此把他保存 for (; num[k] &gt; num[(k - 1) / 2]; k = (k - 1) / 2) num[k] = num[(k - 1) / 2]; //把比num[i]小的父节点往下一层移 num[k] = tempp; } }} 快速排序算法概述：分而治之（递归） 快速排序的细节很重要： 选主元：中分显然是最好的 怎么分成两个独立子集 最好情况： 每次选择的主元位置正好在中分位置， T(N) = O(NlogN) 选主元 随机选主元：随机函数也很浪费时间。。 选头中尾三个数的中位数 这里主元是 12345678910111213141516171819202122232425262728293031// 主元所在的位置int Center = (Left + Right) / 2;// 比较 左中右三个数，并排序if (A[Left] &gt; A[Center]) Swap (&amp;A[Left], &amp;A[Center]);if (A[Left] &gt; A[Right]) Swap (&amp;A[Left], &amp;A[Right]);if (A[Center] &gt; A[Right]) Swap (&amp;A[Center], &amp;A[Right]);// 再耍个小聪明， 将 pivot 藏在最右边Swap (&amp;A[Center], &amp;A[Right-1]);// 只需考虑 A[Left+1]...A[Right-2]return A[Right-1]; // 返回 pivot 子集划分调用过上面的 Median3 函数之后得到的数组是： {8, 1, 4, 9, 0, 3, 5, 2, 7, 6}， Left 和 Right 不用考虑了，已经在正确的子集了，pivot 也放在了剩下部分的最右边。 这里 i j 是两个指针，其实就是元素下标。当两边指针i遇到比pivot大式，停止。转去考虑右边的指针j，当j遇到小于pivot的数时也停止。此时两边都发出红色警告（也就是左边大于pivot， 右边小于pivot），则交换两边的数。 递归：对左子集重复上面的操作，对右子集重复上面的操作。 快速排序为什么快？ 因为选择了一个主元，然后划分完子集后，这个主元就处于它最终所在的位置上。 比如插入排序的数的位置都是临时的，插入排序新增加一个数，从后往前逐渐对比，发现比前面的小的话，相当于把前面的数都往后挪。 如果有两元素正好等于 pivot 怎么办？ 停下来交换，这样的目的是让 pivot 最终的位置更靠近数组中分的位置。之前做过时间复杂度分析时，我们知道这样的时间复杂度是最小的。 小规模数据的处理 当递归的数据规模较小时，则停止递归，使用插入排序。 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165#include&lt;stdio.h&gt;#include&lt;malloc.h&gt;#include&lt;stdbool.h&gt;void InsertSort(int num[], int N) { int i, j, temp; for (i = 1; i &lt; N; i++) { for (j = i; j &gt; 0; j--) { int flag = 0; if (num[j] &lt; num[j - 1]) { temp = num[j]; num[j] = num[j - 1]; num[j - 1] = temp; flag = 1; } if (flag == 0) break; } }}/*选主元*/void swap(int num[],int m, int n) { int temp; temp = num[m]; num[m] = num[n]; num[n] = temp;;}int Median3(int num[], int left,int right) { int median = (left + right) / 2; if (num[left] &gt; num[median]) swap(num,left,median); if (num[left] &gt; num[right]) swap(num, left, right); if (num[median] &gt; num[right]) swap(num, right, median); swap(num,median,right-1); return num[right - 1];}void Quick_sort(int num[],int left,int right) { int Cutoff = 10; if (Cutoff &lt;= right - left) { int pivot; pivot = Median3(num, left, right); int i = left, j = right - 1; for (;;) { while (num[i++]&lt;pivot) {} while (num[j--]&gt;pivot) {} if (i &lt; j) swap(num,i,j); else break; } swap(num,i,right-1); Quick_sort(num, left, i - 1); Quick_sort(num, i + 1, right); } else InsertSort(num + left, right - left + 1); //num+left这种写法？？}void QuickSort(int num[], int N){ Quick_sort(num, 0, N - 1);}int main() { int N, i, j; scanf_s(&quot;%d&quot;, &amp;N); int *num = (int*)malloc(N * sizeof(int)); for (i = 0; i &lt; N; i++) { scanf_s(&quot;%d&quot;, &amp;num[i]); } QuickSort(num, N); //int pivot = Median3(num, 0, N - 1); //printf(&quot;%d &quot;, pivot); printf(&quot;%d&quot;, num[0]); for (j = 1; j &lt; N; j++) printf(&quot; %d&quot;, num[j]);} 在此过程中出现的问题： 在快速排序的C代码出现了“段错误”Segmentation fault (core dumped) 错误代码： 1234567891011121314151617181920212223242526272829303132333435363738394041void Quick_sort(int num[],int left,int right) { int Cutoff = 0; if (Cutoff &lt;= right - left) { int pivot; pivot = Median3(num, left, right); int i = left, j = right - 1; for (;;) { while (num[++i]&lt;pivot) {} while (num[--j]&gt;pivot) {} if (i &lt; j) swap(num,i,j); else break; } swap(num,i,right-1); Quick_sort(num, left, i - 1); Quick_sort(num, i + 1, right); } else InsertSort(num + left, right - left + 1); //num+left这种写法？？} 出现bug，在Linux系统下使用gdb进行调试： 分析出现bug原因： 快速排序，递归的最后会出现 left = right-1 = 0,此时num[]中只有一个元素num[0],因此进入while (num[++i]&lt;pivot)循环时，++i会导致内存越界。 解决办法是cutoff&gt;0就好了，当元素较少的时候采用插入排序~","link":"/2018/07/06/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"title":"论文笔记, Attention Is All You Need","text":"Attention Is All You Need 1. paper reading1.1 IntroductionRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. RNN模型有两个很致命的缺点： $$y_t=f(y_{t-1},x_t)$$ 一是无法解决长句子的长期依赖的问题（这是因为反向传播，loss的梯度很难传递到比较靠前的位置，造成前面的词对整体的影响偏小，[Gradient flow in recurrent nets: the difficulty of learning long-term dependencies](http://www.bioinf.jku.at/publications/older/ch7.pdf)）； 二是计算无法并行化的问题（后一个时刻的计算依赖于前一个时刻的计算），导致训练速度很慢。 Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network. Attention机制能够有效解决RNN无法长时间依赖的问题，但是对于无法并行化计算的问题依旧存在。 2. Background2.1 Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]2.2 Self-attentionSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. A structured self-attentive sentence embedding 2.3 End-to-end memory networksEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks. End-to-end memory networks 2.4 TransformerTransformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. 本文主要和以下三篇文章对比： Neural GPUs learn algorithms Neural machine translation in linear time Convolutional sequence to sequence learning Model Architecture在以往的encoder-decoder模型中： the encoder maps an input sequence of symbol representations $(x_1,…,x_n)$ to a sequence of continuous representations $z = (z_1,…,z_n)$. Given z, the decoder then generates an output sequence $(y_1,…,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 以往的attention虽然也能解决long dependecy的问题，但是受制于RNN的原因，每一步的计算必须在上一时间步完成后进行。因此无法并行计算。 Transformer 也是由 encoder 和 decoder 组成。 Encoder其中 Encoder 由6个完全相同的layer堆叠（stack）而成。每一层layer由两个 sub-layer 组成，分别是 multi-head self-attention mechanism 和 point-wise fully connected feed-forward network. 每一个 sub-layer 应用一个残差连接（residual connection）,然后再连接一个 normalization 层。 Decoder跟 encoder 非常类似，同样由6由6个完全相同的layer堆叠而成，但是每一层有3个 sub-layer， 增加了一个 multi-head attention. 同样的也有残差链接和normalization层。 对 self-attention 进行了修改，masking: We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 目的应该就是让下一个t时刻的生成词只依赖于t时刻之前的词。 AttentionReally love this short description of attention: An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Scaled Dot-Product Attention queries: $Q\\in R^{n\\times d_k}$ keys: $K\\in R^{n\\times d_k}$ values: $V\\in R^{n\\times d_v}$ 计算向量內积作为相似度，并使用softmax计算权重，然后加权求和。 这种 attention 其实也很常见了，这里 google 算是给这种结构一个官方的名字吧。 论文中作者还对比了比较常用的另一种attention机制， additive attention (Neural Machine Translation by Jointly Learning to Align and Translate 这篇非常经典的文章中提出的)，additive 是使用的前馈神经网络来计算 (具体公式可以看这里cs224d-lecture10 机器翻译和注意力机制). 虽然从计算复杂度上来讲，两者是差不多的，但在实际应用中 dot-product 更快，而且空间复杂度更低，因为可以通过矩阵优化计算。关于attention机制的对比可参考Massive Exploration of Neural Machine Translation Architectures 有一点需要注意的是，这里使用了归一化，也就是 Scaled. 当 $d_k$ 很大时， additive attention 的效果要优于 dot-product attention， 作者怀疑(suspect)是当 $d_k$ 太大时，通过 softmax 计算得到的权重都会很接近0或1,导致梯度很小。 $q\\cdot k=\\sum_{i=1}^{d_k}q_ik_i$ 当 $d_k$ 很大时，$q\\cdot k$ 的方差也会很大。 Multi-Head Attention 接下来按照整个模型的数据流过程来介绍模型中的每一个模块。 Components and Training前面三部分 Encoder, Decoder, Attention 组成了 Transformer 模型的基本架构。其中具体细节，以及 Training 实现过程将通过代码实现。 EncoderStage1Training data and batchingWMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. 作者使用了： byte-pair 这篇论文中有介绍：Massive exploration of neural machine translation architectures word-piece Google’s neural machine translation system: Bridging the gap between human and machine translation 这里我将使用创新工厂举办的 challenge.ai 的中英文比赛数据：https://challenger.ai/datasets/translation 这里暂时先不管数据预处理，在模型中使用占位符 placeholder. 12345678910111213# add placeholdersself.input_x = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len])self.input_y = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len])# define decoder inputsself.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:1])*2, self.input_y[:,:-1]],axis=-1) # 2:&lt;S&gt; 这里的sentence_len 指的是源语言句子的最大长度和目标语言句子的最大长度。长度不足的需要zero padding. decoder 中self-attention的 query, keys, values 都是相同的，初始值是随机初始化的，shape 与 self.input_y 一致即可。 Embeddingwe use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. In our model, we share the same weight matrix between the two embedding layers. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def embedding(inputs, vocab_size, num_units, zero_pad=True, scale=True, reuse=None): &quot;&quot;&quot; :param inputs: A `Tensor` with type `int32` or `int64` containing the ids to be looked up in `lookup table`. shape is [batch, sentence_len] :param vocab_size: vocabulary size :param num_units: Number of embedding hidden units. in the paper, it is called d_model :param zero_pad: If True, all the values of the fist row (id 0) should be constant zeros. :param scale: If True. the outputs is multiplied by sqrt num_units. :param reuse: Boolean, whether to reuse the weights of a previous layer by the same name. :return: A `Tensor` with one more rank than inputs's. The last dimensionality should be `num_units`. &quot;&quot;&quot; with tf.variable_scope(&quot;embedding-layer&quot;, reuse=reuse): embedding = tf.get_variable(&quot;embedding&quot;, [vocab_size, num_units], initializer=xavier_initializer()) if zero_pad: embedding = tf.concat([tf.zeros([1, num_units]), embedding[1:, :]], axis=0) # index=0 for nil word output = tf.nn.embedding_lookup(embedding, inputs) # [batch, sentence_len, num_units] if scale: output = output * np.sqrt(num_units) return output 通常embedding我们在写的参数输入 vocab_size 和 num_units(也就是 embed_size)，但机器翻译中设计到两种语言，直接定义一个函数，并将input作为输入会让程序更简洁吧。。 这里将vocabulary 中index=0的设置为 constant 0, 也就是作为 input 中的 zero padding 的词向量。 归一化，除以 np.sqrt(num_units). 不懂为何要这么做？有论文研究过吗？ position encoding pos 是word在句子中的位置， i 是对应 $d_{model}$ 词向量中的第 i 维。 That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. 也就是说，位置编码的每个维度对应于正弦曲线。波长形成从2π到10000·2π的几何级数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127def position_encoding_mine(n_position, d_model): &quot;&quot;&quot; Init the sinusoid position encoding table. :param n_position: the lenght of sentence :param d_model: the same with embedding :return: &quot;&quot;&quot; # keep dim -1 for padding token position encoding zero vector # pos=-1 用于 padded zero vector encoding = np.zeros([n_position, d_model], np.float32) for pos in range(1, n_position): for i in range(0, d_model): encoding[pos, i] = pos /np.power(10000, 2.*i/d_model) encoding[1:-2, 0::2] = np.sin(encoding[1:-2, 0::2]) # dim 2i encoding[1:-2, 1::2] = np.cos(encoding[1:-2, 1::2]) # dim 2i+1 return encodingdef positional_encoding(inputs, num_units, zero_pad=True, scale=True, scope=&quot;positional_encoding&quot;, reuse=None): '''Sinusoidal Positional_Encoding. Args: inputs: A 2d Tensor with shape of (N, T). num_units: Output dimensionality zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper) scope: Optional scope for `variable_scope`. reuse: Boolean, whether to reuse the weights of a previous layer by the same name. Returns: A 'Tensor' with one more rank than inputs's, with the dimensionality should be 'num_units' ''' N, T = inputs.get_shape().as_list() # N means batch_size, T means the sentence length. with tf.variable_scope(scope, reuse=reuse): position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # [N, T] # First part of the PE function: sin and cos argument position_enc = np.array([ [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)] for pos in range(T)]) # [T, num_units] # Second part, apply the cosine to even columns and sin to odds. position_enc[:, 0::2] = np.sin(position_enc[:, 0::2]) # dim 2i position_enc[:, 1::2] = np.cos(position_enc[:, 1::2]) # dim 2i+1 # Convert to a tensor lookup_table = tf.convert_to_tensor(position_enc, dtype=tf.float32) if zero_pad: lookup_table = tf.concat((tf.zeros(shape=[1, num_units]), lookup_table[1:, :]), axis=0) outputs = tf.nn.embedding_lookup(lookup_table, position_ind) # [N, T, num_units] if scale: outputs = outputs * num_units**0.5 return outputs 关于 position encoding 的解释，可以参考这篇blog The Transformer – Attention is all you need. In RNN (LSTM), the notion of time step is encoded in the sequence as inputs/outputs flow one at a time. In FNN, the positional encoding must be preserved to represent the time in some way to preserve the positional encoding. In case of the Transformer authors propose to encode time as sine wave, as an added extra input. Such signal is added to inputs and outputs to represent time passing. In general, adding positional encodings to the input embeddings is a quite interesting topic. One way is to embed the absolute position of input elements (as in ConvS2S). However, authors use “sine and cosine functions of different frequencies”. The “sinusoidal” version is quite complicated, while giving similar performance to the absolute position version. The crux is however, that it may allow the model to produce better translation on longer sentences at test time (at least longer than the sentences in the training data). This way sinusoidal method allows the model to extrapolate to longer sequence lengths. 说真的，还是不太理解。。。 可视化 encoding 矩阵: 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport matplotlib.pyplot as pltnum_units = 100sentence_len = 10i = np.tile(np.expand_dims(range(num_units), 0), [sentence_len, 1]) # (100,)-&gt; (1, 100) -&gt;(10, 100)pos = np.tile(np.expand_dims(range(sentence_len), 1), [1, num_units]) #(10,)-&gt; (10, 1) -&gt; (10, 100)pos = np.multiply(pos, 1/10000.0)i = np.multiply(i, 2.0/num_units)matrix = np.power(pos, i)matrix[:, 1::2] = np.sin(matrix[:, 1::2])matrix[:, ::2] = np.cos(matrix[:, ::2])im = plt.imshow(matrix, aspect='auto')plt.show() Stage2scaled dot-product attention$$Attention(Q,K,V)=softmax\\dfrac{QK^T}{\\sqrt d_k}V$$ Multi-head attentionTransformer reduces the number of operations required to relate (especially distant) positions in input and output sequence to a O(1). However, this comes at cost of reduced effective resolution because of averaging attention-weighted positions. h = 8 attention layers (aka “heads”): that represent linear projection (for the purpose of dimension reduction) of key K and query Q into $d_k$-dimension and value V into $d_v$-dimension: $$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\\dots,h$$ 其中： $$W^Q_i, W^K_i\\in\\mathbb{R}^{d_{model}\\times d_k}, W^V_i\\in\\mathbb{R}^{d_{model}\\times d_v}, for\\ d_k=d_v=d_{model}/h = 64$$ scaled-dot attention applied in parallel on each layer (different linear projections of k,q,v) results in $d_v$-dimensional output. concatenate outputs of each layer (different linear projection; also referred as ”head”): Concat$(head_1,…,head_h)$ linearly project the concatenation result form the previous step: $$MultiHeadAttention(Q,K,V) = Concat(head_1,\\dots,head_h) W^O$$ where $W^0\\in\\mathbb{R}^{d_{hd_v}\\times d_{model}}$ 关于 attention 在模型中的应用，有三种情况 1.In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder。 2.The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. 3.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −1) all values in the input of the softmax which correspond to illegal connections. 总结下就是： Transformer 中的attention机制总共有三种情况： 1.encoder模块中的 self-attention，其中 queries, keys, values 都是来自 input_x, 也就是源语言的词表示。通过多层 multi-head attention, FFN, 得到最后的 input sentence 的向量表示，在没有使用RNN，CNN的情况下，其中的每个词都包含了其他所有词的信息，而且效果比 RNN，CNN 得到的向量表示要好。 2.encoder-encoder模块中的 attention. 其中 queries 来自上一个sub-layer, 也就是 decoder 中 masked multi-head attention 的输出，keys-values 来自 encoder 的输出。 3.decoder模块中的 self-attention，其中 queries, keys, values 都是来自于上一个 decoder 的输出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193def multiheadattention(q, k, v, d_model, heads, keys_mask=None, causality=None, dropout_keep_prob=0.1, is_training=True): &quot;&quot;&quot; multi scaled dot product attention :param q: A 3d tensor with shape of [batch, length_q, d_k]. :param k: A 3d tensor with shape of [batch, lenght_kv, d_k]. :param v: :param heads:An int. Number of heads. :param dropout_keep_prob: :param causality: If true, units that reference the future are masked. :return: &quot;&quot;&quot; # 1. Linear projections with tf.variable_scope('linear-projection-multiheads'): q_proj = tf.layers.dense(q, d_model) # [batch, lenght_q, d_model] k_proj = tf.layers.dense(k, d_model) # [batch, lenght_kv, d_model] v_proj = tf.layers.dense(v, d_model) # [batch, lenght_kv, d_model] with tf.variable_scope(&quot;multihead-attention&quot;): # d_k = d_v = d_model/heads if d_model % heads != 0: raise ValueError(&quot;Key\\values\\query depth (%d) must be divisible by&quot; &quot;the number of attention heads (%d)&quot; %(d_model, heads)) # 2. split and concat q_ = tf.concat(tf.split(q_proj, heads, axis=2), axis=0) # [batch*heads, length_q, d_k] k_ = tf.concat(tf.split(k_proj, heads, axis=2), axis=0) # [batch*heads, length_kv, d_k] v_ = tf.concat(tf.split(v_proj, heads, axis=2), axis=0) # [batch*heads, length_kv, d_v] # 3. attention score # outputs.shape=[batch*heads, length_q, length_kv] # 要理解这个矩阵运算，对一个keys的句子长度为length_kv,需要计算的其中的每一个词与query中每一个词的內积。所以最后的score是[length_q, lenght_kv] scalar = tf.rsqrt(d_model/heads) # 1/sqrt(d_k) outputs = tf.matmul(q_*scalar, k_, transpose_b=True) # [batch*heads, length_q, lenght_kv] # 4. mask if keys_mask is not None: # `y = sign(x) = -1` if `x &lt; 0`; 0 if `x == 0` or `tf.is_nan(x)`; 1 if `x &gt; 0`. key_masks = tf.sign(tf.abs(tf.reduce_sum(k, axis=-1))) # (batch, length_kv) key_masks = tf.tile(key_masks, [heads, 1]) # (batch*heads, length_kv) key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, q.get_shape()[1], 1]) # (batch*heads, length_q, length_kv) # def where(condition, x=None, y=None, name=None) # The `condition` tensor acts as a mask that chooses, based on the value at each # element, whether the corresponding element / row in the output should be taken # from `x` (if true) or `y` (if false). paddings = tf.ones_like(outputs) * (-2 ** 32 + 1) outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # [batch*heads, length_q, lenght_kv] # Causality = Future blinding # causality参数告知我们是否屏蔽未来序列的信息（解码器self attention的时候不能看到自己之后的那些信息）， # 这里即causality为True时的屏蔽操作。 if causality: diag_vals = tf.ones_like(outputs[0, :, :]) # [length_q, lenght_kv] tril = LinearOperatorLowerTriangular(diag_vals).to_dense() # [length_q, lenght_kv] 得到一个三角阵，下标index大于当前行的值都变为0 masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # [batch*heads, length_q, lenght_kv] paddings = tf.ones_like(masks) * (-2 ** 32 + 1) outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # [batch*heads, length_q, lenght_kv] # 将socre转换为概率 outpts = tf.nn.softmax(outputs) # Query Masking query_mask = tf.sign(tf.abs(tf.reduce_sum(q, axis=-1, keepdims=False))) # [batch, lenght_q] query_mask = tf.tile(query_mask, [heads, 1]) # [batch*heads, length_q] # 目的是为了让query和outputs保持形状一致 query_mask = tf.tile(tf.expand_dims(query_mask, axis=-1), [1, 1, tf.shape(k)[-1]]) # [batch*heads, length_q, length_kv] paddings = tf.ones_like(outputs) * (-2 ** 32 + 1) outputs = tf.where(tf.equal(query_mask, 0), paddings, outputs) # [batch*heads, length_q, length_kv] # Dropout if is_training: outputs = tf.layers.dropout(outputs, dropout_keep_prob, ) # weights sum outputs = tf.matmul(outputs, v_) # [batch*heads, length_q, k_v] # restore shape outputs = tf.concat(tf.split(outputs, heads, axis=0), axis=-1) #[batch,length_q, k_v*heads] = [batch, lenght_q, d_model] # Residual connection outputs += q # [batch, lenght_q, d_model] # Normalize outputs = Normalize(outputs) return outputs # [batch, length_q, d_model] 关于代码的详细解析，可以看这篇blog 机器翻译模型Transformer代码详细解析. Stage3: Position-wise Feed-Forward Networks$$FFN(x) = MAX(0, xW_1+b_1)W_2+b_2$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109def position_wise_feed_forward(inputs, num_units1=2048, num_units2=512, reuse=None): &quot;&quot;&quot; Point-wise feed forward net. :param inputs: A 3D tensor with shape of [batch, length_q, d_model] :param num_units1: A integers. :param num_units2: A integers :param reuse: Boolean, whether to reuse the weights of a previous layer by the same name. :return: A 3d tensor with the same shape and dtype as inputs &quot;&quot;&quot; with tf.variable_scope(&quot;feed-forward-networks&quot;): # inner layers params1 = {&quot;inputs&quot;:inputs, &quot;filters&quot;:num_units1, &quot;kernel_size&quot;:1, &quot;activation&quot;:tf.nn.relu, &quot;use_bias&quot;:True, &quot;strides&quot;:1} outputs = tf.layers.conv1d(**params1) # readout layer params2 = {&quot;inputs&quot;:outputs, &quot;filters&quot;:num_units2, &quot;kernel_size&quot;:1, &quot;activation&quot;:None, &quot;use_bias&quot;:True, &quot;strides&quot;:1} outputs = tf.layers.conv1d(**params2) # residual connection outputs += inputs # Normalize outputs = Normalize(outputs) return outputsdef position_wise_feed_forward_mine(inputs, num_units1=2048, num_units2=512, reuse=None): with tf.variable_scope(&quot;feed-forward-networks&quot;): W1 = tf.get_variable(&quot;weight1&quot;, [inputs.get_shape()[-1], num_units1],initializer=xavier_initializer()) b1 = tf.get_variable('bias1', [num_units1], initializer=tf.constant_initializer(0.1)) outputs = tf.einsum('aij,jk-&gt;aik', inputs, W1) + b1 # [batch, length_q, num_units1] W2 = tf.get_variable(&quot;weight1&quot;, [outputs.get_shape()[-1], num_units2], initializer=xavier_initializer()) b2 = tf.get_variable('bias1', [num_units2], initializer=tf.constant_initializer(0.1)) outputs = tf.einsum('aij,jk-&gt;aik', inputs, W2) + b2 # [batch, length_q, num_units1] # residual connection outputs += inputs # Normalize outputs = Normalize(outputs) return outputs encoder 各模块组合在一起1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677def _encoder(self): with tf.variable_scope(&quot;encoder&quot;): # 1. embedding with tf.variable_scope(&quot;embedding-layer&quot;): self.enc = embedding(inputs=self.input_x, vocab_size=self.vocab_size_cn, num_units=self.d_model, scale=True) # [batch, sentence_len, d_model] # 2. position encoding with tf.variable_scope(&quot;position_encoding&quot;): encoding = position_encoding_mine(self.enc.get_shape()[1], self.d_model) self.enc *= encoding # 3.dropout self.enc = tf.layers.dropout(self.enc, rate=self.dropout_keep_prob, training=self.is_training) # 4. Blocks for i in range(self.num_layers): with tf.variable_scope(&quot;num_layer_{}&quot;.format(i)): # multihead attention # encoder: self-attention self.enc = multiheadattention(q=self.enc, k=self.enc, v=self.enc, d_model=self.d_model, heads=self.heads, causality=False, dropout_keep_prob=self.dropout_keep_prob, is_training=True) # Feed Froward self.enc = position_wise_feed_forward(self.enc, num_units1= 4*self.d_model, num_units2= self.d_model, reuse=False) return self.enc Decoderdecoder 模块中 self-attention 的初始输入： 12345# define decoder inputsself.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:1])*2, self.input_y[:,:-1]],axis=-1) # 2:&lt;S&gt; 与encoder 不同的是，分为 encoder-decoder attention 和 self-attention. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def _decoder(self): with tf.variable_scope(&quot;decoder&quot;): # embedding self.dec = embedding(self.decoder_inputs, vocab_size=self.vocab_size_en, num_units=self.d_model) # [batch, sentence_len, d_model] # position decoding encoding = position_encoding_mine(self.dec.get_shape()[1], self.d_model) self.dec *= encoding # blocks for i in range(self.num_layers): with tf.variable_scope(&quot;num_layers_{}&quot;.format(i)): # self-attention with tf.variable_scope(&quot;self.attention&quot;): self.dec = multiheadattention(q=self.dec, k=self.dec, v=self.dec, d_model=self.d_model, heads=self.heads, keys_mask=True, causality=True) # encoder-decoder-attention with tf.variable_scope(&quot;encoder-decoder-attention&quot;): self.dec = multiheadattention(q=self.dec, k=self.enc, v=self.enc, d_model=self.d_model, heads=self.heads, keys_mask=True, causality=True) self.dec = position_wise_feed_forward(self.dec, num_units1= 4*self.d_model, num_units2= self.d_model) # [batch, sentence_len, d_model] return self.dec Optimizer 123456789def add_train_op(self): self.optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.9, beta2=0.98, epsilon=1e-9) self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step) return self.train_op RegularizationResidual Dropoutlabel smoothingDuring training, we employed label smoothing of value \u000fls = 0:1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 12345678910111213141516171819def label_smoothing(inputs, epsilon=0.1): &quot;&quot;&quot; Applies label smoothing. See https://arxiv.org/abs/1512.00567 :param inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary. :param epsilon: Smoothing rate. For example, &quot;&quot;&quot; K = inputs.get_shape().as_list()[-1] # number of channels return ((1-epsilon) * inputs) + (epsilon/K) Reference: Attention and Augmented Recurrent Neural Networks The Transformer – Attention is all you need. 机器翻译模型Transformer代码详细解析","link":"/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/"},{"title":"论文笔记 - Attention 综述","text":"Attention and Augmented Recurrent Neural Networks 循环神经网络是深度学习的根基之一，她允许神经网络对序列数据进行处理，比如文本，语音和视频。 这句话形容特别贴切，意思就是rnn能将序列转换成包含了理解，语义的表示。 They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch! 基本的 rnn 在解决 long sequence 时非常挣扎，其变体 LSTM 有效的解决这一问题（但问题依然存在）。 随着时间的发展，出现了各种 augument RNNs. 其中以下4种 stand out exciting. 这些变体都是 RNN 有力的拓展，更令人惊奇的是，他们还能有效的结合在一起，似乎只是广阔空间中的一点。除此之外，他们都依赖于同样的 underlying trick — attention. Neural Turing MachinesNeural Turing Machines 神经图灵机将 RNNs 和外部 memory bank 结合在一起。向量用来表示神经网络中的自然语言，memory 是向量的数组。 如上图所示，能够看出其原理是将每一个词的向量表示存储在 memory 中。那么从 memory 中读、以及写入 memory 是怎么操作的呢？ 最大的难点在于让整个过程可微(differentiable).特别的是，我们希望在读出和写入的位置具有差异性，以便我们可以知道从哪儿读和写。这很棘手(tricky)，因为 memory 地址基本上是离散的。 NMTs 采取了一个非常聪明的方法，每一步读和写都包括 memory 中的所有位置，只是对于不同的位置，读和写的程度不同。 举个例子，这里我们只关注 reading. RNN 输出一个 “attention distribution”，表示对于 memory 不同位置的读取程度。这样，读入的操作可以看作加权求和。 $$r \\leftarrow \\sum_ia_iM_i$$ 类似的，在读入的过程中也是对 memory 中所有位置。再一次，输出一个 “attention distribution” 用来表示对每个位置的写入程度。我们在一个 memory 位置获得新的 value 是旧的 memory 和写入值的 凸结合(convex combination). $$M_i \\leftarrow a_iw+(1-a_i)M_i$$ 其中 $w$ 是写入值(write value)。 但是 NTMs 如何确定去注意 memory 中的那一个位置呢？ 他们使用了两种不同的方法的结合： content-based attention 和 location-based attention. 前者允许 NMTs 通过匹配 memory 中的每一个位置，并 focus on 最 match 的位置。后者允许 memory 中的相对移动，保证 NMT 能循环。 这种读和写的能力允许 NTM 执行许多简单的算法，而这些算法以前超越了神经网络。 例如，他们可以在 memory中存储一个长序列，然后遍历它，反向重复它。 当他们这样做时，我们可以看他们读和写的地方，以更好地理解他们在做什么： 关于 repeat copy 的实验，可以参考这篇论文Show, attend and tell: Neural image caption generation with visual attention 他们能够模仿一个 lookup table,甚至可以 sort numbers(althought they kind of cheat). 但另一方面，他们仍然不能做很多基本的事儿，比如 add or multiply numbers. 自从 NTM 这篇论文之后，又出现了很多相同方向的令人 exciting 的文章. The Neural GPU 4 overcomes the NTM’s inability to add and multiply numbers. Zaremba &amp; Sutskever 5 train NTMs using reinforcement learning instead of the differentiable read/writes used by the original. Neural Random Access Machines 6 work based on pointers. Some papers have explored differentiable data structures, like stacks and queues [7, 8]. memory networks [9, 10] are another approach to attacking similar problems. Code Neural Turing Machine Taehoon Kim’s Neural GPU publication TensorFlow Models repository Memory Networks, Taehoon Kim’s 关于这篇 paper 真的很难看懂， Attention Interfaces当我翻译一个句子时，我特别注意我正在翻译的单词。 当我录制录音时，我会仔细聆听我正在积极写下的片段。 如果你要求我描述我正在坐的房间，那么我会浏览我正在描述的物体。 神经网络能够通过 attention 机制完成上述行为，也就是 focusing on 给出信息的部分内容。举个例子，一个 RNN 能够 attend over 另一个 RNN 的输出，并在每一个时间步， focus on 另一个 RNN 的不同的位置。 为了让 attention 可微，我们采用跟 NTM 同样的方法，focus 所有的位置，每个位置的程度不同。 上图中颜色的深浅表示注意的程度。 其中 attention distribution 是由 content-based attention 生成的。具体过程还是看英文描述吧： The attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution. 最经典的使用 RNNs 之间的 attention 就是机器翻译了，Neural machine translation by jointly learning to align and translate. 传统的 seq2seq 模型通过 RNN 将整个输入序列转化为单个向量(也就是最后一层的隐藏状态)，然后将其展开得到输出(word by word). 注意力机制避免了这一点，它允许 RNN 在生成输出时，能看到所有输入中的每一个词，并且根据他们之间的相关性，来选择性注意部分词。 原图是可以操作的，大牛们的技术真是厉害。。。可视化也很6。。 这种类型的 RNN 还有很多其他的应用，比如在 语音识别上的使用Listen, Attend and Spell, 使用一个 RNN 来处理音频，然后用另一个 RNN 来遍历它，并在生成一个抄本(transcript)时重点关注相关的部分。 这个图看不出来，不能截出那种效果。 其实可以看出对语音的识别，在生成对应的词时更关注的是对应的音频，并不像文本那种需要长时间依赖。 还有很多其他的应用： parsing tree:Grammar as a foreign language conversational modeling:A Neural Conversational Model image captioning: attention interface 也可以是 CNN 和 RNN 之间的，它允许 RNN 在每一个时间步生成文本时能关注图像中的不同的位置。 Then an RNN runs, generating a description of the image. As it generates each word in the description, the RNN focuses on the conv net’s interpretation of the relevant parts of the image. We can explicitly visualize this: 图片来自于Show, attend and tell: Neural image caption generation with visual attention More broadly, attentional interfaces can be used whenever one wants to interface with a neural network that has a repeating structure in its output. Adaptive Computation time Standard RNNs do the same amount of computation for each time step. This seems unintuitive. Surely, one should think more when things are hard? It also limits RNNs to doing O(n) operations for a list of length n. 标准的 RNN 在每一个时间步都做着相同的大量的计算。它貌似是很合理的，但当处理的信息很复杂时，是否可以在一个时间步考虑更多？这样同样的计算量的方式限制了 RNN 在处理长度为 n 的序列时，其复杂度也为 O(n). Adaptive Computation Time [15] is a way for RNNs to do different amounts of computation each step. The big picture idea is simple: allow the RNN to do multiple steps of computation for each time step. 自适应计算时间步(ACT) 能让 RNN 在每一个时间步做不同的计算量. 它的大致原理很简单：允许 RNN 在每一个时间步做多步运算。 In order for the network to learn how many steps to do, we want the number of steps to be differentiable. We achieve this with the same trick we used before: instead of deciding to run for a discrete number of steps, we have an attention distribution over the number of steps to run. The output is a weighted combination of the outputs of each step. 为了让网络学习得到每一个时间步的计算步数，我们希望这个计算步数是可微的（也就是把其步数当作学习得到的参数，然后通过反向传播来学习这个参数）。为了实现这个 trick，我们采用类似之前 NTM 的方式，相比每次计算一个离散的步数，我们使用注意力分布的机制来选择步数，输出的结果是所有步的加权求和。 There are a few more details, which were left out in the previous diagram. Here’s a complete diagram of a time step with three computation steps. 上图中还有更多的细节。下图是 RNN 中的在一个时间步中有3个步数的计算量。 That’s a bit complicated, so let’s work through it step by step. At a high-level, we’re still running the RNN and outputting a weighted combination of the states: 这个图看起来有点复杂，我们会一步一步的解释它。在更高的层次，我们依然运行 RNN 并输出一个状态的加权之和。 S 表示 RNN 中的隐藏状态。 The weight for each step is determined by a “halting neuron.” It’s a sigmoid neuron that looks at the RNN state and gives a halting weight, which we can think of as the probability that we should stop at that step. 每一步的权重由一个“停止神经元”确定。这是一个sigmoid神经元，用来关注当前的 RNN 的状态以及赋予它一个权重。我们可以看作是这一步是否应该停止的概率。 We have a total budget for the halting weights of 1, so we track that budget along the top. When it gets to less than epsilon, we stop 我们设总的停止权重为 1,依次减去每步输出的 halt 值，直到剩余的 halt 值小于 epsilon 后就停止。 When we stop, might have some left over halting budget because we stop when it gets to less than epsilon. What should we do with it? Technically, it’s being given to future steps but we don’t want to compute those, so we attribute it to the last step. 当我们结束后，应该还会遗留一些停止值，因为我们在停止值小于epsilon时停止的。我们该怎么处理这些剩余的停止值？从技术上讲，它们应该传递到后面的计算步骤中去，但我们不想计算这些值，所以我们就把这些剩余的停止值归总到最后一步。 When training Adaptive Computation Time models, one adds a “ponder cost” term to the cost function. This penalizes the model for the amount of computation it uses. The bigger you make this term, the more it will trade-off performance for lowering compute time. 当训练 ACT 模型时，需要给损失函数加上一个惩罚项。用来惩罚整个模型中的总的计算量。这一项越大时，它会在模型性能和计算量的折衷中倾向与减小计算量。 Code： The only open source implementation of Adaptive Computation Time at the moment seems to be Mark Neumann’s (TensorFlow). reference: Attention and Augmented Recurrent Neural Networks","link":"/2018/06/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-%E7%BB%BC%E8%BF%B0/"},{"title":"深度学习-权重初始化","text":"为什么要权重初始化 Xavier初始化的推导 权重初始化In order to avoid neurons becoming too correlated and ending up in poor local minimize, it is often helpful to randomly initialize parameters. 为了避免神经元高度相关和局部最优化，常常需要采用随机初始化权重参数，最常用的就是Xavier initiazation. 为什么我们需要权重初始化？如果权重参数很小的话，输入信号在前向传播过程中会不断减小（在0到1之间），那么每一层layer都会使得输入变小。同样的道理，如果权重参数过大的话，也会造成前向输入越来越大。这样会带来什么样的后果呢？以激活函数sogmoid为例： 如果以sigmoid为激活函数，我们可以发现，在每一层layer输出 $W^Tx$ ，也就是激活函数的输入，其值越接近于0的时候，函数近似于线性的，因而就失去了非线性的性质。这种情况下，我们就失去了多层神经网络的优势了。 如果初始权重过大，在前向传播的过程中，输入数据的方差variance会增长很快。怎么理解这句话？ 以one layer为例，假设输入是 $x\\in R^{1000}$, 线性输出是 $y\\in R^{100}$. $$y_j=w_{j,1}x_1+w_{j,2}x_2+…+w_{(j,1000)}x_{1000}$$ x可以看作是1000维的正态分布，每一维 $x_i\\sim N(0,1)$, 如果 $w_j$值很大，比如 $w_j=[100,100,…,100]$，那么输出神经元 $y_i$ 的方差就是10000，所以就会很大,均值还是0. 那么激活函数的输入很有可能是一个远小于-1或远大于1的数，通过激活函数所得的值会非常接近于0或者1，也就是隐藏层神经元处于饱和状态(saturated)，其梯度也就接近于0了。 所以初始化权重狠狠狠重要。那么应该如何初始化呢，也就是需要保证经过每一层layer，要保证线性输出的方差保持不变。这样就可以避免数值溢出，或是梯度消失。 Xavier Initialization我们的目的是保持线性输出的方差不变。 以线性输出的一个神经元为例，也就是y的一个维度： $$y_j=w_{j,1}x_1+w_{j,2}x_2+…+w_{j,N} x_N+b$$ 其方差： $$var(y_j) = var(w_{j,1}x_1+w_{j,2}x_2+…+w_{j,N} x_N+b)$$ 其中每一项根据方差公式可得： $$var(w_{j,i}x_i) = E(x_i)^2var(w_{j,i}) + E(w_{j,i})^2var(xi) + var(w_{j,i})var(x_i)$$ 来自维基百科： https://en.wikipedia.org/wiki/Variance 其中我们假设输入和权重都是来自于均值为0的正态分布。 $$var(w_{j,i}x_i)=var(w_{j,i})var(x_i)$$ 其中b是常量，那么： $$var(y_j) = var(w_{j,1})var(x_1) + … + var(w_{j,N})var(x_N)$$ 因为 $x_1,x_2,..,x_N$ 都是相同的分布，$W_{j,i}$ 也是，那么就有： $$var(y_j) = N * var(w{j,i}) * var(x_i)$$ 可以看到，如果输入神经元数目N很大，参数权重W的值也很大的话，会造成线性输出的值的方差很大。 我们需要保证 $y_j$ 的方差和 $x_j$ 的方差一样，所以： $$N*var(W_{j,i})=1$$ $$var(W_{j,i})=1/N$$ There we go! 这样我们就得到了Xavier initialization的初始化公式，也就是说参数权重初始化为均值为0，方差为 1/N 的高斯分布，其中N表示当前层输入神经元的个数。在caffe中就是这样实现的。 更多初始化方式Understanding the difficulty of training deep feedforward neural networks 在这篇paper中提出 $$var(w)=2/(N_{in}+N_{out})$$ Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 针对一种专门的初始化方式，使得 $var(w)=2.0/N$, 在实际工程中通常使用这种方式。 123456789101112131415161718192021222324252627### 正态分布 w = np.random.randn(N) * sqrt(2.0/N)### 均匀分布def _xavier_initializer(shape, **kwargs): &quot;&quot;&quot; Args: shape: Tuple or 1-d array that species dimensions of requested tensor. Returns: out: tf.Tensor of specified shape sampled from Xavier distribution. &quot;&quot;&quot; epsilon = np.sqrt(6/np.sum(shape)) out = tf.Variable(tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon)) return out 均匀分布[a,b]的方差：$\\dfrac{(b-a)^2}{12}$ 参考资料： Understanding the difficulty of training deep feedforward neural networks cs231n：Weight Initialization understanding-xavier-initialization-in-deep-neural-networks","link":"/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"title":"论文笔记-Autoencoders Are Vision Learners","text":"DALL-E: Zero-Shot Text-to-Image Generation BEIT: BERT Pre-Training of Image Transformers Discrete representations strengthen vision transformer robustness IBOT: Image BERT Pre-training with online tokenizer Masked Autoencoders Are Scalable Vision Learners VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition","link":"/2021/11/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Autoencoders-Are-Vision-Learners/"},{"title":"论文笔记-CNN与自然语言处理","text":"最近在参加 AI challenge 观点型阅读理解的比赛。数据集形式如下： 最开始尝试的模型主要包括几个部分： Embedding: 使用预训练的中文词向量。 Encoder: 基于 Bi-GRU 对 passage,query 和 alternatives 进行编码处理。 Attention: 用 trilinear 的方式，并 mask 之后得到相似矩阵，然后采用类似于 BiDAF 中的形式 bi-attention flow 得到 attened passage. contextual: 用 Bi-GRU 对 attened passage 进行编码，得到 fusion. match 使用 attention pooling 的方式将 fusion 和 enc_answer 转换为单个 vector. 然后使用 cosin 进行匹配计算得到最相似的答案。 目前能得到的准确率是 0.687. 距离第一名差了 0.1…其实除了换模型，能提升和改进的地方是挺多的。 可以用 ELMO 或 wordvec 先对训练集进行预训练得到自己的词向量。 attention 层可以使用更丰富的方式，很多paper 中也有提到。甚至可以加上人工提取的特征。比如苏剑林 blog 中提到的。 还有个很重要的就是 match 部分， attention pooling 是否可以换成其他更好的方式？ 但是，不断尝试各种模型的前提也要考虑速度吧。。rnn 实在是太慢了，所以决定试试 CNN 的方式来处理 NLP 的任务。 关于使用 CNN 来处理阅读理解的任务的大作还是挺多的，这里主要介绍这两篇： Facebook: Convolutional Sequence to Sequence Learning Fast Reading Comprehension with ConvNets ConvS2Spaper: Convolutional Sequence to Sequence Learning 这篇 paper 对应的 NLP 任务是机器翻译，除了用 CNN 对 sentence 进行编码之外，其核心是在 decoder 的时候也使用 CNN. 对于阅读理解来说，能够借用的是其编码 sentence 的方式。但这里作为学习，也多了解一下 decoder 吧～ 对文本来说，看到 CNN 我们首先想到的是 cnn 能有效利用局部信息，提取出局部特征，所以适合做文本分类。但是对于 机器翻译、阅读理解这样的需要考虑全局信息的任务，CNN 似乎看起来并不那么有效。而且在 decoder 的时候，词的生成是 one by one 的，下一个词的生成是依赖于上一个词的。所以在 decoder 中使用 RNN 也是很自然而然的。 Facebook 的这篇 paper 就改变了这些传统的思维，不仅用 CNN 编码全局信息，而且还能 decoder. Motivation Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher layers. 多层 CNN 具有层级表示结构，相邻的词之间在较低层的 layer 交互，距离较远的词在较高层的 layer 交互（交互的目的就是语义消歧）。 Hierarchical structure provides a shorter path to capture long-range dependencies compared to the chain structure modeled by recurrent networks, e.g. we can obtain a feature representation capturing relationships within a window of n words by applying only O(n/k) convolutional operations for kernels of width k, compared to a linear number O(n) for recurrent neural networks. 层级结构提供了一个更短的路径来获取长期依赖。比如相距为 n 的两个词，在 rnn 中交互需要的步数是 O(n),在层级 CNN 中需要 O(n/k).这样减少了非线性的操作，降低了梯度消失的情况。所以这两个词的交互效果会更好～ Inputs to a convolutional network are fed through a constant number of kernels and non-linearities, whereas recurrent networks apply up to n operations and non-linearities to the first word and only a single set of operations to the last word. Fixing the number of nonlinearities applied to the inputs also eases learning. 输入到 CNN 中每个词都会经历固定的 kernel 和 非线性操作。而输入到 RNN 的，第一个词需要经过 n 个 operations，最后一个词只经历了一个 operations. 作者认为固定的操作更容易学习。 这一点我个人认为并不一定就是合理的，本来一个句子中不同词的重要性就是不一样的。 Ｍodel Architecture 模型分为以下几个部分： position embedding convolution block structure Multi-step attention position encoding 这部分在很多地方都出现过了，在没有 rnn 的情况下，都会用 PE 来编码位置信息。但是在这篇 paper 中，作者通过实验发现，PE 作用似乎并不是很重要。 convolution blocks作者使用的是门激活机制， GLU, gate linear units. 来自于 paper: Language modeling with gated convolutional networks 在这篇 paper 中，作者用无监督的方式，来训练语言模型，将 CNN 得到的语言模型与 LSTM 进行对比。 也就是: $$h_l=(XW+b)\\otimes \\sigma(XV+c)$$ The output of each layer is a linear projection X ∗ W + b modulated by the gates σ(X ∗ V + c). Similar to LSTMs, these gates multiply each element of the matrix X ∗W+b and control the information passed on in the hierarchy. 如果是 LSTM-style，应该是 GTU： $$h_i^l=tanh(XW+b)\\otimes \\sigma(XV+c)$$ 作者将两者进行了对比，发现 GLU 效果更好。 residual connection: 为了得到更 deep 的卷积神经网络，作者增加了残差链接。 $$h_i^l=v(W^l[h_{i-k/2}^{l-1},…,h_{i+k/2}^{l-1}]+b_w^l)+h_i^{l-1}$$ 卷积的整个过程： 论文中举了这样一个例子： For instance, stacking 6 blocks with k = 5 results in an input field of 25 elements, i.e. each output depends on 25 inputs. Non-linearities allow the networks to exploit the full input field, or to focus on fewer elements if needed. 这个怎么算的呢？看下图： 从上图中可以看到，当 k=3 时，3 个 blocks，第三层中的每一个输入都与输入中的 7 列有关。所以计算方式是 k + (k-1)* (blocks-1). 一维卷积和二维卷积的区别： ConvS2S 是 1D 卷积，kernel 只是在时间维度上平移，且 stride 的固定 size 为1,这是因为语言不具备图像的可伸缩性，图像在均匀的进行降采样后不改变图像的特征，而一个句子间隔着取词，意思就会改变很多了。 在图像中一个卷积层往往有多个 filter，以获取图像不同的 pattern，但是在 ConvS2S 中，每一层只有一个 filter。一个句子进入 filter 的数据形式是 [1, n, d]. 其中 n 为句子长度， filter 对数据进行 n 方向上卷积，而 d 是词的向量维度，可以理解为 channel，与彩色图片中的 rgb 三个 channel 类似。 Facebook 在设计时，并没有像图像中常做的那样，每一层只设置一个 filter。这样做的原因，一是为了简化模型，加速模型收敛，二是他们认为一个句子的 pattern 要较图像简单很多，通过每层设置一个 filter，逐层堆叠后便能抓到所有的 pattern. 更有可能的原因是前者。因为在 transorfmer 中，multi-head attention 多头聚焦取得了很好的效果，说明一个句子的 pattern 是有多个的. 这段话是有问题的吧？ filter 的个数难道不是 2d吗？ 只不过这里说的 transorfmer 的多头聚焦是值得聚焦到一个词向量中的部分维度。记得在 cs224d 中 manning 曾经讲过一个例子，经过训练或词与词之间的交互后，词向量中的部分维度发生了变化。 在 paper 中，卷积核的尺寸大小是 $W\\in R^{2d\\times kd}$. For encoder networks we ensure that the output of the convolutional layers matches the input length by padding the input at each layer. However, for decoder networks we have to take care that no future information is available to the decoder (Oord et al., 2016a). Specifically, we pad the input by k − 1 elements on both the left and right side by zero vectors, and then remove k elements from the end of the convolution output. 在 encoder 和 decoder 网络中，padding 的方式是不一样的。因为在 decoder 的时候不能考虑未来信息. 在 encoder 时，将 (k-1) pad 到左右两边，保证卷积层的长度不变。 在 decoder 中，将 (k-1) pad 到句子的左边。因此生成的词依旧是 one by one. Multi-step Attention $$d_i^l=W_d^lh_i^l+b_d^l+g_i$$ $$a_{ij}^l=\\dfrac{exp(d_i^l\\cdot z_j^u)}{\\sum_{t=1}^mexp(d_i^l\\cdot z_j^u)}$$ $$c_i^l=\\sum_{j=1}^ma_{ij}^l(z_j^u+e_j)$$ 上式中，l 表示 decoder 中卷积层的层数，i 表示时间步。 实际上跟 rnn 的 decoder 还是比较接近的。 在训练阶段是 teacher forcing, 卷积核 $W_d^l$ 在 target sentence $h^l$ 上移动做卷积得到 $(W_d^lh_i^l + b_d^l)$，类似与 rnn-decoder 中的隐藏状态。然后加上上一个词的 embedding $g_i$,得到 $d_i^l$. 与 encdoer 得到的 source sentence 做交互，通过 softmax 得到 attention weights $a_{ij}^l$. 得到 attention vector 跟 rnn-decoder 有所不同，这里加上了 input element embedding $e_j$. 至于这里为什么要加 $e_j$? We found adding e_j to be beneficial and it resembles key-value memory networks where the keys are the z_j^u and the values are the z^u_j + e_j (Miller et al., 2016). Encoder outputs z_j^u represent potentially large input contexts and e_j provides point information about a specific input element that is useful when making a prediction. Once c^l_i has been computed, it is simply added to the output of the corresponding decoder layer h^l_i. $z_j^u$ 表示更丰富的信息，而 $e_j$ 能够能具体的指出输入中对预测有用的信息。还是谁用谁知道吧。。 关于 multi-hop attention: This can be seen as attention with multiple ’hops’ (Sukhbaatar et al., 2015) compared to single step attention (Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016). In particular, the attention of the first layer determines a useful source context which is then fed to the second layer that takes this information into account when computing attention etc. The decoder also has immediate access to the attention history of the k − 1 previous time steps because the conditional inputs $c^{l-1}_{i−k}, . . . , c^{l-1}i$ are part of $h^{l-1}{i-k}, . . . , h^{l-1}_i$ which are input to $h^l_i$. This makes it easier for the model to take into account which previous inputs have been attended to already compared to recurrent nets where this information is in the recurrent state and needs to survive several non-linearities. Overall, our attention mechanism considers which words we previously attended to (Yang et al., 2016) and performs multiple attention ’hops’ per time step. In Appendix §C, we plot attention scores for a deep decoder and show that at different layers, different portions of the source are attended to. 这个跟 memory networks 中的 multi-hop 是有点类似。 FAST READING COMPREHENSION WITH CONVNETSGated Linear Dilated Residual Network (GLDR): a combination of residual networks (He et al., 2016), dilated convolutions (Yu &amp; Koltun, 2016) and gated linear units (Dauphin et al., 2017). text understanding with dilated convolution kernel:$k=[k_{-l},k_{-l+1},…,k_l]$, size=$2l+1$ input: $x=[x_1,x_2,…,x_n]$ dilation: d 卷积可以表示为： $$(k*x)_ t=\\sum_{i=-l}^lk_i\\cdot x_{t + d\\cdot i}$$ 为什么要使用膨胀卷积呢？ Why Dilated convolution? Repeated dilated convolution (Yu &amp; Koltun, 2016) increases the receptive region of ConvNet outputs exponentially with respect to the network depth, which results in drastically shortened computation paths. 能够显著缩短两个词之间的计算路径。 作者将 GLDR 和 self-attention,以及 RNN 进行了对比，input sequence length n, network width w, kernel size k, and network depth D. model Architecture作者与 BiDAF 和 DrQA 进行了对比，将 BiDAF 和 DrQA 中的 BiLSTM 部分替换成 GLDR Convolution. The receptive field of this convolutional network grows exponentially with depth and soon encompasses a long sequence, essentially enabling it to capture similar long-term dependencies as an actual sequential model. 感受野的尺寸大小指数增加，能够迅速压缩 long sentence,并 capture 长期依赖。 Convolutional BiDAF. In our convolutional version of BiDAF, we replaced all bidirectional LSTMs with GLDRs . We have two 5-layer GLDRs in the contextual layer whose weights are un-tied. In the modeling layer, a 17-layer GLDR with dilation 1, 2, 4, 8, 16 in the first 5 residual blocks is used, which results in a reception region of 65 words. A 3-layer GLDR replaces the bidirectional LSTM in the output layer. For simplicity, we use same-padding and kernel size 3 for all convolutions unless specified. The hidden size of all GLDRs is 100 which is the same as the LSTMs in BiDAF. 具体网络结构，实际参数可以看 paper 实验部分。","link":"/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"title":"机器学习-英文文本预处理","text":"转载自：http://www.cnblogs.com/pinard/p/6756534.html英文文本挖掘预处理特点英文文本的预处理方法和中文的有部分区别。首先，英文文本挖掘预处理一般可以不做分词（特殊需求除外），而中文预处理分词是必不可少的一步。第二点，大部分英文文本都是uft-8的编码，这样在大多数时候处理的时候不用考虑编码转换的问题，而中文文本处理必须要处理unicode的编码问题。 而英文文本的预处理也有自己特殊的地方，第三点就是拼写问题，很多时候，我们的预处理要包括拼写检查，比如“Helo World”这样的错误，我们不能在分析的时候讲错纠错。所以需要在预处理前加以纠正。第四点就是词干提取(stemming)和词形还原(lemmatization)。这个东西主要是英文有单数，复数和各种时态，导致一个词会有不同的形式。比如“countries”和”country”，”wolf”和”wolves”，我们期望是有一个词。 英文文本挖掘预处理一：数据收集这部分英文和中文类似。获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。 对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“deep learning”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。 对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用beautifulsoup。但是我们我们需要某些特殊的语料数据，比如上面提到的“deep learning”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成。这个我一般使用ache。 ache允许我们用关键字或者一个分类算法模型来过滤出我们需要的主题语料，比较强大。 英文文本挖掘预处理二：除去数据中非文本部分这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用beautifulsoup来去除。另外还有一些特殊的非英文字符(non-alpha),也可以用Python的正则表达式(re)删除。 re 模块参考 blog 正则表达式（Regular Expression）是字符串处理的常用工具，通常被用来检索、替换那些符合某个模式（Pattern）的文本。很多程序设计语言都支持正则表达式，像Perl、Java、C/C++。在 Python 中是通过标准库中的 re 模块 提供对正则的支持。 关于正则表达式的语法可以看 speech and language processing chapter2 正则表达式中的*，+，？以及\\w和\\W的区别等常见问题的总结 编译正则表达式re 模块提供了 re.compile() 函数将一个字符串编译成 pattern object，用于匹配或搜索。函数原型如下： 123re.compile(pattern, flags=0) re.compile() 还接受一个可选的参数 flag，用于指定正则匹配的模式。关于匹配模式，后面将会讲到。 反斜杠的困扰在 python 的字符串中，\\ 是被当做转义字符的。在正则表达式中，\\ 也是被当做转义字符。这就导致了一个问题：如果你要匹配 \\ 字符串，那么传递给 re.compile() 的字符串必须是 ”\\\\\\\\“。 由于字符串的转义，所以实际传递给 re.compile() 的是 ”\\\\“，然后再通过正则表达式的转义，”\\\\“ 会匹配到字符”\\“。这样虽然可以正确匹配到字符 \\，但是很麻烦，而且容易漏写反斜杠而导致 Bug。那么有什么好的解决方案呢？ 原始字符串很好的解决了这个问题，通过在字符串前面添加一个r，表示原始字符串，不让字符串的反斜杠发生转义。那么就可以使用r&quot;\\\\\\\\&quot;来匹配字符 \\了。 patern object 执行匹配一旦你编译得到了一个 pattern object，你就可以使用 pattern object 的方法或属性进行匹配了，下面列举几个常用的方法，更多请看这里。 Pattern.match(string[, pos[, endpos]]) 匹配从 pos 到 endpos 的字符子串的开头。匹配成功返回一个 match object，不匹配返回 None。 pos 的默认值是0，endpos 的默认值是 len(string)，所以默认情况下是匹配整个字符串的开头。 1234567891011pattern = re.compile(&quot;d&quot;)print(pattern.match('dog')) # 在字串开头，匹配成功print(pattern.match('god')) # 不再子串开头，匹配不成功print(pattern.match('ddaa', 1,5)) # 在子串开头,匹配成功print(pattern.match('monday', 3)) 123456789&lt;_sre.SRE_Match object; span=(0, 1), match='d'&gt;&lt;_sre.SRE_Match object; span=(0, 1), match='g'&gt;&lt;_sre.SRE_Match object; span=(1, 2), match='d'&gt;&lt;_sre.SRE_Match object; span=(3, 4), match='d'&gt; regex.search(string[, pos[, endpos]]) 扫描整个字符串，并返回它找到的第一个匹配 和 regex.match() 一样，可以通过 pos 和 endpos 指定范围 1234567891011pattern = re.compile(&quot;ar{1}&quot;)match = pattern.search(&quot;marray&quot;)print(match)&lt;_sre.SRE_Match object; span=(1, 3), match='ar'&gt; regex.findall(string[, pos[, endpos]]) 找到所有匹配的子串，并返回一个 list 可选参数 pos 和 endpos 和上面一样 1234567891011pattern = re.compile(r&quot;\\d+&quot;) # 匹配字符串中的数字lst = pattern.findall(&quot;abc1def2rst3xyz&quot;)print(lst)['1', '2', '3'] regex.finditer(string[, pos[, endpos]]) 找到所有匹配的子串，并返回由这些匹配结果（match object）组成的迭代器 可选参数 pos 和 endpos 和上面一样。 1234567891011121314151617pattern = re.compile(r&quot;\\d+&quot;)p = pattern.finditer(&quot;abc1def2rst3xyz&quot;)for i in p: print(i)&lt;_sre.SRE_Match object; span=(3, 4), match='1'&gt;&lt;_sre.SRE_Match object; span=(7, 8), match='2'&gt;&lt;_sre.SRE_Match object; span=(11, 12), match='3'&gt; match object 获取结果在上面讲到，通过 pattern object 的方法（除 findall 外）进行匹配得到的返回结果都是 match object。每一个 match object 都包含了匹配到的相关信息，比如，起始位置、匹配到的子串。那么，我们如何从 match object 中提取这些信息呢？ match.group([group1, ...])： 返回 match object 中的字符串。 每一个 ( ) 都是一个分组，分组编号从1开始，从左往右，每遇到一个左括号，分组编号+1。 组 0 总是存在的，它就是整个表达式 没有参数时，group1默认为0，这时返回整个匹配到的字符串。 指定一个参数（整数）时，返回该分组匹配到的字符串。 指定多个参数时，返回由那几个分组匹配到的字符串组成的 tuple。 1234567891011121314151617181920212223242526272829pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;) # \\w 匹配任意字母，数字，下划线m = pattern.match(&quot;He _ Kobe Bryant, Lakers player&quot;)print(m)print(m.group())print(m.group(1))print(m.group(2))print(m.group(1,2))&lt;_sre.SRE_Match object; span=(0, 4), match='He _'&gt;He _He_('He', '_') match.groups() 返回由所有分组匹配到的字符串组成的 tuple。 1234567891011m = re.match(r&quot;(\\d+)\\.(\\d+)&quot;, '24.163')m.groups()('24', '163') match.start([group]) 没有参数时，返回匹配到的字符串的起始位置。 指定参数（整数）时，返回该分组匹配到的字符串的起始位置。 123456789pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m.start()) # 0print(m.start(2)) # 5 match.end([group])： 没有参数时，返回匹配到的字符串的结束位置。 指定参数（整数）时，返回该分组匹配到的字符串的结束位置。 123456789pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m.end()) # 11print(m.end(1)) # 4 match.span([group])： 返回一个二元 tuple 表示匹配到的字符串的范围，即 (start, end)。 指定参数时，返回该分组匹配到的字符串的 (start, end)。 123456789pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m.span()) # (0, 11)print(m.span(2)) # (5, 11) 模块级别的函数上面讲到的函数都是对象的方法，要使用它们必须先得到相应的对象。本节将介绍一些Module-Level Functions，比如 match()，search()，findall() 等等。你不需要创建一个 pattern object 就可以直接调用这些函数。 re.match(pattern, string, flags=0) 1234567891011121314151617181920212223pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.match(&quot;Kobe Bryant, Lakers&quot;)print(m)# 相当于m = re.match(r&quot;(\\w+) (\\w+)&quot;,&quot;Kobe Bryant, Lakers&quot;)print(m)&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt;&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt; 1234567891011121314151617181920212223pattern = re.compile(r&quot;(\\w+) (\\w+)&quot;)m = pattern.search(&quot;Kobe Bryant, Lakers&quot;)print(m)# 相当于m = re.search(r&quot;(\\w+) (\\w+)&quot;,&quot;Kobe Bryant, Lakers&quot;)print(m)&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt;&lt;_sre.SRE_Match object; span=(0, 11), match='Kobe Bryant'&gt; re.findall(pattern, string, flags=0):与上面类似。 re.finditer(pattern, string, flags=0):与上面类似 编译标志（匹配模式） re.IGNORECASE：忽略大小写，同 re.I。 re.MULTILINE：多行模式，改变^和$的行为，同 re.M。 re.DOTALL：点任意匹配模式，让’.’可以匹配包括’\\n’在内的任意字符，同 re.S。 re.LOCALE：使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定， 同 re.L。 re.ASCII：使 \\w \\W \\b \\B \\s \\S 只匹配 ASCII 字符，而不是 Unicode 字符，同 re.A。 re.VERBOSE：详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。主要是为了让正则表达式更易读，同 re.X。例如，以下两个正则表达式是等价的： 12345678910111213a = re.compile(r&quot;\\d + \\. \\d *#re.X&quot;) the integral partb = re.compile(r&quot;\\d+\\.\\d*&quot;)print(b.match(&quot;123.45&quot;))&lt;_sre.SRE_Match object; span=(0, 6), match='123.45'&gt; 修改字符串第二部分讲的是字符串的匹配和搜索，但是并没有改变字符串。下面就讲一下可以改变字符串的操作。 分割字符串split()函数在匹配的地方将字符串分割，并返回一个 list。同样的，re 模块提供了两种 split 函数，一个是 pattern object 的方法，一个是模块级的函数。 regex.split(string, maxsplit=0)： maxsplit用于指定最大分割次数，不指定将全部分割。 1234567891011pattern = re.compile(r&quot;[A-Z]+&quot;)m = pattern.split(&quot;abcDefgHijkLmnoPqrs&quot;)print(m)['abc', 'efg', 'ijk', 'mno', 'qrs'] re.split(pattern, string, maxsplit=0, flags=0)： 模块级函数，功能与 regex.split() 相同。 flags用于指定匹配模式。 1234567891011m = re.split(r&quot;[A-Z]+&quot;,&quot;abcDefgHijkLmnoPqrs&quot;)print(m)# 输出结果：['abc', 'efg', 'ijk', 'mno', 'qrs'] 搜索与替换另一个常用的功能是找到所有的匹配，并把它们用不同的字符串替换。re 模块提供了sub()和subn()来实现替换的功能，而它们也分别有自己两个不同版本的函数。 regex.sub(repl, string, count=0)： 使用 repl 替换 string 中每一个匹配的子串，返回替换后的字符串。若找不到匹配，则返回原字符串。 repl 可以是一个字符串，也可以是一个函数。 当repl是一个字符串时，任何在其中的反斜杠都会被处理。 当repl是一个函数时，这个函数应当只接受一个参数（pattern对象），对匹配到的对象进行处理，然后返回一个字符串用于替换。 count 用于指定最多替换次数，不指定时全部替换。 12345678910111213141516171819pattern = re.compile(r&quot;like&quot;, re.I)s1 = pattern.sub(r&quot;love&quot;, &quot;I like you, do you like me?&quot;)s2 = pattern.sub(lambda m:m.group().upper(), &quot;I like you, do you like me?&quot;) # repl 是函数，其参数是 patternprint(s1)print(s2)I love you, do you love me?I LIKE you, do you LIKE me? re.sub(pattern, repl, string, count=0, flags=0)： 模块级函数，与 regex.sub() 函数功能相同。 flags 用于指定匹配模式。 123456789s1 = re.sub(r&quot;(\\w)'s\\b&quot;, r&quot;\\1 is&quot;, &quot;She's Xie Pan&quot;)print(s1)She is Xie Pan regex.subn(repl, string, count=0) 同 sub()，只不过返回值是一个二元 tuple，即(sub函数返回值, 替换次数)。 1234567891011121314151617pattern = re.compile(r&quot;like&quot;, re.I)s1 = pattern.subn(r&quot;love&quot;, &quot;I like you, do you like me?&quot;)s2 = pattern.subn(lambda m:m.group().upper(), &quot;I like you, do you like me?&quot;) # repl 是函数，其参数是 patternprint(s1)print(s2)('I love you, do you love me?', 2)('I LIKE you, do you LIKE me?', 2) re.subn(pattern, repl, string, count=0, flags=0)： 同上 英文文本挖掘预处理三：拼写检查由于英文文本中可能有拼写错误，因此一般需要进行拼写检查。如果确信我们分析的文本没有拼写问题，可以略去此步。 拼写检查，我们一般用pyenchant类库完成。pyenchant的安装很简单：”pip install pyenchant”即可。 对于一段文本，我们可以用下面的方式去找出拼写错误： 1234567# 发现这样安装并不是在虚拟环境下，需要去终端对应的虚拟环境下安装# source avtivate NLP!pip install pyenchant /bin/sh: 1: source: not found Requirement already satisfied: pyenchant in /home/panxie/anaconda3/lib/python3.6/site-packages (2.0.0) \u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m \u001b[33mYou are using pip version 10.0.1, however version 18.0 is available. You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m 1234567891011121314151617from enchant.checker import SpellCheckerchkr = SpellChecker('en_US')chkr.set_text(&quot;Many peopel like too watch In the Name of people&quot;)for err in chkr: print(&quot;ERROR:&quot;, err.word)ERROR: peopel 发现只能找单词拼写错误的，但 too 这样的是没办法找出的。找出错误后，我们可以自己来决定是否要改正。当然，我们也可以用pyenchant中的wxSpellCheckerDialog类来用对话框的形式来交互决定是忽略，改正还是全部改正文本中的错误拼写。 更多操作可参考： https://www.jianshu.com/p/96c01666aeeb https://pythonhosted.org/pyenchant/tutorial.html 英文文本挖掘预处理四：词干提取(stemming)和词形还原(lemmatization)词干提取(stemming)和词型还原(lemmatization)是英文文本预处理的特色。两者其实有共同点，即都是要找到词的原始形式。只不过词干提取(stemming)会更加激进一点，它在寻找词干的时候可以会得到不是词的词干。比如”imaging”的词干可能得到的是”imag”, 并不是一个词。而词形还原则保守一些，它一般只对能够还原成一个正确的词的词进行处理。个人比较喜欢使用词型还原而不是词干提取。 在实际应用中，一般使用nltk来进行词干提取和词型还原。安装nltk也很简单，”pip install nltk”即可。只不过我们一般需要下载nltk的语料库，可以用下面的代码完成，nltk会弹出对话框选择要下载的内容。选择下载语料库就可以了。 1234567891011121314151617import nltknltk.download('wordnet')[nltk_data] Downloading package wordnet to /home/panxie/nltk_data...[nltk_data] Unzipping corpora/wordnet.zip.True 在nltk中，做词干提取的方法有PorterStemmer，LancasterStemmer和SnowballStemmer。个人推荐使用SnowballStemmer。这个类可以处理很多种语言，当然，除了中文。 1234567891011from nltk.stem import SnowballStemmerstemmer = SnowballStemmer(&quot;english&quot;)stemmer.stem(&quot;countries&quot;)'countri' 输出是”countri”,这个词干并不是一个词。 而如果是做词型还原，则一般可以使用WordNetLemmatizer类，即wordnet词形还原方法。 12345678910111213from nltk.stem import WordNetLemmatizerwnl = WordNetLemmatizer()print(wnl.lemmatize('countries'))country 输出是”country”,比较符合需求。 在实际的英文文本挖掘预处理的时候，建议使用基于wordnet的词形还原就可以了。 在这里有个词干提取和词型还原的demo，如果是这块的新手可以去看看，上手很合适。 英文文本挖掘预处理五：转化为小写123456789text = 'XiePan'print(text.lower())xiepan 英文文本挖掘预处理六：引入停用词在英文文本中有很多无效的词，比如“a”，“to”，一些短词，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。个人常用的英文停用词表下载地址在这。当然也有其他版本的停用词表，不过这个版本是我常用的。 在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。这个方法和前文讲中文停用词的方法相同，这里就不写出代码，大家参考前文即可。 123456789101112131415from nltk import word_tokenizefrom nltk.corpus import stopwordsstop = set(stopwords.words('english')) # 停用词stop.add(&quot;foo&quot;) # 增加一个词stop.remove(&quot;is&quot;) # 去掉一个词sentence = &quot;this is a foo bar sentence&quot;[i for i in word_tokenize(sentence.lower()) if i not in stop] ['is', 'bar', 'sentence'] 英文文本挖掘预处理七：特征处理现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在文本挖掘预处理之向量化与Hash Trick中，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在文本挖掘预处理之TF-IDF中，我们也讲到了TF-IDF特征处理的方法。 TfidfVectorizer类可以帮助我们完成向量化，TF-IDF和标准化三步。当然，还可以帮我们处理停用词。这部分工作和中文的特征处理也是完全相同的，大家参考前文即可。 英文文本挖掘预处理八：建立分析模型有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而主题模型是自然语言处理比较特殊的一块，这个我们后面再单独讲。 英文文本挖掘预处理总结上面我们对英文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如有时候需要做词性标注，而有时候我们也需要英文分词，比如得到”New York”而不是“New”和“York”，因此这个流程仅供自然语言处理入门者参考，我们可以根据我们的数据分析目的选择合适的预处理方法。","link":"/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%8B%B1%E6%96%87%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"},{"title":"从0开始GAN-3-文本生成planning","text":"写这篇博客源于在知乎上看到大佬Towser在 “BERT模型在NLP中目前取得如此好的效果，那下一步NLP该何去何从？” 这个问题下的回答，对于文本生成的总结觉得太赞了。所以基于大佬的回答，画了一个脑图(http://www.xmind.net/m/AcA3bE)，接下来一两个月的时间也决定按照这个路线进行学习。 Reward Augmented Maximum Likelihood for Neural Structured PredictionMotivationMaximum likilihood based method对于NMT或者其他的 conditional generation，最常用的seq2seq模型是基于maximum likilihood(ML)来最小化下面这个目标函数的： $$L_{ML}=\\sum_{(x,y^* )\\in D}-logp_{\\theta}(y^* |x)$$ 但是这种方式存在几个问题: Minimizing this objective increases the conditional probability of the target outputs, $logp_{\\theta}p(y^* |x)$, while decreasing the conditional probability of alternative incorrect outputs. According to this objective, all negative outputs are equally wrong, and none is preferred over the others. 在最大化目标函数，意味着增加 ground truth output的概率 $logp_{\\theta}p(y^* |x)$，减少其他错误输出的概率。这个过程中，对于错误的output，模型认为所有的negative output都是同等的，这其实是不太正确的。 Generating Sentences from a Continuous Space:However, by breaking the model structure down into a series of next-step predictions, the rnnlm does not expose an interpretable representation of global features like topic or of high-level syntactic properties. 将结构化输出的预测问题，分解成一系列word prediction(seq2seq在训练阶段，其目标函数的loss是将所有的word对应的cross entropy加起来，并没有将sentence作为一个整体来进行优化)，所以使得模型很难学到global feature，类似于topic或者high-level的句法特性。 exposure bias的问题。Quantifying Exposure Bias for Neural Language Generation RL based method$$L_{RL}(\\theta;\\tau,D)=\\sum_{(x,y^* )\\in D}{-\\tau\\mathbb{H}(p_{\\theta}(y^* |x))-\\sum_{y\\in \\mathbb{Y}}p_{\\theta}(y|x)r(y,y^* )}\\quad{(1)}$$ D 表示 training parallel data. $\\mathbb{H}(p)$ 表示概率分布 $p_{\\theta}$ 对应的交叉熵, $H(p(y))=\\sum_{y\\in \\mathbb{Y}p(y)logp(y)}$. $\\tau$ 表示 temperature parameter，是一个超参。这个公式的理解可以与上一篇blog中seqgan的公式对应起来： $$J(\\theta)=E[R_T|s_0,\\theta]=\\sum_{y_1\\in V}G_{\\theta}(y_1|s_0)\\cdot Q_{D_{\\phi}}^{G_{\\theta}}(s_0,y_1)\\quad{(2)}$$ （1）式中的第2项就是（2）式。那么（1）式中的第一项表示的是Maximum likilihood的交叉熵？ 使用RL based的方法存在这样两个问题： 使用随机梯度下降SGD来优化 $L_{RL}(\\theta;\\tau)$ 非常困难，因为reward对应的gradients的方差很大(large variance). 没能有效利用到监督信息。 作者提出了一种新的方法，能结合ML和RL的优势。 RAMI作者在output space定义了一个 exponentiated payoffdistribution, 表示ML和RL的central distribution： 其中 $Z(y^* ,\\tau)=\\sum_{y\\in \\mathbb{Y}}exp{r(y, y^* )/\\tau}$. 简单点理解就是基于 $r(y,y^* )$ 计算得到的reward r，然后softmax得到的分布。 $$q(y|y^* ;\\tau)=\\dfrac{1}{Z(y^* ,\\tau)}exp{r(y, y^* )/\\tau}\\quad(3)$$ 显然，这个 $r(y,y^* )$ 的计算是基于 BLEU 来计算的。这样一来，既考虑到了不同的 y 之间的差异性，也将 BLEU 的计算转换成了 distribution. 然后作者推导了各种公式证明了从ML的角度来优化 $q(y|y^* ;\\tau)$ 和 $p_{\\theta}(y|x)$ 的KL散度等效于优化 $L_{RL}$. 所以 reward-augmented maximum likelihood (RAML) 的loss function可以写成: Note that the temperature parameter, $\\tau \\ge 0$, serves as a hyper-parameter that controls the smoothness of the optimal distribution around correct targets by taking into account the reward function in the output space. optimization对于 $L_{RAMI}(\\theta;\\tau)$ 的优化很简单，就是直接通过 $q(y|y^* ;\\tau)$ 来sampling出 unbiased samples y. 如果超参数 $\\tau=0$,那么就只能sample出 $y^* $. 对公示（７）求导，可以得到： 这里 $q(y|y^* ;\\tau)$ 是通过 $y^* $ 来sample y,不包含需要训练的参数的。所以 RAMI 也就是优化log-likelihood,不过这里的 y 不是ground truth，而是基于 ground truth和评估指标metric来sample得到的y. 对比基于 RL 的优化，作者进行了吐槽： RL中sample得到的样本 y 是通过生成模型得到的，而且这个model还是不断进化的。这使得训练速度很慢，比如 seqgan 中的roll-out policy. reward 在高维output空间非常稀疏，这使得优化很困难。 actor-critique methods. Sampling from the exponentiated payoff distribution在通过公式（9）进行优化之前，需要先通过 exponentiated payoff distribution $q(y|y^* ;\\tau)$ 来sample得到 y. This sampling is the price that we have to pay to learn with rewards. 这个sample过程与RL相比是没有参数的，瞬间简单了很多啊。。 那么具体是怎么sample的呢，作者使用的基于edit distance的方法。 给定的ground truth $y^* $ 长度是m 基于edit distance $y^* $ sample出与 $y^* $ 距离在 e 范围内的sentences y, 其中 $e\\in {0,…,2m}$. 知乎上有大佬对这篇paper做了一个简单的总结, NLP八卦每日谈 2. RL: x –&gt; 通过decoder sample一个句子y’ –&gt; 和y计算metric –&gt; 把metric作为reward，算policy gradient RAML: y –&gt; 通过和metric对应的一个distribution sample一个句子y* –&gt; 把y* 作为GT进行ML训练 这样做的好处是RL的sample是根据decoder sample，而decoder有参数，所以需要policy gradient。而RAML，是根据y（target sentence）来sample句子。这样就没有参数的问题，也就不需要policy gradient了。 RAML看起来几乎完美，不存在任何优化问题。可天下没有免费的午餐。RAML的难点在于如何将Metric转化成对应的distribution。RAML只提供了将诸如edit distance等metric转化成dist的方法，但对于BLEU等却无能为力。 所以目前为止，RAML的主要贡献在于让我们理解RL language generation到底train了个啥。简单来说就是不学ground truth distribution，而学习一个跟metric相关的dense distribution。这么做的好处是y的distribution更大，相对来说更容易学习 关于结构化预测related work(a) supervised learning approaches that ignore task reward and use supervision; (b) reinforcement learning approaches that use only task reward and ignore supervision; (c) hybrid approaches that attempt to exploit both supervision and task reward. Generating Sentences from a Continuous Space Samuel这是非常早期的一篇基于变分自编码做文本生成的论文，我们都知道VAE和GAN是非常类似的。所以在看 GAN text generation相关的paper之前先学习下如何用VAE做文本生成。 关于 VAE 有两篇非常不错的blog: 苏剑林变分自编码器（一）：原来是这么一回事 Variational Autoencoders Explained 何为 VAEMotivation传统 RNNLM 在做text生成的时候，其结构是把一个序列breaking成一个个next word的prediction. 这使得模型并没有显示的获取文本的全局信息，也没有获取类似于topic和句法相关的高级特征。 于是乎，作者提出了一种基于vatiational encoder的方法，能有效获取global feature，并且能避免 MLM 带来的几乎不可能完成的计算。同时，作者认为基于传统的language model的验证方法并不能有效展示出global feature的存在，于是提出了一种新的 evaluation strategy. For this task, we introduce a novel evaluation strategy using an adversarial classifier, sidestepping the issue of intractable likelihood computations by drawing inspiration from work on non-parametric two-sample tests and adversarial training.","link":"/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-3-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90planning/"},{"title":"从0开始GAN-5-NAT Decoding","text":"non-autoregressive decode 相关的paper： Non-autoregressive neural machine translation. Gu et al. 2018 ICLR End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinemen paper1Constant-Time Machine Translation with Conditional Masked Language Models Motivation把auto-regressive转换成non-autoregressive. 其做法是先确定一个target sentece的长度，然后可以看作是每一个time-step的分类任务了。这样decoder就是可并行的了。 architecture模型架构采用transformer的架构。 原生的 Transformer: source-language encoder: self-attention, 包括padding mask. translation model decoder self-attention, 包括padding mask和 look ahead mask，用以mask掉future information. interaction attention with enc_out, 包括 padding mask. 这篇paper中的 conditional mask language model(CMLM) 与transormer的区别在于 decoder 部分的self-attention去掉了 look ahead mask. 所以可以类似于 BERT 那样基于上下文来预测被 mask 的词，decoder 是 bi-directional. Decoding with Mask-Predictdecoder 的具体操作是一个迭代的过程。 | src | Der Abzug der franzsischen Kampftruppen wurde am 20. November abgeschlossen . | | :—– | :—————————————————- | |t=0|The withdrawal of French combat troops was completed on November 20th .| |t=1|The departure of the French combat completed completed on 20 November .| |t=2|The departure of the French combat completed completed on 20 November .| 表中加粗的部分是被 mask 的。可以看到随着迭代进行，mask的词越来越少。 如何选择mask的词： mask词的数量n: 基于一个递减的公式, $n=N\\dfrac{T-t}{T}$. t 是迭代次数。 mask哪些词呢：$Y^{(t)}_{mask}=argmin_i(p_i,n)$ $p_i$ 表示上一次prediction得到的每一个词的置信度，选择概率最低的 n 个词。 基于 encoder_src, $Y_{obs}$ 对 mask token 进行预测: target sequence length这中 non-Autoregressive 存在的一个大问题就是如何确定target sentence 的长度。在 auto-egressive 里面是根据 ${}$ 来确定句子长度的。 针对这个问题，作者采用了类似于 BERT 中 CLS 的做法。使用了 $LENGTH$ 来预测sentence的长度，也是一个分类任务，这个 LENGTH 对应的词表应该就是长度~ 作者选取 top b length，类似于 beam search. 然后选择 candidated b sentence 中概率最大的. $$\\dfrac{1}{N}\\sum logp_i^{(T)}$$ code readingpaper2Non-Autoregressive Neural Machine Translation Motivation:现有的机器翻译模型在inference时，需要在生成前一个单词的基础上继续生成下一个单词，这种自回归的特性严重影响了推理的速度。 并且与训练阶段的不一致导致存在exposure bias。作者提出一个非自回归的方法，在infer阶段并行输出。 Exposure bias: training 阶段上一个token是ground truth infer 阶段上一个token是生成得到的，这样自回归生成整个句子存在误差累积 两个阶段生成target的方式不一样，也就是 exposure bias. Model Architecture: 前一项表示基于监督学习来预测targets 句子的长度。在本文中作者使用了这个词 fertilities(生育能力) 来表示通过source句子通过encode之后所包含的知识. 后一项依旧是极大似然估计，也就是 independent cross-entropy losses on each output distribution。 但不同的是，在inference阶段也是可以并行的。 这里有个疑问，在训练阶段会预测得到一个长度T，但是训练阶段时groud truth长度的，这个怎么解决？ 这里在训练阶段显然需要长度与 ground truth 的target sentence长度一致，才能计算 word-wise corss entropy loss. Decoder Stack1.decoder input 首先关于 decoder 的初始输入，在已有的模型中，训练阶段 decoder 的输入是 time-shifted target outputs，推理阶段是前面时间步预测的输出。 对于NAT模型，需要提前确定 target output 的长度，作者提出了两种方法： Copy source inputs uniformly Copy source inputs using fertilities， 如上图中输入的每个时间步都有其对应的 fertility. 然后把source input按照其对应的次数copy到decoder的输入。 2.Non-causal self-attention 因为不是自回归，也就是下一个词的生成并不依赖于previous的tokens，所以可以去掉transformer中decoder部分的cause-mask,也就是可以结合上下文的词，而不仅仅只是上文。 3.position attention We also include an additional positional attention module in each decoder layer, which is a multi-head attention module with the same general attention mechanism used in other parts of the Transformer network. 为了强调位置信息。 Modeling fertility to tackle the multimodality problem $P_F(f_{t’}|x_{1:T’}; \\theta)$ 表示 fertility 在 t’ 时间步的概率分布，其是通过encoder顶层的 mlp + softmax 得到的。","link":"/2019/06/29/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-5-decoding/"},{"title":"论文笔记-CoQA","text":"paper: CoQA: A Conversational Question Answering Challenge Motivation We introduce CoQA, a novel dataset for building Conversational Question Answering systems.1 Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. CoQA, 对话式阅读理解数据集。从 7 个不同领域的 8k 对话中获取的 127k 问答对。 The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. CoQA 跟传统的 RC 数据集所面临的挑战不一样，主要是指代和推理。 We ask other people a question to either seek or test their knowledge about a subject. Depending on their answer, we follow up with another question and their answer builds on what has already been discussed. This incremental aspect makes human conversations succinct. An inability to build up and maintain common ground in this way is part of why virtual assistants usually don’t seem like competent conversational partners. 我们问其他人一个问题，来寻求或者测试他们对某一个主题的知识。然后依赖于他的答案，我们提出一个新的问题，他根据刚才我们讨论的来回答这个新的问题。 这使得对话变得很简短。而正是这种建立和维持共同点的能力缺失，使得虚拟助手看起来并不是一个有能力的对话者。 而 CoQA 就是要测试这种能力。 Introduction In CoQA, a machine has to understand a text passage and answer a series of questions that appear in a conversation. We develop CoQA with three main goals in mind. The first concerns the nature of questions in a human conversation. Posing short questions is an effective human conversation strategy, but such questions are a pain in the neck for machines. 第一点：人类在对话时，会提出很简短的问题，但这对于机器来说却很难。比如 Q5 “Who?” The second goal of CoQA is to ensure the naturalness of answers in a conversation. Many existing QA datasets restrict answers to a contiguous span in a given passage, also known as extractive answers (Table 1). Such answers are not always natural, for example, there is no extractive answer for Q4 (How many?) in Figure 1. In CoQA, we propose that the answers can be free-form text (abstractive answers), while the extractive spans act as rationales for the actual answers. Therefore, the answer for Q4 is simply Three while its rationale is spanned across multiple sentences. 第二点：答案不是抽取式的 extractive，而是总结性的 abstractive, free-from text. 比如 Q4. 好难啊！！！ The third goal of CoQA is to enable building QA systems that perform robustly across domains. The current QA datasets mainly focus on a single domain which makes it hard to test the generalization ability of existing models. 第三点：数据来自多种 domain，提高泛化性。 Dataset collection数据集具体详情： It consists of 127k conversation turns collected from 8k conversations over text passages (approximately one conversation per passage). The average conversation length is 15 turns, and each turn consists of a question and an answer. It contains free-form answers. Each answer has an extractive rationale highlighted in the passage. Its text passages are collected from seven diverse domains — five are used for in-domain evaluation and two are used for out-of-domain evaluation. Almost half of CoQA questions refer back to conversational history using coreferences, and a large portion requires pragmatic reasoning making it challenging for models that rely on lexical cues alone. 大部分涉及到对话历史的问题都用到了指代和逻辑推理，这对于仅仅是依赖于词汇提示（语义匹配）的模型来说会很难。 The best-performing system, a reading comprehension model that predicts extractive rationales which are further fed into a sequence-to-sequence model that generates final answers, achieves a F1 score of 65.1%. In contrast, humans achieve 88.8% F1, a superiority of 23.7% F1, indicating that there is a lot of headroom for improvement. Baseline 是将抽取式阅读理解模型转换成 seq2seq 形式，然后从 rationale 中获取答案，最终得到了 65.1% 的 F1 值。 question and answer collection We want questioners to avoid using exact words in the passage in order to increase lexical diversity. When they type a word that is already present in the passage, we alert them to paraphrase the question if possible. questioner 提出的问题应尽可能避免使用出现在 passage 中的词，这样可以增加词汇的多样性。 For the answers, we want answerers to stick to the vocabulary in the passage in order to limit the number of possible answers. We encourage this by automatically copying the highlighted text into the answer box and allowing them to edit copied text in order to generate a natural answer. We found 78% of the answers have at least one edit such as changing a word’s case or adding a punctuation. 对于答案呢，尽可能的使用 passage 中出现的词，从而限制出现很多中答案的可能性。作者通过复制 highlighted text(也就是 rationale 吧) 到 answer box，然后让 answerer 去生成相应的 answer. 其中 78% 的答案是需要一个编辑距离，比如一个词的大小写或增加标点符号。 passage collection Not all passages in these domains are equally good for generating interesting conversations. A passage with just one entity often result in questions that entirely focus on that entity. Therefore, we select passages with multiple entities, events and pronominal references using Stanford CoreNLP (Manning et al., 2014). We truncate long articles to the first few paragraphs that result in around 200 words. 如果一个 passage 只有一个 entity，那么根据它生成的对话都会是围绕这个 entity 的。显然这不是这个数据集想要的。因此，作者使用 Stanford CoreNLP 来对 passage 进行分析后选择多个 entity 和 event 的 passage. Table 2 shows the distribution of domains. We reserve the Science and Reddit domains for out-ofdomain evaluation. For each in-domain dataset, we split the data such that there are 100 passages in the development set, 100 passages in the test set, and the rest in the training set. For each out-of-domain dataset, we just have 100 passages in the test set. In domain 中包含 Children, Literature, Mid/HIgh school, News, Wikipedia. 他们分出 100 passage 到开发集(dev dataset), 其余的在训练集 (train dataset). out-of-diomain 包含 Science Reddit ，分别有 100 passage 在开发集中。 test dataset: Collection multiple answers Some questions in CoQA may have multiple valid answers. For example, another answer for Q4 in Figure 2 is A Republican candidate. In order to account for answer variations, we collect three additional answers for all questions in the development and test data. 一个问题可能出现多种回答，因此在dev dataset 和 test dataset 中有三个候选答案。 In the previous example, if the original answer was A Republican Candidate, then the following question Which party does he belong to? would not have occurred in the first place. When we show questions from an existing conversation to new answerers, it is likely they will deviate from the original answers which makes the conversation incoherent. It is thus important to bring them to a common ground with the original answer. 比如上图中 Q4, 如果回答是 A Republican candidate. 但是整个对话是相关的，所以接下来的问题就会使整个对话显得混乱了。 We achieve this by turning the answer collection task into a game of predicting original answers. First, we show a question to a new answerer, and when she answers it, we show the original answer and ask her to verify if her answer matches the original. For the next question, we ask her to guess the original answer and verify again. We repeat this process until the conversation is complete. In our pilot experiment, the human F1 score is increased by 5.4% when we use this verification setup. 因为机器在学习的时候是有 original answer 进行对比的，同样的这个过程在人工阶段也是需要的，可以减少上诉的混乱情况，answerer 在给出一个答案后，作者会告诉他们是否与 original 匹配，然后直到整个过程完成。 Dataset AnalysisWhat makes the CoQA dataset conversational compared to existing reading comprehension datasets like SQuAD? How does the conversation flow from one turn to the other? What linguistic phenomena do the questions in CoQA exhibit? We answer these questions below. 在 question 中： 指代词(he, him, she, it, they)出现的更为频繁， SQuAD 则几乎没有。 SQuAD 中 what 几乎占了一半，CoQA 中问题类型则更为多样， 比如 did, was, is, does 的频率很高。 CoQA 的问题更加简短。见图 3. answer 有 33% 的是 abstractive. 考虑到人工因素，抽取式的 answer 显然更好写，所以这高于作者预期了。yes/no 的答案也有一定比重。 Conversation FlowA coherent conversation must have smooth transitions between turns. 一段好的对话是具有引导性的，不断深入挖掘 passage 的信息。 作者将 passage 均匀分成 10 chunks，然后分析随着对话 turn 的变化，其对应的 passage chunks 变化的情况。 Linguistic Phenomena Relationship between a question and its passage： lexical match: question 和 passage 中至少有一个词是匹配的。 Paraphrasing: 解释型。虽然 question 没有与 passage 的词，但是确实对 rationale 的一种解释，也就是换了一种说法，当作问题提出了。通常这里面包含： synonymy(同义词), antonymy(反义词), hypernymy(上位词), hyponymy(下位词) and negation(否定词). Pragmatics: 需要推理的。 Relationship between a question and its conversation history： No coref Explicit coref. Implicit coref.","link":"/2018/12/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CoQA/"},{"title":"Dynamic Routing Between Capsules","text":"paper: Dynamic Routing Between Capsules blog: Understanding Hinton’s Capsule Networks. 浅析第一篇Capsule：Dynamic Routing Between Capsules Part 1, IntutionCNN 是如何工作的？ 浅层的卷积层会检测一些简单的特征，例如边缘和颜色渐变。 更高层的 layer 使用卷积操作将简单的特征加权合并到更复杂的特征中。 最后，网络顶部的网络层会结合这些非常高级的特征去做分类预测。 在第二步中，需要理解的是，CNN 是通过加权求和的方式将低层次的特征组合到高层次的特征的(weighted, added, nonlinear)。在这个过程中，简单的特征在组合成更复杂的特征之前，他们之间是存在位置关系的(pose translation and ratational)。 对于这个位置关系的问题，CNN 是通过 max pooling/successive convolution layer 来解决这个问题的。通过减小图像的尺寸，增加高层神经元的感受野（field of view），这使得他们能在更大的区域内检测出更高阶的特征。 CNN 存在的问题但是不要被 CNN 的表现好所迷惑，尽管 CNN 的表现优于其他模型。Hinton认为：max pooling工作得这么好其实是一个大灾难。 Hinton: “The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.” 就算你不使用max pooling，传统CNN依然存在这样的关键问题，学习不到简单与复杂对象之间的重要的空间层次特征。 Internal data representation of a convolutional neural network does not take into account important spatial hierarchies between simple and complex objects. CNN只关注要检测的目标是否存在，而不关注这些组件之间的位置和相对的空间关系。如下例，CNN判断人脸只需要检测出它是否存在两个眼睛，两个眉毛，一个鼻子和一个嘴唇，现在我们把右眼和嘴巴换个位置，CNN依然认为这是个人。 个人理解：神经网络能否具备模糊识别的能力？比如仅仅是鼻子和眼睛调换了位置，但是大致看起来依然是一张人脸。神经网络能否像我们人类一样，知道这张图片想去表达一张人脸，只不过有些地方除了问题，那么它是否知道问题出在哪儿了呢？而不仅仅是识别的问题对吧？ 再比如在文本领域，一两个字颠倒顺序并不影响我们人类阅读，神经网络也能理解它的意思，但是能否准确的知道这两个字顺序是颠倒的呢？ 这能否作为一个课题。。神经网络纠错。。当然，如果给出了错误的数据集，那也就是监督学习以及分类的问题，能不能在没有给出这个有部分错误的数据情况下，依旧识别出来呢？ CNN 对旋转不具备不变性，学不到 3D 空间信息。例如下面的自由女神，我们只看自由女神的一张照片，我们可能没有看过其它角度的照片，还是能分辨出这些旋转后的照片都是自由女神，也就是说，图像的内部表征不取决于我们的视角。但是CNN做这个事情很困难，因为它无法理解3D空间。 另外，神经网络一般需要学习成千上万个例子。人类学习数字，可能只需要看几十个个例子，最多几百个，就能区别数字。但是CNN需要成千上万个例子才能达到比较好的效果，强制去学习。并且关于前面的旋转不变性，CNN可以通过增强数据的手段去改善，但是这也就需要用到大量的数据集。 Hardcoding 3D World into a Neural Net: Inverse Graphics ApproachCapsules 就是为了解决这些问题。其灵感来源于计算机图形学中的渲染技术。 Computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data. 在计算机图形领域，是通过几何数据的内部层次表示来重构一个视觉图像的。这项技术叫 “渲染 (rendering)”. Inspired by this idea, Hinton argues that brains, in fact, do the opposite of rendering. He calls it inverse graphics: from visual information received by eyes, they deconstruct a hierarchical representation of the world around us and try to match it with already learned patterns and relationships stored in the brain. This is how recognition happens. And the key idea is that representation of objects in the brain does not depend on view angle. Hiton 认为大脑是反向渲染的一个过程。接受视觉信息，然后对这样一个具有层次的信息表示进行解构，并与我们已知的模式进行匹配。这里的关键是对象信息的表示在大脑中是不依赖于某一个视角的。 So at this point the question is: how do we model these hierarchical relationships inside of a neural network? 那么我们如何通过神经网络对一个层次信息进行建模呢？ Capsules Capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network. Capsule可以学习到物体之间的位置关系，例如它可以学习到眉毛下面是眼睛，鼻子下面是嘴唇，可以减轻前面的目标组件乱序问题。 Capsule可以对3D空间的关系进行明确建模，capsule可以学习到上面和下面的图片是同一个类别，只是视图的角度不一样。Capsule可以更好地在神经网络的内部知识表达中建立层次关系 Capsule只使用CNN数据集的一小部分，就能达到很好的结果，更加接近人脑的思考方式，高效学习，并且能学到更好的物体表征。 小结：这一节引用了capsule的概念，可以用于深度学习，更好地在神经网络的内部知识表达中建立层次关系，并用不同的方法去训练这样一个神经网络。 Part2, How Capsules WorkWhat is a Capsule?CNN 在解决视角的不变性时是通过 max pooling 解决的。选择一块区域的最大值，这样我们就能得到激活的不变性(invariance of activities). 不变性意味着，稍微改变输入的一小部分，输出依旧不变。并且，在图像中移动我们识别的目标，我们依然能检测出这个目标来。 但是 max pooling 会丢失很多信息，并且 CNN 不能编码特征之间的相对空间关系。所以采用 capsules. Capsules encapsulate all important information about the state of the feature they are detecting in vector form. Capsule 是一个神经元向量（activity vector） 这个向量的模长表示某个entity存在的概率，entity可以理解为比如鼻子，眼睛，或者某个类别，因为用vector的模长去表示概率，所以我们要对vector进行压缩，把vector的模长压缩到小于1，并且不改变orientation，保证属性不变化。 这个向量的方向表示entiry的属性（orientation），或者理解为这个vector除了长度以外的不同形态的instantiation parameter，比如位置，大小，角度，形态，速度，反光度，颜色，表面的质感等等。 How does a capsule work? 传统神经元是一个 scaler, 神经元与神经元之间通过加权求和的方式连接。其计算方式： 计算输入标量 $x_i$ 的权重 $w_i$ 对输入标量 $x_i$ 进行加权求和 通过非线性激活函数，进行标量与标量之间的变换，得到新标量 $h_j$ capsule 的前向转换的计算方式： matrix multiplication of input vectors（输入向量 $u_i$ 的矩阵 W 乘法） scalar weighting of input vectors（输入向量 $\\hat u_i$ 的标量加权 $c_i$） sum of weighted input vectors（输入向量的加权求和） vector-to-vector nonlinearity（向量到向量的非线性变换） 输入向量 $u_i$ 的矩阵 W 乘法Affine transform: $$\\hat u_{j|i}=W_{ij}u_i$$ 向量的长度表示低维的胶囊能检测出对应实体的概率。向量的方向则编码了检测对象的中间状态表示。我们可以假设低维胶囊 $u_i$ 分别表示眼睛，嘴巴，鼻子这三个低层次的特征，高维胶囊 $u_j$ 检测脸部高层次特征。 矩阵 W 编码了低层次特征之间或低层次特征与高层次特征之间的重要的空间或其他关系。 $u_i$ 乘以相应的权重矩阵 W 得到prediction vector（注意这个图里只画了一个 prediction vector，也就是 $\\hat u_i$，因为这里只对应了一个 capsule 输出，如果下一层有 j 个 capusles，$u_i$ 就会生成 j 个 prediction vectors） 例如，矩阵 $W_{2j}$ 可以对鼻子和面部的关系进行编码:面部以鼻子为中心，其大小是鼻子的10倍，而在空间上的方向与鼻子的方向一致。矩阵 $W_{1j}$ 和 $W_{3j}$ 也是类似的。经过矩阵相乘之后，我们可以得到更高层次特征的预测位置。比如，$\\hat u_1$ 根据眼睛预测人脸位置，$\\hat u_2$ 根据嘴巴预测人脸位置，$\\hat u_3$ 根据鼻子预测人脸位置。最后如果这3个低层次特征的预测指向面部的同一个位置和状态，那么我们判断这是一张脸 输入向量 $\\hat u_i$ 的标量加权 $c_i$weighting: $$s_{j|i} = c_{ij}\\hat u_{j|i}$$ $$s_{k|i} = c_{ik}\\hat u_{k|i}$$ 前面提到的传统神经元在这一步对输入进行加权，这些权重是在反向传播过程中得到的，但 Capsule 是通过 dynamic routing 的方式进行交流的。在这一步，低层次 capsule 需要决定怎么把自己的输出分配给高层次capsule。 $c_{ij}$ 是耦合系数 (coupling coefficients), 通过迭代动态路由过程来决定。比如上图中的，高层次有两个胶囊 capsule J 和 capsule K. 那么对于 capsule i 通过上一步的矩阵 W 就可以得到 $\\hat u_{j|i}, \\hat u_{k|i}$, 他们对应的权重分别是 $c_{ij}, c_{ik}$, 并且有 $c_{ij} + c_{ik} = 1$. 在动态路由的过程中是如何确定权重的呢，Routing by agreement： 在这张图片中，我们现在有一个低层次的已经被激活的capsule，它对下层每个capsule会生成一个prediction vector，所以这个低层次capsule现在有两个prediction vector，对这两个prediction vectors分配权重分别输入到高层次capsule J和K中。 现在，更高层次的capsule已经从其他低层次的capsule中获得了许多输入向量，也就是图片中的点，标红的部分是聚集的点，当这些聚集点意味着较低层次的capsule的预测是相互接近的。 低层次capsule希望找到高一层中合适的capsule，把自己更多的vector托付给它。低层次capsule通过dynamic routing的方式去调整权重c。 例如，如果低层次capsule的prediction vector远离capsule J中“correct”预测的红色集群，并且接近capsule K 中的“true”预测的红色集群，那么低层次capsule就会调高capsule K对应的权重，降低capsule J对应的权重。最后，如果高层次capsule接收到的这些prediction都agree这个capsule，那么这个capsule就会被激活，处于active状态，这就叫Routing by agreement。 加权输入向量的总和sum: $$s_j = \\sum_i c_{ij}\\hat u_{j|i}$$ 高层次capsule根据前面计算的权重c，对所有低层次capsule的prediction vectors进行加权，得到一个向量。 向量到向量的非线性变换Squashing是一种新的激活函数，我们对前面计算得到的向量施加这个激活函数，把这个向量的模长压缩到1以下，同时不改变向量方向，这样我们就可以利用模长去预测概率，并且保证属性不变化。 这个公式的左边是变换尺度，右边是单位向量。 小结：这一节介绍了capsule的基本概念，capsule把单个神经元扩展到神经元向量，有更强大的特征表征能力，并且引入矩阵权重来对不同layer特性之间的重要层次关系进行编码，结果说明了以下两种性质： Invariance 不变性：物体表示不随变换变化，例如空间的 Invariance，是对物体平移之类不敏感（物体不同的位置不影响它的识别) Equivariance同变性：用变换矩阵进行转换后，物体表示依旧不变，这是对物体内容的一种变换 Part 3, Dynamic Routing Between Capsules算法的核心思想： Lower level capsule will send its input to the higher level capsule that “agrees” with its input. This is the essence of the dynamic routing algorithm. 动态路由算法的设计是为了动态调整 $c_{ij}$ 的值： $c_{ij}$ 为非负 scalar 对于每一个的 low level capsule i，所有的权重之和 $c_{ij}$ 为 1，j 表示 high-level capsule j. 对于每一个 low-level capsule, 对应的权重数量等于 high-level 的数量 权重由动态路由算法确定 下面我们逐行解释伪代码： 这个过程的输入是 第 $l$ 层的 capsule 经过矩阵变换之后的 prediction vector $\\hat u$. 迭代步数 r 初始化 $b_{ij}$ 为 0， $b_{ij}$ 是用来计算 $c_{ij}$ 的 对接下来4-6行代码迭代r次，计算第 $l+1$ 层 capsule j 的 output 在第 $l$ 层，每个 capsule i 对 $b_{ij}$ 做 softmax 得到 $c_{ij}$，$c_{ij}$ 是第 $l$ 层 capsule i 给第 $l+1$ 层 capsule j 分配的权重。在第一次迭代的时候，所有的 $b_{ij}$ 都初始化为 0，所以这里得到的 $c_{ij}$ 都是相等的概率，当然后面迭代多次后，$c_{ij}$ 的值会有更新。 在第 $l+1$ 层，每个 capsule j 利用 $c_ij$ 对 $\\hat u_j|i$ 进行加权求和，得到输出向量 $s_j$ 在第 $l+1$ 层，使用squash激活函数对 $s_j$ 做尺度的变换，压缩其模长不大于1 我们在这一步更新参数，对所有的 $l$ 层 capsule i和 $l+1$ 层 capsule j，迭代更新 $b_{ij}$，更新 $b_{ij}$ 为旧 $b_{ij}$ + capsule j 的输入和输出的点乘，这个点乘是为了衡量这个 capsule 的输入和输出的相似度，低层 capsule 会把自己的输出分给跟它相似的高层 capsule。 经过r次循环，高层capsule可以确定低层分配的权重以及计算其输出，前向传播可以继续连接到下一个网络层。这个r一般推荐设置为3，次数太多了可能会过拟合。 susht 大佬画的图，不过没有把 $b_{ij}$ 表示出来。 $b_{ij}$ 的参数量和 $c_{ij}$ 以及 $u_{ij}$ 的个数是一致的。 假设有两个高层capsule，紫色向量v1和v2分别是这两个capsule的输出，橙色向量是来自低层中某个capsule的输入，黑色向量是低层其它capsule的输入。 左边的capsule，橙色u_hat跟v1方向相反，也就是这两个向量不相似，它们的点乘就会为负，更新的时候会减小对应的c_11数值。右边的capsule，橙色u_hat跟v2方向相近，更新的时候就会增大对应的c_12数值。 那么经过多次迭代更新，所有的routing权重c都会更新到这样一种程度：对低层capsule的输出与高层capsule的输出都进行了最佳匹配。 小结：这节介绍了 dynamic routing algorithm by agreement 的方式去训练 capsNet，主要 idea 是通过点乘去衡量两个 capsule 输出的相似度，并且更新routing的权重参数。 CapsNet Architecture论文给出了一个简单的CapsNet模型，第一层是个普通的conv层，第两层也结合了conv操作去构建初始的capsule，再通过routing的方式和第三层的DigitCaps交流，最后利用DigitCaps的capsule去做分类。 ReLU Conv1: 这是一个普通的卷积层，参数量是 $9\\times 9\\times 256$, 假设输入是 $28\\times 28\\times 1$, 得到的输出是 $20\\times 20\\times 256$. PrimaryCaps: 这里构建了 32 个 channels 的 capsules, 每个 capsule 的向量维度是 8. 依旧是采用卷积的方法，每一个 channels 使用 8 个卷积核 $9\\times 9$. 所以总的参数量是 $9\\times 9\\times 256 \\times 32\\times 8 + 32 \\times 8= 5308672$。 一个 channels 对应一个 feature map，在这里是 $6\\times 6$ 个 8 维的 capsules. 所以最后是 $6\\times 6\\times 32=1152$ 个 capsules。 DigitCaps: 对前面1152个capules进行传播与routing更新，输入是1152个capsules，输出是10个capules，每个 capsule 的维度由 8 变成了 16，表示10个数字类别，最后我们用这10个capules去做分类. 总的参数量是 $1152\\times 8\\times 16+1152+1152=149760$. 后面两个 1152 分别表示 $b_{ij}, c_{ij}$. loss function在 dynamic routing 过程中我们更新了 $c_{ij}$ 参数，其余参数是通过反向传播进行更新，这里对于每一个capsule k我们都计算它的 Margin loss损失函数 $L_k$ 跟 SVM 的 loss function 非常类似。 $$L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$ 不同的是，这里设定了 正类的概率 越小于 $m^+=0.9$ 其贡献的 loss 越小，负类的概率越大于 $m^-=0.1$ 其贡献的 loss 越大。 $\\lambda$ 参数设置为0.5，这是为了防止负例减小了所有capsule向量的模长。 Decoder: reconstruction as a regularization method 这里会mask掉一些capsule，用有代表性的capsule去重构图像，通过几层全连接层得到784维的输出，也就是原始图像的像素点个数，以这个输出跟原始像素点的欧几里得距离作为重构损失函数。 CapsNet与CNN的联系相同之处：CNN可以上下左右平移在图像上进行扫描，也就是它在这个地方学到的东西可以用到下一个地方，可以有效识别图片的内容，所以capsule也采用了convolution的特点，最后一层只用capsule. 不同之处：capsule不采用max pooling的做法，因为Max pooling只保留了区域内最好的特征，而忽视了其它的特征，routing并不会丢失这些信息，它可以高效处理高度层叠的例子。浅层capsule会把位置信息通过place-coded的方式记录在激活的capsule中，高层的capsule会通过rate-coded的方式记录在capsule vector的值当中。 Capsules做分类的优点 适合输出有多个类别的多分类问题，softmax只适合只有一个输出的多分类问题，因为softmax会提高某一类的概率，而压低其余所有类别的概率。 另外，我们也可以用k sigmoid去代替softmax，把多分类任务转换成K个二分类任务.","link":"/2019/01/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Capsules-Network/"},{"title":"论文笔记-Discrete Latent Variables Based Generation","text":"VQ-VAE: Neural Discrete Representation Learning (NIPS2017) VQ-VAE2: Generating Diverse High-Resolution Images with VQ-VAE-2 DALL-E: Zero-Shot Text-to-Image Generation VideoGPT: Video Generation using VQ-VAE and Transformers LVT: Latent Video Transformer Feature Quantization Improves GAN Training (ICML2020) DVT-NAT: Fast Decoding in Sequence Models Using Discrete Latent Variables (ICML2018) NWT: Towards natural audio-to-video generation with representation learning NUWA: Visual Synthesis Pre-training for Neural visUal World creAtion VQ-VAE在认识VQ-VAE之前，先回顾一下AE、VAE。很早以前看李宏毅老师的视频学过一次，这里就直接使用之前整理的PPT笔记了. 以及对应的代码可以看这里 (vae.ipynb - Colaboratory (google.com)) Auto-Encoder我们在重构一个图像时，通常输入和输出都是图像本身。为了保证神经网络不是直接的copy，很自然会想到这种降维再升维的方式。这里的latent vector/codings 就是我们希望学到的一个原图像压缩后的低维特征表示。 模型代码如下： 12345678910111213141516171819202122232425262728293031class Encoder(nn.Module): def __init__(self, latent_dims): super(Encoder, self).__init__() self.linear1 = nn.Linear(784, 512) self.linear2 = nn.Linear(512, latent_dims) def forward(self, x): z = torch.flatten(x, start_dim=1) z = F.relu(self.linear1(z)) return self.linear2(z)class Decoder(nn.Module): def __init__(self, latent_dims): super(Decoder, self).__init__() self.linear1 = nn.Linear(latent_dims, 512) self.linear2 = nn.Linear(512, 784) def forward(self, z): x_hat = F.relu(self.linear1(z)) x_hat = torch.sigmoid(self.linear2(x_hat)) return x_hat.reshape((-1, 1, 28, 28)) class Autoencoder(nn.Module): def __init__(self, latent_dims): super(Autoencoder, self).__init__() self.encoder = Encoder(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z = self.encoder(x) return self.decoder(z) 看源代码可以直观的发现，对于每一个样本，我们学到了对应的压缩后的特征表示的维度是 latent_dims=2.（潜空间的维度当然也可以更大，这里为了可视化更方便，我们设置为2）。 可视化代码： 12345678def plot_latent(autoencoder, data, num_batches=100): for i, (x, y) in enumerate(data): z = autoencoder.encoder(x.to(device)) z = z.to('cpu').detach().numpy() plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10') if i &gt; num_batches: plt.colorbar() break 将得到的这些2 维的 latent vectors可视化如下图： 通过上图我们可以发现，我们训练的每一个样本对应着潜空间中的一个点，这些点是离散的。这意味着，我们并不能生成新的图像，而只能“复制”原有的图像（也可以说，如果在潜空间采样得到的点在这些离散的点之间/外时，生成的样本会比较模糊）。但也有有趣的发现，降维确实是起到了一个聚类的效果，只是效果一般吧，类与类之间并没有完全的分开。这个应该是可以人为控制的。 我们可以试着去生成一些潜空间的vector，然后在此基础上去重构，看看会发现什么？ 重构代码： 12345678910def plot_reconstructed(autoencoder, r0=(-10, 5), r1=(-5, 10), n=12): w = 28 img = np.zeros((n*w, n*w)) for i, y in enumerate(np.linspace(*r1, n)): for j, x in enumerate(np.linspace(*r0, n)): z = torch.Tensor([[x, y]]).to(device) x_hat = autoencoder.decoder(z) x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy() img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat plt.imshow(img, extent=[*r0, *r1]) 代码中我们选取的潜空间的范围是 $x\\in [-5, 10]$ , $y\\in [10, 5]$ ,在结合可视化的图，可以看到主要是集中在数字 “0” 的区域。右上角是 (10, 5) 可以从可视化的图中也能看到，有部分接近“1”的区域，但生成的样本很模糊。 自编码除了潜空间是离散的，还有个缺点，就是当神经网络太强时，会overfitting。去噪自编码可以缓解这个问题。 VAE在前面提到AE的缺点是，其得到的潜空间是非连续的，导致采样发生在离散点之间/外时，会生成比较模糊的图片。而VAE就是我们假设先验 $p(z)$ 的每个维度就是一个连续的distribution. AE是用潜空间中的一个点(fixed vector)来对应一个训练样本，VAE则是用一个连续的分布来对应一个训练样本。通过代码来理解这句话： 12345678910111213141516171819202122232425262728293031323334353637383940class VariationalEncoder(nn.Module): def __init__(self, latent_dims): super(VariationalEncoder, self).__init__() self.linear1 = nn.Linear(784, 512) self.linear2 = nn.Linear(512, latent_dims) self.linear3 = nn.Linear(512, latent_dims) self.kl = 0 def forward(self, x): x = torch.flatten(x, start_dim=1) x = F.relu(self.linear1(x)) mu = self.linear2(x) sigma = torch.exp(self.linear3(x)) z = mu + sigma*torch.randn_like(sigma) self.kl = 0.5*(sigma**2 + mu**2 - torch.log(sigma) - 1).sum() return zclass Decoder(nn.Module): def __init__(self, latent_dims): super(Decoder, self).__init__() self.linear1 = nn.Linear(latent_dims, 512) self.linear2 = nn.Linear(512, 784) def forward(self, z): x_hat = F.relu(self.linear1(z)) x_hat = torch.sigmoid(self.linear2(x_hat)) return x_hat.reshape((-1, 1, 28, 28)) class VariationalAutoencoder(nn.Module): def __init__(self, latent_dims): super(VariationalAutoencoder, self).__init__() self.encoder = VariationalEncoder(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z = self.encoder(x) return self.decoder(z) 第16行代码 z = mu + sigma*torch.randn_like(sigma) 中得到的是潜空间的多维特征表示 z, 每个维度都是一个正态分布(均值为 mu, 方差为 sigma). 第17行代码的kl散度看着很疑惑，这里需要推导得到，先暂放一边。 先验 $p_{\\theta}(z)$ 似然 $p_{\\theta}(x|z)$ 后验 $p_{\\theta}(z|x)$ 右图中实线时生成模型（先验*似然），我们的优化目标可以有两种选择，一个是最大化似然概率(MLH)，另一个是最大化后验概率(MAP). 但是对于 $p(x|z)p(z)$ 这个的优化是很难的，我们不可能去采样所有的 $p(z)$。 于是为了解决这个问题，我们可以采用变分近似的方法，也就是我们通过另一个判别模型来代替这个后验概率。也就是 $q_{\\phi}(z|x)\\sim p_{\\theta}(z|x)$ . 这个判别模型就是下图中的encoder。 如代码中13-14行显示那样，我们通过encoder直接生成每一个维度的均值和方差，得到一个多维的正态分布，然后在此基础上去解码。 接下来我们再回过头来说说第17行代码的kl散度是咋回事。我们了解的kl散度通常作为一个loss来衡量两个分布是否接近，那这里我们也确实有两个分布需要拉近，就是下式中的第一项，先验和后验我们需要尽量一致： 后验$q_{\\phi}(z|x)$就是我们的encoder，先验$p_{\\theta}(z|x)$ 就是我们的先验。除此之外，还有第二项极大似然（也就是我们希望最大化观测的样本的概率）。 对于如何通过极大似然推导得到我们的目标函数，李宏毅老师PPT中讲的我觉得没有下面这个清楚。这个就很直观了。 假设之前的我们懂了，我们总归是得到了这样的一个目标损失函数。一个是重构loss，一个是kl散度。 对于第一项kl散度，我们假设先验 $p(z)$ 是正态分布，通过推导可以得到： 这样就跟第17行中的代码一样了。也就是 lower bound的第一项。 对于第二项重构损失： 这里用到的就是一种在参数技巧。直接生成均值和方差，把后验分布这个转换成一个多维的正态分布。 至此，loss中的两项就讲完了。训练代码： 12345678910111213141516def train(model, data, epochs=20): opt = torch.optim.Adam(model.parameters()) for epoch in range(epochs): print(f&quot;epoch: {epoch+1}/{epochs}&quot;) for x, y in data: x = x.to(device) # GPU opt.zero_grad() x_hat = model(x) loss = ((x - x_hat)**2).sum() + model.encoder.kl loss.backward() opt.step() return model latent_dims = 2vae = VariationalAutoencoder(latent_dims).to(device) # GPUvae = train(vae, data) 同样为了可视化方便，我们把latent_dims=2. 可视化代码与之前一致： 边缘的位置还是看起来并不连续，可能是可视化样本比较少。 重构代码与之前一致： 效果确实好了一些。。那么VAE的“后验崩塌”在这个图里有体现吗，好像确实更倾向于生成“1”和“7” ？ VQ-VAEVAE生成的是连续的潜空间，AE生成的是离散的潜空间。那么问题来了，这不是回到原始AE了吗。确实，VQ-VAE本质上更像AE，而不是VAE。但是不同于AE的是，对于vanilla AE，一个训练样本/图像对应一个fixed vector(维度为latent_dims)，也就是潜空间中的一个点。而VQ-VAE则是将图像中的一个patch映射到一个codebook中的一个embedding vector(可视化这些codes可能会有有趣的发现)。可以这么说，一个样本/图像是由潜空间中的多个点构成的，这个潜空间就我们需要学习/维护的codebook。 我的理解，不同的潜空间中的点组合就可以生成新的样本/图像了？而且相比AE，潜空间明显更强大了，所以可以避免“后验崩塌”（decoder直接忽略潜向量去生成样本/图像） “Introducing the VQ-VAE model, which is simple, uses discrete latents, does not suffer from “posterior collapse” and has no variance issues. “ 这是论文中的一句话。 下面是reddit网友/大佬的解释： Unlike VAEs with continuous latent spaces, VQVAEs use a discrete space by having a codebook containing a large number of continuous vectors. The VQVAE encoding process maps an image to a continuous space then for each spatial location changes each vector to the closest one in the codebook. Brief recap of posterior collapse in case you’re not sure: my understanding is that VAE models struggle with posterior collapse when (a) the latents contain little information about the input data and (b) a powerful generative model (e.g. an autoregressive decoder) is used that can model the data distribution without any latents. At the start of training the latents often contain little information about the data so the generative model can ignore them and focus on modelling the data distribution on its own. This results in a lack of gradients to the encoder amplifying the problem and so the latents are never used (i.e. posterior collapse). Specifically with VQVAEs the latents (although discrete) are pretty high dimensional so can store a LOT of information about the input, so this helps with (a). As for (b), the decoder tends to be a fairly simple conv net so the latents are definitely needed to reconstruct the input. As for the variance issues since VAE encoders are probabilistic, training requires sampling the encoder outputs which can have high variance. Gaussian VAEs bypass this using the reparameterisation trick (here is a great discussion on this https://ermongroup.github.io/cs228-notes/extras/vae/). While VQVAEs have deterministic encoders, the discretisation process can introduce variance, however, they use the straight through estimator which is biased but has low variance, I believe. This leads to other issues such as codebook collapse, where some codes are never used. DALL-E on the other hand uses Gumbel Softmax. 这段话解释了VQ-VAE为啥能解决“后验崩塌”的问题，但是也带来了新的问题，“codebook collapse”，也就是通过encoder压缩之后的vector映射到codebook中的向量时，只会使用到codebook中的很小一部分vectors。 模型前向的框架如下： 很好理解的一个过程，我们可以把 $z_e \\rightarrow z_q$ 看作是一个聚类的过程（也确实可以用kmeans方法），甚至也可以看作是一个再参数化的过程，只不过这个过程不可导。 因为存在argmin所以不可导，那么这个就是我们需要解决的一个问题。 怎么解决这个问题呢，把 $z_q$ 的梯度传递到 $z_e$，这听起来太玄乎了。让我们来看看代码吧： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class Encoder(nn.Module): def __init__(self, latent_dims, pic_channels=1): super(Encoder, self).__init__() self.conv1 = nn.Conv2d(in_channels=pic_channels, out_channels=latent_dims//2, kernel_size=4) self.conv2 = nn.Conv2d(in_channels=latent_dims//2, out_channels=latent_dims, kernel_size=4) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.conv2(x) #print(x) return x class Decoder(nn.Module): def __init__(self, latent_dims, pic_channels=1): super(Decoder, self).__init__() self.conv_trans1 = nn.ConvTranspose2d( in_channels=latent_dims, out_channels=latent_dims//2, kernel_size=4) self.conv_trans2 = nn.ConvTranspose2d( in_channels=latent_dims//2, out_channels=pic_channels, kernel_size=4) def forward(self, x): x = self.conv_trans1(x) x = F.relu(x) x = self.conv_trans2(x) return x class VectorQuantizer(nn.Module): def __init__(self, latent_dims, num_codes=32, beta=0.25): super(VectorQuantizer, self).__init__() self.K = num_codes self.D = latent_dims self.beta = beta self.codebook = nn.Embedding(self.K, self.D) self.codebook.weight.data.uniform_(-1 / self.K, 1 / self.K) self.vq_loss = 0 def forward(self, latents): ''' latents: (batch, dim, height, width) codebook: (K, dim) ''' # convert latents from BCHW -&gt; BHWC latents = latents.permute(0, 2, 3, 1).contiguous() # (B, H, W, dim) latents_shape = latents.shape # Flatten latent flat_latent = latents.view(-1, self.D) # (BHW, dim) # Compute L2 distance between latents and codes in codebook dist = (flat_latent.unsqueeze(1) - self.codebook.weight.unsqueeze(0)) ** 2 # (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim) dist = dist.sum(-1) # (BHW, K) # Get the code index that has the min distance nearest_idxs = torch.argmin(dist, dim=1).unsqueeze(1) # (BHW, 1) # Convert to one-hot nearest_one_hot = torch.zeros(nearest_idxs.size(0), self.K, device=latents.device) # (BHW, K) nearest_one_hot.scatter_(1, nearest_idxs, 1) # .scatter(dim,index,src) # Quantize the latents quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) # (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim) # Compute the VQ Losses commitment_loss = F.mse_loss(quantized_latents.detach(), latents) codebook_loss = F.mse_loss(quantized_latents, latents.detach()) self.vq_loss = commitment_loss * self.beta + codebook_loss # convert quantized from BHWC -&gt; BCHW return quantized_latents.permute(0, 3, 1, 2).contiguous() class VQVariationalAutoencoder(nn.Module): def __init__(self, latent_dims, ema=True): super(VQVariationalAutoencoder, self).__init__() self.encoder = Encoder(latent_dims) if ema: self.vector_quantizer = VectorQuantizerEMA(latent_dims) else: self.vector_quantizer = VectorQuantizer(latent_dims) self.decoder = Decoder(latent_dims) def forward(self, x): z_e = self.encoder(x) z_q = self.vector_quantizer(z_e) # (batch, dim, 22, 22) return self.decoder(z_q) 看代码其实也挺简单的： 52-65行计算 l2 距离，然后选择距离最小的index，再通过这个index去codebook中选取对应的vector 这里的 quantized_latents 也就是我们想到得到的 $z_q$ 68-69行代码对应下式中后两个loss 这里有个不太懂的问题，$z_q$的计算过程因为第58行argmin肯定是不可导的。可是代码中也没有见到做任何处理。在[苏剑林的博客](VQ-VAE的简明介绍：量子化自编码器 - 科学空间|Scientific Spaces)中讲解了这一点： 按照这个思路，我觉得第74行代码应该改成下面这样才对，这样确确实实做到了把 $z_q$ 的梯度传递给 $z_e$. 12quantized_latents = latents + (quantized_latents - latents).detach()return quantized_latents.permute(0, 3, 1, 2).contiguous() 提供的代码链接确实是有问题的，改成上述两行代码之后就可以跑出效果了～ 接下来我们训练VQ-VAE,并可视化训练过程中的codebook和重构情况。为了方便可视化，我们同样设置latent_dims=2 12345678910111213141516171819202122232425262728293031323334353637383940def plot_codebook(autoencoder): codes = autoencoder.vector_quantizer.codebook.weight.detach().cpu().numpy() for i in range(codes.shape[0]): plt.scatter(codes[i][0], codes[i][1]) plt.show() def plot_recon(autoencoder, data, n=10): x = next(iter(data))[0][:n] x_hat = autoencoder(x.to(device)) x_hat = x_hat.to('cpu').detach().numpy().squeeze(1) w = x_hat.shape[1] img = np.zeros((w, n*w)) print(&quot;original:&quot;) for i in range(x_hat.shape[0]): img[:, i*w:(i+1)*w] = x[i] plt.imshow(img) plt.show() print(&quot;reconstructed:&quot;) for i in range(x_hat.shape[0]): img[:, i*w:(i+1)*w] = x_hat[i] plt.imshow(img) plt.show() def train(autoencoder, data, epochs=20): opt = torch.optim.Adam(autoencoder.parameters()) for epoch in range(epochs): print(f&quot;epoch: {epoch+1}/{epochs}&quot;) for x, y in data: x = x.to(device) # GPU opt.zero_grad() x_hat = autoencoder(x) loss = ((x - x_hat)**2).sum() + autoencoder.vector_quantizer.vq_loss loss.backward() opt.step() plot_codebook(autoencoder) plot_recon(autoencoder, data) return autoencoder 训练的可视化过程如下： 这效果有点玄学，第一次跑出来效果挺好，之后跑出来就很模糊。。 把维度扩大到 latent_dims=32后效果会变好，效果如下： EMA update codebook： 对于loss function中的第二项 $||sg(E(x))- e_{k}||$ 只是用来更新codebook，这个可以用 EMA的方法来更新。也就是对于某一个code $e_i$，它是被 $z_{i,1},z_{i,2},…,z_{i,n}$ 选中的code vector，因此它应该于这些encoder output vector的均值 $\\frac{1}{n}\\sum_{j}^{n}z_{i,j}$ 更接近。 当使用小批量(minibatches) 训练时，由于数据量不足，直接用这个均值来更新 $e_i$是不准确的。所以用 指数滑动平均(exponential moving average) 来更新 $e_i$： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class VectorQuantizerEMA(nn.Module): def __init__(self, latent_dims, num_codes=32, beta=0.25, gamma=0.99, epsilon=1e-5): super(VectorQuantizerEMA, self).__init__() self.K = num_codes self.D = latent_dims self.beta = beta self.gamma = gamma self.epsilon = 1e-5 self.codebook = nn.Embedding(self.K, self.D) self.codebook.weight.data.normal_() self.N = None self.m = None self.vq_loss = 0 def forward(self, latents): ''' latents: (batch, dim, height, width) codebook: (K, dim) ''' # convert latents from BCHW -&gt; BHWC latents = latents.permute(0, 2, 3, 1).contiguous() # (B, H, W, dim) latents_shape = latents.shape # Flatten latent flat_latent = latents.view(-1, self.D) # (BHW, dim) # Compute L2 distance between latents and codes in codebook dist = (flat_latent.unsqueeze(1) - self.codebook.weight.unsqueeze(0)) ** 2 # (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim) dist = dist.sum(-1) # (BHW, K) # Get the code index that has the min distance nearest_idxs = torch.argmin(dist, dim=1).unsqueeze(1) # (BHW, 1) # Convert to one-hot nearest_one_hot = torch.zeros(nearest_idxs.size(0), self.K, device=latents.device) # (BHW, K) nearest_one_hot.scatter_(1, nearest_idxs, 1) # .scatter(dim,index,src) # Quantize the latents quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) # (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim) # Compute the VQ Losses commitment_loss = F.mse_loss(quantized_latents.detach(), latents) self.vq_loss = commitment_loss * self.beta # EMA update cookbook n = torch.sum(nearest_one_hot, 0) # (K) self.N = self.N * self.gamma + (1 - self.gamma) * n if self.N is not None else n N_ = torch.sum(self.N.data) # Laplace smoothing of the cluster size self.N = (self.N + self.epsilon) / (N_ + self.K * self.epsilon) * N_ z = torch.matmul(nearest_one_hot.T, flat_latent) # (K, BHW) * (BHW, dim) = (K, dim) self.m = nn.Parameter(self.m * self.gamma + (1 - self.gamma) * z) if self.m is not None else nn.Parameter(z) self.codebook.weight = nn.Parameter(self.m / self.N.unsqueeze(1)) # convert quantized from BHWC -&gt; BCHW quantized_latents = latents + (quantized_latents - latents).detach() return quantized_latents.permute(0, 3, 1, 2).contiguous() 潜空间维度 latent_dims=2， 可视化如下： 潜空间维度为32， 可视化如下： 前面这些可视化只是在训练过程中的重构，那么我们是否可以像VAE那样，丢掉encoder，直接从 latent space中采样去生成新的样本/图像呢？ 但是随机采样出来的codes组合起来的image大概乱七八糟的。。这个时候就在这个低维空间去训练一个自回归模型，来正确采样出有效有意义的codes 在训练VQ-VAE完之后。我们并不能去生成新的样本/图像。怎么去从潜空间采样出这些离散的codes呢？现在我们需要训一个自回归的模型，让这些codes的排列变得有意义。我们可以把这些codes的序列看成一句话，然后训练这样一个自回归模型，也就是stage2 prior training。下图是VQ-VAE-2的伪代码（2.0版本可以看作是一个层次化的VQ-VAE). 简单解释下： $e_{top}\\leftarrow Quantize(h_{top})$ 中 $h_{top}$ 就是encoder的输出，通过量化(映射)之后得到 $e_{top}$ 对于bottom也是一样的 prior training: $T_{top}$ 就是 $e_{top}$ 的序列 $p_{top}=TrainPixelCNN(T_{top})$ 训练自回归模型 VideoGPT 左边就是原始的VQ-VAE. 右侧是训练一个自回归网络。这么看这篇paper确实没啥创新点啊。。就是把vq-vae应用在了video上，不过代码其实复杂了很多。 分两个阶段： VQ-VAE training VideoGPT training 看完代码，第二阶段forward过程如下： 需要注意的是，经过vqvae.encoder之后的维度变成了 [t/4, h/4,w/4], 这就是 latent vectors的个数，后续就是在这个上面做self-attention以及自回归 (注意自回归需要decoder input right-shift). LVT这篇跟 VideoGPT很像，都是在潜空间内做自回归，但是这篇paper在VideoGPT之前发表。而且看了openviewer的审稿意见，reviewers 就怼VideoGPT跟这篇很像。。但是两者的模型结构是有区别的 DVT-NAT Fast Decoding in Sequence Models Using Discrete Latent Variables, (ICML2018) 这篇paper好呀好呀，是真好！！关键还是18年就发表的，是真的牛逼！反思一下为啥别人就能在紧跟前沿，而且还能从CV领域follow出这么厉害的NLP的工作呢？ Abstract 提出了一种对目标序列进行离散建模的方法，能有效提高自回归建模的效率，从而提升解码速度 To overcome this limitation, we propose to introduce a sequence of discrete latent variables $l_1 . . . l_m$, with $m &lt; n$, that summarizes the relevant information from the sequence $y_1 . . . y_n$. We will still generate $l_1 . . . l_m$ autoregressively, but it will be much faster as $m &lt; n$ (in our experiments we mostly use $m = n/8$ ). Then, we reconstruct each position in the sequence $y_1 . . . y_n$ from $l_1 . . . l_m$ in parallel. 以前的seq2seq方式是 $(x_1,…,x_L) \\rightarrow (y_1,…,y_n)$ ，现在是 $(x_1,…,x_L) \\rightarrow (l_1,..,l_m)\\rightarrow (y_1,…,y_n)$ ,其中 $l_1,…,l_m$ 的生成依然是自回归的，但是长度更短， 所以提高了效率。 将 latent transformer 应用在机器翻译上，提升翻译效率。但是在BLEU上仍然比自回归的方法要差很多。 Contribution 提出了一个基于离散潜变量的快速解码框架 提出新的离散化技术，能有效缓解VQ-VAE中的index collapse问题 将 latent transofmer 应用在机器翻译上，提升了翻译速度 Discretization Techniques Gumbel-Softmax 将argmax/argmin可微化。将采样过程用可导的softmax代替，同时加上gumbel noise使得最终的结果跟 PyTorch 32.Gumbel-Softmax Trick - 科技猛兽的文章 - 知乎 https://zhuanlan.zhihu.com/p/166632315 Improved Semantic Hashing Vector Quantization Decomposed Vector Quantizationmotivationindex collapse, where only a few of the embedding vectors get trained due to a rich getting richer phenomena Sliced Vector Quantization break up the encoder output enc(y) into n_d smaller slices, like multi-head attention in transformer (eq. 11) Latent Transformer Main Steps VAE encoder encodes target sentence $y$ into shorter discrete latent variables $l$ (parallel) Latent prediction model - Transformer, is trained to predict $l$ from source sentence $x$ (autoregressive) VAE decoder decodes predicted $l$ back to sequence $y$ (parallel) Loss Function reconstruction loss $l_r$ from VAE latent prediction loss $l_{lp}$ from Latent Transformer in first 10k steps, true targets $y$ is given to transformer-decoder instead of decompressed latents $l$ which ensures self-attention part has reasonable gradients to train the whole architecture Architectures of VAE Encoder conv residual blocks + attention + conv to scale down the dimension $C = n/m, C=2^c$, in the setting, $C=8, c=3$ Decoder conv residual blocks + attention + up-conv to scale up the dimension Transformer Decoder","link":"/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/"},{"title":"论文笔记-Denoising Diffusion Probabilistic Models","text":"What are Diffusion Models? (lilianweng.github.io) Yang Song | Generative Modeling by Estimating Gradients of the Data Distribution (yang-song.github.io) Diffusion Models Beat GANs on Image Synthesis (2105.05233.pdf (arxiv.org)) Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes 2111.12701.pdf (arxiv.org) Cascaded Diffusion Models for High Fidelity Image Generation cascaded_diffusion.pdf (cascaded-diffusion.github.io) DDPM 1: https://arxiv.org/pdf/1503.03585.pdf DDPM 2: https://arxiv.org/pdf/2006.11239.pdf WaveNet: https://arxiv.org/pdf/1609.03499.pdf DiffWave: https://arxiv.org/pdf/2009.09761.pdf How to Train Your Energy-Based Models https://arxiv.org/pdf/1906.02691.pdf； Generating Diverse High-Fidelity Images with VQ-VAE-2 https://arxiv.org/pdf/1906.00446.pdf； PIXELCNN++: Improving The PlxelcnnI With Discretized Logistic Mixture Likelihood And Other Modifications https://openreview.net/pdf?id=BJrFC6ceg； U-Net: Convolutional Networks for Biomedical Image Segmentation https://arxiv.org/pdf/1505.04597.pdf； On Maximum Likelihood Training of Score-Based Generative Models https://arxiv.org/pdf/2101.09258.pdf； Flow-based Deep Generative Models https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html； From Autoencoder to Beta-VAE https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html； WaveNet: A generative model for raw audio https://deepmind.com/blog/artic","link":"/2021/11/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Denoising-Diffusion-Probabilistic-Models/"},{"title":"论文笔记-GAN tutorial NIPS 2016","text":"为什么要学习 GAN？ High-dimensional probability distributions, 从高维概率分布中训练和采样的生成模型具有很强的能力来表示高维概率分布。 Reinforcement learning. 和强化学习结合。 Missing data. 生成模型能有效的利用无标签数据，也就是半监督学习 semi-supervised learning。 生成模型如何工作的Maximum likehood estimation极大似然估计就是在给定数据集的情况下，通过合理的模型（比如语言模型）计算相应的似然（概率），使得这个概率最大的情况下，估计模型的参数。 $$\\sum_i^mp_{\\text{model}}(x^{(i)}; \\theta)$$ m 是样本数量。 将上诉概率转换到 log 空间（log 函数是单调递增的），可以将乘积转换为相加，简化了计算。 同样的，也可以把上诉似然估计的极大化看做是最小化 KL 散度（或者交叉熵）。这样一来，相当于把无监督问题转换成了有监督问题。（不知理解的是否正确？） 相对熵熵是信息量 $log{\\dfrac{1}{p(i)}}$ (概率越大信息量越少)。 p 是真实分布， q 是非真实分布。 交叉熵是用 q 分布来估计 p 分布所需要消除的代价 cost: $$H(p,q) = \\sum_ip(i)log{\\dfrac{1}{q(i)}}$$ 用真实分布 p 估计真实分布所需要的 cost: $$H(p) = \\sum_ip(i)log{\\dfrac{1}{p(i)}}$$ 从这里也能看出，当概率 p 为 1 时，所需要的 cost 就为 0 了。 相对熵，又称 KL 散度(Kullback–Leibler divergence)，就是指上诉两者所需要消耗的 cost 的差值： $$D(p||q) = H(p,q)-H(p) = \\sum_ip(i)log{\\dfrac{p(i)}{q(i)}}$$ GAN 是如何工作的？GAN 框架 判别器 discriminator 生成器 gererator 在左边场景，判别器学习如何分别样本是 real or fake. 所以左边的输入是 half real and half fake. 在右边的v场景，判别器和生成器都参与了。G 用来生成 G(z), D 用来判别 G(z) 是 real or fake. 结构化概率模型 Structured Probabilistic Modelsfor Deep Learning containing latent variables z and observed variables x. z 是需要学习的隐藏变量，x 是可观察到的变量。 判别器 D 的输入是 x，其需要学习的参数是 $\\theta^{(D)}$. 生成器 G 的输入是 z, 其需要学习的参数是 $\\theta^{(G)}$. 两个玩家都有各自的损失函数。 $J^{(D)}(\\theta^{(D)}, \\theta^{(G)})$, 但是 $J^{(D)}$ 并不影响参数 $\\theta^{(G)}$. 同样的道理，反过来也是 $J^{(G)}(\\theta^{(G)}, \\theta^{(D)})$. 每个玩家的损失函数依赖于另一个玩家的参数，但是确不能控制它的参数。所以这是一种 Nash equilibrium 问题。找到这样一个局部最优解 $tuple(\\theta^{(G)}, \\theta^{(D)})$ 使得 $J^{(D)}$ 关于 $\\theta^{(D)}$ 局部最小，$J^{(G)}$ 关于 $\\theta^{(G)}$ 局部最小。 Generatordifferentiable function G, 可微分函数 G. 实际上就是 神经网络。z 来自简单的先验分布，G(z) 通过模型 $p_{model}$ 生成样本 x. 实践中，对于 G 的输入不一定只在第一层 layer, 也可以在 第二层 等等。总之，生成器的设计很灵活。 Training process两个 minibatch 的输入： x 来自 training dataset. z 来自先验隐藏变量构成的模型 $p_\\text{model}$。通过优化算法 SGD/Adam 对两个损失 $J^{(D)} J^{(G)}$ 通过进行优化。梯度下降一般是同步的，也有人认为两者的迭代步数可以不一样。在这篇 tutorial 的时候，得到的共识是同步的。 cost function目前大多数 GAN 的损失函数，$J(D)$ 都是一样的。区别在于 $J(G)$. The discriminator’s cost, J (D) 这是个标准的用于二分类的交叉熵损失函数。只不过这里，正分类都来自于训练集，正分类的概率是 $\\dfrac{1}{2}E_{x～d_{data}}$, 负分类来自于生成器，则其概率是 $\\dfrac{1}{2}E_z$ 通过训练判别器，我们可以得到这样一个比例: $$\\dfrac{p_{data}(x)}{p_{\\text{model}}(x)}$$ GANs 通过监督学习来获得这个 ratio 的估计，这也是 GANs 不同于 变分自编码 和 波尔兹曼机 (variational autoencoders and Boltzmann machines) 的区别。 但是 GANs 通过监督学习来估计这个 ratio,会向监督学习一样遇到同样的问题：过拟合和欠拟合。但是通过足够的数据和完美的优化可以解决这个问题。 Minimax, zero-sum game设计 cost function for generator. 前面我们知道，两个玩家 player 或者说 神经网络 G，D 实际上是一种博弈，D 希望能找出 G(z) 是 fake，而 G 希望能让 G(z) 尽可能像 real. 所以最简单的一种方式就是 all player’s cost is always zero. $$J^{(G)} = -J^{(D)}$$ 这样 $J^{(G)}, J^{(D)}$ 都可以用 value function 表示： $$V(\\theta^{(D)}, \\theta^{(G)})=-J^{(D)}(\\theta^{(D)}, \\theta^{(G)})$$ 那么整个 game 也就是一个 minmax 游戏： outer loop 是关于 $\\theta^{(G)}$ 的最小化，inner loop 是关于 $\\theta^{(D)}$ 的最大化。 但是这种 function 在实际中并不能使用，因为其非凸性。 In practice, the players are represented with deep neural nets and updates are made in parameter space, so these results, which depend on convexity, do not apply Heuristic, non-saturating game Minimizing the cross-entropy between a target class and a classifier’s predicted distribution is highly effective because the cost never saturates when the classifier has the wrong output. 对一个分类器，最小化 目标类和预测概率的交叉熵 是一个非常有效的方法，因为当 分类器 存在误分类时，损失函数就永远不可能饱和。 所以，对于生成器的 cost 依旧使用交叉熵，但如果使用和 判别器一模一样的 cost(这里应该就是把正负分类反过来？)： $$J^{(G)}(\\theta^{(D)}, \\theta^{(G)})=-\\dfrac{1}{2}E_zlogD(G(z))-\\dfrac{1}{2}E_{x～p_{data}}log(1-D(x))$$ 猜想应该是这样，文中没有给出。 不幸的是这样的在是实际中，也并不可行。当 判别器 拒绝一个高置信度(high confidence) 的样本时，生成器会出现梯度消失。 所以，改进之后就是: $$J^{(G)}(\\theta^{(D)}, \\theta^{(G)})=-\\dfrac{1}{2}E_zlogD(G(z))$$ In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the logprobability of the discriminator being mistaken. 在 Minmax，zero-game 中，生成器的目的是最小化 判别器 自认为自己判别对了的 log-probability, 而在 non-saturating game 中，生成器是最大化 判别器判别错误 的 log-probability. Maximum likelihood game前面提到极大似然估计是最小化模型与数据之间的 KL 散度。 GANs 使用极大似然估计则是对比不同的模型。 $$J^{(G)}=-\\dfrac{1}{2}E_zexp(\\sigma^{-1}(D(G(z))))$$ in practice, both stochastic gradient descent on the KL divergence and the GAN training procedure will have some variance around the true expected gradient due to the use of sampling (of x for maximum likelihood and z for GANs) to construct the estimated gradient. 在实践中，由于使用采样（x表示最大似然，z表示GAN）来构建估计的梯度，因此KL散度和GAN训练过程的随机梯度下降都会在真实预期梯度附近产生一些变化。 Is the choice of divergence a distinguishing feature of GANs?Jensen-Shannon divergence， reverse KL许多人认为 GANs 能够生成更加清晰、逼真的样本是因为他们最小化的是 Jensen-Shannon divergence. 而 VAEs 生成模糊的样本是因为他们最小化的是 KL divergence between the data and the model. KL 散度并不是对称的。$D_{KL}(p_{data}||q_{model})$ 与 $D_{KL}(p_{model}||q_{data})$ 是不一样的。极大似然估计是前者，最小化 Jensen-Shannon divergence 则更像后者。 比较 $D_{KL}(p_{data}||q_{model})$ 与 $D_{KL}(p_{model}||q_{data})$ 区别。在模型能力不足以拟合数据的分布时，表现的尤为明显，如上图。给定的数据是两个高斯分布混合的分布。而模型是一个高斯模型。然后分别用极大似然 Maximum likehood 、reverse KL 散度作为 criterion，也就是 cost. 可以看到前者选择去平均两个模态，并希望在两个模态上都能得到较高的概率。而后者只选择其中一个模态，也有可能是另外一个模态，两个模态对于 reverse KL 都含有局部最优解。 所以，从这个视角来看， Maximum likehood 倾向于给 data 出现的位置更高的概率，而 reverse KL 则倾向于给没有出现 data 的位置较低的概率。所以 $D_{KL}(p_{model}||q_{data})$ 可以生成更棒的样本，因为模型不会生成不常见的样本，因为数据之间具有欺骗性的模态。 然而，也有一些新的研究表明，Jensen-Shannon divergence 并不是 GANs 能生成更清晰样本的原因。 f-GAN 证明，KL 散度也能生成清晰的sample，并且也只选择少量的modes, 说明 Jensen-Shannon divergence 并不是 GANs 不同于其他模型的特征。 GANs 通常选择少量的 mode 来生成样本，这个少量指的是小于模型的能力。 而 reverse KL 则是选择更可能多的 mode of the data distribution 在模型能力范围内。它通常不会选择更少的 mode. 这也解释了 mode collapse 并不是散度选择的原因。 Altogether, this suggests that GANs choose to generate a small number of modes due to a defect in the training procedure, rather than due to the divergence they aim to minimize. 所以，GANs 选择少量的 mode 是因为训练过程中的其他缺陷，而不是 散度 选择的问题。 Comparison of cost functions生成对抗网络可以看做一种 reinforcement learning. 但是$j^{(G)}$ 并没有直接参考 training data，所有关于 training data 的信息都来自于 判别器 的学习。 所以和传统的强化学习是有区别的： 比较不同的 cost function： $D(G(z))$ 表示 判别器 给 generate sample 为真的概率。 在左侧，Minimax 和 Maximum likehood 都趋向于饱和。也就是说，当一个样本很明显为 fake 时，cost 接近于 0. 似然估计还有个问题，cost 主要来源于 特别像真 的少部分样本，这也是不好的。需要用到 variance reduction techniques. Maximum likelihood also suffers from the problem that nearly all of the gradient comes from the right end of the curve, meaning that a very small number of samples dominate the gradient computation for each minibatch. This suggests that variance reduction techniques could be an important research area for improving the performance of GANs, especially GANs based on maximum likelihood. The DCGAN architecture DCGAN 的结构。 GAN，NCE， MLE 的对比 相同点： MiniMax GAN 和 NCE 的 cost function 相同 不同点： 更新策略不一样，GAN 和 MLE 都是梯度下降，而 MLE copies the density model learned inside the discriminator and converts it into a sampler to be used as the generator. NCE never updates the generator; it is just a fixed source of noise. Tips and TricksHow to train a GAN: https://github.com/soumith/ganhacks Research FrontiersNon-convergencemode collapse","link":"/2019/01/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-GAN-tutorial-NIPS-2016/"},{"title":"论文笔记-Explicit Semantic Analysis","text":"paper: Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysi, IJCAI2008 Wikipedia-based Semantic Interpretation for Natural Language Processing Motivation对于自然语言的语义表示，需要的大量的 common sense 和 world knowledge. 前人的研究使用统计的方法，例如 WordNet，仅仅只利用了有限的词典知识（lexicographic knowledge），并不能有效的利用语言本身背后的背景知识（ background knowledge）。 作者提出了 Explicit Semantic Analysis (ESA)，能够对文本或单词进行可解释性的细粒度的语义表示。 Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. 显示的使用 wiki 中的概念（concepts）来表示任意长度的 text. 作者通过文本分类和计算自然语言的文本片段之间的相似度来验证 ESA 的有效性。由于语义表示使用的是 natural concepts，ESA 模型的可解释性非常强。 传统的方法： 词袋模型: 将 text 看作是 unordered bags of words, 每一个单词看作是一维特征。但是这并不能解决 NLP 中的两个主要问题： 一词多义和同义词（polysemy and synonymy）。 隐语义分析：Latent Semantic Analysis (LSA) LSA is a purely statistical technique, which leverages word co-occurrence information from a large unlabeled corpus of text. LSA does not use any explicit human-organized knowledge; rather, it “learns” its representation by applying Singular Value Decomposition (SVD) to the words-by-documents co-occurrence matrix. LSA is essentially a dimensionality reduction technique that identifies a number of most prominent dimensions in the data, which are assumed to correspond to “latent concepts”. Meanings of words and documents are then represented in the space defined by these concepts. LSA 是一种纯粹的统计技术，它利用来自大量未标记文本语料库的单词共现信息。 LSA不使用任何明确的人类组织知识; 相反，它通过将奇异值分解（SVD）应用于逐个文档的共现矩阵来“学习”其表示。 LSA本质上是一种降维技术，它识别数据中的许多最突出的维度，假设它们对应于“潜在概念”。 然后，在这些概念定义的空间中表示单词和文档的含义。 词汇数据库，WordNet. However, lexical resources offer little information about the different word senses, thus making word sense disambiguation nearly impossible to achieve.Another drawback of such approaches is that creation of lexical resources requires lexicographic expertise as well as a lot of time and effort, and consequently such resources cover only a small fragment of the language lexicon. Specifically, such resources contain few proper names, neologisms, slang, and domain-specific technical terms. Furthermore, these resources have strong lexical orientation in that they predominantly contain information about individual words, but little world knowledge in general. 词汇资源几乎没有提供关于不同词义的信息，因此几乎不可能实现词义消歧。这种方法的另一个缺点是词汇资源的创建需要词典专业知识以及大量的时间和精力，因此 资源只涵盖语言词典的一小部分。 具体而言，此类资源包含很少的专有名称，新词，俚语和特定于域的技术术语。 此外，这些资源具有强烈的词汇取向，因为它们主要包含关于单个单词的信息，但总体上缺乏世界知识。 concept 定义 Observe that an encyclopedia consists of a large collection of articles, each of which provides a comprehensive exposition focused on a single topic. Thus, we view an encyclopedia as a collection of concepts (corresponding to articles), each accompanied with a large body of text (the article contents). 维基百科中每一个词条对应一个 concept. example: 对于文本：”Bernanke takes charge” 通过算法我们可以找到维基百科中相关的 concept: Ben Bernanke, Federal Reserve, Chairman of the Federal Reserve, Alan Greenspan (Bernanke’s predecessor), Monetarism (an economic theory of money supply and central banking), inflation and deflation. 对于文本：”Apple patents a Tablet Mac” 相关的 concept: Apple Computer 2 , Mac OS (the Macintosh operating system) Laptop (the general name for portable computers, of which Tablet Mac is a specific example), Aqua (the GUI of Mac OS X), iPod (another prominent product by Apple), and Apple Newton (the name of Apple’s early personal digital assistant). ESA 对一个 texts 的表示是 wiki 中所有的 concept 的 weighted combination，这里为了展示方便，只列举了最相关的一些 concept. ESA(explicit semantic analysis)通过 wiki 得到一个 basic concepts: $C_1, C_2,…, C_n$, 其中 $C_k$ 都是来源于 wiki. 表示一个通用的 n 维语义空间。 然后将任意长度的文本 t 表示成与上述向量长度相同的 权重向量 $w_1, w_2,…, w_n$ 分别表示 t 与 $C_k$ 之间的相关程度。 接下来两个步骤就是： the set of basic concepts the algorithm that maps text fragments into interpretation vectors 如何构建 concept 集合1.using Wikipedia as a Repository of Basic Concepts 维基百科词条中的内容也很关键，用来计算 concept 与输入文本中单词的相似度。 2.building a semantic interpreter 根据 wiki 得到基本的 concept，以及对应的文档， $d_1,..,d_n$. 构建一个 sparse 表格 T， 其中，列表示 concept，行表示文档中的单词对应的 TDIDF 值。也就是计算文档中的单词与所有文档 $\\bigcup_{i=1..n}d_i$ 的频率关系。 $$T[i,j]=tf(t_i, d_j)\\cdot log\\dfrac{n}{df_i}$$ 其中，Term Frequency - Inverse Document Frequency： TF 表示在文档 $d_j$ 中，单词 $t_i$ 出现的频率。 $$tf(t_i, d_j)=\\begin{cases} 1 + log\\ count(t_i, d_j), &amp;\\text{if count(t_i, d_j) &gt; 0} \\ 0, &amp;\\text{otherwise} \\end{cases}$$ IDF 表示逆文档频率。反应一个词在不同的文档中出现的频率越大，那么它的 IDF 值应该低，比如介词“to”。而反过来如果一个词在比较少的文本中出现，那么它的 IDF 值应该高。 $$IDF=log\\dfrac{n}{df_i}$$ $df_i=|{d_k:t_i\\in d_k}|$ 表示出现该单词的文档个数，n 表示总的文档个数。 正则化，cosine normalization: $$T[i,j]\\leftarrow \\dfrac{T[i,j]}{\\sqrt{\\sum_{l=1}^r T[i,j]^2}}$$ r 表示单词的总量。也就是除以所有单词对应的向量二范数之和平方。 得到 table T 之后，一个单词的向量 $t_i$ 表示就是第 i 行。一个文本片段 $&lt;t_1,..,t_k&gt;$ 的向量表示是文本中所有单词的质心。 如何将文本片段映射成向量表示","link":"/2019/02/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Explicit-Semantic-Analysis/"},{"title":"论文笔记-Multi-cast Attention Networks","text":"paper: [Multi-Cast Attention Networks for Retrieval-based Question Answering and Response Prediction](https://arxiv.org/abs/1806.00778) Motivation Our approach performs a series of soft attention operations, each time casting a scalar feature upon the inner word embeddings. The key idea is to provide a real-valued hint (feature) to a subsequent encoder layer and is targeted at improving the representation learning process. 在 encoder layer 之前将 document 和 query 进行交互，然后将权重赋予到 document 和 query 之后，在通过 contextual/encoder layer 编码融合了上下文信息的向量表示。这样做的目地，是为后续层提供提示（特征），提升表示学习的性能。 The key idea of attention is to extract only the most relevant information that is useful for prediction. In the context of textual data, attention learns to weight words and sub-phrases within documents based on how important they are. In the same vein, co-attention mechanisms [5, 28, 50, 54] are a form of attention mechanisms that learn joint pairwise attentions, with respect to both document and query. attention 注意力的关键思想是仅提取对预测有用的最相关信息。在文本数据的上下文中，注意力学习根据文档中的单词和子短语的重要性来对它们进行加权。 Attention is traditionally used and commonly imagined as a feature extractor. It’s behavior can be thought of as a dynamic form of pooling as it learns to select and compose different words to form the final document representation. 传统的 attention 可以看做为一个特征提取器。它的行为可以被认为是一种动态的pooing形式，因为它学习选择和组合不同的词来形成最终的文档表示，。 This paper re-imagines attention as a form of feature augmentation method. Attention is casted with the purpose of not compositional learning or pooling but to provide hints for subsequent layers. To the best of our knowledge, this is a new way to exploit attention in neural ranking models. 这篇 paper 将 attention 重新设想为一种特征增强的方式，Attention的目的不是组合学习或汇集，而是为后续层提供提示（特征）。这是一种在神经排序模型中的新方法。 不管这篇paper提供的新的 attention 使用方式是否是最有效的，但这里对 attention 的很多解释让人耳目一新，可以说理解的很透彻了。 An obvious drawback which applies to many existing models is that they are generally restricted to one attention variant. In the case where one or more attention calls are used (e.g., co-attention and intra-attention, etc.), concatenation is generally used to fuse representations [20, 28]. Unfortunately, this incurs cost in subsequent layers by doubling the representation size per call. 很多 paper 中只受限于一种 attention，这明显是不够好的。也有使用多种 attention 的，比如 co-attention 和 intra-attention， 然后直接拼接起来。这样会使得接下来的 modeling layer 维度加倍。。（在 ai challenge 的比赛中就是这么干的。。没啥不好啊。。） The rationale for desiring more than one attention call is intuitive. In [20, 28], Co-Attention and Intra-Attention are both used because each provides a different view of the document pair, learning high quality representations that could be used for prediction. Hence, this can significantly improve performance. 直觉上，使用多种 multi-attention 是靠谱的。 co-attention 和 intra-attention 提供了不同的视角去审视 document,用以学习高质量的向量表示。 关于各种 attention： https://zhuanlan.zhihu.com/p/35041012 [50] co-attention: dynamic coattention networks for question answering [5] Attentive Pooling Networks [28] [Inter-Weighted Alignment Network for Sentence Pair Modeling](https://aclanthology.info/pdf/D/D17/D17-1122.pdf) [54] Attentive Interactive Neural Networks for Answer Selection in Community Question intra-attention:Attention is all your need Moreover, Co-Attention also comes in different flavors and can either be used with extractive max-mean pooling [5, 54] or alignment-based pooling [3, 20, 28]. Each co-attention type produces different document representations. In max-pooling, signals are extracted based on a word’s largest contribution to the other text sequence. Mean-pooling calculates its contribution to the overall sentence. Alignment-pooling is another flavor of co-attention, which aligns semantically similar sub-phrases together. co-attention可以用于提取max-mean pooling或alignment-based pooling。每种co-attention都会产生不同的文档表示。在max-pooling中，基于单词对另一文本序列的最大贡献来提取特征；mean-pooling计算其对整个句子的贡献；alignment-based pooling是另一种协同注意力机制，它将语义相似的子短语对齐在一起。因此，不同的pooling操作提供了不同的句子对视图。 [3] Enhanced LSTM for Natural Language Inference [20] [A Decomposable Attention Model for Natural Language Inference](https://arxiv.org/abs/1606.01933) Our approach is targeted at serving two important purposes: (1) It removes the need for architectural engineering of this component by enabling attention to be called for an arbitrary k times with hardly any consequence and (2) concurrently it improves performance by modeling multiple views via multiple attention calls. As such, our method is in similar spirit to multi-headed attention, albeit efficient. To this end, we introduce Multi-Cast Attention Networks (MCAN), a new deep learning architecture for a potpourri of tasks in the question answering and conversation modeling domains. 两个方面的贡献： （1）消除调用任意k次注意力机制所需架构工程的需要，且不会产生任何后果。 （2）通过多次注意力调用建模多个视图以提高性能，与multi-headed attention相似。 In our approach, attention is casted, in contrast to the most other works that use it as a pooling operation. We cast co-attention multiple times, each time returning a compressed scalar feature that is re-attached to the original word representations. The key intuition is that compression enables scalable casting of multiple attention calls, aiming to provide subsequent layers with a hint of not only global knowledge but also cross sentence knowledge. 与大多数其他用作池化操作的工作相反，在我们的方法中，注意力被投射。通过多次投射co-attention，每次返回一个压缩的标量特征，重新附加到原始的单词表示上。压缩函数可以实现多个注意力调用的可扩展投射，旨在不仅为后续层提供全局知识而且还有跨句子知识的提示（特征）。 Model Architecture: Multi-cast Attention Networks Figure 1: Illustration of our proposed Multi-Cast Attention Networks (Best viewed in color). MCAN is a wide multi-headed attention architecture that utilizes compression functions and attention as features. 模型输入是 document/query 语句对。 Input Encoderembedding layer映射到向量空间： $w\\in W^d$ Highway Encoder： Highway Networks可以对任意深度的网络进行优化。这是通过一种控制穿过神经网络的信息流的闸门机制所实现的。通过这种机制，神经网络可以提供通路，让信息穿过后却没有损失，将这种通路称为information highways。即highway networks主要解决的问题是网络深度加深、梯度信息回流受阻造成网络训练困难的问题。 highway encoders can be interpreted as data-driven word filters. As such, we can imagine them to parametrically learn which words have an inclination to be important and not important to the task at hand. For example, filtering stop words and words that usually do not contribute much to the prediction. Similar to recurrent models that are gated in nature, this highway encoder layer controls how much information (of each word) is flowed to the subsequent layers. 在本文模型中，每个词向量都通过highway编码器层。highway网络是门控非线性变换层，它控制后续层的信息流。许多工作都采用一种训练过的投影层来代替原始词向量。这不仅节省了计算成本，还减少了可训练参数的数量。本文将此投影层扩展为使用highway编码器，可以解释为数据驱动的词滤波器，它们可以参数化地了解哪些词对于任务具有重要性和重要性。例如，删除通常对预测没有多大贡献的停用词和单词。与自然门控的循环模型类似，highway编码器层控制每个单词流入下一层多少信息。 $$y=H(x,W_H)\\cdot T(x,W_T) + (1-T(x,W_T))\\cdot x$$ 其中 $W_H, W_T\\in R^{r\\times d}$ 是可学习参数. H(.) 和 T(.) 分别是全连接加上 relu 和 sigmoid 的函数，用以控制信息的流向下一层。 co-attention Co-Attention [50] is a pairwise attention mechanism that enables attending to text sequence pairs jointly. In this section, we introduce four variants of attention, i.e., (1) max-pooling, (2) mean-pooling, (3) alignment-pooling, and finally (4) intra-attention (or self attention). 协同注意力机制是成对的注意力机制，能够同时关注文本序列对。作者引入了 4 中注意力机制。 1.affinity/similarity matrix$$s_{ij}=F(q_i)^TF(d_j)$$ 其中，F(.) 是多层感知机，通常可选择的计算相似矩阵的方式有： $$s_{ij}=q_i^TMd_j, s_{ij}=F[q_i;d_j]$$ 以及在 BiDAF 和 QANet 中使用的 $s_{ij}=F[q_i;d_j;q_i\\circ d_j]$. 2. Extractive poolingmax-pooling关注于另一个序列交互后，最匹配的那个词。 $$q’=soft(max_{col}(s))^Tq, d’=soft(max_{row}(s))^Td$$ soft(.) 是 softmax 函数。$q’,d’$ 是 co-attentive representations of q and d respectively. mean-pooling关注另一个句子的全部，取平均值。 $$q’=soft(mean_{col}(s))^Tq, d’=soft(mean_{row}(s))^Td$$ each pooling operator has different impacts and can be intuitively understood as follows: max-pooling selects each word based on its maximum importance of all words in the other text. Mean-pooling is a more wholesome comparison, paying attention to a word based on its overall influence on the other text. This is usually dataset-dependent, regarded as a hyperparameter and is tuned to see which performs best on the held out set. 不同的 pooling 操作有不同的影响，获取的信息也不一样。 max-pooling根据每个单词在其他文本中所有单词的最大重要性选择每个单词。mean-pooling是基于每个词在其他文本上的总体影响来关注每个词。 这其实是与数据集和任务相关的。可以看作超参数，然后调整看哪个在对应的任务和数据集上表现更佳。 3. Alignment-Pooling$$d_i’:=\\sum^{l_q}{j=1}\\dfrac{exp(s{ij})}{\\sum_{k=1}^{l_q}exp(s_{ik})}q_j$$ 其中 $d_i’$ 是 q 和 $d_i$ 的软对齐。直观的说，$d_i’$ 是关于 $d_i$ 的 ${q_j}^{l_q}_{j=1}$ 的加权和。 $$q_i’:=\\sum^{l_d}{j=1}\\dfrac{exp(s{ij})}{\\sum_{k=1}^{l_d}exp(s_{ik})}d_j$$ $q_i’$ 是 $q_i$ 和 d 的软对齐。也就是，$q_i’$ 是 关于 $q_i$ 的 ${d_j}^{l_d}_{j=1}$ 的加权和。 4. intra-Attention$$x_i’:=\\sum^{l}{j=1}\\dfrac{exp(s{ij})}{\\sum_{k=1}^{l}exp(s_{ik})}x_j$$ 也就是自注意力机制。相比 attention is all your need,可能就是相似矩阵不一样吧。 Multi-Cast AttentionCasted Attention用 $x$ 来表示 q 或 d，$\\overline x$ 表示 经过 co-attention 和 soft-attention alignment 后的序列表示 $q’, d’$. $$f_c=F_c[\\overline x, x]$$ $$f_m=F_m[\\overline x \\circ x]$$ $$f_s=F_m[\\overline x-x]$$ 其中 $\\circ$ 是 Hadamard product. $F(.)$ 是压缩函数，将特征压缩到 scalar. Intuitively, what is achieved here is that we are modeling the influence of co-attention by comparing representations before and after co-attention. For soft-attention alignment, a critical note here is that x and $\\overline x$ (though of equal lengths) have ‘exchanged’ semantics. In other words, in the case of q, $\\overline q$ actually contains the aligned representation of d. Compression Function The rationale for compression is simple and intuitive - we do not want to bloat subsequent layers with a high dimensional vector which consequently incurs parameter costs in subsequent layers. We investigate the usage of three compression functions, which are capable of reducing a n dimensional vector to a scalar. 本节定义了Fc(.) 使用的压缩函数，不希望使用高维向量膨胀后续层，这会在后续层中会产生参数成本。因此本文研究了三种压缩函数的用法，它们能够将n维向量减少到标量。 Sum Sum（SM）函数是一个非参数化函数，它对整个向量求和，并输出标量 $$F(x)=\\sum_i^nx_i, x_i\\in x$$ Neural networks $$F(x)=RELU(W_cx+b)$$ 其中 $W_c\\in R^{n\\times 1}$ Factorization Machines 因子分解机是一种通用机器学习技术，接受实值特征向量 $x\\in R^n$ 并返回标量输出。 FM是表达模型，使用分解参数捕获特征之间的成对相互作用。 k是FM模型的因子数。 Multi-Cast我们的架构背后的关键思想是促进k个注意力投射，每个投射都用一个实值注意力提示来增强原始词向量。 对于每个query-document对，应用Co-Attention with mean-pooling，Co-Attention with max-Pooling和Co-Attention with alignment-pooling。 此外，将Intra-Attention分别单独应用于query和document。 每个注意力投射产生三个标量（每个单词），它们与词向量连接在一起。最终的投射特征向量是 $z\\in R^{12}$。 因此，对于每个单词 w_{i} ，新的表示成为 $\\bar{w_{i}}=[w_{i};z_{i}]$ Long Short-Term Memory Encoder接下来，将带有casted attetnion的单词表示 $\\bar{w_{1}},\\bar{w_{2}},…,\\bar{w_{l}}$ 传递到序列编码器层。采用标准的长短期记忆（LSTM）编码器. As such, the key idea behind casting attention as features right before this layer is that it provides the LSTM encoder with hints that provide information such as (1) longterm and global sentence knowledge and (2) knowledge between sentence pairs (document and query). LSTM在document和query之间共享权重。 关键思想是LSTM编码器通过使用非线性变换作为门控函数来学习表示序列依赖性的表示。因此，在该层之前引人注意力作为特征的关键思想是它为LSTM编码器提供了带有信息的提示，例如长期和全局句子知识和句子对（文档和查询）之间的知识。 Pooling Operation 最后，在每个句子的隐藏状态 $h_{1},…h_{l}$ 上应用池化函数。将序列转换为固定维度的表示。 $$h=MeanMax[h_1,…,h_l]$$ 所以得到的 q 和 d 的最终表示是 [1, hiden_size]? Prediction Layer and Optimization$$y_{out} = H_2(H_1([x_q; x_d ; x_q \\circ x_d ; x_q − x_d ]))$$ 其中 $H_1,H_2$ 是具有 ReLU 激活的 highway 网络层。然后将输出传递到最终线性 softmax 层。 $$y_{pred} = softmax(W_F · y_{out} + b_F )$$ 其中 $W_F\\in R^{h\\times 2}, b_F\\in R^2$. 使用 multi-class cross entropy，并带有 L2 正则化。","link":"/2018/11/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Multi-cast-Attention-Networks/"},{"title":"论文笔记-QANet","text":"paper: combining local convolution with local self-attention for reading comprehension Motivation Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. encoder 编码方式仅仅由 卷积 和 自注意力 机制构成，没了 rnn 速度就是快。 The key motivation behind the design of our model is the following: convolution captures the local structure of the text, while the self-attention learns the global interaction between each pair of words. 这篇论文最主要的创新点：使用 CNN 来捕捉文本结构的局部信息，使用 self-attention 来学习全局中每两个词之间的交互信息，使得其能耦合上下文信息。相比 RNN，attention 能够有效的解决长期依赖问题。只是相比少了词序信息。说到底，也是一种 contextualize 的 encoder 方式。 we propose a complementary data augmentation technique to enhance the training data. This technique paraphrases the examples by translating the original sentences from English to another language and then back to English, which not only enhances the number of training instances but also diversifies the phrasing. 使用了一种数据增强的方式，先将源语言转换成另一种语言，然后再翻译回英语。这样能有效增加训练样本，同时也丰富了短语的多样性。 Model 模型分为5部分: an embedding layer an embedding encoder layer a context-query attention layer a model encoder layer an output layer. the combination of convolutions and self-attention is novel, and is significantly better than self-attention alone and gives 2.7 F1 gain in our experiments. The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth (layer dropout) (Huang et al., 2016), which gives an additional gain of 0.2 F1 in our experiments. CNN 和 self-attention 的结合比单独的 self-attention 效果要好。同时使用了 CNN 之后能够使用常用的正则化方式 dropout, 这也能带来一点增益。 Input embedding layer obtain the embedding of each word w by concatenating its word embedding and character embedding. 由词向量和字符向量拼接而成。其中词向量采用预训练的词向量 Glove，并且不可训练，fixed. 只有 OOV (out of vocabulary) 是可训练的，用来映射所有不在词表内的词。 Each character is represented as a trainable vector of dimension p2 = 200, meaning each word can be viewed as the concatenation of the embedding vectors for each of its characters. The length of each word is either truncated or padded to 16. We take maximum value of each row of this matrix to get a fixed-size vector representation of each word. 字符向量的处理。每个字母是可训练的，对应的维度是 200 维。然后每个词都 truncated 或者 padded 成16个字母，保证每个词的向量维度是一样大小。 所以一个词的向量维度是 $300+200=500$. Embedding encoding layer The encoder layer is a stack of the following basic building block: [convolution-layer × # + self-attention-layer + feed-forward-layer] 其中： convolution: 使用 depthwise separable convolutions 而不是用传统的 convolution，因为作者发现 it is memory efficient and has better generalization. 怎么理解这个，还得看原 paper. The kernel size is 7, the number of filters is d = 128. self-attention: the multi-head attention mechanism 论文笔记, Attention Is All You Need Each of these basic operations (conv/self-attention/ffn) is placed inside a residual block, shown lower-right in Figure 1. For an input x and a given operation f, the output is f(layernorm(x))+x. 在 cnn/self-attention/ffn 层都有 layer normalization. 为什么要用 CNN：用来获取局部信息 k-gram features 相信看了这个图能对 QANet 中的 cnn 怎么实现的更清楚了。上图中每个卷积核的尺寸分别是 [2, embed_size], [3, embed_size], [3, embed_size]. padding参数 使用的是 “SAME”. 得到 3 个 [1, sequence_len]，然后拼接起来, 得到最终结果 [filters_num, sequence_len]. 在 QANet 的实现中，kernel_size 都设置为7, num_filters=128. 为什么要用 self-attention用来获取全局信息。 上图中的这种方式显然不太好，复杂度高且效果不好。于是有了 self-attention. 矩阵内部向量之间作內积，并通过 softmax 得到其他词对于 “The” 这个词的权重大小（权重比例与相似度成正比，这里看似不太合理 similarity == match??，但实际上效果很不错，可能跟词向量的训练有关）。 然后将对应的权重大小 $[w_1,w_2,w_3,w_4,w_5]$ 与对应的词相乘，累和得到蕴含了上下文信息的 contextualized “The”. 并且，这是可以并行化的。大大加速了训练速度。 Context-Query Attention Layer跟 BIDAF 是一样的。来，不看笔记把公式过一遍。 content: $C={c_1, c_2,…,c_n}$ query: $Q={q_1,q_2,…q_m}$. 所以 embeded 之后， content: [batch, content_n, embed_size] query: [batch, query_m, embed_size] 做矩阵相乘得到相似矩阵 similarity matrix $S\\in R^{n\\times m}$: sim_matrix: [batch, content_n, query_m] The similarity function used here is the trilinear function (Seo et al., 2016). $f(q,c)=W_0[q,c,q\\circ c]$. 相似矩阵的计算可以不是直接矩阵相乘，而是加个前馈神经网络。毕竟 similarity 不一定等于 match. content-to-query对 S 每一行 row 做 softmax 得到对应的概率，得到权重矩阵 $\\tilde S\\in R^{n\\times m}$, shape = [batch, content_n, query_m]. 然后与 query $Q^T$ [batch, query_m, embed_size] 矩阵相乘得到编码了 query 信息的 content: $A = \\tilde SQ^T$, shape = [batch, content_n, embed_size] query_to_content Empirically, we find that, the DCN attention can provide a little benefit over simply applying context-to-query attention, so we adopt this strategy. 这里没有采用 BiDAF 里面的方法，而是采用 DCN 中的方式，利用了 $\\tilde S$. 对 S 每一列 column 做 softmax 得到矩阵 $\\overline S$, shape = [batch, content_n, query_n]. 然后矩阵相乘得到 $B=\\tilde S \\overline S^T C^T$. $\\tilde S$.shape=[batch, content_n, query_m] $\\overline S^T$.shape=[batch, query_m, content_n] $C^T$.shape=[batch, query_m, embed_size] 所以最后 B.shape=[batch, content_n, embed_size] Model Encoder Layer同 BiDAF 一样输入是 $[c,a,c\\circ a,c\\circ b]$， 其中 a, b 分别是 attention matrix A，B 的行向量。不过不同的是，这里不同 bi-LSTM，而是类似于 encoder 模块的 [conv + self-attention + ffn]. 其中 conv 层数是 2, 总的 blocks 是7. Ouput layer$$p^1=softmax(W_1[M_0;M_1]), p^2=softmax(W_2[M_0;M_2])$$ 其中 $W_1, w_2$ 是可训练的参数矩阵，$M_0, M_1, M_2$ 如图所示。 然后计算交叉熵损失函数： $$L(\\theta)=-\\dfrac{1}{N}\\sum_i^N[log(p^1_{y^1})+log(p^2_{y^2})]$$ QANet 哪里好，好在哪儿？ separable conv 不仅参数量少，速度快，还效果好。将 sep 变成传统 cnn, F1 值减小 0.7. 去掉 CNN， F1值减小 2.7. 去掉 self-attention, F1值减小 1.3. layer normalization residual connections L2 regularization 参考文献 Dynamic Coattention Networks For Question Answering Xception: Deep Learning with Depthwise Separable Convolutions Attention Is All You Need qanet_talk_v1.pdf","link":"/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/"},{"title":"论文笔记-Using monoligual data in machine transaltion","text":"Monolingual Data in NMT Why Monolingual data enhancement Large scale source-side data: enhancing encoder network to obtain high quality context vector representation of source sentence. Large scale target-side data: boosting fluency for machine translation when decoding. The methods of using monolingual data Multi-task learningTarget-side language model: Integrating Language Model into the Decoder shallow fusion both an NMT model (on parallel corpora) as well as a recurrent neural network language model (RNNLM, on larger monolingual corpora) have been pre-trained separately before being integrated. Shallow fusion: rescore the probability of the candidate words. deep fusion multi-task learning Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning, EMNLP, 2017 利用 target-side 的单语多了一个训练语言模型的任务。事实上（b）就是上一张 PPT 中的方法，这篇paper在这个基础上增加了语言模型的 loss。 $\\sigma$ 参数在两个任务训练时都会更新。而 $\\theta$ 参数仅仅在训练翻译模型时才会更新参数。 auto-encoder 通过 自编码 的形式，重构对应的 mono-data，作为辅助任务，与 NMT 模型共享 encoder 参数。 Semi-Supervised Learning for Neural Machine Translation, ACL, 2016 Back-translationWhat is back-translation?Synthetic pseudo parallel data from target-side monolingual data using a reverse translation model. why back-translation and motivation?It mitigates the problem of overfitting and fluency by exploiting additional data in the target language. 目标语言必须始终是真实句子才能让翻译模型翻译的结果更流畅、更准确，而源语言即便有少量用词不当、语序不对、语法错误，只要不影响理解就无所谓。其实人做翻译的时候也是一样的：翻译质量取决于一个人译出语言的水平，而不是源语言的水平（源语言的水平只要足够看懂句子即可） Different aspects of the BT which influence the performance of translation: Size of the Synthetic Data Direction of Back-Translation Quality of the Synthetic Data Size of the Synthetic Data Direction of Back-Translation Quality of the Synthetic Data copy mechanism 作者的实验设置：用 target-side mono-data 来构建伪平行语料，一部分是直接 copy，另一部分是通过 back-translate 得到的。也就是 mono-data 出现了两次。 总觉得哪里不对。。。 Dummy source sentencePseudo parallel data: + target-side mono-data The downside: the network ‘unlearns’ its conditioning on the source context if the ratio of monolingual training instances is too high. Improving Neural Machine Translation Models with Monolingual Data, Sennrich et al, ACL 2016 Self-learningSynthetic target sentences from source-side mono-data: Build a baseline machine translation (MT) system on parallel data Translate source-side mono-data into target sentences Real parallel data + pseudo parallel data reference Improving Neural Machine Translation Models with Monolingual Data, Sennrich et al, ACL 2016 Using Monolingual Data in Neural Machine Translation: a Systematic Study, Burlot et al. ACL 2018 Copied Monolingual Data Improves Low-Resource Neural Machine Translation, Currey et al. 2017 In Proceedings of the Second Conference on Machine Translation Semi-Supervised Learning for Neural Machine Translation, Cheng et al. ACL 2016 Exploiting Source-side Monolingual Data in Neural Machine Translation, Zhang et al. EMNLP 2016 Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning, Domhan et al. EMNLP 2018 On Using Monolingual Corpora in Neural Machine Translation, Gulcehre, 2015 Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation, EMNLP 2018 Understanding Back-Translation at Scale, Edunov et al. EMNLP 2018","link":"/2019/03/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Using-monoligual-data-in-machine-transaltion/"},{"title":"论文笔记-batch,layer,weights normalization","text":"paper: Batch Normalization Layer Normalization weights Normalization Batch Normalization在之前的笔记已经详细看过了:深度学习-Batch Normalization Layer NormalizationMotivation batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. 关于 batch normalisztion. 从 Ng 的课上截来的一张图，全链接层相比卷积层更容易理解点，但形式上是一样的. 样本数量是 m，第 l 层经过激活函数输出是第 l+1 层的输入，其中第 i 个神经元的值: 线性输出： $z_i^l={w_i^l}^Th^l$. 非线性输出： $h_i^{l+1} = a_i^l=f(z_i^l+b_i^l)$ 其中 f 是非线性激活函数，$a_i^l$ 是下一层的 summed inputs. 如果 $a_i^l$ 的分布变化较大（change in a highly correlated way）,下一层的权重 $w^{l+1}$ 的梯度也会相应变化很大（反向传播中 $w^{l+1}$ 的梯度就是 $a_i^l$）。 Batch Normalization 就是将线性输出归一化。 其中 $u_i^l$ 是均值，$\\sigma_i^l$ 是方差。 $\\overline a_i^l$ 是归一化之后的输出。 $g_i^l$ 是需要学习的参数，也就是 scale. 有个疑问？为什么 BN 要在激活函数之前进行，而不是之后进行呢？ 上图中是单个样本，而所有的样本其实是共享层与层之间的参数的。样本与样本之间也存在差异，所以在某一个特征维度上进行归一化，（每一层其中的一个神经元可以看作一个特征维度）。 batch normalization requires running averages of the summed input statistics. In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps. BN 不是用于 RNN 是因为 batch 中的 sentence 长度不一致。我们可以把每一个时间步看作一个维度的特征提取，如果像 BN 一样在这个维度上进行归一化，显然在 RNN 上是行不通的。比如这个 batch 中最长的序列的最后一个时间步，他的均值就是它本身了，岂不是出现了 BN 在单个样本上训练的情况。 In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. 所以作者在这篇 paper 中提出了 Layer Normalization. 在单个样本上计算均值和方差进行归一化。然而是怎么进行的呢？ Layer Normalizationlayer normalization 并不是在样本上求平均值和方差，而是在 hidden units 上求平均值和方差。 其中 H 是 hidden units 的个数。 BN 和 LN 的差异： Layer normalisztion 在单个样本上取均值和方差，所以在训练和测试阶段都是一致的。 并且，尽管求均值和方差的方式不一样，但是在转换成 beta 和 gamma 的方式是一样的，都是在 channels 或者说 hidden_size 上进行的。 Layer normalized recurrent neural networks RNN is common among the NLP tasks to have different sentence lengths for different training cases. This is easy to deal with in an RNN because the same weights are used at every time-step. But when we apply batch normalization to an RNN in the obvious way, we need to to compute and store separate statistics for each time step in a sequence. This is problematic if a test sequence is longer than any of the training sequences. Layer normalization does not have such problem because its normalization terms depend only on the summed inputs to a layer at the current time-step. It also has only one set of gain and bias parameters shared over all time-steps. 这一部分也解释了 BN 不适用于 RNN 的原因，从 test sequence longer 的角度。RNN 的每个时间步计算共享参数权重. $a^t=W_{hh}h^{t-1}+W_{xh}x^t$ 其中 b 和 g 是可学习的参数。 layer normalize 在 LSTM 上的使用： tensorflow 实现batch Normalization12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061tf.reset_default_graph()from tensorflow.python.training.moving_averages import assign_moving_averagefrom tensorflow.contrib.layers import batch_norm### batch normalizationdef batch_norm(inputs, decay=0.9, is_training=True, epsilon=1e-6): &quot;&quot;&quot; :param inputs: [batch, length, width, channels] :param is_training: :param eplison: :return: &quot;&quot;&quot; pop_mean = tf.Variable(tf.zeros(inputs.shape[-1]), trainable=False, name=&quot;pop_mean&quot;) pop_var = tf.Variable(tf.ones(inputs.shape[-1]), trainable=False, name=&quot;pop_variance&quot;) def update_mean_and_var(): axes = list(range(inputs.shape.ndims)) batch_mean, batch_var = tf.nn.moments(inputs, axes=axes) moving_average_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1-decay)) # 也可用 assign_moving_average(pop_mean, batch_mean, decay) moving_average_var = tf.assign(pop_var, pop_var * decay + batch_var * (1-decay)) # 也可用 assign_moving_average(pop_var, batch_var, decay) with tf.control_dependencies([moving_average_mean, moving_average_var]): return tf.identity(batch_mean), tf.identity(batch_var) mean, variance = tf.cond(tf.equal(is_training, True), update_mean_and_var, lambda: (pop_mean, pop_var)) beta = tf.Variable(initial_value=tf.zeros(inputs.get_shape()[-1]), name=&quot;shift&quot;) gamma = tf.Variable(initial_value=tf.ones(inputs.get_shape()[-1]), name=&quot;scale&quot;) return tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon) layer normalization12345678910111213141516171819202122232425262728293031323334353637import tensorflow as tfbatch = 60hidden_size = 64whh = tf.random_normal(shape=[batch, hidden_size], mean=5.0, stddev=10.0)whh_norm = tf.contrib.layers.layer_norm(inputs=whh, center=True, scale=True)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(whh) print(whh_norm) print(sess.run([tf.reduce_mean(whh[0]), tf.reduce_mean(whh[1])])) print(sess.run([tf.reduce_mean(whh_norm[0]), tf.reduce_mean(whh_norm[5]), tf.reduce_mean(whh_norm[59])])) print(sess.run([tf.reduce_mean(whh_norm[:,0]), tf.reduce_mean(whh_norm[:,1]), tf.reduce_mean(whh_norm[:,63])])) print(&quot;\\n&quot;) for var in tf.trainable_variables(): print(var) print(sess.run(var)) 12345678910111213141516171819202122232425262728293031Tensor(&quot;random_normal:0&quot;, shape=(60, 64), dtype=float32)Tensor(&quot;LayerNorm/batchnorm/add_1:0&quot;, shape=(60, 64), dtype=float32)[5.3812757, 4.607581][-1.4901161e-08, -2.9802322e-08, -3.7252903e-09][-0.22264712, 0.14112064, -0.07268284]&lt;tf.Variable 'LayerNorm/beta:0' shape=(64,) dtype=float32_ref&gt;[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]&lt;tf.Variable 'LayerNorm/gamma:0' shape=(64,) dtype=float32_ref&gt;[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 发现一个很奇怪的问题， layer norm 是在每一个训练样本上求均值和方差，为啥 beta 和 gamma 的shape却是 [hidden_size]. 按理说不应该是 [batch,] 吗？ 带着疑问去看了源码，原来是这样的。。 将源码用简介的方式写出来了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 import tensorflow as tfdef layer_norm_mine(inputs, epsilon=1e-12, center=True, scale=True): &quot;&quot;&quot; inputs: [batch, sequence_len, hidden_size] or [batch, hidden_size] &quot;&quot;&quot; inputs_shape = inputs.shape inputs_rank = inputs_shape.ndims params_shape = inputs_shape[-1:] beta, gamma = None, None if center: beta = tf.get_variable( name=&quot;beta&quot;, shape=params_shape, initializer=tf.zeros_initializer(), trainable=True ) if scale: gamma = tf.get_variable( name=&quot;gamma&quot;, shape=params_shape, initializer=tf.ones_initializer(), trainable=True ) norm_axes = list(range(1, inputs_rank)) mean, variance = tf.nn.moments(inputs, norm_axes, keep_dims=True) # [batch] inv = tf.rsqrt(variance + epsilon) inv *= gamma return inputs*inv + ((beta-mean)*inv if beta is not None else - mean * inv)batch = 60hidden_size = 64whh = tf.random_normal(shape=[batch, hidden_size], mean=5.0, stddev=10.0)whh_norm = layer_norm_mine(whh) layer_norm_mine 得到的结果与源码一致。可以发现 计算均值和方差时， tf.nn.moments 中 axes=[1:-1]. （tf.nn.moments 中 axes 的含义是在这些维度上求均值和方差）. 也就是说得到的均值和方差确实是 [batch,]. 只是在转换成 beta 和 gamma 的分布时，依旧是在最后一个维度上进行的。有意思，所以最终的效果应该和 batch normalization 效果是一致的。只不过是否符合图像或文本的特性就另说了。 LayerNormBasicLSTMCell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245class LayerNormBasicLSTMCell(rnn_cell_impl.RNNCell): &quot;&quot;&quot;LSTM unit with layer normalization and recurrent dropout. This class adds layer normalization and recurrent dropout to a basic LSTM unit. Layer normalization implementation is based on: https://arxiv.org/abs/1607.06450. &quot;Layer Normalization&quot; Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton and is applied before the internal nonlinearities. Recurrent dropout is base on: https://arxiv.org/abs/1603.05118 &quot;Recurrent Dropout without Memory Loss&quot; Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth. &quot;&quot;&quot; def __init__(self, num_units, forget_bias=1.0, input_size=None, activation=math_ops.tanh, layer_norm=True, norm_gain=1.0, norm_shift=0.0, dropout_keep_prob=1.0, dropout_prob_seed=None, reuse=None): &quot;&quot;&quot;Initializes the basic LSTM cell. Args: num_units: int, The number of units in the LSTM cell. forget_bias: float, The bias added to forget gates (see above). input_size: Deprecated and unused. activation: Activation function of the inner states. layer_norm: If `True`, layer normalization will be applied. norm_gain: float, The layer normalization gain initial value. If `layer_norm` has been set to `False`, this argument will be ignored. norm_shift: float, The layer normalization shift initial value. If `layer_norm` has been set to `False`, this argument will be ignored. dropout_keep_prob: unit Tensor or float between 0 and 1 representing the recurrent dropout probability value. If float and 1.0, no dropout will be applied. dropout_prob_seed: (optional) integer, the randomness seed. reuse: (optional) Python boolean describing whether to reuse variables in an existing scope. If not `True`, and the existing scope already has the given variables, an error is raised. &quot;&quot;&quot; super(LayerNormBasicLSTMCell, self).__init__(_reuse=reuse) if input_size is not None: logging.warn(&quot;%s: The input_size parameter is deprecated.&quot;, self) self._num_units = num_units self._activation = activation self._forget_bias = forget_bias self._keep_prob = dropout_keep_prob self._seed = dropout_prob_seed self._layer_norm = layer_norm self._norm_gain = norm_gain self._norm_shift = norm_shift self._reuse = reuse @property def state_size(self): return rnn_cell_impl.LSTMStateTuple(self._num_units, self._num_units) @property def output_size(self): return self._num_units def _norm(self, inp, scope, dtype=dtypes.float32): shape = inp.get_shape()[-1:] gamma_init = init_ops.constant_initializer(self._norm_gain) beta_init = init_ops.constant_initializer(self._norm_shift) with vs.variable_scope(scope): # Initialize beta and gamma for use by layer_norm. vs.get_variable(&quot;gamma&quot;, shape=shape, initializer=gamma_init, dtype=dtype) vs.get_variable(&quot;beta&quot;, shape=shape, initializer=beta_init, dtype=dtype) normalized = layers.layer_norm(inp, reuse=True, scope=scope) return normalized def _linear(self, args): out_size = 4 * self._num_units proj_size = args.get_shape()[-1] dtype = args.dtype weights = vs.get_variable(&quot;kernel&quot;, [proj_size, out_size], dtype=dtype) out = math_ops.matmul(args, weights) if not self._layer_norm: bias = vs.get_variable(&quot;bias&quot;, [out_size], dtype=dtype) out = nn_ops.bias_add(out, bias) return out def call(self, inputs, state): &quot;&quot;&quot;LSTM cell with layer normalization and recurrent dropout.&quot;&quot;&quot; c, h = state args = array_ops.concat([inputs, h], 1) concat = self._linear(args) dtype = args.dtype i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1) if self._layer_norm: i = self._norm(i, &quot;input&quot;, dtype=dtype) j = self._norm(j, &quot;transform&quot;, dtype=dtype) f = self._norm(f, &quot;forget&quot;, dtype=dtype) o = self._norm(o, &quot;output&quot;, dtype=dtype) g = self._activation(j) if (not isinstance(self._keep_prob, float)) or self._keep_prob &lt; 1: g = nn_ops.dropout(g, self._keep_prob, seed=self._seed) new_c = ( c * math_ops.sigmoid(f + self._forget_bias) + math_ops.sigmoid(i) * g) if self._layer_norm: new_c = self._norm(new_c, &quot;state&quot;, dtype=dtype) new_h = self._activation(new_c) * math_ops.sigmoid(o) new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h) return new_h, new_state","link":"/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/"},{"title":"论文笔记 Pointer Networks and copy mechanism","text":"paper: Pointer Networks, NIPS, 2015 Incorporating Copying Mechanism in Sequence-to-Sequence Learning Pointer NetworkMotivation We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. 提出来了一种新的架构来学习得到这样的输出序列的条件概率，其中输出序列中的元素是输入序列中离散的 tokens. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. 这样简单的从输入序列中 copy 输出相关的序列在 seq2seq 或是神经图灵机都很难实现，因为在 decoder 的每一步输出的次的类别依赖于输入序列的长度，这个长度是变化的。 Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. 和这类问题类似的还有给不定长序列的排序，组合优化等问题。 It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. 同之前的 attention 不同的是，之前的 attention 是 decoder 时每一步计算通过 RNN 编码后的输入序列的隐藏变量与当前向量表示的 attention vector，然后生成当前词。而 Ptr-Net 则是使用 attention 作为指针，从输入序列中选择成员作为输出。 We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems – finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem – using training examples alone. Ptr-Net 可以用来学习类似的三个几何问题。 Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. Ptr-Net 不仅可以提升 seq2seq with attention,而且能够泛化到变化的 dictionayies. 从摘要以及 Introduction 来说， Ptr-Net 主要是解决两个方面的问题。 一是，简单的 copy 在传统的方法中很难实现，而 Ptr-Net 则是直接从输入序列中生成输出序列。 而是，可以解决输出 dictionary 是变化的情况。普通的 Seq2Seq 的 output dictionary 大小是固定的，对输出中包含有输入单词(尤其是 OOV 和 rare word) 的情况很不友好。一方面，训练中不常见的单词的 word embedding 质量也不高，很难在 decoder 时预测出来，另一方面，即使 word embedding 很好，对一些命名实体，像人名等，word embedding 都很相似，也很难准确的 reproduce 出输入提到的单词。Point Network 以及在此基础上后续的研究 CopyNet 中的 copy mechanism 就可以很好的处理这种问题，decoder 在各 time step 下，会学习怎样直接 copy 出现在输入中的关键字。 Model Architecture 在介绍 Ptr-Net 之前，作者先回顾了一下基本模型 seq2seq 和 input-attention. sequence-to-sequence Model实际上 seq2seq 解决的问题是在当前样本空间里面，给定输入下，使得输出序列的概率最大化。其实类似的 MT，QA，Summarization 都可以看作是这一类问题。只不过根据输入和输出之间的关系，调整相应的模型。 $$p(C^P|P;\\theta)=\\sum_{i=1}^m(P)p_{\\theta}(C_i|C_1,…,C_{i-1},P;\\theta)$$ 通过训练学习得到参数使得条件概率最大： $$\\theta^* = {argmax}{\\theta}\\sum{P,C^P}logp(C^P|P;\\theta)$$ 其中类和是在训练样本上。 In this sequence-to-sequence model, the output dictionary size for all symbols $C_i$ is fixed and equal to n, since the outputs are chosen from the input. Thus, we need to train a separate model for each n. This prevents us from learning solutions to problems that have an output dictionary with a size that depends on the input sequence length. 在 seq2seq 模型中，输出的 dictionary 是固定大小的。因为不能解决 dictionary 是变化的情况。 Content Based Input Attention 在每一个 decoder step，先计算 $e_{ij}$ 得到对齐概率(或者说 how well input position j matches output position i)，然后做一个 softmax 得到 $a_{ij}$，再对 $a_{ij}$ 做一个加权和作为 context vector $c_i$，得到这个 context vector 之后在固定大小的 output dictionary 上做 softmax 预测输出的下一个单词。 This model performs significantly better than the sequence-to-sequence model on the convex hull problem, but it is not applicable to problems where the output dictionary size depends on the input. Nevertheless, a very simple extension (or rather reduction) of the model allows us to do this easily. Ptr-Netseq2seq 模型的输出词是在固定的 dictionary 中进行 softmax，并选择概率最大的词，从而得到输出序列。但这里的输出 dictionary size 是取决于 input 序列的长度的。所以作者提出了新的模型，其实很简单。 $$u_j^i=v^Ttanh(W_1e_j+W_2d_i) ，j\\in(1,…,n)$$ $$p(C_i|C_1,…,C_{i-1},P)=softmax(u^i)$$ i 表示decoder 的时间步，j 表示输入序列中的index. 所以$e_j$ 是 encoder 编码后的隐藏向量，$d_i$ 是 decoder 当前时间步 i 的隐藏向量。跟一般的 attention 基本上一致。只不过得到的 softmax 概率应用在输入序列 $C_1,…,C_{i-1}$ 上。 Dataset Structure TensorFlow implementation of “Pointer Networks”：https://github.com/devsisters/pointer-network-tensorflow Dataset：https://drive.google.com/drive/folders/0B2fg8yPGn2TCMzBtS0o4Q2RJaEU CopyNetMotivation We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. 还是前面提到的问题，seq2seq 很难解决简单的 copy 问题。而在人类的对话中，出现 copy 的现象是很常见的。尤其是 命令实体 或者是长短语。 The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. 这也是 seq2seq 模型所需面对的挑战。 For example: 可以看到，对于 Chandralekha 这类实体词，可能是 OOV，也可能是其他实体或者是日期等很难被 decoder “还原” 出来的信息，CopyNet 可以更好的处理这类的信息。 那么问题来了： What to copy: 输入中的哪些部分应该被 copy? Where to paste: 应该把这部分信息 paste 到输出的哪个位置？ Model Architecture作者从两个角度来理解 CopyNet: From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity. 从认知学角度，copy机制近似于死记硬背，不需要太多的理解，但是要保证文字的保真度。 From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model. 从模型的角度，copy 操作更加死板和符号化，这也使得相比 soft attention 机制更难整合到一个完整的可微分的神经模型中去。 整体还是基于 encoder-decoder 模型。 Encoder: LSTM 将 source sequence 转换为隐藏状态 M(emory) $h_1,…,h_{T_S}$. Decoder: 同 cannonical 的 decoder 一样，使用 RNN 读取 encoder 的隐藏状态 M. 但和传统的 decoder 不一样，他有如下区别： Prediction: COPYNET predicts words based on a mixed probabilistic model of two modes, namely the generate-mode and the copymode, where the latter picks words from the source sequence. 下一个词的预测由两种模式混合而成。生成 generate-mode 和 copy-mode. 后者就像前面 Ptr-Net 所说的，在 source sentence 获取词。 State Update: the predicted word at time t−1 is used in updating the state at t, but COPYNET uses not only its word-embedding but also its corresponding location-specific hidden state in M (if any). 更新 decoder 中的隐藏状态时，t 时间步的隐藏状态不仅与 t-1 步生成词的 embedding vector 有关，还与这个词对应于 source sentence 中的隐藏状态的位置有关。 Reading M: in addition to the attentive read to M, COPYNET also has“selective read” to M, which leads to a powerful hybrid of content-based addressing and location-based addressing. 什么时候需要 copy，什么时候依赖理解来回答，怎么混合这两种模式很重要。 个人思考： 感觉不管要不要 copy 都应该是在基于理解的基础上进行的。但是因为 OOV 或者当前词的 embedding vector 训练的不好，那就无法理解了对吧？ 是否可以添加 gate 机制呢？ 机器到底还是没理解语言对吧？ 貌似是个可以创新的点。 接下来会详细讲解这三个不同之处怎么实现的。 Prediction with Copying and Generation:$s_t\\rightarrow y_t$这部分是从 decoder 隐藏状态 $s_t$ 到输出词 $y_t$ 的过程。传统的encoder-decoder 是一个线性映射就可以了。 词表 $\\mathcal{V}={v_1,…,v_N}$, 未登录词 OOV(out of vocabulary) 用 UNK 来表示（unk应该也会有对应的 embedding vector）. 以及用来表示输入序列中的 unique words $X={x_1,…,x_{T_S}}$. 其中 X 使得 copynet 输出 OOV. 对于三者有这样的集合关系（先不要看公式，后面会说到）： 简而言之(In a nutshell), 对于当前 source sentence X 输出的词表范围 $\\mathcal{V}\\cup \\text{UNK} \\cup X$. 给定 decoder 中当前时间步的隐藏状态 $s_t$, 以及 encoder 的隐藏状态序列 M. $$p(y_t|s_t,y_{t-1},c_t,M)=p(y_t,g|s_t,y_{t-1},c_t,M) + p(y_t,c|s_t,y_{t-1},c_t,M)$$ 其中 g 代表 generate mode. c 代表 copy mode. 我们知道对于 encoder 部分的输出 $h_1,…,h_{T_S}$， 记做 M，M 其实同时包含了语义和位置信息。那么 decoder 对 M 的读取有两种形式： Content-base Attentive read from word-embedding location-base Selective read from location-specific hidden units 两种模式对应的概率计算，以及 score function: $$p(y_t,g|\\cdot)=\\begin{cases} \\dfrac{1}{Z}e^{\\psi_g(y_t)}&amp;y_t\\in V\\ 0,&amp;y_t\\in X \\bigcap \\overline V\\ \\dfrac{1}{Z}e^{\\psi_g(UNK)},&amp;y_t\\notin V\\cup X \\end{cases}$$ $$p(y_t,c|\\cdot)=\\begin{cases}\\dfrac{1}{Z}\\sum_{j:x_j=y_t}{e^{\\psi_c(x_j)}},&amp;y_t\\in X\\0&amp;\\text {otherwise}\\end{cases}$$ 上面两个公式叠加(相加)可以表示为下图（可以将目标词看作类别为 4 的分类。）： 其中 $\\psi_g(\\cdot)$ 和 $\\psi_c(\\cdot)$ 是 generate mode 和 copy mode 的 score function. Z 是两种模型共享的归一化项，$Z=\\sum_{v\\in V\\cup{UNK}}e^{\\psi_g(v)}+\\sum_{x\\in X}e^{\\psi_c(x)}$. 然后对相应的类别计算对应的 score. Generate-Mode: $$\\psi_g(y_t=v_i)=\\nu_i^TW_os_t, v_i\\in V\\cup UNK$$ $W_o\\in R^{(N+1)\\times d_s}$ $\\nu_i$ 是 $v_i$ 对应的 one-hot 向量. 得到的结果是当前词的概率。 generate-mode 的 score $\\psi(y_t=v_i)$ 和普通的 encoder-decoder 是一样的。全链接之后的 softmax. copy-mode: $$\\psi(y_t=x_j)=\\sigma(h_j^TW_c)s_t,x_j\\in \\mathcal{V}$$ $h_j$ 是 encoder hidden state. j 表示输入序列中的位置。 $W_c\\in R^{d_h\\times d_s}$ 将 $h_j$ 映射到跟 $s_t$ 一样的语义空间。 作者发现使用 tanh 非线性变换效果更好。同时考虑到 $y_t$ 这个词可能在输入中出现多次，所以需要考虑输入序列中所有的为 $y_t$ 的词的概率的类和。 state update上面一部分讲的是怎么从 decoder 中的隐藏状态计算对应的 vocabulary，也就是 $s_t\\rightarrow y_t$. 那么怎么计算当前时间步的隐藏状态呢？ 我们知道传统的 encoder-decoder 中隐藏状态就是 content-based atention vector. 但是在 copynet 里面，作者对 $y_{t-1}\\rightarrow s_t$ 这个计算方式做了一定的修改。 先回顾下基本的 attention 模块，decoder 中隐藏状态的更新 $s_t=f(y_{t-1},s_{t-1},c_t)$, 其中 $c_t$ 也就是 attention 机制： $$c_t=\\sum_{\\tau=1}^{T_S}\\alpha_{t\\tau}$$ $$\\alpha_{t\\tau}=\\dfrac{e^{\\eta(s_{t-1},h_{\\tau})}}{\\sum_{\\tau’}e^{\\eta(s_{t-1},h_{\\tau’})}}$$ CopyNet 的 $y_{t-1}$ 在这里有所不同。不仅仅考虑了词向量，还使用了 M 矩阵中特定位置的 hidden state，或者说，$y_{t−1}$ 的表示中就包含了这两个部分的信息 $[e(y_{t−1});\\zeta(y_{t−1})]$，$e(y_{t−1})$ 是词向量，后面多出来的一项 $\\zeta(y_{t−1})$ 叫做 selective read, 是为了连续拷贝较长的短语。和attention 的形式差不多，是 M 矩阵中 hidden state 的加权和. $$\\zeta(y_{t-1})=\\sum_{\\tau=1}^{T_S}\\rho_{t\\tau}h_{\\tau}$$ $$\\rho_{t\\tau}=\\begin{cases}\\dfrac{1}{K}p(x_{\\tau},c|s_{t-1},M),&amp; x_{\\tau}=y_{t-1}\\ 0,&amp; \\text{otherwise} \\end{cases}$$ 当 $y_{t-1}$ 没有出现在 source sentence中时， $\\zeta(y_{t-1})=0$. 这里的 $K=\\sum{\\tau’:x_{\\tau’}=y_{t-1}}p(x_{\\tau’},c|s_{t-1},M)$ 是类和。还是因为输入序列中可能出现多个当前词，但是每个词在 encoder hidden state 的向量表示是不一样的，因为他们的权重也是不一样的。 这里的 p 没有给出解释，我猜跟前面计算 copy 的 score 是一致的？ 直观上来看，当 $\\zeta(y_{t-1})$ 可以看作是选择性读取 M (selective read). 先计算输入序列中对应所有 $y_{t-1}$ 的权重，然后加权求和，也就是 $\\zeta(y_{t-1})$. Hybrid Adressing of M包括两种 Addressing 方式： content-based and location-based assressing. location-based Addressing: $$\\zeta(y_{t-1}) \\longrightarrow{update} \\ s_t \\longrightarrow predict \\ y_t \\longrightarrow sel. read \\zeta(y_t)$$ Learning最小化概率的负对数： $$L=-\\dfrac{1}{N}\\sum_{k=1}^N\\sum_{t=1}^Tlog[p(y_t^{(k)}|y_{&lt;t}^{(k)}, X^{(k)})]$$ N 是batch size，T 是 object sentence 长度。","link":"/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/"},{"title":"论文笔记-QA BiDAF","text":"paper: BiDAF:Bidirectional Attention Flow for Machine Comprehension Match-LSTM:Machine Comprehension Using Match-LSTM and Answer Pointer Motivation Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. 机器阅读的定义，query 和 context 之间的交互。 Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. 传统的使用 attention 机制的方法。 In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. 本文提出的方法 BiDAF. 使用多阶层次双向 attention flow 机制来表示内容的不同 levels 的粒度，从而获得 query-aware 的 context，而不使用 summarization. Introduction Attention mechanisms in previous works typically have one or more of the following characteristics. First, the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed-size vector. Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step. Third, they are usually uni-directional, wherein the query attends on the context paragraph or the image. 对 atention 在以前的研究中的特性做了一个总结。 1.attention 的权重用来从 context 中提取最相关的信息，其中 context 压缩到一个固定 size 的向量。 2.在文本领域，context 中的表示在时间上是动态的。所以当前时间步的 attention 权重依赖于之前时间步的向量。 3.它们通常是单向的，用 query 查询内容段落或图像。 Model ArchitectureBiDAF 相比传统的将 attention 应用于 MC 任务作出如下改进: First, our attention layer is not used to summarize the context paragraph into a fixed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer. This reduces the information loss caused by early summarization. 1）并没有把 context 编码到固定大小的向量表示中，而是让每个时间步计算得到的 attended vactor 可以流动（在 modeling layer 通过 biLSTM 实现）这样可以减少早期加权和造成的信息丢失。 Second, we use a memory-less attention mechanism. That is, while we iteratively compute attention through time as in Bahdanau et al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step. 2）memory-less，在每一个时刻，仅仅对 query 和当前时刻的 context paragraph 进行计算，并不直接依赖上一时刻的 attention. We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer. It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation (the output of the attention layer). It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps. 也就是对 attention layer 和 modeling layer 进行分工，前者关注于 context 和 query 之间的交互。而后者则关注于 query-aware context 中词于词之间的交互，也就是加权了 attention weights 之后的 context 表示。这使得 attention 在每个时间步不受之前错误的影响。 Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other. 计算了 query-to-context（Q2C） 和 context-to-query（C2Q）两个方向的 attention 信息，认为 C2Q 和 Q2C 实际上能够相互补充。实验发现模型在开发集上去掉 C2Q 与 去掉 Q2C 相比，分别下降了 12 和 10 个百分点，显然 C2Q 这个方向上的 attention 更为重要 论文提出6层结构： Character Embedding Layer and Word Embedding Layer -&gt; Contextual Embedding Layer -&gt; Attention Flow Layer -&gt; Modeling Layer -&gt; Output Layer Character Embedding Layer and word embedding alyer charatter embedding of each word using CNN.The outputs of the CNN are max-pooled over the entire width to obtain a fixed-size vector for each word. pre-trained word vectors, GloVe concatenation of them above and is passed to a two-layer highway networks. context -&gt; $X\\in R^{d\\times T}$ query -&gt; $Q\\in R^{d\\times J}$ contextual embedding layermodel the temporal interactions between words using biLSTM. context -&gt; $H\\in R^{2d\\times T}$ query -&gt; $U\\in R^{2d\\times J}$ 前三层网络是在不同的粒度层面来提取 context 和 query 的特征。 attention flow layer the attention flow layer is not used to summarize the query and context into single feature vectors. Instead, the attention vector at each time step, along with the embeddings from previous layers, are allowed to flow through to the subsequent modeling layer. 输入是 H 和 G，输出是 query-aware vector G, 以及上一层的 contextual layer. 这一层包含两个 attention，Context-to-query Attention 和 Query-to-context Attention. 它们共享相似矩阵 $S\\in R^{T\\times J}$(不是简单的矩阵相乘，而是类似于 Dynamic Memory Networks 中的计算方式). $$S_{tj}=\\alpha(H_{:t},U_{:j})\\in R$$ 其中 $\\alpha(h,u)=w_{(S)}^T[h,u,h\\circ u]$, $w_{(S)}\\in R^{6d}$ Context-to-query Attention: 计算对每一个 context word 而言哪些 query words 和它最相关。所以 计算 t-th context word 对应的 query 每个词的权重: $$a_t=softmax(S_{t:})\\in R^J$$ 然后将权重赋予到 query 上然后再加权求和(叠加赋予了权重的 query 中的每一个词)，得到 t-th 对应的 query-aware query: $$\\tilde U_{:t}=\\sum_j a_{tj}U_{:j}\\in R^{2d}$$ 然后 context 中的每一个词都这样计算，$\\tilde U\\in R^{2d\\times T}$ 就是通过 context 和 query 计算相似性后，通过 sortmax 转化为概率，然后作为权重赋予到 query 上，得到 context 每一个词对应的 attended-query. Query-to-context Attention: 跟 C2Q 一样计算相似矩阵 S 后，计算对每一个 query word 而言哪些 context words 和它最相关，这些 context words 对回答问题很重要。 先计算相关性矩阵每一列中的最大值，max function $max_{col}(S)\\in R^T$, 然后softmax计算概率: $$b=softmax(max_{col}(S))\\in R^T$$ 权重 b 表示与整个 query 比较之后，context 中每一个词的重要程度，然后与 context 加权和： $$\\tilde h = \\sum_tb_tH_{:t}\\in R^{2d}$$ 在 tile T 次后得到 $\\tilde H\\in R^{2d\\times T}$. 比较 C2Q 和 Q2C，显然 Q2C 更重要，因为最终我们要找的答案是 context 中的内容。而且两者的 attention 计算方式有区别是：对 query 进行加权和时，我们考虑的是 context 中的每一个词，而在对 context 进行加权和时，我们要考虑所有的 query 中相关性最大的词，是因为 context 中某个词只要与 query 中任何一个词有关，都需要被 attend. 将三个矩阵拼接起来，得到 G: $$G_{:t}=\\beta (H_{:t},\\tilde U_{:t}, \\tilde H_{:t})\\in R^{d_G}$$ function $\\beta$ 可以是 multi-layers perceptron. 在作者的实验中： $$\\beta(h,\\tilde u,\\tilde h)=[h;\\tilde u;h\\circ \\tilde u;h\\circ \\tilde h]\\in R^{8d\\times T}$$ Modeling Layercaptures the interaction among the context words conditioned on the query. 使用 biLSTM, 单向 LSTM 的输出维度是d，所以最终输出： $M\\in R^{2d\\times T}$. Output Layer输出 layer 是基于应用确定的。如果是 QA，就从段落中找出 start p1 和 end p2. 计算 start index: $$p^1=softmax(W^T(p^1)[G;M])$$ 其中 $w_{(p^1)}\\in R^{10d}$ 计算 end index，将 M 通过另一个 biLSTM 处理，得到 $M^2\\in R^{2d\\times T}$ $$p^2=softmax(W^T(p^2)[G;M^2])$$ Training目标损失函数： $$L(\\theta)=-{1 \\over N} \\sum^N_i[log(p^1_{y_i^1})+log(p^2_{y_i^2})]$$ $\\theta$ 包括参数： the weights of CNN filters and LSTM cells $w_{S}$,$w_{p^1},w_{p^2}$ $y_i^1,y_i^2$ 表示i样本中开始可结束位置在 context 中的 index. $p^1,p^2\\in R^T$ 是经过 softmax 得到的概率，可以将 gold truth 看作是 one-hot 向量 [0,0,…,1,0,0,0]，所以对单个样本交叉熵是: $$- log(p^1_{y_i^1})-log(p^2_{y_i^2})$$ TestThe answer span $(k; l)$ where $k \\le l$ with the maximum value of $p^1_kp^2_l$ is chosen, which can be computed in linear time with dynamic programming.","link":"/2018/08/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QA%20BiDAF/"},{"title":"论文笔记-image-based contrastive learning","text":"simCLR MoCo BYOL Swin-ssl BraVe What Makes for Good Views for Contrastive Learning? BYOL works even without batch statistics Understanding Self-Supervised Learning Dynamics without Contrastive Pairs Big Self-Supervised Models are Strong Semi-Supervised Learners Understanding contrastive representation learning through alignment and uniformity on the hypersphere. simCLRA Simple Framework for Contrastive Learning of Visual Representations 作者提出了一个简单的对比学习框架，不需要特殊的网络结构和memory bank. Introduction现有的无监督视觉表示学习的方法主要分为两类：生成式和判别式。 生成式主要包括以下三类： deep belief nets Auto-encoding Generative adversarial nets pixel-level 生成式算法非常消耗计算资源，因而对于有效的表示学习并不是必须的。 判别式方法的目标函数更接近监督学习，不过其对应的监督任务是从没有标签的数据集中自行构造的，因而学到的视觉表示能力受限于预定义的任务，而泛化能力有限。现有的达到sota的几篇paper[4][5][6] 作者提出了一个简单的对比学习方法，不仅达到了sota，而且不需要复杂的网络结构[6][7]，也不需要memory bank[8][9][10][11]. 为了系统的理解怎样才能获得有效的的对比学习，作者研究了以下几个重要组成部分： data augmentation：相比有监督学习，对比学习更需要数据增强 $t\\sim T$ nonlinear projection：如图所示，在视觉表示和contrast loss之间增加一个非线性projection $g(\\cdot)$ 很有必要 normalized embeddings and an appropriately adjusted temperature parameter: 归一化的embedding和可调整的temperature parameter. larger batch size and more training steps Method如上图所示，simCLR 主要包括四部分： A stochastic data augmentation module: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur A neural network base encoder $f(\\cdot)$，作者采用的是 ResNet. $h_i = f(\\tilde x_i) = ResNet(\\tilde x_i)$ A small neural network projection head $g(\\cdot)$, $z_i = g(h_i) = W^{(2)}σ(W^{(1)}h_i)$. 作者发现在 $z_i$ 上计算 contrast loss，比 $h_i$ 效果更好。 A contrastive loss function：NT-Xent (the normalized temperature-scaled cross entropy loss. 其中 $sim(u,v)=\\dfrac{u^Tv}{\\lVert u \\rVert \\lVert v\\rVert}$. Training with Large Batch Size作者采用了更大的batch size(256 $\\rightarrow$ 8192)，因而不需要memory bank. 这样一个batch有 ($8192\\times 2=16382$) 个负样本。 在超大的batch size情况下，使用SGD/Momentum学习率不稳定，因此作者使用LARS optimizer. 作者使用 32-128 cores TPU进行训练。（这真的劝退。。。 Global BN在分布式训练的场景下，BN的均值和方差是在单个device上计算的。而两个正样本是在同一个device上计算的，因此在拉进两个正样本之间的agreement时，BN会造成信息泄露。为了解决这个问题，作者采用的方法是在所有的device上计算BN的均值和方差。类似地解决这一问题的方法还有：shuffling data examples across devices[^10], replacing BN with layer norm[^7]. 这点其实不太理解，为啥BN会造成信息泄露？ Evaluation ProtocolDataset and Metrics.作者先在CIFAR-10上进行试验，得到了94.0%的准确率（有监督的准确率是95.1%）. 为了验证学习得到的视觉表示，作者采用广泛使用的linear evaluation protocol[^5][^6]: a linear classifier is trained on top of the frozen base network, and test accuracy is used as a proxy for representation quality. 除了linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning. Default settingWe use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space. As the loss, we use NT-Xent, optimized using LARS with learning rate of 4.8 (= 0.3 × BatchSize/256) and weight decay of 10−6. We train at batch size 4096 for 100 epochs. Data Augmentation for Contrastive Representation LearningData augmentation defines predictive tasks数据增强定义预预测任务。 随机裁剪既包括了 global and local views, 也包括了 adjacent views. Composition of data augmentation operations is crucial for learning good representations 为了验证不同的数据增强对于表示学习的影响，作者进行了ablation实验，只对图2中的某一分支进行transformation. 实验结果如图5所示，对角线只有一种augmentation方法，非对角线是两种组合。 结果表明，单一的增强方法都不能学到好的表示。两种组合时，预测任务越难，学习到的表示能力越好。最好的组合是 random crop 和 color distortion. 但是只是单独用其中某一种效果都不好。 作者对只用单独一种数据增强方法不好的原因进行了解释： We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Figure 6 shows that color histograms alone suffice to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features. Contrastive learning needs stronger data augmentation than supervised learning相比监督学习，stronger数据增强对contrastive learning更为重要。 When training supervised models with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning. Architectures for Encoder and HeadUnsupervised contrastive learning benefits (more) from bigger models对比学习在大模型下获益更多。 A nonlinear projection head improves the representation quality of the layer before it作者探究了projection的三种方式： identity mapping linear projection non-linear projection 结果表明，非线性projection更好，没有的话效果很差。 除此之外，使用project head之前的hidden layer $h(i)$ 比project layer之后的表示 $z(i)$ 效果更好, $\\ge 10%$。 为什么使用 non-linear projection head 之前的hidden layer效果更好？ 作者认为对比loss会损失信息。z = g(h) 被训练成transformation invariant 变换不变性（因为contrast loss要拉近两个不同变换的正样本）。因此，$g(\\cdot)$ 会丢失信息。 作者通过实验验证这一猜想，在保证最终的dimension不变的情况下， Loss Functions and Batch SizeNormalized cross entropy loss with adjustable temperature works better than alternatives 实验表明 NT-Xent 效果最好。这是因为其他的目标函数并没有衡量负样本的难度：unlike cross-entropy, other objective functions do not weight the negatives by their relative hardness. $l_2$ normalization 和 temperture 很重要：$l_2$ normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives; 没有 $l_2$ normalization,尽管对比准确率很高，但是学习到的表示能力并不好。 合适的temperture也很重要 Contrastive learning benefits (more) from larger batch sizes and longer training 实验表明，batch size很重要，越大收敛的越快，但最终效果也不是越大越好。随着训练的增加，batch size造成的表现差异也随着逐渐消失。 Comparison with State-of-the-art作者采用了三种方法来验证performance。 Linear evaluation相比fine-tune，linear evaluation 的区别在于学习率的设置。 没搞懂为啥叫 linear evaluation？ 和fine-tune的区别就在于学习率的设置？ Semi-supervised learningsample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (∼12.8 and ∼128 images per class respectively). Transfer learning在imageNet上训练，在其他数据集上测试。同上，采用了两种方式， Linear evaluation 和 fine-tune. ### speech-basedRepresentation Learning with Contrastive Predictive Codingwav2vec: Unsupervised pre-training for speech recognition encoder: $X \\rightarrow Z$ content-network: $C\\rightarrow C$ noise contrastive binary classification task. $\\sigma$ 是 sigmoid 函数 $c_i$ 是当前step的content feature $h_k(x_i)$ 是step-specific affine transformation $h_k(c_i) = W_kc_i+b_k$ $z_{i+k}$ 是距离当前step为k的encoded feature, 为正样本 $\\hat z\\sim p_n$是 (T-k) 帧中均匀选择10个负样本，得到对数概率的期望后，再乘以 $\\lambda=10$. –&gt; MoCo BYOLpaper: Bootstrap Your Own Latent A New Approach to Self-Supervised Learning [1] : A fast learning algorithm for deep belief nets.[2]: Auto-encoding variational bayes.[3]: Generative adversarial nets. NIPS2014[4]: Discriminative unsupervised feature learning with convolutional neural networks. NIPS2014[5]: Representation learning with contrastive predictive coding. arXiv2018[6]: Learning representations by maximizing mutual information across views. NIPS2019[7]: CPC: Data-efficient image recognition with contrastive predictive coding, arXiv2019[8]: Unsupervised feature learning via non-parametric instance discrimination. CVPR2018[9]: Contrastive multiview coding. arXiv2019[10]: MoCo: Momentum contrast for unsupervised visual representation learning, arXiv2019[11]: Self-supervised learning of pretext-invariant representations. arXiv2019","link":"/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/"},{"title":"论文笔记-Contextual Augmentation","text":"paper 1Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations 在 NLP 领域，数据增强比图像领域要复杂的多。它很难有一个统一的规则的去适用于各种 domain. 目前常用的方法是 基于 WordNet 的同义词替换，以及根据距离计算的词的相似度。但是，同义词是很少的，并且，一个词本身也是多义的，它在 WordNet 中的同义词也许并不适合当前的语境，而这一限制也是很难通过一个规则去限定。所以，传统的方法都很难发挥。 传统的数据增强方法： 基于规则，聚类，人工干预。 Wang and Yang(2015) 基于近义词 Character-level convolutional networks for text classification 作者提出了一种新的方法，基于语言模型的 contextual augmentation. 根据上下文预测得到的不同的词具有范式关系 paradigmatic relations. 更进一步的，为了让生成的词与当前句子的 label 是兼容的 (compatible)，作者加入了 label-conditional. 根据上文预测 target position i $w_i$, $P(\\cdot|S/{w_i})$. 加上 label 限制条件之后就是 $P(\\cdot|y, S/{w_i})$ 作者并不是直接使用 top-k. 而是利用的退火的方法，temperature parameter $\\tau$, 这样预测分布就是 $$P(\\cdot|y, S/{w_i})^{1/\\tau}$$ 当 $\\tau \\rightarrow \\infty$ 时，那么对应的预测词分布是 uniform distribution. 当 $\\tau \\rightarrow 0$ 时，预测得到的就是概率最大的词。我猜作者这样做的目的是让生成的词更丰富，避免单一化。 在不同任务上的对比实验，与 synonym 同义词替换进行的对比，分类模型都用的 CNN. 效果还可以，但是并不明显。而 w/synonym 反而会有反作用。。 其中某个样本的效果展示： paper 2Conditional BERT Contextual Augmentation 看完paper真的想吐槽下中国人写的论文真的给人一种粗制滥造的感觉。。尽管paper出来的很及时，与 Bert 结合也很赞，但是 。。。其实还可以好好写，还可以多分析分析，看 paper 都能感觉到一种为了发论文而发论文的感觉，我自己又何尝不是呢。。。 Bert 想对于 LSTM 更获得更深层的含义。 将 segmentation embedding 换成 label embedding. 然后使用预训练的 BERT 模型进行 fine-tune. 迭代直到收敛，生成新的句子后，在进行下游任务。 When the task-specific dataset is with more than two different labels,we should re-train a label size compatible label embeddings layer instead of directly fine-tuning the pre-trained one. 对于多标签任务，作者是这么说的，不太明白。","link":"/2018/12/27/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-data-augmentation/"},{"title":"论文笔记-dropblock","text":"paper: DropBlock: DropBlock: A regularization method for convolutional networks Variational Dropout：A Theoretically Grounded Application of Dropout in Recurrent Neural Networks Zoneout：Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations dropblock 是关于 CNN 的，后两篇是关于 RNN 的正则化。 DropBlockMotivation Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. 通常深度神经网络在过参数化，并在训练时加上大量的噪声和正则化，比如权重衰减和 dropout，这个时候神经网络能很好的 work. 但是 dropout 对于全链接网络是一个非常有效的正则化技术，它对于卷积神经网络却没啥效果。这可能是因为卷积神经网络的激活是空间相关的，即使 drop 掉部分 unit，信息仍然会传递到下一层网络中去。 Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. 作者为卷积神经网络提出了专门的正则化方式， dropblock. 同时 drop 掉一个连续的空间。作者发现将 dropblock 应用到 ResNet 能有效的提高准确率。同时增加 drop 的概率能提高参数的鲁棒性。 回顾了一下 skip/shortcut connection: 目的是避免梯度消失。可以直接看 GRU 的公式：参考笔记 dropblock In this paper, we introduce DropBlock, a structured form of dropout, that is particularly effective to regularize convolutional networks. In DropBlock, features in a block, i.e., a contiguous region of a feature map, are dropped together. As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data (see Figure 1). 具体的算法很简单，主要关注两个参数的设置： block_size 和 $\\gamma$. block_size is the size of the block to be dropped $\\gamma$ controls how many activation units to drop. We experimented with a shared DropBlock mask across different feature channels or each feature channel has its DropBlock mask. Algorithm 1 corresponds to the latter, which tends to work better in our experiments. 对于 channels， 不同的 feature map 具有不同的 dropblock 相比所有的 channels 共享 dropblock 效果要好。 Similar to dropout we do not apply DropBlock during inference. This is interpreted as evaluating an averaged prediction across the exponentially-sized ensemble of sub-networks. These sub-networks include a special subset of sub-networks covered by dropout where each network does not see contiguous parts of feature maps. 关于 infer 时， dropblock 的处理和 dropout 类似。 block_size: In our implementation, we set a constant block_size for all feature maps, regardless the resolution of feature map. DropBlock resembles dropout [1] when block_size = 1 and resembles SpatialDropout [20] when block_size covers the full feature map. block_size 设置为 1 时, 类似于 dropout. 当 block_size 设置为整个 feature map 的 size 大小时，就类似于 SpatialDropout. setting the value of $\\gamma$: In practice, we do not explicitly set $\\gamma$. As stated earlier, $\\gamma$ controls the number of features to drop. Suppose that we want to keep every activation unit with the probability of keep_prob, in dropout [1] the binary mask will be sampled with the Bernoulli distribution with mean 1 − keep_prob. However, to account for the fact that every zero entry in the mask will be expanded by block_size2 and the blocks will be fully contained in feature map, we need to adjust $\\gamma$ accordingly when we sample the initial binary mask. In our implementation, $\\gamma$ can be computed as 作者并没有显示的设置 $\\gamma$. 对于 dropout，每一个 unit 满足概率为 keep_prob 的 Bernoulli 分布，但是对于 dropblock, 需要考虑到 block_size 的大小，以及其与 feature map size 的比例大小。 keep_prob 是传统的 dropout 的概率，通常设置为 0.75-0.9. feat_size 是整个 feature map 的 size 大小。 (feat_size - block_size + 1) 是选择 dropblock 中心位置的有效区域。 The main nuance of DropBlock is that there will be some overlapped in the dropped blocks, so the above equation is only an approximation. 最主要的问题是，会出现 block_size 的重叠。所以上诉公式也只是个近似。 Scheduled DropBlock: We found that DropBlock with a fixed keep_prob during training does not work well. Applying small value of keep_prob hurts learning at the beginning. Instead, gradually decreasing keep_prob over time from 1 to the target value is more robust and adds improvement for the most values of keep_prob. 定制化的设置 keep_prob, 在网络初期丢失特征会降低 preformance, 所以刚开始设置为 1,然后逐渐减小到 target value. 所以是随着网络深度加深而变化，还是随着迭代步数变化，应该是后者吧，类似于 scheduled learning rate. Experiments In the following experiments, we study where to apply DropBlock in residual networks. We experimented with applying DropBlock only after convolution layers or applying DropBlock after both convolution layers and skip connections. To study the performance of DropBlock applying to different feature groups, we experimented with applying DropBlock to Group 4 or to both Groups 3 and 4. 实验主要在讨论在哪儿加 dropblock 以及 如何在 channels 中加 dropblock。 Variational Dropout","link":"/2018/12/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dropblock/"},{"title":"论文笔记-fast transformer","text":"Long range arena: A benchmark for efficient transformers Convolution and Transformer Swin transformer: Hierarchical vision transformer using shifted windows. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding Incorporating convolution designs into visual transformers On the relationship between self-attention and convolutional layers local windows Image transformer Blockwise self\u0002-attention for long document understanding Axial pattern attention in multidimensional transformers. Adaptive span Adaptive attention span in transformers. approximation Linformer: Self-attention with linear complexity Rethinking attention with performers. Linear Transformer: Transformers are RNNs: Fast autoregressive transformers with linear attention. Efficient attention: Attention with linear complexities Nyströmformer: A nyström-based algorithm for approximating self-attention. Fnet: Mixing tokens with fourier transforms. XCiT: Cross-Covariance Image Transformers Scatterbrain: Unifying Sparse and Low-rank Attention Approximation Transformer dissection: An unified understanding for transformer’s attention via the lens of kernel.","link":"/2021/11/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-fast-transformer/"},{"title":"论文笔记-Match LSTM","text":"Motivation SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. 针对 SQuAD 这样的阅读理解式任务提出的端到端的模型。 SQuAD 的答案不是从候选词中提取，而是类似于人类的回答，是不同长度的句子。 The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. 主要是基于 Pointer Networks 关于阅读理解的数据集 benchmark dataset： MCTest: A challenge dataset for the open-domain machine comprehension of text. Teaching machines to read and comprehend. The Goldilocks principle: Reading children’s books with explicit memory representations. Towards AI-complete question answering: A set of prerequisite toy tasks. SQuAD: 100,000+ questions for machine comprehension of text. SQuAD Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering, including syntactic parsing, named entity recognition, question classification, semantic parsing, etc. Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension. 传统的智能问答任务整个流程包括 句法分析、命名实体识别、问题分类、语义分析等。。随着深度学习的发展，端到端的模型开始出现。 End-to-end model architecture: Teaching machines to read and comprehend. The Goldilocks principle: Reading children’s books with explicit memory representations. Attention-based convolutional neural network for machine comprehension Text understanding with the attention sum reader network. Consensus attention-based neural networks for chinese reading comprehension. However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset. 之前的模型的 answer 要么是从候选答案中选择，要么是一个简单的符号。这都不适合 SQuDA. 模型是基于作者早期提出的用于 textual entailment 的 match-LSTMLearning natural language inference with LSTM，然后进一步应用了 Pointer Net(https://papers.nips.cc/paper/5866-pointer-networks), 从而允许预测的结果能够从输入中获得，而不是从一个固定的词表中获取。 We propose two ways to apply the Ptr-Net model for our task: a sequence model and a boundary model. We also further extend the boundary model with a search mechanism. 作者提出的两种模型。 Model ArchitectureMatch-LSTMPointer NetworkPointer Network (Ptr-Net) model : to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence. Instead of picking an output token from a fixed vocabulary, Ptr-Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol. 从输入 sentences 中生成 answer. 类似于 Pointer Network 的模型： Incorporating copying mechanism in sequence-to-sequence learning. Text understanding with the attention sum reader network. MATCH-LSTM AND ANSWER POINTER 模型主要分为3部分： An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs. 使用 LSTM 处理 question 和 passage. A match-LSTM layer that tries to match the passage against the question. 使用 match-LSTM 对lstm编码后的 question 和 passage 进行匹配。 An Answer Pointer (Ans-Ptr) layer that uses Ptr-Net to select a set of tokens from the passage as the answer. The difference between the two models only lies in the third layer. 使用 Pointer 来选择 tokens. LSTM preprocessing Layer$$H^p=\\overrightarrow {LSTM}(P), H^q=\\overrightarrow {LSTM}(Q)$$ 直接使用单向LSTM，每一个时刻的隐含层向量输出 $H^p\\in R^{l\\times P}, H^q\\in R^{l\\times Q}$ 只包含左侧上下文信息. Match-LSTM Layer$$\\overrightarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\\overrightarrow {h^r}_{i-1}+b^p)\\otimes e_Q)\\in R^{l\\times Q}$$ $$\\overrightarrow \\alpha_i=softmax(w^T\\overrightarrow G_i + b\\otimes e_Q)\\in R^{1\\times Q}$$ the resulting attention weight $\\overrightarrow α_{i,j}$ above indicates the degree of matching between the $i^{th}$ token in the passage with the $j^{th}$ token in the question. 其中 $W^q,W^p,W^r \\in R^{l\\times l}, b^p,w\\in R^l, b\\in R$ 所以 $\\overrightarrow α_{i}$ 表示整个 question 与 passage 中的第 i 个词之间的 match 程度，也就是通常理解的 attention 程度。 传统的 attention 就是将 passage 和 question 矩阵相乘，比如 transformer 中 query 和 keys 相乘。复杂一点可能就是 dynamic memory networks 中的将 两个需要 match 的向量相减、element-wise相乘之后，使用两层的前馈神经网络来表示。 这里的 attention score 的计算方式又不一样了。 $\\overrightarrow{h^r_{i-1}}$ 是通过 LSTM 耦合 weighted queston 和 passage 中上一个词得到的信息。 其中： $$\\overrightarrow z_i=\\begin{bmatrix} h^p \\ H^q\\overrightarrow {\\alpha_i^T} \\ \\end{bmatrix} $$ $$h^r=\\overrightarrow{LSTM}(\\overrightarrow{z_i},\\overrightarrow{h^r_{i-1}})$$ 然后类似于LSTM将 $\\overrightarrow{h_{i-1}^r}$ 和 当前 passage 的表示 $H^p_i$ 耦合得到的 $R^{l\\times 1}$ 的向量重复Q 次，得到 $R^{l\\times Q}$，所以 $\\overrightarrow G_i\\in R^{l\\times Q}$, 在通过一个softmax-affine网络得到 attention weights. 整个思路下来，就是 attention score 不是通过矩阵相乘，也不是向量 $h^p_i, H^q$ 相减之后通过神经网络得到。但是也相似，就是对当前要匹配的两个向量 $h^p_i, H^q$ 通过两层神经网络得到,其中的对当前向量 $H_i^p$ 和 $\\overrightarrow {h_{i-1}^r}$ 要重复 Q 次。。。其实跟 DMN 还是相似的，只不过不是简单的 attention 当前的向量，还用了 LSTM 来耦合之前的信息。 最终得到想要的结合了 attention 和 LSTM 的输出 $\\overrightarrow h^r$. 作者做了一个反向的 LSTM. 方式是一样的： $$\\overleftarrow G_i=tanh(W^qH^q+(W^pH_i^p+W^r\\overleftarrow {h^r}_{i-1}+b^p)\\otimes e_Q)$$ $$\\overleftarrow \\alpha_i=softmax(w^T\\overleftarrow G_i + b\\otimes e_Q)$$ 同样得到 $\\overleftarrow {h_i^r}$. $\\overrightarrow {H^r}\\in R^{l\\times P}$ 表示隐藏状态 $[\\overrightarrow {h^r_1}, \\overrightarrow {h^r_2},…,\\overrightarrow {h^r_P}]$. $\\overleftarrow {H^r}\\in R^{l\\times P}$ 表示隐藏状态 $[\\overleftarrow {h^r_1}, \\overleftarrow {h^r_2},…,\\overleftarrow {h^r_P}]$. 然后把两者堆叠起来得到通过 question 匹配之后的 passage 向量表示： $H^r=\\begin{bmatrix} \\overrightarrow H^r \\ \\overleftarrow H^r \\end{bmatrix} \\in R^{2l\\times P}$ Answer Pointer LayerThe Sequence ModelThe answer is represented by a sequence of integers $a=(a_1,a_2,…)$ indicating the positions of the selected tokens in the original passage. 再一次利用 attention，$\\beta_{k,j}$ 表示 answer 中第 k 个token选择 passage 中第 j 个次的概率。所以 $\\beta_k\\in R^{P+1}$. $$F_k=tanh(V\\tilde {H^r}+(W^ah^a_{k-1}+b^a)\\otimes e_{P+1})\\in R^{l\\times P+1}$$ $$\\beta_k=softmax(v^TF_k+c\\otimes e_{P+1}) \\in R^{1\\times (P+1)}$$ 其中 $\\tilde H\\in R^{2l\\times (P+1)}$ 表示 $H^r$ 和 zero vector 的叠加, $\\tilde H=[H^r, 0], V\\in R^{l\\times 2l}, W^a\\in R^{l\\times l}, b^a,v\\in R, c\\in R$. 所以还是跟 match-LSTM 一样，先对 $H^r$ 中的每一个词通过全链接表示 $W^ah^a_{k+1}+b^a$, 然后重复 P+1 次，得到 $R^{l\\times (P+1)}$. 在通过激活函数 tanh， 再通过一个全连接神经网络，然后使用 softmax 进行多分类。 $$h_k^a=\\overrightarrow{LSTM}(\\tilde {H^r}\\beta_k^T, h^a_{k-1})$$ 这里是把 $\\tilde H^r$ 与权重 $\\beta_k$ 矩阵相乘之后的结果作为 LSTM k 时刻的输入。很玄学， 感觉可以看作是 self-attention 结合了 LSTM. 对生成 answer sequence 的概率进行建模： $$p(a|H^r)=\\prod_k p(a_k|a_1,a_2,…,a_{k-1}, H^r)$$ 其中： $$p(a_k=j|a_1,a_2,…,a_{k-1})=\\beta_{k,j}$$ 目标函数 loss function: $$-\\sum_{n=1}^N logp(a_n|P_n,Q_n)$$ The Boundary ModelSo the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to Hr, and the probability of generating an answer is simply modeled as: $$p(a|H^r)=p(a_s|H^r)p(a_e|a_s, H^r)$$ Search mechanism, and bi-directional Ans-Ptr. TrainingDatasetSQuAD: Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics. Each passage is a single paragraph from a Wikipedia article, and each passage has around 5 questions associated with it. In total, there are 23,215 passages and 107,785 questions. The data has been split into a training set (with 87,599 question-answer pairs), a development set (with 10,570 questionanswer pairs) and a hidden test set configuration dimension l of the hidden layers is set to 150 or 300. Adammax: $\\beta_1=0.9, \\beta_2=0.999$ minibatch size = 30 no L2 regularization. Result","link":"/2018/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Match-LSTM/"},{"title":"论文笔记-character embedding and ELMO","text":"paper: Character-Aware Neural Language Models paper: Deep contextualized word representations character embeddingMotivation A language model is formalized as a probability distribution over a sequence of strings (words), and traditional methods usually involve making an n-th order Markov assumption and estimating n-gram probabilities via counting and subsequent smoothing (Chen and Goodman 1998). The count-based models are simple to train, but probabilities of rare n-grams can be poorly estimated due to data sparsity (despite smoothing techniques). 对语言模型的描述：语言模型是 一个单词序列的概率分布 的形式化描述（什么意思？就是比如这个句子长度为 10, 那么每个位置可能是词表中的任意一个词，而出现当前词是有一个概率的, 这个概率是依赖于之前的词的）。 在传统的方法主要是运用 n阶马尔可夫假设来估计 n-gram 的概率，通过统计计数，以及子序列平滑的方式。这种基于计数的模型虽然简单，但是在数据稀疏的情况下，对不常见的 n-gram 的概率估计会很差。 While NLMs have been shown to outperform count-based n-gram language models (Mikolov et al. 2011), they are blind to subword information (e.g. morphemes). For example, they do not know, a priori, that eventful, eventfully, uneventful, and uneventfully should have structurally related embeddings in the vector space. Embeddings of rare words can thus be poorly estimated, leading to high perplexities for rare words (and words surrounding them). This is especially problematic in morphologically rich languages with long-tailed frequency distributions or domains with dynamic vocabularies (e.g. social media). neural language models 将词嵌入到低维的向量中，使得语义相似的词在向量空间的位置也是相近的。然后 Mikolov word2vec 这种方式不能有效的解决子单词的信息问题，比如一个单词的各种形态，也不能认识前缀。这种情况下，不可避免的会造成不常见词的向量表示估计很差，对于不常见词会有较高的困惑度。这对于词语形态很丰富的语言是一个难题，同样这种问题也是动态词表的问题所在（比如社交媒体）。 Recurrent Neural Network Language Model给定词表为 V，之前的序列是 $w_{1:t}=[w_1,..,w_t]$,在 RNN-LM 中通过全链接 affine transformation 计算 $w_{t+1}$ 个词的概率分布： $$Pr(w_{t+1}=j|w_{1:t})=\\dfrac{exp(h_t\\cdot p^j+q^j)}{\\sum_{j’\\in V}exp(h_t\\cdot p^{j’}+q^{j’})}$$ 其中 $h_t$ 是当前 t 时刻的隐藏状态。也就是先通过全链接映射到词表的 V 的维度，然后通过 softmax 计算其是词表中第 j 个词的概率。 然后假设训练预料库的 sentence 是 $w_{1:T}=[w_1,…,w_T]$,那么训练也就是最小化这个序列的 似然概率的负对数： $$NLL=-\\sum_{T}^{t=1}logPr(w_t|w_{1:t-1})$$ Chracter-level Convolution Neural Network 以单词 absurdity 为例，有 l 个字符（通常会 padded 到一个固定size），通过 character embedding 映射成矩阵 $C\\in R^{d\\times l}$. d 是 embedding size. 图中 embedding size 为 4. 然后使用卷积核 kernel H 做卷积运算, $H\\in R^{d\\times w}$，所以得到的 feature map $f^k\\in R^{l-w+1}$. 跟之前 CNN 做文本分类其实挺像的, kernel 的长是 embedding size d, 宽度 w 分别是 2,3,4. 上图中蓝色区域为例，filter 宽度为 2 的个数是3, 那么卷积得到的 featur map 是 $3 \\times (9-2+1) = 3\\times 8$. $$f^k[i]=tanh(&lt;C^k[* ,i:i-w+1], H&gt; +b)$$ &lt;&gt;表示做卷积运算(Frobenius inner product). 然后加上 bias 和 非线性激活函数 tanh. 接着基于 times 维度做 max pooling. 上图中 filter 宽度为 3,2,4 的个数分别为 4,3,5.所以得到长度为 4+3+5=12 的向量。 这里每一个 filter matrix 得到一个相应的特征 feature. 在通常的 NLP 任务中这些 filter 的总数 $h\\in[100, 1000]$ Highway Network通过卷积层得到单词 k 的向量表示为 $y^k$. Highway Network 分为两层 layer. one layer of an MLP applies an affine transformation: $$z=g(W_y+b)$$ one layer 有点类似 LSTM 中的 gate 机制： $$z=t\\circ g(W_Hy+b_H)+(1-t)\\circ y$$ 其中 g 是非线性函数。$t=\\sigma(W_Ty+b_T)$. t 成为 transform gate, (1-t) 是 carry gate. 同 LSTM 类似， highway network 允许输出能自适应的从 $y^k$ 中直接获取信息。 ELMo传统的提取 word embedding 的方法，比如 word2vec 和 language model， 前者是通过词与词之间的共现，后者是 contextual，但他们都是获得固定的 embedding，也就是每一个词对应一个单一的 embedding. 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练 language model, 而 word embedding 是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 且 ELMo 输出多个层的 embedding 表示, 试验中已经发现每层 LM 输出的信息对于不同的任务效果不同, 因此对每个 token 用不同层 embedding 表示会提升效果. 个人觉得，可以从这个角度去理解。RNN 可以看做一个高阶马尔可夫链，而不同于 马尔可夫模型，RNN 中的状态转移矩阵是用神经网络来模拟的，也就是我们计算隐藏层所用的 $h_t=tanh(w_{hh}h_{t-1}+w_{hx}x_t)$. 这个状态转移是动态的，也是不断更新的。而使用 语言模型 来训练 RNN/LSTM 目的就是得到这样的一套参数，使得它能学习到任何 合理的，自然的 sentence. 所以，这个语料库越大越好。事实上，有监督的训练也可以达到这个目的，但是有监督的数据有限，并且整个模型是有偏置的，比如文本分类的任务去训练，那么它更倾向于 局部信息。相比之下，机器翻译作为有监督的效果会更好，最好的还是语言模型呢，不仅可用的数据量很大，而且因为要预测每一个词的信息，它会努力结合每一个词的上下文去学习这个词的表示。这也正是我们需要的。ELMo 和 BERT 都是这样的道理，而 BERT 的优势前一篇 blog 说过了。 Bidirectional language models给定 sentence $t_1, t_2,…,t_N$, 通过前面的词 $t_1,..,t_{k-1}$ 计算 token $t_k$ 的概率分布: 反向： 语言模型的训练就是采用极大似然估计，最大化这个概率： 传统的方法就是 提取出对应位置的向量表示作为对应位置的词向量 context-independent token representation $x_k^{LM}$. ELMo ELMo is a task specific combination of the intermediate layer representations in the biLM. ELMo 实际上只是下游任务的中间层，跟 BERT 一样。但也有不同的是， ELMo 每一层的向量表示会获得不同的 信息。底层更能捕捉 syntax and semantics 信息，更适用于 part-of-speech tagging 任务，高层更能获得 contextual 信息，更适用于 word sense disambiguation 任务。所以对不同的任务，会对不同层的向量表示的利用不同。 在使用 ELMo 进行下游有监督训练时，通常是这样 $[x_k; ELMo_k^{task}]$. 对于 SQuAD 这样的任务，$[h_k, ELMo_k^{task}]$. Model architecture The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation. 具体模型还是得看代码。","link":"/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/"},{"title":"论文笔记 memory networks","text":"Memory Networks 相关论文笔记。 Memory Network with strong supervision End-to-End Memory Network Dynamic Memory Network Paper reading 1: Memory Networks, Jason WestonMotivationRNNs 将信息压缩到final state中的机制，使得其对信息的记忆能力很有限。而memory work的提出就是对这一问题进行改善。 However, their memory (encoded by hidden states and weights) is typically too small, and is not compartmentalized enough to accurately remember facts from the past (knowledge is compressed into dense vectors). RNNs are known to have difficulty in performing memorization. Memory Networks 提出的基本动机是我们需要 长期记忆（long-term memory）来保存问答的知识或者聊天的语境信息，而现有的 RNN 在长期记忆中表现并没有那么好。 Memory Networks four components: I:(input feature map) 把输入映射为特征向量，可以包括各种特征工程，比如parsing, coreference, entity resolution,也可以是RNN/LSTM/GRU。通常以句子为单位，将sentence用向量表示，一个句子对应一个sparse or dense feature vector. G:(generalization) 使用新的输入数据更新 memories O:(output feature map) 给定新的输入和现有的 memory state，在特征空间里产生输出 R:(response) 将输出转化为自然语言 详细推导过程 1.I component: :encode input text to internal feature representation. 可以选择多种特征，比如bag of words, RNN encoder states, etc. 2.G component: generalization 就是结合 old memories和输入来更新 memories. $m_i=G(m_i, I(x),m), ∀i$ 最简单的更新memory的方法是 $m_{H(x)}=I(x)$, $H(x)$ 是一个寻址函数slot selecting function，G更新的是 m 的index，可以把新的memory m，也就是新的输入 I(x) 保存到下一个空闲的地址 $m_n$ 中，并不更新原有的memory. 更复杂的 G 函数可以去更新更早的memory，甚至是所有的memory. 这里的新的input，如果在QA中就是question 和 old memmory的组合 $[I(x), m_i]$. 3.O component: reading from memories and performing inference, calculating what are the relevant memories to perform a good response. 给定新的输入和memory，在memories中寻找最相关的k个记忆 如果k=2： $$o_1=O_1(q,m)=argmax_{i=1,2,..,N}s_O(q,m_i)$$ $$o_2=O_2(q,m)=argmax_{i=1,2,..,N}s_O([q,o_1],m_i)$$ output: $[q,o_1, o_2]$ 也是module R的输入. $s_O$ is a function that scores the match between the pair of sentences x and mi. $s_O$ 用来表征 question x 和 记忆 $m_i$ 的相关程度。 $$s_O=qUU^Tm$$ $s_O$ 表示问题q和当前memory m的相关程度 U：bilinear regression参数，相关事实的 $qUU^Tm_{true}$ 的score高于不相关事实的分数 $qUU^Tm_{random}$ 4.R component : 对 output feature o 进行解码，得到最后的response: r=R(o) $$r=argmax_{w\\in W}s_R([q,m_{o_1},m_{o_2}],w)$$ W 是词典，$s_R$ 表示与output feature o 最相关的单词。 $s_R$ 和 $s_O$ 的形式是相同的。 $$s(x,y)=xUU^Ty$$ Huge Memory 问题如果memory太大，比如 Freebase or Wikipedia， 可以按 entity 或者 topic 来存储 memory，这样 G 就不用在整个 memories 上操作了 如果 memory 满了，可以引入 forgetting 机制，替换掉没那么有用的 memory，H 函数可以计算每个 memory 的分数，然后重写 还可以对单词进行 hashing，或者对 word embedding 进行聚类，总之是把输入 I(x) 放到一个或多个 bucket 里面，然后只对相同 bucket 里的 memory 计算分数 损失函数损失函数如下，选定 2 条 supporting fact (k=2)，response 是单词的情况： 多类支持向量机损失: minimize: $L_i = \\sum_{j\\ne y_i}max(0,s_j - s_{y_i}+\\Delta)$ 其中 $\\overline f, \\overline f’,\\overline r$ 表示负采样。比如（8）式中r表示 true response, 而 $\\overline r$ 表示随机抽样词典中的其他词。 QA实例： (6) 有没有挑选出正确的第一句话 (7) 正确挑选出了第一句话后能不能正确挑出第二句话 (6)+(7) 合起来就是能不能挑选出正确的语境，用来训练 attention 参数 (8) 把正确的 supporting fact 作为输入，能不能挑选出正确的答案，来训练 response 参数 Paper reading 2 End-To-End Memory Networksmotivation上一篇paper中的缺陷： The model in that work was not easy to train via backpropagation, and required supervision at each layer of the network. 这篇论文可以看作是上一篇论文memory networks的改进版。 Our model can also be seen as a version of RNNsearch with multiple computational steps (which we term “hops”) per output symbol. 也可以看做是将multiple hops应用到RNNsearch这篇论文上 Neural Machine Translation by Jointly Learning to Align and Translate。 Model architectureSingle layer 输入： input: $x_1,…,x_i$ query: q answer: a 对于单层网络，主要分为以下几个步骤： 1.将input和query映射到特征空间 memory vector {$m_i$}: ${x_i}\\stackrel A\\longrightarrow {m_i}$ internal state u: $q\\stackrel B \\longrightarrow u$ 2.计算attention，也就是query的向量表示u，和input中各个sentence的向量表示 $m_i$ 的匹配度。compute the match between u and each memory mi by taking the inner product followed by a softmax. $$p_i=softmax(u^Tm_i)$$ p is a probability vector over the inputs. 3.得到context vector output vector: ${x_i}\\stackrel C\\longrightarrow {c_i}$ The response vector from the memory o is then a sum over the transformed inputs ci, weighted by the probability vector from the input: $$o = \\sum_ip_ic_i$$ 和 Memory Networks with Strong Supervision 版本不同，这里的 output 是加权平均而不是一个 argmax 4.预测最后答案，通常是一个单词 $$\\hat a =softmax(Wu^{k+1})= softmax(W(o^k+u^k))$$ W可以看做反向embedding，W.shape=[embed_size, V] 5.对 $\\hat a$ 进行解码，得到自然语言的response $$\\hat a \\stackrel C \\longrightarrow a$$ 其中： A: intput embedding matrix C: output embedding matrix W: answer prediction matrix B: question embedding matrix 单层网络实例： 这里的 memory {$m_i$} 直接用于输出向量 $c_i$. 其实我也疑惑，为啥要重新用一个output embedding C，直接用 $m_i$ 不好吗。其实这些小tricks也说不准好不好，都是试出来的吧，因为怎么说都合理。。。 Multiple Layers/ Multiple hops多层结构（K hops）也很简单，相当于做多次 addressing/多次 attention，每次 focus 在不同的 memory 上，不过在第 k+1 次 attention 时 query 的表示需要把之前的 context vector 和 query 拼起来，其他过程几乎不变。 $$u_{k+1}=u^k+o^k$$ 对比上一篇paper来理解多层网络也可以看做是四个组件构成的： input components: 就是将query和sentences映射到特征空间中 generalization components： 更新memory，这里的memory也是在变化的，${m_i}=AX$， 但是embedding matrix A 是逐层变化的 output components: attention就是根据inner product后softmax计算memory和query之间的匹配度，然后更新input，也就是[u_k,o_k]， 可以是相加/拼接，或者用RNN. 区别是，在上一篇论文中是argmax，$o_2=O_2(q,m)=argmax_{i=1,2,..,N}s_O([q,o_1],m_i)$, 也就是选出匹配程度最大的 memory $m_i$, 而这篇论文是对所有的memory进行加权求和 response components: 跟output components类似啊，上一篇论文是与词典中所有的词进行匹配，求出相似度最大的 $r=argmax_{w\\in W}s_R([q,m_{o_1},m_{o_2}],w)$，而这篇论文是 $\\hat a=softmax(Wu^{k+1})=softmax(W(u^k+o^k))$ 最小化交叉熵损失函数训练得到 answer prediction matrix W. Overall, it is similar to the Memory Network model in [23], except that the hard max operations within each layer have been replaced with a continuous weighting from the softmax. 一些技术细节每一层都有 mebedding matrices $A^k, C^k$,用来embed inputs {$x_i$},为了减少训练参数.作者尝试了以下两种情况： Adjacent 上一层的output embedding matrix 是下一层的 input embedding matrix, 即 $A^{k+1}=C^k$ 最后一层的output embedding 可用作 prediction embedding matrix， 即 $W^T=C^k$ question embedding matrix = input embedding matrix of the first layer, $B=A^1$ Layer-wise (RNN-like) $A^1=A^2=…=A^k, C^1=C^2=…C^k$ $u^{k+1} = Hu^k+o^k$ ExperimentsDataset数据集来源：Towards AI-complete question answering: A set of prerequisite toy tasks 总共有 20 QA tasks，其中每个task有 $I(I\\le 320)$ 个sentence {$x_i$}, 词典大小 V=170, 可以看做这是个玩具级的任务。每个task有1000个problems Modle detailsSentence representations也就是将input和query映射到特征空间，有两种方式： 1.Bag of words(BOW) representation $$m_i=\\sum_jAx_{ij}$$ $$c_i=\\sum_jCx_{ij}$$ $$u=\\sum_jBq_j$$ 分别对每个词embed，然后sum，缺点是没有考虑词序 2.encodes the position of words within the sentence 考虑词序的编码 $$m_i=\\sum_jl_j\\cdot Ax_{ij}$$ i表示第i个sentence，j表示这个sentence中的第j个word $$l_{kj}=(1-j/J)-(k/d)(1-2j/J)$$ 查看源码时发现很多代码的position encoder与原paper不一样，比如domluna/memn2n中公式是： $$l_{kj} = 1+4(k- (d+1)/2)(j-(J+1)/2)/d/J$$ 原本词 $x_{ij}$ 的向量表示就是embeded后的 $Ax_{ij},(shape=[1, embed_size])$, 但现在要给这个向量加一个权重 $l_j$,而且这个权重不是一个值，而是一个向量，对 $Ax_{ij}$ 中每一个维度的权重也是不一样的。 令J=20, d=50. 具体两个公式的差别可以查看 wolframalpha1 wolframalpha2 也就是说不仅跟word在sentence中的位置有关，还和embed_size中的维度有关。这就很难理解了。。。 好像跟句子的结构相关，北大有篇相关的论文A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification 其中 J 表示sentence的长度，d表示 dimension of the embedding. 这种sentence representation称为 position encoding(PE).也就是词序会影响memory $m_i$. position encoding 代码实现 12345678910111213141516171819202122232425262728293031323334353637def position_encoding(sentence_size, embedding_size): &quot;&quot;&quot; Position Encoding described in section 4.1 [1] &quot;&quot;&quot; encoding = np.ones((embedding_size, sentence_size), dtype=np.float32) le = embedding_size+1 ls = sentence_size + 1 for k in range(1, le): for j in range(1, ls): # here is different from the paper. # the formulation in paper is: l_{kj}=(1-j/J)-(k/d)(1-2j/J) # here the formulation is: l_{kj} = 1+4(k- (d+1)/2)(j-(J+1)/2)/d/J, # 具体表现可查看 https://www.wolframalpha.com/input/?i=1+%2B+4+*+((y+-+(20+%2B+1)+%2F+2)+*+(x+-+(50+%2B+1)+%2F+2))+%2F+(20+*+50)+for+0+%3C+x+%3C+50+and+0+%3C+y+%3C+20 encoding[k-1, j-1] = (k - (embedding_size+1)/2) * (j - (sentence_size+1)/2) encoding = 1 + 4 * encoding / embedding_size / sentence_size # Make position encoding of time words identity to avoid modifying them encoding[:, -1] = 1.0 # 最后一个sentence的权重都为1 return np.transpose(encoding) # [sentence_size, embedding_size] Temporal Encoding将memory改进为： $$m_i=\\sum_jAx_{ij}+T_A(i)$$ 其中 $T_A(i)$ is the ith row of a special matrix $T_A$ that encodes temporal information. 用一个特殊的矩阵 $T_A$ 来编码时间信息。$T_A(i)$ i表示第i个sentence的包含时间信息？？ 同样的output embedding: $$c_i=\\sum_jCx_{ij}+T_C(i)$$ Learning time invariance by injecting random noisewe have found it helpful to add “dummy” memories to regularize TA. Training Details1.learning rate decay 2.gradient clip 3.linear start training 4.null padding, zero padding 完整代码实现https://github.com/PanXiebit/text-classification/blob/master/06-memory%20networks/memn2n_model.py Paper reading 3 Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingMotivationMost tasks in natural language processing can be cast into question answering (QA) problems over language input. 大部分的自然语言处理的任务都可以看作是QA问题，比如QA, sentiment analysis, part-of-speech tagging. Model Architecture 可以分为以下4个模块： Input Module: 将输入文本编码为distribution representations Question Module: 将question编码为distribution representations Episodic Memory Module: 通过attention机制选择focus on输入文本中的某些部分，然后生成memory vector representation. Answer Module: 依据the final memory vector生成answer Detailed visualization: Input Module主要分为两种情况： 1.输入是single sentence，那么input module输出的就是通过RNN计算得到的隐藏状态 $T_C= T_I$, $T_I$ 表示一个sentence中的词的个数。 2.输入是a list of sentences，在每个句子后插入一个结束符号 end-of-sentence token, 然后每个sentence的final hidden作为这个sentence的representation. 那么input module输出 $T_C$, $T_C$等于sequence的sentence个数。 然后RNN使用的是GRU，作者也尝试过LSTM，发现效果差不多，但LSTM计算量更大。 Question Module同样的使用GRU编码，在t时间步， 隐藏状态 $$q_t=GRU(L[w_t^Q],q_{t-1})$$ L代表embedding matrix. 最后输出 final hidden state. $$q=q_{T_Q}$$ $T_Q$ 是question的词的个数。 Episodic Memory Module由 internal memory, attention mechansim, memory update mechanism 组成。 输入是 input module 和 question module 的输出。 把 input module 中每个句子的表达（fact representation c）放到 episodic memory module 里做推理，使用 attention 原理从 input module 中提取相关信息，同样有 multi-hop architecture。 1.Needs for multiple Episodes: 通过迭代使得模型具有了传递推理能力 transitive inference. 2.Attention Mechanism: 使用了一个gating function作为attention机制。相比在 end-to-end MemNN 中attention使用的是linear regression，即对inner production通过softmax求权重。 这里使用一个两层前向神经网络 G 函数. $$g_t^i=G(c_t,m^{i-1},q)$$ $c_t$ 是candidate fact, $m_{i-1}$ 是previous memory， question q. t 表示sentence中的第t时间步，i表示episodic的迭代次数。 这里作者定义了 a large feture $z(c,m,q)$ 来表征input, memory, question之间的相似性。 $$z_t^i=[c_t, m^{i-1},q, c_t\\circ q,c_t\\circ m^{i-1},|c_t-q|,|c_t-m^{i-1}|, c_t^TW^{(b)}q, c_t^TW^{(b)}m^{i-1}]$$ 总的来说，就是根据向量内积，向量相减来表示相似度。 跟cs224d-lecture16 dynamic Memory networkRichard Socher本人讲的有点区别，不过这个既然是人工定义的，好像怎么说都可以。 然后通过G函数，也就是两层前向神经网络得到一个scale score. $$G = \\sigma(W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)})$$ 将 $c_t$, $m^{i-1}, q 带入到G函数，即可求得$$g_i^t$，也就是candidate fact $c_i$ 的score. 计算完每一次迭代后的分数后，来更新episode $e^i$, 相当于 context vector, soft attention 在之前的attention机制中，比如cs224d-lecture10-机器翻译和注意力机制介绍的attention得到的context vector，在end-to-end MemNN中attention也是fact representation的加权求和。 attention based GRU 但这篇论文中应用了GRU，对fact representation c 进行处理，然后加上gate $$h_t^i=g_t^iGRU(c_t,h_{t-1}^i)+(1-g_t^i)h_{i-1}^t$$ 所以这里的GRU应该是 $T_C$步吧？？ 每次迭代的context vector是对 input module 的输出进行 attention-based GRU编码的最后的隐藏状态: $$e^i=h_{T_C}^i$$ 总结一下： 这部分attention mechanism目的就是生成episode $e^i$,$e^i$ 是第i轮迭代的所有input相关信息的summary.也就是 context vector,将input text压缩到一个向量表示中，end-to-end MemNN用了soft attention，就是加权求和。而这里用了GRU，各个时间步的权重不是直接相乘，而是作为一个gate机制。 3.Memory Update Mechanism 上一步计算的episode $e^i$ 以及上一轮迭代的memory $m^{i-1}$ 作为输入来更新memory $m_i$ $$m_i=GRU(e^i,m^{i-1})$$ $m^0=q$, 所以这里的GRU是单步的吧 经过 $T_M$ 次迭代： $m=m^{T_M}$, 也就是episodic memory module的输出，即answer module的输入。 在end-to-end MemNN的memory update中，$u_{k+1}=u^k+o^k$, 而在这篇论文中,如果也采用这种形式的话就是 $m^{i}=e^i+m^{i-1}$，但作者采用了 RNN 做非线性映射，用 episode $e_i$ 和上一个 memory $m_{i−1}$ 来更新 episodic memory，其 GRU 的初始状态包含了 question 信息，$m_0=q$。 4.Criteria for stopping Episodic Memory Module 需要一个停止迭代的信号。一般可以在输入中加入一个特殊的 end-of-passes 的信号，如果 gate 选中了该特殊信号，就停止迭代。对于没有显性监督的数据集，可以设一个迭代的最大值。 Answer Module使用了GRU的decoder。输入是question module的输出q和上一个时刻的hidden state $a_{t-1}$,初始状态是episodic memory module的输出 $a_0=m^{T_M}$. $$y_t=softmax(W^{(a)}a_t)$$ $$a_t=GRU([y_{t-1},q],a_{t-1})$$ 这里应该就是单步GRU吧，毕竟question的向量表示q只有一个呀。 Train使用 cross-entroy 作为目标函数。如果 数据集有 gate 的监督数据，还可以将 gate 的 cross-entroy 加到总的 cost上去，一起训练。训练直接使用 backpropagation 和 gradient descent 就可以。 总结:对比上一篇论文End-to-end memory networks input components: end2end MemNN 采用embedding，而DMN使用GRU generalization components: 也就是memory update，End2End MemNN采用线性相加 $u^{k+1}=u^k+o^k$,其中的 $o^k$ 就是经过attention之后得到的memory vector output components: end2end MemNN采用的是对比memory和query,用内积求相似度，然后softmax求权重，最后使用加权求和得到context vector. 而DMN采用的是人工定义相似度的表示形式，然后用两层前向神经网络计算得到score，再对score用softmax求权重，再然后把权重当做gate机制，使用GRU求context vector response components: end2end MemNN 直接使用最后的 top memory layer 预测，而DMN是把top memory 当做init hidden state 总之，DMN实在是太太太复杂了。。每一个module都用到了RNN Paper reading 4 DMN+paper:Dynamic Memory Networks for Visual and Textual Question Answering (2016) Motivate提出了DMN+，是DMN的改进版，同时将其应用到 Visual Question Answering 这一任务上。 However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. 这段话是描述DMN的缺点的，在没有标注 supporting facts的情况下表现不好。但是DMN貌似也并不需要标注 supporting facts啊。。。 Like the original DMN, this memory network requires that supporting facts are labeled during QA training. End-toend memory networks (Sukhbaatar et al., 2015) do not have this limitation. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. 这篇文章对DMN中的 input module进行了修改，并且提出了新的模型架构适用于图像的。 DMN 存在的两个问题： 输入模块只考虑了过去信息，没考虑到将来信息 只用 word level 的 GRU，很难记忆远距离 supporting sentences 之间的信息。 总的来说这篇文章贡献主要还是在应用到图像上了，至于作者所说的 input module的改进，只是为了减少计算量，而且改进版中的 bi-RNN 和 position encoding 都是在别人的论文中出现了的。 Model Architecture同DMN一样，也分为 input module, question module, episodic module 和 answer module. Input Moduleinput module for text QA 主要分为两个组件： sentence reader 和 input fusion layer. sentence reader: 用encoding position代替RNN对单个sentence进行编码。用 positional encoding 的原因是在这里用 GRU/LSTM 编码句子计算量大而且容易过拟合（毕竟 bAbI 的单词量很小就几十个单词。。），这种方法反而更好。 input fusion layer: 使用 bi-directional GRU 来得到context 信息，兼顾过去和未来的信息。 总的来说： DMN+ 把 single GRU 替换成了类似 hierarchical RNN 结构，一个 sentence reader 得到每个句子的 embedding，一个 input infusion layer 把每个句子的 embedding 放入另一个 GRU 中，得到 context 信息，来解决句子远距离依赖的问题。 input module for VQA 1.Local region feature extraction: 获取局部特征信息，使用VGG预训练得到的特征。局部特征 feature vector 通过一个linear layer 和 tanh activation 得到 feature embedding. 2.Input fusion layer: 将 feature embedding 放入到 bi-GRU 中。 Without global information, their representational power is quite limited, with simple issues like object scaling or locational variance causing accuracy problems. 强调了为什么要使用 input fusion layer. Question Module这部分跟DMN是一样的, question 都是文本，用RNN编码。 Episodic Modulescore mechanisminput module 的输出是: $$\\overleftrightarrow F=[\\overleftrightarrow f_1, …,\\overleftrightarrow f_N]$$ 同DMN一样，作者也是用了人工特征，相比DMN简化一点： $$z_i^t=[\\overleftrightarrow f_i\\circ q,\\overleftrightarrow f_i\\circ m^{t-1},|\\overleftrightarrow f_i-q|,|\\overleftrightarrow f_i-m^{i-1}|]$$ 这里与前面DMN的公式有点区别，就是这里的i表示input module中的时间步， t 表示episodic迭代次数。 同样使用一个两层前向神经网络： $$G = W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)}$$ 但是这里不是使用 sigmoid 函数来求的 score，而是使用softmax 来求score $g_i^t$. $$g_i^t=\\dfrac{Z_i^t}{\\sum_{k=1}^{M_i}exp(Z_k^t)}$$ attention mechanism比较了 soft attention 和 attention-based-GRU.相比DMN那篇论文，这里给出了详细的比较。 soft attention, 就是简单的加权求和。$$c^t=\\sum_{i=1}^Ng_i^t\\overleftrightarrow f_i$$ 其缺点在于丢失了位置信息和词序信息。 感觉简单的attention已经很好了吧。。前面 $\\overleftrightarrow f_i$ 不就是考虑了词序信息的么，然后再用GRU对 $\\overleftrightarrow f_i$ 处理不会过拟合吗？？？ attention based GRU使用attention gate $g_i^t$ 代替 update gate $u_i$. 我们知道 $u_i$ 是通过 current input 和 previous hidden state得到的。 而使用 attention gate $g_i^t$ 能够考虑到 question 和 previous memory. 因为我们这里是要更新memory， 所以这样很合理呀。。厉害了 $$h_i=g_i^t\\circ \\tilde h_i+(1-g_i^t)\\circ h_{i-1}$$ memory update mechanism在DMN中，更新memory在基于 previous memory 和 当前的 context vector 的GRU编码得到的。 DMN+采用的是将 previous memory $m^{t-1}$, 当前 context $c^t$，和question q 拼接起来，然后通过全连接层，以及relu激活函数得到的： $$m_t = ReLU(W^t[m^{t-1},c^t,q]+b)$$ 使用relu的全连接层能提升0.5%的准确率。 Answer Module同DMN. reference: 徐阿衡-论文笔记 - Memory Networks","link":"/2018/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-memory-networks/"},{"title":"论文笔记-sentence embedding","text":"句子的向量表示方法主要分为两类。 一类是通过无监督的方法得到 universal sentence embedding, 另一类是基于特定的任务，有标签的情况下，通过有监督学习得到 sentence embedding. supervised learninga structured self-attentive sentence embeddingpaper: A Structured Self-attentive Sentence Embedding, ICLR 2017 传统的基于 RNN/LSTM 得到 sentence 的向量表示的方法通常是利用隐藏状态的最后一个状态，或者是所有时间步的隐藏状态，然后采用 sum/average/max pooling 的的方式得到 sentence embedding. 本文使用的方法就是在 LSTM 上加上一层 attention，通过注意力机制自动选择 sentence 中的某些方面，也就是赋予 sentence 中的每一个词一个权重，然后加权求和得到一个 vector. 本文另一个创新点在于，不仅仅得到一个 vector，而是一个 matrix，用来表示一个 sentence 中的不同方面。 Model Architecture: 模型也很简单，输入 sentence n tokens. word embedding: $S\\in R^{n\\times d}$, d 表示词向量维度 $$S=(w_1,w_2,…,w_n)$$ bidirection-LSTM: $H\\in R^{n\\times 2u}$, u 表示隐藏状态维度 $$H=(h_1,h_2,…,h_n)$$ single self-attention: $a\\in R^n$, 表示 sentence 中对应位置的权重。与 encoder 之后的 sentence 加权求和得到 attention vector $m\\in R^{2u}$. $$a=softmax(w_{s2}tanh(W_{s1}H^T))$$ r-dim self-attention：有 r 个上述的 attention vector，并转换成矩阵形式，$A\\in R^{n\\times r}$ 与 encode 之后的句子表示 H 加权求和得到 embedding matrix $M\\in R^{r\\times 2u}$ $$A=softmax(W_{s2}tanh(W_{s1}H^T))$$ $$M=AH$$ penalization term上面模型中使用 r 个 attention，很可能会出现冗余的情况，也就是得到的 r 个 attention vector( 论文中说的是 summation weight vectors) 可能得到的是同一个东西，所以需要 diversity. The best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors. However, we found that not very stable in our case. We conjecture it is because we are maximizing a set of KL divergence (instead of minimizing only one, which is the usual case), we are optimizing the annotation matrix A to have a lot of sufficiently small or even zero values at different softmax output units, and these vast amount of zeros is making the training unstable. There is another feature that KL doesn’t provide but we want, which is, we want each individual row to focus on a single aspect of semantics, so we want the probability mass in the annotation softmax output to be more focused. but with KL penalty we cant encourage that. 一个最直观的方法是 Kullback Leibler divergence，也就是相对熵。因为得到的 attention vector 是一个概率分布 (distribution ), 任意两个分布差异越大，对应的相对熵越大。但是作者实验发现这种方法不稳定，原因作者推测是这里需要最大化的是多个 KL 散度的集合，并且在优化 annotation matrix A 时，在不同的softmax输出单元上有很多足够小甚至零值，而这些大量的零点使得训练不稳定. 另一方面，KL 散度不能 focus on 语义中的单个方面。 针对上面这两点，作者提出了一个新的正则项： $$P=||(AA^T-I)||^2_{F}$$ $AA^T$ 是协方差矩阵，对角线元素是同一向量的内积，非对角线元素不同向量的内积。将其作为惩罚项加到 original loss 上，期望得到的是不同 vector 内积越小越好（内积越小，差异越大），并且向量的模长越大越好（概率分布更集中于某一两个词）。 最终得到矩阵级别的句子向量表示。 training3 different datasets: the Age dataset the Yelp dataset the Stanford Natural Language Inference (SNLI) Corpus 根据不同的任务有监督的训练。 unsupervised learningsent2vecpaper: Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features","link":"/2019/02/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sentence-embedding/"},{"title":"论文笔记-DETR and Deformable DETR","text":"DETR: End-to-End Object Detection with Transformers Deformable DETR： Deformable Transformer for End-to-End Object Detection DETR: End-to-End Object Detection with Transformerspaper link: https://arxiv.org/abs/2005.12872 Motivation传统的目标检测范式： 使用CNN Backbone提取feature map; 使用 Region Proposal Network (RPN) 在feature map上枚举所有的windows,输出N个候选boxes 使用分类器对每个box进行分类 这样存在很多问题： 问题1：Enumerate candidate boxes in RPN 枚举所有的pixel 对每个pixel，枚举出所有预定义大小的boxes 大部分的候选框都是无效的，因此 Inefficient, slow 问题2：Redundant boxes and NMS RPN 输出大量的boxes Non-maximum suppression (NMS) merges/removes redundant boxes These hand-designed components have a few hyperparameters Model tuning is complex Architecture of DETR因此，作者提出了更为简单的end-to-end的目标检测模型： Self-attention vs. Convolution in VisionTransformer 已经非常熟悉了，就不详细介绍了。这里对比下 self-attention 和 CNN 在视觉领域的应用 每个pixel可以看作是自然语言中的 token convolution is used to integrate pixels Recognize patterns within a small window of pixels Difficult to integrate non-local pixels Have to make network very deep to “see the big picture” Self-attention (transformer) also integrates multiple pixels Works when the correlated pixels are non-local Trunk, tail, legs of an elephant to a whole elephant Architecture 整个结构可以看作四部分： CNN backbone Transformer Encoder Transformer Decoder bipartite matching loss CNN backbone + Transformer Encoder前两个好理解，需要注意的是 position encoding 与NLP中不同。考虑到输入是二维图像，对应的位置 (x,y) 也是二维的。 总结下和原始transformer编码器不同的地方： 输入编码器的位置编码需要考虑2-D空间位置。 位置编码向量需要加入到每个Encoder Layer中。 在编码器内部位置编码Positional Encoding仅仅作用于Query和Key，即只与Query和Key相加，Value不做任何处理。 Transformer Decoderdecoder与原始的transformer decoder的区别在于两点： 其输入是 Object queries. 是可学习的 nn.Embedding, 维度为 [100, bsz, 256]. 其实可以理解成可学习的位置编码。 非自回归的可并行解码。 bipartite matching lossdecoder 的输出张量的维度是 分类分支：[bsz, 100, class + 1] 和回归分支：[bsz, 100, 4]. 其中 class + 1 表示类别总数+背景。有物体的boxes计算回归任务；没有物体的背景框，则不用回归。 问题来了：这100个候选框如何去与 不确定数量的 targets 进行匹配呢，预测框和真值是怎么一一对应的？换句话说：你怎么知道第47个预测框对应图片里的狗，第88个预测框对应图片里的车？等等。 相比Faster R-CNN等做法，DETR最大特点是将目标检测问题转化为无序集合预测问题(set prediction)。论文中特意指出Faster R-CNN这种设置一大堆anchor，然后基于anchor进行分类和回归其实属于代理做法即不是最直接做法，目标检测任务就是输出无序集合，而Faster R-CNN等算法通过各种操作，并结合复杂后处理最终才得到无序集合属于绕路了，而DETR就比较纯粹了。现在核心问题来了：输出的 [bsz, 100] 个检测结果是无序的，如何和 ground-truth bounding box 计算loss？这就需要用到经典的双边匹配算法了，也就是常说的 匈牙利算法，该算法广泛应用于最优分配问题。 整个loss function的计算分为两个步骤： 依据匈牙利算法先找到最优的匹配 根据找到的匹配，计算最终的loss 如何找到最优的匹配呢？假设一张图片有3个目标：dog, cat, car. 那么我们可以得到一个 [100, 3] 的矩阵, 矩阵的值分别表示这100个候选token为其中某一目标的“损失”. 让这个损失最小的匹配就是我们要找的最优匹配。这个过程就是匈牙利算法。 再介绍匈牙利算法之前，我们要知道这个矩阵对应的损失值是啥呢？也就是怎么衡量这个候选token是某个目标的损失值？ 假设 $y_i$ 对应的预测是 $\\hat y_{\\sigma_i}$,那么这个构成这个搭配的损失就是 $\\hat y_{\\sigma_i}$ 对应的类别为 $c_i$ 以及 box 的 mse 值。 $$L_{match}(y_i, \\hat y_{\\sigma(i)})=-\\mathbb{1}{c_i\\ne \\varnothing}\\hat p{\\sigma(i)}(c_i) + 1_{c_i\\ne \\varnothing}L_{box}(b_i, \\hat b_{\\sigma_i})$$ $L_{match}$ 就是矩阵中的元素。然后通过匈牙利算法找到对应的匹配。 找到匹配之后计算最终的loss Deformable DETRDETR 的优缺点： 优点 纯端到端，不需要手工设计的NMS之类 DETR 缺点： 小目标检测的低精度，高精度的feature map的复杂度是DETR所不能忍受的。 这是因为 attention 机制的原因，复杂度是 O(L^2) 收敛速度慢，slow convergence。原因是 pattern 是稀疏的，很难快速学到吧 when $N_k$ is large, it will lead $N_k$to ambiguous gradients for input features. Thus, long training schedules are required so that the attention weights can focus on specific keys. Deformable DETR 的设计初衷： motivation: mitigates the slow convergence and high complexity issues of DETR 具体设计方法 融合Deformable conv的空间稀疏采样和transformer的关系建模的优势 combines the best of the sparse spatial sampling of deformable convo- lution, and the relation modeling capability of Transformers deformable attention modules： 代替Transformer attention module Multi-Head Attention in Transformers常规的 attention: $A_{mqk}$ 是attention matrix. $z_q\\in R^C$ 是query feature. $x_k\\in R^C$ 是key/value feature. $\\Omega_k$ 是key index的集合. $m$ 是attention head index. 总的计算量是: $O(N_qC^2) + 2O(N_kC^2) + 2O(N_qN_kC^2)$ 复杂度：$O(N_qC^2 + N_kC^2 + N_qN_kC)$ 在长文本，或者image patched之后，$N_q=N_k &gt;&gt; C$. 所以复杂度取决于 $O(N_qN_kC)$ Deformable attention module 相比常规的attention, 需要考虑所有的key features，也就是 HW 个。 deformable attention只考虑K个keys. 如下面两个公式所示 $\\sum_{k\\in \\Omega_k} \\rightarrow \\sum_{k=1}^K$. 因此复杂度会大大降低，收敛速度也会加快。 其中： m 表示attention head index $z_q$ 表示query feature $K &lt;&lt; HW$ 表示sampling number $p_q$ 表示 $z_q$ 对应的参考2D点 $\\Delta p_{mqk}$ 表示相对参考点的sampling offset $A_{mqk}$ 则是对应采样点的weights, $\\sum_{k=1}^{K}A_{mqk}=1$ 现在的问题在于，这K个keys是怎么选择的呢？也就是 offset $\\Delta p_{mqk}$ 是怎么来的？文中是这么解释的： 这个过程如图2所示： 通过 linear projection 得到 2D 实数值 $\\Delta p_{mqk}$， 然后通过线性插值得到 $p_q + \\Delta p_{mqk}$. 对应的权重 $A_{mqk}$ 也是通过linear projection得到的。 这样 deformable attention 的复杂度：$N_qC^2 + O(N_qKC)$. Multi-scale Deformable Attention ModuleDeformable attention 可以很自然的拓展到multi-scale情况下。 m 表示 attention head index l 表示 input feature level k 表示 sampling index $A_{mlqk}$ 表示 $k^{th}$ sampling point在 $m^{th}$ head和 $l^{th}$ level的权重, $\\sum_{l=1}^{L}\\sum_{k=1}^{K}A_{mlqk}=1$. $\\Delta p_{mlqk}$ 表示sampling offset 为了确保multi-scale中 point的对应关系，作者用 normalized coordinates $\\hat p_q\\in [0,1]^2$ 来表示参考点。 $(0,0), (1,1)$ 表示top-left和bottom-right. $\\phi_l(\\hat p_q)$ 表示将归一化之后的 $\\hat p_q$ rescale 到 $l^{th}$ level feature中。 Deformable Transformer Encoder从 resnet $C_3$ 到 $C_5$ 抽取multi-scale特征图. 其中 $C_l$ 表示分辨率是输入图像的 $\\dfrac{1}{2^l}$. 这样就有3层feature map了，然后最后一层feature map是通过 kernel=$3\\times 3$, stride=$3$ 的卷积在 $C_5$ 上得到的。 总共4 levels feature map. key和query来自feature map中的pixels. 对于每一个query pixel, 其reference point是其本身。除了位置编码之外，作者还加入了level编码 $e_l$，用来表示在哪一层level. 位置编码是固定的，level编码是可训练的。","link":"/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/"},{"title":"论文笔记-unlikelihood training","text":"paper list: Neural Text Generation with Unlikelihood Training Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning 基于标准的似然训练的语言模型在解码时会出现很dull的重复问题。如下图所示： 诚然将likelihood作为训练目标，能得到强大的具有语言理解能力的模型。但是将似然作为目标会导致语言的平淡和奇怪的重复。而且基于强大的GPT2-117M，使用beam search，理论上beam size越大，生成的句子概率越大，效果越好才对。但事实却是反直觉的，beam size越大，语言的退化效果更明显。（我猜这也是很多时候beam size设为10左右，而不会更大。） Holtzman[^1] 揭示了语言模型的这种退化现象，他们将机器生成的语言和人类语言进行对比，如下图所示，人类文本在生成每个token的困惑度中表现出相当大的波动，而由最大似然解码产生的机器文本的分布则出现不自然的平坦和较高的token概率. 事实上，语言模型在生成词的时候，大部分的概率集中在几百个tokens上。这限制了机器文本的多样性，也是导致模型在长文本生成退化的原因。基于此，如下图所示，top-k, nucleus sampling是不错的方法。 但是sampling的方法并没有改变模型生成的概率其本身。生成文本退化的真正原因还未知，已有的论文认为有以下几种可能： 模型架构选择的by-product，例如 Transformer 更喜欢重复; 人类语言的内在属性，而不是模型缺陷； 语料有限; 相比之下，Welleck[^2] 认为生成模型以最大似然最为训练目标会导致文本生成退化。其原因有： 将注意力集中在argmax或top-k,而不是优化整个distribution 只是集中于下一个token的生成，而不是整个句子的优化 为此，Welleck[^2] 提出了 unlikelihood training: 依据likelihood来优化target tokens，并给予较大的概率 依据unlikelihood来更新，避免给予target tokens太大的概率。（不知理解的对错，原文如下） Unlikelihood training works by combining two types of updates: a likelihood update on the true target tokens so they are assigned high probability, and an unlikelihood update on tokens that are otherwise assigned too high a probability. unlikelihood lossunlikelihood loss的核心就是降低negative condidates $C^t$ 的似然概率。 token-level unlikelihood loss以自回归的语言模型为例，下一个时间步的loss计算包括target的似然最大化，以及negative candidates的似然最小化。 negative candidates 是当前试了之前的词汇。 sequence_level unlikelihood loss我们知道基于自回归模型的训练和解码是存在exposure bias的，也就是解码的时候有误差累积。其实这种distribution mismatch是maxmimum-likelihood追求当前时刻的概率最大话。而没有从整个句子的层面去考虑。比如重复问题，你上一个词出现过了，下一个词还出现它；你这句话说过一遍了，你还要再说一遍。。这不离谱吗。但是模型就是这么傻，或者说模型没有大局观的原因是之前的优化都是在token-level层面。 因此，Welleck[^2] 提出了sequence-level unlikelihood loss. 这个公式看起来跟前面一样，但是区别在于 negative condidates的选择。这里的negative是从n-gram层面来考量的。 也就是对于当前时间步，如果它是重复的n-gram的一部分，那么它就是negative candidate. 这里有点不太好理解，token-level就是把之前出现的词作为negative candidate. 如果有些词在前面并没有出现，但它的出现会导致重复的n-gram？这不合理啊，它都没出现，怎么可能出现包含它的n-gram呢？？ 这样想 sequence-level 不就是 token-level的一种特殊形式。 带着疑问去看代码吧。看完代码，sequence-level是在训练完token-level之后，再进行finetune，对导致出现重复的ngram的某个time-step进行惩罚。。在我的实验上不太靠谱，重复反而变多了 [^1]: The Curious Case of Neural Text Degeneration[^2]: Neural text degeneration with unlikelihood training","link":"/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/"},{"title":"论文笔记-dynamic convolution and involution","text":"paper list: CARAFE: Content-Aware ReAssembly of FEatures Involution: Inverting the Inherence of Convolution for Visual Recognition Pay less attention with lightweight and dynamic convolutions ConvBERT: Improving BERT with Span-based Dynamic Convolution Dynamic Region-Aware Convolution Involution为什么要叫反卷积呢？卷积的特性是： space-agnostic：空间不变性，也就是用一个kernel在feature map上滑动。这样学的到feature是单一的为了增加feature的丰富性，采用很大的channel channel-specific: 通道特异性。尽管channel增加能学到更多特征，但是通道太大其实是有冗余的，有人做低秩实验，发现很多channel对应的参数是线性相关的 于是，作者设计了一个与卷积完全相反的算子，反卷积： space-specific: 根据content生成相应的weights channel-agnostic: 共享参数。类似attention的projection和feedforward. 12345678910111213141516def forward(self, x): # 1. 生成pixel-wise对应的权重，每个pixel对应的权重是 [kernel_size^2*group]. # print(&quot;after avgpool: &quot;, self.avgpool(x).shape) # print(&quot;after conv1: &quot;, self.conv1(x if self.stride == 1 else self.avgpool(x)).shape) weight = self.conv2(self.conv1(x if self.stride == 1 else self.avgpool(x))) # print(&quot;weight: &quot;, weight.shape) b, c, h, w = weight.shape weight = weight.view(b, self.groups, self.kernel_size**2, h, w).unsqueeze(2) print(&quot;weight: &quot;, weight.shape) # 2. 将x通过unfold以kernel_size为大小，stride为步长i女性展开 print(&quot;after unfold: &quot;, self.unfold(x).shape) # [bs, channel*kernel*2, ((h-kernel+1+2*pad)/stride))^2] out = self.unfold(x).view(b, self.groups, self.group_channels, self.kernel_size**2, h, w) print(&quot;out: &quot;, out.shape) out = (weight * out).sum(dim=3).view(b, self.channels, h, w) return out 思路很像local attention, 区别在于这个weight是通过content+linear得到的，而不是通过pixel之间的relation得到的。而且，看源代码这个weights并没有做normalization. 然后把生成的weights与对应的pixel周围的[kernel,kernel]的pixels进行加权求和。","link":"/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/"},{"title":"论文笔记-constrast learning in NLP","text":"paper list: An efficient framework for learning sentence representations. CLEAR: Contrastive Learning for Sentence Representation Declutr: Deep contrastive learn- ing for unsupervised textual representations. arXiv SimCSE: Simple Contrastive Learning of Sentence Embeddings R-Drop: R-Drop: Regularized Dropout for Neural Networks Coco-lm: Correcting and contrasting text sequences for language model pretraining. Learning dense representations of phrases at scale. An unsupervised sentence embedding method by mutual information maximization. Representation degeneration problem in training natural language generation models CosReg: Improving neural language generation with spectrum control. CLAPS: Contrastive Learning with Adversarial Perturbations for Conditional Text Generation LINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding Adversarial PerturbationInterpretable Adversarial Perturbation in Input Embedding Space for Text Contrastive Learning for Many-to-many Multilingual Neural Machine Translation Unsupervised Data Augmentation for Consistency Training CLEAR 看图就能理解了。关键在于离散文本的 augmentation 如何实现的。 这样做的问题在于，对于离散的文本，删除一个词都可能改变整个句子的语义吧。这样继续作为正样本似乎有点问题。 simCSE 无监督的对比学习： 构造正样本：two different dropout 负样本： mini-batch中的其他样本 有监督的对比学习：利用了NLI的数据集，entailment pairs作为正样本对，contradiction pairs 作为负样本对。 R-Drop同一组数据，经过两次网络。然后使得 $p(y|x, drop1), p(y|x, drop2)$ 两者的 kl 散度最小。 CosRegpaper: Improving neural language generation with spectrum control. 定义了representation degeneration problem. 生成模型预训练后的模型，不同的词的语言会集中到一个很小的空间内，因而降低了语义表示能力。 作者分析会产生这种现象的原因是，一个词的频率相比整个词表是很有限的。训练的过程中，当这个词作为label时，我们会增大其对应的最大似然，但同时，也在其他的cls-loss中我们也会降低这个词的似然。因此，所有的词都会以降低起其似然的方向推向negative方向。这就会导致上面说的问题。 作者将 word/category embedding 可视化得到如上图所示。 因而作者采取的方式增大这个cane的容量。 Then a straightforward approach is to improve the aperture of the cone which is defined as the maximum angle between any two boundaries of the cone. For the ease of optimization, we minimize the cosine similarities between any two word embeddings to increase the expressiveness. u1s1 理论推导没太懂（果然我这辈子都发不了ICLR/ICML… 不过大概意思懂了,就是一个减小词之间相似度的正则化项。 CLASP 作者认为直接利用mini-batch的其他实例作为负样本太简单了，需要很大的batch size，否则学不到有用的信息。因此，作为提出了一种通过在hidden size上增加扰动的方法来构造正负样本。 正样本 Distant-Targets：增加较大的扰动，但是最大化其似然，保证语义与原句子语义一致。 在正梯度方向增加较大的扰动。 负样本 Imposters: 增加较小的扰动，但是最小化其似然，保证生成错误且较难分辨的样本。在负梯度方向增加较小的扰动。","link":"/2021/07/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-constrast-learning-in-NLP/"},{"title":"论文笔记-video transformer","text":"paper list: Training data-efficient image transformers &amp; distillation through attention. An image is worth 16x16 words: Transformers for image recognition at scale. ViViT: A Video Vision Transformer. Is space-time attention all you need for video understanding Video transformer network. Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows What Makes for Hierarchical Vision Transformer? WiderNet Go Wider Instead of Deeper CoAtNet: Marrying Convolution and Attention for All Data Sizes DeiT利用attention机制设计了一种特殊的知识蒸馏机制，能显著提升收敛速度，并一定程度提高准确率。 ViViT: A Video Vision Transformer作者通过大量实验来研究 vision transformer，并探索最合适的且efficient的结构使其能适用于small datasets. 其中，主要包括以下三个方面的探索： tokenization strategies model architecture regularisation methods. transformer-based architecture的缺点：相比卷积网络，transformer缺乏了一定的归纳偏置能力，比如平移不变性。因此，需要大量的数据来学习。 WiderNet CoAtNet 作者从泛化性generalization和模型容量model capacity两个方面来分析conv和attn的区别： convolutional layers tend to have better generalization with faster converging speed thanks to their strong prior of inductive bias attention layers have higher model capacity that can benefit from larger datasets. Motivation how to effectively combine convolution and attention to achieve better trade-offs between accuracy and efficiency. ContributionIn this paper, we investigate two key insights: First, we observe that the commonly used depthwise convolution can be effectively merged into attention layers with simple relative attention; Second, simply stacking convolutional and attention layers, in a proper way, could be surprisingly effective to achieve better generalization and capacity. SOTA performances under comparable resource constraints across different data sizes Model Architecture How to combine the convolution and self-attention within one basic computational block? How to vertically stack different types of computational blocks together to form a complete network? Merging convolution and self-attention convolution relies on a fixed kernel to gather information from a local receptive field self-attention allows the receptive field to be the entire spatial locations and computes the weights based on the re-normalized pairwise similarity between the pair $(x_i, x_j)$ the good properties of conv and attn: the conv kernel is input-independent, but the attention weight dynamically depends on the input, it is much easier for the self-attention to capture complicated relational interactions between different spatial positions. translation equivalence of the conv is lacked in attn. the size of the receptive field is different between conv and attn. Combine the desirable properties​ $y_i^{pre}$ corresponds to a particular variant of relative self-attention. （pre就是相对位置attention的变种啊！这么解释，很神奇！） Vertical Layout Designto overcome quadratic complexity of self-attention, there three options: (A) down-sampling to reduce the spatial size and employ the global relative attention after the feature map reaches manageable level. (B) Enforce local attention (C) Replace the quadratic Softmax attention with certain linear attention variant which only has a linear complexity w.r.t. the spatial size the authors experiment with C without getting reasonable good results. B requires many shape formatting operation which are slow on TPU. THus they select the option A. Filve variants: ViT$_{rel}$ , CTTT, CCTT, CCCT, CCCC. From the ImageNet-1K results, in terms of generalization capability: As for model capacity, from the JFT comparison: ​ To decide the CCTT and CTTT, they conduct another transferability test","link":"/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/"},{"title":"论文笔记-video generation","text":"GenHFi: Generating high fidelity images with subscale pixel networks and multidimensional upscaling. (ICLR2019) paper2: Scaling autoregressive video models (ICLR2020) Video pixel networks. (CoRR2016) Parallel: Parallel multiscale autoregressive density estimation VQGAN: Taming Transformers for High-Resolution Image Synthesis TeCoGAN: Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation ImaGINator: Conditional Spatio-Temporal GAN for Video Generation Temporal Shift GAN for Large Scale Video Generation MoCoGAN: Decomposing Motion and Content for Video Generation Playable Video Generation (CVPR2021) GenHFi title: Generating high fidelity images with subscale pixel networks and multidimensional upscaling Abstract: Subscale Pixel Network (SPN): a conditional decoder architecture that generates an image as a sequence of sub-images of equal size Multidimensional Upscaling: grow an image in both size and depth via intermediate stages utilising distinct SPNs Introduction The multi-facted relationship between MLE scores and the fidelity of samples MLE is a well-defined measure as improvements in held-out scores generally produce improvements in the visual fidelity of the samples. MLE forces the model to support the entire empirical distribution. This guarantees the model’s ability to generalize at the cost of allotting capacity to parts of the distribution that are irrelevant to fidelity. A 256 × 256 × 3 image has a total of 196,608 positions that need to be architecturally connected in order to learn dependencies among them. Contribution Multidimensional Upscaling Small size, lower depth -&gt; large size, lower depth -&gt; large size, high depth Subscale Pixel Network (SPN) architecture divides an image of size $N\\times N$ into sub-images of size $\\dfrac{N}{S}\\times \\dfrac{N}{S}$ sliced out at interleaving positions SPN consists of two networks, a conditioning network that embeds previous slices and a decoder proper that predicts a single target slice given the context embedding. Architecture VQ-GAN Contribution take the convolution and transformer together: use a convolutional approach to efficiently learn a codebook of context-rich visual parts and, subsequently, learn a model of their global compositions. The long-range interactions within these compositions require an expressive transformer architecture to model distributions over their consituent visual parts. utilize an adversarial approach to ensure that the dictionary of local parts captures perceptually important local structure to alleviate the need for modeling low-level statistics with the transformer architecture. Approach Learning an Effective Codebook of Image Constituents VA-VAE: $$\\hat x = G(z_q) = G(q(E(x)))$$ Learning a Perceptually Rich Codebook using GAN and perception loss Learning the Composition of Images with Transformers latent transformer Conditioned Synthesis TeCoGAN paper: Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation Contribution propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. Related works Sequential generation tasks: [Kim et al. 2019; Xie et al. 2018]. Conditional video generation tasks [Jamriška et al. 2019; Sitzmann et al. 2018; Wronski et al. 2019; Zhang et al. 2019] motion estimation [Dosovitskiy et al. 2015; Liu et al. 2019] explicitly using variants of optical flow networks [Caballero et al. 2017; Sajjadi et al. 2018; Shi et al. 2016] GANS: Zhu et al. [2017] focuses on images without temporal constrains RecycleGAN [Bansal et al. 2018] proposes to use a prediction network in addition to a generator a concurrent work [Chen et al. 2019] chose to learn motion translation in addition to the spatial content translation. Metrics: perceptual metrics [Prashnani et al. 2018; Zhang et al. 2018] are proposed to reliably consider semantic features instead of pixel-wise errors. Adversarial temporal losses: tempoGAN for fluid flow [Xie et al. 2018] vid2vid for video translation [Wang et al. 2018a] 3D discriminator DeepFovea [Kaplanyan et al. 2019] Bashkirova et al. [2018] For tracking and optical flow estimation, L2-based time-cycle losses [Wang et al. 2019b] MethodSpatio-Temporal Adversarial Learning Notation: $\\alpha$ Input domain, $b$ target domain, $g$ generated domain $w$ motion compensation, Self-Supervision for Long-term Temporal Consistency When inferring this in a frame-recurrent manner, the generated result should not strengthen any invalid features from frame to frame. Rather, the result should stay close to valid information and be symmetric, i.e., the forward result $g_t=G(a_t, {t-1})$ and the one generated from the reversed part, ${g_t}^{‘}=G(a_t, {g{t+1}}^{‘})$, should be identical. bi-directional “Ping-Pong” loss ​ $$\\mathcal{L}{pp}=\\sum{t=1}^{n-1}||g_t-{g_t}^{‘}||_2$$ Parallel multiscale","link":"/2021/09/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-generation/"},{"title":"论文笔记-对话系统","text":"paperA Survey on Dialogue Systems: Recent Advances and New Frontiers, Chen et al. 2018 motivation这是一篇关于对话系统的综述。 对话系统主要分为两大类： 任务导向型（task-oriented) 对话系统 非任务导向型（non-task-oriented）对话系统 序列到序列模型 sequence-to-sequence models 检索式模型 retrieval-based methods task-oriented dialogue system面向任务的系统旨在帮助用户完成实际具体的任务，例如帮助用户找寻商品，预订酒店餐厅等。 有两种方式： pipeline methods end-to-end methods pipeline methods 其流程是4个步骤： language understanding dialogue state tracking policy learning natural language generation language understanding第一步是 utterance 理解。将给定的 utterance 映射成对应的语义槽 (semantic slots). Given an utterance, natural language understanding maps it into semantic slots. The slots are pre-defined according to different scenarios. slots 都是根据特定的场景定义好的。 看表格能发现，包含三个任务： intent dection: 这个是 utterance-level classification，也就是一个分类任务 domain classification: 也是分类任务 slot filling: 这是 word-level 的任务，可以定义成序列标注问题，输入是一个 utterance，输出是对应每个 word 的 semantic label. 关于 slot filling 的 paper: CRF baseline DBNs: Deep belief network based semantic taggers for spoken language understanding Use of kernel deep convex networks and end-to-end learning for spoken language understanding RNN: Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding, 2013 Deep belief nets for natural language call-routing. 2011 Recurrent neural networks for language understanding, 2013 Spoken language understanding using long short-term memory neural networks. 2014 Dialogue State Tracking对话的状态跟踪，预测每一轮对话 user 的 goal. 对话状态跟踪是确保对话系统健壮性的核心组件。它在对话的每一轮次对用户的目标进行预估，管理每个回合的输入和对话历史，输出当前对话状态。这种典型的状态结构通常称为槽填充或语义框架。 所以对应的 state 是根据场景预定义好了的嘛？比如 online shopping，对应的 state 可能就有 推荐，比较，下单等等？ 基于传统方法的有很多，基于 deep learning 的有： [26] Deep neural network approach for the dialog state tracking challenge. 2013 [58] Multi-domain dialog state tracking using recurrent neural networks. 2015 [59] Neural belief tracker: Data-driven dialogue state tracking, 2017 Policy learning根据上一步得到的 state，来制定下一步的 action. 很符合强化学习的理念啊，不过需要解决 热启动 (warm-start) 的问题。 基于规则的监督学习（state 的状态需要规则来定义）: [111] Building task-oriented dialogue systems for online shopping, deep reinforcement learning: [14] Strategic dialogue management via deep reinforcement learning, 2015 基于强化学习的方法已经超过监督学习了。 natural language generation一个好的生成器通常依赖于几个因素:适当性、流畅性、可读性和变化性。传统的NLG方法通常是执行句子计划。它将输入语义符号映射到代表话语的中介形式，如树状或模板结构，然后通过表面实现将中间结构转换为最终响应。深度学习比较成熟的方法是基于LSTM的encoder-decoder形式，将问题信息、语义槽值和对话行为类型结合起来生成正确的答案。同时利用了注意力机制来处理对解码器当前解码状态的关键信息，根据不同的行为类型生成不同的回复。 [123] Context-aware nat- ural language generation for spoken dialogue systems. Zhou et al, 2016 COLING adopted an encoder-decoder LSTM-based structure to incorporate the question information, semantic slot values, and dialogue act type to generate correct answers. It used the attention mechanism to attend to the key information conditioned on the current decoding state of the decoder. Encoding the di- alogue act type embedding, the neural network-based model is able to generate variant answers in response to different act types. end-to-end model传统的 pipeline 的方法的缺点： user 的反馈很难传递到每一个 module 每一个 module 都是相互依赖的 (process interde- pendence) 也就是在不同的 domain 或者 scenarios 时，pipeline 设计的对话系统可能就不使用的，因为 slots 和 features 都是 task-specificed，都会相应的改变。而这些过程都需要大量的人工工程。 因此我们需要 end-to-end model。与传统的 pipeline 模型不同，端到端模型使用一个模块，并与结构化的外部数据库交互。 network-based end-to-end 模型需要大量的标注数据 Learning end-to-end goal-oriented dialog, Bordes et al, 2017 ICLR A network-based end-to-end trainable task-oriented di- alogue system, 2017 ACL end-to-end reinforcement learning 在对话管理中，联合训练 state tracking 和 policy learning， 从而使得模型鲁棒性更强。 Towards end-to-end learn- ing for dialog state tracking and management us- ing deep reinforcement learning，2016 ACL task-completion neural dialogue system, 其目标就是完成一个任务。 End-to-end task- completion neural dialogue systems，2017 以任务为导向的对话系统通常还需要查询外部知识库。传统的采用的方法就是通过 semantic parsing 形成一个 query，然后去匹配外部知识库，通过检索得到想要的 entries. 其缺点是： 检索的结果不包含有关语义分析中的不确定性信息 检索的过程是不可微的 (non-differentiabl), 因此 semantic parsing 和 dialogue policy 只能分别训练，导致 online end-to-end 的模型很难部署。 解决这个问题的 paper: Key-value retrieval networks for task-oriented dialogue, 2017 augmented existing recurrent network architectures with a differentiable attention-based key-value retrieval mechanism over the entries of a knowledge base, which is inspired by key-value memory networks. Towards end-to-end reinforcement learning of dialogue agents for information ac- cess, 2017 ACL replaced symbolic queries with an induced “soft” posterior distribution over the knowledge base that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. 2017 ACL combined an RNNwith domain-specific knowledge encoded as software and system action templates. Non-task-oriented dialogue system非任务导向型对话系统也就是聊天机器人， 是通过生成模型或基于检索的方法实现的。 生成模型能够生成更合适的回复（也就是跟上下文语义更接近），而这些回复可能从来没有出现在语料库中，而基于检索的模型则能得到具有信息充裕 (informative) 和 fluent 的回复。 Neural Generative models深度学习在机器翻译中的成功应用，即神经机器翻译，激发了人们对神经生成对话研究的热情。 最开始也有一篇 paper，将对话当最机器翻译来做的 paper. 把对话看作是将 post 翻译成 response。但是区别在于 response 的范围很广，而且 post 和 response 并不像翻译的两个句子之间存在对齐关系。 目前神经生成模型的热门研究课题，主要是讨论： response diversity modeling topics and personalities leveraging outside knowledge base the interactive learning evaluation Sequence-to-Sequence Models 这个就是基本的 seq2seq 模型。好奇的是，如何解决多轮对话，如何结合 history 信息，如何控制对话的状态，这些都需要深入看 paper 吧。 Dialogue Context考虑历史对话的历史信息的能力是建立可保持对话活跃的对话系统的关键。 基于 RNN language model 的形式： A neural network approach to context-sensitive generation of conversational responses, ACL 2015 通过连续的表示或单词和短语的嵌入来表示整个对话历史（包括当前消息），类似于 RNN language model 的 decoder 过程，这样就能保证前后生成的 response 是有关系的，后者是依赖于前者。[12] 也是这么干的。 使用 hierarchical model: [68] 先分别对 individual utterance 进行建模，然后将他们整合在一起。Hierarchical recurrent attention network for response generation, 2017 引入了 attention mechanism, 从而 focus 最相关或者是最重要的部分 word-level or utterance-level. How to make context more useful? an empirical study on context-aware neural conversational models., ACL 2017 对有无层次结构的模型进行了对比，证明有层次结构的模型效果更好。 Response Diversity A challenging problem in current sequence-to-sequence dialogue systems is that they tend to generate trivial or non-committal, universally relevant responses with little meaning, which are often involving high frequency phrases along the lines of I dont know or Im OK. 在当前的序列对话系统中，一个具有挑战性的问题是，它们倾向于产生意义不大的普通或不重要的、普适的回答，而这些回答往往涉及到“我不知道”或者“我很好”这样的高频率短语。 MMI and IDF 模型的这种行为可以归咎于模型赋予了 “safe” response 更高的概率。A diversity-promoting objective function for neural con- versation models. ACL 2016 使用了 Maximum Mutual Information 作为优化目标,这是最初在语音识别领域引入的。 它测量了输入和输出之间的相互依赖关系，并考虑了消息回复的逆向依赖性。 An attentional neural conversation model with improved speci- ficit, 2016 结合逆文档频率（IDF）到训练过程来评价回复的多样性。（在不同的 document 中出现的回复次数越多，其相应的权重越低）。 beam-search 一些研究表明，解码的过程也是回复冗余的另一个缘由。[86][72][42] 发现 beam-search 在生成候选答案时中缺乏多样性。[86] 提出了一种衡量不同回复之间的相似度的方法，类似于正则惩罚项吧，来增强 beam-search 的目标函数。[72] 提出了一种随机 beam-search 的方法，[42] 则使用了一个惩罚项来惩罚来自同一父节点中的子节点的展开。 re-ranking [38][77][72] 结合全局特征，重新执行 re-ranking 的步骤，从而避免生成 dull or generic 的回复。 PMI [57] 猜测问题不仅仅在于解码和 respones 的频率，而且消息本身也缺乏足够的信息。 它提出使用逐点互信息（PMI）来预测名词作为关键词，反映答复的主要依据，然后生成一个包含给定关键字的答复. latent variable 另一系列工作着重于通过引入随机隐变量来产生更多不同的输出。 他们表明，自然对话不是确定性的 —— 对同一信息的答复可能会因人而异。 但是，当前回复是从确定性 encoder-decoder 模型中采样的。 通过整合隐变量，这些模型的优点是，在生成时，他们可以通过首先对隐变量的分配进行采样，然后确定性地进行解码，从分布中采样回复。 A hierarchical latent vari- able encoder-decoder model for generating dialogues, AAAI 2019 将因变量引入到 hierachical dialogue model framework，The latent variable is designed to make high-level decisions like topic or sentiment. A conditional variational framework for dialog generation. ACL 2017 conditioned the latent variable on explicit attributes to make the latent variable more interpretable. These attributes can be either manually assigned or automatically detected such topics, and personality. Topic and Personality明确对话的内在属性是提高对话多样性和保证一致性的另一种方法。在不同的属性中，主题和个性被广泛地进行研究探讨。 Topic aware neural response generation, AAAI 2017 注意到人们经常把他们的对话与主题相关的概念联系起来，并根据这些概念做出他们的回复。他们使用Twitter LDA模型来获取输入的主题，将主题信息和输入表示输入到一个联合注意模块中，并生成与主题相关的响应。 Multiresolution recurrent neural networks: An application to dialogue response generation. AAAI 2017 对粗粒度的 tokens sequence 和 dialogue generation 进行联合建模，粗粒度的 tokens 主要是用来探索 high-level 的语义信息，通常是 name entity 或 nouns. Emotional chatting machine: Emotional conversation generation with internal and external memory 将情感 embedding 融入到了对话生成中。Affective neural response generation, 2017 通过三种方式增强回复的情感： incorporating cognitive engineered affective word embeddings augmenting the loss objective with an affect-constrained objective function injecting affective dissimilarity in diverse beam-search inference procedure Assigning personality/identity to a chatting machine for coherent conversation generation 让对话个性化，并且保持一致性。Neural per- sonalized response generation as domain adaptation 提出了一种两阶段的训练方法，使用大规模数据对模型进行初始化，然后对模型进行微调，生成个性化响应。 Personalizing a dialogue system with transfer reinforcement learning 使用强化学习来消除对话的前后不一致性。 Outside Knowledge Base人类对话与对话系统之间的一个重要区别是它是否与现实相结合。结合外部知识库(KB)是一种很有前途的方法，可以弥补背景知识之间的差距，即对话系统和人之间的差距。记忆网络（Memory Network）是一种以知识库处理问题的经典方法。因此，它非常直接的别用于在对话生成中。实际研究表明，所提出的模型能够通过参考知识库中的事实来生成对问题的自然和正确答案。 A knowledge-grounded neural conversation model, 2017 A neural network approach for knowledge-driven response generation. COLING 2016 Neural generative question answering,IJCAI 2016 Interactive Dialogue learning通过交互来学习是对话系统的最终目标之一。Deep reinforcement learning for dialogue generation, ACL 2016 利用两个虚拟智能体模拟对话。它们定义了对描述一个较好的对话的汇报的一个简单的启发式的估计：好的对话是有前瞻性[1]或者交互式的（当前轮为下一轮对话铺垫），是信息丰富的和连贯的。一个RNN的编码器-解码器所有参数定义了一个在无穷大的动作空间上从所有可能的话语中进行选择的策略。智能体是通过策略梯度方法 Simple statistical gradient-following al- gorithms for connectionist reinforcement learning, 1992 来优化由开发者定义的长期奖励，而不是通过标准seq2seq的MLE目标函数来学习策略。[32]进一步试图提高机器人从交互中学习的能力。通过对文本和数字反馈使用策略学习和前向预测，该模型可以通过（半）在线方式与人进行交互来提高自身性能。 由于大多数人类在对答案并不自信时通常会要求提供一些澄清或者提示，所有机器人拥有这种能力也是相当自然的。Learning through dialogue interactions by asking questions. 2017 定义了机器人在回答问题时遇到困难时的三种情况。与不采用提问的实验结果相比，这种方法在一些情况下有了很大的改进。Deal or no deal? end-to-end learning of negotiation dialogues, ACL 2017 在谈判任务中进行了探索。由于传统的序列到序列模型模拟人类的对话没有优化具体的目标，这项工作采取了面向目标的训练和解码方法，并展示了一个有价值的视角。 Evaluation评价生成回复的质量是对话系统的一个重要方面。任务导向型的对话系统可以基于人工生成的监督信号进行评估，例如任务完成测试或用户满意度评分等，然而，由于高回复的多样性，自动评估非任务导向的对话系统所产生的响应的质量仍然是一个悬而未决的问题。目前的方法有以下几种： BLEU, METEOR, and ROUGE 值，也就是直接计算 word overlap、ground truth和你生成的回复。由于一句话可能存在多种回复，因此从某些方面来看，BLEU 可能不太适用于对话评测。 计算 embedding的距离，这类方法分三种情况：直接相加求平均、先取绝对值再求平均和贪婪匹配。 进行图灵测试，用 retrieval 的 discriminator 来评价回复生成。 Retrieval-based Methods基于检索的方法从候选回复中选择回复。检索方法的关键是消息-回复匹配，匹配算法必须克服消息和回复之间的语义鸿沟。 single-turn response match$$match(x,y)=x^TAy$$ Convolutional neu- ral network architectures for matching natural lan- guage sentences, 2014 利用深度卷积神经网络体系结构改进模型，学习消息和响应的表示，或直接学习两个句子的相互作用表示，然后用多层感知器来计算匹配的分数。 multi-turn response The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems, ACL 2015 encoded the context (a concatenation of all previous utterances and current message) and candidate response into a context vector and a response vector through a RNN/LSTM based structure, respectively, and then computed the matching degree score based on those two vectors. Learning to respond with deep neural networks for retrieval-based human- computer conversation system, ACM 2016 selected the previous utterances in different strategies and combined them with current messages to form a reformulated context. Multi-view response selection for human-computer conversation. ACL 2016 performed context-response matching on not only the general word level context vector but also the utterance level context vector. Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots. ACL 2017 further improved the leveraging of ut- terances relationship and contextual information by match- ing a response with each utterance in the context on multi- ple levels of granularity with a convolutional neural network, and then accumulated the vectors in a chronological order through a recurrent neural network to model relationships among utterances Hybrid Methods将生成和检索方法结合起来能对系统性能起到显著的提升作用。基于检索的系统通常给出精确但是较为生硬的答案，而基于生成的系统则倾向于给出流畅但却是毫无意义的回答。在集成模型中，被抽取的候选对象和原始消息一起被输入到基于RNN的回复生成器中。这种方法结合了检索和生成模型的优点，这在性能上具备很大的优势。 Two are better than one: An ensemble of retrieval- and generation-based dialog systems, 2016 Alime chat: A sequence to sequence and rerank based chatbot engine. ACL 2017 A deep reinforcement learning chatbot, 2017 展望端到端的框架不仅在非面向任务的聊天对话系统中流行，而且在面向任务的对话系统中逐步流行起来。深度学习能够利用大量的数据，从而模糊了任务导向型对话系统和非任务导向型对话系统之间的界限。值得注意的是，目前的端到端模型仍然远非完美。尽管取得了上述成就，但这些问题仍然具有挑战性。接下来，我们将讨论一些可能的研究方向。 Swift Warm-Up，在一些新的领域，特定领域对话数据的收集和对话系统的构建是比较困难的。未来的趋势是对话模型有能力从与人的交互中主动去学习。 Deep Understanding. 深度理解。现阶段基于神经网络的对话系统极大地依赖于大量标注好的数据，结构化的知识库以及对话语料数据。在某种意义上产生的回复仍然缺乏多样性，有时并没有太多的意义，因此对话系统必须能够更加有效地深度理解语言和真实世界。 Privacy Protection. 目前广泛应用的对话系统服务于越来越多的人。很有必要注意到的事实是我们使用的是同一个对话助手。通过互动、理解和推理的学习能力，对话助手可以无意中隐蔽地存储一些较为敏感的信息。因此，在构建更好的对话机制时，保护用户的隐私是非常重要的。","link":"/2019/03/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"},{"title":"论文笔记-BERT","text":"BERT(Bidirectional Encoder Representations from Transformers.)对于 BERT 重点在于理解 Bidirectional 和 masked language model. Why Bidirectional?对于预训练的表示，单向语言模型因为无法融合下文的信息，其能力是非常有限的，尤其是对类似于 SQuAD 这样需要结合上下文信息的任务。 对比 OpenAI GPT 和 BERT. 为什么 OpenAI GPT 不能采用双向 self-attention 呢？ 传统的语言模型的定义，计算句子的概率： $$P(S)=p(w_1,w_2, …, w_n)=p(w1)p(w_2|w_1)…p(w_n|w_1…w_{n-1})=\\prod_{i=1}^m p(w_i|w_1…w_{i-1})$$ 前向 RNN 语言模型： $$P(S)=\\prod_{i=1}^m p(w_i|w_1…w_{i-1})$$ 也就是当前词的概率只依赖前面出现词的概率。 后向 RNN 语言模型 $$P(S)=\\prod_{i=1}^m p(w_i|w_{i+1}…w_{m})$$ 也就是当前词的概率只依赖后面出现的词的概率。 ELMo 就是这样的双向语言模型(BiLM) 但是 RNN 相比 self-attention 对上下文信息 (contextual information)的利用相对有限，而且 ELMo 只能是一层双向，并不能使用多层。其原因和 GPT 无法使用 双向 编码的原因一样。 对于 GPT 如果它使用双向，那么模型就能准确的学到到句子中的下一个词是什么，并能 100% 的预测出下一个词。比如 “I love to work on NLP.” 在预测 love 的下一个词时，模型能看到 to，所以能很快的通过迭代学习到 “to” 100% 就是 love 的下一个词。所以，这导致模型并不能学到想要的东西（句法、语义信息）。 那么 BERT 是怎么处理双向这个问题的呢？ 它改变了训练语言模型的任务形式。提出了两种方式 “masked language model” and “next sentence generation”. 再介绍这两种训练方式之前，先说明下输入形式。 Input representation position embedding: 跟 Transformer 类似 sentence embedding, 同一个句子的词的表示一样，都是 $E_A$ 或 $E_B$. 用来表示不同的句子具有不同的含义 对于 [Question, Answer] 这样的 sentence-pairs 的任务，在句子末尾加上 [SEP]. 对于文本分类这样的 single-sentence 的任务，只需要加上 [CLS], 并且 sentence embedding 只有 $E_A$. masked language model何为 “masked LM”? idea 来源于 closed tasked. 原本的语言模型是预测所有语料中的下一个词，而 MLM 是在所有的 tokens 中随机选取 15% 的进行 mask，然后只需要预测被 mask 的词。这样以来，就能训练双向语言模型了。 但是存在一个问题，这样 pre-training 训练出来的语言模型并不能拿去做 fine-tune. 原因是在 fine-token 中从来没有见过 &lt;MASK&gt; 这个词。作者采用这样的策略： 具体的操作，以 “My dog is hairy” 为例，mask “hairy” 这个词： “My dog is &lt;MASK&gt;“. 80% 被 代替 “My dog is apple”. 10% 被一个随机的 token 代替 “My dog is hairy”. 10% 保持原来的样子 为什么不用 &lt;MASK&gt; 代替所有的 token？ If the model had been trained on only predicting ‘&lt;MASK&gt;’ tokens and then never saw this token during fine-tuning, it would have thought that there was no need to predict anything and this would have hampered performance. Furthermore, the model would have only learned a contextual representation of the ‘&lt;MASK&gt;’ token and this would have made it learn slowly (since only 15% of the input tokens are masked). By sometimes asking it to predict a word in a position that did not have a ‘&lt;MASK&gt;’ token, the model needed to learn a contextual representation of all the words in the input sentence, just in case it was asked to predict them afterwards. 如果模型在预训练的时候仅仅只预测 &lt;MASK&gt;, 然后在 fine-tune 的时候从未见过 &lt;MASK&gt; 这个词，那么模型就不需要预测任何词，在 fine-tune 时会影响性能。 更严重的是，如果仅仅预测 &lt;MASK&gt;, 那么模型只需要学习 &lt;MASK&gt; 的上下文表示，这会导致它学习的很慢。 如果让模型在某个位置去预测一个不是 &lt;MASK&gt; 的词，那么模型就需要学习所有 tokens 的上下文表示，因为万一需要预测这个词呢。 只需要 random tokens 足够吗？为什么还需要 10% 的完整的 sentence? Well, ideally we want the model’s representation of the masked token to be better than random. By sometimes keeping the sentence intact (while still asking the model to predict the chosen token) the authors biased the model to learn a meaningful representation of the masked tokens. 使得模型具有偏置，更倾向于获得有意义的 masked token. 在知乎上问了这个问题，大佬的回复跟这篇 blog 有点差异，但实际上意思是一样的： 总结下： 为什么不能完全只有 &lt;MASK&gt; ? 如果只有 &lt;MASK&gt;, 那么这个预训练模型是有偏置的，也就是学到一种方式，用上下文去预测一个词。这导致在 fine-tune 时，会丢一部分信息，也就是知乎大佬第一部分所说的。 所以加上 random 和 ture token 是让模型知道，每个词都是有意义的，除了上下文信息，还要用到它本身的信息，即使是 &lt;MASK&gt;. 也就是知乎上说的，提取这两方面的信息。 再回过头，从语言模型的角度来看，依然是需要预测每一个词，但是绝大多数词它的 cross entropy loss 会很小，而主要去优化得到 &lt;MASK&gt; 对应的词。而 random/true token 告诉模型，你需要提防每一个词，他们也需要好好预测，因为他们不一定就是对的。 感谢知乎大佬！ random tokens 会 confuse 模型吗？不会， random tokens 只占 15% * 10% = 1.5%. 这不会影响模型的性能。 还有一个问题， &lt;MASK&gt; 所占的比例很小，主要优化对象迭代一次对整个模型影响会很小，因而需要更多次迭代. next sentence generation对于下游是 Question Answering(QA), Natural Language Inference(NLI) 这样需要理解句子之间的相关性的任务，仅仅通过语言模型并不能获得这方面的信息。为了让模型能够理解句子之间的关系，作者提出了一个 binarized next sentence prediction. 具体方式是： 50% 是正确的相邻的句子。 50% 是随机选取的一个句子。这个任务在预训练中能达到 97%-98% 的准确率，并且能很显著的提高 QA NLI 的任务。 pre-training procudure作者预训练使用的语料：BooksCorpus (800M words)，English Wikipedia (2,500M words)。 使用文档级别的语料很关键，而不是 shffule 的句子级别的语料，这样可以获得更长的 sentence. 获得训练样本：从预料库中抽取句子对，其中 50% 的两个句子之间是确实相邻的，50% 的第二个句子是随机抽取的。具体操作看代码吧 batch_size 256. 每一个 sentences 对： 512 tokens 40 epochs Adam lr=1e-4, $\\beta_1=0.9$, $\\beta_2=0.999$, L2 weight decay 0.01 learning rate warmup 10000 steps 0.1 dropout gelu instead of relu Fine-tune proceduresequence-level tasks 比如 sentences pairs 的 Quora Question Pairs(QQP) 预测两个句子之间语义是否相同。如下图中（a）. 如果是 single sentence classification 比如 Stanford Sentiment Treebank（SST-2）和 Corpus of Linguistic Acceptability（CoLA）这种分类问题。如下图（b） 只需要输出 Transformer 最后一层的隐藏状态中的第一个 token，也就是 [CLS]. 然后接上一个全链接映射到相应的 label 空间即可。 fine-tune 时的超参数跟 pre-training 时的参数大致相同。但是训练速度会很快 Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Number of epochs: 3, 4 语料库越大，对参数的敏感度越小。 token-level tasks. 对于token-level classification(例如NER)，取所有token的最后层transformer输出，喂给softmax层做分类。 如何使用 BERT文本分类https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py 主要涉及到两个 类: 数据预处理 预训练模型加载 12345678910111213141516171819from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertConfig, BertAdam， PYTORCH_PRETRAINED_BERT_CACHEtokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)tokenizer = BertTokenizer.from_pretrained(&quot;./pre_trained_models/bert-base-uncased-vocab.txt&quot;)model = BertForSequenceClassification.from_pretrained('bert-base-uncased', cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(args.local_rank), num_labels = num_labels)model = BertForSequenceClassification.from_pretrained(&quot;pre_trained_models/bert-base-uncased.tar.gz&quot;, num_labels=2) 其中 bert-base-uncased 可以分别用具体的 词表文件 和 模型文件 代替。从源代码中提供的链接下载即可。 数据处理123456789from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertConfig, BertAdam， PYTORCH_PRETRAINED_BERT_CACHEtokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)tokenizer = BertTokenizer.from_pretrained(&quot;./pre_trained_models/bert-base-uncased-vocab.txt&quot;) 前一种方式是根据代码中提供的 url 去下载词表文件，然后缓存在默认文件夹下 /home/panxie/.pytorch_pretrained_bert 。后者是直接下载词表文件后，放在本地。相对来说，后者更方便。 这部分代码相对比较简单，根据自己的任务，继承 DataProcessor 这个类即可。 作为模型的输入，features 主要包括三个部分： input_ids 是通过词典映射来的 input_mask 在 fine-tune 阶段，所有的词都是 1, padding 的是 0 segment_ids 在 text_a 中是 0, 在 text_b 中是 1, padding 的是 0 这里对应了前面所说的，input_idx 就是 token embedding, segment_ids 就是 Sentence Embedding. 而 input_mask 则表示哪些位置被 mask 了，在 fine-tune 阶段都是 1. 加载预训练模型123456789!tar -tf pre_trained_models/bert-base-uncased.tar.gz./pytorch_model.bin./bert_config.json 下载好的文件包中含有两个文件，分别是 config 信息，以及模型参数。 如果不用具体的文件，则需要从代码中提供的 url 下载，并缓存在默认文件夹 PYTORCH_PRETRAINED_BERT_CACHE = /home/panxie/.pytorch_pretrained_bert 作为分类任务， num_labels 参数默认为 2. 运行时会发现提取预训练模型会输出如下信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565712/26/2018 17:00:41 - INFO - pytorch_pretrained_bert.modeling - loading archive file pre_trained_models/bert-base-uncased.tar.gz12/26/2018 17:00:41 - INFO - pytorch_pretrained_bert.modeling - extracting archive file pre_trained_models/bert-base-uncased.tar.gz to temp dir /tmp/tmpgm506dcx12/26/2018 17:00:44 - INFO - pytorch_pretrained_bert.modeling - Model config { &quot;attention_probs_dropout_prob&quot;: 0.1, &quot;hidden_act&quot;: &quot;gelu&quot;, &quot;hidden_dropout_prob&quot;: 0.1, &quot;hidden_size&quot;: 768, &quot;initializer_range&quot;: 0.02, &quot;intermediate_size&quot;: 3072, &quot;max_position_embeddings&quot;: 512, &quot;num_attention_heads&quot;: 12, &quot;num_hidden_layers&quot;: 12, &quot;type_vocab_size&quot;: 2, &quot;vocab_size&quot;: 30522}12/26/2018 17:00:45 - INFO - pytorch_pretrained_bert.modeling - Weights of BertForSequenceClassification not initialized from pretrained model:['classifier.weight', 'classifier.bias']12/26/2018 17:00:45 - INFO - pytorch_pretrained_bert.modeling - Weights from pretrained model not used in BertForSequenceClassification:['cls.predictions.bias', 'cls.predictions.transform.dense.weight','cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight','cls.seq_relationship.weight', 'cls.seq_relationship.bias','cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] 不得不去观察 from_pretrained 的源码：https://github.com/huggingface/pytorch-pretrained-BERT/blob/8da280ebbeca5ebd7561fd05af78c65df9161f92/pytorch_pretrained_bert/modeling.py#L448 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455missing_keys = []unexpected_keys = []error_msgs = []# copy state_dict so _load_from_state_dict can modify itmetadata = getattr(state_dict, '_metadata', None)state_dict = state_dict.copy()if metadata is not None: state_dict._metadata = metadatadef load(module, prefix=''): local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {}) module._load_from_state_dict( state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs) for name, child in module._modules.items(): if child is not None: load(child, prefix + name + '.')load(model, prefix='' if hasattr(model, 'bert') else 'bert.')if len(missing_keys) &gt; 0: logger.info(&quot;Weights of {} not initialized from pretrained model: {}&quot;.format( model.__class__.__name__, missing_keys))if len(unexpected_keys) &gt; 0: logger.info(&quot;Weights from pretrained model not used in {}: {}&quot;.format( model.__class__.__name__, unexpected_keys))if tempdir: # Clean up temp dir shutil.rmtree(tempdir)return model 这部分内容解释了如何提取模型的部分参数. missing_keys 这里是没有从预训练模型提取参数的部分，也就是 classifier ['classifier.weight', 'classifier.bias']层，因为这一层是分类任务独有的。 unexpected_keys 则是对于分类任务不需要的，但是在预训练的语言模型中是存在的。查看 BertForMaskedLM 的模型就能看到，cls 层，是专属于语言模型的，在下游任务中都需要去掉。 所以这部分代码实际上学到了如何选择预训练模型的部分参数～～棒啊！","link":"/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"title":"论文笔记-预训练语言模型2-ULMFiT","text":"Motivation对比之前的几种模型concatenate embeddings: ELMo Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness. 这篇 paper 是在 elmo 之后，而 elmo 虽然相对出名，影响力更大，但是 elmo 仍旧只是一种 word embedding 的预训练，在下游任务中还是需要从头训练模型。 ELMo有以下几个步骤： 利用LM任务进行预训练 再利用目标领域的语料对LM模型做微调 最后针对目标任务进行 concatenate embedding，然后训练模型 pretraining LM: In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability. 直接使用在 general-domain 上预训练好的语言模型，然后通过 fine-tune 进行迁移学习， 仍旧需要大量的 in-domain 的文档才能获得比较好的 performance. ULMFiT We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods. 作者认为，预训练语言模型的方式并不是不好，只是训练方法的问题导致了他们表现局限性。想对于 CV， NLP 中的很多任务所需要的语义更浅层。而将 LMs 在小数据集上 fine-tune 时会导致严重的遗忘。 于是，作者提出了 Universal Language Model Fine-tuning(ULMFiT) 通用的语言模型微调 discriminative fine-tuning, slanted triangular learning rates gradual unfreezing Universal Language Model Fine-tuning 主要分为 3 部分： General-domain LM pretraining Target task LM fine-tuning Target task classifier fine-tuning General-domain LM pretrainingWikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words. 在足够大的 general-domain 语料库上进行预训练。 Target task LM fine-tuningdiscriminative fine-tunin在目标语料库 in-domain 上进行 fine-tune. 这部分会收敛的很快，并且在小数据集上依旧会有很好的泛化性。 As different layers capture different types of information (Yosinski et al., 2014), they should be fine-tuned to different extents. 不同的 layer 能捕捉不同程度的信息，于是，作者提出了 discriminative fine-tuning. 不同的 layer 具有不同的 learning rate. L 表示总的 layer 数目。 $${\\theta^1,\\theta^2, …, \\theta^L}$$ $${\\eta^1,\\eta^2, …, \\eta^L}$$ Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. 原本的 SGD 是这样的： $$\\theta_t = \\theta_{t-1}-\\eta\\cdot\\nabla_{\\theta}J(\\theta)$$ 改进之后： $$\\theta_t^l = \\theta_{t-1}^l-\\eta^l\\cdot\\nabla_{\\theta^l}J(\\theta)$$ 作者通过经验发现：先选择最后一层的学习率 $\\eta^L$，然后计算每一层的学习率 $\\eta^{l-1}=\\eta^l/2.6$ Slanted triangular learning rates T 是迭代次数，这里实际上是 $epochs \\times \\text{number of per epoch}$ cut_frac 是增加学习率的迭代步数比例 cut 是学习率增加和减少的临界迭代步数 p 是一个分段函数，分别递增和递减 ratio 表示学习率最小时，与最大学习率的比例。比如 t=0时，p=0, 那么 $\\eta_0=\\dfrac{\\eta_{max}}{ratio}$ 作者通过实验发现，cut_frac=0.1, ratio=32, $\\eta_max=0.01$ Target task classifier fine-tuning针对分类任务，加上 two additional linear blocks. concat pooling gradul unfreezing逐渐 unfreeze layers: We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration. BPTT for Text Classificationbackpropagation through time(BPTT) We divide the document into fixed length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences (Merity et al., 2017a). 什么意思？并不是一个 batch 更新一次梯度，而是累加一定的 batch 之后在更新梯度？ 能增加泛化性？ Bidirectional language model独立的对 forward-LM, backward-LM 进行 fine-tune, 然后平均。 experiment与其他模型对比 ablations “from scratch”: 没有 fine-tune “supervised”: 表示仅仅在 label examples 进行 fine-tune “semi-supervised”: 表示在 unable examples 上也进行了 fine-tune 对 tricks 进行分析 “full” :fine-tuning the full model “discr”: discriminative fine-tuning “stlr”: slanted triangular learning rates","link":"/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/"},{"title":"迁移学习系列 1-Neural Transfer Learning for NLP","text":"迁移学习与监督学习的区别 training domain 和 target domain 不一致时，需要知识迁移。 那么暂时的问题来了？ 1.如何界定 domain 的范围，尤其是NLP领域。从医学文本能迁移到科幻小说吗，感觉不可以。。 2.从 big domain 到 small domain 的迁移也是属于迁移学习的范畴吧？比如像 BERT 这样在超大的训练集上进行 training，然后在小的子集上 fine-tune，都能表现的很好是吗？ Why transfer learning 目前的监督模型依旧非常脆弱，Jia and Liang, EMNLP 2017 这篇 paper 证明了目前的 SOTA 的模型对对抗样本非常敏感。 迁移学习能解决这个问题吗，疑惑？？ Abstract： Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely. Synthetic and Natural Noise Both Break Neural Machine Translation, Belinkov and Bisk (ICLR 2018) 这篇 paper 中提到基于字符级别的翻译模型能有效解决 OOV 等问题，但是却使得模型对 noise 非常敏感且脆弱。如果出现 phonetic 拼写错误，omission 省略， key swap 关键字母交换，都会导致 BLEU 值严重下降。 Abstract Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. Iyyer et al. (NAACL 2018) 这篇 paper 提出了一个句法规则控制下的释义生成模型，syntactically controlled paraphrase networks (SCPNs). 然后发现这样的对抗样本很容易愚弄训练好的监督模型。 Abstract： We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoderdecoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled)paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) “fool” pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data 人工标注所有 domain 或者任何语言的数据是不可理的，因此需要 transfering knowledge from a related setting to the target setting. NLP 很多重大的基础性的研究都可以看作是迁移学习的一种形式。 LSA Brown clusters word embedding 已有工作的局限性： 限制度太高：预设定好的相似度指标，hard 参数共享 条件设定太过于具体：单一的 task baseline 太弱：缺少与传统方法的对比 模型脆弱：在 out-of-domain 不work，依赖于相似的语言/任务 效率低：需要大量参数，时间和样本 研究目标 迁移学习 传导式迁移学习（相同的任务，只有sourced domain有label） 领域自适应（不同的 domain） 跨语言学习（不同的 language） 归纳式迁移学习（不同的任务， target domain 也有标签） 多任务学习 序列迁移学习 大佬太强了。。。。强到爆炸啊 domain adaptationPropose two novel methods that bridge the domain discrepancy by selecting relevant and informative data for unsupervised domain adaptation. 提出两方法，替无监督的域适应选择相关的，具有信息量的数据来弥合域之间的差异。 Based on Bayesian OptimisationLearning to select data for transfer learning with Bayesian Optimization, EMNLP2017 还不太懂 bayesian optimisation: https://zhuanlan.zhihu.com/p/29779000 https://www.jiqizhixin.com/articles/2017-08-18-5 A Tutorial on Bayesian Optimization Using semi-supervised learning and multi-task learningStrong Baselines for Neural Semi-supervised Learning under Domain Shift, Ruder &amp; Plank, ACL 2018 Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state-of-the-art for NLP. Hence classic approaches constitute an important and strong baseline. 大佬的论文真的难。。太 hardcore 了。。 cross-lingual Learning On the Limitations of Unsupervised Bilingual Dictionary Induction A Discriminative Latent-Variable Model for Bilingual Lexicon Induction multi-task learning sequential transfer learning","link":"/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/"},{"title":"迁移学习系列-3-王晋东迁移学习手册阅读","text":"迁移学习的定义迁移学习，是指利用数据、任务、或模型之间的相似性，将在旧领域学习过的模型，应用于新领域的一种学习过程。 综述文章： A survey on transfer learning [Pan and Yang, 2010] 为什么要学习迁移学习？ 缺少数据标注 缺少足够算力 普适化模型与个性化需求之间的矛盾 特定应用需求 迁移学习如何解决这些问题： 大数据与少标注：迁移数据标注 大数据与弱计算：模型迁移 普适化模型与个性化需求：自适应学习 特定应用的需求：相似领域知识迁移（比如cross-lingual） 迁移学习与传统机器学习的区别： 迁移学习与领域自适应的区别： 领域自适应问题是迁移学习的研究内容之一，它侧重于解决特征空间一致、类别空间一致，仅特征分布不一致的问题。而迁移学习也可以解决上述内容不一致的情况。 迁移学习的常用分类 按照目标域标签分类 监督迁移学习 (Supervised Transfer Learning) 半监督迁移学习 (Semi-Supervised Transfer Learning) 无监督迁移学习 (Unsupervised Transfer Learning) 按照学习方法分类 基于实例的迁移学习方法 (Instance based Transfer Learning)： 基于特征的迁移学习方法 (Feature based Transfer Learning) 基于模型的迁移学习方法 (Model based Transfer Learning) 基于关系的迁移学习方法 (Relation based Transfer Learning) 基于实例的迁移，简单来说就是通过权重重用，对源域和目标域的样例进行迁移。就是说直接对不同的样本赋予不同权重，比如说相似的样本，我就给它高权重，这样我就完成了 迁移，非常简单非常非常直接。 基于特征的迁移，就是更进一步对特征进行变换。意思是说，假设源域和目标域的特征 原来不在一个空间，或者说它们在原来那个空间上不相似，那我们就想办法把它们变换到一个空间里面，那这些特征不就相似了？这个思路也非常直接。这个方法是用得非常多的，一 直在研究，目前是感觉是研究最热的。 基于模型的迁移，就是说构建参数共享的模型。这个主要就是在神经网络里面用的特别多，因为神经网络的结构可以直接进行迁移。比如说神经网络最经典的 finetune 就是模型参数迁移的很好的体现。 基于关系的迁移，这个方法用的比较少，这个主要就是说挖掘和利用关系进行类比迁移。比如老师上课、学生听课就可以类比为公司开会的场景。这个就是一种关系的迁移。 按照特征分类 同构迁移学习 (Homogeneous Transfer Learning) 异构迁移学习 (Heterogeneous Transfer Learning) 这也是一种很直观的方式：如果特征语义和维度都相同，那么就是同构；反之，如果特征完全不相同，那么就是异构。举个例子来说，不同图片的迁移，就可以认为是同构；而图片到文本的迁移，则是异构的。 按离线与在线形式分 离线迁移学习 (Offline Transfer Learning) 在线迁移学习 (Online Transfer Learning) 迁移学习的应用 计算机视觉在 CV 领域，迁移学习主要是方法是领域自适应 domain adaption. 侧重于解决特征空间一致、类别空间一致，仅特征分布不一致的问题。 个人理解：在图像上，即使类别差别很大，但是在特征空间上仍然是一致的，或者说有很多相似之处。比如人和狗，从轮廓、颜色、五官等等特征都是具有可迁移性的。再比如人和桌子，也是具有相似特征的。 那么问题是，特征空间完全一致吗？不一致的部分呢？ 文本分类由于文本数据有其领域特殊性，因此，在一个领域上训练的分类器，不能直接拿来作用到另一个领域上。这就需要用到迁移学习。例如，在电影评论文本数据集上训练好的分类器，不能直接用于图书评论的预测。这就需要进行迁移学习。图 11是一个由电子产品评论 迁移到 DVD 评论的迁移学习任务。 那么问题来了，这种从一个domain 迁移到另一个 domain，到底改变了什么？比如同样一个词，在 电子产品中对应的 vector 经过迁移后（finetune?），发生了怎样的变化，这个可以可视化嘛？ 时间序列 行为识别(Activity Recognition) 室内定位 (Indoor Location) 医疗健康不同于其他领域，医疗领域研究的难点问题是，无法获取足够有效的医疗数据。在这一领域，迁移学习同样也变得越来越重要。","link":"/2019/03/08/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-3-%E7%8E%8B%E6%99%8B%E4%B8%9C%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%E9%98%85%E8%AF%BB/"},{"title":"邓公数据结构与算法1-算法分析","text":"算法分析 迭代、递归 动态规划 算法分析 级数的大 O 分析 循环 vs 级数 非极端元素+冒泡排序 算法分析： 通过挖掘算法中的不变性、单调性，进而证明正确性是算法分析的重要技巧。 迭代与递归减而治之 (decrease and conquer)求解一个大规模问题，可以将其划分为两个子问题，一个 naive，一个规模缩减。 算法分析：线性递归，使用递归跟踪，仅使用于简明的递归 example: 颠倒数组任意给定数组 A[0, n), 将其中的子区间 A[lo, hi] 颠倒 1234567891011121314151617void reverse (int* A, int lo, int hi){ if (lo &lt; hi) { swap (A[lo], A[hi]); reverse (A, lo++, hi--) }} 迭代： 123456789101112131415void reverse (int* A, int lo, int hi){ while(lo &lt; hi) { swap(A[lo--], A[hi++]) }} 分而治之 (divide and conquer)求解一个大规模问题，可以将其划分为两个子问题，规模大体相当。 分别求解子问题 由子问题的解，得到原问题的解。 Example： MAX2，二分递归从数组区间 A[lo,hi] 中找出最大的两个整数。 迭代解法 遍历整个数组，分别与 x1, x2 比较。 每迭代一次，比较 1/2 次。O(2n-3). 递归+分治解法 分而治之： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455void max2(int A[], int lo, int hi, int x1, int x2){ // 递归基, 总共只有 3个/4个 元素 if (lo + 2 == hi) {* ... * ; return;} if (lo + 3 == hi) {* ... * ; return;} // 递归过程，分为两组，两边至少2个元素 int mi = (lo + hi)/2; // 递归 int x1L, x2L; max2(A, lo, mi, x1L, x2L); int x1R, x2R; max2(A, mi, hi, x1R, x2R); // 每个递归实例所需操作 if (A[x1L] &gt; A[x1R]) { x1 = x1L; x2 = (A[x2L] &gt; A[x1R]) ? x2L : x1R; } else { x1 = x1R; x2 = (A[x1L] &gt; A[x2R]) ? x1L : x2R; }} 动态规划记忆法：example Fabonacci$$fib(n)=fib(n-1)+fib(n-2)$$ 123456789int fib(n){ return (2 &gt; n) ? n : fib(n-1) + fib(n-2);} 速度很慢很慢！ 因为大量的递归实例被重复的调用。 解决方法： 记忆，通过制表，避免重复调用 动态规划：颠倒计算方向，自顶而下递归，转换为自底而上迭代 这个可以类比上台阶，每一步可以是 1 或 2 级台阶。 那么走到第 n 级台阶的方式是 第 n-1 阶 和 第n-2 阶的方式之和。 123456789101112131415f = 1; g = 0;while( 0 &lt; n-- ){ g = g + f; f = g - f;}return g; 不断的从下而上的更新 g, f. example: LCS (longest common sequence)题目详情（子序列不同于子串，子序列可以是不连续的）： 思考：从递归的角度去想，缩小规模无非是 分而治之 和 减而治之。 将规模缩小，A[0, n], B[0, m], LCS[A, B] 只有三种情况： n=-1 或 m=-1, 则为空 A[n] = B[m] = “X”, 减而治之，同时缩小两个字串 LCS(A[0, n-1], B[0, m-1]) + “X” $A[n] \\ne B[m]$, 分而治之，LCS(A[0, n-1], B[0,m]) 和 LCS(A[0, n],B[0, m-1]) 中的较大值。 分析算法的可行性： 解决这种递归实例反复调用的方法： 将所有子问题列成一张表 颠倒计算方向，从 LCS(A[0], B[0]) 依次计算所有项。 从上图可以很清楚的明白制表的过程，每一个递归实例都可能被反复的经过。 比如： $A[n] \\ne B[m]$. 那么分为两条路径： LCS(A[0, n-1], B[0,m]) 和 LCS(A[0, n],B[0, m-1]). 这两个的计算都需要计算 LCS(A[0, n-1], B[0,m-1]), 所以会造成重复计算。 所以如何制表呢？ 递归公式， 用 $C[i,j]$ 来表示表格中 $[A_i, B_j]$ 处的长度。 具体填表的过程参考： https://blog.csdn.net/hrn1216/article/details/51534607 想清楚表格的物理意义： 可以看作某一个序列固定（列），然后逐渐增加另一个序列的长度（行），也就是逐渐填入行。 初始的情况也要想清楚，某一个序列长度为 0 时，公共序列肯定也为 0。 $m\\times n$ 的表格 初始化， i=0 或 j=0 先填入第一行，一个字符为 “3”，没有与之相同的字符，所有这一行都为 0. 再填入第二行，填 [2,1] 处，都为3，C[2,1]=C[1,1]+1，然后按照递归公式走下去。 第二行其他空格 $A[i]\\ne B[j]$，C[i,j]=max(C[i-1,j], C[i, j-1]) 回溯构造 LCS 我们根据递归公式构建了上表，我们将从最后一个元素c[8][9]倒推出S1和S2的LCS。 C[8, 9] = 5，且$S_1[8] \\ne S_2[9]$，所以倒推回去，C[8,9]的值来源于 C[8,8]的值(因为C[8,8] &gt; c[7,9]) C[8, 8] = 5, 且 $S_1[8] = S_2[8]$, 所以倒推回去，C[8,8]的值来源于 C[7, 7]。 可能会出现分歧的情况，$S_1[i] \\ne S_2[j]$, 且 C[i-1,j]=C[i, j-1]. 这个时候选择一个方向。 两种结果： 分歧处选择另一个方向。 时间复杂度分析： 构建表格需要 O(mn), 回溯输出一个 LCS 需要 O(m+n) leetcode 300. Longest increasing sequenceGiven an unsorted array of integers, find the length of longest increasing subsequence. Example: Input: [10,9,2,5,3,7,101,18] Output: 4 Explanation: The longest increasing subsequence is [2,3,7,101], therefore the length is 4. Note: There may be more than one LIS combination, it is only necessary for you to return the length. Your algorithm should run in O(n2) complexity. Follow up: Could you improve it to O(n log n) time complexity? Brute Force [Time Limit Exceeded]: recursive12345678910111213141516171819202122232425262728293031323334353637int lenofLIS(vector&lt;int&gt; &amp;nums, int prev, int curpos);class Solution {public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) { return lenofLIS(nums, INT_MIN, 0); }};int lenofLIS(vector&lt;int&gt; &amp;nums, int prev, int curpos){ if (curpos == nums.size()) return 0; int taken = 0; if (nums[curpos] &gt; prev) taken = 1 + lenofLIS(nums, nums[curpos], curpos+1); int notaken = lenofLIS(nums, prev, curpos+1); return max(taken, notaken);} 时间复杂度分析： 数组中每一个位置都可能出现或者不出现在 LIS 中，也就是说每一个递归实例的操作是 2，所以最终时间复杂度是 $O(2^n)$ 空间复杂度分析：$O(n^2)$ Recursion with memorization [Memory Limit Exceeded]解决上面这种递归实例反复调用的问题，通常有两种方法，在上面学习中也说到了。这里先采用记忆法，将所有子问题列成一张表。 Dynamic Programming [Accepted]参考： https://www.geeksforgeeks.org/longest-increasing-subsequence-dp-3/ Optimal Substructure: Let arr[0..n-1] be the input array and L(i) be the length of the LIS ending at index i such that arr[i] is the last element of the LIS. Then, L(i) can be recursively written as: L(i) = 1 + max( L(j) ) where 0 &lt; j &lt; i and arr[j] &lt; arr[i]; or L(i) = 1, if no such j exists. To find the LIS for a given array, we need to return max(L(i)) where 0 &lt; i &lt; n. Thus, we see the LIS problem satisfies the optimal substructure property as the main problem can be solved using solutions to subproblems. Following is a simple recursive implementation of the LIS problem. It follows the recursive structure discussed above. 1234567891011121314151617181920212223242526272829303132333435363738394041int lenofLIS(vector&lt;int&gt; &amp;nums, int prev, int curpos);class Solution {public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) { if (nums.size() == 0) return 0; int dp[nums.size()]; dp[0] = 1; for (int i=1; i&lt; nums.size(); i++){ dp[i] = 1; for (int j=0; j &lt; i; j++){ if (nums[i] &gt; nums[j] &amp;&amp; dp[i] &lt; dp[j]+1){ dp[i] = dp[j] + 1; } } } return *max_element(dp, dp+nums.size()); }}; 复杂度分析： Time complexity: $O(n^2)$ Two loops of n. Space complexity: $O(n)$ dp array size n is used. 总结再回过头思考下动态规划的含义： 动态规划常常适用于有重叠子问题[1]和最优子结构性质的问题，动态规划方法所耗时间往往远少于朴素解法。 动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再根据子问题的解以得出原问题的解。 通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量：一旦某个给定子问题的解已经算出，则将其记忆化存储，以便下次需要同一个子问题解之时直接查表。这种做法在重复子问题的数目关于输入的规模呈指数增长时特别有用。 对于 longest increasing subsequence 里面就存在最有子问题。对于一个序列，每增加一个元素，都可以看作一个子问题。 子问题的解是固定的，但是子问题与当前步的结合又是动态变化的。比如这里，当前 i 位置的值大于 j 的值和小于 j 的值的处理方式就不太一样。我们要做的就是找出这个规律 （递推公式），然后根据填写好的子问题的解的表格，进一步扩大问题规模。 所以如前面所说，动态规划：自顶而下递归，自底而上迭代。","link":"/2018/12/19/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%951-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/"},{"title":"论文笔记-无监督机器翻译","text":"Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation王威廉老师组的一篇文章，大致看了下跟最近自己做的研究相关性挺大的。文中也简单的介绍了无监督机器翻译的一些方法，所以借这个机会把无监督机器翻译也好好了解下。记得在三星研究院实习时，有个中科院自动化所的师姐（据说是宗成庆老师的学生）说过一句话，2018年是无监督机器翻译元年。但当时我在搞QA，就没怎么深入研究。感觉很多NLP其他方向的做法都是源于 NMT，所以还是很有必要看一下的。 MotivationBack-translation 得到的伪平行语料，是基于 pure target sentence 得到 pesudo source sentence，然后把 prue target sentence 作为 label 进行监督学习(保证target 端是pure sentence，source端的sentence可以稍微 noisy)。这实质上就是一个 reconstruction loss. 其缺点在于 pesudo source sentence 质量无法保证，会导致误差累积（pesudo source sentence 并没有得到更新，所以并没有纠正存在的错误）。 基于此，作者提出了一种新的范式，extract-edit. related work单语语料的选择neural-based methods aim to select potential parallel sentences from monolingual corpora in the same domain. However, these neural models need to be trained on a large parallel dataset first, which is not applicable to language pairs with limited supervision. 通过平行语料训练翻译模型，进而从单语中选择 domain related sentences. 这并不是完全的无监督，还是需要有限的平行语料得到 NMT 模型之后，去选择合适的单语。 Parallel sentence extraction from comparable corpora with neural network features, LERC 2016 Bilingual word embeddings with bucketed cnn for parallel sentence extraction, ACL 2017 Extracting parallel sentences with bidirectional recurrent neural networks to improve machine translation, COLING 2018 完全的无监督机器翻译The main technical protocol of these approaches can be summarized as three steps: Initialization Language Modeling Back-Translation InitializationGiven the ill-posed nature of the unsupervised NMT task, a suitable initialization method can help model the natural priors over the mapping of two language spaces we expect to reach. 初始化的目的基于自然语言的一些先验知识来对两种语言的映射关系进行建模。 there two main initiazation methods: bilingual dictionary inference 基于双语词典的推理 Word translation without parallel data. Conneau, et al. ICLR 2018 Unsupervised neural machine translation, ICLR 2018 Unsupervised machine translation using monolingual corpora only, ICLR 2018a BPE Phrase-based &amp; neural unsupervised machine translation. emnlp Lample et al. 2018b 本文作者采用的是 Conneau, et al. 中的方式，并且类似于 Lample 2018b 中的方式两种语言共享 bpe(需要在看下相关论文). 这里实际上就是训练得到两种语言的 word embedding，并不是 word2vec 那种对单种语言的无监督，而是训练得到两种语言的 share embedding. language modelingTrain language models on both source and target languages. These models express a data-driven prior about the composition of sentences in each language. 在初始化之后，在 share embedding 的基础上分别对 source 和 target 的语言进行建模。 In NMT, language modeling is accomplished via denosing autoencoding, by minimizing: 本文作者采用的 Lample 2018a 的方式。共享 encoder 和 decoder 的参数？？？ Back-Translation Dual learning for machine translation, NIPS 2016 Improving neural machine translation models with monolingual data. ACL 2016 Extract-Edit Extract: 先根据前两步得到的 sentence 表示，从 target language space 中选择与 source sentence 最接近的 sentence（依据相似度？）. Edit: 然后对选择的 sentence 进行 edit. 作者还提出了一个 comparative translation loss。 Extract因为在 language model 阶段作者已经共享了 encoder 和 decoder，所以在这个场景下对于 two language 的表示，都可以用 encoder 得到。 在 target language space 中选择出与 source sentence 最接近的 top-k extracted sentences. 为什么是 top-k 而不是 top-1 呢，确保召回率，并获得更多更相关的 samples. Edit简单点就是 max-pooling + decode employ a maxpooling layer to reserve the more significant features between the source sentence embedding $e_s$ and the extracted sentence embedding $e_t$ ($t\\in M$), and then decode it into a new sentence $t’$. 具体是怎么操作的呢，这似乎需要看代码。 $e_s$: [es_length, encoder_size] $e_t$: [et_length, encoder_size] 这怎么 max-pooling 呢（句子长度都可能不一样），然后 decode 得到新的 sentence 吧。。 Evaluate虽然 M’ 中可能存在潜在的 parallel sentence 对应 source sentence s. 但是依然不能用 (s, t’) 作为 ground-truth stence pairs 来训练 NMT 模型。因为 NMT 模型对噪声非常敏感。 作者提出了一个 evaluation network R, 实际上就是多层感知机，也许是个两层神经网络吧，具体没说。two labguage 共享 R. $$r_s=f(W_2f(W_1e_s+b_1)+b_2)$$ $$r_t=f(W_2f(W_1e_t’+b_1)+b_2)$$ 假设是这样，也就是将 t’ 转换成 t* 了。 理解错了 其目的是将 s 和 t’ 映射到同一向量空间，然后计算两者的相似度： 接下来将 $\\alpha$ 转换成概率分布。 也就是计算 top-k 个 extracted-edited 得到的 target sentences t* 与 source sentence s 相似的概率，并且这些概率相加为 1. 其中 $\\lambda$ 可以看作是 inverse temperature， $\\lambda$ 越小，表示所有 t* 平等看待，越大，表示更看重 $\\alpha$ 最大的那一句。显然前面的 $\\alpha$ 是通过 cosine 计算的，也就是更看重 k 个 t* 中与 s 距离最近的那个 sentence. learningComparative Translation cosine 相似度越大越接近，所以 -logP 越小越好。这里面涉及到的参数 $\\theta_{enc}, \\theta_R$ Basically, the translation model is trying to minimize the relative distance of the translated sentence t* to the source sentence s compared to the top-k extracted-and-edited sentences in the target language space. Intuitively, we view the top-k extracted-and-edited sentences as the anchor points to locate a probable region in the target language space, and iteratively improve the source-to-target mapping via the comparative learning scheme. Adversarial Objective we can view our translation system as a “generator” that learns to generate a good translation with a higher similarity score than the extracted-and-edited sentences, and the evaluation network R as a “discriminator” that learns to rank the extracted- and-edited sentences (real sentences in the target language space) higher than the translated sentences. 借助于对抗学习的思想，可以把 translation system 看作是 生成器 generator， 用来学习得到 translated target sentence，使得其优于 extracted-and-edited sentences. 把 evalution newtork R 看作是判别器，其目的就是判别 extracted-and-edited sentences 优于 translated target sentences. 因此对于 evaluation network R，有 final adversarial objective Model selection无监督学习因为没有平行语料，所以需要一个指标来表示模型的好坏，也就是翻译质量。 Basically, we choose the hyper-parameters with the maximum expectation of the ranking scores of all translated sentences. Implementation detailsInitializationcross-lingual BPE embedding, set BPE number 60000. 然后用 Fasttext 训练得到 embedding， 512 dimension. 其中 Fasettext 设置 window size 5 and 10 negative samples Model structureall encoder parameters are shared across two languages. Similarly, we share all decoder parameters across two languages. The λ for calculating ranking scores is 0.5. As for the evaluation network R, we use a multilayer perceptron with two hidden layers of size 512.","link":"/2019/04/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E6%97%A0%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91-Extract-and-Eit/"},{"title":"迁移学习系列-0-NLP classification with transfer learning and weak supervision","text":"paper: Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision A Brief Introduction to Weakly Supervised Learning motivation现在的 state-of-the-art 技术都严重依赖于大量的数据，可以说数据是 NLP 应用的瓶颈(bottleneck)。比如，标注医学领域的电子健康记录需要大量的医学专业知识。 随着 transfer learning, multi-task learning 以及 weak supervision 的发展，NLP 可以尝试着去解决这些问题。 这里将介绍如何在没有公开数据集的情况下，使用少量的数据，来构建一个 dectect anti-smitic tweets 分类器。分为以下 3 个步骤： Collect a small number of labeled examples (~600) Use weak supervision to build a training set from many unlabeled examples using weak supervision Use a large pre-trained language model for transfer learning Weak Supervision何为弱监督学习？ paper: A Brief Introduction to Weakly Supervised Learning 翻译版 弱监督学习是一个总括性的术语，它涵盖了试图通过较弱的监督来构建预测模型的各种研究。弱监督通常分为三种类型。 不完全监督(Incomplete Supervision): 只有训练数据集的一个（通常很小的）子集有标签，其它数据则没有标签。 不确切监督(inexact supervision): 只有粗粒度的标签。以图像分类任务为例。我们希望图片中的每个物体都被标注；然而我们只有图片级的标签而没有物体级的标签。 不准确监督(inaccurate supervision)，即给定的标签并不总是真值。 图1:三种弱监督学习的示意图。长方形表示特征向量；红色或蓝色表示标签；“？”表示标注可能是不准确的。中间的子图表示了几种弱监督的混合情形 不完全监督可以形式化为：$D={(x_1,y_1),…,(x_l,y_l),x_{l+1},…, x_m}$ 即有 $l$ 个数据有标签（如 $y_i$ 所示），$u = m-l$ 个数据没有标签。 解决这类问题有两种技术： 主动学习(active learning), 也就是有个专家来标注 unlabeled 数据. 半监督学习(semi-supervision), 有一种特殊的半监督学习，叫 transductive learning(传导式学习)，它与（纯）半监督学习之间的差别在于，对测试数据（训练模型要预测的数据）的假设不同。传导式学习持有“封闭世界”的假设，即测试数据是事先给定的，且目标就是优化模型在测试数据上的性能；换句话说，未标注数据就是测试数据。纯半监督学习持有“开放世界”的假设，即测试数据是未知的，且未标注数据不一定是测试数据。实际中，绝大多数情况都是纯半监督学习。 有人为干预主动学习 active learning. 无人为干预半监督学习[3-5]是指在不询问人类专家的条件下挖掘未标注数据。为什么未标注数据对于构建预测模型也会有用？做一个简单的解释[19]，假设数据来自一个由 n 个高斯分布混合的 高斯混合模型(参考以前的笔记)，也就是说： $$f(x | \\theta) = \\sum_{j=1}^n \\alpha_j f(x | \\theta_j)\\quad\\text{(1)}$$ 其中 $\\alpha_j$ 为混合系数，$\\sum_{j=1}^n \\alpha_j = 1$ 并且 $\\theta = {\\theta_j}$ 是模型参数。在这种情况下，标签 $y_i$ 可以看作一个随机变量，其分布 $P(y_i | x_i, g_i)$ 由混合成分 $g_i$ 和特征向量 $x_i$ 决定。最大化后验概率有： $$h(x) = {argmax}c \\sum{j=1}^n P(y_i = c | g_i = j, x_i) \\times P(g_i = j | x_i)\\quad(2)$$ 其中：$P(g_i = j | x_i) = \\dfrac{\\alpha_j f(x_i | \\theta_j)} {\\sum_{k=1}^n \\alpha_k f(x_i | \\theta_k)}\\quad(3)$ $h(x)$ 可以通过用训练数据估计 $P(y_i = c | g_i = j, x_i)$ 和 $P(g_i = j | x_i)$ 来求得。很明显只有第一项需要需要标签信息。因此，未标注数据可以用来估计提升对第二项的估计，从而提升学习模型的性能。 图中 $+,-$ 表示标注样本。而测试样本 $\\bigcirc$ 正好在两者中间。 图3给出了一个直观的解释。如果我们只能根据唯一的正负样本点来预测，那我们就只能随机猜测，因为测试样本恰好落在了两个标注样本的中间位置；如果我们能够观测到一些未标注数据，例如图中的灰色样本点，我们就能以较高的置信度判定测试样本为正样本。在此处，尽管未标注样本没有明确的标签信息，它们却隐晦地包含了一些数据分布的信息，而这对于预测模型是有用的。 实际上，在半监督学习中有两个基本假设，即聚类假设（cluster assumption）和流形假设（manifold assumption）；两个假设都是关于数据分布的。前者假设数据具有内在的聚类结构，因此，落入同一个聚类的样本类别相同。后者假设数据分布在一个流形上，因此，相近的样本具有相似的预测。两个假设的本质都是相似的数据输入应该有相似的输出，而未标注数据有助于揭示出样本点之间的相似性 半监督学习有四种主要方法，即生成式方法（generative methods），基于图的方法（graph-based methods），低密度分割法（low-density separation methods）以及基于分歧的方法（disagreement methods）。 生成式方法[19，20]假设标注数据和未标注数据都由一个固有的模型生成。因此，未标注数据的标签可以看作是模型参数的缺失，并可以通过EM算法（期望-最大化算法）等方法进行估计[21]。这类方法随着为拟合数据而选用的不同生成模型而有所差别。为了达到好的性能，通常需要相关领域的知识来选择合适的生成模型。也有一些将生成模型和判别模型的优点结合起来的尝试[22]。 基于图的方法构建一个图，其节点对应训练样本，其边对应样本之间的关系（通常是某种相似度或距离），而后依据某些准则将标注信息在图上进行扩散；例如标签可以在最小分割图算法得到的不同子图内传播[23]。很明显，模型的性能取决于图是如何构建的[26-28]。值得注意的是，对于m个样本点，这种方法通畅需要O(m^2)存储空间和O(m^3)计算时间复杂度。因此，这种方法严重受制于问题的规模；而且由于难以在不重建图的情况下增加新的节点，所以这种方法天生难以迁移。 基于分歧的方法[5，32，33]生成多个学习器，并让它们合作来挖掘未标注数据，其中不同学习器之间的分歧是让学习过程持续进行的关键。最为著名的典型方法——联合训练（co-traing），通过从两个不同的特征集合（或视角）训练得到的两个学习器来运作。在每个循环中，每个学习器选择其预测置信度最高的未标注样本，并将其预测作为样本的伪标签来训练另一个学习器。这种方法可以通过学习器集成来得到很大提升[34，35]。值得注意的是，基于分歧的方法提供了一种将半监督学习和主动学习自然地结合在一起的方式：它不仅可以让学习器相互学习，对于两个模型都不太确定或者都很确定但相互矛盾的未标注样本，还可以被选定询问“先知”。 不确切监督不确切监督是指在某种情况下，我们有一些监督信息，但是并不像我们所期望的那样精确。一个典型的情况是我们只有粗粒度的标注信息。例如，在药物活性预测中[40]，目标是建立一个模型学习已知分子的知识，来预测一种新的分子是否能够用于某种特殊药物的制造。一种分子可能有很多低能量的形态，这种分子能否用于制作该药物取决于这种分子是否有一些特殊形态。然而，即使对于已知的分子，人类专家也只知道其是否合格，而并不知道哪种特定形态是决定性的。 形式化表达为，这一任务是学习 $f: X\\rightarrow Y$ ，其训练集为 $D = {(X_1, y_1), …, (X_m, y_m)}$，其中 $X_i = {x_{I, 1}, …, x_{I, m_i}}$, $X_i$ 属于X，且被称为一个包（bag），$x_{i, j}$ 属于 X，是一个样本（j属于 ${1, …, m_i}）$。$m_i$ 是 $X_i$ 中的样本个数，$y_i$ 属于 $Y = {Y, N}$。当存在 $x_{i, p}$ 是正样本时，$X_i$ 就是一个正包（positive bag），其中p是未知的且 p 属于 ${1, …, m_i}$。模型的目标就是预测未知包的标签。这被称为多示例学习（multi-instance learning）[40，41] 多示例学习已经成功应用于多种任务，例如图像分类、检索、注释[48-50]，文本分类[51，52]，垃圾邮件检测[53]，医疗诊断[54]，人脸、目标检测[55，56]，目标类别发现[57]，目标跟踪[58]等等。在这些任务中，我们可以很自然地将一个真实的目标（例如一张图片或一个文本文档）看作一个包；然而，不同于药物活性预测中包里有天然的示例（即分子的不同形态），这里的示例需要生成。一个包生成器明确如何生成示例来组成一个包。通常情况下，从一幅图像中提取的很多小图像块就作为可以这个图像的示例，而章节、段落甚至是句子可以作为一个文本文档的示例。尽管包生成器对于学习效果有重要的影响，但直到最近才出现关于图像包生成器的全面研究[59]；研究表明一些简单的密集取样包生成器要比复杂的生成器性能更好。图5显示了两个简单而有效的图像包生成器。 51.Convex and Scalable Weakly Labeled SVMs [52.Towards making unlabeled data never hurt](http://www.icml-2011.org/papers/548_icmlpaper.pdf) 不准确监督不准确监督关注监督信息不总是真值的情形；换句话说，有些标签信息可能是错误的。其形式化表示与概述结尾部分几乎完全相同，除了训练数据集中的y_i可能是错误的。 一个最近出现的不准确监督的情景发生在众包模式中(crowdsourcing)[74],即一个将工作外包给个人的流行模式。 在带有真值标签的大量训练样本的强监督条件下，监督学习技术已经取得了巨大的成功。然而，在真实的任务中，收集监督信息往往代价高昂，因此探索弱监督学习通常是更好的方式。 Snorkelpaper:Snorkel: Rapid Training Data Creation with Weak Supervision motivationdeep learning 需要大量的标注数据。这对于一些大公司尚且能够雇佣标注人员，而很多小公司则将目标转向弱监督学习。尤其是，当数据的标注需要领域专家时(subject matter experts (SMEs))，标注数据变得更加困难。 弱监督学习包括一下形式： distant supervision: the records of an external knowledge base are heuristically aligned with data points to produce noisy labels [4,7,32] crowsourced labels[37,50] rules and heuristics for labeling data[39,52] Snorkel learns the accuracies of weak supervision sources withoust access to ground truth using a generative model [38]. Furthermore, it also learns correlations and other statistical dependencies among sources, correcting for dependencies in labeling functions that skew the estimated accuracies [5] Snorkel 生成训练数据的方法来自于作者的另外一篇论文：Data programming: Creating large training sets, quickly, NIPS 2016,不仅能给出弱监督得到的样本的置信度，还能学习得到样本之间的相关性和统计依赖。","link":"/2019/02/28/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-0-NLP%20classification%20with%20transfer%20learning%20and%20weak%20supervision/"},{"title":"邓公数据结构与算法2-向量","text":"这章以向量为例，讲解了如何写接口并实现。 接口与实现Abstract Data Type vs. Data Structure 抽象数据类型 = 数据类型 + 定义在该模型上的一些列操作 抽象定义， 操作和定义 不考虑时间复杂度，不涉及数据的存储方式 数据结构 = 基于某种特定的语言，实现 ADT 的一整套算法。 与复杂度密切相关 要考虑数据的具体存储机制 向量向量就是抽象数据类型：从数组到向量，循秩访问。 接下来的部分，通过模板类来实现 向量 ADT 接口 size() get(r ) put(r, e) 用e替换秩为 r 元素的数值 insert(r, e) e作为秩为r元素插入，原后继依次移动 remove(r) 删除秩为 r 的元素，返回该元素原值 disordered() 判断所有元素是否已按非降序排列 sort() 调整各元素的位置，使之按非降序排列 find(e) 查找元素 e search(e) 查找e，返回不大于e且秩最大的元素 仅适用于有序向量 deduplicate(),uniquify() 剔除重复元素 traverse() 遍历向量并统一处理所有元素 向量模板类定义那么如何构造 向量 的模板类呢？ “myvector.h” 头文件，主要是对各种成员数据、成员函数的声明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179#ifndef MYVECTOR_H_INCLUDED#define MYVECTOR_H_INCLUDEDtypedef int Rank; //秩#define DEFAULT_CAPACITY 3 //默认的初始容量（实际应用中可设置为更大）template &lt;typename T&gt; class MyVector{protected: //只能被成员函数和友元访问 Rank _size; // 规模 int _capacity; //容量 T* _elem; // 数据区 _elem 是指针，所以可用数组形式来表示 vector_ void copyFrom(T const* A, Rank lo, Rank hi); //复制数据区间 A[lo, hi) void expand(); //空间不足时扩容 void shrink(); //装填因子过小时压缩 bool bubble ( Rank lo, Rank hi ); //扫描交换 void bubbleSort ( Rank lo, Rank hi ); //起泡排序算法 Rank max ( Rank lo, Rank hi ); //选取最大元素 void selectionSort ( Rank lo, Rank hi ); //选择排序算法 void merge ( Rank lo, Rank mi, Rank hi ); //归并算法 void mergeSort ( Rank lo, Rank hi ); //归并排序算法 Rank partition ( Rank lo, Rank hi ); //轴点构造算法 void quickSort ( Rank lo, Rank hi ); //快速排序算法 void heapSort ( Rank lo, Rank hi ); //堆排序（稍后结合完全堆讲解）public: // 公有成员，提供给用户使用的// 构造函数的几种形式 MyVector ( int c = DEFAULT_CAPACITY, int s = 0, T v = 0 ) //容量为c、规模为s、所有元素初始为v { _elem = new T[_capacity = c]; for ( _size = 0; _size &lt; s; _elem[_size++] = v ); //s&lt;=c } MyVector ( T const* A, Rank n ) { copyFrom ( A, 0, n ); } // 数组 整体复制_ MyVector ( T const* A, Rank lo, Rank hi ) { copyFrom ( A, lo, hi ); } //复制数组区间 MyVector ( MyVector&lt;T&gt; const&amp; V ) { copyFrom ( V._elem, 0, V._size ); } //复制 向量 整体_ MyVector ( MyVector&lt;T&gt; const&amp; V, Rank lo, Rank hi ) { copyFrom ( V._elem, lo, hi ); } //复制向量区间_// 析构函数 ~MyVector() { delete [] _elem; } //释放内部空间_// 只读访问接口 const 关键字 // 通常一个类如果想被广泛使用，其中不修改类数据成员的成员函数应该声明为 const 成员函数 Rank size() const { return _size; } //规模_ bool empty() const { return !_size; } //判空_ int disordered() const; //判断向量是否已排序 Rank find ( T const&amp; e ) const { return find ( e, 0, _size ); } //无序向量整体查找_ Rank find ( T const&amp; e, Rank lo, Rank hi ) const; //无序向量区间查找 Rank search ( T const&amp; e ) const //有序向量整体查找 { return ( 0 &gt;= _size ) ? -1 : search ( e, 0, _size ); }// 可写访问接口_ T&amp; operator[] ( Rank r ); //重载下标操作符，可以类似于数组形式引用各元素, 可以作为左值 const T&amp; operator[] ( Rank r ) const; //仅限于做右值的重载版本，const T&amp; Vector&lt;T&gt; &amp; operator= ( Vector&lt;T&gt; const&amp; ); //重载赋值操作符，以便直接克隆向量 T remove ( Rank r ); //删除秩为r的元素 int remove ( Rank lo, Rank hi ); //删除秩在区间[lo, hi)之内的元素 Rank insert ( Rank r, T const&amp; e ); //插入元素 Rank insert ( T const&amp; e ) { return insert ( _size, e ); } //默认作为末元素插入_ void sort ( Rank lo, Rank hi ); //对[lo, hi)排序 void sort() { sort ( 0, _size ); } //整体排序_ void unsort ( Rank lo, Rank hi ); //对[lo, hi)置乱 void unsort() { unsort ( 0, _size ); } //整体置乱_ int deduplicate(); //无序去重 int uniquify(); //有序去重// 遍历 void traverse ( void (* ) ( T&amp; ) ); //遍历（使用函数指针，只读或局部性修改 template &lt;typename VST&gt; void traverse ( VST&amp; ); //遍历（使用函数对象，可全局性修改）};#endif // MYVECTOR_H_INCLUDED 可以看到模板类定义主要是以下几个部分： 内部函数 构造函数 析构函数 只读接口 可写接口 遍历接口 模板类和函数定义先复习一下模板函数和模板类的构造，关键字 typename typename和class的区别 在c++ Template中很多地方都用到了typename与class这两个关键字，而且好像可以替换，是不是这两个关键字完全一样呢? 相信学习C++的人对class这个关键字都非常明白，class用于定义类，在模板引入c++后，最初定义模板的方法为： template&lt;class T&gt;…… 在这里class关键字表明T是一个类型，后来为了避免class在这两个地方的使用可能给人带来混淆，所以引入了typename这个关键字，它的作用同 class一样表明后面的符号为一个类型，这样在定义模板的时候就可以使用下面的方式了： template&lt;typename T&gt;…… 在模板定义语法中关键字class与typename的作用完全一样。 typename难道仅仅在模板定义中起作用吗？ 其实不是这样，typename另外一个作用为：使用嵌套依赖类型(nested depended name)，如下所示： 123456789101112131415161718192021222324252627class MyArray{ public： typedef int LengthType; .....}template&lt;class T&gt;void MyMethod( T myarr ){ typedef typename T::LengthType LengthType; LengthType length = myarr.GetLength;} 这个时候typename的作用就是告诉c++编译器，typename后面的字符串为一个类型名称，而不是成员函数或者成员变量，这个时候如果前面没有typename，编译器没有任何办法知道T::LengthType是一个类型还是一个成员名称(静态数据成员或者静态函数)，所以编译不能够通过。 模板成员函数定义成员函数的模板函数： “#include “vector_functions.h”” 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#ifndef VECTOR_CONSTRUCTOR_BY_COPYING_H_INCLUDED#define VECTOR_CONSTRUCTOR_BY_COPYING_H_INCLUDED//基于复制的构造函数template &lt;typename T&gt; // 元素类型void Vector&lt;T&gt;::copyFrom(T const*A, Rank lo, Rank hi) //以数组区间A[lo, hi)为蓝本复制向量{ _elem = new T[_capacity = 2 * ( hi - lo ) ]; _size = 0; //分配空间，规模清零 while ( lo &lt; hi ) //A[lo, hi)内的元素逐一 _elem[_size++] = A[lo++]; //复制至_elem[0, hi - lo)}//遍历template &lt;typename T&gt; void Vector&lt;T&gt;::traverse ( void ( *visit ) ( T&amp; ) ) //借助函数指针机制{ for ( int i = 0; i &lt; _size; i++ ) visit ( _elem[i] ); } //遍历向量template &lt;typename T&gt; template &lt;typename VST&gt; //元素类型、操作器void Vector&lt;T&gt;::traverse ( VST&amp; visit ) //借助函数对象机制{ for ( int i = 0; i &lt; _size; i++ ) visit ( _elem[i] ); } //遍历向量//删除template &lt;typename T&gt; T Vector&lt;T&gt;::remove ( Rank r ) { //删除向量中秩为r的元素，0 &lt;= r &lt; size T e = _elem[r]; //备份被删除元素 remove ( r, r + 1 ); //调用区间删除算法，等效于对区间[r, r + 1)的删除 return e; //返回被删除元素}template &lt;typename T&gt; int Vector&lt;T&gt;::remove ( Rank lo, Rank hi ) { //删除区间[lo, hi) if ( lo == hi ) return 0; //出于效率考虑，单独处理退化情况，比如remove(0, 0) while ( hi &lt; _size ) _elem[lo++] = _elem[hi++]; //[hi, _size)顺次前移hi - lo个单元 _size = lo; //更新规模，直接丢弃尾部[lo, _size = hi)区间 return hi - lo; //返回被删除元素的数目}//冒泡排序//....#endif // VECTOR_CONSTRUCTOR_BY_COPYING_H_ INCLUDED 可以看到模板类和模板函数的定义，需要加上 template &lt;typename T&gt;… 通过主函数进行测试 “main.cpp” 123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;using namespace std;#include &quot;myvector.h&quot;#include &quot;vector_functions.h&quot;int main(){ cout &lt;&lt; &quot;Hello world!&quot; &lt;&lt; endl; Vector&lt;int&gt; myvector(10, 10, 0); int res = myvector.remove(0); cout &lt;&lt; res &lt;&lt; endl; return 0;} 所以模板类和模板函数的定义，以及使用大致就是这个样子。接下来，对类定义中的声明的成员函数进行定义。既然是作为模板类，当然函数的效率越高越好，所以会涉及到很多算法分析的内容～ 可扩充向量：扩容 expand()通过算法分析得：加倍扩容～ 1234567891011121314151617181920212223//加倍扩容template &lt;typename T&gt; void Vector&lt;T&gt;::expand () // 向量不足时扩容{ if (_size &lt; _capacity) return; // 尚未满员，不需要扩容_ if ( _capacity &lt; DEFAULT_CAPACITY ) _capacity = DEFAULT_CAPACITY; //容量不低于最小容量_ T* oldelem = _elem; // 重新申请地址 _elem = new T[_capacity &lt;&lt; 1]; //容量加倍 for (int i=0; i&lt;_size; i++) _elem[i] = oldelem[i]; //复制原向量内容（T为基本类型，或已重载赋值操作符'='）_ delete [] oldElem; //释放原空间_} 需要注意的是扩容时，是重新申请地址空间，保证连续？ 为什么要使用加倍策略呢？ 通过平摊法分析可得加倍扩容累计增容时间是 O(n). 元素访问需要重载下标操作符 “[]”， 复习一波操作符重载 如果 T 不是内置数据类型，那么就需要对下标操作符进行重载。 1234567891011template &lt;typename T&gt; T&amp; Vector&lt;T&gt;::operator[] ( Rank r ) //重载下标操作符{ return _elem[r]; } // assert: 0 &lt;= r &lt; _sizeconst template &lt;typename T&gt; T&amp; Vector&lt;T&gt;::operator[] ( Rank r ) const //仅限于做右值{ return _elem[r]; } // assert: 0 &lt;= r &lt; _size 插入 insert() 主要是否需要扩容 注意元素向后移动的顺序，自后向前 删除 remove() 删除单个元素和删除区间 删除区间时，需要注意删除顺序，从左到右逐个复制 有序向量的去重12345678910111213141516171819202122232425template &lt;typename T&gt; int Vector&lt;T&gt;::uniquify(){ Rank i = 0, j = 0; //各对互异“相邻”元素的秩 while(++j &lt; _size){ if (_elem[j] != _elem[i]) _elem[++i] = _elem[j]; } _size = ++i; shrink(); return j-1; //被删除元素的总数} 去重高效版的图解。回过头来看，实际上就是解决了一种问题，比如 5 这个元素不在是一步步向前，而是直接就到了最终的位置，这显然更高效。 有序向量的查找减而治之： fibdearch binsearch 二分查找123456789101112131415161718192021// 二分查找算法（版本A）：在有序向量的区间[lo, hi)内查找元素e，0 &lt;= lo &lt;= hi &lt;= _sizetemplate &lt;typename T&gt; static Rank binSearch ( T* A, T const&amp; e, Rank lo, Rank hi ) { while ( lo &lt; hi ) { //每步迭代可能要做两次比较判断，有三个分支 Rank mi = ( lo + hi ) &gt;&gt; 1; //以中点为轴点（区间宽度的折半，等效于宽度之数值表示的右移） if ( e &lt; A[mi] ) hi = mi; //深入前半段[lo, mi)继续查找 else if ( A[mi] &lt; e ) lo = mi + 1; //深入后半段(mi, hi)继续查找 else return mi; //在mi处命中 } //成功查找可以提前终止 return -1; //查找失败}//有多个命中元素时，不能保证返回秩最大者；查找失败时，简单地返回-1，而不能指示失败的位置 复杂度是 1.5log(n). fibonacci 查找fibonacci 查找和 binary 查找本质上差不多，区别就在于 middle 的选取。在比对当前位置失败之后，我们需要比较查找元素与当前位置元素的大小，向左侧深入需要 +1, 向右侧深入需要 +2. 所以这就造成了一种不平衡。 通过公式证明，$\\lambda$ 选在何处时，复杂度最小。 二分查找改进版二分查找的改进版： 使得向左右两侧深入的代价平衡，不能及时命中轴点所在的元素。牺牲了最好情况，但整体性能更趋于稳定。 123456789101112131415161718192021// 二分查找算法（版本B）：在有序向量的区间[lo, hi)内查找元素e，0 &lt;= lo &lt; hi &lt;= _sizetemplate &lt;typename T&gt; static Rank binSearch(T* A, T const&amp; e, Rank lo, Rank hi){ while (1 &lt; hi-lo) //每步迭代仅需做一次比较判断，有两个分支；成功查找不能提前终止 { Rank mi = (lo + hi) &gt;&gt; 1; (e &lt; A[mi]) ? (hi = mi) : (lo = mi); //经比较后确定深入[lo, mi)或[mi, hi) } //出口时hi = lo + 1，查找区间仅含一个元素A[lo] return ( e == A[lo] ) ? lo : -1 ; //查找成功时返回对应的秩；否则统一返回-1} //有多个命中元素时，不能保证返回秩最大者；查找失败时，简单地返回-1，而不能指示失败的位置 二分查找最终版但是，在这之前所有的 search() 算法都未严格满足定义的语义，也就是返回 不大于 e 的元素的最后一个元素的 秩。 多个元素命中，必须返回 秩 最大者 失败时，应返回小于 e 的最大者（含哨兵 [lo-1]） 123456789101112131415161718192021// 二分查找算法（版本C）：在有序向量的区间[lo, hi)内查找元素e，0 &lt;= lo &lt;= hi &lt;= _sizetemplate &lt;typename T&gt; static Rank binSearch(T* A, T const&amp; e, Rank lo, Rank hi){ while (lo &lt; hi) //每步迭代仅需做一次比较判断，有两个分支 { Rank mi = (lo + hi) &gt;&gt; 1; (e &lt; A[mi]) ? (hi=mi) : (lo = mi + 1); //经比较后确定深入[lo, mi)或(mi, hi) } //成功查找不能提前终止 return --lo; //循环结束时，lo为大于e的元素的最小秩，故lo - 1即不大于e的元素的最大秩} //有多个命中元素时，总能保证返回秩最大者；查找失败时，能够返回失败的位置 插值查找Interpolation search 如果有数据的先验分布，比如是均匀分布的，那么可以根据先验知识有效的提高查找效率。 $$\\dfrac{mi-lo}{hi-lo} = \\dfrac{e-A[lo]}{A[hi]-A[lo]}$$ 这样可以更快的接近所需查找的元素。但是在小范围内，却容易被恶性分布所干扰，比如不满足均匀分布的情况下。 最好的查找方式是：插值查找 -&gt; 二分查找 -&gt; 顺序查找 排序前面的查找算法都是对应于有序向量的，所以我们需要用排序算法先把向量变得有序起来，这样才能更高效的 查找 或 去重。 冒泡排序冒泡排序：从后向前逐渐有序 其改进 改进一：如果在某一趟循环之后，已经完全有序了，后面的循环也就不需要了。所以需要设置设否有序的标志 改进二： 如果数据只是在前缀的一小部分乱序，后缀很多一部分已经有序了。这个时候我们是否还需要亦步亦趋的从后往前一步步的有序呢？ 所以可以在一趟交换走完之后，标记出交换的最后位置。然后只需要考虑前缀无序的部分即可。 改进一1234567891011121314151617181920212223242526272829template &lt;typename T&gt; //向量的起泡排序void Vector&lt;T&gt;::bubbleSort ( Rank lo, Rank hi ) //assert: 0 &lt;= lo &lt; hi &lt;= size{ while ( !bubble ( lo, hi-- ) ); } //逐趟做扫描交换，直至全序//一趟扫描交换template &lt;typename T&gt; bool Vector&lt;T&gt;::bubble ( Rank lo, Rank hi ) { bool sorted = true; //整体有序标志 while ( ++lo &lt; hi ) //自左向右，逐一检查各对相邻元素 if ( _elem[lo - 1] &gt; _elem[lo] ) { //若逆序，则_ sorted = false; //意味着尚未整体有序，并需要 swap ( _elem[lo - 1], _elem[lo] ); //通过交换使局部有序_ } return sorted; //返回有序标志} 自己动手实现一个整体的形式。。 12345678910111213141516171819202122232425262728293031323334353637template &lt;typename T&gt; void Vector&lt;T&gt;::my_bubbleSort(Rank lo, Rank hi){ while (lo &lt; hi) { bool sorted = true; Rank a = lo; while ( ++a &lt; hi) //自左向右，逐一检查各对相邻元素 { if ( _elem[a - 1] &gt; _elem[a] ) //若逆序，则_ { sorted = false; //意味着尚未整体有序，并需要 swap ( _elem[a - 1], _elem[a] ); //通过交换使局部有序_ } } if (sorted == true) break; hi--; }} 改进二123456789101112131415161718192021222324252627282930313233343536373839// 改进后的冒泡排序template &lt;typename T&gt; //向量的起泡排序void Vector&lt;T&gt;::bubbleSort ( Rank lo, Rank hi ) //assert: 0 &lt;= lo &lt; hi &lt;= size{ while ( lo &lt; ( hi = bubble_fast(lo, hi) ) );} //逐趟做扫描交换，直至全序template &lt;typename T&gt; Rank Vector&lt;T&gt;::bubble_fast ( Rank lo, Rank hi ) { //一趟扫描交换 Rank last = lo; //最右侧的逆序对初始化为[lo - 1, lo] while ( ++lo &lt; hi ) //自左向右，逐一检查各对相邻元素 { if ( _elem[lo - 1] &gt; _elem[lo] ) { //若逆序，则 last = lo; //更新最右侧逆序对位置记录，并 swap ( _elem[lo - 1], _elem[lo] ); //通过交换使局部有序 } } return last; //返回最右侧的逆序对位置} 归并排序分治策略： 一分为二 子序列递归排序 合并两个子序列 代码形式就是这样： 123456789101112131415template &lt;typename T&gt; //向量归并排序void Vector&lt;T&gt;::mergeSort ( Rank lo, Rank hi ) { //0 &lt;= lo &lt; hi &lt;= size if ( hi - lo &lt; 2 ) return; //单元素区间自然有序，否则... int mi = ( lo + hi ) / 2; //以中点为界 mergeSort ( lo, mi ); mergeSort ( mi, hi ); //分别排序 merge ( lo, mi, hi ); //归并} 前两个比较简单，那么第三步归并（merge）怎么实现呢？ 下图是个实例图解 每次只比较首个元素。 接下来具体如何实现两个子序列的归并。 把两个子序列分别是 B，C. 合并后的序列是 A. 其中复制 [lo, mi) 到 B. C 子序列不需要复制。 12345678910111213141516171819202122232425262728293031template &lt;typename T&gt; //有序向量的归并void Vector&lt;T&gt;::merge ( Rank lo, Rank mi, Rank hi ) { //各自有序的子向量[lo, mi)和[mi, hi) T* A = _elem + lo; //合并后的向量A[0, hi - lo) = _elem[lo, hi) // 复制前字向量 int lb = mi - lo; T* B = new T[lb]; //前子向量B[0, lb) = _elem[lo, mi) for ( Rank i = 0; i &lt; lb; i++ ) B[i] = A[i]; //复制前子向量 // 后子向量 不需要复制 int lc = hi - mi; T* C = _elem + mi; //后子向量C[0, lc) = _elem[mi, hi) // B 和 C 中较小者续至A末尾 for ( Rank i = 0, j = 0, k = 0; ( j &lt; lb ) || ( k &lt; lc ); ) { //B[j]和C[k]中的小者续至A末尾 if ( ( j &lt; lb ) &amp;&amp; ( ! ( k &lt; lc ) || ( B[j] &lt;= C[k] ) ) ) A[i++] = B[j++]; if ( ( k &lt; lc ) &amp;&amp; ( ! ( j &lt; lb ) || ( C[k] &lt; B[j] ) ) ) A[i++] = C[k++]; } delete [] B; //释放临时空间B}//归并后得到完整的有序向量[lo, hi) 其中需要注意的是，将 B 和 C 接到 A 中时，跳出循环的条件是： ( j &lt; lb ) || ( k &lt; lc ) 即两个子序列都越界了。 但其实 B 已经越界后，C 后面的并不需要一一复制到 A 中。所以这里也是可以改进的。 123456789for ( Rank i = 0, j = 0, k = 0; j &lt; lb ; ) { //B[j]和C[k]中的小者续至A末尾 if ( ( j &lt; lb ) &amp;&amp; ( B[j] &lt;= C[k] ) ) A[i++] = B[j++]; if ( ! ( j &lt; lb ) || ( C[k] &lt; B[j] ) ) A[i++] = C[k++];} 所以两个条件分别是： B 还没越界，并且 B 中元素较小， 则 B -&gt; A B 已经越界，或者 B 中元素较大， 则 C -&gt; A. 邓老师提供了一个思路，假设 B 已经越界，可以理解为在哨兵处有一个无穷大的元素，那么等同于 B 中元素始终较大。 课后习题","link":"/2018/12/20/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%952-%E5%90%91%E9%87%8F/"},{"title":"邓公数据结构与算法3-列表","text":"事实上 2017 年的这个时候我看了一遍浙大陈越老师的《数据结构》，并且有详细的做笔记。但现在回过头来看，这些好像基本上又都忘了。。问题应该是在于学过之后不怎么使用，当时也只是看了视频，而没有去刷题。 所以这次总结下经验，只看视频并做笔记是远远不够的。视频的缺陷在于不像书本那样，能够随意的翻到你不懂或者模糊的知识点。所以，这次我应该做的是，把视频快速过一遍，记下老师说的知识点，然后对照书本去敲代码。更更更重要的是，不断的练习。 列表（链表）这部分内容的结构跟 向量 那一章是一样的。先介绍操作接口，然后逐个介绍它们的实现。 列表，也就是链表，与向量对称且互补。 向量： 循秩访问 call-by-rank 列表： 循位置访问 call-by-position 双向链表和单向链表。 邓老师讲的是双向链表，所以含有 头节点 header 和 尾节点 trailer。 header 和 trailer 是与生俱来的，也是不可见的。所以初始化的链表节点中含有两个空节点。 双向链表的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485# ifndef DOUBLE_LINK_HPP# define DOUBLE_LINK_HPP/*双向链表的节点结构*/template &lt;typename T&gt;struct Node{public: Node()= default; Node(T value, Node&lt;T&gt;* preptr, Node&lt;T&gt;* nextptr) :_value(value), pre_ptr(preptr), next_ptr(nextptr){}public: T _value; Node&lt;T&gt;* pre_ptr; Node&lt;T&gt;* next_ptr;};/** 双向链表类*/template&lt;typename T&gt;class DoubleLink{public: typedef Node&lt;T&gt;* pointer;public: DoubleLink(); ~DoubleLink(){};public: Node&lt;T&gt;* insert(int index, T value); Node&lt;T&gt;* insert_front(T value); Node&lt;T&gt;* insert_last(T value); Node&lt;T&gt;* del(int index); Node&lt;T&gt;* delete_front(); Node&lt;T&gt;* delete_last(); bool isEmpty(); int size(); T get(int index); T get_front(); T get_last(); Node&lt;T&gt;* getHead();private: Node&lt;T&gt;* phead; int count;private : Node&lt;T&gt;* getNode(int index);};/** 构造函数**/template &lt;typename T&gt;DoubleLink&lt;T&gt;::DoubleLink(){ phead = new Node&lt;T&gt;(0, NULL, NULL); phead-&gt;next_ptr = phead; phead-&gt;pre_ptr = phead; count = 0;};template&lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::getHead(){ return phead;}/**返回链表长度*/template &lt;typename T&gt;int DoubleLink&lt;T&gt;::size(){ return count;}/*获取指定下标的元素*/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::getNode(int index){ if (index &gt;= count || index &lt; 0) return NULL; if (index&lt;=count / 2) //如果在前半部分 { Node&lt;T&gt;* pnode = phead-&gt;next_ptr; while (index) { pnode = pnode-&gt;next_ptr; index--; } return pnode; } //在后半部分 index = count - index-1; Node&lt;T&gt;* pnode = phead-&gt;pre_ptr; while (index) { pnode = pnode-&gt;pre_ptr; index--; } return pnode;};/**将新节点插到第一个位置*/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::insert_front(T value){ Node&lt;T&gt;* newNode = new Node&lt;int&gt;(value, phead, phead-&gt;next_ptr); phead-&gt;next_ptr -&gt;pre_ptr= newNode; phead-&gt;next_ptr = newNode; count++; return newNode;};/**将新节点插到链表尾部*/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::insert_last(T value){ Node&lt;T&gt; * newNode = new Node&lt;int&gt;(value, phead-&gt;pre_ptr, phead); phead-&gt;pre_ptr-&gt;next_ptr = newNode; phead-&gt;pre_ptr = newNode; count++; return newNode;};/**将节点位置插到index位置之前*/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::insert(int index, T value){ if (index == 0) return insert_front(value); Node&lt;T&gt;* pNode = getNode(index); if (pNode == NULL) return NULL; Node&lt;T&gt;* newNode = new Node&lt;T&gt;(value, pNode-&gt;pre_ptr, pNode); pNode-&gt;pre_ptr-&gt;next_ptr = newNode; pNode-&gt;pre_ptr = newNode; count++; return newNode;};/**删除链表第一个节点*返回删除后链表第一个节点*/template&lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::delete_front(){ if (count == 0) //空树，返回NULL { return NULL; } Node&lt;T&gt;* pnode = phead-&gt;next_ptr; phead-&gt;next_ptr = pnode-&gt;next_ptr; pnode-&gt;next_ptr-&gt;pre_ptr = phead; delete pnode; count--; return phead-&gt;next_ptr;};/**删除链表的末尾节点*返回删除后链表尾部元素*/template&lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::delete_last(){ if (count == 0) { return NULL; } Node&lt;T&gt;*pnode = phead-&gt;pre_ptr; pnode-&gt;pre_ptr-&gt;next_ptr = phead; phead-&gt;pre_ptr = pnode-&gt;pre_ptr; delete pnode; count--; return phead-&gt;pre_ptr;}/**删除指定位置的元素**/template &lt;typename T&gt;Node&lt;T&gt;* DoubleLink&lt;T&gt;::del(int index){ if (index == 0) return delete_front(); if (index == count - 1) return delete_last(); if (index &gt;= count) return NULL; Node&lt;T&gt;* pnode = getNode(index); pnode-&gt;pre_ptr-&gt;next_ptr = pnode-&gt;next_ptr; pnode-&gt;next_ptr-&gt;pre_ptr = pnode-&gt;pre_ptr; Node&lt;T&gt;* ptemp = pnode-&gt;pre_ptr; delete pnode; count--; return ptemp;};template &lt;typename T&gt;bool DoubleLink&lt;T&gt;::isEmpty(){ return count == 0;};/**获取第一个节点的值*/template&lt;typename T&gt;T DoubleLink&lt;T&gt;::get_front(){ return phead-&gt;next_ptr-&gt;_value;};/**获取最后一个节点的值*/template &lt;typename T&gt;T DoubleLink&lt;T&gt;::get_last(){ return phead-&gt;pre_ptr-&gt;_value;};/**获取指定位置节点的值*/template &lt;typename T&gt;T DoubleLink&lt;T&gt;::get(int index){ Node&lt;T&gt; pnode = getNode(index); return pnode-&gt;_value;};# endif 排序leetcode 148: https://leetcode.com/problems/sort-list/submissions/ 选择排序实际上 冒泡排序 也可以看做是选择排序，不同的是，在选择最大元素的时候，冒泡排序每比较一次都会进行一次交换，这样亦步亦趋的方式增加了消耗。而选择排序，只是进行比较，每一趟遍历只交换一次。 邓老师这里的代码会比较复杂，是基于双向链表实现的。这里我们只需要理解算法思想。 列表的排序算法， 对于起始于位置 p 的 n 个元素的排序。 1234567891011121314151617181920212223#define ListNodePosi(T) ListNode&lt;T&gt;* //列表节点位置template &lt;typename T&gt; //列表的选择排序算法：对起始于位置p的n个元素排序void List&lt;T&gt;::selectionSort ( ListNodePosi(T) p, int n ) { //valid(p) &amp;&amp; rank(p) + n &lt;= size ListNodePosi(T) head = p-&gt;pred; ListNodePosi(T) tail = p; for ( int i = 0; i &lt; n; i++ ) tail = tail-&gt;succ; //待排序区间为(head, tail) while ( 1 &lt; n ) { //在至少还剩两个节点之前，在待排序区间内 ListNodePosi(T) max = selectMax ( head-&gt;succ, n ); //找出最大者（歧义时后者优先） insertB ( tail, remove ( max ) ); //将其移至无序区间末尾（作为有序区间新的首元素） tail = tail-&gt;pred; n--; }} 1234567891011121314151617template &lt;typename T&gt; //从起始于位置p的n个元素中选出最大者ListNodePosi(T) List&lt;T&gt;::selectMax ( ListNodePosi(T) p, int n ) { ListNodePosi(T) max = p; //最大者暂定为首节点p for ( ListNodePosi(T) cur = p; 1 &lt; n; n-- ) //从首节点p出发，将后续节点逐一与max比较 if ( !lt ( ( cur = cur-&gt;succ )-&gt;data, max-&gt;data ) ) //若当前元素不小于max，则 max = cur; //更新最大元素位置记录 return max; //返回最大节点位置} 插入排序归并排序对列表的归并排序，相比向量要更复杂些，主要在怎么找到中间点 归并排序的整体思想 找到一个链表的中间节点的方法 合并两个已排好序的链表为一个新的有序链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: ListNode* getMid(ListNode* head) { if (!head) return NULL; if (!head-&gt;next) return head; ListNode* slow = head; ListNode* fast = head-&gt;next; while(fast &amp;&amp; fast-&gt;next) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; } return slow; } ListNode* mergeLists(ListNode* l1, ListNode* l2) { if (l1 == NULL) return l2; if (l2 == NULL) return l1; ListNode dummy(-1); ListNode *tail = &amp;dummy; while(l1 &amp;&amp; l2) { if (l1-&gt;val &lt; l2-&gt;val) { tail-&gt;next = l1; tail = tail-&gt;next; l1 = l1-&gt;next; } else { tail-&gt;next = l2; tail = tail-&gt;next; l2 = l2-&gt;next; } } if (l1) tail-&gt;next = l1; if (l2) tail-&gt;next = l2; return dummy.next; } ListNode* sortList(ListNode* head) { if (!head) return NULL; if (head-&gt;next==NULL) return head; // 找到中间点 ListNode *mid = getMid(head); // 将链表分成两个子链表 ListNode *nextPart = NULL; if (mid) { nextPart = mid-&gt;next; mid-&gt;next = NULL; } return mergeLists(sortList(head), sortList(nextPart)); }};","link":"/2018/12/21/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%953-%E5%88%97%E8%A1%A8/"},{"title":"迁移学习系列-2-Combining semi-supervised learning with transfer learning","text":"paperStrong Baselines for Neural Semi-Supervised Learning under Domain Shift motivation这篇paper的工作就是提出了一个经典方法实现的strong baseline.他的motivation就是前面很多研究比如基于deep learning的，对比的经典算法都很weak，或者是在专有的数据集上跑（容易过拟合）。 对比的三种传统方法， self-traning, tritraining, tri-training with disagreement self-training: 使用有标签的数据，训练一个模型 用这个模型去预测无标签的数据，得到对应样本属于某一类别的概率 选择一个阈值，大于这个阈值的样本，可以打上伪标签。但是通常来说，阈值不太好确定，所以可以使用相对阈值，也就是选取概率相对较高的 top N. 模型的缺点在于：如果预测错了某些样本，那么错误会累积并放大。 tri-training: 使用有标签的数据，训练三个模型 m1, m2, m3 使用 bootstrapping 的方法，sample部分无标签的数据，然后使用三个模型进行预测，当 m1 预测样本属于某一类的概率低时，而 m2, m3 预测样本属于这一类的概率高时，将这个样本打上伪标签，加入到 m1 的训练集中去 迭代这个过程，直到分类器不在变化，可以同时更新三个分类器？ motivation：模型应该增强它相对较弱的地方。其实也就是 ensamble 的 sense. 缺点：计算量太大， 耗费时间和空间 multi-task tritraining: 多任务训练，这里的任务其实可以看作是一致的，底层 encoder 层参数共享，softmax层，也就是 decoder 层参数不一致。 要尽可能让 m1, m2 具有差异性 diversity，加上了正则化项 模型 m3 只在伪标签数据上进行训练。其目的是让模型在 domain shift 情况下鲁棒性更强。 paper2Semi-Supervised Sequence Modeling with Cross-View Training (EMNLP 2018)","link":"/2019/03/06/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-2-Combining-semi-supervised-learning-with-transfer-learning/"},{"title":"论文笔记-sign language recognition and translation","text":"paper list to read: BABEL: Bodies, Action and Behavior with English Labels, CVPR2021 Fingerspelling Detection in American Sign Language, CVPR2021 American Sign Language fingerspelling recognition in the wild. SLT2018 Fingerspelling recognition in the wild with iterative visual attention. ICCV2019 Fingerspelling Detection手指拼写的作用： 专有名词 技术术语 缩写等没有对应手势的词汇 也用于强调和方便 手指拼写占ASL的 12%-35%. 这个比例比大部分手语词汇量都要大。 Fingerspelling is used for multiple purposes, including for words that do not have their own signs (such as many proper nouns, technical terms, and abbreviations) [39] but also sometimes for emphasis or expediency. Fingerspelling accounts for 12% to 35% of ASL, where it is used more than in other sign languages [40]. 手指拼写对应的字母与翻译出来的英语是单调对齐的。这有点类似于翻译中的直接音译。 手指拼写的检测对于下游手语识别任务有显著提升作用。","link":"/2021/07/11/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sign-language-recognition-and-translation/"},{"title":"论文笔记-再看 Capsules 以及 capsules 在文本分类上的应用","text":"参考： blog: 苏剑林：揭开迷雾，来一顿美味的Capsule盛宴 paper：Investigating Capsule Networks with Dynamic Routing for Text Classification 再看 Capsules苏剑林同学在他的那篇博客中对 capsule 的理解很有道理，胶囊也就是用 “vector in vector out” 取代了 “scaler in scaler out”。 在我的上一篇 blog 中在 PrimaryCaps 模块中，将前一步通过卷积得到的输出是 [batch, 20, 20, 256]. 经过 PrimaryCaps 第一步 affine-transform 转换成 [batch, 6, 6, 32, 8]. 实际上就是在这一步将一个像素点的特征转换成了一个 8d-vector 的胶囊。 事实上，在 NLP 的任务中，这种用向量来表示一维特征的做法确实最基本的。比如 one-hot 向量，word2vec 将词转换成 dense vector. 在传统的神经网络中，从低层次的特征逐步抽象，归纳为高层次的特征，是通过权重加权求和得到的，比如卷积啊，全连接都是这样，然后通过梯度反向传播，更新这些权重参数。 这个过程某种程度上模拟了人的层次分类做法，从而完成对最终目标的输出，并且具有比较好的泛化能力。的确，神经网络应该是这样做的，然而它并不能告诉我们它确确实实是这样做的，这就是神经网络的难解释性，也就是很多人会将深度学习视为黑箱的原因之一。 而 Hiton 提出了 Capsule 就具有很好的可解释性，那么其中的 “抛弃梯度下降” 又是怎么一回事呢？苏神在 blog 中给了很好的解释。 胶囊的计算在前面的 blog 中我们已经理解了什么是“胶囊”。神经元是标量，胶囊就是向量！Hinton的理解是：每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。不仅如此，上一篇 blog 中有提到的 CNN 中的不足，主要在于两点，一是 max pooling 丢失了部分信息（这在低层次的layer中可能影响不大，但是在高层次的layer就会有比较大的影响），二是 CNN 不能提取低维特征与高维特征之间在空间中的相对位置信息。而胶囊的方向能表示这一部分信息。比如下面这张图就很明显的表示出来了。 从低层次胶囊到高层次胶囊的计算细节，主要分为 3 个步骤： affine transform weighting and sum squash 如果考虑更多的胶囊，可以抽象到下面这张图。 我们只关注其中的某一部分就是： 关于从低层次特征如何整合到高层次特征以及这样做的原因是啥，苏同学这里说的是相同透彻了。 动态路由在前面的blog中我们差不多能理解：动态路由实际上就是 低层次的胶囊将部分的自己交付给高层次的胶囊，而这个部分的权重却又取决于 低层次胶囊和高层次胶囊的相关性。 我们原本也是可以通过反向传播来解决这个问题的（不显示的计算出相关性，而是直接用神经网络来代替，不知是否可用梯度下降来处理胶囊？），而 Hinton 使用的动态路由算法可解释更强。 动态路由算法就是来解决这个权重分配的问题。 对于动态路由的理解，苏同学给上了两道小菜。总的理解就是，高层胶囊在启动阶段，我们并不知道它是多少，那么前面的相似度也就没法计算。于是，我们只能初始化一个值，也就是取低层次胶囊的均值。如同上图中第二步将 $b_ij$ 设置成 0，那么低层次胶囊分配给高层次胶囊的权重 $c_{ij}$ 就都是相等的。 $$c_{ij}=\\dfrac{exp(b_{ij})}{\\sum_k exp(b_ik)}$$ 然后反复迭代。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。至于这个动态路由的细节，其实是不固定的，取决于聚类的算法，比如关于Capsule的新文章《MATRIX CAPSULES WITH EM ROUTING》就使用了Gaussian Mixture Model来聚类。 共享版 or 全连接版全连接版在 Capsule 中，低层次特征是通过普通的卷积神经网络提取，然后通过一个矩阵变换得到的。其中的 $W$ 是需要学习的参数。$v_j$ 是作为输入 $u_i$ 的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。那么为了实现“多角度看特征”，于是可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵做变换. $$v_j = \\text{squash}\\sum_i\\dfrac{e^{&lt;\\hat u_{j|i}, v_j&gt;}}{\\sum_k e^{&lt;\\hat u_{k|i}, v_k&gt;}}\\hat u_{j|i}, \\hat u_{j|i}=W_{ij}u_i$$ 共享版全连接层只能处理定长输入，全连接版的Capsule也不例外。而CNN处理的图像大小通常是不定的，提取的特征数目就不定了，这种情形下，全连接层的Capsule就不适用了。因为在前一图就可以看到，参数矩阵的个数等于输入胶囊数目乘以输出胶囊数目，既然输入数目不固定，那么就不能用全连接了。 所以跟CNN的权值共享一样，我们也需要一个权值共享版的Capsule。所谓共享版，是指对于固定的上层胶囊j，它与所有的底层胶囊的连接的变换矩阵是共用的，即 $W_{ji}≡W_j$. 采用 Hiton 论文中的参数来计算就是： 输入 [batch, 6, 6, 32, 8]=[batch, 1152, 8], 输入有 6x6x32=1152 个 capsules. 输出 [batch, 16, 10]， 输出有 10 个 capsules. 对于全连接版，权重参数是 $1152\\times 8\\times 16\\times N + 1152\\times N + 1152\\times N$. N 表示 low-level capsules 的数目。 对于共享版， 权重参数是 $8\\times 16\\times 1152 + 1152 + 1152$. 反向传播现在又有了 $W_{ji}$，那么这些参数怎么训练呢？答案是反向传播。读者也许比较晕的是：现在既有动态路由，又有反向传播了，究竟两者怎么配合？其实这个真的就最简单不过了。从形式上来看，就是往模型中添加了三层罢了，剩下的该做什么还是什么，最后构建一个loss来反向传播。 这样看来，Capsule里边不仅有反向传播，而且只有反向传播，因为动态路由已经作为了模型的一部分，都不算在迭代算法里边了。 capsules 在文本分类上的应用Investigating Capsule Networks with Dynamic Routing for Text Classification Model Architecture N-gram Convolutional Layer普通的卷积操作。 输入：[batch, L, embed_size, 1] 输出：[batch, L-k1+1, 1, B] 其中： kernel: [k1, embed_size, 1, B] B 表示卷积核的个数 k1 是sentence 长度维度上的 sliding-window 尺寸 Primary Capsule Layer初始化成 capsules, 但依然只是简单的卷积操作。 输入：[batch, L-k1+1, 1, B] 输出：[batch, L-k1+1, 1, C, d] 其中： d 表示 capsule 的维度 实际上依然是普通的卷积操作，不同的是，原本是从 channels B 到 channels C.现在每个 channels C 对应的有 d 个。也就是初始化的 capsules. kernel: [1, 1, B, Cd], 实现时先生成 Cd channels, 然后 split. Convolutional Capsule Layer从低层次 feature 到高层次 feature, 在 Hinton 中是capsules版的全连接，在这里是 capsules 版的卷积操作，其中涉及到动态路由算法。 输入：[batch, L-k1+1, 1, C, d] 输出：[batch, L-k1-k2+2, 1, D, d] 其中： 输出的 capsules 维度依旧是 d 但是 capsules 的个数发生了变化，在 Hinton 论文中是通过全连接维度的变换，这里是通过卷积的操作来实现 capsules 个数的变换的。 与 Hinton 的论文类似，第一步是 affine transform 矩阵变换操作，在这篇 paper 中，作者提出了两种方式，实际上就是苏同学博客中的全连接版和共享版（低层次的 capsules 是否共享同样的矩阵变换参数）。 shared: $W\\in R^{N\\times d\\times d}$. N 是 capsules 的个数 no-shared: $W\\in R^{H\\times N\\times d\\times d}$.H 是低维的 capsules 的个数。","link":"/2019/01/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%86%8D%E7%9C%8B-Capsules-%E4%BB%A5%E5%8F%8A-capsules-%E5%9C%A8%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8/"},{"title":"论文笔记-Clip!!!","text":"paper list: CLIP: Learning Transferable Visual Models From Natural Language Supervision ActionCLIP: A New Paradigm for Video Action Recognition CLIPLearning Transferable Visual Models From Natural Language Supervision 思路很简单，就是image和text做对比学习，zero shot能力强大。 ActionCLIP","link":"/2021/07/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Clip/"},{"title":"邓公数据结构与算法5-树","text":"树树： 无环连通图 极小连通图 极大无环图 任一节点 v 与根节点 r 存在唯一的路径 path(v, r) = path( r) 树的表示二叉树二叉树的实现先序遍历中序遍历层次遍历重构","link":"/2019/02/07/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%955-%E6%A0%91/"},{"title":"邓公数据结构与算法4-栈和队列","text":"栈和队列 都是线性结构的特例，因此其实现都可以用 向量或列表 来实现。 STL 中 stack 的接口使用栈的使用栈与递归递归算法所需的空间量，主要决定于最大递归深度。在达到这一深度的时刻，同时活跃的递归实例最多。 那么操作系统是如何实现函数（递归）调用的呢？如何同时记录调用与被调用函数（递归）实例之间的关系？如何实现函数（递归）调用的返回？又是如何维护同时活跃的所有函数（递归）实例的？ 这些问题，都归结于栈。[课本 88 页] 场景一：逆序输出逆序输出：conversion, 输出次序与处理过程颠倒，递归深度和输出长度不易预知。 进制转换场景二：递归嵌套具有自相似性的问题多可嵌套的递归描述，但因分支位置和嵌套深度并不固定，其递归算法的复杂度不易控制。栈结构及其操作天然的具有递归嵌套性，故可以高效的解决这类问题。 括号匹配课本[92页] 算法其实很简单，主要是思考的过程。使用减治 和 分治 都不能解决这个问题。 从大规模问题到小规模问题并不完全等效。也就是分成子问题之后，不满足条件的情况下，合成大问题却可以满足情况。 用 栈 真的太合适了。而且可以推广到多种括号的情况。 栈混洗借助一个中间 栈， 将 栈A 的数转移到 栈B 中去。称作 栈混洗(stack permutation). 在 pytorch 中根据 index 进行重新排列 permutation，实际上也是用的 栈 对吧？ index_select n 个数的栈混洗的总数， 恰好是著名的 catalan 数 $$\\dfrac{2n!}{(n+1)!n!}$$ 怎么甄别出哪些不是栈混洗的序列呢？ 这里有一个禁止的情况出现。对于任何三个互异的数 $1\\le i &lt; i &lt; j &lt; k \\le n$ 出现 $k…,i,…,j$ 则必非为栈混洗。 怎么判断一个序列 B 是否是另一个序列 A 的 栈混洗？ 通过 栈 能实现，线性复杂度的算法。 https://blog.csdn.net/became_a_wolf/article/details/49002593 https://www.cnblogs.com/Inkblots/p/4950331.html 延时队列","link":"/2019/01/09/%E9%82%93%E5%85%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%954-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/"}],"tags":[{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"generation","slug":"generation","link":"/tags/generation/"},{"name":"CSAPP","slug":"CSAPP","link":"/tags/CSAPP/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"machine translation","slug":"machine-translation","link":"/tags/machine-translation/"},{"name":"ai challenger","slug":"ai-challenger","link":"/tags/ai-challenger/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"cs224d","slug":"cs224d","link":"/tags/cs224d/"},{"name":"sign language recognition","slug":"sign-language-recognition","link":"/tags/sign-language-recognition/"},{"name":"sentiment classification","slug":"sentiment-classification","link":"/tags/sentiment-classification/"},{"name":"Machine Translation","slug":"Machine-Translation","link":"/tags/Machine-Translation/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"open set recognition","slug":"open-set-recognition","link":"/tags/open-set-recognition/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/tags/Tensorflow/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"GAN，RL","slug":"GAN，RL","link":"/tags/GAN%EF%BC%8CRL/"},{"name":"文本分类","slug":"文本分类","link":"/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/tags/TensorFlow/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"MRC and QA","slug":"MRC-and-QA","link":"/tags/MRC-and-QA/"},{"name":"capsules","slug":"capsules","link":"/tags/capsules/"},{"name":"generative models","slug":"generative-models","link":"/tags/generative-models/"},{"name":"ESA","slug":"ESA","link":"/tags/ESA/"},{"name":"text matching","slug":"text-matching","link":"/tags/text-matching/"},{"name":"contrastive learning","slug":"contrastive-learning","link":"/tags/contrastive-learning/"},{"name":"data augmentation","slug":"data-augmentation","link":"/tags/data-augmentation/"},{"name":"transformer","slug":"transformer","link":"/tags/transformer/"},{"name":"language model","slug":"language-model","link":"/tags/language-model/"},{"name":"sentence embedding","slug":"sentence-embedding","link":"/tags/sentence-embedding/"},{"name":"vision transformer","slug":"vision-transformer","link":"/tags/vision-transformer/"},{"name":"transfer learning","slug":"transfer-learning","link":"/tags/transfer-learning/"},{"name":"sign language","slug":"sign-language","link":"/tags/sign-language/"},{"name":"vision-language","slug":"vision-language","link":"/tags/vision-language/"}],"categories":[{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"generation","slug":"generation","link":"/categories/generation/"},{"name":"CSAPP","slug":"CSAPP","link":"/categories/CSAPP/"},{"name":"interview","slug":"interview","link":"/categories/interview/"},{"name":"machine translation","slug":"machine-translation","link":"/categories/machine-translation/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"DRL","slug":"DRL","link":"/categories/DRL/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/categories/reinforcement-learning/"},{"name":"cs224d","slug":"cs224d","link":"/categories/cs224d/"},{"name":"sign language recognition","slug":"sign-language-recognition","link":"/categories/sign-language-recognition/"},{"name":"论文笔记","slug":"论文笔记","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"pytorch","slug":"pytorch","link":"/categories/pytorch/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/categories/TensorFlow/"},{"name":"GAN","slug":"GAN","link":"/categories/GAN/"},{"name":"GAN, RL","slug":"GAN-RL","link":"/categories/GAN-RL/"},{"name":"文本分类","slug":"文本分类","link":"/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"Machine Translation","slug":"论文笔记/Machine-Translation","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"},{"name":"ML","slug":"ML","link":"/categories/ML/"},{"name":"open set recognition","slug":"论文笔记/open-set-recognition","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"},{"name":"DL","slug":"论文笔记/DL","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"NLP","slug":"论文笔记/NLP","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"},{"name":"MRC and QA","slug":"论文笔记/MRC-and-QA","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"},{"name":"capsules","slug":"论文笔记/capsules","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"},{"name":"generative models","slug":"generative-models","link":"/categories/generative-models/"},{"name":"GAN","slug":"论文笔记/GAN","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"},{"name":"ESA","slug":"论文笔记/ESA","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"},{"name":"text matching","slug":"论文笔记/text-matching","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"},{"name":"machine translation","slug":"论文笔记/machine-translation","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"},{"name":"constrast learning","slug":"论文笔记/constrast-learning","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"},{"name":"data augmentation","slug":"论文笔记/data-augmentation","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"},{"name":"transformer","slug":"transformer","link":"/categories/transformer/"},{"name":"language model","slug":"论文笔记/language-model","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"},{"name":"sentence embedding","slug":"论文笔记/sentence-embedding","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"},{"name":"Transformer","slug":"论文笔记/Transformer","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"},{"name":"dialogue system","slug":"论文笔记/dialogue-system","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"},{"name":"transfer learning","slug":"transfer-learning","link":"/categories/transfer-learning/"},{"name":"computer vision","slug":"论文笔记/computer-vision","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"},{"name":"vision-language","slug":"论文笔记/vision-language","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"}]}