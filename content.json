{"pages":[{"title":"","text":"**Pan Xie** *New main building E813, Beihang University, Beijing, 100191, China* Email: [ftdpanxie@gmail.com](ftdpanxie@gmail.com) Homepage: [panxiaoxie.cn](panxiaoxie.cn) Github: [https://github.com/PanXiebit](https://github.com/PanXiebit) Research Interest: machine translation, natural language generation","link":"/resume/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"C plus plus prime-类","text":"类定义类头、类体、类域 数据成员数据成员声明在类体中，可以是任何类型，eg. 指针，类 分为：非静态（nonstatic），静态（static）. 成员函数成员函数在类域之外是不可见的。通过 “.” 或 “-&gt;” 来引用。 注意区分全局域/全局函数，类域/成员函数。 成员访问信息隐藏（information hiding）：类成员的访问限制，通过访问限定符来实现的。 public 公有成员，提供给用户使用的 private 只能被成员函数和友元访问 protected 对派生类表现的像 public, 对其他程序表现的像 private. 友元关键字 find， 友元可以是一个名字空间函数、一个前面定义的类的一个成员函数、也可以是一个完整的类。 允许一个类授权其他的函数访问他的非公有成员，通常声明放在类头之后。 类声明和定义类声明，是只有类头，没有类体。无法确定类类型的大小，类成员也是未知的。但是可以声明指向该类类型的指针或引用。 类定义，是具有完整的类体。 什么时候用类声明？什么时候用类定义？ 类对象类定义不会分配存储区，只有定义了一个类的对象，才会分配。 类类型，即定义的一个类。通过它定义的对象是有生命期的，生命期根据它在哪个域中被声明的。 类对象可以被另一个对象初始化或赋值，拷贝一个类对象与拷贝它的成员函数等价。 当一个类对象被指定为函数实参或函数返回值时，它就被按值传递。我们可以把一个函数参数或返回值声明为一个类类型的指针或引用。（7.3节，7.4节） 回顾下指针： 123456789int i = 100;int *p;int* p = &amp;i;int *p = &amp;i; p 是指针变量，用来存储地址。注意数组名是 const 地址常量，代表第一个元素的地址。 指针和引用的区别： 指针：指针是一个变量，只不过这个变量存储的是一个地址，指向内存的一个存储单元；而引用跟原来的变量实质上是同一个东西，只不过是原变量的一个别名而已。如： 12345int a=1; int *p=&amp;a;int a=1; int &amp;b=a; 上面定义了一个整形变量和一个指针变量p，该指针变量指向a的存储单元，即p的值是a存储单元的地址. 而下面2句定义了一个整形变量a和这个整形a的引用b，事实上a和b是同一个东西，在内存占有同一个存储单元。 “sizeof引用”得到的是所指向的变量(对象)的大小，而”sizeof指针”得到的是指针本身的大小； 可以有const指针，但是没有const引用； 指针的值可以为空，但是引用的值不能为NULL，并且引用在定义的时候必须初始化； 指针的值在初始化后可以改变，即指向其它的存储单元，而引用在进行初始化后就不会再改变了 继续回到类对象作为函数参数时，用成员访问操作符来访问类对象的数据成员或成员函数。点操作符与类对象或引用连用; 箭头访问操作符与类对象的指针连用。 1234567891011121314151617# include &quot;Screen.h&quot;bool isEqual(Screen&amp; s1, Screen *s2){ ... s1.height ... s2-&gt;height ... ...} isEqual 是非成员函数，如果要直接引用 s1, s2 中的数据成员 height 是不可以的。所以必须借助于 Screen 中的公有成员函数。 123s2-&gt;height 等价于 (*s2).height 类成员函数一组操作的集合。 inline 和 非 inline 成员函数内联函数的作用：如果在程序中调用某个函数，不但要拷贝实参，保存机器的寄存器，程序还必须转向一个新的位置，这样会降低效率。 而内联函数就是解决这个问题的，在程序的调用节点上，“内联”的展开函数的操作，从而额外的执行开销被消除了。 类体内定义的成员函数自动的作为内联函数处理。 类体内声明，类体外定的函数，需要显示的加上关键字 inline. 同时类体外的定义需要用类名限定修饰(限定修饰符 :: ) 访问类成员成员函数的定义可以引用任何一个类成员，无论该成员是私有还是公有。 成员函数可以直接访问它所属类的成员，无需访问操作符。这实际上是通过 this 指针实现的。 私有与公有成员函数公有函数集定义了类的接口。私有成员函数为其他成员函数提供支持。 特殊的成员函数构造函数：管理类对象并处理初始化、赋值、内存管理、类型转换、析构等活动。每次定义一个类对象或 new 表达式分配一个类对象都会调用它。 构造函数的名字必须与类名相同。 const 和 volatile 成员函数只有被声明为 const 的成员函数才能被一个 const 对象调用。关键字 const 在函数体和参数表之间. 通常一个类如果想被广泛使用，其中不修改类数据成员的成员函数应该声明为 const 成员函数。 但是即使声明了，这个成员函数依然有可能修改数据成员，如果这个类含有指针。 比如： 12345private: char *_text; _text 不能被修改，但是它指向的字符却是可以被修改的。所以程序员这个时候就需要注意了。。 构造函数和析构函数即使不是 const 成员函数，也能被 const 对象调用。 volatile 跟 const 用法一样，它用来提示编译器该对象的值可能在编译器未被检测到的情况下被修改。因为，编译器不能武断的对引用这些对象的代码进行优化。 mutable 数据成员一旦一个对象被声明为 const，它的内容就不能修改。但是其中某些数据成员，比如索引，被修改之后并没有修改对象本身，这个时候就可以将该数据成员（也就是索引）声明为 mutable. 那样，即使 const 对象， const 成员函数修改了该数据成员，也不会有编译错误。 隐含的指针 this每个类成员函数都含有一个指向被调用对象的指针。但是一般不需要显示的写出来，如果写出来也是可以的。 何时使用指针呢？当连续使用成员函数时，比如 myVector.find().sort().insert() 时，对应的成员函数返回的值应该是被调用对象本身，也就是 *this. 还有一种情况， copy 函数： 123456789101112131415void Screen::copy(const Screen&amp; sobj){ if (this != &amp;sobj) { // 把 sobj 的值拷贝到 * this 中 }} 这里的 this 指针含有被调用对象的地址。如果 sobj 的地址 &amp;sobj 与 this 相同，那就不需要拷贝了。 注意这里 &amp; 用法：引用和取地址。 静态类成员静态数据成员对于非静态成员，每个类都有一个自己的拷贝。而静态成员对每个类类型只有一个拷贝。静态数据成员只有一份，由该类类型的所有对象共同访问。 不同于全局对象，它可以隐藏，并且不会与其他全局名字冲突。 关键字 static. 注意与 const 的区别，没有加 const 意味着是可以更新的。只需要更新一次，所有的类对象对应的值都会更新。 静态类成员的显示初始化，在类定义之外，用类名限定修饰。在静态数据成员的定义中也可以直接使用私有成员，这与在类成员函数中直接引用私有成员是一样的。 对于静态数据成员，除了通过类对象使用成员访问操作符访问之外，还可以直接使用类名加限定修饰符访问 123Account::_intersetRate 静态成员函数静态成员函数，只访问静态数据成员，而不访问任何其他数据成员。所以它们与哪个对象来调用这个函数无关。 在类体中声明时需要加关键字 static, 类体外不能指定关键字。并且不能设定为 const 和 volatile. 指向类成员的指针普通函数指针 7.9节成员函数指针","link":"/2018/12/12/Cplusplus-prime/"},{"title":"CSAPP-01.A tour of computer system","text":"CSAPP 第一章 1.1 信息就是位＋上下文 位（bit）由值０和１组成，一个字节有８位。 一个字长，对32位机器是4个字节，对64位机器是8个字节。 1.2 程序被翻译成其他不同的格式从源程序到可执行目标文件的过程分为4个阶段： 执行这4个阶段的程序（预处理器、编译器、汇编器和链接器）一起构成了编译系统。 hello.c（源程序，文本文件） –&gt; hello.i（添加了头文件的源程序，文本） –&gt; hello.s（汇编程序，文本） –&gt; hello.0（目标程序，二进制）–&gt; hello（可执行的目标程序，二进制） 1.3 为什么要了解编译系统是如何工作的 优化程序性能 为了在C程序中做出好的编码选择，需要去了解一起机器代码以及编译器将不同的C语句转化为机器代码的方式。 例如： 一个函数调用的开销有多大？ while循环比for循环更有效吗？ 指针引用比数组引用更有效吗？ 第3章会介绍编译器如何把不同的C语言结构转换成他们的机器语言的。第5章会学习简单的转换C语言代码。 理解链接时出现的错误 链接器报告无法解析一个引用，这是什么意思？ 静态变量和全局变量的区别是什么？ 不同的C文件中定义了两个相同的全局变量会发生什么？ 为什么有些链接错误直到运行时才会出现？ 第7章会解释这些。 避免安全漏洞 缓冲区溢出错误。第3章会描述堆栈原理和缓冲区溢出错误。 1.4 处理器读并解释存储在存储器中的指令1.4.1 系统硬件的组成 总线： 每次传送1个字。 I/O 设备： I/O总线和I/O设备之间传递信息。 主存： 一组动态随机存取存储器(DRAM)芯片组成。存储器是一个线性字节数组，每个字节都有其唯一的地址（即数组索引）。 处理器： 中央处理单元（CPU），解释（或执行）在主存中指令的引擎。处理器核心是一个字长的存储设备（或寄存器），称为程序计数器（PC）。在任何时刻，PC都指向主存中某条机器语言指令（即含有该条指令的地址）。 处理器是按照一个简单的指令执行模型来操作的，这个模型是由指令集结构决定的。PC读取指令，解释指令中的位，执行该指令的简单操作，然后更新PC，使其指向下一个指令，而这条指令并不一定与存储器中刚刚执行的指令相邻。 这样的操作是围绕主存、寄存器文件(register file)、算术逻辑单元（ALU）进行的。 CPU指令可能会要求如下操作： 加载：一个字节或一个字，从主存到寄存器 存储：一个字节或一个字，从寄存器到主存 操作：两个寄存器的内容复制到ALU，进行算术操作 跳转：从指令本身中抽取一个字，复制到PC中。 1.4.2 运行hello程序 图中黑色的线就是整个流程。shell外壳程序将字符读入寄存器，再存放到存储器中。一旦代码和数据加载到主存中，处理器就开始执行hello程序中的main程序中的机器语言指令。 利用存储器存取的技术，数据可以不通过处理器直接从磁盘到达主存。如下图： 然后从主存复制到寄存器文件中，再从寄存器复制到显示设备上。过程如下图： 机器指令最初是放在磁盘上的， 然后加载程序，复制到主存 处理器运行程序时，指令从主存到处理器 同样的，“hello word” 初始是在磁盘上的，然后复制到主存，最后从主存复制到显示器。 1.5 高速缓存至关重要可以发现这些复制减缓了程序的速度。 而读取速度： 存储器 &gt;&gt; 寄存器（存放几百字节） &gt;&gt; 主存（存放几十亿字节） &gt;&gt; 磁盘（可能比主存大1000倍，比如2T的硬盘，2G的内存.） 因此有了高速缓存存储器。 L1、L2高速缓存用一种静态随机访问存储器（SRAM）的硬件技术实现的。让高速缓存存放可能经常访问的数据时，可以大大提高程序的性能。 1.6 存储设备形成层次结构 存储器层次存储的主要思想是，速度更快的一层作为下一层的缓存区。 1.7 操作系统管理硬件 操作系统可以看做应用程序和硬件之间的一层软件。 1.7.1 进程一个CPU看上去像是在并发的执行多个进程，这是通过处理器在进程间切换实现的。操作系统实现这种交错执行的机制称为上下文切换。第8章会详细介绍。 1.7.2 线程一个进程实际上可以由多个称为线程的执行单元组成，每个线程都运行在进程的上下文中，并共享同样的代码和全局数据。多线程比多进程更容易共享数据，线程一般来说比进程更高效。 1.7.3 虚拟内存 程序代码和数据 堆：在运行时动态扩展和收缩。 共享库 栈：用户栈，编译器用它来实现函数调用。 内核虚拟内存：为内核保留的。 1.7.4 文件文件就是字节序列。每个I/O设备都可以看成是文件。系统的所有输入输出都是通过使用一小组称为Unix I/O的系统函数调用读写文件实现的。第10章会学习Unix I/O 1.8 系统之间利用网络通信 网络也是一种I/O设备。 这种客户端与服务器之间交互的类型在所有网络应用中是非常典型的。在第11章中，你将会学会如何构造网络应用程序，并创建一个简单的Web服务器。Exciting! 1.9 重要主题1.9.1 Amdahl定律系统运行时间 $T_{old}$, 系统某部分执行时间占比 $\\alpha(0&lt;\\alpha &lt;1)$, 该部分提升性能比例 $k(k&gt;1)$,则总的执行时间： $$T_{new}=(1-\\alpha)T_{old}+(\\alpha T_{old})/k = T_{old}[(1-\\alpha)+\\alpha/k]$$ $$S=T_{old}/T_{new} = \\dfrac{1}{(1-\\alpha)+\\alpha/k}&lt;k$$ 简单的数学运算，证明只提升系统某一部分的性能，对整体的加速明显小于这一部分提升的比例。也就是说，要加速整个系统，必须加速这个系统中的大部分的速度。 练习题 1.1 T_{old}=25 T_{new}=1500/150+(2500-1500)/100=20 加速比： T_{old}/T_{new}=1.25 1.9.2 并发（concurrency）和并行（parallelism） 线程级并发 多核处理器： 超线程，有时又称为同时多线程（simultaneous multi-threading），是一项允许一个CPU执行多个控制流的技术。第12章会更深入的讨论并发。 指令集并行 最近的处理器可以保持每个始终2~4条指令的执行效率。其实每条指令从开始到结束需要长得多的时间，大约20个或更多个周期。在第4章，会研究流水线(pipelining)的使用。在流水线中，将执行一条指令所需要的活动划分成不同的步骤，将处理器的硬件组织成一些列的阶段，每个阶段执行一个步骤。 处理器达到比一个周期一条指令更快的执行速率，就称之为超标量（super-scalar）处理器。 单指令、多数据并行 一条指令产生多个可以并行执行的操作，这种方式称为单指令、多数据，即 SIMD 并行。 1.9.3 计算机系统中抽象的重要性 文件是对 I/O 设备的抽象，虚拟内存是对程序存储器的抽象，而进程是对一个正在运行的程序的抽象。 虚拟机是对整个计算机的抽象。 1.10 小结","link":"/2018/06/08/CSAPP-01-A-tour-of-computer-system/"},{"title":"AI challenger 参会记录","text":"答辩听了观点型阅读理解和细粒度情感分类两个，相对来说后者更加干货满满，大佬云集的，基本上代表了国内 NLP 的四座大山，清/北/中科院/哈工大。造成前者干货较少的主要原因作为主持人的搜狗大佬也说了， BERT 的提出在阅读理解这样更加需要上下文理解任务的提升实在太多，使得选手的其它工作都黯然失色，导致大家的模型都趋向同一化。而 BERT 对于分类任务的提升就相对较少了，所以下午的答辩显得更加丰富，各种操作和 trick. 但也有选手说 BERT 作为单模型对这个分类任务依然能取得很不错的效果，所以 BERT 是真强啊 因为答辩的屏幕是真小，根本看不清楚。。所以记录会很零散，也许只是些关键词，后续还需要自行 google. 观点型阅读理解 取得好成绩的主要操作： 通过简单的正则匹配将三个观点转化为作为 “正/负/无法确定” 的三分类问题。训练集中 95% 的数据可以很准确的转化为这种形式，还有 5% 的是实体类问题，比如 “韩国/美国/无法确定”，有选手的做法是将 query 中对两个实体进行排序，比如韩国在前，美国在后。同样对应的 answer 就是 “韩国/美国/无法确定”. 将文本理解的问题，转换为分类问题之后，对整个模型的复杂度需求就降低太多了。但事实上，这是数据 bug … 模型关键词： BERT multiway attention + R-Net RCZoo 浙大大佬的：多层 LSTM 模型，浅层+主要+深层 三个 loss 优化。具体忘了拍照，以及真的看不清楚。。 基于 query 的 attention 还是基于 passage 的 attention 作为最终的 answer selection/matching. 说句不马后炮的话，这里面大部分我也都想到了啊，只是做与没做，以及用与没用 BERT 。。。 细粒度用户评论情感分析 seq2seq 选手这么做的原因是 他觉得各个 粒度 之间存在一定的关联，所以采用 decoder 的形式能有效的利用这些信息。很神奇的操作，是否真的有效朱小燕老师有问到，好像作者并没有做对照实验。 ELMo 提升最多 改进的注意力机制，其实就是 multi-head attention PRAUC 损失函数， 这个我好像在哪儿见过，我不记得了 大佬感觉可以发 paper 了。。 others其他的也很强，但没有 seq2seq 这么具有特殊性，所以可以一起说。 词嵌入部分微调，没太懂？ 哪一部分微调，以及非监督的情况下，如何保证微调的程度 F1 指标的优化，这个对于 unbalanced 数据看起来比 过/欠 采样有效。以及刘洋老师提到的可以基于 rainforce 对 F1 进行优化 附上刘洋老师照片一张，侧脸看起来真像李健啊，都是清华男神吧～ 伪朴素贝叶斯特征，PPT 里面说的很清楚～每次输入几个样本其提取的是局部特征，而伪朴素贝叶斯特征能体现一个词的全局特征。感觉很棒啊 数据增强方式： drop words 随机 mask shuffle words 打乱词序 组合增强策略 对抗训练 模型集成： 贪婪式模型选择 简单概率平均，最后采取了这种。。。anyway 根据验证集调整分类阈值，对当前的验证集当然会有较大提升。但是对于 测试集 可能出现过拟合，引入正则化和 Ensamble 策略。 $$b_i^j=\\text{argmax}_b[\\text{marco-}F_1(S^j[:,i]+b)-C|b]$$ 第 j 个情感要素第 i 类别上的偏置， C&gt;0 为正则系数。 还有些关键词，有些来不及拍照。。 BiSRU 未完待续。。","link":"/2018/12/19/AI-challenger-%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/"},{"title":"chapter14 dependency Parsing1","text":"CS224d-Lecture6:Dependency Parsing Dependency ParsingUnlabled Dependency Parses root is a special root symbol Each dependency is a pair (h, m) where h is the index of a head word, m is the index of a modifier word. In the figures, we represent a dependency (h, m) by a directed edge from h to m Dependencies in the above example are (0, 2), (2, 1), (2, 4), and (4, 3). (We take 0 to be the root symbol.) Conditions on Dependency Structures 从root到任何一个word都有一条直接路径 no crossing (比如右下角的笔记就不行～) 对于 “John saw Mary” 有5中dependency parse 有crossing的结构叫 non-projective structure dependency parsing resource: Conll 2007 McDonald dependency banks 一个 treebank 通过 lexicalization 可以转换成 dependency bank.也就是一个lexicalizated PCFG可以转换为一个 dependency bank. efficiency of dependency parsing dynamic programming - Jason Eisner very efficiencyat Parsing very useful representations CS224什么是dependency structure describing the structure of a sentence by taking each word and saying what it’s a dependent on.So, if it’s a word that kind of modifies or is an argument of another word that you’re saying, it’s a dependent of that word. ambiguity: PP attachments attachment ambiguities:A key parsing decision is how we ‘attach’ vairous constituents PPs 介词短语, adverbial or participial phrase 副词和分词短语, infinitives 不定式, coordinations 并列关系 Catalan numbers: $C_n=(2n)!/[n+1]!n!$ ??需要在查资料!! 人工标注也就太太麻烦了～ dependency Grammar and Dependency structureuniversal dependency the arrow connects a head (governor, superior, regent) with a dependent (modifier, inferior, subordinate) usually, dependencies from a tree(connected, acyclic 非周期的, single-head) Question: compare CFGs and PCFGs and do they, dependency grammars look strongly lexicalized,they’re between words and does that makes it harder to generalize? dependency conditioning preferences关于如何确定dependency? 有以下几点perferences？ dependency parsing 这里需要注意一点：是否有交叉 crossing(non-projective). 在图中的例子中，是有crossing的。 但是dependency tree的定义是 no crossing。 methods of dependency Parsing 第4种方法是目前最主流的方法。 paper presentation: Improving distributional similarity with lessions learned from word embeddings, Omer Levy, Yoav Goldberg, Ido Dagan shift reduced parsing？？？ Greedy transition-based parsing The parser: a stack $\\sigma$， written with top to the right which starts with the ROOT symbol a buffer $beta$, written with the top to the left which starts with the input sentence s set of dependency arcs A which starts off empty a set of actions $Left-Arc_r$ $\\sigma|w_i|w_j,\\beta,A \\rightarrow \\sigma|w_j, \\beta, A\\bigcup{r(w_j,w_i)}$ 表示stack $\\sigma$ 中的两个元素 $w_i\\leftarrow w_j$ MaltParser Question: dependency parsing的准确率是否会出现waterfall般的下降，buz one decision will prevent sone other decisions. it’s not bad. dependency parse evaluation suffers much less badly from waterfall effects than CFG parsing when which is worse in that respect. feature representation不太懂这部分。。。 evaluation of dependency parsing why train a neural dependency parser? Indicator Features Revisited. our approach: learn a dense and compact feature representation. a neural dependency parser A Fast and Accurate Dependency Parser using Neural Networks 清华的本科生简直神一样的存在。。。 distributed representations Model Architecture Non-linearities between layers: Why they are needed Google announcement of Parsey McParseface, SyntaxNet","link":"/2018/04/23/chapter14-dependency-Parsing/"},{"title":"chapter12-句法分析","text":"ambiguous and disambiguation PCFGs 如何从语料库中得到PCFGs，极大似然估计，也就是计算频率 Chomsky 范式 SKY，一种bottom-up的dynamic programming算法～ 前言： 为什么要做句法分析呢？主要是避免歧义。在 grammar checking，semantic analysis，question answering， information extraction 中都有很重要的作用。 如何从Treebanks中找到某一个sentence的tree呢？这个章节中会介绍一种动态规划的算法 Cocke-Kasami-Younger(CKY). 歧义 Ambiguity在词性标注中我们遇到了 part-of-speech ambiguity and part-of-speech disambiguation, 在句法分析中也会遇到类似的问题 structure ambiguity. 举个栗子： 先有这样的一个简单的 Treebank $L_1$. 对于同样的一句话，可以从上述Treebank中生成两种 parse tree. I shot an elephant in my pajamas. 显然右边的parse tree才是我们想要的，但左边的也是满足 $L_1$ 的语法规则的。 结构歧义通常分为两种：附着歧义(attachment ambiguity),并列歧义(coordination ambiguity。 事实上，在自然语言处理中，会有很多语法正确，但是语义上不合理的parse。因此我们需要句法消歧 syntactic disambiguation.一个有效的句法消歧算法需要很多语义semantic和语境contextual知识, 但幸运的是，统计的方法就可以很好的解决～ Probabilistic Context-Free Grammars (PCFGs)Speech and language Processing 这本书中关于PCFG内容实在是有点多，而且纯英文的难度看起来真的有点大。。。于是找到Columbia University，Michael Collins 的NLP课程，相比之下内容少了很多，而且老师讲的也很清楚～所以这部分以Michael Collins的NLP讲义为主。 context-free grammars上下文无关语法CFG可定义为4元祖 $G=(N,\\Sigma, R, S)$ (left-most) Derivations给定一个上下文无关语法，一个left-most derivation是一个序列 $s_1…s_n$,其中： $s_1 = S$, the start symbol $s_n\\in \\Sigma^*$, $\\Sigma^*$ 表示任何从 $\\Sigma$ 中得到的words组成 $s_i$, 表示找到 $s_{i-1}$ 中left-most的非终极符号X，并用 $\\beta$ 代替它，并且 $X\\rightarrow \\beta \\in R$ Probabilistic Context-Free Grammars (PCFGs)用 $T_G$ 表示对应与上下文无关语法G所有的left-most 推导的parse tree. 如果 $s\\in \\Sigma^* $, 那么 $T_G(s)$ 表示： $$t:t\\in T_G, yield(t)=s$$ $T_G(s)$ 表示在语法G下，s对应的所有parse tree的集合。 如果一个sentence是有歧义ambiguous的,那么 $|T_G(s)|\\ge 1$ 如果一个sentence是符合语法G的grammatical, 那么 $|T_G(s)|\\ge 0$ 给一个可能的推导，也就是parse tree一个概率,p(t),那么对于任意 $t\\in T_G$: $$p(t)\\ge 0$$ 并且： $$\\sum_{t\\in T_G}p(t)=1$$ 咋一看，这似乎很复杂。每一个parse tree就很复杂，然后这样的parse tree可能是infinite.那么如何定义概率分布p(t)呢？ 知道了p(t)，我们也就知道了一个给定的sentence对应的最有可能的parse tree.也就是： $$arg\\ max_{t\\in T_G(s)p(t)}$$ 有了这个我们就能很好的消除语言中存在的ambiguous了～～～ 那么问题来了： How do we define the function p(t)? How do we learn the parameters of our model of p(t) from training examples? For a given sentence s, how do we find the most likely tree, namely $$arg\\ max_{t\\in T_G(s)p(t)}?$$ definition of PCFGs一个PCFG包括： 上下文无关语法 $G=(N,\\Sigma,S,R)$ 参数 $$q(\\alpha \\rightarrow \\beta)$$ 那么对于 $t\\in T_G$包含rules $\\alpha_2\\rightarrow \\beta_1,\\alpha_2\\rightarrow \\beta_2,…,\\alpha_n\\rightarrow \\beta_n$，则有： $$p(t)=\\prod_{i=1}^nq(\\alpha_i\\rightarrow \\beta_i)$$ 那么对于所有的非终极符号 $X\\in N$: $$\\sum_{\\alpha\\rightarrow \\beta \\in R:\\alpha=X}q(\\alpha\\rightarrow \\beta)=1$$ 也就是从一个非终极符号展开的概率之和为1. 对于一个parse tree的概率，举个例子： 根据直觉，可以将parse tree的生成看做随机过程 stochastical process，其过程如下： Deriving a PCFG from a CorpusHaving defined PCFGs, the next question is the following: how do we derive a PCFG from a corpus? We will assume a set of training data, which is simply a set of parse trees $t_1,t_2,…,t_m$. $yield(t_i)$ is the i’th sentence in the corpus. Each parse tree t i is a sequence of context-free rules: we assume that every parse tree in our corpus has the same symbol, S, at its root. We can then define a PCFG (N, Σ, S, R, q) as follows: 有点疑惑： 一个语料库对应一个PCFG 一个语料库中所有的parse tree的start symbol是相同的S $t_1,t_2,…,t_m$ 具体有多少我们并不知道，也不需要知道吧。。但是有多少sentences是知道的。 比如figure5中显示的那样～ Chomsky Normal Form collins 教授举的例子： VP -&gt; VT NP PP 0.2 转换成 Chomsky Normal Form: VP -&gt; VT-NP 0.2 VP-NT -&gt; VT NP 1 总结下这个过程就是： Parsing using the CKY Algorithm在前面最小编辑距离，Viterbi，Froward算法中，都是先填写表格，表格中包含有所有的子问题。而在句法分析也是类似的，子问题是展示所有成分的parse tree，也可以展示在表格中，但它是三维的，因为不仅仅包含句子长度这一维度，还有对应的短语结构. 对于一个sentence $s=x_1\\cdots x_n$,其中 $x_i$ 表示句子中第i个词。也就是找出概率最大的parse tree $$arg\\ max_{t\\in T_G}p(t)$$ CKY算法是一个动态规划算法～ 具体思路： 给sentence标上index fill the table $(n+1)\\times (n+1)$， 每个 cell 对应一个三维的量 $\\pi[i,j,X]$,$X\\in N, 1\\le i\\le j\\le n$, 其中non-terminal总个数有V个，那么 table 的维度是 $(n+1)\\times (n+1)\\times V$ sentence其中的一部分 $x_i\\cdots x_j$，并且对应的root为非终极符号X. $$\\pi(i,j,X) = max_{t\\in T(i,j,X)}p(t)$$ 那么我们要求的sentence对应的parse tree： $$\\pi(1,n,S) = arg\\ max_{t\\in T_G(s)}$$ 用递归的方法填写table 这里是一种 bottom-up,从下到上的方法，只需要矩阵的上三角部分～ Initialization 初始值， 也就是主对角线的值 $$\\pi(i,i,X) =\\begin{cases} q(X\\rightarrow x_i), &amp; \\text{if $X \\rightarrow x_i \\in$ R} \\ 0, &amp; \\text{otherwise} \\end{cases} $$ 递归 recursive $$\\pi(i,j,X)=max_{X\\rightarrow Y Z \\in R,s\\in {i…(j-1)}}(q(X\\rightarrow Y Z)\\times \\pi(i,s,Y)\\times \\pi(s+1,j,Z))\\tag{1}$$ 伪代码： 填表的过程： The algorithm fills in the $\\pi$ values bottom-up: first the $\\pi(i, i, X)$ values, using the base case in the recursion; then the values for $\\pi(i, j, X)$ such that j = i + 1; then the values for $\\pi(i, j, X)$ such that j = i + 2; and so on. Justification for the Algorithm 考虑 $\\pi(3,8,VP)$ 在语法中 $VP\\rightarrow Y Z$ 只有两种情况 $VP \\rightarrow Vt\\ NP\\ and\\ VP\\rightarrow VP\\ PP$，那么 $\\pi(3,3)$ 这个cell有两个维度，同样的 $\\pi(3,4),…,\\pi(8,8)$ 都有两个维度. 总的计算复杂度是 $O(n^3|N|^3)$,怎么来的呢？ table中选择cell[i,j],复杂度 $n^2$ 对于每个cell[i,j]要考虑 N 个non-terminal,所以是 $n^2N$ 然后 $X\\rightarrow Y Z$ Y Z都有 N 种情况，所以 $n^2N^3$ 然后 $s\\in {i…(j-1)}$ 所以是 $O(n^3|N|^3)$ 整个回顾下，就是我们先从下到上 bottom-up 填写三维table[i,j,V] ，然后直接计算 $\\pi(1,n,S)$， 同样的类似于HMM中需要backpoints. weaknesses of PCFGs Lack of sensitivity to lexical information Lack of sensitivity to structural frequencies Lack of sensitivity to lexical information 在PCFGs中 $NNP\\rightarrow IBM$ 是一个独立的过程，并没有考虑到其他的words，这在NLP中显然是不合理的。 附着歧义(attachment ambiguity）: 可以看到对于同一句话的两种parse tree,它们的区别只有图中黑色加粗的rule，那么比较两个parse tree那个更好仅仅取决于 $q(NP\\rightarrow NP PP) q(VP\\rightarrow VP PP)$ 的大小。 边上的举例 dumped sacks into bin 没太懂，准确率是怎么来的。。。 Lack of sensitivity to structural frequencies close attachment: 通过两个例子，说明PCFGs的劣势，failed to capture the structural preferences. 两个parse tree的rules完全相同，那么对应的概率p(t)也一样。PCFGs failed to display a preference for one parse tree or the other and in particular it completely ignores the lexical information. 参考： Natural Language Processing，Columbia University，Michael Collins youtobe,Natural Language Processing Speech and language Processing","link":"/2018/04/20/chapter12-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"title":"chapter10-词性标注","text":"词性分类 clossed class, open Classes 标记集 Tagset HMM tagging: 生成模型 $p(t_i|w_i)=p(w_i|t_i)p(t_i|t_{i-1})$ 有viterbi算法和greedy算法～ MEMM: 判别模型 $p(t_i|w_i,t_{i-1},…f;w)$ 也有Viterbi和greedy两种算法，更新参数w～ 双向模型 CRF，chapter20讲 前言： 词性标注（parts-of-speech）又称 (pos, word classes, or syntactic categories) 分为8类： noun, verb, pronoun, preposition, adverb, conjunction, participle, and article.（名词，动词，代词，介词，副词，连词，分词和文章。） 词性标注在寻找命名实体（named entities）和其他一些 信息提取(information extraction) 的工作中是很重要的特征。词性标注也会影响形态词缀(morphological affixes)，从而影响词干(stemming)的信息检索;在语音识别中对语音的生成也有很重要的作用，比如CONtent是名词，而conTENT是形容词。 (Mostly) English Word Classes传统上，词类根据形态和语法功能来分类： distributional properties 分布特征:单词出现在相似的环境中； morphological properties 形态特征：单词的词缀具有相似的功能。 词类可分为两大类：封闭类 closed class types 和 开放类 open class type. 封闭类：prepositions 介词，function words， like，of，it,…. 一般都很短，而且频率高;开放类：nouns, verbs,adjectives, and adverbs noun 名词: 专有名词(Proper nouns)和普通名词（common nouns）。专有名词一般不受冠词限制，且一地个字母要大写。普通名词又分为可数名词（count nouns）和不可数名词（mass nouns）. 当某种东西能在概念上按照同质来分组时，就使用物质名词，这样的词是不可数的。（Mass nouns are used when something is conceptualized as a homogeneous group）. verb 动词： 用来表示动作或过程。动词有若干形态，(non-third-person-sg非第三人称单数 (eat), third-person-sg第三人称单数 (eats), 进行时progressive (eating), 过去分词past participle (eaten)). adjectives 形容词： 描述性质和质量的单词。 adverbs 副词： 无论是从语义上还是形态上，都比较杂。通常用来修饰动词，也可以用来修饰其他副词或是动词短语。方位副词和地点副词 Directional adverbs or locative adverbs， 程度副词 degree adverbs (extremely, very, somewhat); 方式副词 manner adverbs (slowly, slinkily, delicately); 时间副词 temporal adverbs (yesterday, Monday). 封闭类： preposition 介词: 出现在名词短语之前，从语义上讲，他们是表示关系的。 particle 小品词： 与介词或副词相似，经常和动词结合，形成动词短语。 determiner 限定词: 其中包括 article 冠词，a,an,the. 其他的限定词，比如 this,that conjunction 连词： 连接两个短语、分句或句子。 pronoun 代词： 简短地援引某些名词短语、实体或事件的一种形式。 auxiliary 助动词： 包括系动词be,两个动词do, have,以及情态动词。 英语中还有很多 叹词(oh,ah,hey,man,alas),否定词(no,not),礼貌标志词(please,thank you). 是否把这些词放在一起,取决于标记的目的。 The Penn Treebank Part-of-Speech Tagset 标记集 Peen Treebank标记集是Brown语料库原有的87个标记集中挑选出来的。还有两个比较大的标记集 C5,C7. 词性标注词性标注(pos tagging)与计算机语言的 tokenization （词形还原？分词） 过程是一致的，但词性标注具有更多的歧义性。在Brown语料库中，只有11.5%的英语词型(word type)是具有歧义的,40%以上的词例(word token)有歧义的。 chapter2中几个容易混淆的概念： 词型 word type 不包括重复词， 词例 word token 包括重复词。 lemma 是词意，am is are是同一个单词be Wordform 是词的形状。 词性标注是歧义消解（disambiguation）的一个重要方面，大多数的标注算法分为两类：基于规则的标注算法(rule-based tagger)，一类是随机标注算法(stochastic tagger). HMM Part-of-Speech Tagging基于隐马尔可夫的词性标注，语料库是观察序列，part-of-speech是隐藏状态,对于Peen Treebank标记集有45中隐藏状态。因为训练数据是有人工标注的，所以我们的目标是，根据已知的观察序列和对应的隐藏状态，可以根据极大似然估计或者相对频率计算出状态转移矩阵和发射矩阵的参数～ 所以词性标注的问题和预测问题是一样的～可以使用viterbi算法，对带标注的序列进行标注～ The basic equation of HMM TaggingHMM decoding:求概率最大的tagging序列 $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)$$ 使用bayes公式： $$\\hat t_1^n=argmax_{t_1^n}\\dfrac{P(w_1^n|t_1^n)P(t_1^n)}{P(w_1^n)}$$ 目的是让观察序列的似然概率最大化～因此观察序列对于任何隐藏序列都是一样的～故可以直接去掉分母： $$\\hat t_1^n=argmax_{t_1^n}P(w_1^n|t_1^n)P(t_1^n)$$ 根据概率图模型中，一个word的生成只取决于其对应的tag，而与其他word和word无关，也就是条件独立可得： $$P(w_1^n|t_1^n)\\approx \\prod_{i=1}^nP(w_i|t_i)$$ 根据bigram假设： $$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1})$$ 联立可得： $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)\\approx argmaxmax_{t_1^n}\\prod_{i=1}^nP(w_i|t_i)P(t_i|t_{i-1})\\tag{10.8}$$ Estimating probabilities 概率估计在HMM tagging中，概率估计是直接通过对训练语料库中进行计数得到的。 状态转移概率 transition probabilities: $P(t_i|t_{i-1})$,shape=(45,45) $$P(t_i|t_{i-1})=\\dfrac{C(t_{i-1},t_i)}{C(t_{i-1})}$$ 发射概率 emission probabilities: $P(w_i|t_i)$,shape=(45, V) $$P(w_i|t_i)=\\dfrac{C(t_i,w_i)}{C(t_i)}$$ Working through an example举个栗子 其中状态转移概率和发射概率依据已经标记好的语料库WSJ corpus计算得到，其对应的状态转移矩阵A和发射矩阵B： 那么对应的可能的状态序列如下图，加粗的黑色路径是概率最大的状态序列。 Viterbi算法先回顾下隐马尔科夫模型中的viterbi算法： 这是在给定模型 $\\lambda=(A,B)$ 的情况下，寻找状态序列使得观察序列的似然概率最大～ 矩阵 viterbi [N+2,T], 第一行和第二行是 states 0 和 $q_F$ 从state 1 开始： $$v_t(j)=\\max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t)$$ 可以看到，行代表的是[start, NNP, MD,…, DT, end]^T,总共 N+2 中状态。 列代表的是时间步 [1,2,3…,t],因为t=0时刻，肯定是start状态，没有emission. 从第一列 t=1 到 第二列 t=2，每一步都是前一步的N种状态转移到当前状态的max. Extending the HMM Algorithm to Trigrams$$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1})$$ 改为： $$P(t_1^n)\\approx \\prod_{i=1}^nP(t_i|t_{i-1},t_{t-2})$$ 每一步的计算复杂度，从N变成 $N^2$. 本书上写的，当前 state-of-art 的HMM 标注算法是 A statistical part-of-speech tagger. In ANLP 2000, Seattle. 论文作者让标注者在每句话结尾处加上 end-of-sequence marker for $t_{n+1}$. 这里的n表示序列长度。 $$\\hat t_1^n = argmax_{t_1^n}P(t_1^n|w_1^n)\\approx argmaxmax_{t_1^n}[\\prod_{i=1}^nP(w_i|t_i)P(t_i|t_{i-1},t_{i-2})]P(t_{n+1}|t_n)$$ 其中，转移概率可以通过语料库计数得到： $$P(t_i|t_{i-1},t_{i-2})=\\dfrac{C(t_{i-2},t_{i-1},t_i)}{C(t_{i-2},t_{i-1})}$$ 但是在测试集中，很可能遇到状态序列（也就是tag序列） $t_{i-2},t_{i-1},t_i$ 在训练集中从未出现过，也就是其概率为zeros，这样就无法对测试集中的序列进行标注了。也就是语言模型中提到的zeros情况～比如图10.7中，某个时间步没有可选择的隐藏状态tag 同样，我们也可以使用插值法～ 其中 $\\lambda$ 的计算可以用 deleted interpolation 不太理解。。。 Unknown WordsSamuelsson (1993) and Brants (2000)根据形态（morphology）来判断unknown word的可能状态. 比如 -s 通常是复数名词NNS, -ed通常是过去时态VBN, -able通常是形容词 JJ…. $$P(t_i|l_{n-i+1}…l_n)$$ 这个概率也是可以通过语料库中相对频率计算得到～ 他们使用尽量短的后缀（shorter and shorter suffixes）应用回退smoothing方法来估计unknow word的可能状态，但要尽量避免其状态为 closed class,比如介词 prepositions,可以将unknow word的状态tag选择从频率 $\\le 10$ 中选择，或者只从open classes 中选择可能的tag. Brants(2000) 还额外使用了首字母的特征信息，将公式(10.21)可改写为： $$P(t_i,c_i|t_{i-1},c_{i-1},t_{i-2},c_{i-2})$$ 这样语料库中标记集就包括首字母大写和小写两种版本，其对应的标记集tagset就增大为2倍。 state-of-art HMM tagging也就是论文Brants(2000) 的准确率达到 96.7% 在使用Penn Treebank标记集。 Maximum Entropy Markov Models最大熵隐马尔可夫模型，是依据logistic回归的，所以它是判别模型 discriminative sequence model, 而HMM是生成模型 generative sequence model. sequence of words: W = $w_1^n$ sequence of tags: T = $t_1^n$ 那么HMM模型的P(T|W)是依据bayes规则和最大似然P(w|T)得到的： 而在最大熵隐马尔科夫模型，直接计算后验概率 posterior P(T|W). $$\\hat T = argmax_TP(T|W) = argmax_T\\prod_iP(t_i|w_i,t_i-1)$$ 对比HMM和MEMM，HMM计算是在tag的条件下观察序列似然最大， MEMM计算是在观察序列的条件下tag序列似然最大～显然在tag已经标注好的训练集里，判别模型是完全可行的，也许会是更好的～ Features in a MEMM在HMM中，我们得到下一个tag的概率只依赖与前一个或两个tag，以及当前的word，如果想考虑更多的特征，比如首字母capitalization，后缀suffix，那样计算复杂度会增加很多。 相比之下，MEMM可以考虑更多的特征： MEMM可以依赖的特征可以是 word，neighboring words, previous tags, and various tags, and various combinations. 可使用 features templates 来表示： 对于之前的例子： Janet/NNP will/MD back/VB the/DT bill/NN 当 $w_i$ 是 back 时，对应的特征模板 features templates,也就是 know-words feature: 除此之外，还有当前词 $w_i$ 的拼写和形状特征，这些特征对于处理 unknow word 很有必要～ 那么根据上述规则，单词 well-dressed 的 word shape 特征就是： 这样一来，特征就很多很多了，通常需要进行一定的cutoff. 其中 $w_{i-l}^{i+l}$ 表示考虑当前词前后 l 个单词， $t_{i-k}^{i-1}$ 表示考虑前k个tags. Decoding and Training MEMMs 训练MEMMs在MEMMs中，每一步都是一个local classifer，然后make a hard decision,选择概率最大的词。。。以此类推。因此，这是贪心greedy算法～ 虽然这样使用greedy的方法，得到的准确率也还不错～ Viterbi算法原始Viterbi： $$v_t(j) = max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t)$$ HMM tagging: $$v_t(j) = max_{i=1}^Nv_{t-1}P(s_j|s_i)P(o_t|s_j)$$ MEMM: $$v_t(j) = max_{i=1}^Nv_{t-1}P(s_j|s_i,o_t)$$ 书上对如何训练参数一笔带过了，我自己总结如下： 初始化转移矩阵和发射矩阵参数，设为w 根据Viterbi算法填写矩阵 $N\\times T$, 也就是每一个网格用概率 $P(t_i|w_{i-1}^{i+l},t_{i-k}^{i-1})$, 并保留 backpointers 比较tag路径与真实路径，然后反向传播，更新参数权重w, 注意有正则化L1,L2 要更深入的理解MEMM，需要和HMM tagging对比更容易理解～～ HMM是生成模型，它是在求已知 $t_i$ 条件下生成 $w_i$ 的概率 $P(w_i|t_i)$,这根据语料库中的频率是可以计算得到的；然后后验概率 $P(t_i|t_{i-1})$ 也是可以通过频率计算得到的～ 知道似然概率和发射概率后根据bayes公式就可以预测了P(T|W)～ MEMM是判别模型，它是直接求 P(T|W),显然是tag生成word，而不是word生成tag，因此无法直接通过频率，但我们将它分解为每一个时间步计算 $P(t_i|w_i,t_{i-1},…)（各种特征），每一个特征有对应的权重w,然后根据最大熵原理，也就是公式（10.29）所示，从前一个tag $t_{i-1}$ 有N中状态，到下一个tag $t_i$, 是max的过程，但整个过程不是greedy的（这需要好好理解，其实也就是Viterbi一样的。。），然后根据backpoints得到的tag序列与真实tag序列对比，不断更新权重参数w！ Bidirectionality这里举了个例子来阐述双向的重要性。 will/NN to/TO fight/VB 通常 to 都是接在名词NN后面，而不会是情态动词 MD 后面，这里的will也应该是 NN。 但是在 $P(t_{will}|&lt; s &gt;)$， $t_{will}$ 更倾向于是情态动词 MD，而且 $P(TO|t_{will},to)$ 的概率接近于1,无论 $t_{will}$ 是啥，所以这时候 $t_{will}$ 就会错误的标注为 MD. 这样的错误叫 label bias or observation bias, 所以我们需要双向～ 条件随机场 Conditional Random Field or CRF 就是这样的～ 任何sequence model都可以是双向的。比如，对于tagging，可以在第一遍 left to right, 而从二遍开始就可以双向了～ SVMTool system 就是这样的，不过它在每一个时间步使用的不是最大熵分类器，而是SVM分类器，然后用Viterbi算法或者是greedy应用到整个sequence model～ Part-of-Speech Tagging for Other Languages中文的分词可以和标注一起进行～ 中文的未登录词问题～ 总结：","link":"/2018/04/17/chapter10-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"title":"SCAPP-02-信息的表示和处理","text":"CSAPP 第二章 信息的表示和处理三种数字表示： 无符号(unsigned)编码：大于或等于零的数字 补码（two’s-complement）编码：表示有符号整数最常见的方式 浮点数(floating-point)编码： 溢出（overflow）:计算机的表示法是用有限数量的位来对一个数字编码。因此结果太大至无法表示时，某些运算就会溢出。 2.1 信息存储计算机使用8位的块，或者字节（byte）。作为最小的可寻址的内存单元，而不是单独的位。 机器级程序将内存视为一个非常大的数字来标识，称为它的内存。内存的每一个字节都由一个唯一的数字来标识，称为它的地址，所有可能地址的集合称为虚拟地址空间(virtual address space)。 接下来几章将讲述编译器和运行时系统是如何将存储器空间划分为更可管理的单元，来存放不同的程序对象，即程序数据、指令和控制信息。 2.1.1 十六进制表示法一个字节是8位： 对于二进制表示法，值域是 $00000000_2-11111111_2$ 对于十进制表示法，值域是 $0_{10}-255_{10}$ 对十六进制表示法，值域是 $00_{16}-FF_{16}$ 练习题 2.1 A. 将ox39A7F8转换为二进制. 0011 1001 1010 0111 1111 1000 B. 将二进制 1100 1001 0111 1011 转换为十六进制。 oxE97D 练习题 2.2 $2^n=2^{i+4j}$ 可以很容易写成十六进制就是 2^i后面跟着j个0 比如： $2^9$ -&gt; $2^{1+2x4}$ -&gt; ox200 $2^19$ -&gt; $2^{3+4x4}$ -&gt; ox80000 进制的转换 将十进制转换为16进制，其实可以理解为将十进制转换为十进制，就是除以10，余数分别是个位、百位…. 同样的道理，转换为16进制就是除以16 十六进制转换为10进制就更简单了，16的幂乘以每个十六进制数。 2.1.2 字数据大小对于一个字长为w位的机器，虚拟地址的范围为0~$2^w-1$，程序最多访问$2^w$个字节。 2.1.3 寻址和字节顺序在几乎所有的机器上，多字节对象都被存储为连续的字节序列，对象的地址为所使用字节中最小的地址。 假设变量x的类型为int， 位于地址0x100处，它的十六进制值为0x01234567. 十六进制的两位占一个字节，因此其地址范围 0x100~0x103. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include &lt;stdio.h&gt;typedef unsigned char *byte_pointer; // 字符指针// 使用 typedef 来命名数据类型void show_bytes(byte_pointer start, size_t len){ size_t i; for (i=0; i&lt;len; i++) printf(&quot; %.2x&quot;, start[i]); printf(&quot;\\n&quot;);}// size_t 表示size type，大小的类型，意味着它是sizeof运算符结果的类型// 在C语言中，可以用指针表示法来引用数组元素。引用start[i]表示我们想要读取以start指向的位置为起始的第i个位置处的字节。void show_int(int x){ show_bytes((byte_pointer)&amp;x, sizeof(int));}void show_float(float x){ show_bytes((byte_pointer)&amp;x, sizeof(float));}void show_pointer(void *x){ show_bytes((byte_pointer)&amp;x, sizeof(void *));}// C 和 C++ 中独有的操作， C的“取地址”运算符 &amp; 创建一个指针 &amp;x, 这个指针的类型取决于 x 的类型，因此这三个指针类型分别为 int*, float*, void**, 数据类型 void * 是一种特殊类型的指针，没有相关联的类型信息。// (byte_pointer)&amp;x 表示强制转换，无论 &amp;x 之前是什么类型，现在就是一个指向 unsigned char的指针。但这并不会改变真实的指针，只是编译器以新的数据类型来看待被指向的数据。void test_show_bytes(int val){ int ival = val; float fval = (float) ival; int * pval = &amp;ival; show_int(ival); show_float(fval); show_pointer(pval);}int main(){ test_show_bytes(12345); return 0;} 我的机器是Linux64，运行结果是这样的： 分析下为什么是这样: 12345，十六进制为 ox3039, 对于int数据采用的是4字节： 二进制数表示是: 0000 0000 0000 0000 0011 0000 0011 1001 在小端法机器的十六进制表示为: 39 30 00 00 在大端法机器的十六进制表示为: 00 00 30 39 以一个字节为单位，十六进制一个字节占两位，所以每两个数一个字节。二进制是每8个数一个字节。 为什么浮点数 12345.0 的十六进制是这样的呢？之后会讲到～ 练习题 2.5 十六进制数 ox87654321 小端法： 21 43 65 87 A. 小端法： 21 大端法：87 B. 小:21 43 大： 87 65 C. 小:21 43 65 大: 87 65 43 练习题 2.6 3510593 -&gt; ox00359141 -&gt; 0000 0000 0011 0101 1001 0001 0100 0001 3510593.0 -&gt; ox4A564504 -&gt; 0100 1010 0101 0110 0100 0101 0000 0100 最大匹配位数： 2.1.4 表示字符串每个字符对应一个ASCII码。总共有127个ascii码。 十进制数x的ASCII码正好是 0x3x, 比如要显示字符 0，其ASCII码是48，用十六进制就是0x30. ‘A’‘Z’的ASCII码十进制表示是 6590，‘a’‘z’十进制是 97122，十六进制是ox61~0x7A. null的ASCII码是 0，也就是0x00. 1234567891011121314151617181920212223void show_bytes(byte_pointer start, size_t len){ size_t i; for (i=0; i&lt;len; i++) printf(&quot; %.2x&quot;, start[i]); printf(&quot;\\n&quot;);}const char *s = &quot;abcdef&quot;;show_bytes((byte_pointer) s, strlen(s)); 运行结果： 61 62 63 64 65 66 2.1.5 表示代码 不同机器类型使用不同的且不兼容的指令和编码方式。 2.1.6 布尔代数简介0和1的研究～ 非、与、或、异或(表示P或者Q为真，但不同时为真时，P^Q为真) 布尔运算可拓展到位向量的运算： a=[0110], b=[1100], 那么 a&amp;b, a|b, a^b, ~b 分别为： 位向量可以用来表示有限集合。$[a_{w-1},…,a_1,a_0]$（注意是 $a_{w-1}$ 在左边，$a_0$ 在右边） 编码任何子集 $A\\in {0,1,2,…,w-1}$. 比如 a=[00101]就表示 A={0,2}. 其实可以把 a 看做多标签的 one-hot向量。。。 练习题 2.9 蓝色|绿色 = [001]|[010] = [011] = 蓝绿色 红色 ^ 红紫色 = [100]^[101] = [001] = 蓝色 2.1.7 C语言中的位级运算布尔位运算。 从十六进制转换为二进制，进行运算，在转换回十六进制。 练习题 2.10 任意一位向量 a，有 a ^ a = 0. 那么 a^a^b=b. 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;stdio.h&gt;void inplace_swap(int *x, int *y){ * y = * x ^ * y; * x = * x ^ * y; * y = * x ^ * y;}int main(){ int a=5, b=4; printf(&quot;%p, %p\\n&quot;,&amp;a, &amp;b); inplace_swap(&amp;a, &amp;b); printf(&quot;%p, %p\\n&quot;,&amp;a, &amp;b); printf(&quot;%i, %i\\n&quot;, a, b); return 0;} 运行结果： 12345670x7ffe90686210, 0x7ffe906862140x7ffe90686210, 0x7ffe906862144, 5 我们发现这里其实a和b的地址没有改变，是地址对应的值在进行异或运算。 5^4=1, 5^1=4, 4^1=5. 这就是二进制运算，也就是位运算。 6^5=3, 6^3=5, 5^3=6. 不过不要误会，跟加减法没关系，原理还是二进制 a^a=0. 所以能进行交换，就是因为 a^b^a=b. 结果是： step1 a a^b step2 a^(a^b)=b a^b step3 b b^(a^b)=a 练习题 2.11 将 &lt;= 改为 &lt; 即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;stdio.h&gt;void inplace_swap(int *x, int *y){ * y = * x ^ * y; * x = * x ^ * y; * y = * x ^ * y;}void reverse_array(int a[], int cnt){ int first, last; for (first = 0, last = cnt -1; first &lt; last; first++, last--) inplace_swap(&amp;a[first], &amp;a[last]);}int main(){ int a[] = {1,2,3,4,5}; reverse_array(a, 5); for (int i=0; i&lt;5; i++) printf(&quot;%i&quot;, a[i]); printf(&quot;\\n&quot;); return 0;} 2.1.8 C语言中的逻辑运算包括 ||, &amp;&amp; 和 !, 逻辑运算认为所有非零参数为TRUE，参数0 表示 FALSE. 要注意与位运算区分开来。 习题 2.15 if ((x^y)|(x^y)) return 1 else return 0 2.1.9 C语言中的移位运算x &lt;&lt; k: 表示向左移动k位，右边补0 x &gt;&gt; k: 有两种情况： 逻辑右移：向右移动k位，左边补0 算术右移：向右移动k位，左边补最高位的值。 可以看到一定是在一个字节内的，也就是以一个字节为整体。 对于有符号数： 在java中 x&gt;&gt;k 是算术右移， x&gt;&gt;&gt;&gt;k 是逻辑右移。 C语言中没有明确规定，但几乎所有的编译器采用算术右移。也就是补最高位的值。 对于无符号数，右移必须是逻辑右移。 移位运算符的优先级低于加法和减法。 2.2 整数表示整数操作术语： 2.2.1 整型数据类型32位机器： 64位机器： 跟机器相关的整型数据类型就只有 long，在64位中占8个字节。在32位中占4个字节。 可以稍微记一下这些数： 有符号字符， 1个字节。 范围 $-2^7=128 \\sim 2^7-1=127$ 无符号字符， 1个字节。 范围 $0 \\sim 2^8-1=255$ 有符号短整型，2个字节。范围 $-2^15=-32768 \\sim 2^15-1=32767$ 无符号短整型，2个字节。范围 $0 \\sim 2^16-1=65535$ 有符号整型， 4个字节。范围 $-2^31=-2 147 483 648 \\sim 2^31-1=2 147 483 647$ 无符号整型， 4个字节。范围 $0 \\sim 2^32-1=4 294 967 295$ 无符号的数值范围是有符号的两倍，是因为有符号数需要拿出一位来表示符号，所以就小了一倍了。比如：一个字符是一个字节，8位，无符号范围就是 $2^8-1=255$, 有符号就是 $2^7-1=127$. 然而后面介绍了补码，少一位编码是对的，但是负数的计算并不是这样哈哈哈，打脸了，并没有这么简单啦～～～ 这也解释了为啥负数是 -128， 正数是 127. C语言标准定义的每种数据类型必须表示的最小的取值范围。 2.2.2 无符号数的编码将二进制写成向量。 2.2.3 补码编码为了标书负数值，最常见的就是补码（two’s-complement）形式， 将字的最高有效位解释为负权（negative weight）. 最高位的权重是 $-2^{w-1}$,对该数是正是负取决定性作用。其为1时，为负；其为0时，为正。 所以w位补码能表示的范围： $$-2^{w-1} \\sim \\sum_{i=1}^{w-1}2^i=2^{w-1}-1$$ 重要的数字: UMax 表示最大无符号数，UMin没有写，就是0啦 TMax 表示最大有符号数，TMin 表示最小有符号数。 可以发现： 补码不对称，$|TMin| = |TMax| + 1$ 无符号最大值是有符号最大值的2倍大一点。比如一个字节8位为例，(2^8-1) = 2(2^7-1)+1. 练习题2.18 看不懂 2.2.4 有符号数和无符号数之间的转换强制转换保持位值，只是转换了其解释方式。 比如32为的无符号最大值 4294967295($UMax_32$)，与补码形式的-1的位模式是完全一样的。 有符号转无符号 $T2U_w$12345678910111213int main(){ unsigned u = 4294967295u; int tu = (int) u; printf(&quot;u=%u, tu=%d\\n&quot;, u, tu);} 运行结果： 123u=4294967295, tu=-1 补码和无符号码之间的关系，以16位为例： 有符号码： 12345的二进制： 0011 0000 0011 1001 -12345的二进制数： 1100 1111 1100 0111 53191的无符号码和-12345的二进制表示一样。 可以发现 $12345 + 53919 = 2^{16}$ 其实很容易推导出来, 可以得到如下关系： 从符号转无符号，如果是负数 u , 假设除了最高位的数表示为 y，那么： $-2^{w-1} + y = u$ 现在要求无符号： $2^{w-1}+y = u+ 2*2^{w-1}= 2^w + u$ 无符号转有符号 $U2T_w$ $TMax_{w} = 2^{w-1}-1$ 当 $u &gt; TMax_w$ 时，显然转换为有符号应该是负的。 无符号： $2^{w-1}+y = u$ 有符号： $-2^{w-1} +y = u-2*2^{w-1}=u-2^w$ 2.2.5 C语言中的有符号数和无符号数一般都默认为是有符号数，要创建一个无符号数，必须加上后缀 ‘u’ 或 ‘U’. 显示的强制转换： 隐式的强制转换： 输出 %d, %u, %x 分别表示十进制、无符号十进制和十六进制。所以在printf输出时，也会发生转换。 总之，记住输出的类型只是一种解释形式，底层的位值是不变的。 当执行运算时，一个运算数是无符号的，一个是有符号的。那么有符号的会隐式的转换为无符号进行计算。 比如32位机器中 -1 &gt; 0u. 这里的-1是 4294967295u 2.2.6 拓展一个数字的位表示 无符号拓展 $B2U_w(\\overrightarrow u)=B2U_{w’}=(\\overrightarrow u’)$ 在表示的开头添加 w’-w 个0 有符号拓展 $B2T_w(\\overrightarrow x) = B2T_{w’}(\\overrightarrow x’)$ 在表示开头添加 w’-w 个 $x_{w-1}$ 对于无符号很好理解，添加0后值不会改变。但对于有符号的负数，添加1之后，其值也是没有改变的。比如有符号 [101] 表示 -4+1=-3. 扩展一位 [1101]= -8+4+1= -3. 这是因为每扩展一位 $-2^w+2^{w-1}=-2^{w-1}$. 所以不变～～ 2.2.7 截断数字截断无符号数，其实就是取模运算$x=B2U_w(\\overrightarrow x), x’=B2U_k(\\overrightarrow x’)$ 则 x’=x mod $2^k$. 即对 2^k 取余。 截断有符号数$[x_{w-1},x_{w-2},…,x_0]$ 先按照无符号数截断，也就是对 $2^k$ 取余。然后在转换为有符号数。 2.2.8 关于有符号数与无符号数的建议练习题 2.25 出现错误是因为 length 是无符号数，当length=-1时，会转换为 2^31-1. 循环会无限循环下去。 将 unsigned lenght 修改为 int length 即可。 练习题2.26 strlen 返回的 size_t 是unsigned int， 所以 相减为-1之后会转换。 所以改为 int (strlen(s)- strlen(t)) &gt; 0; 无符号数有时候会很有用的。 2.3 整数运算2.3.1 无符号加法x， y是无符号数，0 &lt; x,y &lt; $2^w-1$. 如果溢出，说明 $2^{w} \\le x+y &lt; 2^{w+1}-2$. 需要 w+1 位才能表示出来。 s = x + y. 当溢出时，s = x + y - 2^w 溢出时，w+1位为1，丢弃它相当于从和中减去了 $2^w$。正常情况下，w+1 位为0，不会影响。 检测无符号溢出的条件： 如果溢出：$s = x + y -2^w$, 那么 $s = x+(y-2^w) &lt;x$, 同样 $s = y+(x-2^w)&lt;y$ 习题 2.27 1234567891011int uadd_ok(unsigned x, unsigned y){ unsigned s = x + y; return s &gt;= x;} 无符号数求反模数加法形成一种数据结构，称为阿贝尔群，也就是求 x 的逆元 $-^u_w x$. 什么意思呢，就是 这个逆元加上 x 对2^w 取模等于 0. 所谓模数加法，超过了模 $2^w$, 就是除以模 $2^w$ 的余数 练习题2.28 | x 十六进制 | x 十进制 | $-^u_4x$ 十进制 | $-^u_4x$十六进制 | | ———- | ——– | ————— | —————- | | 0 | 0 | 0 | 0 | | 5 | 5 | 11 | B | | 8 | 8 | 8 | 8 | | D | 13 | 3 | 3 | | F |15 | 1 | 1 | 2.3.2 补码加法 注意跟无符号数正溢出的区别，无符号数溢出之后是去掉 w+1 位，但是有符号数正溢出时，$x+y&lt;2^w$，w位还是要考虑的，只是变为负数了。所以是 $x+y-2*2^{w-1}$, 而不能认为是 $x+y-2^{w-1}$ 但是负溢出，就会溢出到 w+1 位了，那么去掉 w+1 位，就是 $x+y+ 2^w$ 了 推导的话，因为不论有符号还是无符号，都是相同的位级表示，二进制的加法也是进位同样的方式。 只是解释方式不一样而已。所以可以将有符号转换为无符号数进行 $+^u_w$ 的模 $2^w$ 的模数加法运算，然后在转换为有符号数即可。 补码溢出的条件 习题 2.30 补码溢出判断： 1234567891011121314151617int tadd_ok(int x, int y){ int sum = x+y; if ((x&gt;0 &amp;&amp; y&gt;0 &amp;&amp; s&lt;=0) || (x&lt;0 &amp;&amp;y&lt;0 &amp;&amp; s&gt;=0)) return 0; return 1;} 习题 2.31 123456789101112131415/* Determine whether arguments can be added without overflow *//* WARRING: This code is buggy */int tadd_ok(int x, int y){ int sum = x+y; return (sum-x == y) &amp;&amp; (sum-y ==x);} 无论正确与否，都是返回 1. 模数加法形成了一种数据结构，称为阿贝尔群(Abelian group). 因此代码中的判断，无论溢出与否，都是成立的，都返回 1； 具体推导如下： 如果 x+y 溢出， $sum = x+y-2^w$, 那么 $sum-x = y-2^w$. 因为 $y &lt; 2^{w-1}-1$, 则 $y-2^w&lt; -2^{w-1} -1$, 仍然负溢出。 所以 $sum-x= y-2^w+2^w = y$. 习题 2.32 x -y 不溢出时，返回1. 1234567891011/* Determine whether arguments can be subtracted without overflow *//* WARNNING: This code is buggy. */int tsub_ok(int x, int y){ return tadd_ok(x, -y);} x 和 y 取什么值是，会产生错误的结果？ 当 $y = TMin_w = -2^{w-1}$ 时， $-y = 2^{w-1}$ 原本应该是正数的，但已经溢出了，转换成了负数。 $-y = y$. 此时对于 tadd_ok(x, -y) 来说，当 x 为负时都认为溢出，返回 0，而 x 为非负时，都认为没有溢出返回 1。而情况恰恰相反，tsub_ok(x, TMin)，当 x 为 负数时，应认为没有溢出，返回 1，而 x 为非负时，应认为溢出返回 0. 2.3.3 补码的非 因为补码又负数，所以一般情况下，其逆元就是 -x， 而对于 $TMin_w$, $-TMin_w$ 会溢出，就不属于一般情况了。 练习题 2.33 | x 十六进制 | x 十进制 | $-^t_4x$ 十进制 | $-^t_4x$十六进制 | | ———- | ——– | ————— | —————- | | 0 | 0 | 0 | 0 | | 5 | 5 | -5 | -5 | | 8 | -8 | -8 | 8 | | D | -3 | 3 | 3 | | F | -1 | 1 | 1 | 第二种方法: 执行位级补码非，然后加1 对于无符号数或有符号数，非（或者逆元），他们的位模式是一样的。都是位模式取反+1， 对任意整数 非（逆元）-x 和 其位模式取反（位级补码非）加1 ~x+1 都是一样的。 2.3.4 无符号乘法$0\\le x,y \\le 2^w-1$, $0\\le x\\cdot y z^{2w}-2^{w-1}+1$ 需要2w 位来表示。将其截断为 w 位，等价于计算该值模 $2^w$. $$x*^u_w y = (x\\cdot y)mod\\ 2^w$$ 2.3.5 补码乘法补码乘法和无符号的乘法运算的位级表示是一样的。 对于 $TMin_w\\le x,y\\le TMax_w$ 有： $$x*^t_wy=U2T_w((x\\cdot y)mod\\ 2^w)$$ 原理是： $$T2B_w(x^t_wy)=U2B_w(x’ ^u_w y’)$$ x’,y’ 是x，y的无符号转换后的数。 证明略。 虽然完整的乘积的位级表示可能不同，但截断后是一样的。 练习题 2.34 | 模式 | x | y | $x\\cdot y$ | 截断后的 $x\\cdot y$ | | —— | ——– | ——– | ———– | ——————- | | 无符号 | [100]=4 | [101]=5 | [10100]=20 | [100]=4 | | 补码 | [100]=-4 | [101]=-3 | [1100] = 12 | [100]=-4 | | 无符号 | [010]=2 | [111]=7 | [1110]=14 | [110]=6 | | 补码 | [010]=2 | [111]=-1 | [110] =-2 | [110]=-2 | 练习题 2.35 12345678910111213141516171819/* Determine whether arguments can be multiplied without overflow */int tmult_ok(int x, int y){ int p = x*y; /* Either x is 0, or dividing p by x gives y * / return !x || p/x == y;} 见题 2.31，我们不能用减法来检验加法是否溢出，但这里可以用除法为检验乘法是否溢出。 证明 ？？ 练习题 2.36 1234567891011121314151617int tmul_ok(int x, int y){ /* Compute product without overflow*/ /* 这一行的强制类型转换至关重要。如果写成 int64_t p = x*y; * / /* 就会用 32 位值来计算乘积(可能溢出），再符号扩展到 64 位*/ int64_t p = (int64_t) x * y; /* See if casting to int preserves value*/ return (int)p == p; 2.3.6 乘以常数在大多数机器上，整数乘法指令相当慢，需要10个或者多个时钟周期。而加法、减法、位级运算和移位只需要1个时钟周期，因此编译通常把乘法转换为移位和加法的组合运算。 1.先考虑乘以 2 的幂 与2的幂的无符号乘法： x &lt;&lt; k 产生数值 $x*^u_w 2^k$ 与2的幂的补码乘法： 补码与无符号的位级操作等价，因此补码运算的2的幂的乘法也类似。 x &lt;&lt; k 产生数值 $x*^t_w 2^k$ 无论是补码还是无码，都可能溢出。即使溢出，通过移位和乘法的结果也是一致的，截断就好了。 对任意数的乘法可写成： $x14 = x(2^3+2^2+2^1)$ 或者 $x14 = x(2^4-2^1) = (x&lt;&lt;4)-(x&lt;&lt;1)$ 练习题 2.38 (a&lt;&lt;k)+b， 其中k可以为0,1,2,3, b可为0或a 123456789101112131415a 的倍数 LEA 指令2a (a&lt;&lt;1)+03a (a&lt;&lt;1)+a4a (a&lt;&lt;2)+05a (a&lt;&lt;2)+a8a (a&lt;&lt;3)+09a (a&lt;&lt;3)+a 乘法还可以这么看， x*K. 编译器将k的二进制表达为一组0和1交替的序列。 n表示第一段开始的n个连续的1，n-1表示第二段连续的1。 比如以8位为例，14 可以写成 [(0000)(111)(0)],从第3位开始到第1位为连续的1. 那么 x*14=(x&lt;&lt;3)+(x&lt;&lt;2)+(x&lt;&lt;1) 或者 x*14=(x&lt;&lt;4)-(x&lt;&lt;1) 很神奇。。。 练习题2.40 练习题2.41 编译器A、B两种形式选择哪一种： 选择操作次数少的。 2.3.7 除以2的幂除法更慢，需要30多个时钟周期。也可以采用移位运算实现。只不过是右移。无符号是逻辑右移（补0），补码是算术右移（补1） 定义向下取整 $\\lfloor a \\rfloor=a’. a’\\le a\\le a’+1$. 对于非负整数是满足除法的，但 $\\lfloor -3.14 \\rfloor = -4$ 是不满足的。 除以2的幂的无符号除法x&gt;&gt;k 等价于 $\\lfloor x/2^k\\rfloor$ 除以2的幂的补码除法算术右移，对与正数来说，最高有效位是0，与无符号的逻辑右移是一样的。 但对于负数来说，最高有效位是1，右移后效果如下图： $[x_{w-1},..,x_{w-1},x_{w-2},…,x_k]$ 是 $\\lfloor x/2^k\\rfloor$ 的补码表示，但是它不是向零舍入。需要使用“偏置”来修正这种不合适的舍入。 (x+(1&lt;&lt;k)-1)&gt;&gt;k 等价于 $x/2^k$ 向上取整。 偏置技术利用了如下属性： $\\lceil x/y \\rceil = \\lfloor (x+y-1)/y \\rfloor$ 推导： 假设 x = qy+r. $\\lfloor (x+y-1)/y\\rfloor=q+\\lfloor(r+y-1)/y\\rfloor$ 当r=0时，也就是能整除，$\\lfloor(r+y-1)/y\\rfloor=0$. 这也是要-1的原因，不然对于整除的时候就不符了。 当r&gt;0时，y&gt;r&gt;1,$\\lfloor(r+y-1)/y\\rfloor=1$, 结果相当于 q+1 所以，当 y=2^k 时， (x+(1&lt;&lt;k)-1)&gt;&gt;k = $\\lceil x/2^k\\rceil$ 总结： 对于使用算术右移的补码机器，C表达式： (x&lt;0 ? x+(1&lt;&lt;k)-1 : x) &gt;&gt; k 练习题2.42 写一个函数 div16，对于整数参数 x 返回 x/16 的值。要求不能使用除法、模运算、乘法、条件语句 (if 和 ? : )、比较运算符、循环等。假设 int 为 32 位，补码，算术右移。 1234567891011121314151617int div16(int x){ /* (x+bias) &gt;&gt; k 对于 x 为正数， bias=0 对于 x 为负数， bias=(1&lt;&lt;4)-1=15*/ int bias = (x&gt;&gt;31) &amp; 0xf; printf(&quot;%d&quot;, bias); return (x+bias) &gt;&gt; 4;} 练习题 2.43 下面的代码中，省略了常数 M 和 N 的定义： 1234567891011121314151617#define M /* Mystery number 1 */#define N /* Mystery number 2 */int arith(int x, int y){ int result = 0; result = x*M + y/N; return result;} 用某个 M 和 N 的值编译上面代码后，编译器将优化乘除操作。下面是产生的机器码翻译回 C 的结果： 1234567891011121314151617181920212223/* Translation of assembly code for arith */int optarith(int x, int y){ int t = x; x &lt;&lt;= 5; x -= t; /* x = x*2^5 -x，使用了形式B的优化：(x&lt;&lt;(n+1))-(x&lt;&lt;m)， 这里 n=4, m=0, 从而 M = [11111] = 31*/ if (y &lt; 0) y += 7; /* y + (1&lt;&lt;3)-1*/ y &gt;&gt;= 3; /* Arithmetic shift，从而 N 为 2 的 3 次方，即为 8*/ return x+y;} M=31, N=8 2.3.8 关于整数运算的最后思考整数运算实际上是一种模运算形式。 整数表示的有限字长限制了运算结果的取值范围，从而可能会溢出。无论是补码表示还是无符号数，它们的加减乘除操作，在位级上都完全一样或非常类似。 C 中的 unsigned 数据类型变量，在运算中会隐式地要求将其它参数先转换成 unsigned 数据类型再计算，从而会导致难以察觉的 BUG。 习题2.44 A. x=TMin_w 时为假， $x = -2^{w-1}-1 = -2^{w-1}-1+2^w=2^{w-1}-1 &gt;0$ B. 举反例就是两边都为假。左边 (x&amp;7)=7=[000…0111], 那么最后三位都为1. 右边 x&lt;&lt;29 &gt;0,那么倒数第三位为0. 所以不存在同时两边都为假的x。 C. 当 $x=2^31-1$, $x*x=2^62-2$,溢出，截断之后最高位为1，肯定小于0 2.4 浮点数2.4.1 二进制小数 小数的二进制表示法只能表示 $x\\times 2^y$ 的数，其他的只能近似表示。二进制表示的长度可以提高表示的精度。 例如 $\\dfrac{1}{5}$ 可以用十进制0.20精确表示，但并不能准确的用二进制表示。 $0.111…1_2$ 表示刚好小于0的数。用 $1.0-\\epsilon$ 表示 2.4.2 IEEE浮点表示 符号位 s 阶码（exponent）E的作用是对浮点数加权。 n为小数字段 $frac=f_{n-1}\\cdots f_1f_0$, 编码出来的也依赖与阶码段的值是否为0 单精度： s, exp, frac字段为 1,8,23 位 双精度： s,exp, frac字段为 1,11,52 位 1.规格化的值 exp的位模式不全为0，也不全为1。 阶码字段被解释为以偏置形式的有符号整数。 阶码的值 $E=e-Bias$, e是无符号数，为表示是 $e_{k-1}\\cdots e_1e_0$. Bias等于 $2^{k-1}-1$. 所以单精度范围： $e_{max}=2^8-2, e_{min}=1$ $E_{max}=2^8-2-2^7+1=127$ $E_{min}=1-(2^7-1)=-126$ 2.非规格化的值 阶码全为0， E=1-Bias. 3.特殊值 当阶码全为1时，小数域全为0，得到的值表示无穷。s=0时是正无穷，s=1是负无穷。 阶码全为1， 小数域非0，表示NaN. 2.4.3 数字示例","link":"/2018/06/09/CSAPP-02-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/"},{"title":"chapter13 Lexicalized PCFG","text":"Lexicalized PCFGs Lexicalization of a treebank： 给treebank添加head Lexicalized probabilistic context-free Grammars： Lexicalized PCFGs中的参数，也就是rules的概率 Parsing with lexicalized PCFGs：使用动态规划CKY对测试集中的sentences寻找最优parse tree Parameter estimation in lexicalized probabilistic context-free grammars： 通过训练集，也就是语料库corpus得到Lexicalized PCFG的参数 Accuracy of lexicalized PCFGs：测试集的准确率计算 相关研究 Lexicalized PCFGs 词汇化PCFGsLexicalization of a treebank 给每个rules添加一个head，前面也介绍过～不过么看懂，这里又一次讲到了。 关于怎么找到这个head，一个rule中最中心的部分～a core idea in syntax the central sub-constituent of each rule the sementic predicate in each rule 在语料库中是没有标注出head的，那么需要一些规则来人为选定head。 以NP为例，right-most 以VP为例， left-most 添加head之后有啥用呢？ make the PCFG more sensitive to lexical information. propagate lexical items bottom-up throught these threes using head annotations where each non-terminal receives its head from its head child. Lexicalized probabilistic context-free Grammars Chomsky Normal Form 突然发现自己不看字母也能听懂 Michael Collins 的英语了～发音真的好听而且清晰！！还有自己截图右下角都有个预览的框框。。因为鼠标截图时不得不回碰到youtobe预览的进度条。。。 Lexicalized context-free grammars in chomsky normal form 跟regular CFG很相似，但多了head words Parameters in a Lexicalized PCFG 在PCFG中，是 $S\\rightarrow$ 在 Lexicalized PCFG中， 是 $S(saw)\\rightarrow$ 因此参数多了很多很多，比如 $S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$, $S(saw)\\rightarrow_2 NP(women)\\ VP(saw)$ 这都有对应的概率～we heve every possible lexicalized rule, 同时smoothing技术在这会直接用到～ Parsing with lexicalized PCFGs 对于一个rule grammar，其non-terminal展开是 $|\\Sigma|^2\\times |N|^3$ 对于一个长度为n的sentence，使用dynamic programming算法需要的计算复杂度为 $O(n^3|\\Sigma|^2|N|^3)$,但是 词典 $|\\Sigma|$ 可能会巨大！！！ 但可以这么简化，就是不用考虑整个词典 $|\\Sigma|$, 只需要考虑在sentence出现过的词，也就是 n 个。 那么总的计算复杂度为 $O(n^5|N|^3)$. 棒啊！～ Parameter estimation in lexicalized probabilistic context-free grammars 为什么 Lexicalized PCFGs 要好于 PCFG？ 使用prepositional phrase ambiguity举例说明: rules: 7个non-terminal: $$S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$$ $$NP(man)\\rightarrow_2 DT(the)\\ NN(man)$$ $$VP(saw)\\rightarrow_1 VP(saw)\\ PP(with)\\tag{~}$$ $$VP(saw)\\rightarrow_1 Vt(saw)\\ NP(dog)\\tag{~}$$ $$NP(dog)\\rightarrow_2 DT(the)\\ NN(dog)$$ $$PP(with)\\rightarrow_1 IN(with)\\ NP(telescope)$$ $$NP(telescope)\\rightarrow_2 DT(with)\\ NN(telescope)$$ 9个terminal： $$\\cdots$$ 7个non-terminal: $$S(saw)\\rightarrow_2 NP(man)\\ VP(saw)$$ $$NP(man)\\rightarrow_2 DT(the)\\ NN(man)$$ $$VP(saw)\\rightarrow_1 VP(saw)\\ PP(dog)\\tag{~}$$ $$NP(dog)\\rightarrow_1 NP(dog)\\ PP(with)\\tag{~}$$ $$NP(dog)\\rightarrow_2 DT(the)\\ NN(dog)$$ $$PP(with)\\rightarrow_1 IN(with)\\ NP(telescope)$$ $$NP(telescope)\\rightarrow_2 DT(with)\\ NN(telescope)$$ 9个terminal： $$\\cdots$$ 两者的区别在于（～）的概率大小。如果没有lexicalized,parse tree的选择就仅仅只取决于短语结构，而完全忽视了语义的信息。 如何计算参数 $q(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))$ 是条件概率，可以看做是已知S,saw,从CFG语法中选出 $S\\rightarrow_2 NP\\ VP$, 并且从NP的 word 中选出 man 的概率. $$q(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))=q(S\\rightarrow_2 NP\\ VP|S,saw) \\times q(man|S\\rightarrow_2NP\\ VP,saw)$$ 用极大似然估计可得： 第一项： $q(S\\rightarrow_2 NP\\ VP|S,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP\\ VP)}{count(S(saw))}$ 第二项： $q(man|S\\rightarrow_2NP\\ VP,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP(man)\\ VP(saw))}{count(S(saw)\\rightarrow_2 NP\\ VP,saw)}$ 用线性插值法估计参数: $$\\lambda_1+\\lambda_2=1$$ $$q_{ML}(S\\rightarrow_2NP\\ VP|S,saw)=\\dfrac{count(S(saw)\\rightarrow_2 NP\\ VP)}{count(S(saw))}$$ $$q_{ML}(S\\rightarrow_2NP\\ VP|S) = \\dfrac{count(S\\rightarrow_2 NP\\ VP)}{count(S)}$$ 依然还有很多需要解决的问题： deal with rules with more child incorporate parts of speech(useful in smoothing) encode preferences for close attachment further reading: MIchael Collins.2003. Head-Driven Statistical Models for Natural Language Parsing. Accuracy of lexicalized PCFGsgold standard and parsing output: Result介绍了一些相关的研究～ reference: MIchael Collins, Natural Language Processing","link":"/2018/04/22/chapter13-Lexicalized-PCFG/"},{"title":"chapter11-上下文无关语法CFG","text":"上下文无关语法 CFG，又叫短语结构语法 英语语法中的各种规则： 句子级的结构，分4种 名词短语，中心词，以及围绕中心词的前后修饰语 动词短语 并列关系 Treebanks：经过剖析后的语料库，包含了句法syntatic信息和语义semantic信息 Treebanks as Grammars Heads and Head Finding 语法等价与范式 Lexicalized Grammars 词汇语法 前言： 上下文无关语法 context-free gramma. 上下文无关文法是许多自然语言句法形式化模型的支柱. Constituency 组成性何为组成性（constituency）：groups of words behaving as a single units, or constituents. 以名词短语(noun phrase)为例: 一个明显的证据是名词短语通常在动词 verb 之前。 Context-Free Grammars 上下文无关语法上下文无关语法 Context-Free Grammar, or CFG, 又叫短语结构语法 Phrase-Structure Grammars, 其形式化方法等价于 Backus-Naur Form, or BNF. 一个上下文无关语法，由规则 rules 或 产生式 productions ,以及单词和符号的一个词表 lexicon. 举例： 一个名词短语（NP or noun phrase）可以由一个专有名词(ProperNoun)组成或者是一个限定词(Det)后接一个名词性成分（Nominal）,而一个名词性成分又可以是一个或多个名词。 parse tree 剖析树 NP是树的根，也就是 CFG 定义的形式语言的初始符号 start symbol, 用 S 表示，通常可以解释为 “句子”，所以由 S 推导出来的符号串的集合就是句子的集合。 a, fight 这样的单词是终极符号 terminal symbol, 词表是引入终极符号的规则的集合 表示终极符号的聚类或概括的符号称为非终极符号 non-ternimal symbol,比如箭头左边的 NP.Det, Noun…. 用 “|” 表示非终极符号的展开方式： 语法规则，用 $L_0$ 表示～ 剖析树也可以用更为简洁的方式表示： Formal Definition of Context-Free Grammar 上下文无关语法的形式定义诸如 $L_0$ 的CFG定义了一个形式语言，在chapter2 中讲过，形式语言是符号串的集合。使用形式语言来模拟自然语言的语法称为 “生成语法（generative grammar）”. 一个上下文无关语法有四个参数（也称为四元组，4-tuple）： 非终极符号的集合 N 终极符号的集合 $\\Sigma$ 生成式的集合 R， 每个生成式的形式为 $A\\rightarrow \\beta$,其中 A 是非终极符号， $\\alpha$ 是由符号串的无限集 $(\\Sigma \\bigcup N)* $ 的符号构成的符号串 把单词的符号串映射到剖析树的问题称为 Syntactic parsing 句法分析. 在chapter12中会讲到～ Some Grammar Rules for English本书中关于英语短语结构只是点到为止，需要了解更多可以看 [Huddleston, R. and Pullum, G. K. (2002). The Cambridge Grammar of the English Language. Cambridge University Press.] Sentence-Level Constructions 句子级的结构除了上文中介绍的陈述句，还有4中比较常见和重要的结构：declaratives 陈述式结构, imperatives 命令式结构, yes-no questions yes-no疑问式结构, and wh-questions 疑问式结构. imperatives 命令式结构 动词短语开头，没有主语。 yes-no questions yes-no疑问式结构 助动词开头，后面跟一个主语NP，再跟一个VP. wh-questions 疑问式结构 比较复杂，有两种情况： wh主语疑问式（wh-subject-question）结构，主语疑问式结构与陈述句结构相同 wh非主语疑问式（wh-non-subject-question）结构，wh短语不是句子的主语 类似wh-non-subject-question 的结构我们称为 long-distance dependencies,因为 wh-NP 远离在语义上和它相关的谓词 have. The Noun Phrase 名词短语关于名词短语主要有： pronouns代词， proper nouns专有名词，和NP $\\rightarrow det\\ Nominal$. 接下来的部分主要介绍最后一种结构 NP $\\rightarrow det\\ Nominal$. 在名词短语中，包括一个中心词 head, 围绕中心词的有 前修饰语(prehead modifier) 和 后修饰语(post-head modifier). The Determiner 限定词，出现在中心词之前 The Nominal $$Nominal \\rightarrow Noun$$ Before the Head Noun: cardinal numbers 基数词, ordinal numbers 序数词, quantifiers 数量修饰语, and adjectives after the Head Noun: postmodifiers 主要分为三类： 介词短语 非限定从句 关系从句 介词短语 prepositional phrases 非限定从句 non-finite postmodifiers，有三种： 动名词gerundive (-ing), -ed, and infinitive forms. （1）动名词 gerundive (-ing) 列举一些例子： 其对应的形式语言： （2）infinitives不定式 and -ed形式 关系从句(postnominal relative clause), 准确的说是 限制性关系从句(restrictive relative clause), 通常用关系代词 relative pronoun 开头. before the Noun Phrase: 在NPs之前的词，通常叫 predeterminers, 最常见的就是 all. 复杂的名词短语的剖析树举例 The Verb Phrase 动词短语比较简单的动词短语的结构： 除此之外还有更复杂的，在动词后嵌入完整的句子,这样的成分叫做句子补语 sentential complements 在VP后潜在的成分可能是另一个VP。动词可以与不同类型的补语相容，但不是每个动词都与每个动词短语相容。比如 及物动词(transitive) 和 非及物动词(intransitive) 之分。 将动词 次范畴化(subcategorize), 也就是按照 NP或其他补语 再分类。动词的这些可能的补语的集合称为该动词的 次范畴化框架（subcategorize frame）。 Coordination 并列关系conjunctions: and, or and but. NP $\\rightarrow$ NP and NP Nominal $\\rightarrow$ Nominal and Nominal VP $\\rightarrow$ VP and VP S $\\rightarrow$ S and S Treebanks何为Treebanks: a corpus where every sentence in the collection is paired with a corresponding parse， Such a syntactically annotated corpus is called a treebank 一个语料库中所有的sentences都有其对应的剖析树。 Penn Treebank project (whose POS tagset we introduced in Chapter 10) has produced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal corpora of English, as well as treebanks in Arabic and Chinese. Penn Treebank 是一个treebank，其语料库来自于 Brown等。。 Treebanks as Grammars Viewed as a large grammar in this way, the Penn Treebank III Wall Street Journal corpus, which contains about 1 million words, also has about 1 million non-lexical rule tokens, consisting of about 17,500 distinct rule types. Heads and Head Finding每一个组成成分constituent，也就是树中的一个节点都有一个词汇头部 （lexical head）. 为什么需要head？ 暂时还不太懂，学过后面的内容应该就能理解了吧～ 举个栗子： 每个非终极non-terminal符号，也就是非叶节点都有一个head. 那么怎么找每个节点对应的head呢？ Instead of specifying head rules in the grammar itself, heads are identified dynamically in the context of trees for specific sentences. In other words, once a sentence is parsed, the resulting tree is walked to decorate each node with the appropriate head. 也就是说，没有具体的规则，根据具体的句子动态的定义对应的head，这些规则也都是 hand-written rules. 以NP中的head为例： Grammar Equivalence and Normal Form 语法等价与范式 强等价 strong equivalence 和 弱等价 weak equivalence 如果两个语法生成相同的符号串集合，而且对每个句子都指派相同的短语结构(只改变终极符号)，这样的两个语法是强等价 如果生成的符号串集合相同，但是不给每个句子指派相同的短语结构，就是弱等价，其实就是对同一个句子，理解不同～ 举个栗子： 一个上下文无关语法CFG是自由的，并且它的每个生成式的形式都是 $A\\rightarrow B C$ 或 $A \\rightarrow a$, 也就是说，每个规则 rule的右边要么是两个非终极符号，要么是一个终极符号，那么这个CFG就是 Chomsky normal form. 任何一个CFG都可以写成弱等价的Chomsky范式语法。 举个栗子： VBD是动词的过去式， Lexicalized Grammars 词汇语法可以看到，CFG过分强调短语结构，而忽视了词汇的作用，也就是单词的语义。但这样的短语结构都太复杂和笨重(cumbersome)了，而且 语法冗余，难以管理和脆弱（redundant,hard to manage, and brittle）。为了解决这样的问题，需要更好的利用lexicon。 接下来介绍其中的一种方法： **Combinatory Categorial Grammar，CCG ***,考虑到 句法 syntactic 和 语义 semantic 的高度 词汇化lexicalized 的方法. 下一章会详细讲到词汇化～ 总结 参考： Speech and language Processing，Chapter11 Natural Language Processing, Michael Collins, Columbia University","link":"/2018/04/19/chapter11-%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95/"},{"title":"chapter3-有限状态自动机","text":"有限状态机 FSA 确定性的识别器 DFSA 确定性的识别器 NFSA：深度优先搜索 or 广度优先搜索 形式语言 有限状态自动机 finite-state automation, FSA前面一章节中的正则表达式只是一种用于文本搜索的方便的元语言，它是描述有限状态机的一种方法。 任何正则表达式又可以用有限状态机来实现（除了使用存储特性的那些正则表达式）。比如上一章中用于描述羊的语言的正则表达式： /baa+!/ 就可以表示为这样的有限自动机： FSA可以用来识别（或接受）符号串。接受方式如下：把输入想象成一个长长的带子(tape),带子上的一个单元格(cell)可以写一个符号： 怎么理解这个输入带子和自动机呢？就是当输入带上的字母和自动机中离开当前状态的弧相匹配时，就穿过这个弧，进入到自动机中的下一个状态和带子上的下一个符号。比如上图中初始状态 $q_0$ 只有遇到b时才会匹配成功而进入下一个状态和输入符号，所以上图中的自动机不能接受输入。对图2.10中 $q_3$ 状态，他可以匹配 a或!. 我们可以用状态转移表(state-transition table)来表示自动机。状态转移表可以表示初始状态、接收状态和符号在状态之间的转移情况。 一个有限自动机可以用下面5个参数来定义： 其中Q表示N种状态的集合，$\\Sigma$ 表示有限的输入符号字母表， $q_0$是初始状态， $F\\in Q$ 是终极状态。 $\\delta(q,i)$ 是转移矩阵，其中 $q\\in Q,i\\in \\Sigma$. $\\delta(q,i)$ 返回一个新的状态，$q’\\in Q$，则 $\\delta(q,i)$ 是从 $Q \\times \\Sigma$ 到 $Q$ 的一个关系. 在HMM中，状态转移矩阵是N*N的，跟这里有区别。 DFSA可以用状态转移表来识别字符串，就是把一个字符串输入进去，然后用自动机来判别是accept还是reject。这种算法是“确定性的是识别器”，deterministic算法，简称 D-RECOGNIZE. 伪代码： 在图2.13中，状态之间的转移矩阵是 transition-table[current-state, tape[index]]. current-state表示行，tape[index]表示列。其中index = index + 1. 如果将状态转移矩阵中的空状态称为失败状态(fail state)或者吸收状态(sink state).那么对任何状态和任何输入，自动机都能找到可以转移的地方。 NFSA存在某个状态，难以判断接下来怎么走的自动机称为非确定的FSA(或NFSA).如下图： 还有一种NFSA,用到了 $\\epsilon-$ 转移的新弧。如果到达了状态3，那么可以进入带有标记!的弧，也可以不看带子上的符号，直接转移到状态2. 那么其对应的状态转移表如下图： 用NFSA来接受符号串: 有三种方法： 回退 前瞻 并行 这里我们采用回退法： 代码阅读： function ND-RECOGNIZE(tape, machine) return accept or reject 创建进程表(agenda): 其中的元素是current-search-state, 是由自动机的一个结点（状态）和带子上的一个位置组合而成的。 agenda = {(Initial state of machine, beginning of tape)} current-search-state = NEXT(agenda) 主回路：用来确定输入带子上的全部内容是否都被自动机全部识别了，这可以用ACCEPT-STATE?函数实现，如果当前搜索状态既包括一个接受的机器状态,也包含一个子结尾的指针，那么就返回accept.这一步类似于 D-RECOGNIZE. 如果不行，那就调用 GENERATE-NEW-STATE 来生成一系列可能的下一个状态。 1234567891011121314151617if ACCEPT-STATE?(current-search-state) return ture : return ACCEPTelse agenda = agenda and GENERATE-NEW-STATE(current-search-state) //这一步不太理解？？？if agenda is empty: return rejectelse current-search-state = NEXT(agenda) NEXT函数从进程表返回一个为探测过的状态。那么怎么定义函数NEXT，这是一个值得研究的问题。 进程表用stack实现，那就是深度优先搜索(depth-first search)或后进先出(Last In First Out, LIFO). 进程表用queue实现，那就是广度优先搜索(breadth-first search)或先进先出(First In First Out, FIFO). ACCEPT-STATE? 函数用来确定输入带子上的全部内容是否都被自动机成功的识别了。 123456789101112131415function ACCEPT-STATE? (search-state) returns ture of false current-node = the node search-state is in index = the point on the tape search-state is looking at if index is at the end of the tape and current-node is an accept state of machine: return ture else return false GENERATE-NEW-STATE函数用与选取一种为访问过的路径 需要理解的是：在NEXT(agenda)中,agenda采用的是stack或queue, 那么意味着在只有一种选择的结点处，走过就删掉了，只有有两种选择的地方，另外一种会储存在agenda中。所以current-search-state = NEXT(agenda)生成的是有两种选择处的未访问过的路径。那么GENERATE-NEW-STATE函数就是生成这样的一个路径，然后在ACCEPT-STATE?中判断。 识别就是搜索： 把符号串输入当做输入带，然后用自动机来识别。其实也可以看作是搜索的过程。 深度优先搜索： 广度优先搜索： 形式语言形式语言是早期处理自然语言处理技术中，当时几乎是唯一的方法，可以用于描述自然语言的语法规律，能最大限度的逼近自然语言，并且很容易生成语言内容。 形式语言和自动机之间存在的对应关系，使其天生就容易被计算机处理。 虽然现在自然语言处理主要是基于统计模型来处理的，但形式语言仍然会出现在很多论文中。 图论 无向图 G = (V, E), V 表示定点， E表示无向边。 有向图 G = (V, E), V 是定点，E 是有向边 连通图和回路 文法，形式语法在用计算机系统,来判断一个句子是否是某语言的合法句子时，从句子和语言的结构特征上着手是非常重要的。一般可以通过这个句子是否能有给定语言对应的文法产生来作出判断，如果能，他就是合法的，否则，他就是非法的。对一类语言，可以在字母表上按照一定的规则，根据语言的结构特点，定义一个文法。用文法作为相应语言的有穷描述不仅可以描述出语言的结构特性，而且还可以产生出这个语言的所有句子。","link":"/2018/04/11/chapter3-%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"title":"chapter2:正则表达式、文本标准化和编辑距离","text":"各种正则表达式，析取，组合和优先关系 文本标准化：各种预处理方法 编辑距离：动态规划 前言： 早期的自然语言处理工具ELIZA采用的方法是pattern matching. 而对于文本模式(text pattern)的描述，有一个很重要的工具：正则表达式(regular expression). 对文本处理任务的统称，就是文本标准化(text normalization)。其中有： tokenizing: 分词？ lemmatization: 词形还原。 确定表面不一样的词是否是同一个词根，针对时态比较复杂的语言非常必要。 stemming:词干提取。 针对前缀后缀，一种简单的lemmatization sentence segmantation: 句子分割. 用时态或者标点符号 最后提到编辑距离(edit distance):一种算法，在自然语言处理和语音识别中都很常用。 正则表达式 regular expression 用斜线(slashes)来分隔正则表达式，斜线不是正则表达式的一部分。 正则表达式的区分大小写的，可以用方括号(square braces)来解决这个问题。 对所有的数字(digits)可以用/[1234567890]/,但对于所有的字母这样就不太方便了，可用连字符(dash)(-)来表示一个范围(range). 脱字符(caret)(^) 用脱字符表示否定，或者仅仅表示它自身。放在方括号的第一个位置时才有效。 用问号?表示前一个字符是可选的。 问号除了表示可选外，还有贪婪和非贪婪的区别。/.* ?/ 和 /.* / 用 * 表示前一个字符的零个或多个；用 + 表示前一个字符的一个或多个 举个栗子： baa!(至少两个a) baaa! baaaaaa! 用 /baaa*/ 或 /baa+/ 可匹配上面这种形式。 单位数的价格可用 /[0-9]/，一个整数（字符串）的正则表达式：/[0-9][0-9]* / 或者 /[0-9]+/ 通配符(wildcard) /./ /./表示匹配任何字符，那么和星号一起使用，就可以表示任何字符串了。/.* / 锚号(anchor)是一种把正则表达式锚在字符串中的特定位置，最普通的锚号是脱字符^和美元符$. 脱字符 ^ 与行的开始相匹配。/^ The/ 表示单词只出现在一行的开始，这样脱字符就有三种用法了。 美元符 \\$ 表示一行的结尾./^ The dog\\.\\$/表示这一行只有The dog. 其中点号前面必须加反斜杠，因为我们要让它表示点号，而不是通配符。 还有两个锚号： \\b表示词界， \\B表示非词界 非数字、下划线或字母，可以看做词界。 Disjunction,Grouping, and Precedence 析取，组合和优先关系 析取算符(Disjunction operator)(|),正则表达式 /cat|dog/ 表示字符串是dog或者cat,对于后缀guppy和guppies,可以写作 /gupp(y|ies)/ 圆括号算符“()”.我们知道 * 只能表示前一个字符的重复，但如果要重复一个字符串呢，那就得用括号了。比如 /(column [0-9]+_*)*/ 就表示column后面跟一个数字和任意数目空格的重复～ 运算符的优先级 正式因为 * 的优先级高于序列，所以 /the* / 表示与theeeee匹配，而不是与thethe匹配。 还有正则表达式的匹配是贪心的(greedy).比如 /[a-z]* / 可以匹配零个或多个字母，所以结果是尽可能长的符号串。 怎么让它不贪心呢（non-greedy）？ 可以这么写 /[a-z]* ?/ 或 /[a-z]+?/会匹配尽可能少的符号串。 一个简单的栗子要用正则表达式找到the /the/ 并不能找到the位于句子开头的情况The /[tT]he/ 当the嵌入在其他单词之间时theology，也是不对的 /\\b[tT]he\\b/ 加入词界后也不包括the_或者the25了，但如果我们也想找到这种情况中的the呢？那就说明，在the两侧不能出现字母。 /[^a-zA-Z][tT]he[^a-zA-Z]/ 这样仍然有问题，这意味着前面必须有个非字母符。所以应该这样： /(^|[^a-zA-Z])[tT]he[^a-zA-Z]/ 更多算符 通用字符集的别名(aliases) 用于计数符的算符 需要加反斜杠的特殊算符 正则表达式中的替换(substitution)、存储器(capture group)和ELIZAs/regexp1/regexp2/ 表示用第二个正则表达式替换第一个的内容 s/colour/color/ s/([0-9]+)/&lt;\\1&gt;/ 其中 \\1表示参照第一个模式中的内容，也就是括号的内容，然后加上&lt;&gt;后对它进行替换。实际上就是找到这样的，加上&lt;&gt; /the (.* )er they were, the \\1er they will be/ 可以匹配 The bigger they were, the bigger they will be 但不能匹配 bigger they were, the faster they will be. 括号中用于存储的模式叫做 capture group，而用于存储的数字存储器叫做 寄存器(register). 这样一来圆括号就有了两种含义了，可以用来优先级的运算符，也可以用来capture group. 所以必须加以区别，用 ?: 来表示 non-capturing group. (?: pattern ) 举个栗子: /(?:some|a few) (people|cats) like some \\1/ 可以用来匹配 some cats like some people，而不能匹配 some people like some a few. 因为\\1 表示的是(people|cats)这个括号中的内容。 ELIZA： 这可真是“人工”智能啊。。。hahha Lookahead assertions最后，有时候我们需要预测未来look ahead：在文本中向前看，看看有些模式是否匹配，但不会推进匹配游标(match cursor)，以便我们可以处理模式。 不推进匹配游标是什么意思？ lookahead assertions 使用(?=pattern)和(?!pattern). The operator (?= pattern) is true if pattern occurs, but is zero-width. 负向预测： /(ˆ?!Volcano)[A-Za-z]+/ 表示 这个不太理解，到regex.com上试了下： Words and Corpora在我们对word进行处理时，我们需要确定怎么样才算一个word. 语料库： written texts from different genres (newspaper, fiction, non-fiction, academic, etc.), Brown University in 1963–64 (Kučera and Francis,1967). telephone conversations between strangers，(Godfrey et al., 1992). disfluencies, fragment, filled pauses举个栗子： I do uh main- mainly business data processing 对于语句中出现的不流利的地方 (disfluencies). main- 称为片段 (fragment), 像uh和um这样的称为 fillers or filled pauses 我们在处理文本的时候是否需要保留这些不流利的地方呢，这取决于我们的应用。 Disfluencies like uh or um are actually helpful in speech recognition in predicting the upcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech recognition they are treated as regular words. Because people use different disfluencies they can also be a cue to speaker identification. capitalized tokens or uncapitalized tokensthey 和 They 是否需要当做同一个单词处理。 我们知道在 part-of-speech or named-entity tagging 中首字母大写是很有用的特征，这需要保留下来。 lemma词意 and wordform 一句话中的WORD可以用两种不同的标准来区分。一种是Lemma，一种是wordform。 wordform就是词的形状，而lemma则是词意。比如 am is are ，都是一个lemma，但是3个wordform。在阿拉伯语中，需要将lemmatization，可能因为他们同一个词意，能用的词太多了吧，我记得看哪个视频的时候说过骆驼，有四十多种。。。对于英语的话，wordform就够了。 word type 词型 and word token 词例倘若以wordform 词形的形式来界定一个词，那么一句话中WORD的数目还可以用两种不同的标准来区分。Type是相同的词都算一个，Token是每个词出现几次都算。所以 “no no no …. it is not possible” 这样的一句话，Type 有5个，Token 有7个。Token包括重复词。 其中 Tokens N 和 types |V| 有这样的关系： $$|V|=kN^{\\beta}$$ $\\beta$ 取决于语料库的大小(size)和类型(genre).当语料库至少有上图中的大小时， $\\beta$ 的值的大小为0.67到0.75之间。 Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words. 另外一种是以lemmas来界定一个词，而不是wordform. 文本标准化 Text Normalization在进行自然语言处理之前，都需要对文本进行标准化处理。 Segmenting/tokenizing words from running text 分词 Normalizing word formats 单词格式归一化 Segmenting sentences in running text. 句子分割 Unix tools for crude tokenization and normalization介绍了一个Linux命令 tr 可用来统计词频 但这个统计非常简单粗暴，去掉了所有的标点符号和数字 Word Tokenization and Normalization介绍了标点符号在很多地方的用途: Ph.D,m.p.h… 时间(09/04/18)..等等 email, urls clitic contractions by apostrophes. 用’号表示的缩写 what’re,we’re 根据应用不同，tokenize也会不同，比如New York通常也会标记为一个词。在 name entity detection 中Tokenization会很有用。 tokenize standard: Penn Treebank tokenization standard 由Linguistic Data Consortium(LDC)发布。 case folding: everything is mapped to lower case. 在语音识别和信息检索中会比较常用。 但是在sentiment anal- ysis and other text classification tasks, information extraction, and machine transla- tion 中大小写是很有用的，因此通常不会使用case folding. 下一章中的有限状态自动机 finite state automata 就是用基于正则表达式判别算法编译而成的。 中文词分割：maximum matching/MaxMatch 最大匹配算法一种贪心算法，需要一个字典(dictionary/wordlist)进行匹配. 伪代码： 代码参考：http://www.cnblogs.com/by-dream/p/6429615.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;//宏，计算数组个数#define GET_ARRAY_LEN(array, len){len=sizeof(array)/sizeof(array[0]);}string dict[] = {&quot;计算&quot;,&quot;计算语言学&quot;,&quot;课程&quot;,&quot;有&quot;,&quot;意思&quot;};//是否为词表中的词或词表中的前缀bool inDict(string str){ bool res = false; int i; int len = 0; GET_ARRAY_LEN(dict, len); for (i=0; i&lt;len; i++){ if (str == dict[i].substr(0, str.length())) { res = true; } } return res;}int main(){ string sentence = &quot;计算语言学课程有意思&quot;; string word = &quot;-&quot;; int wordlen = word.length(); // 1 int i; string s1 = &quot;&quot;; for (i=0; (unsigned)i&lt;sentence.length(); i += wordlen) { string tmp = s1 + sentence.substr(i, wordlen); //每次增加一个词 if (inDict(tmp)) { s1 = s1 + sentence.substr(i, wordlen); } else // 如果不在词表中，先打印出之前的结果，然后从下一个词开始 { cout &lt;&lt; &quot;分词结果：&quot; &lt;&lt; s1 &lt;&lt; endl; s1 = sentence.substr(i, wordlen); } } cout &lt;&lt; &quot;分词结果：&quot; &lt;&lt; s1 &lt;&lt; endl;} 如果词表足够大的话，就可以对更多的句子进行分词了。 我们用一个指标来量化分词器的准确率，称为 word error rate. 怎么计算word error rate:通过计算最小编辑距离 We compare our output segmentation with a perfect hand-segmented (‘gold’) sentence, seeing how many words differ. The word error rate is then the normalized minimum edit distance in words between our output and the gold: the number of word insertions插入, deletions删除, and substitutions替换 divided by the length of the gold sentence in words. 作者还提到最准确的中文分词算法是通过监督学习训练的统计 sequence models, 在chapter 10中会讲到。 Lemmatization and Stemming 词形还原和词干提取Lemmatization： 词形还原，am, is，are有共同的词元(Lemma)：be 举例说明： He is reading detective stories. –&gt; He be read detective story. 那么lemmatization是怎么实现的呢？ The most sophisticated methods for lemmatization involve complete morphological parsing(形态解析) of the word. morphological parsing会在chapter3中讲到。 Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes(语素). 语素包括两类： stems：词干 affixes: 词缀 关于词形还原的工具：blog，词形还原工具对比 Python: NLTK Python: Pattern Python: TextBlob Tree Tagger The Porter Stemmer通常我们用finite-state transducers 来处理 morphological parser,但我们有时候也会使用简单粗暴的去掉词缀的方法 stemming. 这里作者就介绍了一种这样的算法 Poster algorithm. 算法的原理主要是基于一些规则 cascade. Sentence Segmentation 句子分割主要是用标点符号啦～ 比较unambiguous的标点符号有：Question marks and exclamation points 而Periods就比较ambiguous了。 具体的句子分割算法垢面chapter会讲到 Minimum Edit DIstance 最小编辑距离用来表示两个句子之间的相似性。 deletion 删除： cost 1 insertion 插入： cost 1 substitution 替换： cost 2 The Minimum Edit Distance Algorithm一种动态规划的算法。 dynamic programming,Bellman, R. (1957). Dynamic Programming. Princeton University Press. that apply a table-driven method to solve problems by combining solutions to sub-problems. source string X[1…i…n] target string Y[1…j…m] 用D(i,j)来定义X中前i个字符到Y中前j个字符的编辑距离，那么X到Y的编辑距离就是D(n,m) 计算D[i,j],也就是递推有三种方式： 定义cost: 初始情况： D(i,0) = i，也就是 source substring of length i but an empty target string D(o,j) = j，也就是 With a target substring of length j but an empty source 那么伪代码： 1234567891011121314151617181920212223242526272829303132333435# 创建矩阵[n+1,m+1]D = np.zeros(n+1, m+1)# 1. Initialization:D[0,0] = 0for each row i for i to n: D[i,0] = D[i-1] + 1for each column j from 1 to m: D[0,j] = D[0,i-1] + 1# 2. Recurrence:for each row i from 1 to n: for each column j from 1 to m: D[i,j] = min(D[i-1,j]+1, D[i-1,j]+1, D[i−1, j−1]+2)# 3. Termination:return D[n,m] 我们知道了最小编辑距离是多少，但是我们还想知道最小编辑距离对应的两个字符串对齐方式 alignment.据说alignment在语音识别和机器翻译中很有用～ 最小编辑距离和viterbi算法、前向算法很相似。 最小编辑距离：递推一步有三种选择方式，然后取最小值。每一步中三种方式的权重weight也是有意义的。 Viterbi算法：递推一步有N个路径，然后取max，可以看做最小编辑距离的拓展，权重在这里就是概率。 前向算法：递推每一步有N个路径，然后取sum. 其中最小编辑距离和Viterbi算法有 backtrace. 同样的，在前向递推的过程中填表： 填表的过程就是从D(0,0)开始，每进入一个 boldfaced cell(除了第0行和第0列)都有三种选择，然后选择最小的。 计算 alignment path，分为两步骤： 在算法计算的过程中，存储后指针backpointer backtrace：从最后一行最后一列的cell开始，沿着指针，每一步都是最小的。 总结： 介绍了各种正则表达式 用 - 表示range 脱字符 ^ 的三种用法：自身，方括号中的否定，与行开头匹配 问号 ? 表示前一个字符是可选的 表示前一个字符零个或多个， + 表示前一个字符一个或多个 . 表示通配符，匹配任意一个字符，/.* /匹配任意长度字符，且贪心的 锚号 ^ 和 \\$ 匹配行开头和结尾 锚号 \\b和\\B 词界和非词界 析取，组合和优先关系 主要是析取算符|和圆括号()的用法，以及运算符优先级 替换和寄存器 s/regexp1/regexp2/ \\1 基于正则表达式的分词和文本标准化 用于词干提取stemming的简单粗暴的算法 Porter algorithm 用于描述字符串相似度的算法，最小编辑距离 leetcode上有个编辑距离的题目：https://leetcode.com/problems/edit-distance/description/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class Solution {public: int minDistance(string word1, string word2) { int n = word1.length(); int m = word2.length(); int a[n+1][m+1]; a[0][0] = 0; for (int i=1; i&lt;=n; i++){ a[i][0] = a[i-1][0] + 1; } for (int j=1; j&lt;=m; j++){ a[0][j] = a[0][j-1] + 1; } for (int i=1; i&lt;=n; i++){ for (int j=1; j&lt;=m; j++){ if (word1[i-1] != word2[j-1]){ int tmp = min(a[i-1][j-1] + 1, a[i-1][j] + 1); a[i][j] = min(tmp, a[i][j-1] + 1); } else { a[i][j] = a[i-1][j-1]; } } } return a[n][m]; }};","link":"/2018/04/09/chapter2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E3%80%81%E6%96%87%E6%9C%AC%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"title":"chapter7-logistic回归","text":"logistic regression 模型 p(y|x,w) 针对语言模型的特征处理 $f_i(c,x)$ 训练模型 正则化 特征选择：信息增益 分类器选择：bias-variance 前言: 分类算法：multinomial logistic regression, 当应用到NLP时，又称 maximum entropy, MaxEnt. 再一次说到了生成模型和判别模型。上一章已经说了了，就不写了～ logistic regression$$\\hat y=argmax_yP(y|x)$$ 这句话可以说解释的很清楚了～ 然鹅，能够直接计算出对应的概率P(y|x)吗？像这样： $$P(y|x)?=\\sum_{i=1}^Nw_if_i$$ $$?=w\\cdot f$$ 显然，这计算出来的并不是一个合理的概率，因为 $\\sum_{i=1}^N$ 的范围是 $-\\infty\\ to\\ \\infty$. 怎么解决这个问题呢？就是让得到的概率在0-1之间。 对于二分类：$y\\in {0,1}$可以使用sigmoid函数： $$\\sigma(z)=\\dfrac{1}{1+e^{-z}}$$ 将z压缩到0-1范围内。 则有： $$\\hat y = \\dfrac{1}{1+e^{-w^Tx}}$$ $$p(y=1|x) = \\dfrac{1}{1+e^{-w^Tx}}$$ $$p(y=0|x) = \\dfrac{1}{1+e^{w^Tx}}$$ 根据cross-entroy函数： $$L = -p(x)logq(x)$$ 对于单个样本有： $真实分布p(x):(y,1-y),对应的预测分布(\\hat y, 1-\\hat y)$ 带入可得： $$L(\\hat y, y)=-ylog(\\hat y)-(1-y)log(1-\\hat y)$$ 对于多分类softmax分类器： $$p(c|x)=\\dfrac{exp(\\sum_{i=1}^Nw_if_i(c,x))}{\\sum_{c’\\in C}exp(\\sum_{i=1}^Nw_if_i(c’,x))}$$ 其中 $f_i(c,x)$ 就是表示在给定样本条件下类别c对应的输入数据处理后的特征i。 怎么理解 $w_if_i(c,x)$ 呢？how to convince myself~ 假设单个样本 $X:$ (3072,1) 总共有10个类别 $c\\in C$ (10,) 则对应的权重： $W$ (10, 3072) 其实可以认为权重W的每一行对应一个分类器 $w_i$，也就是特征提取器。$f_i(c,x)$ 应该就是对不同类别的输入数据进行特征处理吧。 在图像处理中，可能并不需要提前对输入数据进行处理，但在NLP中先对输入数据进行特征工程是很重要的。 Features in Multinomial Logistic Regression假设document x含有词great,其class是 +($f_1$).则对应的 $f_1(c,x)$ $w_1(x)$ 表示great作为 class + 的权重。 Classification in Multinomial Logistic Regression这是一个简单的二分类positive or negative，其实也可以看起来跟图像是一样的，比如这里有4个词，也就是4个特征, $x = (1,1,1,1)^T$ $w_+=(0,0,0,1.9)$,$w__ =(0.7,0.9,-0.8)$ 在预测属于哪一类是，可以简化计算： Learning Logistic Regression怎么计算权重呢？ 训练样本数据通过 条件极大似然估计（conditional maximum likelihood estimation.） 对于单个样本 $(x^{(j)},y^{(j)})$ ，优化权重： $$\\hat w = argamx_w logP(y^{(j)}|x^{(j)})$$ 那么对于整个样本集： $$\\hat w = argamx_w \\sum_jlogP(y^{(j)}|x^{(j)})$$ 通过优化似然概率 $L(w)$ 来学习得到参数w $$L(w) = \\sum_jlogP(y^{(j)}|x^{(j)})\\tag{7.12}$$ 这里和我们前面讲过的softmax回归有点区别，softmax是先求出 $\\hat y$,然后与真实分布y进行比较，最小化差异；而multinomial logistic regression 是直接将真实分布y和观测数据x联合在一起最大化p(y|x). 同样也是一个凸优化问题（convex optimization problem），通过采用随机梯度上升～ $L’(w)关于权重求导$ 正则化当模型对训练数据过拟合（overfitting）时，给公式（7.12）增加正则化项，用来惩罚权重较大的项。 L2正则化Euclidean distance L1正则化Manhattan distance L1正则化和L2正则化都可以通过贝叶斯来解释～ L1正则化可以看作是权重满足Laplace分布. L2正则化可以看做是权重满足均值为0的高斯分布 以L2正则化为例，$w_j$服从高斯分布 然后假设均值 $\\mu=0$，$2\\sigma^2=1$,在对数域对w求导可得： 这与公式（7.17）一致～ 知乎上有一篇文章很好的解释了L1正则化与L2正则化 Feature Selection 特征选择对于生成模型如 naive bayes 无法使用正则化，因此需要 feature selection 如何进行特征选择，就是通过一些metric对特征进行排序，选择重要的特征。 information gain 这部分参考宗成庆老师的《统计自然语言处理》 信息增益（IG）法依据某项特征 $w_i$ 为整个分类所能提供的信息量的多少来衡量该特征项的重要程度。某个特征的信息增益指的是有该特征和没有该特征时，为整个分类所能提供的信息量的差别。其中，信息量的多少由熵来衡量。 因此信息增益即不考虑任何特征时文档的熵和考虑该特征后文档的熵的插值： $P(c_i)$ 表示训练样本中 $c_i$ 类文档的概率。 $P(w)$ 表示训练样本中包含特征w的文档占总文档数的概率。假设某一个文档中有两个词 ‘great’,那么需要将它数量变为1，这在chapter6中有讲到。 $P(c_i|w)$ 表示文档中包含特征w且类别为 $c_i$ 的概率。 在李航老师的《统计学习方法》中，决策树这一章中也有讲到使用信息增益来进行特征选择。 $$H(C|w) = P(w)\\sum_{i=1}^CP(c_i|w)logP(c_i|w)$$ 表示在特征w条件下对训练样本进行分类的不确定性，也就是条件熵的期望。 公式（7.23）中第一项是经验熵，就是对训练数据集进行不确定性的度量。第二项是经验条件熵，也就是在特征w给定的条件下对训练数据集进行分类的不确定性。 Choosing a classifier and features显然logistic回归要比naive bayes要好，因为naive bayes中假设特征 $f_1,f_2,…,f$ 相互独立，如果特征 $f_1 和 f_2$ 具有一定的相关性，那么naive bayes就overestimate这个特征。而logistic相比之下对具有相关性的特征的处理鲁棒性要强很多，如果 $f_1,f_2$ 完全正相关，那么他们的权重都会赋值减少为原来的 1/2. The overly strong conditional independence assumptions of Naive Bayes mean that if two features are in fact correlated naive Bayes will multiply them both in as if they were independent, overestimating the evidence. Logistic regression is much more robust to correlated features; if two features f 1 and f 2 are perfectly correlated, regression will simply assign half the weight to w 1 and half to w 2 . 当特征具有很强的相关性时，logistic的准确率要高于Naive bayes。但当数据集较小时，naive bayes的准确率要高于logistic和SVM，而且naive bayes更容易训练。 bias-variance tradeoff 偏差bias 较高: 欠拟合 underfitting 方差variance 较高： 过拟合 overfitting 如何选择各种分类器classifier: low bias : SVM with polynomial or RBF kernels, downweighting or removing features low variance: naive bayes, add more features feature interactions :特征工程很重要。 常见的分类器有：Support Vector Machines (SVMs) with polynomial or RBF kernels, and random forests. 总结 参考： Speech and language Processing，Chapter7 知乎：L1正则化与L2正则化 宗成庆，《统计自然语言处理》，第13章 李航，《统计学习方法》，第5章","link":"/2018/04/16/chapter7-logistic%E5%9B%9E%E5%BD%92/"},{"title":"chapter6-朴素贝叶斯和情感分类","text":"Naive bayes 模型 p(x|y) 训练：求根据极大似然估计（频率代替概率） p(y|x),p(x)，无参估计 优化：各种预处理和特征提取 验证模型： Precision, Recall, F-measure 对于多分类的处理 交叉验证 比较分类器：统计显著性测试 前言：文本分类(text categorization) 的应用： 情感分析（sentiment analysis）:the extraction of sentiment 垃圾邮件检测（spam detection） 作者归属（author attribution） 主题分类（subject category classification） 除了文本分类外，分类在NLPL领域还有很多其他应用： 句号消歧（period disambiguation） 单词标记（word tokenization） part-of-speech tagger 命名实体标注 named-entity tagging 本章节深入介绍 multinomial naive Bayes, 下一章将 nultinomial logistic regression, 又叫 maximum entropy. 同时，简单介绍了一下生成算法和判别算法的区别：可参考前面的机器学习-生成模型到高斯判别分析再到GMM和EM算法 进一步的理解下： 判别算法： $p(y|x;\\theta)$ 根据交叉熵 $H(\\hat y, y)$ 等损失函数，使用梯度下降等来优化参数 $theta$. 生成算法:p(x|y)， 可以分为两种，有标签的和无标签的 有标签的是监督学习，以高斯判别分析GDA和本章节中的Naive Bayes为例，就是根据 $p(y|x) = \\dfrac{p(x|y)p(y)}{p(x)}$,在有标签的情况下，可以根据MLE（用相对频度代替概率）来计算出 likelihood $p(x|y)$ 和先验概率prior $p(y)$,然后带入预测样本数据，得到对于每一个分类p(x|y=0,1,…)的概率，然后 $argmax_yp(x|y)$ 就是预测数据的类别。 无标签的是无监督学习，高斯混合模型GMM为例，需要引入隐藏变量z，计算 $p(x,z)=p(x|z)p(z)$,假设p(x|z)是多维高斯分布，然后使用EM算法对高斯分布的参数，以及先验概率p(z)进行估计~ 但因为类标签是不存在的，所以只能对样本数据进行聚类，而无法给对预测数据进行分类。 Naive Bayes Classifiers 朴素贝叶斯分类器样本数据： $(d_1,c_1),…,(d_N,c_N),\\quad c\\in C$ 贝叶斯分类器是一个概率分类器： $$\\hat c = argmax_{c\\in C}P(c|d)$$ Bayesian Inference: $$p(x|y) = \\dfrac{p(y|x)p(x)}{p(y)}$$ 则有： $$\\hat c = argmax_{c\\in C}P(c|d)=argmax_{c\\in C}\\dfrac{P(d|c)P(c)}{P(d)}$$ 可以直接去掉P(d),因为我们要计算对于每一类 $c\\in C$的概率 $\\dfrac{P(d|c)P(c)}{P(d)}$, 而P(d)对于任何类别都是不变的。而我们要求的是最后可能的class，其对应的P(d)都一样，所以可以直接去掉。 $$\\hat c = argmax_{c\\in C}P(c|d)=argmax_{c\\in C}P(d|c)P(c)$$ 其中P(d|c)是样本数据的 似然概率likelihood, P(c)是 先验概率prior,可以写成特征的形式： $$\\hat c = argmax_{c\\in C}P(f_1,f_2,…,f_n|c)p(c)$$ 显然 $P(f_1,f_2,…,f_n|d)$ 是很难计算的，以语言模型为例，你不可能考虑所有可能的词的组合，因此朴素贝叶斯分类器做了两个假设以简化模型： bag of words assumption：不考虑词的顺序，也就是说 love 这个词不管出现在 1st,20th等，它对这个sequence所属类别的影响是一样的。 naive bayes assumption:所有特征之间相互独立 $$P(f_1,f_2,…,f_n|c)=P(f_1|c)cdot P(f_2|c)\\cdots P(f_n|c)$$ 因此有： $$c_{NB}=argmax_{c\\in C}P(c)\\prod_{f\\in F}P(f|c)$$ 将朴素贝叶斯分类器应用于文本分类，我们需要考虑词的位置，这里其实也没有考虑词的顺序嘛。。 $$positions \\leftarrow all\\ word\\ position\\ in\\ test\\ document$$ $$c_{NB}=argmax_{c\\in C}P(c)\\prod_{i\\in position}P(w_i|c)$$ 为避免数值下溢，在对数域进行计算： $$c_{NB}=argmax_{c\\in C}logP(c)+\\sum_{i\\in position}logP(w_i|c)$$ Training the Naive Bayes Classifier分别计算概率P(c), $P(f_i|c)$ $$\\hat P(c)=\\dfrac{N_c}{N_{doc}}$$ 这里 $N_c$ 表示c类中所有词的总数，$N_{doc}$ 表示所有词的总数。 $$\\hat P(w_i|c)=\\dfrac{count(w_i,c)}{\\sum_{w\\in V}count(w,c)}$$ 分母表示c类中所有词的总数。词典V表示所有类别的词的总数，不仅仅是c类别的。 有个问题: 如果词“fantastic”，从未出现在 positive类别中，那么： 这是不合理的，所以采用 add-one smoothing: 这里要想清楚为啥是V，而不是c类中词的个数？要保证 $\\sum_iP(w_i|c)=1$ 参考习题： 对于 unknown words: 直接删除。。 对于 stop words: 也就是 the, a,..,这样高频词，可以删掉。也可以不管，效果差不多～ 算法流程 总结下： 1234567891011121314151617181920212223for each class in C: 计算 logprior[c] for each word in V: 计算 word在当前class下的likelihood loglikelihood[word,c]for each class in C: sum[c] = logprior[c] for each position in testdoc: if word in V: sum[c] += loglikehood[word,c] return argmax(sum[c]) Optimizing for Sentiment Analysisbinary multinominal naive Bayes or binary NB. 将每一个sequence或是documents中的重复词的数量变为1. 比如 great 出现在两个documents中，最后一个有2个great，那么positive中great总数为2，有点类似于归一化。任何一个词的数量不会超过document的数量。 deal with negation遇到logical negation(n’t, not, no, never)后，前置前缀 NOT_ 给每一个单词，直到遇到标点符号。 sentiment lexicons如果没有足够多的已经标注好的数据，可以使用预注释好的情感词。四个非常流行的词库： General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). 以 MPQA 为例，6885 words, 2718 positive and 4912 negative 话说怎么使用。。 Evaluation: Precision, Recall, F-measure 对于数据不均衡是，使用accuracy是不准确了。 举个栗子： a million tweet: 999,900条不是关于pie的，只有100条是关于pie的 对于一个stupid分类器，他认为所有的tweet都是跟pie无关的，那么它的准确率是99.99%！但这个分类器显然不是我们想要的，因而accuracy不是一个好的metric，当它目标是rare，或是complete unbalanced. 引入另外两个指标： 精度 precision: 是分类系统预测出来的positive的中真实positive的比例 召回 call： 是真实positive中分类系统也预测为positive的比例。 上面的例子中，目的是要找到tweet中和pie相关的一类，那么其召回率就是 0/10 = 0， 其precision是 0/0 Ｆ-measure $$F_{\\beta}=\\dfrac{(\\beta^2+1)PR}{\\beta^2P+R}$$ 当 $\\beta&gt;1$时，Recall的比重更大;当 $\\beta&lt;1$时，precision的比重更大。使用最多的是 $\\beta=1$,也就是 $F_{\\beta=1}, F_1$. $\\beta$ 的的取值取决于实际应用。 $$F_1 = \\dfrac{2PR}{P+R}$$ Ｆ-measure 是 precision 和 recall 的 **加权调和平均值(weighted harmonic mean)**。 调和平均值是倒数的算术平均值的倒数。 为什么要使用调和平均值呢？因为它是一个更 保守的度量(conservative metric). 相比直接计算 P 和 R 的平均值， F-measure的值更看重两者中的较小值。 More than two classes多分类有两种情况： 一种是一个document对应多标签，**（any-of or multi-label classification）** ：解决方法是对每一个类别使用二分类，比如对于类别c，可分类 positive labeled c 和 negative not labeled c. 另外一种是一个document只对应一个标签，但总的类别数大于2. (one-of or multinomial classification) confusion metrix: 将三个分类分开来看，pooled表示汇总 Test sets and Cross-validation 测试和交叉验证10折交叉验证： Statistical Significance Testing 统计显著性测试比较两个分类器A和B的好坏。 假设检验对于两个分类器 classifier A and B. 我们需要知道A的性能一定比B好吗？还是只是在特定的数据集上表现比B好？ null hypothesis: A is not really better than B，A 和 B在指标F-measure的差距是 $\\delta(x)$ 随机变量X是测试集的集合。 接受域： $H_0: P(\\delta (X)&gt; \\delta (x)|H_0)$ 当p-value(X)&lt;0.05 or 0.01,时我们拒绝假设 $H_0$ bootstrap test在NLP中通常不使用传统的统计方法，因为metrics并不是正态分布(normal distribution),违反了测试的假设。 对于Bootstrap知乎上有个比较清楚的答案：https://zhuanlan.zhihu.com/p/24851814 本质上，Bootstrap方法，是将一次的估计过程，重复上千次上万次，从而便得到了得到上千个甚至上万个的估计值，于是利用这不止一个的估计值，我们就可以估计待估计值的均值以外的其他统计量：比如标准差、中位数等。 应用在这里是因为但样本数据较少时，一次样本估计无法准确的计算出两个分类器A和B的差值，因而采用bootstrap. 参考： Speech and Language Processing ,3rd, Chapter 6 知乎：https://zhuanlan.zhihu.com/p/24851814","link":"/2018/04/14/chapter6-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%92%8C%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"title":"chapter4:语言模型和N元语法","text":"N-gram: Markov chain是其中一个特例 验证N-gram模型： 困惑度 预处理：泛化和zeros情况 smoothing：用来处理zeros～ 困惑度和交叉熵 前言: 通过实例，简单的介绍了assign probabilities to sequences of words 的重要性，以及在这些方面的应用： speech recognition, handwriting recognition, spelling correction, augmentative communication. 给句子或是词语赋予概率的模型就是语言模型(language models,LM)，这个chapter主要介绍一个简单的语言模型N-grams. 2-gram(bigram), 3-gram(trigram). N-grams不论是整个句子的概率还是一个序列中预测下一个单词的概率，都要使用概率模型。 计算一个完整的句子的概率 $P(w_1,w_2,…,w_n)?$ ，我们用 $w_1…w_n 或 w_1^n$ 来表示N个单词组成的句子。可使用链式法则计算概率： $$P(w_1…w_n) = P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)…P(w_n|w_1^{n-1})$$ $$P(w_1…w_n) = \\prod_{k=1}^nP(w_k|w_1^{k-1})$$ 但是 $P(w_n|w_1^{n-1})$ 很难计算,因为： 这样我们引入了N-gram模型，就是考虑预测单词之前的少数单词，而不是之前的所有词。比如 bigram 就是2-gram模型，生成下一个单词只依赖于前一个单词： $$P(w_n|w_1^{n-1}) = P(w_n|w_{n-1})$$ 也就是 Markov假设～针对语言来说可能不太合理吧，毕竟语言一两个词包含的信息并不多，但对于很多其他的，比如天气，只依赖于前两天是很靠谱的对吧～ 那么怎么估计bigram和N-gram的概率，一个直接的方法是 极大似然估计(maximum likelihood estimation, MLE). $$P(w_n|w_{n-1}) = \\dfrac{C(w_{n-1}w_n)}{C(w_{n-1})}$$ 其中我们需要对每个句子加上特殊的begin-symbol&lt; s &gt; 和 end-symbol &lt; /s &gt;. 对于一般的N元语法，其参数估计为： $$P(w_n|w_{n-N+1}^{n-1}) = \\dfrac{C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}$$ 这里前面N-1个单词同时出现的频度来除整个序列出现的频度，这个比值称为相对频度(relative frequency).在最大似然估计中，相对频度是概率估计的一种方法，即在给定模型M的情况下，计算得到的模型参数能使得训练集T的似然度P(T|M)最大。 Some pratical issues: 通常使用 trigran，4-gram,甚至是5-gram效果要比bigram更好。但这需要更大的语料库。 由于相对概率都是大于0小于1的,概率相乘之后会更小，可能会导致数值下溢(numerical underflow)。因此采用对数概率(log probabilities). Evaluating Language Models外在评估, extrinsic evaluation . end-to-end evaluation， 应用到实际，然后去评估模型的好坏，但这太麻烦了。 内在评估, metric: intrinsic evaluation 内在评估: : training set or training corpus. development test set or devset: fresh test set often: 80% training, 10% development, 10% test. Perplexity 这里简单的提了一下perplexity(困惑度)：perplexity is a normalized version of the probability of the test set，可以理解为 weight average barnching factor. barnching factor是指预测下一个词出现的概率。 在后面结合信息熵还会再次讲到。这里可以先简单的理解为： 测试集的概率 $P(w_1w_2…w_N)$ 越大，也就是准确率越高，其困惑度就越小。 同时也可以看到，N-gram中N越大，模型提取的信息越多，其困惑度就越小。 说到这儿，N元语法到底从句子中提取到了什么信息？ 总结下来，还是一个概率性的问题。什么词更容易出现在什么词身后，eat后面更容易出现名词和形容词，这是句法(syntactic)信息;在对话中，I 更容易出现在句首； 还有文化相关的，人们looking for Chinses food的概率比english food大。 Generalization and Zeros The N-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a given training corpus. Another implication is that N-grams do a better and better job of modeling the training corpus as we increase the value of N. the value of N 随着N的增加，外在评估效果也越来越好。但在4-gram中 it cannot be but so.这个是直接从 king henry 中得到的，原因是因为在莎士比亚文集中，it cannot be but下面可持续的单词只有5个(that, I, he, thou, so). 所以，语料库相对4-gram太小了。 在回顾以下整个过程： 在training data中，对语料库中的句子加上&lt;\\s&gt;（不需要加 &lt; s &gt;）,这里并没有训练，只是计算了对应的unigram, bigram, trigram的概率. 然后在test data上进行验证，来判断哪个模型好，那个坏？这是内在评估 然后外在评估就是每一次random一个词，在这个基础上根据概率random下一个词，直到生成&lt;\\s&gt;. the corpus不同的语料库训练出来的模型，生成得到的序列会相差很远。怎么解决这个问题呢？ How should we deal with this problem when we build N-gram models? One way is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish. To build a language model for translating legal documents, we need a training corpus of legal documents. To build a language model for a question-answering system, we need a training corpus of questions. zeros 这里是个大坑测试集中出现了训练集中没有出现的词，或者二元组，这里不是,可能offer在其他地方出现过，但在denied the后面出现的次数为0，那么它的概率就是0，然后困惑度perplexity就无穷大了，这显然是不合理的。如下图中所示。 这是个很重要的问题！后面smoothing算法都是在解决这个问题！ Unknow Wordsclosed vocabulary:测试集中的词也都是来自于词库的(lexicon),比如语音识别和机器翻译，并不会出现unknown词。 但我们也会需要处理一些我们没有见过的词 **unknow or out of vocabulary(OOV)**， 我们通过给 open vocabulary 增加一个 pseudo-word . 解决这个问题有两种常见的方法： 第一种就是把问题转换为 closed vocabulary，也就是提前建立一一个词汇表 话说我不是太理解这个方法，应该是只出现在test data中吧，那么在train data中的概率就是0啊，那么得到的模型的概率也是0啊，那么在test时，首先它肯定不会生成,而且碰到时，下一个词怎么生成。。。。 第二种方法还好理解一点，就是在train data中把出现字数较少的词当做，那么这个时候就像普通的词一样也是有它的概率的。但有一个缺点： it takes some time to be convinced. Smoothing 平滑主要是针对前面那个坑的，就是zeros的情况。不是，但是这个二元组或者是三元组在test data中出现了，在train data中从来没有出现过。 解决办法就是：shave off a bit of probability mass from some more frequent events and give it to the events we’ve never seen. This modification is called smoothing or discounting. 有以下这些方法：**add-1 smoothing, add-k smoothing, Stupid backoff, and Kneser-Ney smoothing.** Laplace Smoothing 拉普拉斯平滑事实上，关于前面一部分极大似然估计和laplace smoothing这一段没看太懂。但是拉普拉斯平滑整体还是能够理解的。 对于bigram: 下图分别是伯克利餐馆对话的语料库(V=1446)以及加1平滑后的bigram： 可以看到，分子是所有的零和非零项都加1，所以对于分母C(w_{n-1})来说，也就是上面二维数组中某一行都加1，也就是增加了一个词表的数量V。理论上来讲是可以的，但总感觉有点问题是吧？ $$P(want|I) = \\dfrac{C(want|I)}{C(I)}\\le \\dfrac{C(want|I)+1}{C(I)+V}$$ 显然这个数变小了！因为未出现过的trigram占了一定的概率空间，除此之外，p(to|i),p(chinese|i)…这些概率都认为相等也是否合理呢？ 这个博客blog中提到了我的疑问： 如此一来，训练语料中未出现的n-Gram的概率不再为 0，而是一个大于 0 的较小的概率值。Add-one 平滑算法确实解决了我们的问题，但显然它也并不完美。由于训练语料中未出现n-Gram数量太多，平滑后，所有未出现的n-Gram占据了整个概率分布中的一个很大的比例。因此，在NLP中，Add-one给训练语料中没有出现过的 n-Gram 分配了太多的概率空间。此外，认为所有未出现的n-Gram概率相等是否合理其实也值得商榷。而且，对于出现在训练语料中的那些n-Gram，都增加同样的频度值，这是否欠妥，我们并不能给出一个明确的答案。 Add-k smoothing$$P_{add-k}(w_i|w_{i-n+1}\\cdots w_{i-1}) = \\frac{C(w_{i-n+1}\\cdots w_i)+k}{C(w_{i-n+1}\\cdots w_{i-1})+k|V|}$$ 通常，add-k算法的效果会比Add-one好，但是显然它不能完全解决问题。至少在实践中，k 必须人为给定，而这个值到底该取多少却莫衷一是。 Backoff and Interpolation 回退和插值回退通常我们会认为高阶模型更加可靠，前面的例子也表明，当能够获知更多历史信息时，其实就获得了当前推测的更多约束，这样就更容易得出正确的结论。所以在高阶模型可靠时，尽可能的使用高阶模型。但是有时候高级模型的计数结果可能为0，这时我们就转而使用低阶模型来避免稀疏数据的问题。 回退的三元语法： $$\\hat P(w_i|w_{i-2}w_{i-1}) =\\begin{cases}P(w_i|w_{i-2}w_{i-1})&amp;,if\\quad C(w_{i-2}w_{i-1})&gt;0\\ \\alpha_1P(w_i|w_{i-1})&amp;,if\\quad C(w_{i-1})&gt;0\\ \\alpha_2P(w_i)&amp;, otherwise \\end{cases}$$ 一般性的回退模型,又叫 Katz backoff ： $$P_BO(w_n|w_{n-N+1}^{n-1}) =\\begin{cases} \\tilde P(w_n|w_{n-N+1}^{n-1})&amp;, if\\quad C(w_{n-N+1 \\cdots w_{n-1} }&gt;0)\\ \\theta(P(w_n|w_{n-N+1}^{n-1}))\\alpha P_BO(w_n|w_{n-N+2}^{n-1})&amp;,otherwise\\end{cases}$$ 其中： $$\\theta(x) =\\begin{cases}1&amp;,if\\quad x=0\\ 0&amp;,otherwise\\end{cases}$$ 试想一下：为什么这里要用 $\\alpha$ ,没有会怎么样？ 如果没有 $\\alpha$ 值，等式就不是一个概率了，因为三元语法是根据相对频度来计算的，原本为0的概率，回退到一个低阶模型后，相当于把多余的概率量加到等式中来，这样一来，单词的总概率就将大于1. 因此，所有的回退模型都要打折（discounting）. 其中 $\\tilde P$ 用于给最大似然估计MLE的概率打折,也就是直接从计数计算出来的旧的相对频度节省概率量。$\\alpha$ 用于保证低阶N元语法概率量之和恰好等于前面 $\\tilde p$ 省下来的概率量。 插值插值和回退的思想其实非常相像。设想对于一个trigram的模型，我们要统计语料库中 “like chinese food” 出现的次数，结果发现它没出现过，则计数为0。在回退策略中，将会试着用低阶gram来进行替代，也就是用 “chinese food” 出现的次数来替代。 在使用插值算法时，我们把不同阶别的n-Gram模型线形加权组合后再来使用。简单线性插值（Simple Linear Interpolation） 可以用下面的公式来定义： $$\\hat P(w_n|w_{n-1}w_{n-1})=\\lambda_1P(w_n|w_{n-2}w_{n-1})+\\lambda_2P(w_n|w_{n-1})+\\lambda_3P(w_n)$$ 其中: $\\sum_i\\lambda_i=1$ $\\lambda_i$ 可以根据试验凭经验设定，也可以通过应用某些算法确定，例如EM算法。 在简单单线形插值法中，权值 $\\lambda_i$ 是常量。显然，它的问题在于不管高阶模型的估计是否可靠（毕竟有些时候高阶的Gram计数可能并无为 0），低阶模型均以同样的权重被加入模型，这并不合理。一个可以想到的解决办法是让 λi 成为历史的函数。也就是 条件插值（conditional interpolation） 则有: $$\\hat P(w_n|w_{n-1}w_{n-1})=\\lambda_1(w_{n-2}w_{n-1})P(w_n|w_{n-2}w_{n-1})+\\lambda_2(w_{n-2}w_{n-1})P(w_n|w_{n-1})+\\lambda_3(w_{n-2}w_{n-1})P(w_n)$$ 可使用EM算法来训练 $\\lambda$ 的值，使得从主语料库中分出来的语料库的似然度最大。具体怎么训练的。。看文献吧，(Jelinek and Mercer, 1980). absolute discounting想想之前的Add-one，以及Add-k算法。我们的策略，本质上说其实是将一些频繁出现的 N-Gram 的概率匀出了一部分，分给那些没有出现的 N-Gram 上。因为所有可能性的概率之和等于1，所以我们只能在各种可能的情况之间相互腾挪这些概率。 既然我们打算把经常出现的一些N-Gram的概率分一些出来，其实也就等同于将它们出现的次数减去（discount）一部分，那到底该discount多少呢？Church &amp; Gale (1991) 设计了一种非常巧妙的方法。首先他们在一个 留存语料库（held-out corpus） 考察那些在训练集中出现了4次的bigrams出现的次数。具体来说，他们首先在一个有2200万词的留存语料库中检索出所有出现了4次的bigrams （例如： “chinese food”，“good boy”，“want to”等），然后再从一个同样有2200万词的训练集中，分别统计这些bigrams出现的次数（例如：C(“chinese food”)=4，C(“good boy”)=3，C(“want to”)=3）。最终，平均下来，他们发现：在第一个2200万词的语料中出现4次的bigrams，在第二个2200万词的语料中出现了3.23次。下面这张表给出了 c 从0到9取值时（也就是出现了 c 次），统计的bigrams在留存集和训练集中出现的次数。 其实你应该已经发现其中的规律了。除了计数为0和为1的bigram之外，held-out set中的平均计数值，都大约相当于training set中的计数值减去0.75。 基于上面这个实验结果所诱发的直觉，Absolute discounting 会从每一个计数中减去一个（绝对的）数值 d。这样做的道理就在于，对于出现次数比较多的计数我们其实已经得到了一个相对比较好的估计，那么当我们从这个计数值中减去一个较小的数值 d 后应该影响不大。上面的实验结果暗示在实践中，我们通常会对从2到9的计数进行处理。 $$P_{AbsDiscount}(w_i|w_{i-1})=\\frac{C(w_{i-1}w_i)-d}{C(w_{i-1})}+\\lambda(w_{i-1})P(w_i)$$ 好好理解下： 就是通过 held out set 的对比得到的直觉后，我们在train data中，将所有的 $C(w_{i-1}w_i)$ 减去一个数d，分母不变，概率肯定变小了，但只是减去一个很小的数，影响并不大。然后加上 $\\lambda(w_{i-1})P(w_i)$，这样一来，原本bigram中为0的概率就不是0了，因为unigram肯定不为0嘛～但后面这一项怎么求，是接下来一部分的内容 从上一篇文章中，我们已经知道，对于bigram model而言，$P(w_i|w_{i−1})=C(w_{i−1}w_i)/C(w_{i−1})$，所以上述方程等号右侧第一项即表示经过 discounting 调整的概率值，而第二项则相对于一个带权重 λ 的 unigram 的插值项。通常，你可以把 d 值就设为 0.75，或者你也可以为计数为 1 的 bigram 设立一个单独的等于 0.5 的 d 值（这个经验值从上面的表中也能看出来）。 Kneser-Ney discountingKneser-Ney discounting是在absolute discounting的基础上，对低阶gram进行一些处理～ 这个博客对这部分讲的非常清楚：https://blog.csdn.net/baimafujinji/article/details/51297802 ，我就直接贴过来了。。。 如果我们使用bigram和unigram的插值模型来预测下面这句话的下一个词： I can’t see without my reading _______. 我们的直觉是 glasses，但是如果语料库中出现 Kong 的频率非常高，因为 Hong Kong 是高频词。所以采用unigram模型， Kong 具有更高的权重，所以最终计算机会选择 Kong 这个词（而非glasses）填入上面的空格，尽管这个结果看起来相当不合理。这其实就暗示我们应该调整一下策略，最好仅当前一个词是 Hong 时，我们才给 Kong 赋一个较高的权值，否则尽管在语料库中 Kong 也是高频词，但我们并不打算单独使用它。 如果说 P(w) 衡量了 w 这个词出现的可能性，那么我们现在想创造一个新的 unigram 模型，叫做 $P_{continuation}$ ，它的意思是将 w 这个词作为一个新的接续的可能性。注意这其实暗示我们要考虑前面一个词（即历史）的影响。或者说，为了评估 $P_{continuation}$ （注意这是一个 unigram 模型），我们其实需要考察使用了 w 这个词来生成的不同 bigram 的数量。注意这里说使用了 w 这个词来生成的不同类型 bigram 的数量，是指当前词为 w ，而前面一个词不同时，就产生了不同的类型。例如：w = “food”, 那么不同的 bigram 类型就可能包括 “chinese food”，“english food”，“japanese food”等。每一个 bigram 类型，当我们第一次遇到时，就视为一个新的接续（novel continuation）。 也就是说 $P_continuation$ 应该同所有新的接续（novel continuation）构成的集合之势（cardinality）成比例。所以，可知: $$P_{continuation}(w_i)\\propto \\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert$$ 如果你对此尚有困惑，我再来解释一下上面这个公式的意思。当前词是 $w_i$，例如“food”，由此构成的不同类型的 bigram 即为 $w_{i−1}w_i$，其中 $w_{i−1}$ 表示前一个词（preceding word）。显然，所有由 $w_{i−1}w_i$ 构成的集合的势，其实就取决于出现在 $w_i$ 之前的不同的 $w_{i−1}$ 的数量。 然后，为了把上面这个数变成一个概率，我们需要将其除以一个值，这个值就是所有 bigram 类型的数量，即 $\\lvert {(w_{j-1},w_j):C(w_{j-1}w_j)&gt;0}\\rvert$,这里大于0的意思就是“出现过”。于是有: $$P_{continuation}(w_i)=\\frac{\\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert}{\\lvert{(w_{j-1},w_j):C(w_{j-1}w_j)&gt;0}\\rvert}$$ 也可以写成这种形式： $$P_{continuation}(w_i)=\\frac{\\lvert { w_{i-1}:C(w_{i-1}w_i)&gt;0}\\rvert} {\\sum_{w’_ {i-1}} \\lvert{w’_ {i-1}:C(w’_ {i-1}w’_ i)&gt;0}\\rvert}$$ 其实要理解这个很简单，绝对值里面的看成一个字典 dict,出现 $w_{i-1}w_i$ 了一次它的数量就是1，以 $P_continuation(food)$ 为例，语料库中出现了 chinese food, japan food, english food,那么分子就是计算不同 $w_{i-1}$ 的个数，其大小就是3。 而分母呢，就是所有的bigram，放到dict中，相当于去重，然后它的总数就是分母的值。即所有不同的 bigram 的数量就等于出现在单词 $w’_ i$ 前面的所有不同的词 $w’_ {i−1}$ 的数量，这个计算复杂度应该就是V吧，遍历整个词表即可。 如此一来，一个仅出现在 Hong 后面的高频词 Kong 只能获得一个较低的接续概率（continuation probability）。由此，再结合前面给出的Absolute Discounting 的概率计算公式，就可以得出 Interpolated Kneser-Ney Smoothing 的公式，即 $$P_{KN}(w_i|w_{i-1})=\\frac{max(C(w_{i-1}w_i)-d,0)}{C(w_{i-1})}+\\lambda(w_{i-1})P_{continuation}(w_i)\\tag{4.33}$$ 其中，$max(C(w_{i−1}w_i)−d,0)$ 的意思是要保证最后的计数在减去一个 d 之后不会变成一个负数。其次，我们将原来的 $P(w_i)$ 替换成了 $P_continuation(w_i)$。此外，**$\\lambda$ 是一个正则化常量，用于分配之前discount的概率值**（也就是从高频词中减去的准备分给那些未出现的低频词的概率值）： $$\\lambda(w_{i-1})=\\frac{d}{C(w_{i-1})}\\cdot \\lvert {w:C(w_{i-1},w)&gt;0}\\rvert\\tag{4.34}$$ 以 $P_{KN}(Kong|Hong)$ 为例，（4.33）式中第一项是对bigram (Kong|Hong)打了折，那么后面一项应该在此基础上补回来一点。 （4.34）第一项 $\\dfrac{d}{C(w_{i-1})}$ 是折扣归一化（normalized discount）；第二项 $\\lvert {w:C(w_{i-1},w)&gt;0}\\rvert$ 是打折的次数。 不太好理解，看原文章： 将上述公式泛化，可用递归表示为： $$P_{KN}(w_i|w_{i-n+1}\\cdots w_{i-1})=\\frac{max(0,C_{KN}(w_{i-n+1} \\cdots w_i) - d)}{C_{KN}(w_{i-n+1}\\cdots w_{i-1})} +\\lambda(w_{i-n+1}\\cdots w_{i-1})\\cdot P_{KN}(w_i|w_{i-n+2}\\cdots w_{i-1})$$ 其中 $C_{KN}$ 取决于用什么插值方式，在（4.33）式中只采用了bigram，如果采用trigram，bigram，unigram的联合插值，那么对于最高阶的 trigram 在计数时并不需要使用接续计数（采用普通计数即可），而其他低阶，即 bigram 和 unigram 则需要使用接续计数。这是因为在 unigram 中，我们遇到了一个 Kong，我们可以参考它的 bigram，同理在 bigram，我还可以再参考它的 trigram，但是如果我们的插值组合中最高阶就是 trigram，那么现在没有 4-gram来给我们做接续计数。用公式表示即为： $$C_{KN}(\\cdot)=\\begin{cases}count(\\cdot) &amp;,for\\ the\\ highest\\ order\\ continuationcount(\\cdot) &amp;,for\\ all\\ other\\ lower\\ orders \\end{cases}$$ 为什么低阶的gram要采用continuation呢？当trigram的概率为0时，使用bigram会增大概率量，所以都要打折，所以对应的bigram也得用continuation才好对吧～ 我们前面提到Kneser-Ney Smoothing 是当前一个标准的、广泛采用的、先进的平滑算法。这里我们所说的先进的平滑算法，其实是包含了其他以 Kneser-Ney 为基础改进、衍生而来的算法。其中，效果最好的Kneser-Ney Smoothing 算法是由Chen &amp; Goodman（1998）提出的 modified Kneser-Ney Smoothing 算法，它对 discount d使用不同的值 $d_1,d_2,d_3$ 分别对应于unigram, bigram, trigram等等。很多NLP的开发包和算法库中提供有原始的Kneser-Ney Smoothing（也就是我们前面介绍的），以及modified Kneser-Ney Smoothing 算法的实现。 The Web and Stupid Backoff对于特别大的语言模型，效率考虑很重要。 它不是将每个单词存储为一个字符串，而是通常在内存中将其表示为64位散列号，并将单词本身存储起来。通常只使用4-8位（而不是8字节的浮点数）对概率进行量化，并将N-gram存储在反向尝试中。 N-gram也可以通过修剪来缩小，例如只存储计数大于某个阈值的N-gram（例如用于Google N-gram发布的计数阈值40）或使用熵修剪不太重要的N-grams（Stolcke，1998）。 stupid Backoff 没有discounting，没那么多复杂的方法，就是简单的回退到上一阶，所以不是一个概率分布，概率加起来大于1了。如果回退到unigram, $S(w)=\\dfrac{count(w)}{N}$,并且 $\\lambda$ 使用的0.4. Advanced: Perplexity’s Relation to Entropy熵 entropy困惑度是来自信息论中的交叉熵cross-entroy的概念。 前面我的一篇博客从信息论的角度提到了交叉熵：blog 对于一个随机变量 $X=(X_1…X_n)$，它的分布是p(x),那么这个随机变量的熵就是： $$H(X)=-\\sum p(x)logp(x)$$ 对数底数可以为任何数。$-logp(x)$ 是香农定义的信息量，如果我们假设底数为2，那么信息量 $-log_2p(x)$ 表示描述X随机变量取X_1时所需的编码长度，p(X=X_1)概率越大，所需的编码长度越小。那么熵H(X)就表示描述随机变量X编码长度的期望了。 作者这里用一个例子描述了熵这个概念： 文章中对概率最大的 Horse1,其信息量 $-log_2(1/2)=1$,其用二进制编码就是0， Horse2 其信息量就是 $-log_2(1/4)=2$,其用二进制编码就是 10…. 可以看到概率越大，所需的编码长度越小，也就是信息量越小。可以极端点想，对于一个随机事件，如果其分布是(1,0,0,0)和(1/4,1/4,1/4,1/4)，同样发生10次，可能前者的分布就判断出来了，但后者需要更多。那么用来描述这个随机变量的不确定性的度量，就是熵了，也就是信息量的期望。 总结下：所谓熵，就是信息量 -logp(x) 的期望。 信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。 这里引入到语言模型中，对于一个sequence可以看做一个随机变量，$W={ w_0,w_1,…,w_n }$, 对于一个有限长度为n的sequence，其熵为： $$H(w_1,w_2,…,w_n)=-log_{w_1^n\\in L}p(w_1^n)logp(w_1^n)$$ entropy rate, 也可以看作是per-word entropy: $$\\dfrac{1}{n}H(w_1,w_2,…,w_n)=-\\dfrac{1}{n}log_{w_1^n\\in L}p(w_1^n)logp(w_1^n)$$ 交叉熵 cross-entropy但我们在考虑language的熵时，需要认为sequence是无限长度的。可以把sequences生成的过程看成是一个随机过程stochastic process L, 那么L的熵： $$H(L)=-lim_{n\\to \\infty}\\dfrac{1}{n}H(w_1,w_2,…,w_n)=-lim_{n\\to \\infty}\\sum_{W\\in L}p(w_1,w_2,…,w_n)logp(w_1,w_2,…,w_n)$$ cross-entropy: 但我们不知道生成数据的真实分布p时，可以用分布m来表示： $$H(p,m)=lim_{n\\to \\infty}-\\dfrac{1}{n}\\sum_{W\\in L}p(w_1,w_2,…,w_n)logm(w_1,w_2,…,w_n)$$ 对于静态随机过程(stationary ergodic process),静态假设在HMM中有讲到：下一个词对上一个词的依赖的概率，不会随时间的改变而改变。那么可以认为 $p(w_1,w_2,…,w_n)$ 是一个定值。 $$H(p,m)=lim_{n\\to \\infty}-\\dfrac{1}{n}\\sum_{W\\in L}logm(w_1,w_2,…,w_n)\\tag{4.47}$$ 又有：$$H(p)\\le H(p,m)$$ 因此我们可以用m分布来估计p分布。 困惑度和交叉熵N-gram模型 $M=P(w_i|w_{i-N+1} \\cdots w_{i-1})$ 生成序列sequences W, 根据公式(4.47): $$H(W) = -\\dfrac{1}{N}logP(w_1w_2 \\cdots w_N)$$ 基于模型 $M=P(w_i|w_{i-N+1} \\cdots w_{i-1})$ 的困惑度perplexity可定义为交叉熵的指数形式： 似乎也是有点巧啊，交叉熵和困惑度都可以用来表征p分布是否接近真实分布～厉害了 总结 ###参考： [1] Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin, 3rd. Chapter 4 [2] 自然语言处理中N-Gram模型的Smoothing算法","link":"/2018/04/12/chapter4-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8CN%E5%85%83%E8%AF%AD%E6%B3%95/"},{"title":"chapter8-神经网络和自然语言模型","text":"Embeddings 这部分简单的看一下吧～毕竟已经很熟了。。。 chapter 8 - neural networks chapter 15 - semantic representations for words called embeddings chapter 25 - sequence-to-sequence (seq2seq model), applied to language generation: machine translation, conversation agents and summarization. Neural Language Models相比前面chapter4介绍的paradigm(the smoothed N-grams),依据神经网络的语言模型有很多优势～ 不需要smoothing？ 能够依赖更长的历史依据 handle much longer histories 泛化能力更强 除此之外，还是生成模型的基础～ Embeddings词向量： 为什么要用向量表示词？ Vectors turn out to be a really powerful representation for words, because a distributed representation allows words that have similar meanings, or similar grammatical properties, to have similar vectors. 相近意思和相似语法性质的词，具有相似的向量。 用神经网络模型训练词向量，这里是用的4-gram，生成下一个词取决于前三个词： input layer: 1x|V| embedding vector matrix: dx|V| projection layer: 1x3d hidden layer: W.shape (dhx3d) -&gt; 1xdh output layer: U.shape (|V|xdh) -&gt; 1x|V| softmax : $P(w_t=i|w_{t-1},w_{t-2},w_{t-3})$","link":"/2018/04/17/chapter8-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"title":"chapter27-Question Answering","text":"Speech and language Processing, chapter27:Question Answering Question Answering System什么是 Question Answering?主要分为两类： 多文档的智能问答系统 针对一系列文档提出的问题 答案可能出现多次，也可能没有出现 主要应用在于互联网搜索引擎，文本资料库的搜索，比如新闻档案、医学文献、科学文章等 单个文档的智能问答 Question Type Simple (factoid) questions (most commercial systems): 简单的问题，可以用简单的事实回答，答案简短通常是一个 named entity Who wrote the Declaration of Independence? What is the average age of the onset of autism? Where is Apple Computer based? Complex (narrative) questions: 稍微复杂的叙述问题，答案略长 What do scholars think about Jefferson’s position on dealing with pirates? What is a Hajj? In children with an acute febrile illness, what is the efficacy of single medication therapy with acetaminophen or ibuprofen in reducing fever? Complex (opinion) questions: 复杂的问题，通常是关于观点／意见 Was the Gore/Bush election fair? 问答系统分类现代智能问答系统的主要有两个范式(two major modern paradigms of question answering)以及混合方法， 都是关注于 **事实性的回答(factoid question)**。: IR-based question answering 基于信息检索的智能问答 knowledge-based question answering 基于知识库的智能问答 Hybrid approaches (IBM Watson) 对比下三者，具体了解之后可以再会过头来看看： IR-based Factoid question answeringIR-based factoid AQ 的流程图，包括三个阶段：问题处理(question processing), 篇章检索(passage retrieval and ranking), 和 答案处理(answer processing). Question Processing Answer Type Detection: 分析 question，决定 answer type Query Formulation: 形成合适的查询语句进行检索 Passagge Retrieval 通过检索得到 top N documents 把 documents 拆分称合适的单位(unit/passage) Answer Processing 得到候选的 answer 进行排序，选出最佳 answer Question Processing两个任务： answer type detection what kind of entity (person, place) is the answer? 形成合适的 query what is the query to the IR system Answer type: the kind of entity the answer consists of (person, location, time, etc.) Query: the keywords that should be used for the IR system to use in searching for documents Focus: the string of words in the question that are likely to be replaced by the answer in any answer string found 在这个阶段需要做的事： 举个栗子： Answering Type Detection (Question classification)通常而言，我们可以把它当作一个机器学习的分类问题。 定义类别 注释训练数据，给数据打上分类标签 训练分类器，所用特征可以包括 hand-written rules. 定义分类类别前人已经提供了一些 answer type 的层次型分类结构，如 answer type Taxonomy(from Li &amp; Roth). Two-layered Taxonomy 6 coarse classes:（coarse-grained tag） ABBEVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC_VALUE 50 fine classes:（fine-grained tags） HUMAN: group, individual, title, description ENTITY: animal, body, color, currency… LOCATION: city, country, mountain… 提取特征将 answer type 看作一个监督学习任务。question中可以用来分类的特征： words in the questions, the part-of-speech of each word named entities in the questions answer type word or question headword: 通常 wh-word 之后的第一个 NP 可以用来作为特征 semantic information about the words，WordNet synset ID 分类方法总结下可以用来分类的方法： hand-written rules machine learning hybrids 其中 rules 包括： regular expression-baesd rules $\\text{who {is | was | are | were} PERSON}$ PERSON（YEAR-YEAR） Other rules use the question headword headword of first noun phrase after wh‐word 当然也可以把上述某些分类方法当作特征一起进行训练。就分类效果而言，PERSON, LOCATION, TIME 这类的问题类型有更高的准确率，REASON，DESCRIPTION 这类的问题更难识别。 Query Formulation根据 question 产生一个 keyword list，作为 IR 系统的输入 query。可能的流程是去除 stopwords，丢掉 question word(where, when, etc.)，找 noun phrases，根据 tfidf 判断 keywords 的去留等等。 如果 keywords 太少，还可以通过 query expansion 来增加 query terms。 keyword selection algorithm: select all non-stop words in quotations select all NNP words in recognized named entities select all complex nominals with theor adjectival modifiers select all other complex nominals select all nouns with their adjetival modifiers select all other nouns select all verbs select all adverbs select the QFW word(skipped in all previous steps) select all other words 最终得到的 query: Passage Retrieval有了 query，我们进行检索，会得到 top N 的文档，然而文档并不是得到 answer 的最好的单位，下一步我们需要从文档抽取 potential answer passages，来方便后面的 answer processing。passage 可以是 sections, paragraphs, sentence，具体情况具体分析。 1.rank the documents by relevance 2.extract a set of potential answer passages from the retrieved set of documents. the passage could be sections, paragraphs, sentences. 3.passage retrieval: run a named entity or answer type classification on the retrieved passages. 然后对剩下来的 passages 进行排序，通过监督学习进行分类，有以下一些列特征： 对于基于 web 的 QA 系统，我们可以依靠网页搜索来做 passage extraction, 简单的说，可以把网页搜索产生的 snippets 作为 passages。 总结下 passage retrieval: Answer Processing现在，我们已经有了 answer type，也选出了相关的 passages，就可以进一步缩小 candidate answer 的范围。 answer-extraction task 主要有两种方法 answer-type pattern extraction基于正则化匹配的机制。如果 answer types 是 HUMAN 或者 DISTANCE-QUANTITY. 可以直接通过模式匹配得到： 有时候光用 pattern-extraction 方法是不够的，一方面我们不能创造规则，另一方面 passage 里也可能有多个 potential answer。另外，对于没有特定定命名实体类型的答案，我们可以使用正则表达式(人工编写或自动学习)。 真“人工”智能！！！ 前两天北理工交流会，有个自动化所的博士讲的论文就是用神经网络来生成很自然的回答，赞！ N-gram tiling/redundancy-based approachN-gram tiling 又被称为 redundancy-based approach(Brill et al. 2002, Lin 2007)，基于网页搜索产生的 snippet，进行 ngram 的挖掘，具体步骤如下： N-gram mining 提取每个片段中的 unigram, bigram, and trigram, 并赋予权重 N-gram filtering 根据 ngram 和预测的 answer type 间的匹配程度给 ngram 计算分数 N-gram tiling 将重叠的 ngram 连接成更长的答案 **standard greedy method** &gt; 1. start with the highest-scoring candidate and try to tile each other candidate with this candidate 2. add the best-scoring concatenation to the set of candidates 3. remove the lower-scoring candidate 4. continue until a single answer is built 其中怎么求这个 weigth， 也就是 ngram 和 answer type 的匹配度： Ranking Candidate AnswersIn other cases we use machine learning to combine many rich features about which phrase is the answer 可能用到的 feature: 总结 IR-based question answering factoid question answering answer type detection query formulation passage retrieval passage ranking answer extration web-based factoid question answering Knowledge-based Question Answering当大量的信息以结构化的形式存储时， 通过语义分析(semantic parsers) 将 query 映射成一个 logical form. 未完待续。。 reference: Question Answering 28.Question Answering NLP 笔记 - Question Answering System","link":"/2018/06/27/chapter27-Question-Answering/"},{"title":"chapter9 隐马尔可夫模型","text":"chapter6: 隐马尔科夫模型 standford tutorial, 可以说是看过的关于隐马尔科夫最好的教程了。要是看英文原版能和看中文一样easy该多好，可惜文化差异的情况下，即使是单词都认识，理解起来也会有点困难。对此，只能自己重新总结一下吧～ 可以说，斯坦福从未让人失望过，太赞了！ 也是无意中在google上看到这篇文章，才发现了这么好的一本书, Speech and language processing. 前言： 隐马尔可夫模型马尔可夫模型的后裔，它是一个序列模型(sequence model). 对于一个序列模型，它的工作是给序列中的每一个小单元分配标签label或是类别class.其中包括：part-of-speech tagging, named entity tagging, and speech recognition. 马尔可夫链 Markov chains马尔可夫链和隐马尔科夫模型都是有限自动机(finite automation)的拓展。可以将他们看作是有权重的有限自动机(weighted finite automation),包括一些列状态和状态之间的转移关系，其中从一个状态到另一个状态的转移弧线是有权重的。在马尔可夫链中，这样的权重代表着概率，且从一个节点出来的概率之和为1. 上图中不仅包括了状态之间的转移概率，还包括了start和end两种特定的状态。 | states | hot1 | cold2 | warm3 | | —— | ——– | ——– | ——– | | hot1 | $a_{11}$ | $a_{12}$ | $a_{13}$ | | cold2 | $a_{21}$ | $a_{22}$ | $a_{23}$ | | warm3 | $a_{31}$ | $a_{32}$ | $a_{33}$ | start0: {$a_{01},a_{02},a_{03}$} end4: {$a_{14},a_{24},a_{34}$} 根据图9.1，马尔可夫链可以看做是一个概率图模型，其中包括以下几部分： Q代表状态集合，A是状态转移矩阵， $a_{ij}$ 表示从状态 $q_i$ 到状态 $q_j$ 的概率,那么有 $\\sum_{j=1}^na_{ij}=1,i=1,2…N$ $q_0$ 和 $q_F$是初始状态和终止状态。 做两个重要的假设： 以简化模型 (1) The Limited horiqon assumption 齐次假设 对于t时刻的状态，只取决于之前的一个状态。 $$ Markov Assumption: P(q_i|q_1…q_{i-1})=P(q_i|q_{i-1})$$ 也就是一阶马尔科夫链(a first-order Markov)。 图中的 $x_i$ 表示可观测时的 $q_i$ (2) Stationary process assumption 静态过程假设 the conditional distribution over next state given current state does not change over time. 对于状态之间的条件概率不会随时间的变化而改变，也就是状态转移矩阵只有一个～ $$P(q_t|q_{t-1})=P(q_2|q_1);t \\in 2…T$$ 在很多其他的论文中，用 $\\pi$ 来表示初始状态 都是一样的，不过这篇教程中用第一种表示方法。 隐马尔可夫模型为了更形象的描述隐马尔可夫模型，文章举了这样的一个栗子： Imagine that you are a climatologist in the year 2799 studying the history of global warming. You cannot find any records of the weather in Baltimore, Maryland, for the summer of 2007, but you do find Jason Eisner’s diary, which lists how many ice creams Jason ate every day that summer. Our goal is to use these observations to estimate the temperature every day. We’ll simplify this weather task by assuming there are only two kinds of days: cold (C) and hot (H). So the Eisner task is as follows: Given a sequence of observations O, each observation an integer corresponding to the number of ice creams eaten on a given day, figure out the correct ‘hidden’ sequence Q of weather states (H or C) which caused Jason to eat the ice cream. 总结下就是，观察到的序列是每天吃的冰淇淋数目2,3,1,2,3,….，从而判断每天的天气是hot or cold这两种状态中的哪一种。 定义一个隐马尔可夫模型，有以下组成部分： 两个假设 (1)一阶马尔科夫模型： $$ Markov Assumption: P(q_i|q_1…q_{i-1})=P(q_i|q_{i-1})$$ (2)条件独立，在状态 $q_i$ 的条件下，观测 $o_i$ 只取决于 $q_i$， 而与其他的状态和观测值够无关 $$Output Indepence: P(o_i|q_1…q_i,…,q_T,o_1,…,o_i,…,o_T)=P(o_i|q_i)$$ 对ice cream task.问题的描述如下图： 注意到，图中所有的概率都不为零，这种HMM模型叫做 fully connected 或是 ergodic HMM. 但并不是所有状态都可以互相转移的，比如 left-to-right HMM,又称Bakis HMMs,通常用于语音处理。 左图表示Bakis HMM，右图是ergodic HMM. 关于隐马尔可夫模型的三个问题： 问题1：计算似然概率。 前向/后向算法 问题2：解码问题，又称预测问题，已知模型参数和观测序列，求最有可能的状态序列 维特比算法 问题3：学习问题，已知观测序列，估计模型参数使得该模型下观测序列的概率最大。 极大似然估计，Baum-Welch, EM算法 概率计算：前向算法状态已知的话，是监督学习以图9.3为例，观测序列为（3,1,3）假如我们知道隐藏状态是（hot, hot, cold）的话，在此基础上计算似然概率就很简单了。 也就是说，已知观测序列 $O=o_1,o_2,…,o_T$. 且隐藏状态序列已知 $Q=q_0,q_1,…,q_T$,那么似然概率为： $$P(O|Q)=\\prod_{i=1}^TP(o_i|q_i)$$ 状态无法观测，无监督学习但大多数情况下，状态是不知道的，因为我们需要取计算出现观测序列 (3,1,3) 的所有可能的隐藏状态序列。 观测序列 $O=o_1,o_2,…,o_T$，假定存在一个特定的隐藏状态序列 $Q=q_0,q_1,…,q_T$，那么联合概率分布： $$P(O,Q)=P(O|Q)\\times P(Q)=\\prod_{i=1}^Tp(o_i|q_i)\\times \\prod_{i=1}^TP(q_i|q_{i-1})\\tag{9.10}$$ 所以隐马尔科夫是一个双重随机过程。 那么观察序列为（3,1,1）和状态序列为（hot, hot, cold）的联合概率为： 这样我们知道了怎么求一个特定的隐藏序列和观测序列的联合概率，那么所有可能隐藏序列的类和就是观测序列的总似然概率了。 $$P(O)=\\sum_QP(O,Q)=\\sum_QP(O|Q)P(Q)\\tag{9.12}$$ 对冰淇淋的例子，如果观测序列是（3,1,3），那么似然概率为： 如果有N中状态的话，对于长度为T的序列，其计算复杂度就是 $O(N^T)$ 这就太大了，所以得寻求更简单的解法。 forward algorithm前向算法是一种动态规划算法，其计算复杂度是 $O(N^2T)$ The forward algorithm is a kind of dynamic programming algorithm, that is, an algorithm that uses a table to store intermediate values as it builds up the probability of the observation sequence. The forward algorithm computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a single forward trellis. 之所以高效的原因，是它将所有的路径都隐式的折叠到一个前向网格中。。 前向网格(forward trellis)中的每一个单元(cell) $\\alpha_t(j)$ 表示给定模型 $\\lambda$，t时刻观测序列为 $o_1,o_2,…,o_t$,状态为j的概率。 $$\\alpha_t(j)=P(o_1,o_2,…,o_t,q_t=j|\\lambda)\\tag{9.13}$$ 其中， $\\alpha_t(j)$ 的计算是叠加所有可能的路径。 $$\\alpha_t(j)=[\\sum_{i=1}^N\\alpha_{t-1}a_{ij}]b_j(o_t)$$ 表示t时刻状态为j,从t-1时刻到t时刻，有N条路径，叠加～ 以图9.7中的第二时间步和状态2为例， $$\\alpha_2(1)=\\alpha_1(1)×P(H|H)×P(1|H) + α_1(2)×P(H|C)×P(1|H)$$ 下图描述了前向网格中计算一个cell的步骤： 整个似然概率的计算过程，伪代码： 真的讲的太好了太清楚了！！！感动哭。。。说真的，要不中国的大学都改成英文教学吧。。看着那些翻译过来的书籍都头疼。。这么好的英文教学材料，为什么翻译过来之后就那么难理解了。。感觉很多老师可能自己懂了，但是讲出来的课或是写出来的书，完全就是应付任务的吧。。 好吧，回到主题。伪代码中： 概率矩阵 forward [N+2,T] 是包括了初始状态start和结束状态end,那么 forward[s,t]表示 $\\alpha_t(s)$ initialization step: $$\\alpha_1(j)=a_{0j}b_j(o_1),1\\le j \\le N$$ 伪代码中：forward[s,1] &lt;– $a_{0,s}* b_s(o_1)$ 是从状态0到t=1时刻的状态s Recursion (since states 0 anf F are non-emittinf): $$\\alpha_t(j)=[\\sum_{i=1}^N\\alpha_{t-1}(i)a_{ij}]b_j(o_t)$$ 在伪代码中有两个循环，分别是对时间步2到T，以及每个时间步，其中的状态从1到N 1234567for each time step from 2 to T do: for each state s from 1 to N do: forward[s,t] = sum_{s'=1}^N forward[s', t-1] * a_{s',s} * b_s{o_t} $forward[s,t] = sum_{s’=1}^N forward[s’, t-1] * a_{s’,s}$ 可以发现，概率矩阵forward中的每一个值表示的是t时刻状态为s的概率，也就是 $\\alpha_t(s)$ Termination: $$P(O|\\lambda)=\\alpha_T(q_F)=\\sum_{i=1}^N\\alpha_T(i)a_{iF}$$ 伪代码中： forward[$q_F$,T] = $\\sum_{s=1}^N$ forward[s,T] * $a_{s,q_F}$ return forward[$q_F$,T] T时刻状态为 $q_F$的概率。感觉应该是T+1时刻吧。。。？？？ 预测问题：维特比算法Decoding: The Viterbi Algorithm 解码问题（预测问题）：给定HMM模型 $\\lambda=(A,B)$ 和观察序列 $O=o_1,o_2,…,o_T$, 找出概率最大的隐藏状态序列 $Q=q_1q_2…q_T$ 在前向算法中，我们知道了怎么计算特定隐藏状态序列和观测序列的联合概率，也就是公式（9.13），然后找出其中概率最大的序列就可以了对吧？但是我们知道状态序列有 $N^2$ 个，这样计算就太复杂了。于是，有了 Viterbi algorithm. 维特比算法是一种动态规划算法，类似于最小化编辑距离。 上图展示了，冰淇淋例子中，HMM模型参数已知，观测序列为（3,1,3）的情况下，计算最大隐藏状态序列的过程。Viterbi网格中每一个cell为 $v_T(j)$ 表示t时刻，观察序列为 $o_1,o_2,…,o_t$， 隐藏状态为j，前t-1的状态序列为 $q_1q_2…q_{t-1}$ 的概率。 $$v_t(j)=P(q_0q_1…q_{t-1},o_1,o_2,…,o_t,q_t=j|\\lambda)$$ 换句话说，t-1时刻的状态序列是这样 $q_1q_2…q_{t-1}$，有N个这样的序列（因为t-1时刻的状态有N个），然后计算出这N个序列中到t时刻状态为j的概率，找出其中最大值，就是从开始到t时刻状态为j的最大概率序列。 $$v_t(j)=max_{i=1}^N v_{t-1}(i)a_{ij}b_j(o_t)$$ 对应到图（9.10）中，以 $v_2(2)$为例， $$v_2(2)=max(v_1(1)* a_{12}* b_2(1),v_1(2)* a_{22}* b_2(1))$$ $$v_2(2)=max(v_1(1)* P(H|C)* P(1|H), v_1(2)* P(H|H)* P(1|H))$$ 维特比算法的整个过程，伪代码如下： 我们发现Viterbi算法跟前向算法非常相似，除了前向算法是计算的sum，而Viterbi是计算的max，同时我们也发现，Viterbi相比前向算法多了一个部分：backpointers. 原因是因为前向算法只需要计算出最后的似然概率，但Viterbi不仅要计算出最大的概率，还要得到对应的状态序列。因此，在类似于前向算法计算概率的同时，记录下路径，并在最后backtracing最大概率的路径。 The Viterbi backtrace. initialization: $$v_1(j)=a_{0j}b_j(o_1)\\tag{9.20}$$ $$b_{t1}(j)=0\\tag{9.21}$$ 初始状态只有一个节点start，可确定为0 Recursion(recall that states 0 and $q_F$ are non-emitting): $$v_t(j)=max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t);1\\le j \\le N, 1\\le t\\le T\\tag{9.22}$$ $$b_{t_t}(j)=argmax_{i=1}^Nv_{t-1}(i)a_{ij}b_i(o_t);1\\le j \\le N, 1\\le t\\le T\\tag{9.23}$$ $b_{t_t}(j)表示t时刻状态为j的所有N个路径 $(q_1,q_2,…,q_{t-1})$$ 概率最大的路径的第k-1个节点。 以图9.12为例，对于t=2，状态为1的节点，其max()中有2项，分别是 $v_1(1)* a_{11}* b_1(1)$ 和 $v_1(2)* a_{21}* b_1(1)$,其中较大的项的节点就是 $max_{i=1}^Nv_{t-1}(i)a_{ij}b_i(o_t)$, 但t时刻也有N个节点，所以也需要记录，因此用arg Termination: The best score: $$P*=v_T(q_F)=max_{i=1}^Nv_T(i)* a_{iF}\\tag{9.24}$$ 计算出T时刻到end状态的最大概率，也就是所有路径中的最大概率。 The start of backtrace: $$q_T*=b_{t_T}(q_F)=argmax_{i=1}^Nv_T(i)* a_{iF}\\tag{9.25}$$ 表示T时刻中N个路径中概率最大的结点。 其实Viterbi算法的路径数量和前向算法的路径数量是一模一样的，只是一个是max，一个是sum，因此也可以参考图9.8. 学习问题：HMM Training: The Forward-Backward AlgorithmLearning: Given an observation sequence O and the set of possible states in the HMM, learn the HMM parameters A and B. 第三个问题，给定观测序列 $O=(o_1,o_2,…,o_T)$, 估计模型参数使得在该HMM模型下，观测序列的概率 $P(O|\\lambda)$ 概率最大，即用极大似然估计的方法估计参数。 先考虑马尔可夫链马尔可夫链其状态是可观察的，可以看作是退化的隐马尔可夫模型。即没有发射概率(emmision probablities) B.因此，我们需要学习的参数只有状态转移矩阵（probability matrix）A. 其中 $a_ij$ 表示从状态i转移到状态j的概率，可以用大数定律来计算。 $C(i\\rightarrow)$ 表示观察到的序列中从状态i转移到状态j的数量。然后除以所有从状态i转移的总数量。 $$a_{ij}=\\dfrac{C(i\\rightarrow j)}{\\sum_{q\\in Q}C(i\\rightarrow q)}\\tag{9.26}$$ 显然分母不包括最后 T 时刻出现状态 i，因为end不属于Q. 隐马尔可夫模型： Baum-Welch算法 The Baum-Welch algorithm uses two neat intuitions to solve this problem. The first idea is to iteratively estimate the counts. We will start with an estimate for the transition and observation probabilities and then use these estimated probabilities to derive better and better probabilities. The second idea is that we get our estimated probabilities by computing the forward probability for an observation and then dividing that probability mass among all the different paths that contributed to this forward probability. 其实就是EM算法～ 在此之前，先了解一下后向算法，可以看做反向的前向算法。 后向算法对应的后向概率（Backward probability）$\\beta_t(i)$ 表示给定hmm模型 $\\lambda$, 在 t 时刻状态为 i 的条件下，t+1 时刻的观测序列为 $o_{t+1},o_{t+2},…,o_T$的概率. $$\\beta_t(i)=P(o_{t+1},o_{t+2},…,0_T|q_t=i,\\lambda)\\tag{9.27}$$ initialization: $$\\beta_T(i)=a_{iF},1\\le i \\le N$$ 在李航老师的《统计学习导论》这本书上 $\\beta_T(i)=1$. $\\beta_T(i)$ 的定义表示在T时刻状态为i，观测到序列为 $o_{T+1}$, 这东西不存在，可以看做是end吧，所以从T到end其概率应该是 $a_{iF}$,但是在李航老师的书上只有初始状态的概率 $\\pi=a_{01}$,而没有 $a_{iF}$. 感觉跟具体在什么场景下有关系。。 Recursion(again since stetes 0 and $q_F$ are non-emitting) $$\\beta_t(i)=\\sum_{j=1}^N\\beta_{t+1}(j)a_{ij}b_j(o_{t+1}),1\\le j \\le N, 1\\le t\\le T$$ 根据定义很好理解。$\\beta_{t+1}(j)$ 表示给定hmm模型 $\\lambda$, 在t+1时刻状态为j的条件下，t+1时刻的观测序列为 $o_{t+2},o_{t+3},…,o_T$的概率.那么就可以得到 $\\beta_t(i)$ 了。 Termination: $$P(O|\\lambda)=\\alpha_T(q_F)=\\beta_1(q_0)=\\sum_{j=1}^N\\beta_1(j)a_{0j}b_j(o_1)$$ 在初始模型参数下，用大数定律估计新的模型参数，也就是极大似然估计根据公式(9.26)我们可以知道，状态ｉ到ｊ的概率： $$\\hat a_{ij}=\\dfrac{expected\\ number\\ of\\ transitions\\ from\\ state\\ i \\ to\\ state\\ j}{expected\\ number\\ of\\ transitions\\ from\\ state\\ i}$$ 然而怎么计算这些numerator？试想，如果我们知道特定时刻t，从状态i转移到j的概率，那么就能计算所有时刻t的从i转移到j的数量。 定义 $\\zeta_t$ 表示在给定模型参数和观察序列条件下，t时刻状态为i，t+1时刻状态为j的概率。 $$\\zeta_t(i,j)=P(q_t=i,q_{t+1}=j|O,\\lambda)\\tag{9.32}$$ 但是模型参数我们不知道呀，也是我们需要学习得到的。 这样我们先计算一个和 $\\zeta_t$ 相似的概率。 $$not-quite-\\zeta_t(i,j)=P(q_t=i,q_{t+1}=j,O|\\lambda)\\tag{9.33}$$ $\\alpha_t(i)$ 和 $\\beta_t(j)$ 是前向/后向算法中的定义。我们先看下前向算法计算的条件： 也就是problem1中的条件，给定 $\\lambda$ 和 观察序列，求 $P(O|\\lambda)$ 我们可以用 $\\alpha_t(i)$ 和 $\\beta_t(j)$ ，来表示 $\\zeta_t$, 是因为我们在计算 $\\zeta_t$ 时是先假定有这个一个模型参数，比如初始参数～ 那么： $$not-quite-\\zeta_t=\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)\\tag{9.34}$$ 根据bayes公式： $$P(X|Y,Z)=\\dfrac{P(X,Y,Z)}{P(Y,Z)}=\\dfrac{P(X,Y,Z)}{P(Z)P(Y|Z)}=\\dfrac{P(X,Y|Z)}{P(Y|Z)}\\tag{9.35}$$ 对应起来就是： $$P(q_t=i,q_{t+1}=j|O,\\lambda)=\\dfrac{P(q_t=i,q_{t+1}=j,O|\\lambda)}{P(O|\\lambda)}=\\dfrac{not-quite-\\zeta_t}{P(O|\\lambda)}\\tag{9.36}$$ 其中： $$P(O|\\lambda)=\\alpha_T(q_F)=\\beta_1(q_0)=\\sum_{j=1}^N\\alpha_t(j)\\beta_t(j)\\tag{9.37}$$ 这一步最后面一个式子的理解可以看做是前向算法和后向算法在时刻t相遇。 看到这里会发现，李航老师书中179页，公式25-26的推导就有点逻辑不通了。 因此，现在就可以推导出 $\\zeta_t$： $$\\zeta_t{i,j}=\\dfrac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\alpha_T(q_F)}\\tag{9.37}$$ $\\zeta_t$ 表示的是某一个时刻t，那么对于参数 $a_{ij}$ 的估计就是所有的时刻中i到j的总数除以i到k(k=1,2…N)的总数 $$\\hat a_{ij}=\\dfrac{\\sum_{t=1}^{T-1}\\zeta_t(i,j)}{\\sum_{t=1}^{T-1}\\sum_{k=1}^N\\zeta_t(i,k)}\\tag{9.38}$$ 同样的道理，我们可以推理得到发射矩阵B的参数估计 $b_j(v_k)$ 状态为j，观察得到 $v_k,v_k\\in V$的概率： 在t时刻状态为j的概率，定义为 $\\gamma_t(j)$ $$\\gamma_t(j)=P(q_t=j|O,\\lambda)\\tag{9.40}$$ 同样的道理： $$\\gamma_t(j)=\\dfrac{P(q_t=j,O|\\lambda)}{P(O|\\lambda)}\\tag{9.41}$$ 同公式（9.37）一样，前向后向算法在t时刻相遇： $$\\gamma_t(j)=\\dfrac{\\alpha_t(j)\\beta_t(j)}{P(O|\\lambda)}$$ 然后求整个时间段内的j到 $v_k$的总数，除以状态为j的总数。 $$\\hat b_j(v_k)=\\dfrac{\\sum_{t=1,o_t=v_k}^T}{\\sum_{t=1}^T}\\gamma_t(j)$$ 仔细回顾以下这个过程，在初始模型参数和观测序列的条件下，根据大数定律对模型参数进行更新。其实就是在初始模型参数下，根据极大似然估计求得观测序列的极大似然估计，然后在似然概率最大的条件下求得相应的模型参数。 可以看到这里直接用大数定律和李航老师书上，使用极大似然估计，然后求导得到的公式是一样的。 EM算法 E step: 根据初始模型参数或是M step得到的模型参数，得到后验概率。 M step: 根据E step中得到的概率，估计出新的模型参数。这里直接用大数定律得到的，其实其本质原理就是极大似然估计，也就是求出使得概率最大的模型参数。 那么迭代条件呢？什么情况下终止？在GMM中有log函数，这里呢。。 这里应该就是 $P(O|\\lambda)$ 吧，在前向算法中有计算到在模型参数和观察序列条件下的极大似然估计。 根据GMM和HMM对使用EM算法进行参数估计的一点想法：所以EM算法中的E step并不是求期望，而是在对模型参数进行估计时，在初始模型或previous模型的情况下，求得基于观测序列或是训练样本的用极大似然估计或是大数定律求得后验概率。 然后M STEP就是让这个概率最大的条件下更新模型参数。 总结： 在回顾下隐马尔可夫模型的三个问题： 第一个问题，计算概率 已知模型参数 $\\lambda$ 和观测序列 $O$，求在该模型下，出现观测序列的概率。 使用前向算法，一个动态回归的算法，把求长度为T的概率转换为t到t+1的概率sum 这一问题其实主要是为后面两个问题铺垫的，因为一般的场景都是状态未知，更不可能知道模型参数了。 预测问题，又称解码问题 已知模型参数 $\\lambda$ 和观测序列 $O$, 求概率最大的状态序列。 使用Viterbi算法，类似于前向算法，不过每一步不是sum，而是max，并且需要回溯backpointers 这个问题的应用场景就比较广了。 学习问题：模型参数估计 已知观测序列 $O$，估计模型参数 $\\lambda$, 使得观测序列的概率 $P(O|\\lambda)$ 最大。 使用Baum-Welch(极大似然估计)或forward-backward(大数定律)算法，并使用EM算法迭代，对参数进行估计，","link":"/2018/04/06/chapter9-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-standford/"},{"title":"cs224d-lecture11 再看GRU和NMT","text":"主要内容： GRU进一步理解 GRU和LSTM对比 LSTM的进一步理解 训练RNN的一些tips Ensemble MT Evaluation 生成词使用softmax导致的计算量过大的问题 presentation GRU 进一步理解shortcut connection adaptive shortcut connection 使用update gate 自适应的增加shortcut connection prune unnecessary connections adaptively 使用reset gate自适应的修剪不必要的连接。 突然想到个问题，为什么神经网络具有自适应性？我个人的理解是，神经网络是一个参数学习和拟合的过程，在梯度下降的过程中，模型得到优化使其具有自适应性。 question1:how you select the readable subset based on this reset gate? $$r_t=\\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})\\tag{reset gate}$$ $$\\tilde h_t=tanh(Wx_t+r_t\\circ Uh_{t-1})\\tag{new memory}$$ the reset gate decides which parts of the hidden state to read to update the hidden state. So, the reset gate calculates which parts to read based on the current input and the previous hidden state. So it’s gonna say, okay, I wanna pay a lot of attention to dimensions 7 and 52. And so, those are the ones and a little to others. And so those are the ones that will be being read here and used in the calculation of the new candidate update, which is then sort of mixed together with carrying on what you had before. 对此，Christopher老头儿还举了个例子，在隐藏状态中动词保存在47-52 dimensions,当遇到新的verb是，隐藏状态的这部分维度就会得到更新。看到这，真想试试打印出 $r_t$ 看看它随时间步的变化情况。。 question2:how you select the writable subset based on this update gate? $$u_t=\\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})\\tag{update gate}$$ $$h_t=(1-u_t)\\circ \\tilde h_t+u_t\\circ h_{t-1} \\tag{Hidden state}$$ some of the hidden state we’re just gonna carry on from the past. We’re only now going to edit part of the register. And saying part of the register, I guess is a lying and simplifying a bit, because really, you’ve got this vector of real numbers and some said the part of the register is 70% updating this dimension and 20% updating this dimension that values could be one or zero but normally they won’t be. So I choose the writable subset And then it’s that part of it that I’m then updating with my new candidate update which is then written back, adding on to it. And so both of those concepts in the gating, the one gate is selecting what to read for your candidate update. And the other gate is saying, which parts of the hidden state to overwrite? 感觉意思是，update gate主要是为了控制生成当前时间步的隐藏状态 $h_t$，如果更新门的值都是1，那就以为着保存所有的以前的信息。 question3:how does these gates avoid gradient vanishing? $$h_t=f(h_{t-1},x_t)=u_t \\circ \\tilde h_t + (1-u_t)\\circ h_{t-1}$$ the secret is this plus sign. 在回过头来看一下梯度消失的那个公式： $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}\\dfrac{\\partial h_t}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}$$ 举个例子，我们要求t=3时刻的损失函数对 $W_{hh}$ 的导数，那么： $$\\begin{align} \\dfrac{\\partial E_3}{\\partial W} &amp;=\\sum_{k=1}^3\\dfrac{\\partial E_3}{\\partial y_3}\\dfrac{\\partial y_3}{\\partial h_3}\\dfrac{\\partial h_3}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}\\ &amp;=\\dfrac{\\partial E_3}{\\partial y_3}\\dfrac{\\partial y_3}{\\partial h_3}(\\dfrac{\\partial h_3}{\\partial W}+\\dfrac{\\partial h_3}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial W}+\\dfrac{\\partial h_3}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial h_1}\\dfrac{\\partial h_1}{\\partial W}) \\end{align}$$ 可以看到很早之前的隐藏状态 $h_1$ 随着时间的增长，对当前时刻的影响越来越小。而在GRU中，当update gate $u_t=0$ 时，$h_3=h_2$,这说明之前的隐藏状态存储的信息能有效的传递下来， that’s why it can carry information for a very long time.。 question4:how long does a GRU actually end up remembering for? Answer: I kind of think order of magnitude the kind number you want in your head is 100 steps. So they don’t remember forever I think that’s something people also get wrong. question5:Does GRU train faster than lstm? Answer: LSTMs have a slight edge on speed. No huge difference. GRU和LSTM的区别 question6:LSTMs中为什么 $h_t=o_t\\circ tanh(c_t)$ 中要用到tanh？ TA Richard的解释是，对于 new memory cell $\\tilde c = f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$ 这是一个线性的layer，加上tanh非线性因素，能让lstm更powerful. LSTM 直观图解 可以说是很清楚了～～不过这里有点区别,将 $h_{t-1}$ 和 $x_t$ concat在一起了，比如三个gate: $$i_t = \\sigma (W_i[h_{t-1},x_t]+b_i)\\tag{input/update gate}$$ $$o_t = \\sigma (W_o[h_{t-1},x_t]+b_o)\\tag{output gate}$$ $$f_t = \\sigma (W_f[h_{t-1},x_t]+b_f)\\tag{forget gate}$$ 而更新的 new memory cell $\\tilde c_t$: $$\\tilde c_t=\\tanh(W_c[h_{t-1}, x_t]+b_c)$$ 最终的记忆细胞状态 $c_t$: $$c_t= f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$$ 最终的隐藏状态 $h_t$: $$h_t=o_t\\circ tanh(c_t)$$ LSTM的核心，类似于resnet: 用加和，也就是图中的plus sign，原本的rnn的仅有matrix multiply，使得网络具有 long dependency. 训练rnn的一些经验 第7点，万万不能dropout horizontally,那样会丢失很多信息。 Ensemble 之前看到在cnn里面，dropout可以看做是很多模型的集成，不知道rnn是否也可以。 MT Evaluation关于机器翻译的模型验证 notoriously tricky and subjective task，臭名昭著的棘手以及非常具有主观性。 BLEU: a Method for Automatic Evaluation of Machine Translation 原理： n-gram matches $p_n$ = # matched n-grams / # n-grams in candidate translation 其实就是 precision $p_n$ 表示 n-gram 的precision score. 并且，使用 $w_n=1/2^n$ 作为对应的权重。 brevity penalty：短译句容易得高分，因此需要给予惩罚 $$BP=\\begin{cases} 1, &amp; \\text{if c &gt; r}\\ e^{1-r/c}, &amp; \\text{if c $\\le r$} \\end{cases}$$ BLEU: $$BLEU=BP\\cdot exp(\\sum_{n=1}^Nw_nlogp_n)$$ 在对数域： $$log BLEU=min(1-\\dfrac{r}{c},0)+\\sum_{n=1}^Nw_nlogp_n$$ 又是softmax的问题 在每个时间步，从隐藏状态到词表，使用softmax这一步非常消耗计算力。 解决方法： Not GPU-friendly! 不知道为啥。。感觉需要好好了解下GPU 原论文： On Using Very Large Target Vocabulary for Neural Machine Translation Word and character-based models??? presentation","link":"/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/"},{"title":"cs224d-lecture10 机器翻译和注意力机制","text":"主要内容： Seq2Seq 基础模型 seq2seq encoder and decoder Attention Mechanisms: 介绍了三种attention Bahdanau et al. NMT model: 重点是怎么计算 context vector $c_i$ 前言对于NER标注，是通过previous words预测下一个word.还有一类NLP任务，是针对sequential output or outputs that are sequences of potentially varying length. 比如： Translation: taking a sentence in one language as input and outputting the same sentence in another language Conversation: taking a statement or question as input and responding to it. Summarization: taking a large body of text as input and outputting a summary of it. 这一节内容讲的就是根据一个基于深度学习的框架sequence-tosequence模型，用来处理序列生成的问题。 序列生成的历史方法: word-based system: 无法capture句子中的词序 phrase-based system: 无法解决长时间依赖的问题 Seq2Seq模型：can generate arbitrary output sequences after seeing the entire input. They can even focus in on specific parts of the input automatically to help generate a useful translation. sequence-to-sequence Basics Sutskever et al. 2014, “Sequence to Sequence Learning with Neural Networks” Seq2Seq-encoder encoder的目的就是将input sentence读入到模型中，并生成一个固定维度 context vector C. 显然，就一个任意长度的句子的信息压缩到一个固定维度的向量中，这是很困难的。所以encoder通常使用 stacked LSTMs. 通常会将sentence翻转作为输入，以机器翻译为例，翻转后输入的最后一个词对应的翻译，也就是output的第一个词。 明显感觉效果会不太好对吧，所以也就有了后来的attention 举个例子： input sentence：”what is your name” 那么得到的context vector 就是 a vector space representation of the notion of asking someone for their name. Seq2Seq-decoder decoder的目的是生成sentence，在最上面一层LSTM上接着softmax用来生成当前时间步的output词。然后用这个词作为下一个时间步的input word. 一旦得到output sentence,通过最小化交叉熵损失函数，来训练encoder和decoder中的参数。 Bidirectional RNNs Attention MechanismMotivation在seq2seq模型中，使用单一的 context vector：different parts of an input have different levels of significance. Moreover, different parts of the output may even consider different parts of the input “important.” 也就是说输入sentence中，每个词并不是具有同样的重要程度的。比如 “the ball is on the field”,显然”ball” “on” “field”比较重要。 而且，在输出的某一部分也可能更看中input中的某一部分。通常output中的前几个词主要却取决于input中的前几个词，output中的后几个词主要取决于input中的后几个词。 那么Attention mechanisms采用的方法是： providing the decoder network with a look at the entire input sequence at every decoding step; the decoder can then decide what input words are important at any point in time. 在decoder时采用注意力机制，确定在任何时刻生成词时输入序列中每个词的权重。 Bahdanau et al. NMT model原论文： Bahdanau et al. Neural Machine Translation by Jointly Learning to Align and Translate Decoder: General description decoder中生成下一个词的条件概率： $$P(y_i|y_1,…,y_{i-1},X)=g(y_{i-1},s_i,c_i)$$ 其中，当前时间步的隐藏状态 $s_i$ 表示为： $$s_i=f(s_{i-1},y_{i-1},c_i)$$ 也就是： i时刻生成此 $y_i$ 取决于 上一个生成词 $y_{i-1}$ (在生成序列时上一个时间步的输出是下一个时间步的输入) 和 i-1时刻的隐藏状态 $s_{i-1}$ 以及context vector对应的值 $c_i$. 重点是怎么计算每个时间步的context vector $c_i$： 在标准的seq2seq模型中，context vector只有一个，但在attention模型中，每个时间步都有单独的context vector $c_i$,它依赖于输入序列中的所有annotation $(h_1; · ·· ; h_{T_x})$,并赋予他们一定的权重。也就是： $$c_i=\\sum_{j=1}^{T_x}\\alpha_{ij}h_j$$ 其中i表示输出序列的第i时刻，j表示输入序列的第j个word的annotation. 其中对于每个输入词的annotation即 $h_j$ 的权重 $a_{ij}$ 是这么计算的： $$\\alpha_{ij}=\\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$$ 其中： $$e_{ij}=a(s_{i-1},h_j)$$ 是对其模型(alignment model),$s_{i-1}$ 表示输出序列的隐藏状态， $h_j$ 表示输入序列的隐藏状态，所以用来计算输入sentence中第j个位置和输出序列中第i个位置匹配的得分(score).这个得分是基于decoder中的前一个时刻的隐藏状态 $s_{i-1}$, 和输入序列中的第j个annotation $h_j$。 a是任意函数，且得到的值是R。比如可以是一个单层的全连接神经网络，得到了序列 $e_{i,1},…,e_{i,n}$， 然后使用softmax得到 $\\alpha_i=(\\alpha_{i,1},…,\\alpha_{i,n})$. 疑问：知道了 $e_{ij}$ 的意义，但不太明白怎么计算。。还没看论文，猜想既然a是任意函数，那么应该就是用神经网络来表示了。 总结下来： Let $\\alpha_{ij}$ be a probability that the target word yi is aligned to, or translated from, a source word $x_j$. Then, the i-th context vector $c_i$ is the expected annotation over all the annotations with probabilities $\\alpha_{ij}$. 所以 $\\alpha_{ij}$ 表示的就是从词 $x_j$ translate to (or align to) 词 $y_i$ 的概率。也就是说 output中第i个词 $y_i$ 可能由 input中的任意一个词对齐而来的，也不一定就是翻译，就是 翻译 $y_i$ 的时候，input中每一个对它的影响程度，也就是这个权重值 $\\alpha_{ij}$. 而这个概率 $\\alpha_{ij}$ 以及其 associated energy $e_{ij}$ 反映了 $s_{i-1}$ 和 $h_j$ 对生成下一个word的重要性。 疑问：在训练的时候可以通过反向传播，得到参数 $c_i$，但是这个权重也只能用于当前的序列吧。。测试的时候，这些训练的参数还能用么？ Encoder: bidirectional RNN for annotation sequences在encoder时，将输入序列编码为annotation $(h_1,h_2,…,h_{T_x})$. 为了既考虑preceding words，又考虑 following words，采用双向RNN（BiRNN）. forward RNN $\\overrightarrow f$ reads input sentence (from $x_1$ to $x_{T_x}$): $$(\\overrightarrow h_1,…,\\overrightarrow h_{T_x})$$ backward RNN $\\overleftarrow f$ reads input sentence in the reverse order (from $x_{T_x}$ to $x_1$): $$(\\overleftarrow h_1,…,\\overleftarrow h_{T_x})$$ annotation for $x_j$: $$h_j=[\\overrightarrow {h_{T_j}^T},\\overleftarrow {h_{T_j}^T}]$$ Connection with translation alignment在训练的decoder过程中，可以得到这样的一个alignment table, a table mapping words in the source to corresponding words in the target sentence.使用attention score $\\alpha_{i,j}$ 填充这个表格。 这就解决了之前的疑惑了，在测试的时候，context vector的权重 $\\alpha_i$ 直接通过查表得到～ Huong et al. NMT model原论文： Effective Approaches to Attentionbased Neural Machine Translation by Minh-Thang Luong, Hieu Pham an Christopher D. Manning Global attentionencoder 隐藏状态序列： $h_1,…,h_n$ ，n表示序列长度 decoder 隐藏状态序列： $\\overline h_1,…,\\overline h_n$ 对于每一个decoder中的隐藏状态 $\\overline h_i$，计算其基于所有encoder隐藏状态的attention vector $c_i$. $$ score(h_i^T\\overline h_j)=\\begin{cases} h_i^T\\overline h_j \\ h_i^TW\\overline h_j \\quad &amp; \\text{$\\in R$}\\ W[h_i,\\overline h_j] \\end{cases} $$ 类似于Bahdanau et al. NMT model中的 $e_{ij}$, 同样的需要得到的权重 $\\alpha_{ij}$ 是概率，也就是encoder中的 $h_i$ 与 decoder中的 $h_j$ 匹配的概率, attention vector $\\alpha_{i,j}$： $$\\alpha_{i,j}=\\dfrac{exp(score(h_i^T\\overline h_j))}{\\sum_{k=1}^nexp(score(h_k^T\\overline h_j))}$$ 那么context vector: $$c_i=\\sum_{j=1}^n \\alpha_{i,j}h_j$$ 那么使用context vector和隐藏状态 $\\overline h_i$ 生成新的decoder中第i时间步的新的 vector $$\\tilde h_i=f[\\overline h_i,c_i]$$ Local Attentionthe model predicts an aligned position in the input sequence. Then, it computes a context vector using a window centered on this position. The computational cost of this attention step is constant and does not explode with the length of the sentence. window怎么选？？ Christopher 好像说用强化学习。。 Google’s new NMT6 Johnson et el. 2016, “Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation” The new multilingual model not only improved their translation performance, it also enabled “zero-shot translation,” in which we can translate between two languages for which we have no translation training data. For instance, if we only had examples of Japanese-English translations and Korean-English translations, Google’s team found that the multilingual NMT system trained on this data could actually generate reasonable Japanese-Korean translations. The powerful implication of this finding is that part of the decoding process is not language-specific, and the model is in fact maintaining an internal representation of the input/output sentences independent of the actual languages involved. More advanced papers using attention Show, Attend and Tell: Neural Image Caption Generation with Visual Attention by Kelvin Xu, Jimmy Lei Ba,Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel and Yoshua Bengio. This paper learns words/image alignment. Modeling Coverage for Neural Machine Translation by Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu and Hang Li. Their model uses a coverage vector that takes into account the attention history to help future attention. Incorporating Structural Alignment Biases into an Attentional Neural Translation Model by Cohn, Hoang, Vymolova, Yao, Dyer, Haffari. This paper improves the attention by incorporating other traditional linguistic ideas. Sequence model decoders使用统计的方法找到最合适的sequence $\\hat s*$： $$\\hat s* = argmax_{\\hat s}(P(\\hat s|s))$$ Exhaustive search: NP问题 Ancestral sampling $$x_t \\sim P(x_t|x_1,..,x_n)$$ Greedy search $$x_t=argmax_{\\tilde x_t}P(\\tilde x_t|x_1,…,x_n)$$ 如果其中一步错了，对接下来的序列影响很大。 Beam search： the idea is to maintain K candidates at each time step. PresentationGoogle’s Multilingual Neural Machine Translation System: Enabling zero-short Translation","link":"/2018/05/08/cs224d-lecture10-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"title":"cs224d-lecture13 卷积神经网络","text":"主要内容： 为什么要使用CNN single layer connection pooling: max-pooling Multiple-Filters: multiple n-grams Multiple-Channels: 两个词向量 static, dynamic lassification after one CNN layer 为什么要使用CNN？CNN模型在计算机视觉与语音识别方面取得了卓越的成就. 在 NLP 也是可以的. 卷积具有局部特征提取的功能, 所以可用 CNN 来提取句子中类似 n-gram 的关键信息. Single layer connectionConvolution Neural Networks for Sentence Classification 对于单个词，其词向量：$x_i\\in R^k$, k维词向量 n个词concatenate在一起表示为：$x_{1:n}=x_1\\bigoplus x_2 \\bigoplus…\\bigoplus x_n$ 卷积过滤器filter: $w\\in R^{hk}$ h表示过滤器的尺寸，也就是覆盖多少个词. 上图figure13表示： k=2,n=5,h=3 需要注意的是filter是一个长度为 hk 的 vector，与input的每一个时间步做一次卷积（加权和）得到一个实数: $$c_i=f(W^Tx_{i:i|h-1}+b)$$ 那么输出向量：$c=[c_1,c_2,…,c_{n-h+1}]\\in R^{n-h+1}$ 如果h=3，那么最后两个词 my birth 就不够卷积，可以如图figure14所示采用 h-1 zero-vector padding. poolingmax-pooling: $$\\hat c=max{c}, c\\in R$$ filter可以看做和图像中的filter一样，一个图像特征提取器，那么在文本中，一个filter也可以看做是一个n-gram特征提取器，比如一个用来表示positive bigram,那么这个filter和句子中同样表示positive bigram做卷积的话，其值就会很大～那么使用max-pooling就是找到这个值 当然使用min-pooling也是可以的.但更多的时候我们选择用 relu作为激活函数，那么使用min-pooling的话，就会出现更多的 0. Multiple-FiltersWe can use multiple bi-gram filters because each filter will learn to recognize a different kind of bi-gram. Even more generally, we are not restricted to using just bi-grams, we can also have filters using tri-grams, quad-grams and even higher lengths. Each filter has an associated max-pool layer. Thus, our final output from the CNN layers will be a vector having length equal to the number of filters. 同时使用好几个bigram，用以获取不同的pattern. 除了bigram，还会使用trigram,unigram等等。 Multiple-Channels我们有时候会需要在特定的场景下更新词向量，也就是也训练词向量参数，这样能够适应更特殊的任务。但如果在test中出现了train中没有出现的词unseen word， 那么这个词的词向量还会保持初始词向量的值（Glove等）。但是与这个词语义相关的词的词向量却发生了变化，这样就造成了相似的词的的词向量相差较大。这显然是不合理的。 所以我们使用两个词向量，one ’static’ (no gradient flow into them) and one ’dynamic’, which are updated via SGD. Backprop into only one set, keep other “static” Both channels are added to $c_i$ before max-pooling. Classification after one CNN layer First one convolution, followed by one max-pooling To obtain final feature vector: $z=[\\hat c_1,…,\\hat c_m]$ 假设有m个过滤器filter Simple final softmax layer $y=softmax(W^{(S)}z+b)$ Convolution Neural Networks for Sentence Classification 论文中的模型： 第一层： two word-vector channels $n\\times k\\times 2$ 第二层： m个filter得到的m列feature maps，由于有多种filter尺寸，如果没有zero-padding的话，那么得到的feature maps长度是不一致的。 第三层：max-pooling $z=[\\hat c_1,…,\\hat c_m]$ 第四层：fully connectioned layer with dropout and softmax output. Tricks: Dropout$$y=softmax(W^{(S)}(r\\circ z)+b)$$ 针对dropout，在train和test时，处理方式是不一样的： 模型参数选择问题 CNN更容易实现在GPU上的并行处理。 CNN的变种以及应用 Narrow vs Wide Narrow就是没有zero-padding: 那么output长度就是 $n-h+1$ Wide就是前后两端都有h-1 zero-padding: 那么output长度就是： $[n+2\\times (h-1)]- h+1=n+h-1$ k-max pooling 相比max-pooling， k-max pooling是选出最大的k个值 模型对比总结 PresentationCharacter-Aware neural language models,Yon Kim at al.","link":"/2018/05/14/cs224d-lecture13-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"cs224d lecture16 dynamic Memory network","text":"主要内容： paper:Ask Me Anything: Dynamic Memory Networks for Natural Language Processing 是否所有NLP任务都可视作QA？ 在old-school NLP系统中，必须手工整理一个“知识库”；然后在这个知识库上做规则推断。这节课介绍的DMN完全不同于这种小作坊，它能够直接从问答语料中学习所有必要的知识表达。 DMN还可以在问答中做情感分析、词性标注和机器翻译。 所以构建一个joint model用于通用QA成为终极目标。 要实现这个目标，有两个障碍。 没有任何已有研究探讨过如何让单个模型学会这么多的任务。每种任务都有独特的特点，适合不同的神经网络来做： 第二个障碍 Fully joint multitask learning（同一个decoder/classifier，不仅仅共享词向量，而应该共享全部参数）非常困难。 有些不成功的研究发现，只能在低层（词向量）共享参数、如果任务之间没有直接关联则会顾此失彼。 感觉就是迁移学习无法应用于 nlp 上，同时应到到两个不同的任务上，然后对相同的参数进行训练，往往会得到很差的效果。 So if you’re trying to train two tasks together in one model, say you just have two softmaxes on the same Hidden state of your LSTMs. It turns to actually get worse in many cases, too. Dynamic Memory Networks今天介绍的DMN仅仅解决了第一个问题。虽然有些超参数还是得因任务而异，但总算是个通用的架构了。 回答特别难的问题 你无法记住全文，但看了问题之后，只要带着问题扫几眼原文，你就能找出答案。 这种现象启发了DMN。 Dynamic Memory Networks 先来看big picture（接下来会对每个模块单独讲解）： 主要有以下几个module： semantic memory module: 词向量 input module： 使用GRU或LSTM对原文进行encoder，每一个word对应一个hidden vector，同question mudule 共享 GRU 的权重参数 Question Module 和 Episodoc memory Module： 计算出一个Question Vector q，根据q应用attention机制，回顾input的不同时刻。根据attention强度的不同，忽略了一些input，而注意到另一些input。这些input进入Episodic Memory Module，注意到问题是关于足球位置的，那么所有与足球及位置的input被送入该模块。该模块每个隐藏状态输入Answer module，softmax得到答案序列。 attention 的过程实际上是一个 transitive reasoning 传递关系的过程： 比如上图中，问题是关于football的语义 question vector q，那么带着 q 回顾一遍原文，找到 John put down the football， 得到Episodic memory modelu的输出 $m^1$, 然后带着q 和 $m_1$ 再回顾一次原文，然后又找到 John moved to the bedroom 和 John went to the hallway， 计算得到 $m^2$. 具体怎么实现看接下来具体模块的讲解。 input Module 输入模块接受 $T_I$ 个输入单词，输出 $T_C$ 个“事实”的表示。如果输出是一系列词语，那么有 $T_C=T_I$；如果输出是一系列句子，那么约定 $T_C$ 表示句子的数量，$T_I$ 表示句子中单词的数量。我们使用简单的GRU读入句子，得到隐藏状态 $h_t=GRU(x_t,h_{t−1})$，其中 $x_t=L[w_t]$，L是embedding matrix，$w_t$ 是时刻 t 的词语。 事实上，还可以将这个Uni-GRU升级为Bi-GRU： Question Module Episodic Memory Mudule $$h_i^t=g_i^tGRU(s_i,h_{i-1}^t)+(1-g_i^t)h_{i-1}^t$$ 其中: $g_i^t$ is just a single scalar number. Should I pay attention to this sentence.也相当于一个gate机制，当 $g_i^t=0$ 时表示与 $s_i$ 无关。 上标t表示 $t^{th}$ time that we went over the entire input. 如何计算 $g_i^t$ ，也就是怎么判断当前迭代与input中的每个sentence是否相关。 相当简单和直接： sentence similarity: element-wise product or subtraction of sentence vector. 计算sentence相似性： $$z_i^t=[s_i\\circ q; s_i\\circ c^{t-1}; |s_i-q|; |s_i-m^{t-1}|]$$ 一个双层neural network: $$Z_i^t = W^{(2)}tanh(W^{(1)}z_i^t+b^{(1)})+b^{(2)}$$ softmax计算当前迭代次数下每个sentence所占的比重： $$g_i^t=\\dfrac{exp(Z_i^t)}{\\sum_{k=1}^{M_i}exp(Z_i^t)}$$ Answer Module 相关工作有很多已有工作做了类似研究： Sequence to Sequence (Sutskever et al. 2014) Neural Turing Machines (Graves et al. 2014) Teaching Machines to Read and Comprehend (Hermann et al. 2015) Learning to Transduce with Unbounded Memory (Grefenstette 2015) Structured Memory for Neural Turing Machines (Wei Zhang 2015) Memory Networks (Weston et al. 2015) End to end memory networks (Sukhbaatar et al. 2015) 同 MemNet 对比 相同点 都有input, scoring, attention and response模块 不同点 MemNets对于input representations 使用词袋，然后有一些embedding去encode位置， DMN 使用 GRU MemNets迭代运行attention和response 这些不同点都是由于MemNets是个非sequence模型造成的。而DMN是个血统纯正的neural sequence model，天然适合序列标注等任务，比MemNets应用范围更广。 DMN的sequence能力来自GRU，虽然一开始用的是LSTM，后来发现GRU也能达到相同的效果，而且参数更少。（这回答了GRU和LSTM那节课有个学生的问题：哪个计算复杂度更低。Manning当时回答应该是一样的，还不太相信Richard的答案。说明在工程上，还是做实验的一线博士更有经验） Evaluation 这是一个自动生成的QA语料库，里面都是一些简单的问答。部分NLP学者很厌恶机器生成的语料，但如果连机器生成的语料都无法解决，何谈解决真实的复杂问题。 情感分析 依然拿到最高分数。 此时问题永远是相同的，其向量是固定的。 遗憾的是，对于不同的任务，超参数依然必须不同才能拿到最佳结果。 Episodes数量： 其中task3是三段论，理论只需要3个pass，但模型依然需要5个。考虑到这是个end to end训练，没有监督信号指示那些fact是重要的，所以这个表现还挺好。情感分析的NA是因为，计算复杂度实在太高了。分数已经在降低，所以干脆没跑。 情感分析的一些例子 VQA vision question answeringEverthing is Question Answering, 这也太酷了吧～ input module 不太一样，这里是通过CNN提取特征，将图像中的每一块区域用向量表示，然后作为GRU的输入。由于卷积特征并不是序列的，所以输入模块的输出特征只是所有时刻隐藏状态向量的拼接。 显然这需要很好的数据集啊。。 这就真的很吊了～！ 最高分。。 总结 参考 hankcs的博客CS224n笔记16 DMN与问答系统","link":"/2018/05/21/cs224d-lecture16-dynamic-neural-network/"},{"title":"cs224d-lecture1-词向量表示","text":"Word Vectors Skip-gram Continuous Bag of words(CBOW) Negative Sampling Hierarchical SoftmaxM Word2Vec 1. How to represent words?With word vectors, we can quite easily encode this ability in the vectors themselves (using distance measures such as Jaccard, Cosine, Eu-clidean, etc). 2. Word Vectorsencode word tokens into some vector(N-dimensional space, N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfre using speech. one-hot vector: V is the size of vocabulary. each word is a completely independent entity. 3. Iteration Based Methods - Word2Vec 2 algorithms: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary. language model Unigram model : $$P(w_1,w_2,…,w_n) = \\prod_{i=1}^nP(w_i)$$ bigram model: $$P(w_1,w_2,…,w_n) = \\prod_{i=1}^nP(w_i|w_{i-1})$$ 4. Continuous bag of words model(CBOW)predict center word from the context. $$ \\prod_{c=1}^{n}P(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$ negative log likelihood: $$J(\\theta)= -\\sum_{c=1}^{n}logP(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$ the words of context to generate the center word is dependent: $$J(\\theta) = \\dfrac{1}{n}\\sum_{c=1}^T\\sum_{-m\\le j\\le m}logp(w_c|w_{c+j})$$ how to present this probability??? To one sentence: $$ \\begin{align} minimize J &amp;= -logP(w_c|w_{c-m},..,w_{c-1},w_{c+1},…,w_{c+m})\\ &amp;= -log P(u_c|\\hat v)\\tag{1}\\ &amp;= -log \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{2}\\ &amp;= -u_c^T\\hat v + log\\sum_{j=1}^{|V|}exp(u_j^T\\hat v) \\end{align} $$ important: from word to vector the (1) to (2), using the softmax to present the probability $$P(u_c|\\hat v) = \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{* }$$ 其实word2vec可以理解为两个word，他们的上下文越相似，那么他们俩的词向量表示也就越相似.比如 he 和 she 大多数情况下他们的语境，也就是上下文出现的单词v都是很接近的，那么同样与这些词内积得到的概率就会差不多～说到底，也是个频率统计的方法，只不过用了无监督学习这个方式来得到distribution vector了～从这个角度理解就很合理了。错误的理解是 u 和v 出现在同一个窗口，他们的内积的概率就越大，这无法解释任何东西。 4.1 We can use an simple neural networt to train this matrix weightsinput: $x^{(c)}\\in R^{|V|\\times 1}$, the input one-hot vector of context labels: $y^{(c)}\\in R^{|V|\\times 1}$, the one hot vector of the known center word. parameters: $w_i$: word i from vocabulary V $V \\in R^{n\\times |V|}$ input word matrix $v_i$:i-th column of $V$, the input vector representation of word $w_i$ $U\\in R^{|V|\\times n}$: output word matrix $u_i$: i-th row of $U$, the output vector representation of word $w_i$ n is an arbitrary size which defines the size of our embedding space there are some differences with the figure….$W_1^{n\\times |V|}$, $W_2^{|V|\\times n}$ input : $x_1.shape = (|V|, 1)$, $x_2.shape = (|V|, 1)$,…,$x_{2m}.shape = (|V|, 1)$ $W_1$ : input matrix V, $W_1.shape = (n, |V|)$ each column is the representation of $w_i$ hidden layer: $\\hat v = \\dfrac{V.dot(x_1)+…+V.dot(x_{2m})}{2m}$, $\\hat v.shape = (n,1)$ $W_2$ : output matrix U, $W_2.shape = (|V|, n)$ each row is the representation of $w_i$ score: $u.shape = (|V|, 1)$ output: $\\hat y = softmax(u)$, $\\hat y.shape=(|V|,1)$ cross entropy: $$H(\\hat y, y) = -\\sum_{j=1}^{|V|}y_jlog(\\hat y_j)$$ Because y is the one hot vector, and i is the index whose value is 1. $$H(\\hat y, y) = -y_ilog(\\hat y_i)$$ look at the paper word2vec Parameter Learning Explained, it is very cautious, very wonderful!!! The symbols are different from the above. 4.2 one word context inference 4.3 one word context backpropagation 4.4 multi-words context 5. Skip-gram$$ \\begin{align} minimize J &amp;=-logP(w_{c-m},…,w_{c-1},w_{c+1},..,w_{c+m}|w_c)\\ &amp;=-log\\prod_{j=0,j\\neq m }^{2m}P(w_{c-m+j}|w_c)\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}logP(w_{c-m+j}|w_c)\\tag{3}\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}log\\dfrac{exp(u_{c-m+j}^Tv_c)}{\\sum_{k=1}^{|V|}exp(u_{k}^Tv_c)}\\tag{4}\\ &amp;=-\\sum_{j=0,j\\neq m}^{2m}u_{c-m+j}^Tv_c+2m\\ log\\sum_{k=1}^{|V|}exp(u_{k}^Tv_c) \\end{align} $$ important: from word to vector the (1) to (2), using the softmax to present the probability $$P(u_{c-m+j}|w_c) = \\dfrac{exp(u_{c-m+j}^Tv_c)}{\\sum_{j=1}^{|V|}exp(u_j^Tv_c)}\\tag{* }$$ V is the input matrix, U is the output matrix 5.1 We can use the simple neural networks to train matrix weights 5.2 inference and backpropagation Skip-gram treats each context word equally: the models computes the probability for each word of appearing in the context independently of its distance to the center word. 6. Optimizing Computational Efficiency6.1 Hirarchical Softmax 6.2 Negative Samplingloss function: $$E = -log\\sigma(v’^T_{w_O}h)-\\sum_{w_j\\in W_{neg}}log\\sigma(-v’^T_{w_j}h)$$ in the CBOW, $h=\\dfrac{1}{C}\\sum_{c=1}^Cv_{w_c^T}$ in the skip-gram, $h=v_{w_I}^T$ how to choose the K negative samples? As described in (Mikolov et al., 2013b), word2vec uses a unigram distribution raised to the 3/4th power for the best quality of results. 关于负采样的原理的理解： Intuitive explanation of Noise Contrastive Estimation (NCE) loss? The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead. For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of kk randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors. This is important: instead of predicting the next word (the “standard” training technique), the optimized classifier simply predicts whether a pair of words is good or bad. reference: word2vec Parameter Learning Explained word2vec原理推导与代码分析","link":"/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"},{"title":"cs224d-lecture14 Tree-RNN and Constituency Parsing","text":"主要内容： 语言的语义解释 如果将短语结构映射到向量空间中：利用语义的合成性 对比 RNN 和 CNN Recursive neural networks Parsing a sentence with an RNN 使用tree-rnn 进行分类： assignment3 情感分类 语言的语义解释–并不只是词向量 词向量只是词语级别的向量，人们可以用更大颗粒度的文本来表达自己的意思，而不仅仅是词袋中的某个单词。 比如: the country of my birth, the place where I was born Question: how can we represent the meaning of longer phrases? Answer: By mapping them into the same vector space. How should we map phrases into a vector space?利用语义的合成性： use principle of Compositionality the meanings of its words the rules that combine them 其实想想RNN也是将一个sentence或者是phrase压缩到一个向量中去。后面会介绍它们的区别。 通过同时学习句法树和复合性向量表示，就可以得到短语的向量表示了。 句法树： 句法树结构和向量表示Learn Structure and Representation： 问题是：我们真的需要学习这种树结构吗？Do we really need to learn this structure? 从两个角度来说明这个问题，一是对比recursive 和 rnn， 而是从语言的本质来解释。 Recursive vs. RNN Richard mentioned that the recurrent models are really sort of capturing representations of whole prefixes and you’re not getting any representations of smaller units than that. 两者都是递归神经网络，只不过前者在空间上递归，后者在时间上递归。中文有时会把后者翻译为“循环神经网络”，但这明显混淆了等级，令人误解。 它们各有各的优缺点，Recursive neural net需要分析器来得到句法树，而Recurrent neural net只能捕捉“前缀”“上文”无法捕捉更小的单位。 但人们还是更倾向于用后者，LSTM之类。因为训练Recursive neural net之前，你需要句法树；句法树是一个离散的决策结果，无法连续地影响损失函数，也就无法简单地利用反向传播训练Recursive neural net。另外，复杂的结构也导致Recursive neural net不易在GPU上优化。 语言本质是递归的吗？ 在认知科学上虽然有些争议，因为一般一个句子是有长度限制的，人们几乎从不说300个词以上的句子。但是递归是描述语言的最佳方式，比如 [The man from [the company that you spoke with about [the project] yesterday]] 这里面一个名词短语套一个名词短语，一级级下去。从实用的角度讲 1、通过递归地描述句子（句法树），可以有效地消歧： 2、便于指代相消等任务。 3、便于利用语法树结构（基于短语的机器翻译） 从 RNNs 到 CNNsRNN只会为满足语法的短语计算向量，而CNN为每个可能的短语计算向量。从语言学和认知科学的角度来讲，CNN并不合理。甚至recurrent neural network也比tree model和CNN更合理。 两者的关系可以这样想象，RNN将CNN捕捉的不是短语的部分删除了： 得到： So the sort of picture is that for the CNN, you’re sort of making a representation of every pair of words, every triple of words, every four words. Where as the tree recursive neural network is saying well some of those representations don’t correspond to a phrase and so we’re gonna delete them out. So that for the convolultional neural network, you have a representation for every bigram. So you have a representation for there speak and trigram there speak slowly. Whereas for the recursive neural network, you only have representations for the sort of semantically meaningful phrases like people there and speaks slowly going together to give a representation for the whole sentence. Recursive Neural Networks for Structure Prediction 输入： 两个子节点的向量表示 输出： 两个子节点合并后的新节点语义表示，以及新节点成立的分值 Recursive Neural Network Definition可以同时得到句法树和向量表示的一种任务。通过socre来得到句法树。 顺便提一下assignment3: 在 assignment3 是这样的tree-RNN $$h=relu([h^{(1)}{left},h^{(1)}{right}]W+b^{(1)})$$ $$\\hat y = softmax(h^{(1)}U+b^{(s)})$$ $L\\in R^{|V|\\times d},W^{(1)}\\in R^{2d\\times d}, b^{(1)}\\in R^{1\\times d}, U\\in R^{(d\\times 5)}, b^{(s)}\\in R^{1\\times 5}$ 在assignment3中用tree-rnn进行情感分析，是已经通过句法分析得到了句法树的，所以只需要从根节点开始，递归找到子节点，并计算出对应的向量表示，并归一化softmax，然后与每个节点（包括叶节点）真实标签对比，计算得到损失值。然后用梯度下降优化得到模型参数。 那么这里的参数怎么理解？在传统的rnn中 $W_{hh}$ 可以看做是隐藏状态转移矩阵，这里呢？？？关于 $W_{hh}$ 的理解，可以看知乎 HMM和RNN是什么关系？功效上两者有冲突重叠？ Parsing a sentence with an RNNgreedily incrementally building up parse structure. 计算任意两个单词合并的得分（虽然下图是相邻两个，但我觉得那只是绘图方便；就算是我第一次写的玩具级别的依存句法分析器，也是任意两个单词之间计算）： 然后贪心地选择得分最大的一对合并： 重复这一过程:计算任意两个节点，合并得分最大的一对 直到得到根节点： 模型中只有一个合成函数，使用同一个权值矩阵W处理NP、VP、PP……这明显是不合理的。 Max-Margin Framework-Details损失函数使用最大间隔 再回顾一下多分类支持向量机损失 Multiclass Support Vector Machine Loss 我的cs231n笔记 对于单个节点，可以看做单个样本损失 $$L_i=\\sum_{j\\ne y_j}^N max(0, s_j-(s_{y_i}-\\Delta))$$ 其中 $s_{y_j}$ 表示真实标签对应的值，非真实分类的得分不能超过 $s_{y_j}-\\Delta$，凡是超过的都会对 $L_i$ 产生影响。比这个值就没事～ 对于整个sentence： $$J=\\sum_imax(0, s(x_i,y_j)-max_{y\\in A(x_i)}(s(x_i,y)+\\Delta(y,y_i)))$$ $\\Delta$ 表示对所有非正确分类的惩罚 max 表示贪心搜索得到的syntactic tree的得分 有时候也可用beam search 使用 tree-RNN 进行分类任务这里的rnn指的是 递归recursive neural networks.空间结构上的递归，而以前学的RNN也是递归，不过是时间序列上的递归。 以assignment3中的情感分类任务为例，进行前向、反向传播推导。 由于前向传播时每个节点的信号来自所有子节点，所以梯度也来自所有子节点。并且前向传播时父节点的信号是利用子节点信号的拼接计算的，所以梯度需要针对子节点的信号计算： 这个问题其实在TensorFlow那一课已经讲过了，图计算：前向传播信号流入某节点，反向传播误差就得从某节点分流到所有源节点。树只是图的一个特例： Richard Socher 的代码比如softmax之类的可真熟练～ 123456789101112131415161718192021222324252627def forwardProp(self, node): # Recursive ... # This is node's hidden activation node.h = np.dot(self.W, np.hstack([node.left.h, node.right.h])) + self.b # [1,d] # Relu node.h[node.h&lt;0] = 0 # Softmax node.score = np.dot(self.Ws, node.h) + self.bs # [1, 5] node.score -= np.max(node.probs) node.probs = np.exp(node.score)/np.sum(np.exp(node.score)) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def backProp(self.node, error=None): # softmax`grad deltas = node.probs deltas[node.label] -= 1.0 self.dWs = np.outer(deltas, node.h) # Compute the outer product of two vectors. 训练时一个batch只有一个sentence，所有h是向量。 self.dbs += deltas # 对隐藏状态求导 dh = np.dot(self.Ws.T, deltas) # Add deltas from above if error is not None: dh += error # f'(z) now: # relu 反向传播 dh *= (node.h != 0) # Updata word vector if leaf node: # 如果是叶节点，h 就是 L，词向量. if node.isLeaf: self.dL[node.word] += deltas return # Recursively backProp # 如果当前节点不是叶节点，那么需要更新权重W和b，同时将error if not node.isLeaf: self.dW += np.outer(deltas, np.hstack([node.left.h, node.right.h])) self.db += deltas # Error signal to children dh = np.dot(self.W.T, dh) # 就是公式 h=relu([h^{(1)}_{left},h^{(1)}_{right}]W+b^{(1)}) # 递归计算左子节点，node.lef用来计算左子节点自身的损失， dh[:self.hiddenDim]用来计算父节点传递下来的损失 self.backProp(node.left, dh[:self.hiddenDim]) self.backProp(node.right, dh[self.hiddenDim:]) ok！完全弄懂了吧？！～ Syntactically-Untied RNN Version3 Presentation[Deep reinforcement learning for dialogue generation] reference CS224n笔记14 Tree RNN与短语句法分析 Recursive Neural Networks Can Learn Logical Semantics","link":"/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/"},{"title":"cs224d-lecture2-词向量的高级表示","text":"what do word2vec capture? window based coocurrence Matrix GloVe Intrinsic evaluation What do word2vec capture? Go through each word of the whole corpus predict surrounding words of each word word2vec captures coocurrence of words one at a time. Why not capture coocurrence counts directly? word2vec将窗口视作训练单位，每个窗口或者几个窗口都要进行一次参数更新。要知道，很多词串出现的频次是很高的。能不能遍历一遍语料，迅速得到结果呢？ 早在word2vec之前，就已经出现了很多得到词向量的方法，这些方法是基于统计共现矩阵的方法。如果在窗口级别上统计词性和语义共现，可以得到相似的词。如果在文档级别上统计，则会得到相似的文档（潜在语义分析LSA,Latent Semantic Analysis）。 Window based Co-occurrence Matrix 基于窗口的共现矩阵 Solution: Low dimensional vectors SVD的问题： 计算复杂度高：对n×m的矩阵是O(mn2) 不方便处理新词或新文档 与其他DL模型训练套路不同 Count based VS direct prediction 这些基于计数的方法在中小规模语料训练很快，有效地利用了统计信息。但用途受限于捕捉词语相似度，也无法拓展到大规模语料。 而NNLM, HLBL, RNN, Skip-gram/CBOW这类进行预测的模型必须遍历所有的窗口训练，也无法有效利用单词的全局统计信息。但它们显著地提高了上级NLP任务，其捕捉的不仅限于词语相似度。 2. Combining the beat of both words: GloveGloVe: Global Vectors for Word Representation Glove的原理： Using global statistics to predict the probability of word j appearing in the context of word i with a least squares objective. 即利用了词频统计的作用，又利用了word2vec中出现在同一个窗口的两个词的概率，用词向量做内积来表示。 在word2vec中 $$P(u_c|\\hat v) = \\dfrac{exp(u_c^T\\hat v)}{\\sum_{j=1}^{|V|}exp(u_j^T\\hat v)}\\tag{* }$$ 共现矩阵 Co-occurrence Matrix基于频率统计概率～ word $w_j$ 出现在中心词 $w_i$ 的上下文中的概率： $$P_{ij}=P(w_j|w_i)=\\dfrac{X_{ij}}{X_i}=\\dfrac{count(w_i,w_j)}{count(w_i)}$$ Least Squares Objective依据word2vec skip-gram中的原理，$w_j$ 出现在 $w_i$ 的上下文中的概率： $$Q_{ij}=P(w_j|w_i) = \\dfrac{exp(u_j^Tv_i)}{\\sum_{j=1}^{|V|}exp(u_j^Tv_i)}$$ 根据极大似然估计的负数形式： $$J=-\\sum_{i\\in corpus}\\sum_{j\\in context(i)}Q_{ij}$$ 对于共现词 $w_i, w_j$ 同时出现多次，因此可以将上式简化为： $$J=-\\sum_{i=1}^V\\sum_{j=1}^VX_{ij}Q_{ij}$$ 我们知道 $Q_{ij}$ 是表示经过softmax归一化之后的概率，需要遍历整个corpus，这会导致很大的计算量。因此，我们用最小二乘法来作为目标函数： $$\\hat J = \\sum_{i=1}^V\\sum_{j=1}^VX_i(\\hat P_{ij}-\\hat Q_{ij})$$ 其中 $\\hat P_{ij}=X_{ij}$ 和 $\\hat Q_{ij}=exp(u_j^Tv_i)$ 都表示未归一化分布。这里其实做了一个近似～～～将softmax中的很难计算的量 $\\sum_{j=1}^Vexp(u_j^Tv_i)$ 近似成了 $X_i$ 通常语料库较大的情况下 $X_{ij}$ 都会很大，这会使得优化变得困难。一个有效的方法是最小化它们俩的对数形式： $$\\begin{align} \\hat J&amp;=\\sum_{i=1}^V\\sum_{j=1}^V(log(\\hat P)_ {ij}-log(\\hat Q)_ {ij})\\ &amp;=\\sum_{i=1}^V\\sum_{j=1}^VX_i(u_j^Tv_i-logX_{ij})^2 \\end{align}$$ Glove的优点： 训练迅速：也需要遍历整个语料库，但是计算每一个词的概率时并不需要像word2vec那样消耗softmax那么大的计算量 scalable to huge corpora 可拓展性 对于较小的语料库和向量也有很好的性能 3. Intrinsic evaluation3.1 Word Vector Analogies 语义 semantic information： 句法结构 syntactic structure: 3.2 Intrinsic Evaluation Tuning Example: Analogy Evaluations需要调节的超参数： 词向量的维度 dimension of word vectors 语料库的大小 corpus size 语料库的种类 corpus source/type 上下文窗口大小 context window size 上下文对称性 context symmetry 3.4 Further Reading: Dealing With AmbiguityImproving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al, 2012) 4. Extrinsic Taskssentiment, named-entity recognition(NER), given a context and a central word, than classify the central word to be one of many classes. 4.1 retraining word VectorsIf we retrain word vectors using the extrinsic task, we need to ensure that the training set is large enough to cover most words from the vocabulary. presentationLinear algebraic structure of word senses with applications to polysemy reference: CS224n笔记3 高级词向量表示","link":"/2018/04/30/cs224d-lecture2-%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E9%AB%98%E7%BA%A7%E8%A1%A8%E7%A4%BA/"},{"title":"cs224d-lecture3 基于Window的分类与神经网络","text":"分类问题 window classification 如何自己开始一个项目 分类问题sentiment, named-entity recognition(NER)都可以看作是分类问题。给定一个词的词向量，预测其所属的类。这与传统的监督学习都是一样的～ $${x^{(i)},y^{(i)}}_1^N$$ 其中 $x^{(i)}$ 是一个 d-维向量，$y^(i)$ 是一个 C-维one-hot向量，N是总数。 softmaxsoftmax: $$p(y_i=1|x)=\\dfrac{exp(W_jx)}{\\sum_{c=1}^Cexp(W_cx)}$$ softmax与交叉熵损失训练时可以直接最小化正确类别的概率的负对数： $$-log(\\dfrac{exp(W_kx)}{\\sum_{c=1}^Cexp(W_cx)})$$ 其实这个损失函数等效于交叉熵 $H(\\hat y, y)=-logy_ilog(\\hat y)$,其中y是one-hot向量。 对于N个数据点 $$-\\sum_{i = 1}^N\\log \\bigg(\\frac{\\exp(W_{k{(i)}\\cdot}x^{(i)})}{\\sum_{c=1}^C\\exp(W_{c\\cdot}x^{(i)})}\\bigg)$$ 加上正则化： $$-\\sum_{i = 1}^N\\log \\bigg(\\frac{\\exp(W_{k{(i)}\\cdot}x^{(i)})}{\\sum_{c=1}^C\\exp(W_{c\\cdot}x^{(i)})}\\bigg) + \\lambda \\sum_{k=1}^{C\\cdot d + |V|\\cdot d} \\theta_k^2$$ 在传统的机器学习中，我们只需要训练权重参数即可。但在这里我们还可以以重新训练词向量中的权重参数。那么需要训练的参数数量：Nxd+dxV retrain embedding但当你的训练集很小时，可能会使词向量失去泛化效果。 This is because Word2Vec or GloVe produce semantically related words to be located in the same part of the word space. When we retrain these words over a small set of the vocabulary, these words are shifted in the word space and as a result, the performance over the final task could actually reduce. Window classification通过监督学习来对一个single word进行分类显然是不符合自然语言的特性的。因为一个word具有多义性和多词性。需要结合上下文来判断。 用$X^{i}_{window}$ 代替单个词作为输入 $W_i$ cs224d-lecture4 反向传播和项目指导Project QA: A neural network for Factoid Question Answering over Paragraph sentiment: http://nlp.standford.edu/sentiment/ 接下来都是围绕着课程项目的指导与建议，就不啰嗦了。简单写写一些体会： 不要想着一上来就发明个新模型搞个大新闻 也不要浪费大部分时间在爬虫上面，本末倒置 把旧模型用于新领域\\新数据也是不错的项目 先要按部就班地熟悉数据、熟悉评测标准、实现基线方法 再根据基线方法的不足之处思考深度学习如何能带来改进 再实现一个已有的较为前沿的模型 观察该模型犯的错误，思考如何改进 这时才能没准就福至心灵发明一个新方法","link":"/2018/04/30/cs224d-lecture3-Word-Window%E5%88%86%E7%B1%BB%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"cs224d-lecture8-RNN","text":"主要内容： 语言模型 Language model 循环神经网络 recurrent neural network 梯度消失和梯度爆炸问题的原因以及解决方法 双向rnn， deep bi-RNNs 关于依存分析的presentation 语言模型 Language Model语言模型是计算一系列词以特定序列出现的概率。传统的语言模型是基于频率，计算在前n个词的条件下生成下一个词 $w_i$ 的概率。 $$P(w_1,…,w_m)=\\prod_{i=1}^{i=m}P(w_i|w_1,…,w_i-1)\\approx\\prod_{i=1}^{i=m}P(w_i|w_{i-n},…,w_{i-1})$$ 其中： $$P(w_2|w_1)=\\dfrac{count(w_1,w_2)}{count(w_1)}$$ $$P(w_3|w_1,w_2)=\\dfrac{count(w_1,w_2,w_3)}{count(w_1,w_2)}$$ 但基于概率的语言模型并不能捕捉到一些语义信息。 For instance, consider a case where an article discusses the history of Spain and France and somewhere later in the text, it reads “The two countries went on a battle”; clearly the information presented in this sentence alone is not sufficient to identify the name of the two countries. 于是出现了第一个神经网络的语言模型， learning a distributed representation of words $$\\hat y=softmax(W^{(2)}tanh(w^{(1)}x+b^{(1)})+w^{(3)}x+b^{(3)})$$ W^{(1)} 应用于词向量（solid green arrows） W^{(2)} 应用于隐藏层 W^{(3)} 应用于词向量（dashed green arrows） 但如果要记忆更多的词，必须要增大windows size n，这会造成计算量太大而无法计算。 循环神经网络 Recurrent Neural Network language model $$h_t = \\sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$ 其中+表示concatenate还是直接相加？通过作业实现，是相加～ shapes: $h_0\\in R^{D_h}$ is some initialization vector for the hidden layer at time step 0, $x\\in R^{d}$ is the column vector for L at index [t] at time step t $W^{hh}\\in R^{D_h\\times D_h}$ $W^{hx}\\in R^{D_h\\times d}$ $W^{(S))}\\in R^{|V|\\times D_h}$ 当前时间步的输出： $\\hat y \\in R^{|V|}$ 通过softmax得到的在词表V上的概率分布。 那么当前时间步的损失值： $$J^{(t)}(\\theta) = -\\sum_{j=1}^{|V|}y_{t,j}log\\hat y_{t,j}$$ $y_{t,j}$ 表示当前时间步的actual word,是 one-hot vector. 在训练模型时，$\\hat y_t$ 用来计算当前时间步的损失值,从而训练参数。而在测试集中时，也就是生成sentence时，用来作为下一个时间步的输入。 那么对整个sentence的预测的损失值： $$J=\\dfrac{1}{T}\\sum_{t=1}^T(\\theta)=-\\dfrac{1}{T}\\sum_{t=1}^T\\sum_{j=1}^{|V|}y_{t,j}\\times log(\\hat y_{t,j})$$ 困惑度： $$Perplexity=2^J$$ 梯度消失和梯度爆炸问题Training RNNs is incredibly hard! Buz of gradient vanishing and explosion problems. 这篇文章对rnn中梯度消失的问题说的比较清楚，RNN梯度消失和爆炸的原因 这里将rnn简化了,原本应该是： $$h_t=\\sigma (Wf(h_{t-1})+W^{(hx)}x_{|t|})$$ $$\\hat y = softmax(W^{(S)}f(h_t))$$ 这里就按照简化的来推导吧，t时间步的损失值对 $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}\\dfrac{\\partial h_t}{\\partial h_k}\\dfrac{\\partial h_k}{\\partial W}$$ 其实主要是这个式子的问题 $\\dfrac{\\partial h_t}{\\partial h_k}$, $h_t$ 是W 和 $h_t-1$ 的函数， $h_{t-1}$ 又是 W 和 $h_{t-2}$ 的函数….. 也就是说 $h_t$ 是之前所有时刻 $h_k$ 的函数，而 $h_k$ 也是权重 W 的函数 $$\\dfrac{\\partial h_t}{\\partial h_k} = \\prod_{j=k+1}^k\\dfrac{\\partial h_j}{\\partial h_{j-1}}=\\prod_{j=k+1}^tW^T\\times diag[f’(j_{j-1})]$$ 其中 $h\\in R^{D_h}$, 因此其导数 $\\partial h_j/\\partial h_{j-1}$ 是一个 $D_h \\times D_h$ 的雅克比矩阵。 所以有： $$\\dfrac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t\\dfrac{\\partial E_t}{\\partial y_t}\\dfrac{\\partial y_t}{\\partial h_t}(\\prod_{j=k+1}^t\\dfrac{\\partial h_j}{\\partial h_{j-1}})\\dfrac{\\partial h_k}{\\partial W}$$ 定义 $\\beta$ 为范式的下界，那么 $||\\dfrac{\\partial h_j}{\\partial h_{j-1}}||$ 很容易变得很小或很大。 解决梯度爆炸或消失的一些tricks梯度裁剪 gradient clipping对于gradient exploding，有个很简单的trick:gradient clipping 可以动手实践下，也许对梯度会有更深的理解～ 实线Solid lines表示 standard gradient descent trajectories 虚线Dashed lines表示 gradients rescaled to fixed size 将error看作是很多维参数空间的函数,如果是二维的话，那error surface就是一个曲面。在曲面上高曲率的地方(high curvature walls)，其梯度也就很大。 详细的还是看文献吧On the difficulty of training Recurrent Neural Networks, Pascanu 对于梯度消失 vanishing gradients 参数初始化 Initialization relus, Rectified Relus 很难理解为啥用relu能很好的解决梯度消失的问题，的确relu的梯度为1，但它的非线性也太简单了吧。。。所以得看看原论文 A Simple Way to Initialize Recurrent Networks of Rectified Linear Units softmax计算量太大的问题对于每个时间步，从隐藏层到输出 $W^{(S)} \\in R^{D_h, V}$ ,如果词表很大的话，这个矩阵也就很大了～ 序列模型的一些其他任务Classify each word into: NER Entity level sentiment in context opinion expression extraction Opinion Mining with Deep Recurrent Neural Networks 双向 RNNs 其实跟rnn没有太多变化，有两个隐藏层，并且隐藏层的递归分别是从语料库的两个不同的方向。 Deep bidirectional RNNs $$\\overrightarrow {h_t^{(i)}}=f(\\overrightarrow{W^{(i)}}h_t^{(i-1)}+\\overrightarrow{V^{(i)}}h_{t-1}^{(i)}+\\overrightarrow{b^{(i)}})$$ 其中 $h_t^{(i-1)}$ 表示上一层隐藏层的输入， $h_{t-1}^{(i)}$ 表示当前隐藏层层的上一个时间步的输入。 $$\\overleftarrow {h_t^{(i)}}=f(\\overleftarrow{W^{(i)}}h_t^{(i-1)}+\\overleftarrow{V^{(i)}}h_{t-1}^{(i)}+\\overleftarrow{b^{(i)}})$$ 需要训练的参数有：$\\overrightarrow{W^{(i)}},\\overleftarrow{W^{(i)}}$ $\\overrightarrow{V^{(i)}},\\overleftarrow{V^{(i)}}$ $$\\hat y_t=g(Uh_t+c)=g(U[\\overrightarrow{h_t^{(L)}};\\overleftarrow{h_t^{(L)}}]+c)$$ data evalution PresentationStructured Training for Neural Network Transition-Based Parsing, David Weiss, Chris Alberti, Michael Collins, Slav Petrov 表示根本听不懂，只知道使用deeplearning做依存分析。。用state-of-art的SyntaxNet和前人几篇有影响力的进行了对比～","link":"/2018/05/04/cs224d-lecture8-RNN/"},{"title":"cs224d-lecture9 机器翻译","text":"主要内容： RNN Translation Model GRU LSTM Towards a Better Language Modeling RNN Translation Model encoder: $$h_t=\\phi(h_{t-1},x_t)=f(W^{(hh)}h_{t-1}+W^{(hx)}x_t)$$ encoder: $$h_t=\\phi(h_{t-1})=f(W^{(hh)}h_{t-1})$$ $$y_t=softmax(W^{(S)}h_t)$$ corss entropy function: $$max_{\\theta}\\dfrac{1}{N}\\sum_{n=1}^Nlogp_{\\theta}(y^{(n)}|x^{(n)})$$ rnn的几点扩展1.在encoder和decoder中，$W^{(hh)}$ 是不一样的 2.计算decoder中的隐藏神经元时，可以不仅仅只使用上一个隐藏层的信息，而是使用三种input来获取更多的信息 The previous hidden state (standard) Last hidden layer of the encoder (c = hT) Previous predicted output word, $y^{t−1}$ $$h_t=\\phi(h_{t-1},c,y_{t-1})$$ 3.使用deep rnn: 这需要更大的语料库 4.使用bi-directional encoder 5.颠倒词序进行训练 rnn 到底做了什么？we never gave this model an explicit grammar for the source language, or the target language, right? It’s essentially trying, in some really deep, clever, continuous function, general function approximation kind of way, just correlation, basically, right? And it doesn’t have to know the grammar, but as long as you’re consistent and you just reverse every sequence, the same way. It’s still grammatical if you read it from the other side. And the model reads it from potentially both sides, and so on. RNN的缺点我们知道在传统的神经网络传递中$a^{} = g(W_{a}\\cdot[a{},x{}] + b_a)$, 很容易造成梯度消失，并且神经网络不擅长处理长期依赖的问题。以语言模型为例，即序列很难反向传播到比较靠前的部分，也就难以调整序列前面的计算。 GRU(gated recurrent units)原论文：https://arxiv.org/pdf/1406.1078v3.pdf 这个图画的算是很好了的吧。。但是还是复杂了一点，必须对着公式才能看懂。可以看简化图： $$r_t=\\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})\\tag{reset gate}$$ $$u_t=\\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})\\tag{update gate}$$ $$\\tilde h_t=tanh(Wx_t+r_t\\circ Uh_{t-1})\\tag{new memory}$$ $$h_t=(1-u_t)\\circ \\tilde h_t+u_t\\circ h_{t-1} \\tag{Hidden state}$$ 主要就是两个gate： 重置门r：决定了如何将新的输入信息与前面的记忆相结合。所以它的作用对象是 $\\tilde h_t$ 也就是new memory cell. 更新门u：定义了前面记忆保存到当前时间步的量。所以它的作用对象是 $h_t$.也就是当前memory cell保存 $h_{t-1}$ 和 $\\tilde h_t$ 多少信息量。 GRU 背后的原理： 如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。 如果重置门设为0，那么将忽视之前的隐藏状态，这意味着模型可以丢掉之前的信息，当它们与未来的信息不相关时。 更新门u控制着过去的状态对现在的影响。If z close to 1, then we can copy information in that units through many time steps!这意味着 Less vanishing gradient! Units with short-term dependencies often have reset gate very active. 这几句总结可以说是道出了GRU的精髓了！ 仍需要理解的几个问题： 激活函数为什么是tanh ,sigmoid，并不能像概率图模型那样，用数学来解释，就是很玄学吧。。 这篇文章写的不错～经典必读：门控循环单元（GRU）的基本概念与原理 LSTM 这个图有点抽象。 这个图是来自Ng的课，将图中的 $a^{&lt; t &gt;}$ 换成 $h_t$ 就可以了～ 三个gate以及新的记忆细胞，三个sigmoid和一个tanh $$i_t = \\sigma(W^{(i)}x_t+U^{(i)}h_{t-1})\\tag{Input\\update gate}$$ $$f_t = \\sigma(W^{(f)}x_t+U^{(f)}h_{t-1})\\tag{forget gate}$$ $$o_t = \\sigma(W^{(o)}x_t+U^{(o)}h_{t-1})\\tag{Output/Exposure gate}$$ $$\\tilde c_t = tanh(W^{(c)}x_t+U^{(c)}h_{t-1})\\tag{New memory cell}$$ 输入门和遗忘门作用于新的记忆细胞得到最终的记忆细胞: $$c_t=f_t\\circ c_{t-1}+i_t\\circ \\tilde c_t$$ 输出门作用于新的记忆细胞得到最终的隐藏状态： $$h_t=o_t\\circ tanh(c_t)$$ 这里要理解每个gate的目的到底是啥？虽然很难用数学来解释，但是从intuitive上来理解下还是可以的～ New memory cell: 在GRU中也存在，但是是有区别的，这里是通过input word $x_t$ 和 过去的隐藏状态 $h_{t-1}$ 得到的。GRU中虽然也是，但直接使用了reset gate Input gate: 也叫更新门，因为新的记忆细胞 $\\tilde c_t$ 生成时并未考虑current word是否有保留的意义，因此 $i_t$ 作用于 $\\tilde c_t$. Forget gate: 跟输入门是同样的道理，新的记忆细胞 $\\tilde c_t$ 生成时并未考虑past memory cell是否有保留的意义，因此 $f_i$ 作用于 $c_{t-1}$ Final memory generation: 综合考虑了遗忘门作用后的 $c_{t-1}$ 和 输入门作用后的 $\\tilde c_t$ Output/Exposure Gate: 在GRU中不存在，这里是用来区分最终的记忆细胞 $c_t$ 和 最终的隐藏状态 $h_t$ 的。因为记忆细胞中包含有很多对于隐藏状态来说不必要的信息。 使用LSTM的机器翻译效果很神奇！！！ PresentationTowards a Better Language Modeling. Better inputs: word $\\rightarrow$ subword $\\rightarrow$ char Better regularization/Processing Better Model","link":"/2018/05/07/cs224d-lecture9-machine-translation/"},{"title":"论文笔记-contrastive learning","text":"image-basedsimCLRA Simple Framework for Contrastive Learning of Visual Representations 作者提出了一个简单的对比学习框架，不需要特殊的网络结构和memory bank. Introduction现有的无监督视觉表示学习的方法主要分为两类：生成式和判别式。 生成式主要包括以下三类： deep belief nets[^1] Auto-encoding[^2] Generative adversarial nets[^3] pixel-level 生成式算法非常消耗计算资源，因而对于有效的表示学习并不是必须的。 判别式方法的目标函数更接近监督学习，不过其对应的监督任务是从没有标签的数据集中自行构造的，因而学到的视觉表示能力受限于预定义的任务，而泛化能力有限。现有的达到sota的几篇paper[^4][^5][^6] 作者提出了一个简单的对比学习方法，不仅达到了sota，而且不需要复杂的网络结构[^6][^7]，也不需要memory bank[^8][^9][^10][^11]. 为了系统的理解怎样才能获得有效的的对比学习，作者研究了以下几个重要组成部分： data augmentation：相比有监督学习，对比学习更需要数据增强 $t\\sim T$ nonlinear projection：如图所示，在视觉表示和contrast loss之间增加一个非线性projection $g(\\cdot)$ 很有必要 normalized embeddings and an appropriately adjusted temperature parameter: 归一化的cross entropy和可调整的temperature parameter. larger batch size and more training steps Method如上图所示，simCLR 主要包括四部分： A stochastic data augmentation module: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur A neural network base encoder $f(\\cdot)$，作者采用的是 ResNet. $h_i = f(\\tilde x_i) = ResNet(\\tilde x_i)$ A small neural network projection head $g(\\cdot)$, $z_i = g(h_i) = W^{(2)}σ(W^{(1)}h_i)$. 作者发现在 $z_i$ 上计算 contrast loss，比 $h_i$ 效果更好。 A contrastive loss function：NT-Xent (the normalized temperature-scaled cross entropy loss. 其中 $sim(u,v)=\\dfrac{u^Tv}{\\lVert u \\rVert \\lVert v\\rVert}$. Training with Large Batch Size作者采用了更大的batch size(256 $\\rightarrow$ 8192)，因而不需要memory bank. 这样一个batch有 ($8192\\times 2=16382$) 个负样本。 在超大的batch size情况下，使用SGD/Momentum学习率不稳定，因此作者使用LARS optimizer. 作者使用 32-128 cores TPU进行训练。（这真的劝退。。。 Global BN在分布式训练的场景下，BN的均值和方差是在单个device上计算的。而两个正样本是在同一个device上计算的，因此在拉进两个正样本之间的agreement时，BN会造成信息泄露。为了解决这个问题，作者采用的方法是在所有的device上计算BN的均值和方差。类似地解决这一问题的方法还有：shuffling data examples across devices[^10], replacing BN with layer norm[^7]. 这点其实不太理解，为啥BN会造成信息泄露？ Evaluation ProtocolDataset and Metrics.作者先在CIFAR-10上进行试验，得到了94.0%的准确率（有监督的准确率是95.1%）. 为了验证学习得到的视觉表示，作者采用广泛使用的linear evaluation protocol[^5][^6]: a linear classifier is trained on top of the frozen base network, and test accuracy is used as a proxy for representation quality. 除了linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning. Default settingWe use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space. As the loss, we use NT-Xent, optimized using LARS with learning rate of 4.8 (= 0.3 × BatchSize/256) and weight decay of 10−6. We train at batch size 4096 for 100 epochs. Data Augmentation for Contrastive Representation LearningData augmentation defines predictive tasks数据增强定义预预测任务。 随机裁剪既包括了 global and local views, 也包括了 adjacent views. Composition of data augmentation operations is crucial for learning good representations 为了验证不同的数据增强对于表示学习的影响，作者进行了ablation实验，只对图2中的某一分支进行transformation. 实验结果如图5所示，对角线只有一种augmentation方法，非对角线是两种组合。 结果表明，单一的增强方法都不能学到好的表示。两种组合时，预测任务越难，学习到的表示能力越好。最好的组合是 random crop 和 color distortion. 但是只是单独用其中某一种效果都不好。 作者对只用单独一种数据增强方法不好的原因进行了解释： We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Figure 6 shows that color histograms alone suffice to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features. Contrastive learning needs stronger data augmentation than supervised learning相比监督学习，stronger数据增强对contrastive learning更为重要。 When training supervised models with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning. Architectures for Encoder and HeadUnsupervised contrastive learning benefits (more) from bigger models对比学习在大模型下获益更多。 A nonlinear projection head improves the representation quality of the layer before it作者探究了projection的三种方式： identity mapping linear projection non-linear projection 结果表明，非线性projection更好，没有的话效果很差。 除此之外，使用project head之前的hidden layer $h(i)$ 比project layer之后的表示 $z(i)$ 效果更好, $\\ge 10%$。 为什么使用 non-linear projection head 之前的hidden layer效果更好？ 作者认为对比loss会损失信息。z = g(h) 被训练成transformation invariant 变换不变性（因为contrast loss要拉近两个不同变换的正样本）。因此，$g(\\cdot)$ 会丢失信息。 作者通过实验验证这一猜想，在保证最终的dimension不变的情况下， Loss Functions and Batch SizeNormalized cross entropy loss with adjustable temperature works better than alternatives 实验表明 NT-Xent 效果最好。这是因为其他的目标函数并没有衡量负样本的难度：unlike cross-entropy, other objective functions do not weight the negatives by their relative hardness. $l_2$ normalization 和 temperture 很重要：$l_2$ normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives; 没有 $l_2$ normalization,尽管对比准确率很高，但是学习到的表示能力并不好。 合适的temperture也很重要 Contrastive learning benefits (more) from larger batch sizes and longer training 实验表明，batch size很重要，越大收敛的越快，但最终效果也不是越大越好。随着训练的增加，batch size造成的表现差异也随着逐渐消失。 Comparison with State-of-the-art作者采用了三种方法来验证performance。 Linear evaluation相比fine-tune，linear evaluation 的区别在于学习率的设置。 没搞懂为啥叫 linear evaluation？ 和fine-tune的区别就在于学习率的设置？ Semi-supervised learningsample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (∼12.8 and ∼128 images per class respectively). Transfer learning在imageNet上训练，在其他数据集上测试。同上，采用了两种方式， Linear evaluation 和 fine-tune. speech-basedRepresentation Learning with Contrastive Predictive Codingwav2vec: Unsupervised pre-training for speech recognition encoder: $X \\rightarrow Z$ content-network: $C\\rightarrow C$ noise contrastive binary classification task. $\\sigma$ 是 sigmoid 函数 $c_i$ 是当前step的content feature $h_k(x_i)$ 是step-specific affine transformation $h_k(c_i) = W_kc_i+b_k$ $z_{i+k}$ 是距离当前step为k的encoded feature, 为正样本 $\\hat z\\sim p_n$是 (T-k) 帧中均匀选择10个负样本，得到对数概率的期望后，再乘以 $\\lambda=10$. [^1]: A fast learning algorithm for deep belief nets.[^2]: Auto-encoding variational bayes.[^3]: Generative adversarial nets. NIPS2014[^4]: Discriminative unsupervised feature learning with convolutional neural networks. NIPS2014[^5]: Representation learning with contrastive predictive coding. arXiv2018[^6]: Learning representations by maximizing mutual information across views. NIPS2019[^7]: CPC: Data-efficient image recognition with contrastive predictive coding, arXiv2019[^8]: Unsupervised feature learning via non-parametric instance discrimination. CVPR2018[^9]: Contrastive multiview coding. arXiv2019[^10]: MoCo: Momentum contrast for unsupervised visual representation learning, arXiv2019[^11]: Self-supervised learning of pretext-invariant representations. arXiv2019","link":"/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/"},{"title":"论文笔记-dynamic convolution and involution","text":"paper list: CARAFE: Content-Aware ReAssembly of FEatures Involution: Inverting the Inherence of Convolution for Visual Recognition Pay less attention with lightweight and dynamic convolutions ConvBERT: Improving BERT with Span-based Dynamic Convolution Dynamic Region-Aware Convolution Involution为什么要叫反卷积呢？卷积的特性是： space-agnostic：空间不变性，也就是用一个kernel在feature map上滑动。这样学的到feature是单一的为了增加feature的丰富性，采用很大的channel channel-specific: 通道特异性。尽管channel增加能学到更多特征，但是通道太大其实是有冗余的，有人做低秩实验，发现很多channel对应的参数是线性相关的 于是，作者设计了一个与卷积完全相反的算子，反卷积： space-specific: 根据content生成相应的weights channel-agnostic: 共享参数。类似attention的projection和feedforward. 12345678910111213141516def forward(self, x): # 1. 生成pixel-wise对应的权重，每个pixel对应的权重是 [kernel_size^2*group]. # print(&quot;after avgpool: &quot;, self.avgpool(x).shape) # print(&quot;after conv1: &quot;, self.conv1(x if self.stride == 1 else self.avgpool(x)).shape) weight = self.conv2(self.conv1(x if self.stride == 1 else self.avgpool(x))) # print(&quot;weight: &quot;, weight.shape) b, c, h, w = weight.shape weight = weight.view(b, self.groups, self.kernel_size**2, h, w).unsqueeze(2) print(&quot;weight: &quot;, weight.shape) # 2. 将x通过unfold以kernel_size为大小，stride为步长i女性展开 print(&quot;after unfold: &quot;, self.unfold(x).shape) # [bs, channel*kernel*2, ((h-kernel+1+2*pad)/stride))^2] out = self.unfold(x).view(b, self.groups, self.group_channels, self.kernel_size**2, h, w) print(&quot;out: &quot;, out.shape) out = (weight * out).sum(dim=3).view(b, self.channels, h, w) return out 思路很像local attention, 区别在于这个weight是通过content+linear得到的，而不是通过pixel之间的relation得到的。而且，看源代码这个weights并没有做normalization. 然后把生成的weights与对应的pixel周围的[kernel,kernel]的pixels进行加权求和。","link":"/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/"},{"title":"论文笔记-video transformer","text":"paper list: Training data-efficient image transformers &amp; distillation through attention. An image is worth 16x16 words: Transformers for image recognition at scale. ViViT: A Video Vision Transformer. Is space-time attention all you need for video understanding Video transformer network. DeiTViViT: A Video Vision Transformer作者通过大量实验来研究 vision transformer，并探索最合适的且efficient的结构使其能适用于small datasets. 其中，主要包括以下三个方面的探索： tokenization strategies model architecture regularisation methods. transformer-based architecture的缺点：相比卷积网络，transformer缺乏了一定的归纳偏置能力，比如平移不变性。因此，需要大量的数据来学习。","link":"/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/"},{"title":"论文笔记-unlikelihood training","text":"基于标准的似然训练的语言模型在解码时会出现很dull的重复问题。如下图所示： 诚然将likelihood作为训练目标，能得到强大的具有语言理解能力的模型。但是将似然作为目标会导致语言的平淡和奇怪的重复。而且基于强大的GPT2-117M，使用beam search，理论上beam size越大，生成的句子概率越大，效果越好才对。但事实却是反直觉的，beam size越大，语言的退化效果更明显。（我猜这也是很多时候beam size设为10左右，而不会更大。） Holtzman[^1] 揭示了语言模型的这种退化现象，他们将机器生成的语言和人类语言进行对比，如下图所示，人类文本在生成每个token的困惑度中表现出相当大的波动，而由最大似然解码产生的机器文本的分布则出现不自然的平坦和较高的token概率. 事实上，语言模型在生成词的时候，大部分的概率集中在几百个tokens上。这限制了机器文本的多样性，也是导致模型在长文本生成退化的原因。基于此，如下图所示，top-k, nucleus sampling是不错的方法。 但是sampling的方法并没有改变模型生成的概率其本身。生成文本退化的真正原因还未知，已有的论文认为有以下几种可能： 模型架构选择的by-product，例如 Transformer 更喜欢重复; 人类语言的内在属性，而不是模型缺陷； 语料有限; 相比之下，Welleck[^2] 认为生成模型以最大似然最为训练目标会导致文本生成退化。其原因有： 将注意力集中在argmax或top-k,而不是优化整个distribution 只是集中于下一个token的生成，而不是整个句子的优化 为此，Welleck[^2] 提出了 unlikelihood training: 依据likelihood来优化target tokens，并给予较大的概率 依据unlikelihood来更新，避免给予target tokens太大的概率。（不知理解的对错，原文如下） Unlikelihood training works by combining two types of updates: a likelihood update on the true target tokens so they are assigned high probability, and an unlikelihood update on tokens that are otherwise assigned too high a probability. unlikelihood lossunlikelihood loss的核心就是降低negative condidates $C^t$ 的似然概率。 token-level unlikelihood loss以自回归的语言模型为例，下一个时间步的loss计算包括target的似然最大化，以及negative candidates的似然最小化。 negative candidates 是当前试了之前的词汇。 sequence_level unlikelihood loss我们知道基于自回归模型的训练和解码是存在exposure bias的，也就是解码的时候有误差累积。其实这种distribution mismatch是maxmimum-likelihood追求当前时刻的概率最大话。而没有从整个句子的层面去考虑。比如重复问题，你上一个词出现过了，下一个词还出现它；你这句话说过一遍了，你还要再说一遍。。这不离谱吗。但是模型就是这么傻，或者说模型没有大局观的原因是之前的优化都是在token-level层面。 因此，Welleck[^2] 提出了sequence-level unlikelihood loss. 这个公式看起来跟前面一样，但是区别在于 negative condidates的选择。这里的negative是从n-gram层面来考量的。 也就是对于当前时间步，如果它是重复的n-gram的一部分，那么它就是negative candidate. 这里有点不太好理解，token-level就是把之前出现的词作为negative candidate. 如果有些词在前面并没有出现，但它的出现会导致重复的n-gram？这不合理啊，它都没出现，怎么可能出现包含它的n-gram呢？？ 这样想 sequence-level 不就是 token-level的一种特殊形式。 带着疑问去看代码吧。看完代码，sequence-level是在训练完token-level之后，再进行finetune，对导致出现重复的ngram的某个time-step进行惩罚。。在我的实验上不太靠谱，重复反而变多了 [^1]: The Curious Case of Neural Text Degeneration[^2]: Neural text degeneration with unlikelihood training","link":"/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/"},{"title":"论文笔记-DETR and Deformable DETR","text":"DETR: End-to-End Object Detection with Transformers Deformable DETR： Deformable Transformer for End-to-End Object Detection DETR: End-to-End Object Detection with Transformerspaper link: https://arxiv.org/abs/2005.12872 Motivation传统的目标检测范式： 使用CNN Backbone提取feature map; 使用 Region Proposal Network (RPN) 在feature map上枚举所有的windows,输出N个候选boxes 使用分类器对每个box进行分类 这样存在很多问题： 问题1：Enumerate candidate boxes in RPN 枚举所有的pixel 对每个pixel，枚举出所有预定义大小的boxes 大部分的候选框都是无效的，因此 Inefficient, slow 问题2：Redundant boxes and NMS RPN 输出大量的boxes Non-maximum suppression (NMS) merges/removes redundant boxes These hand-designed components have a few hyperparameters Model tuning is complex Architecture of DETR因此，作者提出了更为简单的end-to-end的目标检测模型： Self-attention vs. Convolution in VisionTransformer 已经非常熟悉了，就不详细介绍了。这里对比下 self-attention 和 CNN 在视觉领域的应用 每个pixel可以看作是自然语言中的 token convolution is used to integrate pixels Recognize patterns within a small window of pixels Difficult to integrate non-local pixels Have to make network very deep to “see the big picture” Self-attention (transformer) also integrates multiple pixels Works when the correlated pixels are non-local Trunk, tail, legs of an elephant to a whole elephant Architecture 整个结构可以看作四部分： CNN backbone Transformer Encoder Transformer Decoder bipartite matching loss CNN backbone + Transformer Encoder前两个好理解，需要注意的是 position encoding 与NLP中不同。考虑到输入是二维图像，对应的位置 (x,y) 也是二维的。 总结下和原始transformer编码器不同的地方： 输入编码器的位置编码需要考虑2-D空间位置。 位置编码向量需要加入到每个Encoder Layer中。 在编码器内部位置编码Positional Encoding仅仅作用于Query和Key，即只与Query和Key相加，Value不做任何处理。 Transformer Decoderdecoder与原始的transformer decoder的区别在于两点： 其输入是 Object queries. 是可学习的 nn.Embedding, 维度为 [100, bsz, 256]. 其实可以理解成可学习的位置编码。 非自回归的可并行解码。 bipartite matching lossdecoder 的输出张量的维度是 分类分支：[bsz, 100, class + 1] 和回归分支：[bsz, 100, 4]. 其中 class + 1 表示类别总数+背景。有物体的boxes计算回归任务；没有物体的背景框，则不用回归。 问题来了：这100个候选框如何去与 不确定数量的 targets 进行匹配呢，预测框和真值是怎么一一对应的？换句话说：你怎么知道第47个预测框对应图片里的狗，第88个预测框对应图片里的车？等等。 相比Faster R-CNN等做法，DETR最大特点是将目标检测问题转化为无序集合预测问题(set prediction)。论文中特意指出Faster R-CNN这种设置一大堆anchor，然后基于anchor进行分类和回归其实属于代理做法即不是最直接做法，目标检测任务就是输出无序集合，而Faster R-CNN等算法通过各种操作，并结合复杂后处理最终才得到无序集合属于绕路了，而DETR就比较纯粹了。现在核心问题来了：输出的 [bsz, 100] 个检测结果是无序的，如何和 ground-truth bounding box 计算loss？这就需要用到经典的双边匹配算法了，也就是常说的 匈牙利算法，该算法广泛应用于最优分配问题。 整个loss function的计算分为两个步骤： 依据匈牙利算法先找到最优的匹配 根据找到的匹配，计算最终的loss 如何找到最优的匹配呢？假设一张图片有3个目标：dog, cat, car. 那么我们可以得到一个 [100, 3] 的矩阵, 矩阵的值分别表示这100个候选token为其中某一目标的“损失”. 让这个损失最小的匹配就是我们要找的最优匹配。这个过程就是匈牙利算法。 再介绍匈牙利算法之前，我们要知道这个矩阵对应的损失值是啥呢？也就是怎么衡量这个候选token是某个目标的损失值？ 假设 $y_i$ 对应的预测是 $\\hat y_{\\sigma_i}$,那么这个构成这个搭配的损失就是 $\\hat y_{\\sigma_i}$ 对应的类别为 $c_i$ 以及 box 的 mse 值。 $$L_{match}(y_i, \\hat y_{\\sigma(i)})=-\\mathbb{1}{c_i\\ne \\varnothing}\\hat p{\\sigma(i)}(c_i) + 1_{c_i\\ne \\varnothing}L_{box}(b_i, \\hat b_{\\sigma_i})$$ $L_{match}$ 就是矩阵中的元素。然后通过匈牙利算法找到对应的匹配。 找到匹配之后计算最终的loss Deformable DETRDETR 的优缺点： 优点 纯端到端，不需要手工设计的NMS之类 DETR 缺点： 小目标检测的低精度，高精度的feature map的复杂度是DETR所不能忍受的。 这是因为 attention 机制的原因，复杂度是 O(L^2) 收敛速度慢，slow convergence。原因是 pattern 是稀疏的，很难快速学到吧 when $N_k$ is large, it will lead $N_k$to ambiguous gradients for input features. Thus, long training schedules are required so that the attention weights can focus on specific keys. Deformable DETR 的设计初衷： motivation: mitigates the slow convergence and high complexity issues of DETR 具体设计方法 融合Deformable conv的空间稀疏采样和transformer的关系建模的优势 combines the best of the sparse spatial sampling of deformable convo- lution, and the relation modeling capability of Transformers deformable attention modules： 代替Transformer attention module Multi-Head Attention in Transformers常规的 attention: $A_{mqk}$ 是attention matrix. $z_q\\in R^C$ 是query feature. $x_k\\in R^C$ 是key/value feature. $\\Omega_k$ 是key index的集合. $m$ 是attention head index. 总的计算量是: $O(N_qC^2) + 2O(N_kC^2) + 2O(N_qN_kC^2)$ 复杂度：$O(N_qC^2 + N_kC^2 + N_qN_kC)$ 在长文本，或者image patched之后，$N_q=N_k &gt;&gt; C$. 所以复杂度取决于 $O(N_qN_kC)$ Deformable attention module 相比常规的attention, 需要考虑所有的key features，也就是 HW 个。 deformable attention只考虑K个keys. 如下面两个公式所示 $\\sum_{k\\in \\Omega_k} \\rightarrow \\sum_{k=1}^K$. 因此复杂度会大大降低，收敛速度也会加快。 其中： m 表示attention head index $z_q$ 表示query feature $K &lt;&lt; HW$ 表示sampling number $p_q$ 表示 $z_q$ 对应的参考2D点 $\\Delta p_{mqk}$ 表示相对参考点的sampling offset $A_{mqk}$ 则是对应采样点的weights, $\\sum_{k=1}^{K}A_{mqk}=1$ 现在的问题在于，这K个keys是怎么选择的呢？也就是 offset $\\Delta p_{mqk}$ 是怎么来的？文中是这么解释的： 这个过程如图2所示： 通过 linear projection 得到 2D 实数值 $\\Delta p_{mqk}$， 然后通过线性插值得到 $p_q + \\Delta p_{mqk}$. 对应的权重 $A_{mqk}$ 也是通过linear projection得到的。 这样 deformable attention 的复杂度：$N_qC^2 + O(N_qKC)$. Multi-scale Deformable Attention ModuleDeformable attention 可以很自然的拓展到multi-scale情况下。 m 表示 attention head index l 表示 input feature level k 表示 sampling index $A_{mlqk}$ 表示 $k^{th}$ sampling point在 $m^{th}$ head和 $l^{th}$ level的权重, $\\sum_{l=1}^{L}\\sum_{k=1}^{K}A_{mlqk}=1$. $\\Delta p_{mlqk}$ 表示sampling offset 为了确保multi-scale中 point的对应关系，作者用 normalized coordinates $\\hat p_q\\in [0,1]^2$ 来表示参考点。 $(0,0), (1,1)$ 表示top-left和bottom-right. $\\phi_l(\\hat p_q)$ 表示将归一化之后的 $\\hat p_q$ rescale 到 $l^{th}$ level feature中。 Deformable Transformer Encoder从 resnet $C_3$ 到 $C_5$ 抽取multi-scale特征图. 其中 $C_l$ 表示分辨率是输入图像的 $\\dfrac{1}{2^l}$. 这样就有3层feature map了，然后最后一层feature map是通过 kernel=$3\\times 3$, stride=$3$ 的卷积在 $C_5$ 上得到的。 总共4 levels feature map. key和query来自feature map中的pixels. 对于每一个query pixel, 其reference point是其本身。除了位置编码之外，作者还加入了level编码 $e_l$，用来表示在哪一层level. 位置编码是固定的，level编码是可训练的。","link":"/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/"}],"tags":[{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"CSAPP","slug":"CSAPP","link":"/tags/CSAPP/"},{"name":"ai challenger","slug":"ai-challenger","link":"/tags/ai-challenger/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"cs224d","slug":"cs224d","link":"/tags/cs224d/"},{"name":"contrastive learning","slug":"contrastive-learning","link":"/tags/contrastive-learning/"},{"name":"language model","slug":"language-model","link":"/tags/language-model/"},{"name":"vision transformer","slug":"vision-transformer","link":"/tags/vision-transformer/"}],"categories":[{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"CSAPP","slug":"CSAPP","link":"/categories/CSAPP/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"cs224d","slug":"cs224d","link":"/categories/cs224d/"}]}