<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>分类: ML - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:author" content="panxiaoxie"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":null}</script><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">ML</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-12-24T07:43:08.000Z" title="2018/12/24 下午3:43:08">2018-12-24</time>发表</span><span class="level-item"><time dateTime="2021-01-27T08:44:33.514Z" title="2021/1/27 下午4:44:33">2021-01-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">6 分钟读完 (大约898个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%8D%E5%9D%87%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98/">机器学习-不均衡数据问题</a></h1><div class="content"><p>对于机器学习中不均衡数据的问题是很常见的，之前在三星实习也遇到过，但当时对指标性能要求并不高，差不多也就行了。。但这次参加 kaggle 比赛，0.1% 就有很大差距了，所以还是要好好搞搞 imbalanced data problem.</p>
<p>很多时候，数据不均衡不仅仅数据收集和整理的问题，而是数据本身预期的就是这样，比如表征欺诈性交易（characterize fraudulent transactions），大多数交易都是不具有欺诈性的,只有极少数的 具有欺诈性的（Fraud）, 或者 kaggle 比赛中的 insincere-questions, 大多数问题都是正常的，只有极少数的 insincere-questions.</p>
<h2 id="More-data"><a href="#More-data" class="headerlink" title="More data"></a>More data</h2><p>显然，这是最直接的。在实际中，也是最可行的。但打比赛时，数据是给定的，就需要用到 resampling.</p>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>改变评价指标来选择更优的模型。</p>
<ul>
<li><p>accurayc 显然这个对不均衡数据是不合适的  </p>
</li>
<li><p>Precision: (tp/(tp+fp))  </p>
</li>
<li><p>Recall: (tp/(tp+fn))  </p>
</li>
<li><p>F1 Score (or F-score): precision 和 recall 的一种权衡   </p>
</li>
<li><p>Kappa: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen’s kappa</a>  </p>
</li>
<li><p>ROC 曲线：这些在之前的笔记中都有介绍 <a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/">机器学习-常用指标总结</a>  </p>
</li>
<li><p>PRAUC 损失函数？ ai challenger 比赛中看到的  </p>
</li>
</ul>
<h2 id="Resampling"><a href="#Resampling" class="headerlink" title="Resampling"></a>Resampling</h2><ul>
<li><p>过采样 over-sampling   </p>
</li>
<li><p>欠采样 under-sampling  </p>
</li>
</ul>
<p>实践中可以两种都尝试，并且能做集成增强。</p>
<p>一些采样策略：  </p>
<ul>
<li><p>数据量很大（上万或者十万？），建议欠采样，数据量较小，建议过采样  </p>
</li>
<li><p>可以尝试随机采样，也可以尝试分层采样（划分好）  </p>
</li>
<li><p>尝试不同的比例，并不一定要最后是 1:1  </p>
</li>
</ul>
<h2 id="Generate-Synthetic-Samples"><a href="#Generate-Synthetic-Samples" class="headerlink" title="Generate Synthetic Samples"></a>Generate Synthetic Samples</h2><p>SMOTE(Synthetic Minority Over-sampling Technique): 不是简单的 copy, 而是选取两个或更多的相似的样本（根据距离），然后随机扰动一个样本的属性（某一维特征吧），扰动值在它与相邻样本的差异之间。</p>
<p>paper: <a target="_blank" rel="noopener" href="http://www.jair.org/papers/paper953.html">SMOTE: Synthetic Minority Over-sampling Technique</a></p>
<p>python tool: <a target="_blank" rel="noopener" href="https://github.com/fmfn/UnbalancedDataset">UnbalancedDataset</a></p>
<h2 id="Try-Penalized-Models"><a href="#Try-Penalized-Models" class="headerlink" title="Try Penalized Models"></a>Try Penalized Models</h2><p>在训练过程中，给类别较少的一类增加惩罚项，通常是正则化．这有利于模型能注重 minority class.</p>
<p>对类和权重的惩罚对不同的算法不太一样，所有有专门的版本的算法：penalized-SVM 和 penalized-LDA.</p>
<p>也有通用的惩罚模型，适用于不同的分类器 <a target="_blank" rel="noopener" href="http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html">CostSensitiveClassifier </a></p>
<p>惩罚模型提供了另外一种方法来 “balance” 模型，但设置惩罚矩阵是很复杂的，需要很多尝试。</p>
<h2 id="Try-a-Different-Perspective"><a href="#Try-a-Different-Perspective" class="headerlink" title="Try a Different Perspective"></a>Try a Different Perspective</h2><p>对于之前说过的欺诈性检测和kaggle insincere 问题的发现，从另外一个角度看，也能看做是异常检测（anomaly detection）和 变异检测（change detection）.</p>
<p>类似于 open set recognition 或 out-of-distribution 问题，样本数极少的那个类别 minior class 可以看做是 outliers class.</p>
<blockquote>
<p>Anomaly detection is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by it’s sequence of system calls. The events are rare and when compared to normal operation.</p>
</blockquote>
<blockquote>
<p>Change detection is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-08-07T01:28:38.000Z" title="2018/8/7 上午9:28:38">2018-08-07</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">19 分钟读完 (大约2853个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/">机器学习-常用指标总结</a></h1><div class="content"><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/maybe2030/p/5375175.html#_label0">http://www.cnblogs.com/maybe2030/p/5375175.html#_label0</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/">http://alexkong.net/2013/06/introduction-to-auc-and-roc/</a></p>
</li>
</ul>
<h2 id="精确率-Precision-召回率-Recall-和-F1-值"><a href="#精确率-Precision-召回率-Recall-和-F1-值" class="headerlink" title="精确率 Precision, 召回率 Recall 和 F1 值"></a>精确率 Precision, 召回率 Recall 和 F1 值</h2><p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb10.png"></p>
<p>对于数据不均衡时，使用accuracy是不准确的。</p>
<p>举个栗子：</p>
<p>a million tweet: 999,900条不是关于pie的，只有100条是关于pie的</p>
<p>对于一个stupid分类器，他认为所有的tweet都是跟pie无关的，那么它的准确率是99.99%！但这个分类器显然不是我们想要的，因而accuracy不是一个好的metric，当它目标是rare，或是complete unbalanced.</p>
<p>引入另外两个指标：</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb11.png"></p>
<ul>
<li><strong>精度 precision:</strong> 是检索出来的条目（比如：文档、网页等）有多少是准确的。精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率。  </li>
</ul>
<ul>
<li><strong>召回 call：</strong> 召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率.</li>
</ul>
<p><strong>需要先确定一个正分类：</strong> 这里需要检索出来的是与 pie 无关的，也就是与 pie 无关的是正分类。那么精度就是分类器检测出来的正分类中 gold pos 所占的比例，那么 precision = 0/(0+0)</p>
<p>这里需要检索的是与 pie 无关的，检索出来的 true pos 占样本中 gold pos 的比例，那么 recall = 0/(100+0) = 0.</p>
<p>总结下来，precision 和 recall 都是以 true pos 作为分子，precision 是以分类器预测出来的 pos(true pos + false neg) 作为分母，所以是差准率. recall 则是以总的 gold pos(true pos + false neg) 作为分母，所以是查全率。</p>
<p>当然希望检索结果Precision越高越好，同时Recall也越高越好，但事实上这两者在某些情况下有矛盾的。比如极端情况下，我们只搜索出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就会很低。因此在不同的场合中需要自己判断希望Precision比较高或是Recall比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。</p>
<p><strong>Ｆ-measure</strong>  </p>
<p>$$F_{\beta}=\dfrac{(\beta^2+1)PR}{\beta^2P+R}$$</p>
<p>当 $\beta&gt;1$时，Recall的比重更大;当 $\beta&lt;1$时，precision的比重更大。使用最多的是 $\beta=1$,也就是 $F_{\beta=1}, F_1$. $\beta$ 的的取值取决于实际应用。</p>
<p>$$F_1 = \dfrac{2PR}{P+R}$$</p>
<p>Ｆ-measure 是 precision 和 recall 的 **加权调和平均值(weighted harmonic mean)**。</p>
<p>调和平均值是倒数的算术平均值的倒数。</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb12.png"></p>
<p>为什么要使用调和平均值呢？因为它是一个更 <strong>保守的度量(conservative metric)</strong>. 相比直接计算 P 和 R 的平均值， F-measure的值更看重两者中的较小值。</p>
<h2 id="ROC曲线和AUC"><a href="#ROC曲线和AUC" class="headerlink" title="ROC曲线和AUC"></a>ROC曲线和AUC</h2><p>考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。</p>
<ul>
<li><p><strong>TPR, 真正类率(true positive rate ,TPR),：</strong> 如果一个实例是正类并且也被 预测成正类，即为真正类（True positive），真正类率是指分类器所识别出来的 正实例占所有正实例的比例。就是 Recall 吧～  TPR = TP / (TP + FN)  </p>
</li>
<li><p><strong>FPR, 负正类率：</strong> 分类器错认为正类的负实例占所有负实例的比例，FPR = FP / (FP + TN)  </p>
</li>
<li><p><strong>TNR， 真负类率：</strong> 分类器认为负类的负实例占所有负实例的比例，也就是负类的 Recall 吧～ TNR = TN /(FP + TN) = 1 - FPR</p>
</li>
</ul>
<h3 id="为什么要引入-ROC-曲线"><a href="#为什么要引入-ROC-曲线" class="headerlink" title="为什么要引入 ROC 曲线"></a>为什么要引入 ROC 曲线</h3><ul>
<li>Motivation1：在一个二分类模型中，对于所得到的连续结果，假设已确定一个阀值，比如说 0.6，大于这个值的实例划归为正类，小于这个值则划到负类中。如果减小阀值，减到0.5，固然能识别出更多的正类，也就是提高了识别出的正例占所有正例 的比类，即TPR,但同时也将更多的负实例当作了正实例，即提高了FPR。为了形象化这一变化，引入ROC，ROC曲线可以用于评价一个分类器。</li>
</ul>
<ul>
<li>Motivation2：在类不平衡的情况下,如正样本90个,负样本10个,直接把所有样本分类为正样本,得到识别率为90%。但这显然是没有意义的。单纯根据Precision和Recall来衡量算法的优劣已经不能表征这种病态问题。</li>
</ul>
<h3 id="ROC-曲线"><a href="#ROC-曲线" class="headerlink" title="ROC 曲线"></a>ROC 曲线</h3><p>前面已经说道，对于数据不均衡的情况下，precision 和 recall 不足以表征这类问题，就比如上面的例子中，把找出不关于 pie 的 tweet看作是正类，那么它的 精度和召回率 都很高。所以引入 ROC</p>
<p>因为我们要关注的是正类，所以关注指标是 真正类率 TPR 和 负正类率 FPR，真正类率越高越好，负正类率越低越好。但显然这两者之间是矛盾的，与分类器的阈值有关，ROC 就是用来表征阈值与 TPR 和 FPR 之间的关系曲线。</p>
<p>ROC（Receiver Operating Characteristic）翻译为“接受者操作曲线”。曲线由两个变量1-specificity 和 Sensitivity绘制. 1-specificity=FPR，即负正类率。Sensitivity即是真正类率，TPR(True positive rate),反映了正类覆盖程度。</p>
<blockquote>
<p>为了更好地理解ROC曲线，我们使用具体的实例来说明：  </p>
</blockquote>
<p>如在医学诊断中,判断有病的样本。那么尽量把有病的揪出来是主要任务,也就是第一个指标TPR,要越高越好。而把没病的样本误诊为有病的,也就是第二个指标FPR,要越低越好。  </p>
<p>不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感,稍微的小症状都判断为有病,那么他的第一个指标应该会很高,但是第二个指标也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么第一个指标达到1,第二个指标也为1。</p>
<p>我们以FPR为横轴,TPR为纵轴,得到如下ROC空间。  </p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb8.png"></p>
<p>参考链接中 ROC 曲线的解释: <a target="_blank" rel="noopener" href="http://www.cnblogs.com/maybe2030/p/5375175.html#_label0">http://www.cnblogs.com/maybe2030/p/5375175.html#_label0</a></p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb9.png"></p>
<p>上面的图表示，分类器对于 pos 和 neg 的分别是有个阈值的，阈值很高，也就是 A 处，显然它的 TPR 不会很高，阈值越低，TPR 越高。 但是阈值很低的话，预测为正类的负实例也就越多， FPR 也会越高。</p>
<p>曲线距离左上角越近,证明分类器效果越好。我们用一个标量 AUC 来量化这个分类效果。</p>
<h3 id="怎么得到-ROC-曲线"><a href="#怎么得到-ROC-曲线" class="headerlink" title="怎么得到 ROC 曲线"></a>怎么得到 ROC 曲线</h3><p>我们知道对一个二值分类器，其预测出来的正类的 score 是一个概率。当一个样本为正类的概率大于 threshold时，我们判别它为正类。所以不同的 threshold，其对应的 TPR 和 FPR 的值也就不一样，这样就得到了 ROC 曲线。</p>
<p>详细可参考：<a target="_blank" rel="noopener" href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/">ROC和AUC介绍以及如何计算AUC</a>  非常清楚！！！</p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>AUC值为ROC曲线所覆盖的区域面积,显然,AUC越大,分类器分类效果越好。</p>
<ul>
<li><p>AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。  </p>
</li>
<li><p>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。  </p>
</li>
<li><p>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。  </p>
</li>
<li><p>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。  </p>
</li>
</ul>
<p>AUC的物理意义：假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。</p>
<h3 id="怎么计算-AUC"><a href="#怎么计算-AUC" class="headerlink" title="怎么计算 AUC"></a>怎么计算 AUC</h3><p>AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。</p>
<p>代码实现： <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html">sklearn.metrics.roc_auc_score</a></p>
<h3 id="置信度和置信区间"><a href="#置信度和置信区间" class="headerlink" title="置信度和置信区间"></a>置信度和置信区间</h3><p>这里好像跟置信度和置信区间没啥关系。。。但是了解下也没事</p>
<h4 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h4><p>再来理解置信度，首先要理解 95%置信区间 和 置信度。</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26419030">知乎：95%置信区间</a> 我的理解就是，首先总体均值是固定的，但只有上帝知道。我们就要用样本去估计总体均值。一次次抽样会得到很多样本均值，但是我们无法判断哪个均值最好，最接近总体均值。于是，我们构造区间来看这个区间是否含有总体均值。</p>
<p>怎么构造区间：  </p>
<p>通过一次次抽样得到的样本均值：  </p>
<p>$$M=\dfrac{X_1 + X_2+…+X_n}{n}$$</p>
<p>根据大数定律：  </p>
<p>$$M\sim N(\mu,\dfrac{\sigma^2}{n})$$</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb7.jpg"></p>
<p>通过查表 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/yanjunhelloworld/p/4844741.html">标准正太分布表</a>可知，这样可以计算出置信区间，（如果方差为止，则用样本方差代替）</p>
<p>我们以 $1.96\dfrac{\sigma }{\sqrt{n}}$ 为半径做区间，就构造出了 $95%$  置信区间。按这样去构造的100个区间，其中大约会有95个会包含 $\mu$ ：</p>
<p><img src="/2018/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93/nb6.gif"></p>
<p>那么，只有一个问题了，我们不知道、并且永远都不会知道真实的 $\mu$  是多少:</p>
<p>我们就只有用 $\hat{\mu }$ 来代替 $\mu$ ：</p>
<p>$$P(\hat \mu-1.96\dfrac{\sigma}{n}\le M\le\hat \mu+1.96\dfrac{\sigma}{n}) = 0.95$$</p>
<p>这样可以得到置信区间了。如果抽样100次，对应也就有100个置信区间，那么其中含有总体均值的概率约为 95%.</p>
<h4 id="置信度"><a href="#置信度" class="headerlink" title="置信度"></a>置信度</h4><p>样本数目不变的情况下，做一百次试验，有95个置信区间包含了总体真值。置信度为95%</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-03-29T08:15:26.000Z" title="2018/3/29 下午4:15:26">2018-03-29</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.140Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">40 分钟读完 (大约5973个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/">机器学习-生成模型到高斯判别分析再到GMM和EM算法</a></h1><div class="content"><p>生成模型-高斯判别分析GDA- 高斯混合模型GMM- EM算法</p>
<p>在学习生成模型之前，先学习了解下密度估计和高斯混合模型。</p>
<h3 id="生成学习算法-cs229-Ng"><a href="#生成学习算法-cs229-Ng" class="headerlink" title="生成学习算法(cs229,Ng)"></a>生成学习算法(cs229,Ng)</h3><h4 id="生成算法和判别算法的区别"><a href="#生成算法和判别算法的区别" class="headerlink" title="生成算法和判别算法的区别"></a>生成算法和判别算法的区别</h4><p>举个栗子：</p>
<p>我们要区分elephants(y=1)和dogs(y=0)</p>
<ol>
<li>对判别模型（discriminative），以logistic回归为例：</li>
</ol>
<ul>
<li><p>logistic回归模型：$p(y|x;\theta)，h_{\theta}=g(\theta^Tx)$,对应的模型其中g是sigmoid函数。通过logistic回归，我们找到一条决策边界decision boundary，能够区分elephants和dogs.</p>
</li>
<li><p>这个学习的过程就是找到表征这个决策过程的参数 $\theta$.</p>
</li>
</ul>
<ol start="2">
<li>生成模型（generative）：</li>
</ol>
<p>同样的我们也是要通过给定的特征x来判别其对应的类别y。但我们换个思路，就是先求p(x|y),也就是通过y来分析对应x满足的一个概率模型p(x|y)。然后在反过来看特征x，以二分类为例，p(x|y=0)和p(x|y=1)哪个概率大，那么x就属于哪一类。</p>
<ul>
<li><p>模型：p(x|y)，在给定了样本所属的类的条件下，对样本特征建立概率模型。</p>
</li>
<li><p>p(x|y=1)是elephants的分类特征模型</p>
</li>
<li><p>p(x|y=0)是dogs的分类特征模型</p>
</li>
</ul>
<p>然后通过p(x|y)来判断特征x所属的类别，根据贝叶斯公式：</p>
<p>$$p(y=1|x) = \dfrac{p(x|y=1)p(x)}{p(x)}$$</p>
<p>在给定了x的情况下p(x)是个定值，p(y)是先验分布，那么计算方法如下：</p>
<p>$$arg\max_yp(y|x) = arg\max_{y}\dfrac{p(x|y)p(y)}{p(x)}= arg\max_{y}p(x|y)p(y)$$</p>
<p>总结下就是：</p>
<ul>
<li>生成模型：一般是学习一个代表目标的模型，然后通过它去搜索图像区域，然后最小化重构误差。类似于生成模型描述一个目标，然后就是模式匹配了，在图像中找到和这个模型最匹配的区域，就是目标了。</li>
</ul>
<ul>
<li>判别模型：以分类问题为例，然后找到目标和背景的决策边界。它不管目标是怎么描述的，那只要知道目标和背景的差别在哪，然后你给一个图像，它看它处于边界的那一边，就归为哪一类。</li>
</ul>
<ul>
<li> 由生成模型可以得到判别模型，但由判别模型得不到生成模型。</li>
</ul>
<p>然鹅，生成模型p(x|y)怎么得到呢？不慌，我们先了解下多维正态分布～</p>
<h4 id="多维正态分布-the-multivariate-nirmal-distribution"><a href="#多维正态分布-the-multivariate-nirmal-distribution" class="headerlink" title="多维正态分布(the multivariate nirmal distribution)"></a>多维正态分布(the multivariate nirmal distribution)</h4><p><img src="http://img.blog.csdn.net/20180316155009910?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
<p>关于一维正态分布怎么推导出多维正态分布的概率密度函数，可参考知乎:<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/36339816">多维高斯分布是如何由一维发展而来的？</a></p>
<p>首先一维正态分布:</p>
<p>$p(x) = \dfrac{1}{\sqrt{2\pi}}exp(\dfrac{-x^2}{2})$</p>
<p>二维标准正态分布，就是两个独立的一维标准正态分布随机变量的联合分布：</p>
<p>$p(x,y) = p(x)p(y)=\dfrac{1}{2\pi}exp(-\dfrac{x^2+y^2}{2})$</p>
<p>把两个随机变量组合成一个随机向量：$v=[x\quad y]^T$</p>
<p>$p(v)=\dfrac{1}{2\pi}exp(-\dfrac{1}{2}v^Tv)\quad$ 显然x,y相互独立的话，就是上面的二维标准正态分布公式～</p>
<p>然后从标准正态分布推广到一般正态分布，通过一个线性变化：$v=A(x-\mu)$</p>
<p>$p(x)=\dfrac{|A|}{2\pi}exp[-\dfrac{1}{2}(x-\mu)^TA^TA(x-\mu)]$</p>
<p>注意前面的系数多了一个|A|（A的行列式）。</p>
<p>可以证明这个分布的均值为$\mu$，协方差为$(A^TA)^{-1}$。记$\Sigma = (A^TA)^{-1}$，那就有</p>
<p>$$p(\mathbf{x}) = \frac{1}{2\pi|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \mu) ^T \Sigma^{-1} (\mathbf{x} - \mu) \right]$$</p>
<p>推广到n维：</p>
<p>$$p(\mathbf{x}) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \mu) ^T \Sigma^{-1} (\mathbf{x} - \mu) \right]$$</p>
<blockquote>
<p>需要注意的是：这里的二维、n维到底指的是什么？</p>
</blockquote>
<ul>
<li><p>以飞机检测的数据点为例，假设它由heat和time决定，那么这就是个二维正态分布，数据点的生成所处的位置由其概率决定，也就是$p(\mathbf{x})$</p>
</li>
<li><p>如果这个数据有n个特征，那么其分布就是n维正态分布。</p>
</li>
<li><p>之前一直理解的是，n维正态分布是两个向量巴拉巴拉。。好像一直没搞懂。。</p>
</li>
</ul>
<p>再顺便了解下协方差矩阵吧～</p>
<h4 id="关于协方差矩阵，参考blog"><a href="#关于协方差矩阵，参考blog" class="headerlink" title="关于协方差矩阵，参考blog"></a>关于协方差矩阵，<a target="_blank" rel="noopener" href="http://blog.csdn.net/zhengjihao/article/details/78030918">参考blog</a></h4><p>对多维随机变量$X=[X_1,X_2,…,X_n]^T$，我们往往需要计算各维度之间的协方差，这样协方差就组成了一个n×n的矩阵，称为协方差矩阵。协方差矩阵是一个对角矩阵，对角线上的元素是各维度上随机变量的方差,非对角线元素是维度之间的协方差。 我们定义协方差为$\Sigma$, 矩阵内的元素$\Sigma_{ij}$为:</p>
<p>$$\Sigma_{ij} = cov(X_i,X_j) = E[(X_i - E(X_i)) (X_j - E(X_j))]$$</p>
<p>则协方差矩阵为:</p>
<p>$$\Sigma = E[(X-E(X)) (X-E(X))^T]$$</p>
<p>$$= \left[</p>
<p>\begin{array}{cccc}</p>
<p>cov(X_1,X_1) &amp; cov(X_1,X_2) &amp; \cdots &amp; cov(X_1,X_n) \</p>
<p>cov(X_2,X_1) &amp; cov(X_2,X_2) &amp; \cdots &amp;cov(X_2,X_n) \</p>
<p>\vdots &amp; \vdots&amp; \vdots &amp; \vdots \</p>
<p>cov(X_n,X_1) &amp; cov(X_n,X_2,)&amp;\cdots&amp; cov(X_n,X_n)</p>
<p>\end{array}</p>
<p>\right]</p>
<p>$$</p>
<p>如果X~$N(\mu,\Sigma)$,则$Cov(X)=\Sigma$</p>
<p>可以这么理解协方差，对于n维随机变量X，第一维是体重$X_1$，第二维是颜值$X_2$，显然这两个维度是有一定联系的，就用$cov(X_1,X_2)$来表征，这个值越小，代表他们越相似。协方差怎么求，假设有m个样本，那么所有的样本的第一维就构成$X_1$…不要把$X_1$和样本搞混淆了。</p>
<p>了解了多维正态分布和协方差，我们再回到生成模型p(x|y)。。其实我们就是假设对于n维特征，p(x|y)是n维正态分布～怎么理解呢，下面就说！</p>
<h4 id="高斯判别分析模型The-Gaussian-Discriminant-Analysis-model"><a href="#高斯判别分析模型The-Gaussian-Discriminant-Analysis-model" class="headerlink" title="高斯判别分析模型The Gaussian Discriminant Analysis model"></a>高斯判别分析模型The Gaussian Discriminant Analysis model</h4><p>高斯判别模型就是：假设p(x|y)是一个多维正态分布，为什么可以这么假设呢？因为对于给定y的条件下对应的特征x都是用来描述这一类y的，比如特征是n维的，第一维描述身高，一般都是满足正态分布的吧，第二维描述体重，也可认为是正态分布吧～</p>
<p>则生成模型：</p>
<p>y ~ Bernoulli($\phi)$ 伯努利分布，又称两点分布，0-1分布</p>
<p>x|y=0 ~ $N(u_0,\Sigma)$</p>
<p>x|y=1 ~ $N(u_1,\Sigma)$</p>
<ul>
<li>这里可以看作是一个二分类，y=0和y=1,可以看作是伯努利分布，则$p(y)=\phi^y(1-\phi)^{1-y}$，要学的参数之一: $\phi=p(y=1)$，试想如果是多分类呢，那么要学习的参数就有$\phi_1,\phi_2,….\phi_k$</li>
</ul>
<ul>
<li>其中类别对应的特征x|y=0,x|y=1服从正态分布。怎么理解呢？就是既然你们都是一类人，那么你们的身高啊，体重啊等等应该满足正态分布。。有几维特征就满足几维正态分布</li>
</ul>
<ul>
<li>这里x是n维特征，身高，体重，颜值…balabala，所以x|y=0满足n维正态分布～x|y=1也是啦，只不过对于不同的类，对应n维特征的均值不一样，奇怪为什么协方差矩阵是一样的？？这里是将它特殊化了，后面会讲的一般性的em算法就不是这样的了</li>
</ul>
<ul>
<li>每个分类对应的n维特征的分布显然不是独立的，比如体重和颜值还是有关系的吧～他们的协方差，方差就统统都在$\Sigma$协方差矩阵里面了</li>
</ul>
<p>$$p(x|y=0) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_0) ^T \Sigma^{-1} (x - \mu_0) \right]$$</p>
<p>$$p(x|y=1) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_1) ^T \Sigma^{-1} (x - \mu_1) \right]$$</p>
<p>这样，模型中我们要学习的参数有$\phi,\Sigma, \mu_0,\mu_1$，对于训练数据，就是观测到的数据x,y，既然他们出现了，那么他们的联合概率，也就是似然函数$\prod_{i=1}^mp(x,y)$就要最大～其对数似然log-likelihood：</p>
<p>$$\begin{equation}\begin{aligned}</p>
<p>L(\phi,\Sigma, \mu_0,\mu_1) &amp;= log\prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,\Sigma, \mu_0,\mu_1$) \</p>
<p>  &amp;= log\prod_{i=1}^mp(x^{(i)}|y^{(i)};\phi,\Sigma, \mu_0,\mu_1$) p(y^{(i)};\phi)\</p>
<p>\end{aligned}\end{equation}\label{eq2}$$</p>
<p>其中$p(y^{(i)};\phi)$是已知的，也就是先验概率(class priors)，$p(x^{(i)}|y^{(i)})$就是上面推导的～代入后，分别对参数求导即可：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/GDA.png"></p>
<p>在回过头来看这些公式，</p>
<ul>
<li><p>$\phi$很好理解，就是样本中正分类的概率。</p>
</li>
<li><p>$\mu_0$就是负分类中x对应的均值</p>
</li>
<li><p>$\mu_1$就是正分类中x对应的均值</p>
</li>
<li><p>$\Sigma$就是$(x-\mu_1)$和$x-\mu_2$的协方差矩阵</p>
</li>
</ul>
<p>然后通过p(x|y=0),p(x|y=1)即可对需要预测的x求出对应的概率，然后做出判别了。这样看来，如果直接对x|y=1,和x|y=0做出了正态分布的猜测，就可以直接写出来了。只不过，我们用极大似然估计重新推导了一遍。</p>
<h3 id="高斯混合模型GMM"><a href="#高斯混合模型GMM" class="headerlink" title="高斯混合模型GMM"></a>高斯混合模型GMM</h3><h4 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h4><p>前面GDA是有标签的，也算是有监督学习。而在没有标签的情况下呢，就是无监督学习了，虽然我们无法给出x所属的类叫啥，但是我们可以判断出哪些x是同一类，以及样本中总共有多少类（虽然这个类数嘛。。类似于k-means的类数，可根据交叉验证选择）。</p>
<p>其实和GDA非常相似，不过这里没有了类标签，只有一堆样本特征，${x^{(1)},x^{(2)},…,x^{(m)}}$,</p>
<p>我们不知道这些样本属于几个类别，也不知道有哪些类了。但虽然不知道，我们确定他们是存在的，只是看不见而已。我们可以假设存在k类，${z^{(1)},z^{(2)},…,z^{(k)}}$,看不见的，我们就叫它们隐藏随机变量(latent random variable)，</p>
<p>这样一来，就训练样本就可以用这样的联合分概率模型表示了，$p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})$</p>
<ul>
<li>同GDA不一样的是，这里是多分类，可假定$z^{(i)}\sim Multinomial(\phi)$，多项式分布（二项分布的拓展～），那么$p(z^{(i)})=\phi_j$</li>
</ul>
<ul>
<li>同GDA相同的是，对于每一个类别，其对应的样本满足n维正态分布，也就是：$x^{(i)}|z^{(i)}=j\sim N(\mu_j,\Sigma_j)$,但注意哦，这里每个高斯分布使用了不同的协方差矩阵$\Sigma_j$</li>
</ul>
<p>$$p(x|z^{(1)}) = \frac{1}{(2\pi)^{n/2}|\Sigma_0|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_0) ^T \Sigma_0^{-1} (x - \mu_0) \right]$$</p>
<p>$$p(x|z^{(2)}) = \frac{1}{(2\pi)^{n/2}|\Sigma_1|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_1) ^T \Sigma_1^{-1} (x - \mu_1) \right]$$</p>
<p>$$….$$</p>
<p>$$p(x|z^{(k)}) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_k) ^T \Sigma_k^{-1} (x - \mu_k) \right]$$</p>
<p>然后带入到训练样本的对数似然（log-likelihood）:</p>
<p>$$L(\phi,\mu,\Sigma)=\sum_{i=1}^{m}logp(x^{(i)};\phi,\mu,\Sigma)$$</p>
<p>$$L(\phi,\mu,\Sigma)=\sum_{i=1}^{m}log\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)};\mu,\Sigma) p(z^{(i)};\phi)\$$</p>
<p>这里需要注意下标：对于类别有k类，第一个求和符号是对第i个样本在k个类别上的联合概率，第二个求和符号是m个样本的联合概率。</p>
<p>我们可以注意到，如果我们知道$z^{(i)}$,那么这个似然函数求极大值就很容易了，类似于高斯判别分析，这里的$z^{(i)}$相当于标签，分别对参数求导可得：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/GMM2.png"></p>
<p>其中的参数:</p>
<ul>
<li>$1{z^{(i)}=j}$表示第i个样本为j类时，这个值就为１，那么$\phi_j=\frac{1}{m}\sum_{i=1}^m1{z^{(i)}=j}$表示样本中类别为j的概率</li>
</ul>
<ul>
<li>其中$p(z^{(i)};\phi)$是根据伯努利分布得到的，在GDA中$p(y|\phi)$是已知的频率概率。</li>
</ul>
<p>So $z^{(i)}$ 到底有多少个分类？每个类别的概率是多少？譬如上式中 $\sum_{i=1}^{m}1{z^{(i)}=j}$ 这个没法求对吧～它是隐藏变量！所以还是按照这个方法是求不出来的～</p>
<p>这个时候EM算法就登场了～～～</p>
<h4 id="用EM算法求解GMM模型"><a href="#用EM算法求解GMM模型" class="headerlink" title="用EM算法求解GMM模型"></a>用EM算法求解GMM模型</h4><p>上面也提到了，如果$z^({i})$是已知的话，那么$\phi_j=\frac{1}{m}\sum_{i=1}^m1{z^{(i)}=j}$表示类别j的概率$p(z^{(i)}=j)$也就已知了，但是呢？我们不知道。。所以我们要猜测$p(z^{(i)}=j)$这个值，也就是EM算法的第一步：</p>
<p><strong>Repeat until convergence 迭代直到收敛:{</strong></p>
<p>(E-step):for each i,j,set:</p>
<p>$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)$</p>
<p>$w_j^{(i)}$什么意思呢?就是对于i样本,它是j类的后验概率。在GDA里面，x_i的类别是确定的，在GMM里面呢？不知道它的类别，所以只能假设k类都有可能，它是j类别的概率就是$w_j^{(i)}$，它仅仅取决于$\phi_j$,而在GMM里面，它取决于$\phi_j,\mu_j,\Sigma_j$，实际上$w_j^{(i)}$的值，就包含了两个我们在GMM所做的假设，多项式分布和正态分布。</p>
<blockquote>
<p>The values $w_j$ calculated in the E-step represent our “soft” guesses for</p>
</blockquote>
<p>the values of $z^{(i)}$ .</p>
<p>The term “soft” refers to our guesses being probabilities and taking values in [0, 1]; in</p>
<p>contrast, a “hard” guess is one that represents a single best guess (such as taking values</p>
<p>in {0, 1} or {1, . . . , k}).</p>
<p>硬猜测是k均值聚类，GMM是软猜测。</p>
<p>这样一来，参数更新就可以这样写了，也就是EM算法的第二步：</p>
<p>(M-step) Updata the parameters:</p>
<p>然后对似然函数求导，后面会详细介绍</p>
<p>$$\phi_j:=\frac{1}{m}\sum_{i=1}^mw_j^{(i)}$$</p>
<p>$$\mu_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<p>$$\Sigma_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<p><strong>｝</strong></p>
<p>训练过程的理解可参考<a target="_blank" rel="noopener" href="http://blog.pluskid.org/?p=39">blog</a></p>
<p><strong>$w_j^{(i)}表示第i个样本为ｊ类别的概率，而\phi_j$表示m个样本中j类别的概率，$\mu_j,\Sigma_j$分别表示j类别对应的n维高斯分布的期望和协方差矩阵</strong></p>
<p>所以，求出$w_j^{(i)}$，一切就都解决了吧？对于后验概率$p(z^{(i)}=j|x^{(i)})$可以根据Bayes公式：</p>
<p>$$p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\dfrac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$</p>
<ul>
<li>其中先验概率$p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$可以根据高斯分布的密度函数来求，$z^{(i)}=j\sim N(\mu_j,\Sigma_j)$</li>
</ul>
<ul>
<li>类先验(class priors)$p(z^{(i)}=j;\phi)$可以取决于多项式分布中j类的概率$\phi_j$</li>
</ul>
<blockquote>
<p>The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments $c^(i)$, we instead have the “soft” assignments $w_j$ . Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea.  </p>
</blockquote>
<p>EM算法使我们联想起了k-means,区别在于k-means的聚类是通过欧氏距离c(i)来定义的，而EM是通过$w_j$probabilities来分类的。同k-means一样，这里的EM算法也是局部优化，因此最好采用不同的方式初始化～</p>
<h4 id="convergence"><a href="#convergence" class="headerlink" title="convergence?"></a>convergence?</h4><p>我们知道k-means一定是收敛的，虽然结果不一定是全局最优解，但它总能达到一个最优解。但是EM算法呢，也是收敛的。</p>
<h3 id="The-EM-algorithm"><a href="#The-EM-algorithm" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h3><p>前面我们讲的是基于高斯混合模型的EM算法，但一定所有的类别都是高斯分布吗？还有卡方分布，泊松分布等等呢，接下来我们就将讨论EM算法的一般性。</p>
<p>在学习一般性的EM算法前，先了解下Jensen’s inequality</p>
<h4 id="Jensen’s-inequality"><a href="#Jensen’s-inequality" class="headerlink" title="Jensen’s inequality"></a>Jensen’s inequality</h4><p>如果函数$f$，其二阶导恒大与等于0 $(f^{‘’}\ge 0)$，则它是凸函数f(convec function)。</p>
<p>如果凸函数的输入是向量vector-valued inputs，那么它的海森矩阵(hessian)H是半正定的。Jensen’s 不等式：</p>
<blockquote>
<p>Let f be a convex function, and let X be a random variable.</p>
</blockquote>
<p>Then:</p>
<p>$$E[f (X)] ≥ f (EX).$$</p>
<p>Moreover, if f is strictly convex, then $E[f (X)] = f (EX)$ holds true if and</p>
<p>only if $X = E[X]$ with probability 1 (i.e., if X is a constant).</p>
<p>举个栗子来解释jensen不等式：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/jensen.png"></p>
<p>假设输入随机变量X是一维的哈，然后Ｘ取a,b的概率都是0.5,那么</p>
<p>$$EX=(a+b)/2,f(EX)=f(\dfrac{a+b}{2})$$,$$E[f(X)]=\dfrac{f(a)+f(b)}{2}$$</p>
<p>因为是凸函数，所以 $f(EX)\le E[f(X)]$</p>
<p>同理，如果是凹函数(concave function),那么不等式方向相反$f(EX)\ge E[f(X)]$。后面EM算法里面就要用到log(X)，log(x)就是个典型的凹函数～</p>
<h4 id="The-EM-algorithm-1"><a href="#The-EM-algorithm-1" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h4><p>首先，问题是：我们要基于给定的m个训练样本${x^{(1)},x^{(2)},…,x^{(m)}}$来进行密度估计～</p>
<p>像前面一样，创建一个参数模型p(x,z)来最大化训练样本的对数似然：</p>
<p>$$L(\theta)=\sum_{i=1}^mlogp(x;\theta)$$</p>
<p>$$L(\theta)=\sum_{i=1}^mlog\sum_zp(x,z;\theta)$$</p>
<p>一般性就是把前面特殊化的假设去掉，没有了正态分布和多项式分布。</p>
<p>可以看到，$z^{(i)}$是隐藏的随机变量(latent random variable),关于参数$\theta$的最大似然估计就很难计算了。</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em2.png"></p>
<p>解释下公式中的推导：</p>
<ul>
<li>这里是针对样本i来说，对于样本i，它可能是$z^1,z^2,…,z^k$都有可能，但他们的probability之和为１，也就是</li>
</ul>
<p>$\sum_zQ_i(z)=1$</p>
<ul>
<li>(2)到(3)的推导：可以将</li>
</ul>
<p>$\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$</p>
<p>看做随机变量Ｘ,那么（２）式中的后半部分 $log\sum_{z^{(i)})}[\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]$就是log(EX)了，$logx$是一个凹函数，则其大于$E[log(x)]$</p>
<p><font size="4" color="#D2691E">EM迭代过程(重点):</font></p>
<ul>
<li><p>（1）根据上式可以看做$L(\theta)\ge J(Q,\theta)$.两边都是关于$\theta$的函数，那么将$\theta$固定，调整Q在一定条件下能使等式成立。</p>
</li>
<li><p>（2）然后固定Q,调整$\theta^t$到$\theta^{t+1}$找到下界函数的最大值$J(Q,\theta^{t+1})$.显然在当前Q的条件下，$L(\theta^{t+1})\ne J(Q,\theta^{t+1})$,那么根据Jensen不等式，$L(\theta_{t+1})&gt;J(Q,\theta^{t+1})=L(\theta^{t})$,也就是说找到了使得对数似然L更大的$\theta$.这不就是我们的目的吗？！</p>
</li>
<li><p>然后迭代循环(1)(2)步骤，直到在调整$\theta$时，下界函数$J(Q,\theta)$不在增加，即小于某个阈值。</p>
</li>
</ul>
<p>看下Ng画的图：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em1.png" alt="em1.png"></p>
<p>任意初始化$\theta$和Q,然后找下界函数和$l(\theta)$交接的点，这就是EM算法的第一步：</p>
<p>我们要让不等式相等,即Jensen’s inequality中的随机变量取值是一个常量，看(2)式：</p>
<p>$$\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c$$</p>
<p>对左边分子分母同时对z求类和：</p>
<p>$$\dfrac{\sum_zp(x^{(i)},z^{(i)};\theta)}{\sum_zQ_i(z^{(i)})}=c$$</p>
<p>根据$\sum_zQ_i(z)=1$：</p>
<p>$$\sum_zp(x^{(i)},z^{(i)};\theta)=c$$</p>
<p>带回去可得：</p>
<p>$$Q_i(z^{(i)})=\dfrac{p(x^{(i)},z^{(i)};\theta)}{\sum_zp(x^{(i)},z^{(i)};\theta)}$$</p>
<p>$$Q_i(z^{(i)})=\dfrac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}$$</p>
<p>$$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta)$$</p>
<p>EM总结下来：</p>
<p>Repeat until convergence {</p>
<p>(E-step)</p>
<p>For each i,找到下界函数, set:</p>
<p>$$Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)$$</p>
<p>(M-step)找到下界凹函数的最大值,也就是(3)式 Set:</p>
<p>$$\theta:=arg\max_{\theta}\sum_i^m\sum_{z^{(i)}}^kQ_i(z^{(i)})log\dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$</p>
<p>}</p>
<p><font size="4" color="#D2691E">要理解的是：</font></p>
<p>EM算法只是一种计算方式，对于上式中的$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta)$我们还是要根据假设来求得，比如GMM中的多类高斯分布。然后带回到对数似然中，通过求导得到参数估计。我们费尽心机证明EM算法收敛，只是为了去证明这样去求似然函数的极大值是可行的，然后应用到类似于GMM，HMM中。</p>
<h4 id="training-and-will-converge"><a href="#training-and-will-converge" class="headerlink" title="training and will converge?"></a>training and will converge?</h4><p>首先说是否收敛，答案是肯定收敛的。。懒得输公式了。。直接贴图吧，这个比较好理解：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em3.png" alt="em3.png"></p>
<p>上面写这么多，其实就是证明$L(\theta_{t+1})&gt;L(\theta_t)$.</p>
<h4 id="Mixture-of-Gaussians-revisited"><a href="#Mixture-of-Gaussians-revisited" class="headerlink" title="Mixture of Gaussians revisited"></a>Mixture of Gaussians revisited</h4><p>我们知道了em算法是一种计算方式，用来解决含有隐变量似然对数很难求的问题，那么我们把它运用到GMM中。</p>
<p><font size="4" color="#D2691E">E step:</font></p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em11.png"></p>
<p>$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)$</p>
<p>$$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\dfrac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em13.png"></p>
<ul>
<li><p>其中先验概率$p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$可以根据高斯分布的密度函数来求，$z^{(i)}=j\sim N(\mu_j,\Sigma_j)$</p>
</li>
<li><p>类先验(class priors)$p(z^{(i)}=j;\phi)$可以取决于多项式分布中j类的概率$\phi_j$</p>
</li>
</ul>
<p>这样我们就完成了对$w_j^{(i)}$的soft ‘guess’，也就是E step.</p>
<p><font size="4" color="#D2691E">M step:</font></p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em12.png"></p>
<p>然后对参数求导：</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em14.png"></p>
<p>详细推导过程，参考cs229-notes8</p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E5%86%8D%E5%88%B0GMM%E5%92%8CEM%E7%AE%97%E6%B3%95/em15.jpeg"></p>
<p>我们在整体回顾一下整个过程，所谓的E step就是找到$Q_i(z^{j}),w_i^j$（在一定假设下是可以通过bayes公式求得的），使得下界函数与log函数相等，也就是Jensen取等号时。然后是M step就是在Q的条件下找到下界函数最大值，也就是对参数求导，导数为0的地方。</p>
<p>然后在根据求得的参数，再求Q，再带入求导。。。迭代直到收敛。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-03-29T08:15:26.000Z" title="2018/3/29 下午4:15:26">2018-03-29</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.200Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">29 分钟读完 (大约4421个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/">代码实现高斯混合模型</a></h1><div class="content"><p>sklearn源码阅读，用em算法计算高斯混合模型GMM</p>
<h3 id="代码实现高斯混合模型"><a href="#代码实现高斯混合模型" class="headerlink" title="代码实现高斯混合模型"></a>代码实现高斯混合模型</h3><p>参考这篇博客<a target="_blank" rel="noopener" href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/">Regularized Gaussian Covariance Estimation</a>非常值得一读，同事这篇博客很深入的讲了协方差怎么求的问题，在前文中我也有提到～但我解释的很low。。</p>
<p>代码直接就看sklearn里面的源码吧～网上很多不靠谱。。。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/mixture/gaussian_mixture.py">github源码</a></p>
<h3 id="类初始化"><a href="#类初始化" class="headerlink" title="类初始化"></a>类初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GaussianMixture</span>(<span class="params">BaseMixture</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Representation of a Gaussian mixture model probability distribution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This class allows to estimate the parameters of a Gaussian mixture</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  distribution. 对混合的高斯分布进行参数估计～</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_components=<span class="number">1</span>, covariance_type=<span class="string">&#x27;full&#x27;</span>, tol=<span class="number">1e-3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             reg_covar=<span class="number">1e-6</span>, max_iter=<span class="number">100</span>, n_init=<span class="number">1</span>, init_params=<span class="string">&#x27;kmeans&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             weights_init=<span class="literal">None</span>, means_init=<span class="literal">None</span>, precisions_init=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             random_state=<span class="literal">None</span>, warm_start=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">             verbose=<span class="number">0</span>, verbose_interval=<span class="number">10</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(GaussianMixture, self).__init__(</span><br><span class="line"></span><br><span class="line">        n_components=n_components, tol=tol, reg_covar=reg_covar,</span><br><span class="line"></span><br><span class="line">        max_iter=max_iter, n_init=n_init, init_params=init_params,</span><br><span class="line"></span><br><span class="line">        random_state=random_state, warm_start=warm_start,</span><br><span class="line"></span><br><span class="line">        verbose=verbose, verbose_interval=verbose_interval)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主要是针对3个要学习的参数的初始化</span></span><br><span class="line"></span><br><span class="line">    self.covariance_type = covariance_type <span class="comment"># 协方差矩阵形式</span></span><br><span class="line"></span><br><span class="line">    self.weights_init = weights_init <span class="comment"># 多项式分布，每一类的概率</span></span><br><span class="line"></span><br><span class="line">    self.means_init = means_init  <span class="comment"># 均值 (n_components, n_features)</span></span><br><span class="line"></span><br><span class="line">    self.precisions_init = precisions_init <span class="comment"># 协方差</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>先看初始化构造函数，参数是真的多。。。</p>
<ul>
<li><p><strong>n_components=1:</strong> The number of mixture components.表示混合类别的个数，也就是混合高斯分布的个数</p>
</li>
<li><p><strong>covariance_type=’full’:</strong> 协方差矩阵的类型。{‘full’, ‘tied’, ‘diag’, ‘spherical’} 分别对应完全协方差矩阵（元素都不为零），相同的完全协方差矩阵（HMM会用到），对角协方差矩阵（非对角为零，对角不为零），球面协方差矩阵（非对角为零，对角完全相同，球面特性），默认‘full’ 完全协方差矩阵</p>
</li>
<li><p><strong>tol=1e-3</strong> 收敛阈值，EM iterations will stop when the lower bound average gain is</p>
</li>
</ul>
<p>below this threshold.也就是当下界的平均增益小于阈值时，em迭代就停止。这里的下界指的是公式</p>
<p>（3）中的下界凸函数。我们知道em算法分两步，e step是期望，也就是不等式相等，m setp是最大化，</p>
<p>也就是下界凸函数最大化。这里的阈值平均增益就是指凸函数的最大化过程中的增益。</p>
<ul>
<li><strong>reg_covar=1e-6：</strong> Non-negative regularization added to the diagonal of</li>
</ul>
<p>covariance.Allows to assure that the covariance matrices are all positive.</p>
<p>非负正则化添加到协方差矩阵对角线上，保证协方差矩阵都是正定的。</p>
<ul>
<li><p><strong>max_iter=100:</strong> em算法的最大迭代次数</p>
</li>
<li><p><strong>n_init:</strong> int, defaults to 1.初始化的次数</p>
</li>
<li><p><strong>init_params:</strong> {‘kmeans’, ‘random’}, defaults to ‘kmeans’.</p>
</li>
</ul>
<p>The method used to initialize the weights, the means and the precisionsself.</p>
<p> Must be one of::</p>
<pre><code>-  &#39;kmeans&#39; : responsibilities are initialized using kmeans.

- &#39;random&#39; : responsibilities are initialized randomly.

- 这里对应的初始化，是指的隐藏变量z的分类所占比例，也就是weight_init，kmeans表示“hard”guess， &#123;0, 1&#125; or &#123;1, . . . , k&#125;)
</code></pre>
<p>  random应该就是”soft”guess吧。</p>
<ul>
<li>weights_init : shape (n_components, ), optional The user-provided initial weights, defaults to None. If it None, weights are initialized using the <code>init_params</code> method.</li>
</ul>
<p>  先验权重初始化，对应的就是隐藏变量有n_components类，而每一类所占的比例，也就是多项式分布的初始化～对应$\phi_i$</p>
<ul>
<li><strong>means_init :</strong> array-like, shape (n_components, n_features), optional. The user-provided initial means, defaults to None, If it None, means are initialized using the <code>init_params</code> method.混合高斯分布的均值初始化，注意shape=(n_components, n_features),有n_components这样的多维高斯分布，每个高斯分布有n_features维度</li>
</ul>
<ul>
<li><p><strong>precisions_init :</strong> The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the ‘init_params’ method.The shape depends on ‘covariance_type’::</p>
<ul>
<li><p>(n_components,)                        if ‘spherical’,</p>
</li>
<li><p>(n_features, n_features)               if ‘tied’,</p>
</li>
<li><p>(n_components, n_features)             if ‘diag’,</p>
</li>
<li><p>(n_components, n_features, n_features) if ‘full’</p>
</li>
<li><p>用来初始化高斯分布中的协方差矩阵，协方差矩阵代表的是n_features维向量中每一维特征与其他维度特征的关系，对于一个高斯分布来说是n_features<em>n_features，n_components个混合也就是’full’。其中要学习的参数个数是(n_features+1)</em> n_features/2.具体关于协方差矩阵参考前面那篇博客</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>random_state :</strong> int, RandomState instance or None, optional (default=None) 随机数生成器</li>
</ul>
<ul>
<li><strong>warm_start :</strong> bool, default to False.If ‘warm_start’ is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. 若为True，则fit（）调用会以上一次fit（）的结果作为初始化参数，适合相同问题多次fit的情况，能加速收敛，默认为False。</li>
</ul>
<ul>
<li><strong>verbose :</strong> int, default to 0. Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step. 使能迭代信息显示，默认为0，可以为1或者大于1（显示的信息不同）</li>
</ul>
<ul>
<li><strong>verbose_interval:</strong> 与13挂钩，若使能迭代信息显示，设置多少次迭代后显示信息，默认10次。</li>
</ul>
<h3 id="E-step"><a href="#E-step" class="headerlink" title="E step"></a>E step</h3><p>就是求$w_j^i$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_e_step</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;E step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_prob_norm :</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_responsibility : 后验概率，样本i是j类的概率w_j^&#123;i&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    log_prob_norm, log_resp = self._estimate_log_prob_resp(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.mean(log_prob_norm), log_resp</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>那么如何求$w_j^{i}$呢？</p>
<p>$$w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\dfrac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$</p>
<p><font size="4" color="#D2691E">要注意的是，为了计算方便，有以下几点：</font></p>
<ul>
<li><p>因为分子分母中设计到正态分布，即指数形式，故先计算其log形式。然后带入到M step中取回指数形式即可。</p>
</li>
<li><p>对于协方差矩阵，如果n_features很大的话，计算其逆矩阵和行列式就很复杂，因此可以先计算其precision矩阵，然后进行cholesky分解，以便优化计算。</p>
</li>
</ul>
<h4 id="先计算分子对数形式，两个对数相加："><a href="#先计算分子对数形式，两个对数相加：" class="headerlink" title="先计算分子对数形式，两个对数相加："></a>先计算分子对数形式，两个对数相加：</h4><p>$$logp(x^{(i)}|z^{(i)}=j;\mu,\Sigma)+logp(z^{(i)}=j;\phi)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 计算P(x|z)p(z)的对数形式</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_weighted_log_prob</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Estimate the weighted log-probabilities, log P(X | Z) + log weights.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        weighted_log_prob : array, shape (n_samples, n_component)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._estimate_log_prob(X) + self._estimate_log_weights()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>其中前者是高斯分布概率的对数,根据均值，协方差矩阵的cholesky分解可求得。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_prob</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> _estimate_log_gaussian_prob(</span><br><span class="line"></span><br><span class="line">            X, self.means_, self.precisions_cholesky_, self.covariance_type)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个函数，<code>_estimate_log_gaussian_prob</code>根据高斯分布的参数计算概率，涉及到协方差矩阵，要优化计算，很复杂，放在最后说。先把整个流程走完。</p>
<ol start="2">
<li>后者是每一类高斯分布所占的权重，也就是$\phi_j$</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_weights</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 刚开始是初始值，后面随着m step而更新</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.log(self.weights_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="再计算-w-j-i"><a href="#再计算-w-j-i" class="headerlink" title="再计算$w_j^i$"></a>再计算$w_j^i$</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_prob_resp</span>(<span class="params">self, X</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate log probabilities and responsibilities for each sample.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Compute the log probabilities, weighted log probabilities per</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    component and responsibilities for each sample in X with respect to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    the current state of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_prob_norm : array, shape (n_samples,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        log p(X)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_responsibilities : array, shape (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        logarithm of the responsibilities</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算分子log P(X | Z) + log weights.</span></span><br><span class="line"></span><br><span class="line">    weighted_log_prob = self._estimate_weighted_log_prob(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算分母log P(x)</span></span><br><span class="line"></span><br><span class="line">    log_prob_norm = logsumexp(weighted_log_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> np.errstate(under=<span class="string">&#x27;ignore&#x27;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 忽略下溢，计算log(w_J^j)，也就是两个对数相减</span></span><br><span class="line"></span><br><span class="line">        log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> log_prob_norm, log_resp</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="M-setp"><a href="#M-setp" class="headerlink" title="M setp"></a>M setp</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_m_step</span>(<span class="params">self, X, log_resp</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;M step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_resp : array-like, shape (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Logarithm of the posterior probabilities (or responsibilities) of</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the point of each sample in X.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_samples, _ = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据E step中求得的log_resp,更新权重，均值和协方差</span></span><br><span class="line"></span><br><span class="line">    self.weights_, self.means_, self.covariances_ = (</span><br><span class="line"></span><br><span class="line">        _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,</span><br><span class="line"></span><br><span class="line">                                      self.covariance_type))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新类别权重phi_j</span></span><br><span class="line"></span><br><span class="line">    self.weights_ /= n_samples</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新协方差矩阵的精度矩阵</span></span><br><span class="line"></span><br><span class="line">    self.precisions_cholesky_ = _compute_precision_cholesky(</span><br><span class="line"></span><br><span class="line">        self.covariances_, self.covariance_type)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>具体怎么求，就是根据前面推导的公式了。根据前面的公式分别求对应的估计参数：</p>
<p>$$\Sigma_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<h4 id="协方差矩阵：以‘full’为例"><a href="#协方差矩阵：以‘full’为例" class="headerlink" title="协方差矩阵：以‘full’为例"></a>协方差矩阵：以‘full’为例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_gaussian_covariances_full</span>(<span class="params">resp, X, nk, means, reg_covar</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate the full covariance matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    resp:表示E step中猜测的w_j^&#123;i&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_components, n_features = means.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 协方差矩阵</span></span><br><span class="line"></span><br><span class="line">    covariances = np.empty((n_components, n_features, n_features))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_components):</span><br><span class="line"></span><br><span class="line">        diff = X - means[k]</span><br><span class="line"></span><br><span class="line">        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正则化，flat表示展开成一维，然后每隔n_features取一个元素，单个协方差矩阵shape是</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># [n_features, n_features],所以就是对角线元素加上reg_covar</span></span><br><span class="line"></span><br><span class="line">        covariances[k].flat[::n_features + <span class="number">1</span>] += reg_covar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> covariances</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="然后是正态分布的参数估计-u-j-phi-j"><a href="#然后是正态分布的参数估计-u-j-phi-j" class="headerlink" title="然后是正态分布的参数估计$u_j, \phi_j$"></a>然后是正态分布的参数估计$u_j, \phi_j$</h4><p>$$\phi_j:=\frac{1}{m}\sum_{i=1}^mw_j^{(i)}$$</p>
<p>$$\mu_j:=\dfrac{\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\sum_{i=1}^mw_j^{(i)}}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_gaussian_parameters</span>(<span class="params">X, resp, reg_covar, covariance_type</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate the Gaussian distribution parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : 样本数据 (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    resp : Estep猜测的样本i是j类的概率w_i^&#123;j&#125;, shape (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    reg_covar : 对角线正则化项</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    nk : 当前类别下的样本和 (n_components,) 也就是\sum_i^&#123;m&#125;(w_j^&#123;i&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    means : k个n维正态分布的均值, shape (n_components, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariances : 协方差矩阵</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为要做分母，避免为0</span></span><br><span class="line"></span><br><span class="line">    nk = resp.<span class="built_in">sum</span>(axis=<span class="number">0</span>) + <span class="number">10</span> * np.finfo(resp.dtype).eps</span><br><span class="line"></span><br><span class="line">    means = np.dot(resp.T, X) / nk[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">    covariances = &#123;<span class="string">&quot;full&quot;</span>: _estimate_gaussian_covariances_full,</span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;tied&quot;</span>: _estimate_gaussian_covariances_tied,</span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;diag&quot;</span>: _estimate_gaussian_covariances_diag,</span><br><span class="line"></span><br><span class="line">                   <span class="string">&quot;spherical&quot;</span>: _estimate_gaussian_covariances_spherical</span><br><span class="line"></span><br><span class="line">                   &#125;[covariance_type](resp, X, nk, means, reg_covar)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nk, means, covariances</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="迭代收敛，重复以上过程"><a href="#迭代收敛，重复以上过程" class="headerlink" title="迭代收敛，重复以上过程"></a>迭代收敛，重复以上过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate model parameters with the EM algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The method fit the model `n_init` times and set the parameters with</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    which the model has the largest likelihood or lower bound. Within each</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    trial, the method iterates between E-step and M-step for `max_iter`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    times until the change of likelihood or lower bound is less than</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    `tol`, otherwise, a `ConvergenceWarning` is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    迭代终止条件： 迭代次数ｎ_init，极大似然函数或下界函数的增益小于`tol`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : array-like, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    self</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    X = _check_X(X, self.n_components)</span><br><span class="line"></span><br><span class="line">    self._check_initial_parameters(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># if we enable warm_start, we will have a unique initialisation</span></span><br><span class="line"></span><br><span class="line">    do_init = <span class="keyword">not</span>(self.warm_start <span class="keyword">and</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;converged_&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    n_init = self.n_init <span class="keyword">if</span> do_init <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    max_lower_bound = -np.infty</span><br><span class="line"></span><br><span class="line">    self.converged_ = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    random_state = check_random_state(self.random_state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    n_samples, _ = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化次数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> init <span class="keyword">in</span> <span class="built_in">range</span>(n_init):</span><br><span class="line"></span><br><span class="line">        self._print_verbose_msg_init_beg(init)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 先初始化参数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> do_init:</span><br><span class="line"></span><br><span class="line">            self._initialize_parameters(X, random_state)</span><br><span class="line"></span><br><span class="line">            self.lower_bound_ = -np.infty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n_iter <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line"></span><br><span class="line">            prev_lower_bound = self.lower_bound_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># E step求出后验概率w_j^i或是Q分布</span></span><br><span class="line"></span><br><span class="line">            log_prob_norm, log_resp = self._e_step(X)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ｍ step更新参数</span></span><br><span class="line"></span><br><span class="line">            self._m_step(X, log_resp)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 求出下界函数的最大值</span></span><br><span class="line"></span><br><span class="line">            self.lower_bound_ = self._compute_lower_bound(</span><br><span class="line"></span><br><span class="line">                log_resp, log_prob_norm)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 下界函数的增益</span></span><br><span class="line"></span><br><span class="line">            change = self.lower_bound_ - prev_lower_bound</span><br><span class="line"></span><br><span class="line">            self._print_verbose_msg_iter_end(n_iter, change)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 比较下界函数增益与ｔｏｌ</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(change) &lt; self.tol:</span><br><span class="line"></span><br><span class="line">                self.converged_ = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self._print_verbose_msg_init_end(self.lower_bound_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.lower_bound_ &gt; max_lower_bound:</span><br><span class="line"></span><br><span class="line">            max_lower_bound = self.lower_bound_</span><br><span class="line"></span><br><span class="line">            best_params = self._get_parameters()</span><br><span class="line"></span><br><span class="line">            best_n_iter = n_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.converged_:</span><br><span class="line"></span><br><span class="line">        warnings.warn(<span class="string">&#x27;Initialization %d did not converge. &#x27;</span></span><br><span class="line"></span><br><span class="line">                      <span class="string">&#x27;Try different init parameters, &#x27;</span></span><br><span class="line"></span><br><span class="line">                      <span class="string">&#x27;or increase max_iter, tol &#x27;</span></span><br><span class="line"></span><br><span class="line">                      <span class="string">&#x27;or check for degenerate data.&#x27;</span></span><br><span class="line"></span><br><span class="line">                      % (init + <span class="number">1</span>), ConvergenceWarning)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._set_parameters(best_params)</span><br><span class="line"></span><br><span class="line">    self.n_iter_ = best_n_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="E-step中p-x-z-j"><a href="#E-step中p-x-z-j" class="headerlink" title="E step中p(x|z=j)"></a>E step中p(x|z=j)</h3><p>根据高斯分布的参数计算概率,优化的计算方法。</p>
<p>先计算协方差矩阵的precision矩阵，并进行cholesky分解</p>
<p>Precision matrix 协方差矩阵的逆矩阵：<a target="_blank" rel="noopener" href="https://www.statlect.com/glossary/precision-matrix">https://www.statlect.com/glossary/precision-matrix</a></p>
<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/precision.png"></p>
<p>然后根据精度矩阵的cholesky分解形式,这样可以优化矩阵运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_precision_cholesky</span>(<span class="params">covariances, covariance_type</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the Cholesky decomposition of the precisions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariances : array-like</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The covariance matrix of the current components.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The shape depends of the covariance_type.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The type of precision matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    precisions_cholesky : array-like</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The cholesky decomposition of sample precisions of the current</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        components. The shape depends of the covariance_type.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> covariance_type <span class="keyword">in</span> <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        n_components, n_features, _ = covariances.shape</span><br><span class="line"></span><br><span class="line">        precisions_chol = np.empty((n_components, n_features, n_features))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, covariance <span class="keyword">in</span> <span class="built_in">enumerate</span>(covariances):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">                cov_chol = linalg.cholesky(covariance, lower=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">except</span> linalg.LinAlgError:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">raise</span> ValueError(estimate_precision_error_message)</span><br><span class="line"></span><br><span class="line">            precisions_chol[k] = linalg.solve_triangular(cov_chol,</span><br><span class="line"></span><br><span class="line">                                                         np.eye(n_features),</span><br><span class="line"></span><br><span class="line">                                                         lower=<span class="literal">True</span>).T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        _, n_features = covariances.shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">            cov_chol = linalg.cholesky(covariances, lower=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> linalg.LinAlgError:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(estimate_precision_error_message)</span><br><span class="line"></span><br><span class="line">        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),</span><br><span class="line"></span><br><span class="line">                                                  lower=<span class="literal">True</span>).T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">any</span>(np.less_equal(covariances, <span class="number">0.0</span>)):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(estimate_precision_error_message)</span><br><span class="line"></span><br><span class="line">        precisions_chol = <span class="number">1.</span> / np.sqrt(covariances)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> precisions_chol</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="计算cholesky分解的行列式"><a href="#计算cholesky分解的行列式" class="headerlink" title="计算cholesky分解的行列式"></a>计算cholesky分解的行列式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Gaussian mixture probability estimators</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据cholesky分解计算行列式的log</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_log_det_cholesky</span>(<span class="params">matrix_chol, covariance_type, n_features</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the log-det of the cholesky decomposition of matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    matrix_chol : 协方差矩阵的cholesky分解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    n_features : int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Number of features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_det_precision_chol : array-like, shape (n_components,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The determinant of the precision matrix for each component.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> covariance_type == <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        n_components, _, _ = matrix_chol.shape</span><br><span class="line"></span><br><span class="line">        log_det_chol = (np.<span class="built_in">sum</span>(np.log(</span><br><span class="line"></span><br><span class="line">            matrix_chol.reshape(</span><br><span class="line"></span><br><span class="line">                n_components, -<span class="number">1</span>)[:, ::n_features + <span class="number">1</span>]), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        log_det_chol = (np.<span class="built_in">sum</span>(np.log(np.diag(matrix_chol))))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;diag&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        log_det_chol = (np.<span class="built_in">sum</span>(np.log(matrix_chol), axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">        log_det_chol = n_features * (np.log(matrix_chol))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> log_det_chol</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="计算分子-logp-x-i-z-i-j-mu-Sigma"><a href="#计算分子-logp-x-i-z-i-j-mu-Sigma" class="headerlink" title="计算分子: $logp(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$"></a>计算分子: $logp(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_estimate_log_gaussian_prob</span>(<span class="params">X, means, precisions_chol, covariance_type</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Estimate the log Gaussian probability.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : 样本数据(n_samples, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    means : k个n维正态分布的均值(n_components, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    precisions_chol : 精度矩阵的Cholesky分解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance_type : &#123;&#x27;full&#x27;, &#x27;tied&#x27;, &#x27;diag&#x27;, &#x27;spherical&#x27;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    log_prob : (n_samples, n_components)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">    n_components, _ = means.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># det(precision_chol) is half of det(precision)</span></span><br><span class="line"></span><br><span class="line">    log_det = _compute_log_det_cholesky(</span><br><span class="line"></span><br><span class="line">        precisions_chol, covariance_type, n_features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> covariance_type == <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        log_prob = np.empty((n_samples, n_components))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, (mu, prec_chol) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(means, precisions_chol)):</span><br><span class="line"></span><br><span class="line">            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)</span><br><span class="line"></span><br><span class="line">            log_prob[:, k] = np.<span class="built_in">sum</span>(np.square(y), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;diag&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> covariance_type == <span class="string">&#x27;spherical&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">.5</span> * (n_features * np.log(<span class="number">2</span> * np.pi) + log_prob) + log_det</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="sklearn中实例"><a href="#sklearn中实例" class="headerlink" title="sklearn中实例"></a>sklearn中实例</h3><p>Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid.</p>
<p>We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data.</p>
<p>On the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only the first two dimensions are shown here, and thus some points are separated in other dimensions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">colors = [<span class="string">&#x27;navy&#x27;</span>, <span class="string">&#x27;turquoise&#x27;</span>, <span class="string">&#x27;darkorange&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()  <span class="comment"># data, target</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Break up the dataset into non-overlapping training (75%) and testing</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (25%) sets.</span></span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only take the first fold.</span></span><br><span class="line"></span><br><span class="line">train_index, test_index = <span class="built_in">next</span>(<span class="built_in">iter</span>(skf.split(iris.data, iris.target)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_train = iris.data[train_index]     <span class="comment"># (111, 4)</span></span><br><span class="line"></span><br><span class="line">y_train = iris.target[train_index]   <span class="comment"># (111,)</span></span><br><span class="line"></span><br><span class="line">X_test = iris.data[test_index]       <span class="comment"># (39, 4)</span></span><br><span class="line"></span><br><span class="line">y_test = iris.target[test_index]     <span class="comment"># (39,)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n_classes = <span class="built_in">len</span>(np.unique(y_train))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try GMMs using different types of covariances. 根据协方差矩阵，有4中不同的GMM模型</span></span><br><span class="line"></span><br><span class="line">estimators = <span class="built_in">dict</span>((cov_type, GaussianMixture(n_components=n_classes,</span><br><span class="line"></span><br><span class="line">                   covariance_type=cov_type, max_iter=<span class="number">20</span>, random_state=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">                  <span class="keyword">for</span> cov_type <span class="keyword">in</span> [<span class="string">&#x27;spherical&#x27;</span>, <span class="string">&#x27;diag&#x27;</span>, <span class="string">&#x27;tied&#x27;</span>, <span class="string">&#x27;full&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n_estimators = <span class="built_in">len</span>(estimators)  <span class="comment"># 4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># figsize表示图像的尺寸（width, height in inches）</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">3</span> * <span class="number">5</span> // <span class="number">2</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像之间的间距</span></span><br><span class="line"></span><br><span class="line">plt.subplots_adjust(bottom=<span class="number">.01</span>, top=<span class="number">0.95</span>, hspace=<span class="number">.15</span>, wspace=<span class="number">.05</span>,</span><br><span class="line"></span><br><span class="line">                    left=<span class="number">.01</span>, right=<span class="number">.99</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 椭圆</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_ellipses</span>(<span class="params">gmm, ax</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, color <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> gmm.covariance_type == <span class="string">&#x27;full&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = gmm.covariances_[n][:<span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> gmm.covariance_type == <span class="string">&#x27;tied&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = gmm.covariances_[:<span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> gmm.covariance_type == <span class="string">&#x27;diag&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = np.diag(gmm.covariances_[n][:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> gmm.covariance_type == <span class="string">&#x27;spherical&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            covariances = np.eye(gmm.means_.shape[<span class="number">1</span>]) * gmm.covariances_[n]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 参数估计得到的混合二维高斯分布，将其用椭圆表示出来～</span></span><br><span class="line"></span><br><span class="line">        v, w = np.linalg.eigh(covariances) <span class="comment"># 返回协方差矩阵的特征值和列向量由特征矩阵构成的矩阵</span></span><br><span class="line"></span><br><span class="line">        u = w[<span class="number">0</span>] / np.linalg.norm(w[<span class="number">0</span>]) <span class="comment"># order=None 表示 Frobenius norm，2-norm</span></span><br><span class="line"></span><br><span class="line">        angle = np.arctan2(u[<span class="number">1</span>], u[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        angle = <span class="number">180</span> * angle / np.pi  <span class="comment"># 转换为角度</span></span><br><span class="line"></span><br><span class="line">        v = <span class="number">2.</span> * np.sqrt(<span class="number">2.</span>) * np.sqrt(v)</span><br><span class="line"></span><br><span class="line">        ell = mpl.patches.Ellipse(gmm.means_[n, :<span class="number">2</span>], v[<span class="number">0</span>], v[<span class="number">1</span>],</span><br><span class="line"></span><br><span class="line">                                  <span class="number">180</span> + angle, color=color)</span><br><span class="line"></span><br><span class="line">        ell.set_clip_box(ax.bbox)</span><br><span class="line"></span><br><span class="line">        ell.set_alpha(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        ax.add_artist(ell)  <span class="comment"># 增加文字</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, (name, estimator) <span class="keyword">in</span> <span class="built_in">enumerate</span>(estimators.items()):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we have class labels for the training data, we can</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the GMM parameters in a supervised manner.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里因为有类标签，所以直接用真实均值来初始化GMM的均值。在无标签或者标签较少的情况下，则需要随机初始化</span></span><br><span class="line"></span><br><span class="line">    estimator.means_init = np.array([X_train[y_train == i].mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the other parameters using the EM algorithm.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用em算法来估计其他参数</span></span><br><span class="line"></span><br><span class="line">    estimator.fit(X_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画椭圆</span></span><br><span class="line"></span><br><span class="line">    h = plt.subplot(<span class="number">2</span>, n_estimators // <span class="number">2</span>, index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    make_ellipses(estimator, h)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, color <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors):</span><br><span class="line"></span><br><span class="line">        data = iris.data[iris.target == n]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不同的种类数据用不同的点表示</span></span><br><span class="line"></span><br><span class="line">        plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], s=<span class="number">0.8</span>, color=color,</span><br><span class="line"></span><br><span class="line">                    label=iris.target_names[n])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用xx表示测试集</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, color <span class="keyword">in</span> <span class="built_in">enumerate</span>(colors):</span><br><span class="line"></span><br><span class="line">        data = X_test[y_test == n]</span><br><span class="line"></span><br><span class="line">        plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], marker=<span class="string">&#x27;x&#x27;</span>, color=color)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集的准确率</span></span><br><span class="line"></span><br><span class="line">    y_train_pred = estimator.predict(X_train) <span class="comment"># 预测是选取概率最大的一类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当无标签时是没有办法计算准确率的。但是这里有标签，</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># y_train_pred返回的是概率最大的索引，　y_train的元素是[0,1,2,3]中的一个</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因此可以求得准确率</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(y_train_pred[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(y_train[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    plt.text(<span class="number">0.05</span>, <span class="number">0.9</span>, <span class="string">&#x27;Train accuracy: %.1f&#x27;</span> % train_accuracy,</span><br><span class="line"></span><br><span class="line">             transform=h.transAxes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    y_test_pred = estimator.predict(X_test)</span><br><span class="line"></span><br><span class="line">    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    plt.text(<span class="number">0.05</span>, <span class="number">0.8</span>, <span class="string">&#x27;Test accuracy: %.1f&#x27;</span> % test_accuracy,</span><br><span class="line"></span><br><span class="line">             transform=h.transAxes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.xticks(())</span><br><span class="line"></span><br><span class="line">    plt.yticks(())</span><br><span class="line"></span><br><span class="line">    plt.title(name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.legend(scatterpoints=<span class="number">1</span>, loc=<span class="string">&#x27;lower right&#x27;</span>, prop=<span class="built_in">dict</span>(size=<span class="number">12</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E4%B8%AD%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/Figure_1.png"></p>
</div></article></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">116</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>