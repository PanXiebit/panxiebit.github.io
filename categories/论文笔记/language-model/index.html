<!doctype html>
<html lang="de"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: language model - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="潘小榭"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘小榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="潘小榭"><meta property="og:url" content="http://www.panxiaoxie.cn/"><meta property="og:site_name" content="潘小榭"><meta property="og:image" content="http://www.panxiaoxie.cn/img/og_image.png"><meta property="article:author" content="Xie Pan"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn"},"headline":"潘小榭","image":["http://www.panxiaoxie.cn/img/og_image.png"],"author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/logo.svg"}},"description":null}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></li><li class="is-active"><a href="#" aria-current="page">language model</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-01-15T01:02:14.000Z" title="2019/1/15 上午9:02:14">2019-01-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-29T08:12:09.159Z" title="2021/6/29 下午4:12:09">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/">language model</a></span><span class="level-item">8 minutes read (About 1217 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/">论文笔记-预训练语言模型2-ULMFiT</a></h1><div class="content"><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h3 id="对比之前的几种模型"><a href="#对比之前的几种模型" class="headerlink" title="对比之前的几种模型"></a>对比之前的几种模型</h3><h4 id="concatenate-embeddings-ELMo"><a href="#concatenate-embeddings-ELMo" class="headerlink" title="concatenate embeddings: ELMo"></a><strong>concatenate embeddings</strong>: ELMo</h4><blockquote>
<p>Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.  </p>
</blockquote>
<p>这篇 paper 是在 elmo 之后，而 elmo 虽然相对出名，影响力更大，但是 elmo 仍旧只是一种 word embedding 的预训练，在下游任务中还是需要从头训练模型。</p>
<p>ELMo有以下几个步骤：  </p>
<ul>
<li><p>利用LM任务进行预训练  </p>
</li>
<li><p>再利用目标领域的语料对LM模型做微调  </p>
</li>
<li><p>最后针对目标任务进行 concatenate embedding，然后训练模型</p>
</li>
</ul>
<h4 id="pretraining-LM"><a href="#pretraining-LM" class="headerlink" title="pretraining LM:"></a><strong>pretraining LM:</strong></h4><blockquote>
<p>In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability.  </p>
</blockquote>
<p>直接使用在 general-domain 上预训练好的语言模型，然后通过 fine-tune 进行迁移学习， 仍旧需要大量的 in-domain 的文档才能获得比较好的 performance.</p>
<h4 id="ULMFiT"><a href="#ULMFiT" class="headerlink" title="ULMFiT"></a>ULMFiT</h4><blockquote>
<p>We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.  </p>
</blockquote>
<p>作者认为，预训练语言模型的方式并不是不好，只是训练方法的问题导致了他们表现局限性。想对于 CV， NLP 中的很多任务所需要的语义更浅层。而将 LMs 在小数据集上 fine-tune 时会导致严重的遗忘。</p>
<p>于是，作者提出了 Universal Language Model Fine-tuning(ULMFiT)  </p>
<ul>
<li><p>通用的语言模型微调  </p>
</li>
<li><p>discriminative fine-tuning, slanted triangular learning rates  </p>
</li>
<li><p>gradual unfreezing  </p>
</li>
</ul>
<h2 id="Universal-Language-Model-Fine-tuning"><a href="#Universal-Language-Model-Fine-tuning" class="headerlink" title="Universal Language Model Fine-tuning"></a>Universal Language Model Fine-tuning</h2><p><img src="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/01.png"></p>
<p>主要分为 3 部分：  </p>
<ul>
<li><p>General-domain LM pretraining  </p>
</li>
<li><p>Target task LM fine-tuning  </p>
</li>
<li><p>Target task classifier fine-tuning  </p>
</li>
</ul>
<h3 id="General-domain-LM-pretraining"><a href="#General-domain-LM-pretraining" class="headerlink" title="General-domain LM pretraining"></a>General-domain LM pretraining</h3><p>Wikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words.  </p>
<p>在足够大的 general-domain 语料库上进行预训练。</p>
<h3 id="Target-task-LM-fine-tuning"><a href="#Target-task-LM-fine-tuning" class="headerlink" title="Target task LM fine-tuning"></a>Target task LM fine-tuning</h3><h4 id="discriminative-fine-tunin"><a href="#discriminative-fine-tunin" class="headerlink" title="discriminative fine-tunin"></a>discriminative fine-tunin</h4><p>在目标语料库 in-domain 上进行 fine-tune. 这部分会收敛的很快，并且在小数据集上依旧会有很好的泛化性。  </p>
<blockquote>
<p>As different layers capture different types of information (Yosinski et al., 2014), they should be fine-tuned to different extents.  </p>
</blockquote>
<p>不同的 layer 能捕捉不同程度的信息，于是，作者提出了 discriminative fine-tuning. 不同的 layer 具有不同的 learning rate. L 表示总的 layer 数目。</p>
<p>$${\theta^1,\theta^2, …, \theta^L}$$</p>
<p>$${\eta^1,\eta^2, …, \eta^L}$$</p>
<blockquote>
<p>Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates.</p>
</blockquote>
<p>原本的 SGD 是这样的：</p>
<p>$$\theta_t = \theta_{t-1}-\eta\cdot\nabla_{\theta}J(\theta)$$</p>
<p>改进之后：</p>
<p>$$\theta_t^l = \theta_{t-1}^l-\eta^l\cdot\nabla_{\theta^l}J(\theta)$$</p>
<p>作者通过经验发现：先选择最后一层的学习率 $\eta^L$，然后计算每一层的学习率 $\eta^{l-1}=\eta^l/2.6$</p>
<h4 id="Slanted-triangular-learning-rates"><a href="#Slanted-triangular-learning-rates" class="headerlink" title="Slanted triangular learning rates"></a>Slanted triangular learning rates</h4><p><img src="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/02.png"></p>
<ul>
<li><p>T 是迭代次数，这里实际上是 $epochs \times \text{number of per epoch}$  </p>
</li>
<li><p>cut_frac 是增加学习率的迭代步数比例  </p>
</li>
<li><p>cut 是学习率增加和减少的临界迭代步数  </p>
</li>
<li><p>p 是一个分段函数，分别递增和递减  </p>
</li>
<li><p>ratio 表示学习率最小时，与最大学习率的比例。比如 t=0时，p=0, 那么 $\eta_0=\dfrac{\eta_{max}}{ratio}$  </p>
</li>
</ul>
<p><img src="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/3.png"></p>
<p>作者通过实验发现，cut_frac=0.1, ratio=32, $\eta_max=0.01$</p>
<h3 id="Target-task-classifier-fine-tuning"><a href="#Target-task-classifier-fine-tuning" class="headerlink" title="Target task classifier fine-tuning"></a>Target task classifier fine-tuning</h3><p>针对分类任务，加上 two additional linear blocks.</p>
<h4 id="concat-pooling"><a href="#concat-pooling" class="headerlink" title="concat pooling"></a>concat pooling</h4><p><img src="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/04.png"></p>
<h4 id="gradul-unfreezing"><a href="#gradul-unfreezing" class="headerlink" title="gradul unfreezing"></a>gradul unfreezing</h4><p>逐渐 unfreeze layers:  </p>
<blockquote>
<p>We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration.</p>
</blockquote>
<h4 id="BPTT-for-Text-Classification"><a href="#BPTT-for-Text-Classification" class="headerlink" title="BPTT for Text Classification"></a>BPTT for Text Classification</h4><p>backpropagation through time(BPTT)</p>
<blockquote>
<p>We divide the document into fixed length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences (Merity et al., 2017a).  </p>
</blockquote>
<p>什么意思？并不是一个 batch 更新一次梯度，而是累加一定的 batch 之后在更新梯度？  </p>
<p>能增加泛化性？</p>
<h4 id="Bidirectional-language-model"><a href="#Bidirectional-language-model" class="headerlink" title="Bidirectional language model"></a>Bidirectional language model</h4><p>独立的对 forward-LM, backward-LM 进行 fine-tune, 然后平均。</p>
<h2 id="experiment"><a href="#experiment" class="headerlink" title="experiment"></a>experiment</h2><h3 id="与其他模型对比"><a href="#与其他模型对比" class="headerlink" title="与其他模型对比"></a>与其他模型对比</h3><p><img src="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/05.png"></p>
<h3 id="ablations"><a href="#ablations" class="headerlink" title="ablations"></a>ablations</h3><p><img src="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/06.png"></p>
<p>“from scratch”: 没有 fine-tune  </p>
<p>“supervised”: 表示仅仅在 label examples 进行 fine-tune  </p>
<p>“semi-supervised”: 表示在 unable examples 上也进行了 fine-tune</p>
<h3 id="对-tricks-进行分析"><a href="#对-tricks-进行分析" class="headerlink" title="对 tricks 进行分析"></a>对 tricks 进行分析</h3><p><img src="/2019/01/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B2-ULMFiT/07.png"></p>
<p>“full” :fine-tuning the full model  </p>
<p>“discr”: discriminative fine-tuning  </p>
<p>“stlr”: slanted triangular learning rates</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-12-17T06:50:31.000Z" title="2018/12/17 下午2:50:31">2018-12-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-29T08:12:09.158Z" title="2021/6/29 下午4:12:09">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/">language model</a></span><span class="level-item">21 minutes read (About 3119 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">论文笔记-BERT</a></h1><div class="content"><h2 id="BERT-Bidirectional-Encoder-Representations-from-Transformers"><a href="#BERT-Bidirectional-Encoder-Representations-from-Transformers" class="headerlink" title="BERT(Bidirectional Encoder Representations from Transformers.)"></a>BERT(Bidirectional Encoder Representations from Transformers.)</h2><p>对于 BERT 重点在于理解 Bidirectional 和 masked language model.</p>
<h3 id="Why-Bidirectional"><a href="#Why-Bidirectional" class="headerlink" title="Why Bidirectional?"></a>Why Bidirectional?</h3><p>对于预训练的表示，单向语言模型因为无法融合下文的信息，其能力是非常有限的，尤其是对类似于 SQuAD 这样需要结合上下文信息的任务。</p>
<p>对比 OpenAI GPT 和 BERT. 为什么 OpenAI GPT 不能采用双向 self-attention 呢？</p>
<p>传统的语言模型的定义，计算句子的概率：</p>
<p>$$P(S)=p(w_1,w_2, …, w_n)=p(w1)p(w_2|w_1)…p(w_n|w_1…w_{n-1})=\prod_{i=1}^m p(w_i|w_1…w_{i-1})$$</p>
<p>前向 RNN 语言模型：</p>
<p>$$P(S)=\prod_{i=1}^m p(w_i|w_1…w_{i-1})$$</p>
<p>也就是当前词的概率只依赖前面出现词的概率。</p>
<p>后向 RNN 语言模型  </p>
<p>$$P(S)=\prod_{i=1}^m p(w_i|w_{i+1}…w_{m})$$</p>
<p>也就是当前词的概率只依赖后面出现的词的概率。</p>
<p>ELMo 就是这样的双向语言模型(BiLM)</p>
<p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/02.jpg"></p>
<p>但是 RNN 相比 self-attention 对上下文信息 (contextual information)的利用相对有限，而且 ELMo 只能是一层双向，并不能使用多层。其原因和 GPT 无法使用 双向 编码的原因一样。</p>
<p>对于 GPT 如果它使用双向，那么模型就能准确的学到到句子中的下一个词是什么，并能 100% 的预测出下一个词。比如 “I love to work on NLP.” 在预测 love 的下一个词时，模型能看到 to，所以能很快的通过迭代学习到 “to” 100% 就是 love 的下一个词。所以，这导致模型并不能学到想要的东西（句法、语义信息）。</p>
<p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/03.png"> <img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/04.png"></p>
<p>那么 BERT 是怎么处理双向这个问题的呢？ 它改变了训练语言模型的任务形式。提出了两种方式 “masked language model” and “next sentence generation”. 再介绍这两种训练方式之前，先说明下输入形式。</p>
<h3 id="Input-representation"><a href="#Input-representation" class="headerlink" title="Input representation"></a>Input representation</h3><p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/05.png"></p>
<ul>
<li><p>position embedding: 跟 Transformer 类似    </p>
</li>
<li><p>sentence embedding, 同一个句子的词的表示一样，都是 $E_A$ 或 $E_B$. 用来表示不同的句子具有不同的含义  </p>
</li>
<li><p>对于 [Question, Answer] 这样的 sentence-pairs 的任务，在句子末尾加上 [SEP].  </p>
</li>
<li><p>对于文本分类这样的 single-sentence 的任务，只需要加上 [CLS], 并且 sentence embedding 只有 $E_A$.</p>
</li>
</ul>
<h3 id="masked-language-model"><a href="#masked-language-model" class="headerlink" title="masked language model"></a>masked language model</h3><p>何为 “masked LM”? idea 来源于 closed tasked. 原本的语言模型是预测所有语料中的下一个词，而 MLM 是在所有的 tokens 中随机选取 15% 的进行 mask，然后只需要预测被 mask 的词。这样以来，就能训练双向语言模型了。</p>
<p>但是存在一个问题，这样 pre-training 训练出来的语言模型并不能拿去做 fine-tune. 原因是在 fine-token 中从来没有见过 &lt;MASK&gt; 这个词。作者采用这样的策略：  </p>
<p>具体的操作，以 “My dog is hairy” 为例，mask “hairy” 这个词：</p>
<ul>
<li><p>“My dog is &lt;MASK&gt;“. 80% 被 <MASK> 代替  </MASK></p>
</li>
<li><p>“My dog is apple”.  10% 被一个随机的 token 代替  </p>
</li>
<li><p>“My dog is hairy”.  10% 保持原来的样子  </p>
</li>
</ul>
<h4 id="为什么不用-lt-MASK-gt-代替所有的-token？"><a href="#为什么不用-lt-MASK-gt-代替所有的-token？" class="headerlink" title="为什么不用 &lt;MASK&gt; 代替所有的 token？"></a>为什么不用 &lt;MASK&gt; 代替所有的 token？</h4><blockquote>
<p>If the model had been trained on only predicting ‘&lt;MASK&gt;’ tokens and then never saw this token during fine-tuning, it would have thought that there was no need to predict anything and this would have hampered performance. Furthermore, the model would have only learned a contextual representation of the ‘&lt;MASK&gt;’ token and this would have made it learn slowly (since only 15% of the input tokens are masked). By sometimes asking it to predict a word in a position that did not have a ‘&lt;MASK&gt;’ token, the model needed to learn a contextual representation of all the words in the input sentence, just in case it was asked to predict them afterwards.  </p>
</blockquote>
<p>如果模型在预训练的时候仅仅只预测 &lt;MASK&gt;, 然后在 fine-tune 的时候从未见过 &lt;MASK&gt; 这个词，那么模型就不需要预测任何词，在 fine-tune 时会影响性能。  </p>
<p>更严重的是，如果仅仅预测 &lt;MASK&gt;, 那么模型只需要学习 &lt;MASK&gt; 的上下文表示，这会导致它学习的很慢。  </p>
<p>如果让模型在某个位置去预测一个不是 &lt;MASK&gt; 的词，那么模型就需要学习所有 tokens 的上下文表示，因为万一需要预测这个词呢。</p>
<h4 id="只需要-random-tokens-足够吗？为什么还需要-10-的完整的-sentence"><a href="#只需要-random-tokens-足够吗？为什么还需要-10-的完整的-sentence" class="headerlink" title="只需要 random tokens 足够吗？为什么还需要 10% 的完整的 sentence?"></a>只需要 random tokens 足够吗？为什么还需要 10% 的完整的 sentence?</h4><blockquote>
<p>Well, ideally we want the model’s representation of the masked token to be better than random. By sometimes keeping the sentence intact (while still asking the model to predict the chosen token) the authors biased the model to learn a meaningful representation of the masked tokens.  </p>
</blockquote>
<p>使得模型具有偏置，更倾向于获得有意义的 masked token.</p>
<p>在知乎上问了这个问题，大佬的回复跟这篇 blog 有点差异，但实际上意思是一样的：  </p>
<p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/11.png"></p>
<p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/10.png"></p>
<p>总结下：  </p>
<p>为什么不能完全只有 &lt;MASK&gt; ?  如果只有 &lt;MASK&gt;, 那么这个预训练模型是有偏置的，也就是学到一种方式，用上下文去预测一个词。这导致在 fine-tune 时，会丢一部分信息，也就是知乎大佬第一部分所说的。</p>
<p>所以加上 random 和 ture token 是让模型知道，每个词都是有意义的，除了上下文信息，还要用到它本身的信息，即使是 &lt;MASK&gt;. 也就是知乎上说的，提取这两方面的信息。</p>
<p>再回过头，从语言模型的角度来看，依然是需要预测每一个词，但是绝大多数词它的 cross entropy loss 会很小，而主要去优化得到 &lt;MASK&gt; 对应的词。而 random/true token 告诉模型，你需要提防每一个词，他们也需要好好预测，因为他们不一定就是对的。</p>
<p>感谢知乎大佬！</p>
<h4 id="random-tokens-会-confuse-模型吗？"><a href="#random-tokens-会-confuse-模型吗？" class="headerlink" title="random tokens 会 confuse 模型吗？"></a>random tokens 会 confuse 模型吗？</h4><p>不会， random tokens 只占 15% * 10% = 1.5%. 这不会影响模型的性能。</p>
<p>还有一个问题， &lt;MASK&gt; 所占的比例很小，主要优化对象迭代一次对整个模型影响会很小，因而需要更多次迭代.</p>
<h3 id="next-sentence-generation"><a href="#next-sentence-generation" class="headerlink" title="next sentence generation"></a>next sentence generation</h3><p>对于下游是 Question Answering(QA), Natural Language Inference(NLI) 这样需要理解句子之间的相关性的任务，仅仅通过语言模型并不能获得这方面的信息。为了让模型能够理解句子之间的关系，作者提出了一个 binarized next sentence prediction.</p>
<p>具体方式是：  </p>
<p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/06.png"></p>
<p>50% 是正确的相邻的句子。 50% 是随机选取的一个句子。这个任务在预训练中能达到 97%-98% 的准确率，并且能很显著的提高 QA NLI 的任务。</p>
<h3 id="pre-training-procudure"><a href="#pre-training-procudure" class="headerlink" title="pre-training procudure"></a>pre-training procudure</h3><p>作者预训练使用的语料：BooksCorpus (800M words)，English Wikipedia (2,500M words)。 使用文档级别的语料很关键，而不是 shffule 的句子级别的语料，这样可以获得更长的 sentence.</p>
<p>获得训练样本：从预料库中抽取句子对，其中 50% 的两个句子之间是确实相邻的，50% 的第二个句子是随机抽取的。具体操作看代码吧</p>
<ul>
<li><p>batch_size 256.  </p>
</li>
<li><p>每一个 sentences 对： 512 tokens  </p>
</li>
<li><p>40 epochs  </p>
</li>
<li><p>Adam lr=1e-4, $\beta_1=0.9$, $\beta_2=0.999$, L2 weight decay 0.01  </p>
</li>
<li><p>learning rate warmup 10000 steps  </p>
</li>
<li><p>0.1 dropout  </p>
</li>
<li><p>gelu instead of relu  </p>
</li>
</ul>
<h3 id="Fine-tune-procedure"><a href="#Fine-tune-procedure" class="headerlink" title="Fine-tune procedure"></a>Fine-tune procedure</h3><h4 id="sequence-level-tasks"><a href="#sequence-level-tasks" class="headerlink" title="sequence-level tasks"></a>sequence-level tasks</h4><ul>
<li><p>比如 sentences pairs 的 Quora Question Pairs(QQP) 预测两个句子之间语义是否相同。如下图中（a）.  </p>
</li>
<li><p>如果是 single sentence classification 比如 Stanford Sentiment Treebank（SST-2）和 Corpus of Linguistic Acceptability（CoLA）这种分类问题。如下图（b）  </p>
</li>
</ul>
<p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/07.png"></p>
<p>只需要输出 Transformer 最后一层的隐藏状态中的第一个 token，也就是 [CLS]. 然后接上一个全链接映射到相应的 label 空间即可。</p>
<p>fine-tune 时的超参数跟 pre-training 时的参数大致相同。但是训练速度会很快</p>
<ul>
<li><p>Batch size: 16, 32  </p>
</li>
<li><p>Learning rate (Adam): 5e-5, 3e-5, 2e-5  </p>
</li>
<li><p>Number of epochs: 3, 4  </p>
</li>
</ul>
<p>语料库越大，对参数的敏感度越小。  </p>
<h4 id="token-level-tasks"><a href="#token-level-tasks" class="headerlink" title="token-level tasks."></a>token-level tasks.</h4><p><img src="/2018/12/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/09.png"></p>
<p>对于token-level classification(例如NER)，取所有token的最后层transformer输出，喂给softmax层做分类。</p>
<h2 id="如何使用-BERT"><a href="#如何使用-BERT" class="headerlink" title="如何使用 BERT"></a>如何使用 BERT</h2><h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><p><a target="_blank" rel="noopener" href="https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py">https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py</a></p>
<p>主要涉及到两个 类:  </p>
<ul>
<li><p>数据预处理   </p>
</li>
<li><p>预训练模型加载  </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pytorch_pretrained_bert <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification, BertConfig, BertAdam， PYTORCH_PRETRAINED_BERT_CACHE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;./pre_trained_models/bert-base-uncased-vocab.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>,</span><br><span class="line"></span><br><span class="line">          cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / <span class="string">&#x27;distributed_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(args.local_rank),</span><br><span class="line"></span><br><span class="line">          num_labels = num_labels)</span><br><span class="line"></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&quot;pre_trained_models/bert-base-uncased.tar.gz&quot;</span>, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中 <code>bert-base-uncased</code> 可以分别用具体的 词表文件 和 模型文件 代替。从源代码中提供的链接下载即可。</p>
<h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pytorch_pretrained_bert <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification, BertConfig, BertAdam， PYTORCH_PRETRAINED_BERT_CACHE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;./pre_trained_models/bert-base-uncased-vocab.txt&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>前一种方式是根据代码中提供的 url 去下载词表文件，然后缓存在默认文件夹下 <code>/home/panxie/.pytorch_pretrained_bert</code> 。后者是直接下载词表文件后，放在本地。相对来说，后者更方便。</p>
<p>这部分代码相对比较简单，根据自己的任务，继承 <code>DataProcessor</code> 这个类即可。</p>
<p>作为模型的输入，features 主要包括三个部分：  </p>
<ul>
<li><p>input_ids 是通过词典映射来的  </p>
</li>
<li><p>input_mask 在 fine-tune 阶段，所有的词都是 1, padding 的是 0  </p>
</li>
<li><p>segment_ids 在 text_a 中是 0, 在 text_b 中是 1, padding 的是 0  </p>
</li>
</ul>
<p>这里对应了前面所说的，input_idx 就是 token embedding, segment_ids 就是 Sentence Embedding. 而 input_mask 则表示哪些位置被 mask 了，在 fine-tune 阶段都是 1.</p>
<h4 id="加载预训练模型"><a href="#加载预训练模型" class="headerlink" title="加载预训练模型"></a>加载预训练模型</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">!tar -tf pre_trained_models/bert-base-uncased.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">./pytorch_model.bin</span><br><span class="line"></span><br><span class="line">./bert_config.json</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下载好的文件包中含有两个文件，分别是 config 信息，以及模型参数。</p>
<p>如果不用具体的文件，则需要从代码中提供的 url 下载，并缓存在默认文件夹 <code>PYTORCH_PRETRAINED_BERT_CACHE = /home/panxie/.pytorch_pretrained_bert</code></p>
<p>作为分类任务， num_labels 参数默认为 2.</p>
<p>运行时会发现提取预训练模型会输出如下信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">12</span>/<span class="number">26</span>/<span class="number">2018</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">41</span> - INFO - pytorch_pretrained_bert.modeling -   </span><br><span class="line"></span><br><span class="line">loading archive file pre_trained_models/bert-base-uncased.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="number">12</span>/<span class="number">26</span>/<span class="number">2018</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">41</span> - INFO - pytorch_pretrained_bert.modeling -   </span><br><span class="line"></span><br><span class="line">extracting archive file pre_trained_models/bert-base-uncased.tar.gz to temp <span class="built_in">dir</span> /tmp/tmpgm506dcx</span><br><span class="line"></span><br><span class="line"><span class="number">12</span>/<span class="number">26</span>/<span class="number">2018</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">44</span> - INFO - pytorch_pretrained_bert.modeling -   </span><br><span class="line"></span><br><span class="line">Model config &#123;</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;attention_probs_dropout_prob&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;hidden_act&quot;</span>: <span class="string">&quot;gelu&quot;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;hidden_dropout_prob&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;hidden_size&quot;</span>: <span class="number">768</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;initializer_range&quot;</span>: <span class="number">0.02</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;intermediate_size&quot;</span>: <span class="number">3072</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;max_position_embeddings&quot;</span>: <span class="number">512</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;num_attention_heads&quot;</span>: <span class="number">12</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;num_hidden_layers&quot;</span>: <span class="number">12</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;type_vocab_size&quot;</span>: <span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;vocab_size&quot;</span>: <span class="number">30522</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">12</span>/<span class="number">26</span>/<span class="number">2018</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">45</span> - INFO - pytorch_pretrained_bert.modeling -   </span><br><span class="line"></span><br><span class="line">Weights of BertForSequenceClassification <span class="keyword">not</span> initialized <span class="keyword">from</span> pretrained model:</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;classifier.weight&#x27;</span>, <span class="string">&#x27;classifier.bias&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">12</span>/<span class="number">26</span>/<span class="number">2018</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">45</span> - INFO - pytorch_pretrained_bert.modeling -   </span><br><span class="line"></span><br><span class="line">Weights <span class="keyword">from</span> pretrained model <span class="keyword">not</span> used <span class="keyword">in</span> BertForSequenceClassification:</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;cls.predictions.bias&#x27;</span>, <span class="string">&#x27;cls.predictions.transform.dense.weight&#x27;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;cls.predictions.transform.dense.bias&#x27;</span>, <span class="string">&#x27;cls.predictions.decoder.weight&#x27;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;cls.seq_relationship.weight&#x27;</span>, <span class="string">&#x27;cls.seq_relationship.bias&#x27;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;cls.predictions.transform.LayerNorm.weight&#x27;</span>, <span class="string">&#x27;cls.predictions.transform.LayerNorm.bias&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>不得不去观察 <code>from_pretrained</code> 的源码：<a target="_blank" rel="noopener" href="https://github.com/huggingface/pytorch-pretrained-BERT/blob/8da280ebbeca5ebd7561fd05af78c65df9161f92/pytorch_pretrained_bert/modeling.py#L448">https://github.com/huggingface/pytorch-pretrained-BERT/blob/8da280ebbeca5ebd7561fd05af78c65df9161f92/pytorch_pretrained_bert/modeling.py#L448</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">missing_keys = []</span><br><span class="line"></span><br><span class="line">unexpected_keys = []</span><br><span class="line"></span><br><span class="line">error_msgs = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># copy state_dict so _load_from_state_dict can modify it</span></span><br><span class="line"></span><br><span class="line">metadata = <span class="built_in">getattr</span>(state_dict, <span class="string">&#x27;_metadata&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">state_dict = state_dict.copy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> metadata <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    state_dict._metadata = metadata</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">module, prefix=<span class="string">&#x27;&#x27;</span></span>):</span></span><br><span class="line"></span><br><span class="line">    local_metadata = &#123;&#125; <span class="keyword">if</span> metadata <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> metadata.get(prefix[:-<span class="number">1</span>], &#123;&#125;)</span><br><span class="line"></span><br><span class="line">    module._load_from_state_dict(</span><br><span class="line"></span><br><span class="line">        state_dict, prefix, local_metadata, <span class="literal">True</span>, missing_keys, unexpected_keys, error_msgs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, child <span class="keyword">in</span> module._modules.items():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">            load(child, prefix + name + <span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">load(model, prefix=<span class="string">&#x27;&#x27;</span> <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&#x27;bert&#x27;</span>) <span class="keyword">else</span> <span class="string">&#x27;bert.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(missing_keys) &gt; <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">&quot;Weights of &#123;&#125; not initialized from pretrained model: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line"></span><br><span class="line">        model.__class__.__name__, missing_keys))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(unexpected_keys) &gt; <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">&quot;Weights from pretrained model not used in &#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line"></span><br><span class="line">        model.__class__.__name__, unexpected_keys))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> tempdir:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Clean up temp dir</span></span><br><span class="line"></span><br><span class="line">    shutil.rmtree(tempdir)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这部分内容解释了如何提取模型的部分参数.  </p>
<p> <code>missing_keys</code> 这里是没有从预训练模型提取参数的部分，也就是 <code>classifier</code> <code>[&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]</code>层，因为这一层是分类任务独有的。  </p>
<p> <code>unexpected_keys</code> 则是对于分类任务不需要的，但是在预训练的语言模型中是存在的。查看 <code>BertForMaskedLM</code> 的模型就能看到，<code>cls</code> 层，是专属于语言模型的，在下游任务中都需要去掉。</p>
<p> 所以这部分代码实际上学到了如何选择预训练模型的部分参数～～棒啊！</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-09-24T03:06:40.000Z" title="2018/9/24 上午11:06:40">2018-09-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-29T08:12:09.159Z" title="2021/6/29 下午4:12:09">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/">language model</a></span><span class="level-item">13 minutes read (About 1992 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/">论文笔记-character embedding and ELMO</a></h1><div class="content"><ul>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1508.06615.pdf">Character-Aware Neural Language Models</a>  </p>
</li>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a></p>
</li>
</ul>
<h1 id="character-embedding"><a href="#character-embedding" class="headerlink" title="character embedding"></a>character embedding</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><blockquote>
<p>A language model is formalized as a probability distribution over a sequence of strings (words), and traditional methods usually involve making an n-th order Markov assumption and estimating n-gram probabilities via counting and subsequent smoothing (Chen and Goodman 1998). The count-based models are simple to train, but probabilities of rare n-grams can be poorly estimated due to data sparsity (despite smoothing techniques).  </p>
</blockquote>
<p>对语言模型的描述：语言模型是 一个单词序列的概率分布 的形式化描述（什么意思？就是比如这个句子长度为 10, 那么每个位置可能是词表中的任意一个词，而出现当前词是有一个概率的, 这个概率是依赖于之前的词的）。  </p>
<p>在传统的方法主要是运用 n阶马尔可夫假设来估计 n-gram 的概率，通过统计计数，以及子序列平滑的方式。这种基于计数的模型虽然简单，但是在数据稀疏的情况下，对不常见的 n-gram 的概率估计会很差。  </p>
<blockquote>
<p>While NLMs have been shown to outperform count-based n-gram language models (Mikolov et al. 2011), they are blind to subword information (e.g. morphemes). For example, they do not know, a priori, that eventful, eventfully, uneventful, and uneventfully should have structurally related embeddings in the vector space. Embeddings of rare words can thus be poorly estimated, leading to high perplexities for rare words (and words surrounding them). This is especially problematic in morphologically rich languages with long-tailed frequency distributions or domains with dynamic vocabularies (e.g. social media).  </p>
</blockquote>
<p>neural language models 将词嵌入到低维的向量中，使得语义相似的词在向量空间的位置也是相近的。然后 Mikolov word2vec 这种方式不能有效的解决子单词的信息问题，比如一个单词的各种形态，也不能认识前缀。这种情况下，不可避免的会造成不常见词的向量表示估计很差，对于不常见词会有较高的困惑度。这对于词语形态很丰富的语言是一个难题，同样这种问题也是动态词表的问题所在（比如社交媒体）。</p>
<h2 id="Recurrent-Neural-Network-Language-Model"><a href="#Recurrent-Neural-Network-Language-Model" class="headerlink" title="Recurrent Neural Network Language Model"></a>Recurrent Neural Network Language Model</h2><p>给定词表为 V，之前的序列是 $w_{1:t}=[w_1,..,w_t]$,在 RNN-LM 中通过全链接 affine transformation 计算 $w_{t+1}$ 个词的概率分布：  </p>
<p>$$Pr(w_{t+1}=j|w_{1:t})=\dfrac{exp(h_t\cdot p^j+q^j)}{\sum_{j’\in V}exp(h_t\cdot p^{j’}+q^{j’})}$$</p>
<p>其中 $h_t$ 是当前 t 时刻的隐藏状态。也就是先通过全链接映射到词表的 V 的维度，然后通过 softmax 计算其是词表中第 j 个词的概率。</p>
<p>然后假设训练预料库的 sentence 是 $w_{1:T}=[w_1,…,w_T]$,那么训练也就是最小化这个序列的 似然概率的负对数：</p>
<p>$$NLL=-\sum_{T}^{t=1}logPr(w_t|w_{1:t-1})$$</p>
<h2 id="Chracter-level-Convolution-Neural-Network"><a href="#Chracter-level-Convolution-Neural-Network" class="headerlink" title="Chracter-level Convolution Neural Network"></a>Chracter-level Convolution Neural Network</h2><p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/01.png"></p>
<p>以单词 absurdity 为例，有 l 个字符（通常会 padded 到一个固定size），通过 character embedding 映射成矩阵 $C\in R^{d\times l}$. d 是 embedding size. 图中 embedding size 为 4.</p>
<p>然后使用卷积核 kernel H 做卷积运算, $H\in R^{d\times w}$，所以得到的 feature map $f^k\in R^{l-w+1}$. 跟之前 CNN 做文本分类其实挺像的, kernel 的长是 embedding size d, 宽度 w 分别是 2,3,4. 上图中蓝色区域为例，filter 宽度为 2 的个数是3, 那么卷积得到的 featur map 是 $3 \times (9-2+1) = 3\times 8$.</p>
<p>$$f^k[i]=tanh(&lt;C^k[* ,i:i-w+1], H&gt; +b)$$</p>
<p>&lt;&gt;表示做卷积运算(Frobenius inner product). 然后加上 bias 和 非线性激活函数 tanh.</p>
<p>接着基于 times 维度做 max pooling. 上图中 filter 宽度为 3,2,4 的个数分别为 4,3,5.所以得到长度为 4+3+5=12 的向量。</p>
<p>这里每一个 filter matrix 得到一个相应的特征 feature. 在通常的 NLP 任务中这些 filter 的总数 $h\in[100, 1000]$</p>
<h2 id="Highway-Network"><a href="#Highway-Network" class="headerlink" title="Highway Network"></a>Highway Network</h2><p>通过卷积层得到单词 k 的向量表示为 $y^k$.</p>
<p>Highway Network 分为两层 layer.</p>
<ul>
<li>one layer of an MLP applies an affine transformation:</li>
</ul>
<p>$$z=g(W_y+b)$$</p>
<ul>
<li>one layer 有点类似 LSTM 中的 gate 机制：</li>
</ul>
<p>$$z=t\circ g(W_Hy+b_H)+(1-t)\circ y$$</p>
<p>其中  g 是非线性函数。$t=\sigma(W_Ty+b_T)$. t 成为 transform gate, (1-t) 是 carry gate. 同 LSTM 类似， highway network 允许输出能自适应的从 $y^k$ 中直接获取信息。</p>
<h1 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h1><p>传统的提取 word embedding 的方法，比如 word2vec 和 language model， 前者是通过词与词之间的共现，后者是 contextual，但他们都是获得固定的 embedding，也就是每一个词对应一个单一的 embedding.  而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练 language model, 而 word embedding 是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 且 ELMo 输出多个层的 embedding 表示, 试验中已经发现每层 LM 输出的信息对于不同的任务效果不同, 因此对每个 token 用不同层 embedding 表示会提升效果.</p>
<p>个人觉得，可以从这个角度去理解。RNN 可以看做一个高阶马尔可夫链，而不同于 马尔可夫模型，RNN 中的状态转移矩阵是用神经网络来模拟的，也就是我们计算隐藏层所用的 $h_t=tanh(w_{hh}h_{t-1}+w_{hx}x_t)$. 这个状态转移是动态的，也是不断更新的。而使用 语言模型 来训练 RNN/LSTM 目的就是得到这样的一套参数，使得它能学习到任何 合理的，自然的 sentence. 所以，这个语料库越大越好。事实上，有监督的训练也可以达到这个目的，但是有监督的数据有限，并且整个模型是有偏置的，比如文本分类的任务去训练，那么它更倾向于 局部信息。相比之下，机器翻译作为有监督的效果会更好，最好的还是语言模型呢，不仅可用的数据量很大，而且因为要预测每一个词的信息，它会努力结合每一个词的上下文去学习这个词的表示。这也正是我们需要的。ELMo 和 BERT 都是这样的道理，而 BERT 的优势前一篇 blog 说过了。</p>
<h2 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h2><p>给定 sentence $t_1, t_2,…,t_N$, 通过前面的词 $t_1,..,t_{k-1}$ 计算 token $t_k$ 的概率分布:</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/06.png"></p>
<p>反向：</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/02.png"></p>
<p>语言模型的训练就是采用极大似然估计，最大化这个概率：</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/03.png"></p>
<p>传统的方法就是 提取出对应位置的向量表示作为对应位置的词向量 context-independent token representation $x_k^{LM}$.</p>
<h2 id="ELMo-1"><a href="#ELMo-1" class="headerlink" title="ELMo"></a>ELMo</h2><blockquote>
<p>ELMo is a task specific combination of the intermediate layer representations in the biLM.</p>
</blockquote>
<p>ELMo 实际上只是下游任务的中间层，跟 BERT 一样。但也有不同的是， ELMo 每一层的向量表示会获得不同的 信息。底层更能捕捉 syntax and semantics 信息，更适用于 part-of-speech tagging 任务，高层更能获得 contextual 信息，更适用于 word sense disambiguation 任务。所以对不同的任务，会对不同层的向量表示的利用不同。</p>
<p><img src="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/05.png"></p>
<p>在使用 ELMo 进行下游有监督训练时，通常是这样 $[x_k; ELMo_k^{task}]$. 对于 SQuAD 这样的任务，$[h_k, ELMo_k^{task}]$.</p>
<h2 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h2><blockquote>
<p>The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second</p>
</blockquote>
<p>layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation.</p>
<p>具体模型还是得看代码。</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="潘小榭"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">潘小榭</p><p class="is-size-6 is-block">Blogging is happier than writing essays!</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">111</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-05T02:03:16.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-image-based contrastive learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:07:08.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:05:10.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-16T07:48:48.000Z">2021-04-16</time></p><p class="title"><a href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-12T08:16:35.000Z">2021-04-12</time></p><p class="title"><a href="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/">论文笔记-DETR and Deformable DETR</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">October 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">November 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESA/"><span class="tag">ESA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN%EF%BC%8CRL/"><span class="tag">GAN，RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MRC-and-QA/"><span class="tag">MRC and QA</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Translation/"><span class="tag">Machine Translation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/capsules/"><span class="tag">capsules</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-augmentation/"><span class="tag">data augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/open-set-recognition/"><span class="tag">open set recognition</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentence-embedding/"><span class="tag">sentence embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentiment-classification/"><span class="tag">sentiment classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language-recognition/"><span class="tag">sign language recognition</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-matching/"><span class="tag">text matching</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/transfer-learning/"><span class="tag">transfer learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="tag">数据结构与算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag">8</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("default");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>