<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>迁移学习系列 1-Neural Transfer Learning for NLP - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="迁移学习与监督学习的区别 training domain 和 target domain 不一致时，需要知识迁移。   那么暂时的问题来了？    1.如何界定 domain 的范围，尤其是NLP领域。从医学文本能迁移到科幻小说吗，感觉不可以。。    2.从 big domain 到 small domain 的迁移也是属于迁移学习的范畴吧？比如像 BERT 这样在超大的训练集上进行 train"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="迁移学习与监督学习的区别 training domain 和 target domain 不一致时，需要知识迁移。   那么暂时的问题来了？    1.如何界定 domain 的范围，尤其是NLP领域。从医学文本能迁移到科幻小说吗，感觉不可以。。    2.从 big domain 到 small domain 的迁移也是属于迁移学习的范畴吧？比如像 BERT 这样在超大的训练集上进行 train"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2019-03-04T01:09:08.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:08.504Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="transfer learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/"},"headline":"迁移学习系列 1-Neural Transfer Learning for NLP","image":["http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/01.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/02.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/03.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/04.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/05.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/06.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/07.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/08.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/09.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/10.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/11.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/12.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/14.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/16.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/17.png","http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/18.png"],"datePublished":"2019-03-04T01:09:08.000Z","dateModified":"2021-06-29T08:12:08.504Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"迁移学习与监督学习的区别 training domain 和 target domain 不一致时，需要知识迁移。   那么暂时的问题来了？    1.如何界定 domain 的范围，尤其是NLP领域。从医学文本能迁移到科幻小说吗，感觉不可以。。    2.从 big domain 到 small domain 的迁移也是属于迁移学习的范畴吧？比如像 BERT 这样在超大的训练集上进行 train"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-03-04T01:09:08.000Z" title="2019/3/4 上午9:09:08">2019-03-04</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.504Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/transfer-learning/">transfer learning</a></span><span class="level-item">10 分钟读完 (大约1552个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">迁移学习系列 1-Neural Transfer Learning for NLP</h1><div class="content"><h2 id="迁移学习与监督学习的区别"><a href="#迁移学习与监督学习的区别" class="headerlink" title="迁移学习与监督学习的区别"></a>迁移学习与监督学习的区别</h2><p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/01.png"></p>
<p>training domain 和 target domain 不一致时，需要知识迁移。  </p>
<p>那么暂时的问题来了？  </p>
<ul>
<li><p>1.如何界定 domain 的范围，尤其是NLP领域。从医学文本能迁移到科幻小说吗，感觉不可以。。  </p>
</li>
<li><p>2.从 big domain 到 small domain 的迁移也是属于迁移学习的范畴吧？比如像 BERT 这样在超大的训练集上进行 training，然后在小的子集上 fine-tune，都能表现的很好是吗？  </p>
</li>
</ul>
<h2 id="Why-transfer-learning"><a href="#Why-transfer-learning" class="headerlink" title="Why transfer learning"></a>Why transfer learning</h2><p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/02.png"></p>
<p>目前的监督模型依旧非常脆弱，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.07328">Jia and Liang, EMNLP 2017</a> 这篇 paper 证明了目前的 SOTA 的模型对对抗样本非常敏感。</p>
<p>迁移学习能解决这个问题吗，疑惑？？</p>
<blockquote>
<p><strong>Abstract：</strong>    </p>
</blockquote>
<p>Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.</p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/03.png"></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.02173">Synthetic and Natural Noise Both Break Neural Machine Translation, Belinkov and Bisk (ICLR 2018)</a> 这篇 paper 中提到基于字符级别的翻译模型能有效解决 OOV 等问题，但是却使得模型对 noise 非常敏感且脆弱。如果出现 phonetic 拼写错误，omission 省略， key swap 关键字母交换，都会导致 BLEU 值严重下降。</p>
<blockquote>
<p><strong>Abstract</strong>   </p>
</blockquote>
<p>Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.</p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/04.png"></p>
<p><a target="_blank" rel="noopener" href="http://aclweb.org/anthology/N18-1170">Iyyer et al. (NAACL 2018)</a> 这篇 paper 提出了一个句法规则控制下的释义生成模型，syntactically controlled paraphrase networks (SCPNs). 然后发现这样的对抗样本很容易愚弄训练好的监督模型。</p>
<blockquote>
<p><strong>Abstract：</strong>     </p>
</blockquote>
<p>We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoderdecoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled)paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) “fool” pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data</p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/05.png"></p>
<p>人工标注所有 domain 或者任何语言的数据是不可理的，因此需要 transfering knowledge from a related setting to the target setting.</p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/06.png"></p>
<p>NLP 很多重大的基础性的研究都可以看作是迁移学习的一种形式。  </p>
<ul>
<li><p>LSA  </p>
</li>
<li><p>Brown clusters  </p>
</li>
<li><p>word embedding  </p>
</li>
</ul>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/07.png"></p>
<p>已有工作的局限性：  </p>
<ul>
<li><p>限制度太高：预设定好的相似度指标，hard 参数共享  </p>
</li>
<li><p>条件设定太过于具体：单一的 task  </p>
</li>
<li><p>baseline 太弱：缺少与传统方法的对比  </p>
</li>
<li><p>模型脆弱：在 out-of-domain 不work，依赖于相似的语言/任务  </p>
</li>
<li><p>效率低：需要大量参数，时间和样本  </p>
</li>
</ul>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/08.png"></p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/09.png"></p>
<ul>
<li><p>迁移学习</p>
<ul>
<li><p>传导式迁移学习（相同的任务，只有sourced domain有label）  </p>
<ul>
<li><p>领域自适应（不同的 domain）  </p>
</li>
<li><p>跨语言学习（不同的 language）  </p>
</li>
</ul>
</li>
<li><p>归纳式迁移学习（不同的任务， target domain 也有标签）  </p>
<ul>
<li><p>多任务学习  </p>
</li>
<li><p>序列迁移学习  </p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>大佬太强了。。。。强到爆炸啊</p>
<h2 id="domain-adaptation"><a href="#domain-adaptation" class="headerlink" title="domain adaptation"></a>domain adaptation</h2><p>Propose two novel methods that bridge the domain discrepancy by selecting relevant and informative data for unsupervised domain adaptation.  </p>
<p>提出两方法，替无监督的域适应选择相关的，具有信息量的数据来弥合域之间的差异。</p>
<h3 id="Based-on-Bayesian-Optimisation"><a href="#Based-on-Bayesian-Optimisation" class="headerlink" title="Based on Bayesian Optimisation"></a>Based on Bayesian Optimisation</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.05246">Learning to select data for transfer learning with Bayesian Optimization, EMNLP2017</a></p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/10.png"></p>
<p>还不太懂 bayesian optimisation:  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29779000">https://zhuanlan.zhihu.com/p/29779000</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2017-08-18-5">https://www.jiqizhixin.com/articles/2017-08-18-5</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.02811">A Tutorial on Bayesian Optimization</a></p>
</li>
</ul>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/11.png"></p>
<h3 id="Using-semi-supervised-learning-and-multi-task-learning"><a href="#Using-semi-supervised-learning-and-multi-task-learning" class="headerlink" title="Using semi-supervised learning and multi-task learning"></a>Using semi-supervised learning and multi-task learning</h3><p><a target="_blank" rel="noopener" href="https://acl2018.org/paper/168/">Strong Baselines for Neural Semi-supervised Learning under Domain Shift, Ruder &amp; Plank, ACL 2018</a>  </p>
<blockquote>
<p>Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state-of-the-art for NLP. Hence classic approaches constitute an important and strong baseline.</p>
</blockquote>
<p>大佬的论文真的难。。太 hardcore 了。。</p>
<h2 id="cross-lingual-Learning"><a href="#cross-lingual-Learning" class="headerlink" title="cross-lingual Learning"></a>cross-lingual Learning</h2><p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/12.png"></p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/14.png"></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.03620.pdf">On the Limitations of Unsupervised Bilingual Dictionary Induction</a>  </p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.09334.pdf">A Discriminative Latent-Variable Model for Bilingual Lexicon Induction</a>  </p>
<p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/16.png"></p>
<h2 id="multi-task-learning"><a href="#multi-task-learning" class="headerlink" title="multi-task learning"></a>multi-task learning</h2><p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/17.png"></p>
<h2 id="sequential-transfer-learning"><a href="#sequential-transfer-learning" class="headerlink" title="sequential transfer learning"></a>sequential transfer learning</h2><p><img src="/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/18.png"></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>迁移学习系列 1-Neural Transfer Learning for NLP</p><p><a href="http://www.panxiaoxie.cn/2019/03/04/迁移学习系列-1-Neural-Transfer-Learning-for-NLP/">http://www.panxiaoxie.cn/2019/03/04/迁移学习系列-1-Neural-Transfer-Learning-for-NLP/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-03-04</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/transfer-learning/">transfer learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/03/06/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-2-Combining-semi-supervised-learning-with-transfer-learning/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">迁移学习系列-2-Combining semi-supervised learning with transfer learning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/02/28/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-0-NLP%20classification%20with%20transfer%20learning%20and%20weak%20supervision/"><span class="level-item">迁移学习系列-0-NLP classification with transfer learning and weak supervision</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2019/03/04/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-1-Neural-Transfer-Learning-for-NLP/';
            this.page.identifier = '2019/03/04/迁移学习系列-1-Neural-Transfer-Learning-for-NLP/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>