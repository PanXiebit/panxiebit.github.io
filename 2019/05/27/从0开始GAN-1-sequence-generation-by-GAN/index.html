<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>从0开始GAN-2-sequence generation by GAN - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="paper list Generating Sentences from a Continuous Space  GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution  Categorical Reparameterization with Gum-bel-Softmax  Deep Reinfor"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="paper list Generating Sentences from a Continuous Space  GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution  Categorical Reparameterization with Gum-bel-Softmax  Deep Reinfor"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2019-05-27T08:14:00.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:08.540Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="GAN"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/"},"headline":"从0开始GAN-2-sequence generation by GAN","image":["http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/gan_continuous.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/Reinforcement_learning_diagram.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/chat_bot.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/rl_based_e2e.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/max_excepted_reward.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/policy_gradient.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/rl_dilogue.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/chat_bot_gan.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/no_gradient.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/rl_in_dialogue.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/seqgan.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/max_reward.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/algorithm.png","http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/algorithm.png"],"datePublished":"2019-05-27T08:14:00.000Z","dateModified":"2021-06-29T08:12:08.540Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"paper list Generating Sentences from a Continuous Space  GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution  Categorical Reparameterization with Gum-bel-Softmax  Deep Reinfor"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-05-27T08:14:00.000Z" title="2019/5/27 下午4:14:00">2019-05-27</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.540Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/GAN/">GAN</a></span><span class="level-item">43 分钟读完 (大约6510个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">从0开始GAN-2-sequence generation by GAN</h1><div class="content"><h2 id="paper-list"><a href="#paper-list" class="headerlink" title="paper list"></a>paper list</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.06349">Generating Sentences from a Continuous Space</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.04051">GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01144">Categorical Reparameterization with Gum-bel-Softmax</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.01541v3.pdf">Deep Reinforcement Learning for Dialogue Generation</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661v1">Generative Adversarial Networks</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av9770302/?p=17">李宏毅老师讲seqGAN</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29168803">Role of RL in Text Generation by GAN(强化学习在生成对抗网络文本生成中扮演的角色)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ycy0706/article/details/80425091">好玩的文本生成</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1411.1784">Conditional Generative Adversarial Nets</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1605.05396">Generative Adversarial Text to Image Synthesis</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06547">Adversarial Learning for Neural Dialogue Generation</a>    </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.05101">How (not) to train your generative model: Scheduled sampling, likelihood, adversary?</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.05599">Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation</a>  </p>
</li>
</ul>
<h2 id="为什么GAN不适合文本生成"><a href="#为什么GAN不适合文本生成" class="headerlink" title="为什么GAN不适合文本生成"></a>为什么GAN不适合文本生成</h2><p>前面学过了GAN很自然的就会想到将GAN引入到文本生成中来，比如对话可以看作是conditional GAN, 但实际上却并不如想象中那样简单，原因是GAN只适用于连续数据的生成，对离散数据效果不佳。</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/gan_continuous.png"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29168803">Role of RL in Text Generation by GAN(强化学习在生成对抗网络文本生成中扮演的角色)</a> 这里面从两方面讲的很清楚:  </p>
<ul>
<li><p>sampling：从生成得到的softmax probability到one-hot向量，从而查询出对应index的词，这一步称为“sampling”，显然是不可微的。  </p>
</li>
<li><p>去掉sampling,将softmax probability和one-hot vector作为discriminator的输入，如果是discriminator是一个二分类器的话，判别器D很容易“作弊”，它根本不用去判断生成分布是否与真实分布更加接近，它只需要识别出给到的分布是不是除了一项是 1 ，其余都是 0 就可以了。因此，我们也可以想到用WGAN来解决这个问题。<a href>Improved Training of Wasserstein GANs</a>也给出了文本生成的实验，效果当然是好了很多，不至于直接崩了。</p>
</li>
</ul>
<p>但是WGAN为什么没那么好呢？将一个softmax probability强行拉倒一个one-hot vector真的可行吗？</p>
<h2 id="Gumbel-softmax，模拟Sampling的softmax"><a href="#Gumbel-softmax，模拟Sampling的softmax" class="headerlink" title="Gumbel-softmax，模拟Sampling的softmax"></a>Gumbel-softmax，模拟Sampling的softmax</h2><h2 id="RL-in-text-generation"><a href="#RL-in-text-generation" class="headerlink" title="RL in text generation"></a>RL in text generation</h2><h3 id="reinforcement-learning"><a href="#reinforcement-learning" class="headerlink" title="reinforcement learning"></a>reinforcement learning</h3><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> 和监督学习、非监督学习一起构成机器学习的三大范式。  </p>
<blockquote>
<p>Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.  </p>
</blockquote>
<p>It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between <strong>exploration</strong> (of uncharted territory) and <strong>exploitation</strong> (of current knowledge)</p>
<p>RL所适用的环境是一个典型的马尔科夫决策过程(Markov decision process,MDP)。所以强化学习实际上也可以看作是一种动态规划的方法。不过与传统的dynamic programming方法不同的是，RL不会假设MDP的精确数学模型的知识。我的理解是，在很多DP问题中，状态转移矩阵是已知的，但是RL所处理的问题，从一个状态到另一个状态，不是根据已有的知识，而是取决于当前action带来的reward以及未来的reward,所以这也就涉及到了 exploration 和 exploitation 的平衡问题。</p>
<p> Markov decision process 包括：GANs-in-NLP/Reinforcement_learning_diagram.png</p>
<ul>
<li><p>环境以及agent状态的集合 S;    </p>
</li>
<li><p>agent能采取的动作的集合 $A$  </p>
</li>
<li><p>状态之间转换的规则 $P_a(s,s’)=Pr(s_{t+1}=s’|s_t=s,a_t=a)$  </p>
</li>
<li><p>规定转换之后的即时奖励 $R_a(s,s’)$    </p>
</li>
<li><p>描述主体能够观察到什么的规则(这是啥玩意？？)</p>
</li>
</ul>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/Reinforcement_learning_diagram.png"></p>
<h4 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h4><p>将从头到尾所有的动作连在一起就称为一个“策略”或“策略路径” $pi$ ，强化学习的目标就是找出能够获得最多奖励的最优策略.  </p>
<p>$\pi: A\times S \rightarrow [0,1]$  </p>
<p>$\pi(a,s)=Pr(a_t=a|s_t=s)$</p>
<h4 id="state-value-function"><a href="#state-value-function" class="headerlink" title="state-value function"></a>state-value function</h4><p>状态-值函数 $V_{\pi}(s)$ 定义在当前状态 s下，按照策略 $\pi$ 接下来能获得的 reward.也就是说，given state s，当前以及未来的reward期望.  </p>
<p>$$V_{\pi}(s)=E[R]=E[\sum_{t=0}^{\infty}\gamma^tr_t|s_0=s]$$  </p>
<p>其中 $\gamma^t$ 是折扣因子，因为还是当前利益最重要嘛，所以未来的reward要打个折。</p>
<p>$$R=\sum_{t=0}^{\infty}\gamma^tr_t$$</p>
<h4 id="value-function"><a href="#value-function" class="headerlink" title="value function"></a>value function</h4><p>value funcion 和 state-value function 的区别是后者给定了一个 state. 而value function是计算给定任意初始状态，得到的reward.</p>
<p>$$V^{\pi}=E[R|s,\pi]$$</p>
<p>所以最优的 policy 实际上就是 value function 的期望最大。$\rho^{\pi}=E[V^{\pi}(S)]$， 其中状态S是从一个分布 $\mu$ 随机采样得到的。</p>
<p>尽管 state-value 足够定义最优 policy，再定义一个 action-value 也是很有用的。 given state s, action a, policy $\pi$, action-value:</p>
<p>$$Q^{\pi}(s,a)=E[R|s,a,\pi]$$</p>
<p>个人理解，在强化学习的应用场景中，很多时候是由 action 来确定下一个 state 的。所以 action-value 这个function会更实用吧。比如 text generation，sample当前词就是 action，然后才有下一个时刻的 state.</p>
<h4 id="Monte-Carlo-methods"><a href="#Monte-Carlo-methods" class="headerlink" title="Monte Carlo methods"></a>Monte Carlo methods</h4><h4 id="Temporal-difference-methods"><a href="#Temporal-difference-methods" class="headerlink" title="Temporal difference methods"></a>Temporal difference methods</h4><h3 id="RL应用到对话场景下"><a href="#RL应用到对话场景下" class="headerlink" title="RL应用到对话场景下"></a>RL应用到对话场景下</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.01541v3.pdf">Deep Reinforcement Learning for Dialogue Generation</a></p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/chat_bot.png"></p>
<p>对话生成任务本身非常符合强化学习的运行机理（让人类满意，拿奖励）。</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/rl_based_e2e.png"></p>
<p>输入句子是 h,模型返回的response是 x，其从人类得到的奖励是 $R(h,x)$. 基于RL的目标函数就是最大化对话的期望奖励。上图中 $p_{\theta}(x,h)$ 表示在 $\theta$ 参数下，一组对话 $(x,h)$ 出现的概率。$P(h)$ 表示出现句子 h 的概率。</p>
<p>最大化奖励期望：</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/max_excepted_reward.png"></p>
<p>$$公式(1)$$</p>
<ul>
<li><p>上式中 $h\sim P(h)$ 可以看作是均匀分布，所以 $E_{h\sim P(h)}\approx \dfrac{1}{N}$.  </p>
</li>
<li><p>其中 $E_{x\sim P_{\theta}(x|h)}$ 的计算无法考虑所有的对话，所以通过采样 $(h^1,x^1), (h^2,x^2), .., (h^N,x^N)$ 来计算。</p>
</li>
</ul>
<p>然后问题来了，我们需要优化的参数 $\theta$ 不见了，这怎么对 $\theta$ 进行求导呢？可以采用强化学习中常用的 policy gradient 进行变形：</p>
<p>$$\dfrac{dlog(f(x))}{dx}=\dfrac{1}{f(x)}\dfrac{df(x)}{dx}$$</p>
<p>适当变形后，对 $\theta$ 进行求导：  </p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/policy_gradient.png"></p>
<p>$$公式(2)$$</p>
<p>这样一来，梯度优化的重心就转化到了生成对话的概率上来，也就是说，通过对参数 $\theta$ 进行更新，奖励会使模型趋于将优质对话的出现概率提高，而惩罚则会让模型趋于将劣质对话的出现概率降低。</p>
<blockquote>
<p>自AlphaGo使得强化学习猛然进入大众视野以来，大部分对于强化学习的理论研究都将游戏作为主要实验平台，这一点不无道理，强化学习理论上的推导看似逻辑通顺，但其最大的弱点在于，基于人工评判的奖励 Reward 的获得，让实验人员守在电脑前对模型吐出来的结果不停地打分看来是不现实的，游戏系统恰恰能会给出正确客观的打分（输/赢 或 游戏Score）。基于RL的对话生成同样会面对这个问题，研究人员采用了类似AlphaGo的实现方式（AI棋手对弈）——同时运行两个机器人，让它们自己互相对话，同时，使用预训练（pre-trained）好的“打分器”给出每组对话的奖励得分 R(a^i, x^i) ，关于这个预训练的“打分器” R ，可以根据实际的应用和需求自己DIY。</p>
</blockquote>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/rl_dilogue.png"></p>
<h2 id="SeqGAN"><a href="#SeqGAN" class="headerlink" title="SeqGAN"></a>SeqGAN</h2><p>seqGAN对前面仅基于RL的对话生成进行了改进，也就是前面用pre-trained的打分器（或者是人类），用GAN中的判别器进行了代替。</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/chat_bot_gan.png"></p>
<p>这里问题在于生成得到的response x输入到判别器时，这个过程涉及到了sampling的操作，所以固定discriminator来更新generator时，梯度无法回流。</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/no_gradient.png"></p>
<p>这就需要RL的出现了。</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/rl_in_dialogue.png"></p>
<p>总结一下RL在这里面的作用：这里的discriminator得到的是reward。我们fix住判别器D来优化生成器 $\theta$ 的过程就变成了：生成器不再是原来的sample一个词，作为下一个time step的输入，因为这不可导。而是把当前time step作为一个state，然后采取action，这个action当然也是在词表中选一个词(用Monte Carlo Search). 以前是通过最大化似然概率（最小化交叉熵）来优化生成器，现在是寻找最优的 policy（最大化奖励期望）来优化生成器。而采用policy gradient可以将reward期望写成 $\theta$ 的连续函数，然后就可以根据最大化reward期望来优化 $\theta$,也就是梯度上升。</p>
<p>有了前面的基础再重新阅读<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.05473.pdf">seqGAN</a>这篇paper.</p>
<h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3><p>传统的GAN在序列生成的能力有限主要是两个原因：  </p>
<ol>
<li><p>无法处理离散的数据（前面已经讲过了）  </p>
</li>
<li><p>判别器D只能对完整的序列进行评价（原因是判别器就是基于完整的句子或dialogue进行训练的）。但是在序列生成的过程中，在生成部分序列的时候，对当前部分序列的评价也是很重要的。</p>
</li>
</ol>
<p>传统的基于 RNN/attention 的序列生成模型也存在 exposure bias 的问题，也就是训练阶段和inference阶段不一致的问题。在训练阶段是teacher forcing，而在infer阶段，下一个词的预测仅仅依赖于当前的隐藏状态（attention-based会有attention vector）. Bengio 的弟弟，另一个 Bengio 提出了 scheduled sampling 的方法，但这依然未能完全解决这个问题。</p>
<p>为此，作者提出基于RL的seqGAN。对序列生成的问题进行建模，把序列生成问题看作是马尔可夫决策过程(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.03504">Data generation as sequential decision making</a>)，从而转换成基于RL的寻找最优policy的问题，有效的解决了上述三个问题。</p>
<h3 id="Sequence-Generative-Adversarial-Nets"><a href="#Sequence-Generative-Adversarial-Nets" class="headerlink" title="Sequence Generative Adversarial Nets"></a>Sequence Generative Adversarial Nets</h3><p>这里先介绍一些数学符号：  </p>
<p>我们的目的是训练得到一个生成模型 $G_{\theta}$，使其能生成得到这样的一个序列 $Y_{1:T}=(y_1,…,y_t,…,y_T)$. 其中 $y_t\sim V$. V是候选词表。用RL来描述序列生成的过程就是：  </p>
<ol>
<li><p>当前时间步 t 的状态 state s: $(y_1,…,y_{t-1})$    </p>
</li>
<li><p>action a 是选择下一个 token $y_t$.    </p>
</li>
<li><p>policy也就是生成模型 $G_{\theta}(y_t|Y_{1:t-1})$    </p>
</li>
<li><p>状态的转移取决于 action a. 比如状态转移的概率 $\sigma_{s,s’}^a=1$，也就是在当前状态 $s=Y_{1:t-1}$ 情况下，下一个状态是 $s’$ 的概率为1，那么下一个状态是 $s’=Y_{1:t}$,对应的action也就是 $a=y_t$.</p>
</li>
</ol>
<p>首先我们需要训练一个判别模型 $D_{\phi}(Y_{1:T})$, 通过判断输入来自 real or fake 进行训练。而生成器的训练需要借助于判别器D的输出，也就是 reward.</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/seqgan.png"></p>
<h5 id="SeqGAN-via-Policy-Gradient"><a href="#SeqGAN-via-Policy-Gradient" class="headerlink" title="SeqGAN via Policy Gradient"></a>SeqGAN via Policy Gradient</h5><p>如果不考虑中间每一个时间步的奖励，也就是只考虑整个sentence的reward, 那么基于生成模型（policy）$G_{\theta}(y_t|Y_{1:t-1})$ 的最大奖励期望的函数是:</p>
<p>$$J(\theta)=E[R_T|s_0,\theta]=\sum_{y\sim V}G_{\theta}(y|s_0)\cdot Q_{D_{\phi}}^{G_{\theta}}(s_0,y)$$</p>
<p>其中 $R_T$ 是对整个sentence的奖励, $G_{\theta}(y|s_0)$ 是 given $s_0$,生成 $y$ 的概率，$Q_{D_{\phi}}^{G_{\theta}}(s_0,y )$ 是 action-value 函数，也就是 given $s_0$ 和 policy $G_{\theta}$ 后采取的 action 是 $y$ 时对应的 reward. 在这篇论文里面，reward 就是判别器判断生成的sentence为real的概率。</p>
<p>$$Q_{D_{\phi}}^{G_{\theta}}(a=y_T,s=Y_{1:T-1})=D_{\phi}(Y_{1:T})$$</p>
<p>但是对于序列生成问题，不能仅仅考虑完整的句子的reward，还要考虑到每一个 time step. 但是在每一个time step也不能贪心的只考虑当前最大的reward，还要考虑到未来的情况. 作者提出基于 Monte Carlo search 的方法。</p>
<blockquote>
<p>Monte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement. Monte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy ${\displaystyle \pi }$, the goal is to compute the function values ${\displaystyle Q^{\pi }(s,a)}$ (or a good approximation to them) for all state-action pairs ${\displaystyle (s,a)}$. Assuming (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair ${\displaystyle (s,a)}$ can be computed by averaging the sampled returns that originated from ${\displaystyle (s,a)}$ over time. Given sufficient time, this procedure can thus construct a precise estimate ${\displaystyle Q}$ of the action-value function ${\displaystyle Q^{\pi }}$. This finishes the description of the policy evaluation step.  </p>
</blockquote>
<p>policy iteration分为两个步骤，policy evaluation和policy improvement.蒙特卡洛被用在policy evaluation step中，给定一个静态的，判别型的policy $\pi$，其目标是计算</p>
<p>具体来说，在当前状态 $s=Y_{1:t}$ 下，基于一个 roll-out policy $G_{\beta}$ 生成剩下的 T-t 个tokens，这个过程重复 N 次.</p>
<p>$${Y_{1:T}^1,…,Y_{1:T}^N}=MC^{G_{\beta}}(Y_{1:t;N})$$</p>
<p>式子左边是 N 个完整的sentence。 对于 roll-out policy $G_{\beta}$ 作者在这篇 paper 中采用的与生成模型一样的 $G_{\theta}$. 如果追求速度的话，可以选择更简单的策略。</p>
<p>这样基于 Monte Carlo method 就能计算每一个 time step 的能考虑到 future 的reward.</p>
<p>$$Q_{D_{\phi}}^{G_{\theta}}(s=Y_{1:t-1}, a=y_t)=</p>
<p>\begin{cases}</p>
<p>\dfrac{1}{N}\sum_{n=1}^ND_{\phi}(Y_{1:T}^n),Y_{1:T}^n \sim MC^{G_{\beta}}(Y_{1:t;N}), \quad \text{for t &lt; T}\</p>
<p>D_{\phi}(Y_{1:t}),\quad\text{for t = T}</p>
<p>\end{cases}\quad (4)$$</p>
<p>公式还是比较好理解的。所以事实上判别器 $D_{\phi}$ 依旧是只能判断完整的sentence，但是在每一个 time step 可以借助于 roll-out policy 来得到完整的sentence，进而对当前 action 进行评分，计算得到 $a=y_t$ 的reward。</p>
<p>知道了如何计算reward，就可以利用最大化这个奖励期望来优化我们的生成器（policy $G_{\theta}$）.对 $\theta$ 求导:</p>
<p>$$\nabla J(\theta)=\sum_{t=1}^T\mathbb{E}<em>{Y</em>{1:t-1}\sim G_{\theta}}[\sum_{y_t\sim V}\nabla_{\theta}G_{\theta}({y_t|Y_{1:t-1}})\cdot Q_{D_{\phi}}^{G_{\theta}}(Y_{1:t-1},y_t)]\quad\text{公式(3)}$$</p>
<p>公式（3）与前面李弘毅老师讲的公式（2）是一致的，只不过这里考虑的中间 reward.上式中 $E_{Y_{1:t-1}\sim G_{\theta}}[\cdot]$ 等同于前面提到的 $E_{x\sim P_{\theta}(x|h)}$ 都是通过sample 来计算的。同样 reward 的计算式 $Q_{D_{\phi}}^{G_{\theta}}(Y_{1:t-1},y_t)$ 也是不包含生成器的参数 $\theta$ 的。</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/max_reward.png"></p>
<p>上述公式中 $\sum_{y_t\sim V}\sim G_{\theta}(y_t|Y_{1:t-1})$</p>
<p>然后基于梯度上升来优化参数 $\theta$.</p>
<p>$$\theta \leftarrow \theta + \alpha_h\nabla J(\theta)\quad(8)$$</p>
<p>作者建议使用 Adam 或 RMSprop 优化算法。</p>
<p>除了生成器的优化，这里的判别器D是动态的。这样相比传统基于pre-train的判别器会更叼吧。优化判别器的目标函数是：</p>
<p>$$\min_{\phi}-\mathbb{E}<em>{Y\sim p</em>{data}}[logD_{\phi}(Y)]-\mathbb{E}<em>{Y\sim G</em>{\theta}}[log(1-D_{\phi}(Y))]\quad(5)$$</p>
<p>具体的算法步骤是：</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/algorithm.png"></p>
<blockquote>
<p>And to reduce the vari- ability of the estimation, we use different sets of negative samples combined with positive ones, which is similar to bootstrapping (Quinlan 1996)</p>
</blockquote>
<h3 id="The-Generative-Model-for-Sequences"><a href="#The-Generative-Model-for-Sequences" class="headerlink" title="The Generative Model for Sequences"></a>The Generative Model for Sequences</h3><p>作者使用基于 LSTM 的生成器G。</p>
<p>$$h_t=g(h_{t-1},x_t)$$</p>
<p>$$p(y_t|x_1,…,x_t)=z(h_t)=softmax(c+Vh_t)$$</p>
<h3 id="The-Discriminative-Model-for-Sequences"><a href="#The-Discriminative-Model-for-Sequences" class="headerlink" title="The Discriminative Model for Sequences"></a>The Discriminative Model for Sequences</h3><p>作者使用基于 CNN 的判别器，用来预测一个sentence为real的概率。</p>
<h3 id="一些细节-一些延伸"><a href="#一些细节-一些延伸" class="headerlink" title="一些细节 + 一些延伸"></a>一些细节 + 一些延伸</h3><p>到目前为止，基本理解了seqGAN的大部分细节，需要看看源码消化下。  </p>
<p>接下来会有更多的细节和改进可先参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29168803">Role of RL in Text Generation by GAN(强化学习在生成对抗网络文本生成中扮演的角色)</a></p>
<h3 id="seagan-代码学习"><a href="#seagan-代码学习" class="headerlink" title="seagan 代码学习"></a>seagan 代码学习</h3><h4 id="TensorArray-和-基于lstm的MDP模拟文本生成"><a href="#TensorArray-和-基于lstm的MDP模拟文本生成" class="headerlink" title="TensorArray 和 基于lstm的MDP模拟文本生成"></a>TensorArray 和 基于lstm的MDP模拟文本生成</h4><p>这也是seqgan的核心，用Monte Carlo search代替sampling来选择next token.在看具体代码之前先了解下 tensorarray.</p>
<h4 id="TensorArray"><a href="#TensorArray" class="headerlink" title="TensorArray"></a>TensorArray</h4><blockquote>
<p>Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays</p>
</blockquote>
<p>This class is meant to be used with dynamic iteration primitives such as while_loop and map_fn. It supports gradient back-propagation via special “flow” control flow dependencies.</p>
<p>一个封装了动态大小、per-time-step 写入一次的 tensor数组的类。在序列生成中，序列的长度通常是不定的，所以会需要使用动态tensorarray.</p>
<h5 id="类初始化"><a href="#类初始化" class="headerlink" title="类初始化"></a>类初始化</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               size=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dynamic_size=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               clear_after_read=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               tensor_array_name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               handle=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               flow=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               infer_shape=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               element_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               colocate_with_first_write_call=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>size: int32 scalar <code>Tensor</code>, 动态数组的大小</p>
</li>
<li><p>dynamic_size: Python bool, 是否可以增长，默认false</p>
</li>
</ul>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ul>
<li>stack</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stack</span>(<span class="params">self, name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Return the values in the TensorArray as a stacked `Tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将动态数组 stack 起来，得到最终的 tensor.</p>
<ul>
<li>concat</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat</span>(<span class="params">self, name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Return the values in the TensorArray as a concatenated `Tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将动态数组 concat 起来，得到最终的 tensor.</p>
<ul>
<li>read  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span>(<span class="params">self, index, name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Read the value at location `index` in the TensorArray.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  读过一次之后会清0. 不能读第二次。但可以再次写入之后。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>write  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span>(<span class="params">self, index, value, name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Write `value` into index `index` of the TensorArray.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  - index: int32 scalar <span class="keyword">with</span> the index to write to.</span><br><span class="line"></span><br><span class="line">  - value: tf.Tensor</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>gather  </p>
</li>
<li><p>unstack  </p>
</li>
<li><p>split  </p>
</li>
<li><p>scatter  </p>
</li>
</ul>
<h5 id="tf-while-loop"><a href="#tf-while-loop" class="headerlink" title="tf.while_loop"></a>tf.while_loop</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">while_loop_v2</span>(<span class="params">cond,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  body,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  loop_vars,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  shape_invariants=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  parallel_iterations=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  back_prop=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  swap_memory=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  maximum_iterations=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                  name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;Repeat `body` while the condition `cond` is true.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">- cond: <span class="built_in">callable</span>, <span class="keyword">return</span> boolean scalar tensor. 参数个数必须和 loop_vars 一致。  </span><br><span class="line"></span><br><span class="line">- body: vallable. 循环执行体，参数个数必须和 loop_vars 一致.</span><br><span class="line"></span><br><span class="line">- loop_vars: 循环变量，<span class="built_in">tuple</span>, namedtuple <span class="keyword">or</span> <span class="built_in">list</span> of numpy array.</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="example"><a href="#example" class="headerlink" title="example:"></a>example:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">matrix = tf.random.normal(shape=[<span class="number">5</span>, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">sequence_length = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">gen_o = tf.TensorArray(dtype=tf.float32, size=sequence_length,</span><br><span class="line"></span><br><span class="line">                       dynamic_size=<span class="literal">False</span>, infer_shape=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">init_state = (<span class="number">0</span>, gen_o)</span><br><span class="line"></span><br><span class="line">condition = <span class="keyword">lambda</span> i, _: i &lt; sequence_length</span><br><span class="line"></span><br><span class="line">body = <span class="keyword">lambda</span> i, gen_o : (i+<span class="number">1</span>, gen_o.write(i, matrix[i] * <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">n, gen_o = tf.while_loop(condition, body, init_state)</span><br><span class="line"></span><br><span class="line">gen_o_stack = gen_o.stack()</span><br><span class="line"></span><br><span class="line">gen_o_concat = gen_o.concat()用 LSTM 模拟马尔科夫决策过程</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o)                     <span class="comment"># TensorArray object</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o_stack)               <span class="comment"># tf.Tensor(), [5,]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o_concat)              <span class="comment"># tf.Tensor(), [5,1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o.read(<span class="number">3</span>))             <span class="comment"># -0.22972003, tf.Tensor  读过一次就被清0了</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o.write(<span class="number">3</span>, tf.constant([<span class="number">0.22</span>], dtype=tf.float32)))  <span class="comment"># TensorArray object</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o.concat())            <span class="comment"># tf.Tensor([-2.568663 0.09471891 1.2042408 0.22 0.2832177 ], shape=(5,), dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o.read(<span class="number">3</span>))             <span class="comment"># tf.Tensor([0.22], shape=(1,), dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gen_o.read(<span class="number">3</span>))             <span class="comment"># Could not read index 3 twice because it was cleared after a previous read</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="用-LSTM-模拟马尔科夫决策过程"><a href="#用-LSTM-模拟马尔科夫决策过程" class="headerlink" title="用 LSTM 模拟马尔科夫决策过程"></a>用 LSTM 模拟马尔科夫决策过程</h4><ul>
<li><p>current time t state: $(y_1,…,y_t)$. 但是马尔科夫决策过程的原理告诉我们<strong>一旦当前状态确定后，所有的历史信息都可以扔掉了。这个状态足够去预测 future.</strong> 所以在LSTM里面就是隐藏状态 $h_{t-1}$. 以及当前可观测信息 $x_t$.  </p>
</li>
<li><p>action a: 选择 next token $y_t$.</p>
</li>
<li><p>policy: $G_{\theta}(y_t|Y_{1:t-1})$. 也就是生成next token的策略。下面代码的方法 $o_t \rightarrow log(softmax(o_t))$. 然后基于这个 log-prob 的分布进行 sample. 问题是这个过程不可导呀？  </p>
</li>
</ul>
<h5 id="generator"><a href="#generator" class="headerlink" title="generator"></a>generator</h5><p>这是生成器生成sample的过程，初始状态是 $h_0$.</p>
<p>g_recurrence 就是step-by-step的过程，next_token是通过tf.multinomial采样得到的，其采样的distribution是 log_prob [tf.log(tf.nn.softmax(o_t))]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  self.h0 = tf.zeros([self.batch_size, self.hidden_dim])</span><br><span class="line"></span><br><span class="line">  self.h0 = tf.stack([self.h0, self.h0])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># define variables</span></span><br><span class="line"></span><br><span class="line">  self.g_embeddings = tf.Variable(self.init_matrix([self.vocab_size, self.emb_dim]))</span><br><span class="line"></span><br><span class="line">  self.g_params.append(self.g_embeddings)</span><br><span class="line"></span><br><span class="line">  self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  <span class="comment"># maps h_&#123;t-1&#125; to h_t for generator</span></span><br><span class="line"></span><br><span class="line">  self.g_output_unit = self.create_output_unit(self.g_params)  <span class="comment"># maps h_t to o_t (output token logits)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_unsuper_generate</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">      <span class="string">&quot;&quot;&quot; unsupervised generate. using in rollout policy.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :return: 生成得到的 token index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :param input_x:  [batch, seq_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :param rewards:  [batch, seq_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">      gen_o = tf.TensorArray(dtype=tf.float32, size=self.sequence_length,</span><br><span class="line"></span><br><span class="line">                             dynamic_size=<span class="literal">False</span>, infer_shape=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length,</span><br><span class="line"></span><br><span class="line">                             dynamic_size=<span class="literal">False</span>, infer_shape=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_g_recurrence</span>(<span class="params">i, x_t, h_tm1, gen_o, gen_x</span>):</span></span><br><span class="line"></span><br><span class="line">          h_t = self.g_recurrent_unit(x_t, h_tm1)  <span class="comment"># hidden_memory_tuple</span></span><br><span class="line"></span><br><span class="line">          o_t = self.g_output_unit(h_t)  <span class="comment"># [batch, vocab] , logits not prob</span></span><br><span class="line"></span><br><span class="line">          log_prob = tf.log(tf.nn.softmax(o_t))</span><br><span class="line"></span><br><span class="line">          <span class="comment">#tf.logging.info(&quot;unsupervised generated log_prob:&#123;&#125;&quot;.format(log_prob[0]))</span></span><br><span class="line"></span><br><span class="line">          next_token = tf.cast(tf.reshape(tf.multinomial(logits=log_prob, num_samples=<span class="number">1</span>),</span><br><span class="line"></span><br><span class="line">                                          [self.batch_size]), tf.int32)</span><br><span class="line"></span><br><span class="line">          x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  <span class="comment"># [batch, emb_dim]</span></span><br><span class="line"></span><br><span class="line">          gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.vocab_size, <span class="number">1.0</span>, <span class="number">0.0</span>),</span><br><span class="line"></span><br><span class="line">                                                           tf.nn.softmax(o_t)), <span class="number">1</span>))  <span class="comment"># [batch_size] , prob</span></span><br><span class="line"></span><br><span class="line">          gen_x = gen_x.write(i, next_token)  <span class="comment"># indices, batch_size</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">return</span> i + <span class="number">1</span>, x_tp1, h_t, gen_o, gen_x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      _, _, _,  <span class="function"><span class="keyword">def</span> <span class="title">_super_generate</span>(<span class="params">self, input_x</span>):</span></span><br><span class="line"></span><br><span class="line">      <span class="string">&quot;&quot;&quot; supervised generate.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :param input_x:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :return: 生成得到的是 probability [batch * seq_len, vocab_size]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.device(<span class="string">&quot;/cpu:0&quot;</span>):</span><br><span class="line"></span><br><span class="line">          self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, input_x),</span><br><span class="line"></span><br><span class="line">                                          perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])  <span class="comment"># [seq_len, batch_size, emb_dim]</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># supervised pretraining for generator</span></span><br><span class="line"></span><br><span class="line">      g_predictions = tf.TensorArray(</span><br><span class="line"></span><br><span class="line">          dtype=tf.float32, size=self.sequence_length,</span><br><span class="line"></span><br><span class="line">          dynamic_size=<span class="literal">False</span>, infer_shape=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      ta_emb_x = tf.TensorArray(</span><br><span class="line"></span><br><span class="line">          dtype=tf.float32, size=self.sequence_length)</span><br><span class="line"></span><br><span class="line">      ta_emb_x = ta_emb_x.unstack(self.processed_x) self.gen_o, self.gen_x = tf.while_loop(</span><br><span class="line"></span><br><span class="line">          cond=<span class="keyword">lambda</span> i, _1, _2, _3, _4: i &lt; self.sequence_length,</span><br><span class="line"></span><br><span class="line">          body=_g_recurrence,</span><br><span class="line"></span><br><span class="line">          loop_vars=(tf.constant(<span class="number">0</span>, dtype=tf.int32),</span><br><span class="line"></span><br><span class="line">                     tf.nn.embedding_lookup(self.g_embeddings, self.start_token),</span><br><span class="line"></span><br><span class="line">                     self.h0, gen_o, gen_x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      self.gen_x = self.gen_x.stack()  <span class="comment"># [seq_length, batch_size]</span></span><br><span class="line"></span><br><span class="line">      self.gen_x = tf.transpose(self.gen_x, perm=[<span class="number">1</span>, <span class="number">0</span>])  <span class="comment"># [batch_size, seq_length]</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> self.gen_x</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>所以是通过monte carlo的形式生成fake sample，作为discriminator的输入吗？那这个过程也不可导呀。其实不是这样的。我们再看对抗学习中更新generator的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_reward_train_step</span>(<span class="params">x_batch, rewards</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line"></span><br><span class="line">        g_loss = generator._get_generate_loss(x_batch, rewards)</span><br><span class="line"></span><br><span class="line">        g_gradients, _ = tf.clip_by_global_norm(</span><br><span class="line"></span><br><span class="line">            tape.gradient(g_loss, generator.trainable_variables), clip_norm=<span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">        g_optimizer.apply_gradients(<span class="built_in">zip</span>(g_gradients, generator.trainable_variables))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> g_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.logging.info(<span class="string">&quot;------------------ 6. start Adversarial Training...--------------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> total_batch <span class="keyword">in</span> <span class="built_in">range</span>(TOTAL_BATCH):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># fix discriminator, and train the generator for one step</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">        samples = generator._unsuper_generate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#tf.logging.info(&quot;unsuper generated samples:&#123;&#125;&quot;.format(samples[0]))</span></span><br><span class="line"></span><br><span class="line">        rewards = rollout.get_reward(samples, rollout_num=<span class="number">2</span>, discriminator=discriminator)  <span class="comment"># 基于 monte carlo 采样16，计算并累计 reward.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#tf.logging.info(&quot;reward:&#123;&#125;&quot;.format(rewards[0]))</span></span><br><span class="line"></span><br><span class="line">        gen_reward_train_step(samples, rewards)        <span class="comment"># update generator.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update roll-out parameters</span></span><br><span class="line"></span><br><span class="line">    rollout.update_params()   <span class="comment"># update roll-out policy.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>这儿采用的是 <code>generator._get_generate_loss</code>， 所以它对generator的参数都是可导的吗？ 我们再看这个生成器中这个function的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_super_generate</span>(<span class="params">self, input_x</span>):</span></span><br><span class="line"></span><br><span class="line">      <span class="string">&quot;&quot;&quot; supervised generate.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :param input_x:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      :return: 生成得到的是 probability [batch * seq_len, vocab_size]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.device(<span class="string">&quot;/cpu:0&quot;</span>):</span><br><span class="line"></span><br><span class="line">          self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, input_x),</span><br><span class="line"></span><br><span class="line">                                          perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])  <span class="comment"># [seq_len, batch_size, emb_dim]</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># supervised pretraining for generator</span></span><br><span class="line"></span><br><span class="line">      g_predictions = tf.TensorArray(</span><br><span class="line"></span><br><span class="line">          dtype=tf.float32, size=self.sequence_length,</span><br><span class="line"></span><br><span class="line">          dynamic_size=<span class="literal">False</span>, infer_shape=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      ta_emb_x = tf.TensorArray(</span><br><span class="line"></span><br><span class="line">          dtype=tf.float32, size=self.sequence_length)</span><br><span class="line"></span><br><span class="line">      ta_emb_x = ta_emb_x.unstack(self.processed_x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pretrain_recurrence</span>(<span class="params">i, x_t, h_tm1, g_predictions</span>):</span></span><br><span class="line"></span><br><span class="line">        h_t = self.g_recurrent_unit(x_t, h_tm1)</span><br><span class="line"></span><br><span class="line">        o_t = self.g_output_unit(h_t)</span><br><span class="line"></span><br><span class="line">        g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  <span class="comment"># [batch, vocab_size]</span></span><br><span class="line"></span><br><span class="line">        x_tp1 = ta_emb_x.read(i)                                    <span class="comment"># supervised learning, teaching forcing.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> i + <span class="number">1</span>, x_tp1, h_t, g_predictions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    _, _, _, self.g_predictions = tf.while_loop(</span><br><span class="line"></span><br><span class="line">        cond=<span class="keyword">lambda</span> i, _1, _2, _3: i &lt; self.sequence_length,</span><br><span class="line"></span><br><span class="line">        body=_pretrain_recurrence,</span><br><span class="line"></span><br><span class="line">        loop_vars=(tf.constant(<span class="number">0</span>, dtype=tf.int32),</span><br><span class="line"></span><br><span class="line">                   tf.nn.embedding_lookup(self.g_embeddings, self.start_token),</span><br><span class="line"></span><br><span class="line">                   self.h0, g_predictions))</span><br><span class="line"></span><br><span class="line">    self.g_predictions = tf.transpose(self.g_predictions.stack(),</span><br><span class="line"></span><br><span class="line">                                      perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])  <span class="comment"># [batch_size, seq_length, vocab_size]</span></span><br><span class="line"></span><br><span class="line">    self.g_predictions = tf.clip_by_value(</span><br><span class="line"></span><br><span class="line">        tf.reshape(self.g_predictions, [-<span class="number">1</span>, self.vocab_size]), <span class="number">1e-20</span>, <span class="number">1.0</span>)  <span class="comment"># [batch_size*seq_length, vocab_size]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.g_predictions       <span class="comment"># [batch_size*seq_length, vocab_size]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_generate_loss</span>(<span class="params">self, input_x, rewards</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input_x: [batch, seq_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param rewards: [batch, seq_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        self.g_predictions = self._super_generate(input_x)</span><br><span class="line"></span><br><span class="line">        real_target = tf.one_hot(</span><br><span class="line"></span><br><span class="line">            tf.to_int32(tf.reshape(input_x, [-<span class="number">1</span>])),</span><br><span class="line"></span><br><span class="line">            depth=self.vocab_size, on_value=<span class="number">1.0</span>, off_value=<span class="number">0.0</span>)  <span class="comment"># [batch_size * seq_length, vocab_size]</span></span><br><span class="line"></span><br><span class="line">        self.pretrain_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_target,</span><br><span class="line"></span><br><span class="line">                                                                     logits=self.g_predictions)  <span class="comment"># [batch * seq_length]</span></span><br><span class="line"></span><br><span class="line">        self.g_loss = tf.reduce_mean(self.pretrain_loss * tf.reshape(rewards, [-<span class="number">1</span>]))  <span class="comment"># scalar</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.g_loss</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>所以seqgan的作者是怎么做的呢，利用 <code>generator._unsuper_generate</code>先生成fake sample，然后再利用 <code>generator._super_generate</code> 得到 <code>g_predictions</code>, 将fake sample作为 <code>real_target</code> 与 <code>g_predictions</code> 做交叉熵求出 <code>pretrain_loss</code>，然后乘以每一个token对应的rewards得到最终的loss. 这个过程是可导的。</p>
<blockquote>
<p>通常情况下Monte carlo方法在里面的作用其实就是 collect data. collecting data的过程用到了policy,然后基于reward对policy进行求导。  </p>
</blockquote>
<p>但是seqgan的作者在代码中呈现的是另一种trick. 先用generator生成fake样本，然后用rollout policy对该样本进行打分reward.这里并不是直接对reward求导，而是把fake样本作为target进行MLE训练，得到pretrain_loss，reward作为权重乘以pretrain_loss作为最终的损失函数。</p>
<h5 id="roll-policy"><a href="#roll-policy" class="headerlink" title="roll-policy"></a>roll-policy</h5><p>这个过程比较容易理解，对于给定的 given_num,小于 given_num 的直接 copy，但是 $h_t$ 的计算依旧。大于 given_num 的token采用 <code>generate._unsuper_generate</code>.</p>
<h3 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h3><p>看了代码总觉得代码写得与论文有出入。</p>
<p><img src="/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/algorithm.png"></p>
<p>基于policy gradient来更新policy(generator)，按照公式应该是直接对rewards求导才对吧。基于Monte carlo采样的过程可以看作是sample不同的样本，是一种近似模拟 $o_t$ 分布的方法，是不要求可导的。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>从0开始GAN-2-sequence generation by GAN</p><p><a href="http://www.panxiaoxie.cn/2019/05/27/从0开始GAN-1-sequence-generation-by-GAN/">http://www.panxiaoxie.cn/2019/05/27/从0开始GAN-1-sequence-generation-by-GAN/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-05-27</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/GAN/">GAN</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/06/22/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-3-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90planning/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">从0开始GAN-3-文本生成planning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/05/18/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-from-GAN-to-WGAN/"><span class="level-item">从0开始GAN-1-from-GAN-to-WGAN</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2019/05/27/%E4%BB%8E0%E5%BC%80%E5%A7%8BGAN-1-sequence-generation-by-GAN/';
            this.page.identifier = '2019/05/27/从0开始GAN-1-sequence-generation-by-GAN/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">116</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-12T10:38:37.000Z">2021-09-12</time></p><p class="title"><a href="/2021/09/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Discrete-Latent-Variables-Based-Generation/">论文笔记-Discrete Latent Variables Based Generation</a></p><p class="categories"><a href="/categories/generation/">generation</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-22T03:28:02.000Z">2021-08-22</time></p><p class="title"><a href="/2021/08/22/leetcode/">leetcode</a></p><p class="categories"><a href="/categories/leetcode/">leetcode</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-23T11:13:20.000Z">2021-07-23</time></p><p class="title"><a href="/2021/07/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Clip/">论文笔记-Clip!!!</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/">vision-language</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-11T07:58:30.000Z">2021-07-11</time></p><p class="title"><a href="/2021/07/11/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-sign-language-recognition-and-translation/">论文笔记-sign language recognition and translation</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/">computer vision</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-02T04:37:58.000Z">2021-07-02</time></p><p class="title"><a href="/2021/07/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-constrast-learning-in-NLP/">论文笔记-constrast learning in NLP</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/">constrast learning</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">八月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">七月 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">四月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">九月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">六月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">六月 2018</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">四月 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESA/"><span class="tag">ESA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN%EF%BC%8CRL/"><span class="tag">GAN，RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MRC-and-QA/"><span class="tag">MRC and QA</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Translation/"><span class="tag">Machine Translation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/capsules/"><span class="tag">capsules</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-augmentation/"><span class="tag">data augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/generation/"><span class="tag">generation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/leetcode/"><span class="tag">leetcode</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/open-set-recognition/"><span class="tag">open set recognition</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentence-embedding/"><span class="tag">sentence embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentiment-classification/"><span class="tag">sentiment classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language/"><span class="tag">sign language</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language-recognition/"><span class="tag">sign language recognition</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-matching/"><span class="tag">text matching</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/transfer-learning/"><span class="tag">transfer learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-language/"><span class="tag">vision-language</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="tag">数据结构与算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag">8</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>