<!doctype html>
<html lang="de"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tensorflow Attention API 源码阅读1 - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="潘小榭"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘小榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="这节内容是详细了解下 tensorflow 关于 attention 的api，由于封装的太好，之前使用过发现报错了很难去找出哪儿 bug，所以用 eager execution 来看具体细节。 按照官方教程 Seq2seq Library (contrib) 这里的流程逐步深入。 This library is composed of two primary components:    New"><meta property="og:type" content="blog"><meta property="og:title" content="Tensorflow Attention API 源码阅读1"><meta property="og:url" content="http://www.panxiaoxie.cn/2018/09/01/tensorflow-Attention-API/"><meta property="og:site_name" content="潘小榭"><meta property="og:description" content="这节内容是详细了解下 tensorflow 关于 attention 的api，由于封装的太好，之前使用过发现报错了很难去找出哪儿 bug，所以用 eager execution 来看具体细节。 按照官方教程 Seq2seq Library (contrib) 这里的流程逐步深入。 This library is composed of two primary components:    New"><meta property="og:image" content="https://panxiaoxie.cn/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"><meta property="article:published_time" content="2018-09-01T13:17:28.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:08.539Z"><meta property="article:author" content="Xie Pan"><meta property="article:tag" content="Tensorflow"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://panxiaoxie.cn/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/09/01/tensorflow-Attention-API/"},"headline":"Tensorflow Attention API 源码阅读1","image":["https://panxiaoxie.cn/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"],"datePublished":"2018-09-01T13:17:28.000Z","dateModified":"2021-06-29T08:12:08.539Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/logo.svg"}},"description":"这节内容是详细了解下 tensorflow 关于 attention 的api，由于封装的太好，之前使用过发现报错了很难去找出哪儿 bug，所以用 eager execution 来看具体细节。 按照官方教程 Seq2seq Library (contrib) 这里的流程逐步深入。 This library is composed of two primary components:    New"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/09/01/tensorflow-Attention-API/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Suche" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Gepostet vor&nbsp;<time dateTime="2018-09-01T13:17:28.000Z" title="2018/9/1 下午9:17:28">2018-09-01</time></span><span class="level-item">Aktualisiert vor&nbsp;<time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/TensorFlow/">TensorFlow</a></span><span class="level-item">33 minutes lesen (Über 4894 Wörter)</span></div></div><h1 class="title is-3 is-size-4-mobile">Tensorflow Attention API 源码阅读1</h1><div class="content"><p>这节内容是详细了解下 tensorflow 关于 attention 的api，由于封装的太好，之前使用过发现报错了很难去找出哪儿 bug，所以用 eager execution 来看具体细节。</p>
<p>按照官方教程 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_guides/python/contrib.seq2seq#Attention">Seq2seq Library (contrib)</a> 这里的流程逐步深入。</p>
<p>This library is composed of two primary components:  </p>
<ul>
<li>New attention wrappers for tf.contrib.rnn.RNNCell objects.</li>
</ul>
<ul>
<li>A new object-oriented dynamic decoding framework.  </li>
</ul>
<p>主要包括两个部分，一个是新的基于 attention 的 RNNCell 对象，一个面向对象的动态解码框架。</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention wrappers are RNNCell objects that wrap other RNNCell objects and implement attention. The form of attention is determined by a subclass of tf.contrib.seq2seq.AttentionMechanism. These subclasses describe the form of attention (e.g. additive vs. multiplicative) to use when creating the wrapper. An instance of an AttentionMechanism is constructed with a memory tensor, from which lookup keys and values tensors are created.</p>
<p>attenion wrapper 也是 RNNCell 对象，父类是 tf.contrib.seq2seq.AttentionMechanism,然后其子类是针对不同 attention 形式（additive vs. multiplicative）的实现。AttentionMechanism 的构造是在 memory 的基础上，memory 也就是 attention 过程中的 keys values.</p>
<h2 id="Attention-Mechanisms"><a href="#Attention-Mechanisms" class="headerlink" title="Attention Mechanisms"></a>Attention Mechanisms</h2><p>attention 的提出来自于：    </p>
<ul>
<li><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>  </p>
</li>
<li><p>paper:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></p>
</li>
</ul>
<p><img src="https://panxiaoxie.cn/2018/08/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Pointer-Networks/02.png"></p>
<ol>
<li>encoder 采用单层或多层的单向或双向的 rnn 得到source sentence 的隐藏状态表示 $H=[h_1,…,h_T]$。  </li>
</ol>
<ol start="2">
<li>decoder 的 t 时间步的隐藏状态为 $s_t$, 在 decoder 阶段也是 rnn，其中隐藏状态的更新为： $s_i=f(s_{i-1},y_{i-1},c_i)$ 其中 $s_{i-1}$ 是上一个隐藏状态，$y_{i-1}$ 是上一时间步的输出，$c_i$ 是当前时间步的 attention vector. 那么现在就是怎么计算当前时间步的 $c_i$.  </li>
</ol>
<ol start="3">
<li>当前时间步的 $e_t=a(s_{i-1}, h_j)$, 这是对齐模型，也就是计算上一个隐藏状态 $s_{i-1}$ 与 encoder 中每一个 hidden 的 match 程度，计算这个 score 有很多中方式，其中最常见的，也是 tf api 中使用的两种 BahdanauAttention 和 LuongAttention.</li>
</ol>
<p>$$\text{BahdanauAttention:}\quad e_{ij}=v_a^Ttanh(W^as_{i-1}+U_ah_j)$$</p>
<p>$$\text{LuongAttention:}\quad e_{ij}=h_j^TW^as_i$$</p>
<ol start="4">
<li>然后对得到的对齐 score 使用 softmax 得到相应的概率:</li>
</ol>
<p>$$a_{ij}=\dfrac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}$$</p>
<p>softmax 实际上相比上面的公式有点区别，就是 $exp(e^{ij}-max(e^{ik}))$ 防止数值溢出。</p>
<ol start="5">
<li>将得到的 $s_{i-1}$ 与 encoder 中的 $h_j$ 计算得到的概率与 $h_j$ 做加权和得到当前时间步的 attention vector $c_i$</li>
</ol>
<ol start="6">
<li>在然后使用 $c_{i-1},s_{i-1},y_{i-1}$ 更新decoder 中的隐藏状态，循环下去。。。</li>
</ol>
<ol start="7">
<li>根据当前的隐藏状态 $s_i$ 计算得到当前时间步的输出 $y_t$</li>
</ol>
<p>$$y_t=Ws_{i}+b$$</p>
<h3 id="先看父类-tf-contrib-seq2seq-AttentionMechanism"><a href="#先看父类-tf-contrib-seq2seq-AttentionMechanism" class="headerlink" title="先看父类 tf.contrib.seq2seq.AttentionMechanism"></a>先看父类 tf.contrib.seq2seq.AttentionMechanism</h3><p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionMechanism</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alignments_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>两个属性： alignments_size 和 state_size 分别对应 sequence 的长度，所以这个 alignment_size 是表示 mask 之后的长度吧？接下来看源码。 state_size 表示隐藏层的状态。显然这里的 attention 也是一个时间步内的计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseAttentionMechanism</span>(<span class="params">AttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A base AttentionMechanism class providing common functionality.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Common functionality includes:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    1. Storing the query and memory layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    2. Preprocessing and storing the memory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               query_layer,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_layer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               check_inner_dims_defined=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct base AttentionMechanism class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      query_layer: Callable.  Instance of `tf.layers.Layer`.  The layer&#x27;s depth</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        must match the depth of `memory_layer`.  If `query_layer` is not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        provided, the shape of `query` must match that of `memory_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory: The memory to query; usually the output of an RNN encoder.  This</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        tensor should be shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      probability_fn: A `callable`.  Converts the score and previous alignments</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        to probabilities. Its signature should be:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probabilities = probability_fn(score, state)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_sequence_length (optional): Sequence lengths for the batch entries</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in memory.  If provided, the memory tensor rows are masked with zeros</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for values past the respective sequence lengths.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_layer: Instance of `tf.layers.Layer` (may be None).  The layer&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        depth must match the depth of `query_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If `memory_layer` is not provided, the shape of `memory` must match</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        that of `query_layer`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      check_inner_dims_defined: Python boolean.  If `True`, the `memory`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        argument&#x27;s shape is checked to ensure all but the two outermost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        dimensions are fully defined.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      score_mask_value: (optional): The mask value for score before passing into</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probability_fn`. The default is -inf. Only used if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `memory_sequence_length` is not None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: Name to use when creating ops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (query_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(query_layer, layers_base.Layer)):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;query_layer is not a Layer: %s&quot;</span> % <span class="built_in">type</span>(query_layer).__name__)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (memory_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(memory_layer, layers_base.Layer)):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(</span><br><span class="line"></span><br><span class="line">          <span class="string">&quot;memory_layer is not a Layer: %s&quot;</span> % <span class="built_in">type</span>(memory_layer).__name__)</span><br><span class="line"></span><br><span class="line">    self._query_layer = query_layer</span><br><span class="line"></span><br><span class="line">    self._memory_layer = memory_layer</span><br><span class="line"></span><br><span class="line">    self.dtype = memory_layer.dtype</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">callable</span>(probability_fn):</span><br><span class="line"></span><br><span class="line">      <span class="keyword">raise</span> TypeError(<span class="string">&quot;probability_fn must be callable, saw type: %s&quot;</span> %</span><br><span class="line"></span><br><span class="line">                      <span class="built_in">type</span>(probability_fn).__name__)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> score_mask_value <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      score_mask_value = dtypes.as_dtype(</span><br><span class="line"></span><br><span class="line">          self._memory_layer.dtype).as_numpy_dtype(-np.inf)</span><br><span class="line"></span><br><span class="line">    self._probability_fn = <span class="keyword">lambda</span> score, prev: (  <span class="comment"># pylint:disable=g-long-lambda</span></span><br><span class="line"></span><br><span class="line">        probability_fn(</span><br><span class="line"></span><br><span class="line">            _maybe_mask_score(score, memory_sequence_length, score_mask_value),</span><br><span class="line"></span><br><span class="line">            prev))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ops.name_scope(</span><br><span class="line"></span><br><span class="line">        name, <span class="string">&quot;BaseAttentionMechanismInit&quot;</span>, nest.flatten(memory)):</span><br><span class="line"></span><br><span class="line">      self._values = _prepare_memory(</span><br><span class="line"></span><br><span class="line">          memory, memory_sequence_length,</span><br><span class="line"></span><br><span class="line">          check_inner_dims_defined=check_inner_dims_defined)</span><br><span class="line"></span><br><span class="line">      self._keys = (</span><br><span class="line"></span><br><span class="line">          self.memory_layer(self._values) <span class="keyword">if</span> self.memory_layer  <span class="comment"># pylint: disable=not-callable</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">else</span> self._values)</span><br><span class="line"></span><br><span class="line">      self._batch_size = (</span><br><span class="line"></span><br><span class="line">          self._keys.shape[<span class="number">0</span>].value <span class="keyword">or</span> array_ops.shape(self._keys)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">      self._alignments_size = (self._keys.shape[<span class="number">1</span>].value <span class="keyword">or</span></span><br><span class="line"></span><br><span class="line">                               array_ops.shape(self._keys)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">memory_layer</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._memory_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">query_layer</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._query_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">values</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">keys</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._keys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">batch_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alignments_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._alignments_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._alignments_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initial_alignments</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates the initial alignment values for the `AttentionWrapper` class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is important for AttentionMechanisms that use the previous alignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    to calculate the alignment at the next time step (e.g. monotonic attention).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The default behavior is to return a tensor of all zeros.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: `int32` scalar, the batch_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The `dtype`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A `dtype` tensor shaped `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is the values&#x27; `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    max_time = self._alignments_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> _zero_state_tensors(max_time, batch_size, dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initial_state</span>(<span class="params">self, batch_size, dtype</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates the initial state values for the `AttentionWrapper` class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is important for AttentionMechanisms that use the previous alignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    to calculate the alignment at the next time step (e.g. monotonic attention).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The default behavior is to return the same output as initial_alignments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      batch_size: `int32` scalar, the batch_size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The `dtype`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      A structure of all-zero tensors with shapes as described by `state_size`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.initial_alignments(batch_size, dtype)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>这个类 _BaseAttentionMechanism 是最基本的 attention 类了。可以看到 self._keys 和 self._values 的计算方式都是需要考虑 memory_sequence_length 这个参数的。</p>
<p>有这几个属性：    </p>
<ul>
<li><p>values: 其计算使用了 _prepare_memory 函数对应的是把输入序列 memory 的超过对应实际长度的部分的值变为 0    </p>
</li>
<li><p>keys： self._keys = self.memory_layer(self._values)  是在得到了 values 之后进行全链接的值，其shape=[batch, max_times, num_units]  </p>
</li>
<li><p>state_size 和 alignment_size 是一样的，都是 max_times  </p>
</li>
<li><p>self._probability_fn(score, prev) 使用了 _maybe_mask_score 这个函数计算得到 score 之后并 mask 的概率，然后还要利用 prev state?</p>
</li>
</ul>
<h4 id="maybe-mask-score"><a href="#maybe-mask-score" class="headerlink" title="_maybe_mask_score"></a>_maybe_mask_score</h4><p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_maybe_mask_score</span>(<span class="params">score, memory_sequence_length, score_mask_value</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">  message = (<span class="string">&quot;All values in memory_sequence_length must greater than zero.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> ops.control_dependencies(</span><br><span class="line"></span><br><span class="line">      [check_ops.assert_positive(memory_sequence_length, message=message)]):</span><br><span class="line"></span><br><span class="line">    score_mask = array_ops.sequence_mask(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length, maxlen=array_ops.shape(score)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    score_mask_values = score_mask_value * array_ops.ones_like(score)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> array_ops.where(score_mask, score, score_mask_values)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">score = tf.random_uniform(shape=[<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">tf.shape(score).numpy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>array([ 2, 10], dtype=int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">score = tf.random_uniform(shape=[<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">memeory_sequence_len = [<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">score_mask_value = -<span class="number">100000000</span></span><br><span class="line"></span><br><span class="line">score_mask = tf.sequence_mask(lengths=memeory_sequence_len, maxlen=tf.shape(score)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;true or false: %s\n&quot;</span> %score_mask)</span><br><span class="line"></span><br><span class="line">score_mask_values = score_mask_value * tf.ones_like(score)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-inf: %s\n&quot;</span>%score_mask_values)</span><br><span class="line"></span><br><span class="line">ans = tf.where(score_mask, score, score_mask_values)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ans)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>true or false: tf.Tensor(

[[ True  True  True  True  True False False False False False]

 [ True  True  True  True  True  True  True  True False False]], shape=(2, 10), dtype=bool)



-inf: tf.Tensor(

[[-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08

  -1.e+08]

 [-1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08 -1.e+08

  -1.e+08]], shape=(2, 10), dtype=float32)



tf.Tensor(

[[ 2.3987615e-01  4.9896538e-01  7.2822869e-01  4.7516704e-02

   1.6099060e-01 -1.0000000e+08 -1.0000000e+08 -1.0000000e+08

  -1.0000000e+08 -1.0000000e+08]

 [ 3.5503960e-01  2.5502288e-01  8.1264114e-01  4.3110681e-01

   1.1858845e-01  2.5748730e-02  4.8437893e-01  2.8339624e-02

  -1.0000000e+08 -1.0000000e+08]], shape=(2, 10), dtype=float32)
</code></pre>
<h4 id="prepare-memory"><a href="#prepare-memory" class="headerlink" title="_prepare_memory\"></a>_prepare_memory\</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">self._keys = _prepare_memory(memory, memory_sequence_length,</span><br><span class="line"></span><br><span class="line">check_inner_dims_defined=check_inner_dims_defined)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>其中 _prepare_memory 这个函数,也就是怎么计算 mask 的，其计算如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prepare_memory</span>(<span class="params">memory, memory_sequence_length, check_inner_dims_defined</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Convert to tensor and possibly mask `memory`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    memory: `Tensor`, shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    check_inner_dims_defined: Python boolean.  If `True`, the `memory`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      argument&#x27;s shape is checked to ensure all but the two outermost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dimensions are fully defined.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    A (possibly masked), checked, new `memory`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ValueError: If `check_inner_dims_defined` is `True` and not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `memory.shape[2:].is_fully_defined()`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  memory = nest.map_structure(</span><br><span class="line"></span><br><span class="line">      <span class="keyword">lambda</span> m: ops.convert_to_tensor(m, name=<span class="string">&quot;memory&quot;</span>), memory)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    memory_sequence_length = ops.convert_to_tensor(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length, name=<span class="string">&quot;memory_sequence_length&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> check_inner_dims_defined:</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_dims</span>(<span class="params">m</span>):</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> m.get_shape()[<span class="number">2</span>:].is_fully_defined():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected memory %s to have fully defined inner dims, &quot;</span></span><br><span class="line"></span><br><span class="line">                         <span class="string">&quot;but saw shape: %s&quot;</span> % (m.name, m.get_shape()))</span><br><span class="line"></span><br><span class="line">    nest.map_structure(_check_dims, memory)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    seq_len_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">    seq_len_mask = array_ops.sequence_mask(</span><br><span class="line"></span><br><span class="line">        memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        maxlen=array_ops.shape(nest.flatten(memory)[<span class="number">0</span>])[<span class="number">1</span>],</span><br><span class="line"></span><br><span class="line">        dtype=nest.flatten(memory)[<span class="number">0</span>].dtype)</span><br><span class="line"></span><br><span class="line">    seq_len_batch_size = (</span><br><span class="line"></span><br><span class="line">        memory_sequence_length.shape[<span class="number">0</span>].value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">or</span> array_ops.shape(memory_sequence_length)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_maybe_mask</span>(<span class="params">m, seq_len_mask</span>):</span></span><br><span class="line"></span><br><span class="line">    rank = m.get_shape().ndims</span><br><span class="line"></span><br><span class="line">    rank = rank <span class="keyword">if</span> rank <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> array_ops.rank(m)</span><br><span class="line"></span><br><span class="line">    extra_ones = array_ops.ones(rank - <span class="number">2</span>, dtype=dtypes.int32)</span><br><span class="line"></span><br><span class="line">    m_batch_size = m.shape[<span class="number">0</span>].value <span class="keyword">or</span> array_ops.shape(m)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory_sequence_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      message = (<span class="string">&quot;memory_sequence_length and memory tensor batch sizes do not &quot;</span></span><br><span class="line"></span><br><span class="line">                 <span class="string">&quot;match.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> ops.control_dependencies([</span><br><span class="line"></span><br><span class="line">          check_ops.assert_equal(</span><br><span class="line"></span><br><span class="line">              seq_len_batch_size, m_batch_size, message=message)]):</span><br><span class="line"></span><br><span class="line">        seq_len_mask = array_ops.reshape(</span><br><span class="line"></span><br><span class="line">            seq_len_mask,</span><br><span class="line"></span><br><span class="line">            array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> m * seq_len_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> nest.map_structure(<span class="keyword">lambda</span> m: _maybe_mask(m, seq_len_mask), memory)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>_prepare_memory 其实很简单，就是根据 batch 中每个样本的实际长度，将超出部分设置为 0</p>
<h3 id="tf-contrib-seq2seq-BahdanauAttention"><a href="#tf-contrib-seq2seq-BahdanauAttention" class="headerlink" title="tf.contrib.seq2seq.BahdanauAttention"></a>tf.contrib.seq2seq.BahdanauAttention</h3><p>这里涉及到了两篇 paper:  </p>
<ul>
<li><p>[Neural Machine Translation by Jointly Learning to Align and Translate.”</p>
<p>ICLR 2015. ](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>)  </p>
</li>
<li><p>[Weight Normalization: A Simple Reparameterization to Accelerate</p>
<p> Training of Deep Neural Networks.”](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a>)</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span>(<span class="params">_BaseAttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Bahdanau-style (additive) attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This attention has two forms.  The first is Bahdanau attention,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  as described in:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  ICLR 2015. https://arxiv.org/abs/1409.0473</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The second is the normalized form.  This form is inspired by the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  weight normalization article:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Tim Salimans, Diederik P. Kingma.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Weight Normalization: A Simple Reparameterization to Accelerate</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Training of Deep Neural Networks.&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1602.07868</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  To enable the second form, construct the object with parameter</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  `normalize=True`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               normalize=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="string">&quot;BahdanauAttention&quot;</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the Attention mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: The depth of the query mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory: The memory to query; usually the output of an RNN encoder.  This</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        tensor should be shaped `[batch_size, max_time, ...]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      memory_sequence_length (optional): Sequence lengths for the batch entries</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in memory.  If provided, the memory tensor rows are masked with zeros</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for values past the respective sequence lengths.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      normalize: Python boolean.  Whether to normalize the energy term.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      probability_fn: (optional) A `callable`.  Converts the score to</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        probabilities.  The default is @&#123;tf.nn.softmax&#125;. Other options include</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @&#123;tf.contrib.seq2seq.hardmax&#125; and @&#123;tf.contrib.sparsemax.sparsemax&#125;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Its signature should be: `probabilities = probability_fn(score)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      score_mask_value: (optional): The mask value for score before passing into</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `probability_fn`. The default is -inf. Only used if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `memory_sequence_length` is not None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dtype: The data type for the query and memory layers of the attention</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        mechanism.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      name: Name to use when creating ops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>num_units 是query mechanism 的维度. 它可以既不是 query 的维度,也可以不是 memory 的维度对吧?</p>
</li>
<li><p>query 的维度要和 memory(也就是 keys/values) 的维度一致吗?是不需要的.在 BahdanauAttention 的实现中比较好理解,两个全链接最后的维度一致即可相加.但是在 LuongAttention 中矩阵矩阵相乘时需要注意维度变化.  </p>
</li>
<li><p>memory_sequence_length: 这个参数很重要, mask 消除 padding 的影响.  </p>
</li>
<li><p>score_mask_value: 上一个参数存在时,这个参数才会使用,默认为 -inf.</p>
</li>
</ul>
<p>继续看源码的实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> probability_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">  probability_fn = nn_ops.softmax</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">  dtype = dtypes.float32</span><br><span class="line"></span><br><span class="line">wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line"><span class="built_in">super</span>(BahdanauAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">    query_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">        num_units, name=<span class="string">&quot;query_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">    memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">        num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">    memory=memory,</span><br><span class="line"></span><br><span class="line">    probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">    memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">    score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">    name=name)</span><br><span class="line"></span><br><span class="line">self._num_units = num_units</span><br><span class="line"></span><br><span class="line">self._normalize = normalize</span><br><span class="line"></span><br><span class="line">self._name = name</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li><p>现在理解了 _BaseAttentionMechanism 这个类中 query_layer 和 memory_layer 的意义了.  </p>
</li>
<li><p>score_mask_value 沿用父类中的计算方式.  </p>
</li>
</ul>
<p>继续看 call 函数,也就是 attention 的计算方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, query, state</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Score the query based on the keys and values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, query_depth]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    state: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is memory&#x27;s `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    alignments: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]` (`alignments_size` is memory&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> variable_scope.variable_scope(<span class="literal">None</span>, <span class="string">&quot;bahdanau_attention&quot;</span>, [query]):</span><br><span class="line"></span><br><span class="line">    processed_query = self.query_layer(query) <span class="keyword">if</span> self.query_layer <span class="keyword">else</span> query</span><br><span class="line"></span><br><span class="line">    score = _bahdanau_score(processed_query, self._keys, self._normalize)</span><br><span class="line"></span><br><span class="line">  alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line">  next_state = alignments</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> alignments, next_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>然后看怎么计算的 score.  </p>
<p>score = _bahdanau_score(processed_query, self._keys, self._normalize) 其中</p>
<p>processed_query 和 self._keys 都是通过全链接层后得到的, [batch, alignments_size, num_units]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bahdanau_score</span>(<span class="params">processed_query, keys, normalize</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Bahdanau-style (additive) scoring function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  dtype = processed_query.dtype</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Get the number of hidden units from the trailing dimension of keys</span></span><br><span class="line"></span><br><span class="line">  num_units = keys.shape[<span class="number">2</span>].value <span class="keyword">or</span> array_ops.shape(keys)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.</span></span><br><span class="line"></span><br><span class="line">  processed_query = array_ops.expand_dims(processed_query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  v = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">      <span class="string">&quot;attention_v&quot;</span>, [num_units], dtype=dtype)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> normalize:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar used in weight normalization</span></span><br><span class="line"></span><br><span class="line">    g = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_g&quot;</span>, dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.constant_initializer(math.sqrt((<span class="number">1.</span> / num_units))),</span><br><span class="line"></span><br><span class="line">        shape=())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Bias added prior to the nonlinearity</span></span><br><span class="line"></span><br><span class="line">    b = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_b&quot;</span>, [num_units], dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># normed_v = g * v / ||v||</span></span><br><span class="line"></span><br><span class="line">    normed_v = g * v * math_ops.rsqrt(</span><br><span class="line"></span><br><span class="line">        math_ops.reduce_sum(math_ops.square(v)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> math_ops.reduce_sum(</span><br><span class="line"></span><br><span class="line">        normed_v * math_ops.tanh(keys + processed_query + b), [<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [<span class="number">2</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>源码中计算 score 的最后一步不是全链接，而是这样的：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">v = tf.get_variable(<span class="string">&quot;attention_v&quot;</span>, [num_units])</span><br><span class="line"></span><br><span class="line">score = tf.reduce_sum(v * tanh(keys + processed_query), [<span class="number">2</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tfe.executing_eagerly())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">memory = tf.ones(shape=[<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>]) <span class="comment"># batch=1, max_sequence_len=10, embed_size=5</span></span><br><span class="line"></span><br><span class="line">memory_sequence_len = [<span class="number">5</span>]                   <span class="comment"># 有效长度为 5</span></span><br><span class="line"></span><br><span class="line">attention_mechnism = tf.contrib.seq2seq.BahdanauAttention(num_units=<span class="number">32</span>, memory=memory,</span><br><span class="line"></span><br><span class="line">                                                          memory_sequence_length=memory_sequence_len)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>True
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.state_size, attention_mechnism.alignments_size)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>10 10
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">memory</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>&lt;tf.Tensor: id=3, shape=(1, 10, 5), dtype=float32, numpy=

array([[[1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.]]], dtype=float32)&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">attention_mechnism.values   <span class="comment"># 可以发现 values 就是把 memory 中超过memory_sequence_length 的部分变为 0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&lt;tf.Tensor: id=30, shape=(1, 10, 5), dtype=float32, numpy=

array([[[1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [1., 1., 1., 1., 1.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.],

        [0., 0., 0., 0., 0.]]], dtype=float32)&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.keys.shape)  <span class="comment"># 经过了全链接之后的</span></span><br><span class="line"></span><br><span class="line">attention_mechnism.keys.numpy()[<span class="number">0</span>,<span class="number">1</span>,:]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>(1, 10, 32)



array([ 0.09100786,  0.18448338, -0.7751561 ,  0.00775184,  0.467805  ,

        0.9172474 ,  0.57645243, -0.3915946 , -0.22213435,  0.76866853,

        0.3591721 ,  0.8922573 ,  0.15866229,  0.6033571 ,  0.51816225,

        0.3820553 , -0.39130217,  0.04532939, -0.02089322,  0.6878175 ,

       -0.28697258,  0.59283376, -0.37825382, -0.5865691 ,  0.17466056,

       -0.5915747 ,  0.6070496 , -0.18531135, -0.821724  ,  1.2838829 ,

        0.15700272, -0.2608306 ], dtype=float32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_mechnism.query_layer, attention_mechnism.memory_layer)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&lt;tensorflow.python.layers.core.Dense object at 0x7fa0464da908&gt; &lt;tensorflow.python.layers.core.Dense object at 0x7fa0464dab38&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 利用 call 函数来计算下一个 state 和 attention vector</span></span><br><span class="line"></span><br><span class="line">query = tf.ones(shape=[<span class="number">1</span>, <span class="number">8</span>])  <span class="comment"># query_depth = 10</span></span><br><span class="line"></span><br><span class="line">state_h0 = attention_mechnism.initial_alignments(batch_size=<span class="number">1</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">attention_vector = attention_mechnism(query=query, state=state_h0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(attention_vector)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(&lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;, &lt;tf.Tensor: id=125, shape=(1, 10), dtype=float32, numpy=array([[0.2, 0.2, 0.2, 0.2, 0.2, 0. , 0. , 0. , 0. , 0. ]], dtype=float32)&gt;)
</code></pre>
<h3 id="tf-contrib-seq2seq-LuongAttention"><a href="#tf-contrib-seq2seq-LuongAttention" class="headerlink" title="tf.contrib.seq2seq.LuongAttention"></a>tf.contrib.seq2seq.LuongAttention</h3><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015.</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LuongAttention</span>(<span class="params">_BaseAttentionMechanism</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Luong-style (multiplicative) attention scoring.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               memory_sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               scale=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               probability_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               score_mask_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               name=<span class="string">&quot;LuongAttention&quot;</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> probability_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      probability_fn = nn_ops.softmax</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      dtype = dtypes.float32</span><br><span class="line"></span><br><span class="line">    wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LuongAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._scale = scale</span><br><span class="line"></span><br><span class="line">    self._name = name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以发现 query 没有经过 query_layer 的处理，也就是没有全链接。但是 memory 还是要用全链接处理的，得到 <code>[batch, max_times, num_units]</code></p>
<p>再看使用 call 函数计算对其概率 alignment 和 next_state.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, query, state</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Score the query based on the keys and values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, query_depth]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    state: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      (`alignments_size` is memory&#x27;s `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    alignments: Tensor of dtype matching `self.values` and shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `[batch_size, alignments_size]` (`alignments_size` is memory&#x27;s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      `max_time`).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> variable_scope.variable_scope(<span class="literal">None</span>, <span class="string">&quot;luong_attention&quot;</span>, [query]):</span><br><span class="line"></span><br><span class="line">    score = _luong_score(query, self._keys, self._scale)</span><br><span class="line"></span><br><span class="line">  alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line">  next_state = alignments</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> alignments, next_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>接下来看怎么计算的 score</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_luong_score</span>(<span class="params">query, keys, scale</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements Luong-style (multiplicative) scoring function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query: Tensor, shape `[batch_size, num_units]` to compare to keys.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    keys: Processed memory, shape `[batch_size, max_time, num_units]`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    scale: Whether to apply a scale to the score function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    A `[batch_size, max_time]` tensor of unnormalized score values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ValueError: If `key` and `query` depths do not match.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  depth = query.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  key_units = keys.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> depth != key_units:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Incompatible or unknown inner dimensions between query and keys.  &quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Query (%s) has units: %s.  Keys (%s) have units: %s.  &quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;Perhaps you need to set num_units to the keys&#x27; dimension (%s)?&quot;</span></span><br><span class="line"></span><br><span class="line">        % (query, depth, keys, key_units, key_units))</span><br><span class="line"></span><br><span class="line">  dtype = query.dtype</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Reshape from [batch_size, depth] to [batch_size, 1, depth]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># for matmul.</span></span><br><span class="line"></span><br><span class="line">  query = array_ops.expand_dims(query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Inner product along the query units dimension.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># matmul shapes: query is [batch_size, 1, depth] and</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#                keys is [batch_size, max_time, depth].</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># the inner product is asked to **transpose keys&#x27; inner shape** to get a</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># batched matmul on:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#   [batch_size, 1, depth] . [batch_size, depth, max_time]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># resulting in an output shape of:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#   [batch_size, 1, max_time].</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># we then squeeze out the center singleton dimension.</span></span><br><span class="line"></span><br><span class="line">  score = math_ops.matmul(query, keys, transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  score = array_ops.squeeze(score, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scalar used in weight scaling</span></span><br><span class="line"></span><br><span class="line">    g = variable_scope.get_variable(</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;attention_g&quot;</span>, dtype=dtype,</span><br><span class="line"></span><br><span class="line">        initializer=init_ops.ones_initializer, shape=())</span><br><span class="line"></span><br><span class="line">    score = g * score</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>通过源码可以发现 <code>LuongAttention</code> 调用 call 函数时，其 query 的维度必须是 num_units. 而 BahdanauAttention 并不需要。</p>
<p>其是计算 score 的方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">query_depth = num_units = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">memory_depth = <span class="number">15</span></span><br><span class="line"></span><br><span class="line">max_times = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">embed_size = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">scale = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.random_normal(shape=[batch_size, num_units])</span><br><span class="line"></span><br><span class="line"><span class="comment"># memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># values = self._prepaer_memory(memory)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># keys = memory_layer(values)</span></span><br><span class="line"></span><br><span class="line">values = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span><br><span class="line"></span><br><span class="line">keys = tf.layers.dense(inputs=values, units=num_units)    <span class="comment"># [batch, max_times, num_units]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.expand_dims(query, axis=<span class="number">1</span>)                     <span class="comment"># [batch, 1, num_units]</span></span><br><span class="line"></span><br><span class="line">score = tf.matmul(query, keys, transpose_b=<span class="literal">True</span>)          <span class="comment"># [batch, 1, max_times]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">score = tf.squeeze(score, axis=<span class="number">1</span>)   <span class="comment"># [batch, max_times]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(score.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(2, 10)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 完整的过一遍</span></span><br><span class="line"></span><br><span class="line">memory = tf.random_normal(shape=[batch_size, max_times, memory_depth])</span><br><span class="line"></span><br><span class="line">memory_sequence_len = [<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">query_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">query = tf.random_normal(shape=[batch_size, num_units])</span><br><span class="line"></span><br><span class="line">state = tf.zeros(shape=[batch_size, max_times])</span><br><span class="line"></span><br><span class="line">attention_mechnism = tf.contrib.seq2seq.LuongAttention(num_units=num_units,</span><br><span class="line"></span><br><span class="line">                                                       memory=memory,</span><br><span class="line"></span><br><span class="line">                                                       memory_sequence_length=memory_sequence_len)</span><br><span class="line"></span><br><span class="line">attention_vector = attention_mechnism(query, state)</span><br><span class="line"></span><br><span class="line">attention_vector[<span class="number">0</span>], attention_vector[<span class="number">1</span>]   <span class="comment"># attention_vector 和 state</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>(&lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy=

 array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02,

         6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,

         0.0000000e+00, 0.0000000e+00],

        [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06,

         3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04,

         0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;,

 &lt;tf.Tensor: id=1024, shape=(2, 10), dtype=float32, numpy=

 array([[3.6951914e-01, 5.4255807e-01, 2.4851409e-03, 1.8003594e-02,

         6.7433923e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,

         0.0000000e+00, 0.0000000e+00],

        [3.5050314e-05, 1.2835214e-02, 1.6479974e-03, 1.4405438e-06,

         3.3324495e-01, 6.4109474e-01, 1.0316775e-02, 8.2380348e-04,

         0.0000000e+00, 0.0000000e+00]], dtype=float32)&gt;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.reduce_sum(attention_vector[<span class="number">0</span>][<span class="number">1</span>]).numpy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>1.0
</code></pre>
<p>这只是针对单个 query 的情况，但实际上 query 一般是这样的 [batch, query_len, num_units]，那怎么办呢？</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="最后总结一下"><a href="#最后总结一下" class="headerlink" title="最后总结一下"></a>最后总结一下</h3><p>再看一遍两个 attention 初始化的差异</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">super</span>(BahdanauAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;query_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">super</span>(LuongAttention, self).__init__(</span><br><span class="line"></span><br><span class="line">        query_layer=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line"></span><br><span class="line">            num_units, name=<span class="string">&quot;memory_layer&quot;</span>, use_bias=<span class="literal">False</span>, dtype=dtype),</span><br><span class="line"></span><br><span class="line">        memory=memory,</span><br><span class="line"></span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line"></span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line"></span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line"></span><br><span class="line">        name=name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>作为一个类对象时，<code>AttentionMechanism</code>，<code>BahdanauAttention</code>，<code>LuongAttention</code>它们具有如下属性：</p>
<ul>
<li><p>query_layer: 在 BahdanauAttention 中一般是 tf.layer.dense 的实例对象，其维度是 num_units. 所以 BahdanauAttention 中 query 的维度可以是任意值。而 LuongAttention 中 query_layer 为 None，所以 query 的维度只能是 num_units.  </p>
</li>
<li><p>memory_layer: 在两个 attention 中都是一样的，tf.layer.dense,且维度为 num_units.  </p>
</li>
<li><p>alignments_size: 对齐size，是 memory 的 max_times.  </p>
</li>
<li><p>batch_size: 批量大小  </p>
</li>
<li><p>values: 是经过 mask 处理后的 memory. [batch, max_times, embed_size]  </p>
</li>
<li><p>keys: 是经过 memory_layer 全链接处理后的。 [batch, max_times, num_units].</p>
</li>
<li><p>state_size: 等于 alignment_size.</p>
</li>
</ul>
<p>然后是对应的方法：  </p>
<p><strong><strong>init</strong>:</strong> 初始化类实例，里面的参数：  </p>
<ul>
<li><p>num_units: 在　Bahdanau 中这个参数其实是个中间值，将 query 和 keys 转化为这个维度，叠加，但最后还是要在这个维度上　reduce_sum． 但是在　LuongAttention 中它必须和 query 的维度一致，然后和 memory_layer 处理后的 memory 做矩阵相乘。  </p>
</li>
<li><p>memory: [batch, max_times, embed_size]  </p>
</li>
<li><p>normalize: 是佛有归一化  </p>
</li>
<li><p>probability_fn: <code>tf.nn.softmax</code>，<code>tf.contrib.seq2seq.hardmax</code>，<code> tf.contrib.sparsemax.sparsemax</code>  </p>
</li>
<li><p>memory_sequence_length： 没有经过 padding 时 memory 的长度。其维度应该是 [1, batch_size]  </p>
</li>
</ul>
<p><strong>call(query, state)</strong> 调用该实例  </p>
<ul>
<li><p>query: [batch_size, query_length]. 在 LuongAttention 中 query_length 必须等于 num_units.  </p>
</li>
<li><p>state: [batch_size, alignments_size].  </p>
</li>
</ul>
<p>一直不太理解 state 有啥用？在源码中是用来计算 alignments 的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">alignments = self._probability_fn(score, state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self._probability_fn = <span class="keyword">lambda</span> score, prev: (  <span class="comment"># pylint:disable=g-long-lambda</span></span><br><span class="line"></span><br><span class="line">        probability_fn(</span><br><span class="line"></span><br><span class="line">            _maybe_mask_score(score, memory_sequence_length, score_mask_value),</span><br><span class="line"></span><br><span class="line">            prev))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中 score 是可能需要 mask 的. probability_fn 是 tf.nn.softmax. 所以呢？？？？不需要 prev 啊？</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后发现确实不需要啊。。。一步步往上找</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">probability_fn=wrapped_probability_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wrapped_probability_fn = <span class="keyword">lambda</span> score, _: probability_fn(score)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p><strong>initial_alignments(batch_size, dtype)</strong> 初始化对齐  </p>
<p>Args:  </p>
<ul>
<li><p>batch_size: int32 scalar, the batch_size.  </p>
</li>
<li><p>dtype: The dtype.  </p>
</li>
</ul>
<p>Returns:  </p>
<ul>
<li>A dtype tensor shaped [batch_size, alignments_size]  </li>
</ul>
<p><strong>initial_state(batch_size, dtype)：</strong>  </p>
<p>Creates the initial state values for the AttentionWrapper class.</p>
<ul>
<li>batch_size: int32.   </li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Tensorflow Attention API 源码阅读1</p><p><a href="http://www.panxiaoxie.cn/2018/09/01/tensorflow-Attention-API/">http://www.panxiaoxie.cn/2018/09/01/tensorflow-Attention-API/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2018-09-01</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Tensorflow/">Tensorflow</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/09/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-QANet/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">论文笔记-QANet</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/09/01/tensorflow-rnn-api-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><span class="level-item">Tensorflow RNN API 源码阅读</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Kommentare</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2018/09/01/tensorflow-Attention-API/';
            this.page.identifier = '2018/09/01/tensorflow-Attention-API/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="潘小榭"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">潘小榭</p><p class="is-size-6 is-block">Blogging is happier than writing essays!</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Seiten</p><a href="/archives"><p class="title">111</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Kategorien</p><a href="/categories"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Folgen</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Kategorien</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Letzte Einträge</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-05T02:03:16.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-contrastive learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:07:08.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:05:10.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-16T07:48:48.000Z">2021-04-16</time></p><p class="title"><a href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-12T08:16:35.000Z">2021-04-12</time></p><p class="title"><a href="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/">论文笔记-DETR and Deformable DETR</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archive</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">October 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">November 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESA/"><span class="tag">ESA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN%EF%BC%8CRL/"><span class="tag">GAN，RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MRC-and-QA/"><span class="tag">MRC and QA</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Translation/"><span class="tag">Machine Translation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/capsules/"><span class="tag">capsules</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-augmentation/"><span class="tag">data augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/open-set-recognition/"><span class="tag">open set recognition</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentence-embedding/"><span class="tag">sentence embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sentiment-classification/"><span class="tag">sentiment classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sign-language-recognition/"><span class="tag">sign language recognition</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-matching/"><span class="tag">text matching</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/transfer-learning/"><span class="tag">transfer learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="tag">数据结构与算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag">8</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Abonnieren Sie Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Abonnieren"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("default");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Zurück nach oben" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "Diese Website verwendet Cookies, um Ihre Erfahrung zu verbessern.",
          dismiss: "Verstanden!",
          allow: "Cookies zulassen",
          deny: "Ablehnen",
          link: "Mehr erfahren",
          policy: "Cookie-Richtlinie",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Tippen Sie etwas..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Tippen Sie etwas...","untitled":"(Ohne Titel)","posts":"Seiten","pages":"Pages","categories":"Kategorien","tags":"Tags"});
        });</script></body></html>