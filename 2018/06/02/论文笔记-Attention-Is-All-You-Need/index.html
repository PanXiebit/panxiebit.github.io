<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>论文笔记, Attention Is All You Need - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Attention Is All You Need 1. paper reading1.1 IntroductionRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="Attention Is All You Need 1. paper reading1.1 IntroductionRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2018-06-02T12:46:40.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:09.363Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="Machine Translation"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/"},"headline":"论文笔记, Attention Is All You Need","image":["http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/01.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/02.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/03.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/04.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/11.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/10.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/06.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/07.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/09.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/11.png","http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/12.png"],"datePublished":"2018-06-02T12:46:40.000Z","dateModified":"2021-06-29T08:12:09.363Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"Attention Is All You Need 1. paper reading1.1 IntroductionRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-06-02T12:46:40.000Z" title="2018/6/2 下午8:46:40">2018-06-02</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.363Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/">Machine Translation</a></span><span class="level-item">33 分钟读完 (大约4906个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">论文笔记, Attention Is All You Need</h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
<h3 id="1-paper-reading"><a href="#1-paper-reading" class="headerlink" title="1. paper reading"></a>1. paper reading</h3><h4 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h4><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t.</p>
<p>This inherently sequential nature <strong>precludes parallelization within training examples</strong>, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.</p>
<p>RNN模型有两个很致命的缺点：</p>
<p>$$y_t=f(y_{t-1},x_t)$$</p>
<ul>
<li>一是无法解决长句子的长期依赖的问题（这是因为反向传播，loss的梯度很难传递到比较靠前的位置，造成前面的词对整体的影响偏小，[Gradient flow in</li>
</ul>
<p>recurrent nets: the difficulty of learning long-term dependencies](<a target="_blank" rel="noopener" href="http://www.bioinf.jku.at/publications/older/ch7.pdf)%EF%BC%89%EF%BC%9B">http://www.bioinf.jku.at/publications/older/ch7.pdf)）；</a></p>
<ul>
<li>二是计算无法并行化的问题（后一个时刻的计算依赖于前一个时刻的计算），导致训练速度很慢。</li>
</ul>
<p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.</p>
<p>Attention机制能够有效解决RNN无法长时间依赖的问题，但是对于无法并行化计算的问题依旧存在。</p>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h4><h5 id="2-1-Extended-Neural-GPU-16-ByteNet-18-and-ConvS2S-9"><a href="#2-1-Extended-Neural-GPU-16-ByteNet-18-and-ConvS2S-9" class="headerlink" title="2.1 Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]"></a>2.1 Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]</h5><h5 id="2-2-Self-attention"><a href="#2-2-Self-attention" class="headerlink" title="2.2 Self-attention"></a>2.2 Self-attention</h5><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03130">A structured self-attentive sentence embedding</a></p>
<h5 id="2-3-End-to-end-memory-networks"><a href="#2-3-End-to-end-memory-networks" class="headerlink" title="2.3 End-to-end memory networks"></a>2.3 End-to-end memory networks</h5><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.08895">End-to-end memory networks</a></p>
<h5 id="2-4-Transformer"><a href="#2-4-Transformer" class="headerlink" title="2.4 Transformer"></a>2.4 Transformer</h5><p>Transformer is the first transduction model relying</p>
<p>entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.</p>
<p>本文主要和以下三篇文章对比：</p>
<ul>
<li><p><a href>Neural GPUs learn algorithms</a></p>
</li>
<li><p><a href>Neural machine translation in linear time</a></p>
</li>
<li><p><a href>Convolutional sequence to sequence learning</a></p>
</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>在以往的encoder-decoder模型中：</p>
<blockquote>
<p>the encoder maps an input sequence of symbol representations $(x_1,…,x_n)$ to a sequence of continuous representations $z = (z_1,…,z_n)$. Given z, the decoder then generates an output sequence $(y_1,…,y_m)$ of symbols one element at a time. At each step the model is <strong>auto-regressive [10]</strong>, consuming the previously generated symbols as additional input when generating the next.</p>
</blockquote>
<p>以往的attention虽然也能解决long dependecy的问题，但是受制于RNN的原因，每一步的计算必须在上一时间步完成后进行。因此无法并行计算。</p>
<p>Transformer 也是由 encoder 和 decoder 组成。</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/01.png"></p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>其中 Encoder 由6个完全相同的layer堆叠（stack）而成。每一层layer由两个 sub-layer 组成，分别是 <strong>multi-head self-attention mechanism</strong> 和 <strong>point-wise fully connected feed-forward network.</strong> 每一个 sub-layer 应用一个残差连接（<strong>residual connection</strong>）,然后再连接一个 normalization 层。</p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>跟 encoder 非常类似，同样由6由6个完全相同的layer堆叠而成，但是每一层有3个 sub-layer， 增加了一个 multi-head attention. 同样的也有残差链接和normalization层。</p>
<p>对 self-attention 进行了修改，masking:</p>
<blockquote>
<p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p>
</blockquote>
<p>目的应该就是让下一个t时刻的生成词只依赖于t时刻之前的词。</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>Really love this short description of attention:</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
</blockquote>
<h5 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h5><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/02.png"></p>
<ul>
<li><p>queries: $Q\in R^{n\times d_k}$</p>
</li>
<li><p>keys: $K\in R^{n\times d_k}$</p>
</li>
<li><p>values: $V\in R^{n\times d_v}$</p>
</li>
</ul>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/03.png"></p>
<p>计算向量內积作为相似度，并使用softmax计算权重，然后加权求和。 这种 attention 其实也很常见了，这里 google 算是给这种结构一个官方的名字吧。</p>
<p>论文中作者还对比了比较常用的另一种attention机制， additive attention (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> 这篇非常经典的文章中提出的)，additive 是使用的前馈神经网络来计算 (具体公式可以看这里<a href="http://www.panxiaoxie.cn/2018/05/08/cs224d-lecture10-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">cs224d-lecture10 机器翻译和注意力机制</a>). 虽然从计算复杂度上来讲，两者是差不多的，但在实际应用中 dot-product 更快，而且空间复杂度更低，因为可以通过矩阵优化计算。关于attention机制的对比可参考<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03906">Massive Exploration of Neural Machine Translation Architectures</a></p>
<p>有一点需要注意的是，这里使用了归一化，也就是 Scaled. 当 $d_k$ 很大时， additive attention 的效果要优于 dot-product attention， 作者怀疑(suspect)是当 $d_k$ 太大时，通过 softmax 计算得到的权重都会很接近0或1,导致梯度很小。</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/04.png"></p>
<p>$q\cdot k=\sum_{i=1}^{d_k}q_ik_i$</p>
<p>当 $d_k$ 很大时，$q\cdot k$ 的方差也会很大。</p>
<h5 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/11.png"></p>
<p>接下来按照整个模型的数据流过程来介绍模型中的每一个模块。</p>
<h3 id="Components-and-Training"><a href="#Components-and-Training" class="headerlink" title="Components and Training"></a>Components and Training</h3><p>前面三部分 Encoder, Decoder, Attention 组成了 Transformer 模型的基本架构。其中具体细节，以及 Training 实现过程将通过代码实现。</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/10.png"></p>
<h3 id="Encoder-1"><a href="#Encoder-1" class="headerlink" title="Encoder"></a>Encoder</h3><h4 id="Stage1"><a href="#Stage1" class="headerlink" title="Stage1"></a>Stage1</h4><h5 id="Training-data-and-batching"><a href="#Training-data-and-batching" class="headerlink" title="Training data and batching"></a>Training data and batching</h5><p>WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.</p>
<p>作者使用了：</p>
<ul>
<li>byte-pair <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03906">这篇论文中有介绍：Massive exploration of neural machine translation architectures</a></li>
</ul>
<ul>
<li>word-piece <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.08144">Google’s neural machine translation system: Bridging the gap between human and machine translation</a></li>
</ul>
<p>这里我将使用创新工厂举办的 challenge.ai 的中英文比赛数据：<a target="_blank" rel="noopener" href="https://challenger.ai/datasets/translation">https://challenger.ai/datasets/translation</a></p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/06.png"></p>
<p>这里暂时先不管数据预处理，在模型中使用占位符 placeholder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># add placeholders</span></span><br><span class="line"></span><br><span class="line">self.input_x = tf.placeholder(dtype=tf.int32, shape=[<span class="literal">None</span>, self.sentence_len])</span><br><span class="line"></span><br><span class="line">self.input_y = tf.placeholder(dtype=tf.int32, shape=[<span class="literal">None</span>, self.sentence_len])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define decoder inputs</span></span><br><span class="line"></span><br><span class="line">self.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:<span class="number">1</span>])*<span class="number">2</span>, self.input_y[:,:-<span class="number">1</span>]],axis=-<span class="number">1</span>) <span class="comment"># 2:&lt;S&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>这里的sentence_len 指的是源语言句子的最大长度和目标语言句子的最大长度。长度不足的需要zero padding.</li>
</ul>
<ul>
<li>decoder 中self-attention的 query, keys, values 都是相同的，初始值是随机初始化的，shape 与 self.input_y 一致即可。</li>
</ul>
<h5 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h5><p>we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. In our model, we share the same weight matrix between the two embedding layers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              vocab_size,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              zero_pad=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              scale=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">              reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs:  A `Tensor` with type `int32` or `int64` containing the ids</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         to be looked up in `lookup table`. shape is [batch, sentence_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param vocab_size: vocabulary size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num_units: Number of embedding hidden units. in the paper, it is called d_model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param zero_pad: If True, all the values of the fist row (id 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        should be constant zeros.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param scale: If True. the outputs is multiplied by sqrt num_units.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param reuse: Boolean, whether to reuse the weights of a previous layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        by the same name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        A `Tensor` with one more rank than inputs&#x27;s. The last dimensionality</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        should be `num_units`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embedding-layer&quot;</span>, reuse=reuse):</span><br><span class="line"></span><br><span class="line">        embedding = tf.get_variable(<span class="string">&quot;embedding&quot;</span>, [vocab_size, num_units],</span><br><span class="line"></span><br><span class="line">                                    initializer=xavier_initializer())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> zero_pad:</span><br><span class="line"></span><br><span class="line">            embedding = tf.concat([tf.zeros([<span class="number">1</span>, num_units]),</span><br><span class="line"></span><br><span class="line">                                  embedding[<span class="number">1</span>:, :]], axis=<span class="number">0</span>)  <span class="comment"># index=0 for nil word</span></span><br><span class="line"></span><br><span class="line">        output = tf.nn.embedding_lookup(embedding, inputs)    <span class="comment"># [batch, sentence_len, num_units]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">            output = output * np.sqrt(num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>通常embedding我们在写的参数输入 vocab_size 和 num_units(也就是 embed_size)，但机器翻译中设计到两种语言，直接定义一个函数，并将input作为输入会让程序更简洁吧。。</li>
</ul>
<ul>
<li>这里将vocabulary 中index=0的设置为 constant 0, 也就是作为 input 中的 zero padding 的词向量。</li>
</ul>
<ul>
<li>归一化，除以 np.sqrt(num_units). 不懂为何要这么做？有论文研究过吗？</li>
</ul>
<h5 id="position-encoding"><a href="#position-encoding" class="headerlink" title="position encoding"></a>position encoding</h5><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/07.png"></p>
<p>pos　是word在句子中的位置， i 是对应 $d_{model}$ 词向量中的第 i 维。</p>
<p>That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π.</p>
<p>也就是说，位置编码的每个维度对应于正弦曲线。波长形成从2π到10000·2π的几何级数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_encoding_mine</span>(<span class="params">n_position, d_model</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Init the sinusoid position encoding table.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param n_position: the lenght of sentence</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param d_model: the same with embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep dim -1 for padding token position encoding zero vector</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># pos=-1 用于 padded zero vector</span></span><br><span class="line"></span><br><span class="line">    encoding = np.zeros([n_position, d_model], np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_position):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, d_model):</span><br><span class="line"></span><br><span class="line">            encoding[pos, i] = pos /np.power(<span class="number">10000</span>, <span class="number">2.</span>*i/d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>] = np.sin(encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># dim 2i</span></span><br><span class="line"></span><br><span class="line">    encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>] = np.cos(encoding[<span class="number">1</span>:-<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>]) <span class="comment"># dim 2i+1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        zero_pad=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        scale=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        scope=<span class="string">&quot;positional_encoding&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                        reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Sinusoidal Positional_Encoding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      inputs: A 2d Tensor with shape of (N, T).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: Output dimensionality</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      reuse: Boolean, whether to reuse the weights of a previous layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        by the same name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        A &#x27;Tensor&#x27; with one more rank than inputs&#x27;s, with the dimensionality should be &#x27;num_units&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    N, T = inputs.get_shape().as_list()  <span class="comment"># N means batch_size, T means the sentence length.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=reuse):</span><br><span class="line"></span><br><span class="line">        position_ind = tf.tile(tf.expand_dims(tf.<span class="built_in">range</span>(T), <span class="number">0</span>), [N, <span class="number">1</span>])  <span class="comment"># [N, T]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First part of the PE function: sin and cos argument</span></span><br><span class="line"></span><br><span class="line">        position_enc = np.array([</span><br><span class="line"></span><br><span class="line">            [pos / np.power(<span class="number">10000</span>, <span class="number">2.</span>*i/num_units) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_units)]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(T)])                                       <span class="comment"># [T, num_units]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Second part, apply the cosine to even columns and sin to odds.</span></span><br><span class="line"></span><br><span class="line">        position_enc[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position_enc[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line"></span><br><span class="line">        position_enc[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position_enc[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert to a tensor</span></span><br><span class="line"></span><br><span class="line">        lookup_table = tf.convert_to_tensor(position_enc, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> zero_pad:</span><br><span class="line"></span><br><span class="line">            lookup_table = tf.concat((tf.zeros(shape=[<span class="number">1</span>, num_units]),</span><br><span class="line"></span><br><span class="line">                                      lookup_table[<span class="number">1</span>:, :]), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)  <span class="comment"># [N, T, num_units]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">            outputs = outputs * num_units**<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>关于 position encoding 的解释，可以参考这篇blog</p>
<p><a target="_blank" rel="noopener" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#sf-Transformer-Attention-is-all-you-need-3">The Transformer – Attention is all you need.</a></p>
<p>In RNN (LSTM), the notion of time step is encoded in the sequence as inputs/outputs flow one at a time. In FNN, the positional encoding must be preserved to represent the time in some way to preserve the positional encoding. <strong>In case of the Transformer authors propose to encode time as sine wave, as an added extra input. Such signal is added to inputs and outputs to represent time passing.</strong></p>
<p>In general, adding positional encodings to the input embeddings is a quite interesting topic. One way is to embed the absolute position of input elements (as in ConvS2S). However, authors use “sine and cosine functions of different frequencies”. The “sinusoidal” version is quite complicated, while giving similar performance to the absolute position version. <strong>The crux is however, that it may allow the model to produce better translation on longer sentences at test time (at least longer than the sentences in the training data). This way sinusoidal method allows the model to extrapolate to longer sequence lengths.</strong></p>
<p>说真的，还是不太理解。。。</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/46452020/sinusoidal-embedding-attention-is-all-you-need">可视化 encoding 矩阵</a>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_units = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">sentence_len = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">i = np.tile(np.expand_dims(<span class="built_in">range</span>(num_units), <span class="number">0</span>), [sentence_len, <span class="number">1</span>]) <span class="comment"># (100,)-&gt; (1, 100) -&gt;(10, 100)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pos = np.tile(np.expand_dims(<span class="built_in">range</span>(sentence_len), <span class="number">1</span>), [<span class="number">1</span>, num_units]) <span class="comment">#(10,)-&gt; (10, 1) -&gt; (10, 100)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pos = np.multiply(pos, <span class="number">1</span>/<span class="number">10000.0</span>)</span><br><span class="line"></span><br><span class="line">i = np.multiply(i, <span class="number">2.0</span>/num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">matrix = np.power(pos, i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">matrix[:, <span class="number">1</span>::<span class="number">2</span>] = np.sin(matrix[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">matrix[:, ::<span class="number">2</span>] = np.cos(matrix[:, ::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">im = plt.imshow(matrix, aspect=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/09.png"></p>
<h4 id="Stage2"><a href="#Stage2" class="headerlink" title="Stage2"></a>Stage2</h4><h5 id="scaled-dot-product-attention"><a href="#scaled-dot-product-attention" class="headerlink" title="scaled dot-product attention"></a>scaled dot-product attention</h5><p>$$Attention(Q,K,V)=softmax\dfrac{QK^T}{\sqrt d_k}V$$</p>
<h5 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h5><p>Transformer reduces the number of operations required to relate (especially distant) positions in input and output sequence to a O(1). However, this comes at cost of reduced effective resolution because of averaging attention-weighted positions.</p>
<p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/11.png"></p>
<ul>
<li>h = 8 attention layers (aka “heads”): that represent linear projection (for the purpose of dimension reduction) of key K and query Q into $d_k$-dimension and value V into $d_v$-dimension:</li>
</ul>
<p>$$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\dots,h$$</p>
<p>其中：</p>
<p>$$W^Q_i, W^K_i\in\mathbb{R}^{d_{model}\times d_k}, W^V_i\in\mathbb{R}^{d_{model}\times d_v}, for\ d_k=d_v=d_{model}/h = 64$$</p>
<ul>
<li>scaled-dot attention applied in parallel on each layer (different linear projections of k,q,v) results in $d_v$-dimensional output.</li>
</ul>
<ul>
<li>concatenate outputs of each layer (different linear projection; also referred as ”head”): Concat$(head_1,…,head_h)$</li>
</ul>
<ul>
<li>linearly project the concatenation result form the previous step:</li>
</ul>
<p>$$MultiHeadAttention(Q,K,V) = Concat(head_1,\dots,head_h) W^O$$</p>
<p>where $W^0\in\mathbb{R}^{d_{hd_v}\times d_{model}}$</p>
<h5 id="关于-attention-在模型中的应用，有三种情况"><a href="#关于-attention-在模型中的应用，有三种情况" class="headerlink" title="关于 attention 在模型中的应用，有三种情况"></a>关于 attention 在模型中的应用，有三种情况</h5><ul>
<li>1.In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder。</li>
</ul>
<ul>
<li>2.The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li>
</ul>
<ul>
<li>3.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this</li>
</ul>
<p>inside of scaled dot-product attention by masking out (setting to −1) all values in the input of the softmax which correspond to illegal connections.</p>
<p>总结下就是：</p>
<p>Transformer 中的attention机制总共有三种情况：</p>
<ul>
<li>1.encoder模块中的 self-attention，其中 queries, keys, values 都是来自 input_x, 也就是源语言的词表示。通过多层 multi-head attention, FFN, 得到最后的 input sentence 的向量表示，在没有使用RNN，CNN的情况下，其中的每个词都包含了其他所有词的信息，而且效果比 RNN，CNN 得到的向量表示要好。</li>
</ul>
<ul>
<li>2.encoder-encoder模块中的 attention. 其中 queries 来自上一个sub-layer, 也就是 decoder 中 masked multi-head attention 的输出，keys-values 来自 encoder 的输出。</li>
</ul>
<ul>
<li>3.decoder模块中的 self-attention，其中 queries, keys, values 都是来自于上一个 decoder 的输出。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiheadattention</span>(<span class="params">q,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       k,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       v,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       d_model,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       heads,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       keys_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       causality=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       dropout_keep_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                       is_training=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; multi scaled dot product attention</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param q: A 3d tensor with shape of [batch, length_q, d_k].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param k: A 3d tensor with shape of [batch, lenght_kv, d_k].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param v:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param heads:An int. Number of heads.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param dropout_keep_prob:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param causality: If true, units that reference the future are masked.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Linear projections</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;linear-projection-multiheads&#x27;</span>):</span><br><span class="line"></span><br><span class="line">        q_proj = tf.layers.dense(q, d_model) <span class="comment"># [batch, lenght_q, d_model]</span></span><br><span class="line"></span><br><span class="line">        k_proj = tf.layers.dense(k, d_model) <span class="comment"># [batch, lenght_kv, d_model]</span></span><br><span class="line"></span><br><span class="line">        v_proj = tf.layers.dense(v, d_model) <span class="comment"># [batch, lenght_kv, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;multihead-attention&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># d_k = d_v = d_model/heads</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> d_model % heads != <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Key\values\query depth (%d) must be divisible by&quot;</span></span><br><span class="line"></span><br><span class="line">                             <span class="string">&quot;the number of attention heads (%d)&quot;</span> %(d_model, heads))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. split and concat</span></span><br><span class="line"></span><br><span class="line">        q_ = tf.concat(tf.split(q_proj, heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>)  <span class="comment"># [batch*heads, length_q, d_k]</span></span><br><span class="line"></span><br><span class="line">        k_ = tf.concat(tf.split(k_proj, heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>)  <span class="comment"># [batch*heads, length_kv, d_k]</span></span><br><span class="line"></span><br><span class="line">        v_ = tf.concat(tf.split(v_proj, heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>)  <span class="comment"># [batch*heads, length_kv, d_v]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. attention score</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># outputs.shape=[batch*heads, length_q, length_kv]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 要理解这个矩阵运算，对一个keys的句子长度为length_kv,需要计算的其中的每一个词与query中每一个词的內积。所以最后的score是[length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line">        scalar = tf.rsqrt(d_model/heads)  <span class="comment"># 1/sqrt(d_k)</span></span><br><span class="line"></span><br><span class="line">        outputs = tf.matmul(q_*scalar, k_, transpose_b=<span class="literal">True</span>)   <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. mask</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> keys_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># `y = sign(x) = -1` if `x &lt; 0`; 0 if `x == 0` or `tf.is_nan(x)`; 1 if `x &gt; 0`.</span></span><br><span class="line"></span><br><span class="line">            key_masks = tf.sign(tf.<span class="built_in">abs</span>(tf.reduce_sum(k, axis=-<span class="number">1</span>)))  <span class="comment"># (batch, length_kv)</span></span><br><span class="line"></span><br><span class="line">            key_masks = tf.tile(key_masks, [heads, <span class="number">1</span>])              <span class="comment"># (batch*heads, length_kv)</span></span><br><span class="line"></span><br><span class="line">            key_masks = tf.tile(tf.expand_dims(key_masks, <span class="number">1</span>), [<span class="number">1</span>, q.get_shape()[<span class="number">1</span>], <span class="number">1</span>])  <span class="comment"># (batch*heads, length_q, length_kv)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># def where(condition, x=None, y=None, name=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The `condition` tensor acts as a mask that chooses, based on the value at each</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># element, whether the corresponding element / row in the output should be taken</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># from `x` (if true) or `y` (if false).</span></span><br><span class="line"></span><br><span class="line">            paddings = tf.ones_like(outputs) * (-<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            outputs = tf.where(tf.equal(key_masks, <span class="number">0</span>), paddings, outputs)  <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Causality = Future blinding</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># causality参数告知我们是否屏蔽未来序列的信息（解码器self attention的时候不能看到自己之后的那些信息），</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 这里即causality为True时的屏蔽操作。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> causality:</span><br><span class="line"></span><br><span class="line">            diag_vals = tf.ones_like(outputs[<span class="number">0</span>, :, :])  <span class="comment"># [length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line">            tril = LinearOperatorLowerTriangular(diag_vals).to_dense()  <span class="comment"># [length_q, lenght_kv] 得到一个三角阵，下标index大于当前行的值都变为0</span></span><br><span class="line"></span><br><span class="line">            masks = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), [tf.shape(outputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            paddings = tf.ones_like(masks) * (-<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            outputs = tf.where(tf.equal(masks, <span class="number">0</span>), paddings, outputs)  <span class="comment"># [batch*heads, length_q, lenght_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将socre转换为概率</span></span><br><span class="line"></span><br><span class="line">        outpts = tf.nn.softmax(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Query Masking</span></span><br><span class="line"></span><br><span class="line">        query_mask = tf.sign(tf.<span class="built_in">abs</span>(tf.reduce_sum(q, axis=-<span class="number">1</span>, keepdims=<span class="literal">False</span>))) <span class="comment"># [batch, lenght_q]</span></span><br><span class="line"></span><br><span class="line">        query_mask = tf.tile(query_mask, [heads, <span class="number">1</span>])  <span class="comment"># [batch*heads, length_q] # 目的是为了让query和outputs保持形状一致</span></span><br><span class="line"></span><br><span class="line">        query_mask = tf.tile(tf.expand_dims(query_mask, axis=-<span class="number">1</span>), [<span class="number">1</span>, <span class="number">1</span>, tf.shape(k)[-<span class="number">1</span>]]) <span class="comment"># [batch*heads, length_q, length_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        paddings = tf.ones_like(outputs) * (-<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        outputs = tf.where(tf.equal(query_mask, <span class="number">0</span>), paddings, outputs) <span class="comment"># [batch*heads, length_q, length_kv]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line"></span><br><span class="line">            outputs = tf.layers.dropout(outputs, dropout_keep_prob, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># weights sum</span></span><br><span class="line"></span><br><span class="line">        outputs = tf.matmul(outputs, v_)  <span class="comment"># [batch*heads, length_q, k_v]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># restore shape</span></span><br><span class="line"></span><br><span class="line">        outputs = tf.concat(tf.split(outputs, heads, axis=<span class="number">0</span>), axis=-<span class="number">1</span>) <span class="comment">#[batch,length_q, k_v*heads] = [batch, lenght_q, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line"></span><br><span class="line">        outputs += q    <span class="comment"># [batch, lenght_q, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line"></span><br><span class="line">        outputs = Normalize(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs   <span class="comment"># [batch, length_q, d_model]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>关于代码的详细解析，可以看这篇blog <a target="_blank" rel="noopener" href="http://lib.csdn.net/article/aiframework/68187">机器翻译模型Transformer代码详细解析</a>.</p>
<h4 id="Stage3-Position-wise-Feed-Forward-Networks"><a href="#Stage3-Position-wise-Feed-Forward-Networks" class="headerlink" title="Stage3: Position-wise Feed-Forward Networks"></a>Stage3: Position-wise Feed-Forward Networks</h4><p>$$FFN(x) = MAX(0, xW_1+b_1)W_2+b_2$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_wise_feed_forward</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units1=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units2=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Point-wise feed forward net.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs: A 3D tensor with shape of [batch, length_q, d_model]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num_units1: A integers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param num_units2: A integers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param reuse: Boolean, whether to reuse the weights of a previous layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        by the same name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: A 3d tensor with the same shape and dtype as inputs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;feed-forward-networks&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># inner layers</span></span><br><span class="line"></span><br><span class="line">        params1 = &#123;<span class="string">&quot;inputs&quot;</span>:inputs, <span class="string">&quot;filters&quot;</span>:num_units1, <span class="string">&quot;kernel_size&quot;</span>:<span class="number">1</span>,</span><br><span class="line"></span><br><span class="line">                  <span class="string">&quot;activation&quot;</span>:tf.nn.relu, <span class="string">&quot;use_bias&quot;</span>:<span class="literal">True</span>, <span class="string">&quot;strides&quot;</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">        outputs = tf.layers.conv1d(**params1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># readout layer</span></span><br><span class="line"></span><br><span class="line">        params2 = &#123;<span class="string">&quot;inputs&quot;</span>:outputs, <span class="string">&quot;filters&quot;</span>:num_units2, <span class="string">&quot;kernel_size&quot;</span>:<span class="number">1</span>,</span><br><span class="line"></span><br><span class="line">                  <span class="string">&quot;activation&quot;</span>:<span class="literal">None</span>, <span class="string">&quot;use_bias&quot;</span>:<span class="literal">True</span>, <span class="string">&quot;strides&quot;</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">        outputs = tf.layers.conv1d(**params2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># residual connection</span></span><br><span class="line"></span><br><span class="line">        outputs += inputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line"></span><br><span class="line">        outputs = Normalize(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_wise_feed_forward_mine</span>(<span class="params">inputs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units1=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               num_units2=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">                               reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;feed-forward-networks&quot;</span>):</span><br><span class="line"></span><br><span class="line">        W1 = tf.get_variable(<span class="string">&quot;weight1&quot;</span>, [inputs.get_shape()[-<span class="number">1</span>], num_units1],initializer=xavier_initializer())</span><br><span class="line"></span><br><span class="line">        b1 = tf.get_variable(<span class="string">&#x27;bias1&#x27;</span>, [num_units1], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">        outputs = tf.einsum(<span class="string">&#x27;aij,jk-&gt;aik&#x27;</span>, inputs, W1) + b1  <span class="comment"># [batch, length_q, num_units1]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        W2 = tf.get_variable(<span class="string">&quot;weight1&quot;</span>, [outputs.get_shape()[-<span class="number">1</span>], num_units2], initializer=xavier_initializer())</span><br><span class="line"></span><br><span class="line">        b2 = tf.get_variable(<span class="string">&#x27;bias1&#x27;</span>, [num_units2], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">        outputs = tf.einsum(<span class="string">&#x27;aij,jk-&gt;aik&#x27;</span>, inputs, W2) + b2  <span class="comment"># [batch, length_q, num_units1]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># residual connection</span></span><br><span class="line"></span><br><span class="line">        outputs += inputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line"></span><br><span class="line">        outputs = Normalize(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="encoder-各模块组合在一起"><a href="#encoder-各模块组合在一起" class="headerlink" title="encoder 各模块组合在一起"></a>encoder 各模块组合在一起</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_encoder</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. embedding</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embedding-layer&quot;</span>):</span><br><span class="line"></span><br><span class="line">            self.enc = embedding(inputs=self.input_x,</span><br><span class="line"></span><br><span class="line">                                   vocab_size=self.vocab_size_cn,</span><br><span class="line"></span><br><span class="line">                                   num_units=self.d_model,</span><br><span class="line"></span><br><span class="line">                                   scale=<span class="literal">True</span>)   <span class="comment"># [batch, sentence_len, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. position encoding</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;position_encoding&quot;</span>):</span><br><span class="line"></span><br><span class="line">            encoding = position_encoding_mine(self.enc.get_shape()[<span class="number">1</span>], self.d_model)</span><br><span class="line"></span><br><span class="line">            self.enc *= encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3.dropout</span></span><br><span class="line"></span><br><span class="line">        self.enc = tf.layers.dropout(self.enc,</span><br><span class="line"></span><br><span class="line">                                     rate=self.dropout_keep_prob,</span><br><span class="line"></span><br><span class="line">                                     training=self.is_training)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. Blocks</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;num_layer_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i)):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># multihead attention</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># encoder: self-attention</span></span><br><span class="line"></span><br><span class="line">                self.enc = multiheadattention(q=self.enc,</span><br><span class="line"></span><br><span class="line">                                              k=self.enc,</span><br><span class="line"></span><br><span class="line">                                              v=self.enc,</span><br><span class="line"></span><br><span class="line">                                              d_model=self.d_model,</span><br><span class="line"></span><br><span class="line">                                              heads=self.heads,</span><br><span class="line"></span><br><span class="line">                                              causality=<span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">                                              dropout_keep_prob=self.dropout_keep_prob,</span><br><span class="line"></span><br><span class="line">                                              is_training=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Feed Froward</span></span><br><span class="line"></span><br><span class="line">                self.enc = position_wise_feed_forward(self.enc,</span><br><span class="line"></span><br><span class="line">                                                      num_units1= <span class="number">4</span>*self.d_model,</span><br><span class="line"></span><br><span class="line">                                                      num_units2= self.d_model,</span><br><span class="line"></span><br><span class="line">                                                      reuse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.enc</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder 模块中 self-attention 的初始输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># define decoder inputs</span></span><br><span class="line"></span><br><span class="line">self.decoder_inputs = tf.concat([tf.ones_like(self.input_y[:,:<span class="number">1</span>])*<span class="number">2</span>, self.input_y[:,:-<span class="number">1</span>]],axis=-<span class="number">1</span>) <span class="comment"># 2:&lt;S&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>与encoder 不同的是，分为 encoder-decoder attention 和 self-attention.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_decoder</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;decoder&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding</span></span><br><span class="line"></span><br><span class="line">        self.dec = embedding(self.decoder_inputs,</span><br><span class="line"></span><br><span class="line">                             vocab_size=self.vocab_size_en,</span><br><span class="line"></span><br><span class="line">                             num_units=self.d_model)   <span class="comment"># [batch, sentence_len, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># position decoding</span></span><br><span class="line"></span><br><span class="line">        encoding = position_encoding_mine(self.dec.get_shape()[<span class="number">1</span>], self.d_model)</span><br><span class="line"></span><br><span class="line">        self.dec *= encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># blocks</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;num_layers_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i)):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># self-attention</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;self.attention&quot;</span>):</span><br><span class="line"></span><br><span class="line">                    self.dec = multiheadattention(q=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  k=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  v=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  d_model=self.d_model,</span><br><span class="line"></span><br><span class="line">                                                  heads=self.heads,</span><br><span class="line"></span><br><span class="line">                                                  keys_mask=<span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">                                                  causality=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment"># encoder-decoder-attention</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder-decoder-attention&quot;</span>):</span><br><span class="line"></span><br><span class="line">                    self.dec = multiheadattention(q=self.dec,</span><br><span class="line"></span><br><span class="line">                                                  k=self.enc,</span><br><span class="line"></span><br><span class="line">                                                  v=self.enc,</span><br><span class="line"></span><br><span class="line">                                                  d_model=self.d_model,</span><br><span class="line"></span><br><span class="line">                                                  heads=self.heads,</span><br><span class="line"></span><br><span class="line">                                                  keys_mask=<span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">                                                  causality=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                self.dec = position_wise_feed_forward(self.dec,</span><br><span class="line"></span><br><span class="line">                                                      num_units1= <span class="number">4</span>*self.d_model,</span><br><span class="line"></span><br><span class="line">                                                      num_units2= self.d_model)   <span class="comment"># [batch, sentence_len, d_model]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.dec</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><p><img src="/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/12.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_train_op</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    self.optimizer = tf.train.AdamOptimizer(self.lr, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.98</span>, epsilon=<span class="number">1e-9</span>)</span><br><span class="line"></span><br><span class="line">    self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.train_op</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><h5 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h5><h5 id="label-smoothing"><a href="#label-smoothing" class="headerlink" title="label smoothing"></a>label smoothing</h5><p>During training, we employed label smoothing of value ls = 0:1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label_smoothing</span>(<span class="params">inputs, epsilon=<span class="number">0.1</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Applies label smoothing. See https://arxiv.org/abs/1512.00567</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param epsilon: Smoothing rate.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        For example,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    K = inputs.get_shape().as_list()[-<span class="number">1</span>]  <span class="comment"># number of channels</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ((<span class="number">1</span>-epsilon) * inputs) + (epsilon/K)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>Reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces">Attention and Augmented Recurrent Neural Networks</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#sf-Transformer-Attention-is-all-you-need-3">The Transformer – Attention is all you need.</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="http://lib.csdn.net/article/aiframework/68187">机器翻译模型Transformer代码详细解析</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>论文笔记, Attention Is All You Need</p><p><a href="http://www.panxiaoxie.cn/2018/06/02/论文笔记-Attention-Is-All-You-Need/">http://www.panxiaoxie.cn/2018/06/02/论文笔记-Attention-Is-All-You-Need/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-06-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Machine-Translation/">Machine Translation</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/06/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%B3%BB%E5%88%975-Hierarchical-Attention-Networks/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">文本分类系列5-Hierarchical Attention Networks</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/"><span class="level-item">深度学习-权重初始化</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2018/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Attention-Is-All-You-Need/';
            this.page.identifier = '2018/06/02/论文笔记-Attention-Is-All-You-Need/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">116</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">38</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">35</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>