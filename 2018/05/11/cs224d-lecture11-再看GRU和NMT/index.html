<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>cs224d-lecture11 再看GRU和NMT - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="主要内容：  GRU进一步理解  GRU和LSTM对比  LSTM的进一步理解  训练RNN的一些tips  Ensemble  MT Evaluation  生成词使用softmax导致的计算量过大的问题  presentation"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="主要内容：  GRU进一步理解  GRU和LSTM对比  LSTM的进一步理解  训练RNN的一些tips  Ensemble  MT Evaluation  生成词使用softmax导致的计算量过大的问题  presentation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2018-05-11T06:53:00.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:08.540Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="cs224d"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/"},"headline":"cs224d-lecture11 再看GRU和NMT","image":["http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/04.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/01.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/03.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/05.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/02.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/06.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/07.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/08.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/09.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/10.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/12.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/nb11.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/15.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/16.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/17.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/18.png","http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/11.png"],"datePublished":"2018-05-11T06:53:00.000Z","dateModified":"2021-06-29T08:12:08.540Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"主要内容：  GRU进一步理解  GRU和LSTM对比  LSTM的进一步理解  训练RNN的一些tips  Ensemble  MT Evaluation  生成词使用softmax导致的计算量过大的问题  presentation"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-11T06:53:00.000Z" title="2018/5/11 下午2:53:00">2018-05-11</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.540Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/cs224d/">cs224d</a></span><span class="level-item">11 分钟读完 (大约1719个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">cs224d-lecture11 再看GRU和NMT</h1><div class="content"><p>主要内容：</p>
<ul>
<li><p>GRU进一步理解</p>
</li>
<li><p>GRU和LSTM对比</p>
</li>
<li><p>LSTM的进一步理解</p>
</li>
<li><p>训练RNN的一些tips</p>
</li>
<li><p>Ensemble</p>
</li>
<li><p>MT Evaluation</p>
</li>
<li><p>生成词使用softmax导致的计算量过大的问题</p>
</li>
<li><p>presentation</p>
</li>
</ul>
<span id="more"></span>



<h3 id="GRU-进一步理解"><a href="#GRU-进一步理解" class="headerlink" title="GRU 进一步理解"></a>GRU 进一步理解</h3><p><strong>shortcut connection</strong></p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/04.png"></p>
<p><strong>adaptive shortcut connection</strong></p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/01.png"></p>
<p>使用update gate 自适应的增加shortcut connection</p>
<p><strong>prune unnecessary connections adaptively</strong></p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/03.png"></p>
<p>使用reset gate自适应的修剪不必要的连接。</p>
<blockquote>
<p>突然想到个问题，为什么神经网络具有自适应性？我个人的理解是，神经网络是一个参数学习和拟合的过程，在梯度下降的过程中，模型得到优化使其具有自适应性。</p>
</blockquote>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/05.png"></p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/02.png"></p>
<h4 id="question1"><a href="#question1" class="headerlink" title="question1:"></a>question1:</h4><p>how you select the readable subset based on this reset gate?</p>
<p>$$r_t=\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})\tag{reset gate}$$</p>
<p>$$\tilde h_t=tanh(Wx_t+r_t\circ Uh_{t-1})\tag{new memory}$$</p>
<p>the reset gate decides which parts of the hidden state to read <strong>to update the hidden state</strong>. So, the reset gate calculates which parts to read based on the current input and the previous hidden state. So it’s gonna say, okay, I wanna pay a lot of attention to dimensions 7 and 52. And so, those are the ones and a little to others. And so those are the ones that will be being read here and used in the calculation of the new candidate update, which is then sort of mixed together with carrying on what you had before.</p>
<p>对此，Christopher老头儿还举了个例子，在隐藏状态中动词保存在47-52 dimensions,当遇到新的verb是，隐藏状态的这部分维度就会得到更新。看到这，真想试试打印出 $r_t$ 看看它随时间步的变化情况。。</p>
<h4 id="question2"><a href="#question2" class="headerlink" title="question2:"></a>question2:</h4><p>how you select the writable subset based on this update gate?</p>
<p>$$u_t=\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})\tag{update gate}$$</p>
<p>$$h_t=(1-u_t)\circ \tilde h_t+u_t\circ h_{t-1} \tag{Hidden state}$$</p>
<p>some of the hidden state we’re just gonna carry on from the past. <strong>We’re only now going to edit part of the register</strong>. And saying part of the register, I guess is a lying and simplifying a bit, because really, you’ve got this vector of real numbers and some said the part of the register is 70% updating this dimension and 20% updating this dimension that values could be one or zero but normally they won’t be. So I choose the writable subset And then it’s that part of it that I’m then updating with my new candidate update which is then written back, adding on to it. And so both of those concepts in the gating, the one gate is selecting what to read for your candidate update. And the other gate is saying, which parts of the hidden state to overwrite?</p>
<p>感觉意思是，update gate主要是为了控制生成当前时间步的隐藏状态 $h_t$，如果更新门的值都是1，那就以为着保存所有的以前的信息。</p>
<h4 id="question3"><a href="#question3" class="headerlink" title="question3:"></a>question3:</h4><p>how does these gates avoid gradient vanishing?</p>
<p>$$h_t=f(h_{t-1},x_t)=u_t \circ \tilde h_t + (1-u_t)\circ h_{t-1}$$</p>
<p>the secret is this <strong>plus sign</strong>.</p>
<p>在回过头来看一下梯度消失的那个公式：</p>
<p>$$\dfrac{\partial E_t}{\partial W} = \sum_{k=1}^t\dfrac{\partial E_t}{\partial y_t}\dfrac{\partial y_t}{\partial h_t}\dfrac{\partial h_t}{\partial h_k}\dfrac{\partial h_k}{\partial W}$$</p>
<p>举个例子，我们要求t=3时刻的损失函数对 $W_{hh}$ 的导数，那么：</p>
<p>$$\begin{align}</p>
<p>\dfrac{\partial E_3}{\partial W} &amp;=\sum_{k=1}^3\dfrac{\partial E_3}{\partial y_3}\dfrac{\partial y_3}{\partial h_3}\dfrac{\partial h_3}{\partial h_k}\dfrac{\partial h_k}{\partial W}\</p>
<p>&amp;=\dfrac{\partial E_3}{\partial y_3}\dfrac{\partial y_3}{\partial h_3}(\dfrac{\partial h_3}{\partial W}+\dfrac{\partial h_3}{\partial h_2}\dfrac{\partial h_2}{\partial W}+\dfrac{\partial h_3}{\partial h_2}\dfrac{\partial h_2}{\partial h_1}\dfrac{\partial h_1}{\partial W})</p>
<p>\end{align}$$</p>
<p>可以看到很早之前的隐藏状态 $h_1$ 随着时间的增长，对当前时刻的影响越来越小。而在GRU中，当update gate $u_t=0$ 时，$h_3=h_2$,这说明之前的隐藏状态存储的信息能有效的传递下来， that’s why it can carry information for a very long time.。</p>
<h4 id="question4"><a href="#question4" class="headerlink" title="question4:"></a>question4:</h4><p>how long does a GRU actually end up remembering for?</p>
<p>Answer: I kind of think order of magnitude the kind number you want in your head is 100 steps. So they don’t remember forever I think that’s something people also get wrong.</p>
<h4 id="question5"><a href="#question5" class="headerlink" title="question5:"></a>question5:</h4><p>Does GRU train faster than lstm?</p>
<p>Answer: LSTMs have a slight edge on speed. No huge difference.</p>
<h3 id="GRU和LSTM的区别"><a href="#GRU和LSTM的区别" class="headerlink" title="GRU和LSTM的区别"></a>GRU和LSTM的区别</h3><p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/06.png"></p>
<h4 id="question6"><a href="#question6" class="headerlink" title="question6:"></a>question6:</h4><p>LSTMs中为什么 $h_t=o_t\circ tanh(c_t)$ 中要用到tanh？</p>
<p>TA Richard的解释是，对于 new memory cell $\tilde c = f_t\circ c_{t-1}+i_t\circ \tilde c_t$ 这是一个线性的layer，加上tanh非线性因素，能让lstm更powerful.</p>
<h4 id="LSTM-直观图解"><a href="#LSTM-直观图解" class="headerlink" title="LSTM 直观图解"></a>LSTM 直观图解</h4><p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/07.png"></p>
<p>可以说是很清楚了～～不过这里有点区别,将 $h_{t-1}$ 和 $x_t$ concat在一起了，比如三个gate:</p>
<p>$$i_t = \sigma (W_i[h_{t-1},x_t]+b_i)\tag{input/update gate}$$</p>
<p>$$o_t = \sigma (W_o[h_{t-1},x_t]+b_o)\tag{output gate}$$</p>
<p>$$f_t = \sigma (W_f[h_{t-1},x_t]+b_f)\tag{forget gate}$$</p>
<p>而更新的 new memory cell $\tilde c_t$:</p>
<p>$$\tilde c_t=\tanh(W_c[h_{t-1}, x_t]+b_c)$$</p>
<p>最终的记忆细胞状态 $c_t$:</p>
<p>$$c_t= f_t\circ c_{t-1}+i_t\circ \tilde c_t$$</p>
<p>最终的隐藏状态 $h_t$:</p>
<p>$$h_t=o_t\circ tanh(c_t)$$</p>
<p>LSTM的核心，类似于resnet:</p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/08.png"></p>
<p>用加和，也就是图中的plus sign，原本的rnn的仅有matrix multiply，使得网络具有 long dependency.</p>
<h3 id="训练rnn的一些经验"><a href="#训练rnn的一些经验" class="headerlink" title="训练rnn的一些经验"></a>训练rnn的一些经验</h3><p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/09.png"></p>
<p>第7点，万万不能dropout horizontally,那样会丢失很多信息。</p>
<h3 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h3><p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/10.png"></p>
<p>之前看到在cnn里面，dropout可以看做是很多模型的集成，不知道rnn是否也可以。</p>
<h3 id="MT-Evaluation"><a href="#MT-Evaluation" class="headerlink" title="MT Evaluation"></a>MT Evaluation</h3><p>关于机器翻译的模型验证 notoriously tricky and subjective task，臭名昭著的棘手以及非常具有主观性。</p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/12.png"></p>
<p><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a></p>
<p>原理： n-gram matches</p>
<p>$p_n$ =  # matched n-grams / # n-grams in candidate translation</p>
<p>其实就是 <a target="_blank" rel="noopener" href="https://panxiebit.github.io/2018/04/14/chapter6-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%92%8C%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">precision</a></p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/nb11.png"></p>
<p>$p_n$ 表示 n-gram 的precision score. 并且，使用 $w_n=1/2^n$ 作为对应的权重。</p>
<p>brevity penalty：短译句容易得高分，因此需要给予惩罚</p>
<p>$$BP=\begin{cases} 1, &amp; \text{if c &gt; r}\</p>
<p>  e^{1-r/c}, &amp; \text{if c $\le r$}</p>
<p>\end{cases}$$</p>
<p>BLEU:</p>
<p>$$BLEU=BP\cdot exp(\sum_{n=1}^Nw_nlogp_n)$$</p>
<p>在对数域：</p>
<p>$$log BLEU=min(1-\dfrac{r}{c},0)+\sum_{n=1}^Nw_nlogp_n$$</p>
<h3 id="又是softmax的问题"><a href="#又是softmax的问题" class="headerlink" title="又是softmax的问题"></a>又是softmax的问题</h3><p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/15.png"></p>
<p>在每个时间步，从隐藏状态到词表，使用softmax这一步非常消耗计算力。</p>
<p>解决方法：</p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/16.png"></p>
<p>Not GPU-friendly! 不知道为啥。。感觉需要好好了解下GPU</p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/17.png"></p>
<p>原论文： <a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/P15-1001">On Using Very Large Target Vocabulary for Neural Machine Translation</a></p>
<p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/18.png"></p>
<h3 id="Word-and-character-based-models"><a href="#Word-and-character-based-models" class="headerlink" title="Word and character-based models"></a>Word and character-based models</h3><p>???</p>
<h3 id="presentation"><a href="#presentation" class="headerlink" title="presentation"></a>presentation</h3><p><img src="/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/11.png"></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>cs224d-lecture11 再看GRU和NMT</p><p><a href="http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-再看GRU和NMT/">http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-再看GRU和NMT/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-05-11</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/cs224d/">cs224d</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/05/14/cs224d-lecture13-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">cs224d-lecture13 卷积神经网络</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/05/08/cs224d-lecture10-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"><span class="level-item">cs224d-lecture10 机器翻译和注意力机制</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2018/05/11/cs224d-lecture11-%E5%86%8D%E7%9C%8BGRU%E5%92%8CNMT/';
            this.page.identifier = '2018/05/11/cs224d-lecture11-再看GRU和NMT/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>