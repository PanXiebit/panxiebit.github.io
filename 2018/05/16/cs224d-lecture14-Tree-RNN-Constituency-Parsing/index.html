<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>cs224d-lecture14 Tree-RNN and Constituency Parsing - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="主要内容：  语言的语义解释  如果将短语结构映射到向量空间中：利用语义的合成性  对比 RNN 和 CNN  Recursive neural networks  Parsing    a    sentence    with    an    RNN  使用tree-rnn 进行分类： assignment3 情感分类"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="主要内容：  语言的语义解释  如果将短语结构映射到向量空间中：利用语义的合成性  对比 RNN 和 CNN  Recursive neural networks  Parsing    a    sentence    with    an    RNN  使用tree-rnn 进行分类： assignment3 情感分类"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2018-05-16T13:01:16.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:08.539Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="cs224d"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/"},"headline":"cs224d-lecture14 Tree-RNN and Constituency Parsing","image":["http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn19.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn01.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn20.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn21.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn02.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn19.jpg","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn24.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn04.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn25.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn26.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn27.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn05.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn06.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn07.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn08.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn09.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn10.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn27.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn11.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/bpts03.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/bpts04.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/bpts05.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn12.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn13.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn14.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn15.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn16.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn17.png","http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn18.png"],"datePublished":"2018-05-16T13:01:16.000Z","dateModified":"2021-06-29T08:12:08.539Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"主要内容：  语言的语义解释  如果将短语结构映射到向量空间中：利用语义的合成性  对比 RNN 和 CNN  Recursive neural networks  Parsing    a    sentence    with    an    RNN  使用tree-rnn 进行分类： assignment3 情感分类"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-16T13:01:16.000Z" title="2018/5/16 下午9:01:16">2018-05-16</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/cs224d/">cs224d</a></span><span class="level-item">17 分钟读完 (大约2574个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">cs224d-lecture14 Tree-RNN and Constituency Parsing</h1><div class="content"><p>主要内容：</p>
<ul>
<li><p>语言的语义解释</p>
</li>
<li><p>如果将短语结构映射到向量空间中：利用语义的合成性</p>
</li>
<li><p>对比 RNN 和 CNN</p>
</li>
<li><p>Recursive neural networks</p>
</li>
<li><p>Parsing    a    sentence    with    an    RNN</p>
</li>
<li><p>使用tree-rnn 进行分类： assignment3 情感分类</p>
</li>
</ul>
<span id="more"></span>



<h3 id="语言的语义解释–并不只是词向量"><a href="#语言的语义解释–并不只是词向量" class="headerlink" title="语言的语义解释–并不只是词向量"></a>语言的语义解释–并不只是词向量</h3><p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn19.png"></p>
<p>词向量只是词语级别的向量，人们可以用更大颗粒度的文本来表达自己的意思，而不仅仅是词袋中的某个单词。</p>
<p>比如:</p>
<p>the country of my birth, the place where I was born</p>
<p>Question: how can we represent the meaning of longer phrases?</p>
<p>Answer: By    mapping    them    into    the    same    vector    space.</p>
<h3 id="How-should-we-map-phrases-into-a-vector-space"><a href="#How-should-we-map-phrases-into-a-vector-space" class="headerlink" title="How should we map phrases into a vector space?"></a>How should we map phrases into a vector space?</h3><h4 id="利用语义的合成性：-use-principle-of-Compositionality"><a href="#利用语义的合成性：-use-principle-of-Compositionality" class="headerlink" title="利用语义的合成性： use principle of Compositionality"></a>利用语义的合成性： use principle of <strong>Compositionality</strong></h4><ul>
<li><p>the meanings of its words</p>
</li>
<li><p>the rules that combine them</p>
</li>
</ul>
<blockquote>
<p>其实想想RNN也是将一个sentence或者是phrase压缩到一个向量中去。后面会介绍它们的区别。</p>
</blockquote>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn01.png"></p>
<p>通过同时学习句法树和复合性向量表示，就可以得到短语的向量表示了。</p>
<p>句法树：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn20.png"></p>
<p>句法树结构和向量表示Learn    Structure    and    Representation：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn21.png"></p>
<h4 id="问题是：我们真的需要学习这种树结构吗？"><a href="#问题是：我们真的需要学习这种树结构吗？" class="headerlink" title="问题是：我们真的需要学习这种树结构吗？"></a>问题是：我们真的需要学习这种树结构吗？</h4><p>Do    we    really    need    to    learn    this    structure?</p>
<p>从两个角度来说明这个问题，一是对比recursive 和 rnn， 而是从语言的本质来解释。</p>
<ol>
<li>Recursive vs. RNN</li>
</ol>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn02.png"></p>
<p>Richard mentioned that the recurrent models are really sort of capturing representations of whole prefixes and you’re not getting any representations of smaller units than that.</p>
<p>两者都是递归神经网络，只不过前者在空间上递归，后者在时间上递归。中文有时会把后者翻译为“循环神经网络”，但这明显混淆了等级，令人误解。</p>
<p>它们各有各的优缺点，Recursive neural net需要分析器来得到句法树，而Recurrent neural net只能捕捉“前缀”“上文”无法捕捉更小的单位。</p>
<p>但人们还是更倾向于用后者，LSTM之类。因为训练Recursive neural net之前，你需要句法树；句法树是一个离散的决策结果，无法连续地影响损失函数，也就无法简单地利用反向传播训练Recursive neural net。另外，复杂的结构也导致Recursive neural net不易在GPU上优化。</p>
<ol start="2">
<li>语言本质是递归的吗？</li>
</ol>
<p>在认知科学上虽然有些争议，因为一般一个句子是有长度限制的，人们几乎从不说300个词以上的句子。但是递归是描述语言的最佳方式，比如</p>
<p>[The man from [the company that you spoke with about [the project] yesterday]]</p>
<p>这里面一个名词短语套一个名词短语，一级级下去。从实用的角度讲</p>
<p>1、通过递归地描述句子（句法树），可以有效地消歧：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn19.jpg"></p>
<p>2、便于指代相消等任务。</p>
<p>3、便于利用语法树结构（基于短语的机器翻译）</p>
<h3 id="从-RNNs-到-CNNs"><a href="#从-RNNs-到-CNNs" class="headerlink" title="从 RNNs 到 CNNs"></a>从 RNNs 到 CNNs</h3><p>RNN只会为满足语法的短语计算向量，而CNN为每个可能的短语计算向量。从语言学和认知科学的角度来讲，CNN并不合理。甚至recurrent neural network也比tree model和CNN更合理。</p>
<p>两者的关系可以这样想象，RNN将CNN捕捉的不是短语的部分删除了：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn24.png"></p>
<p>得到：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn04.png"></p>
<ul>
<li>So the sort of picture is that for the CNN, you’re sort of making a representation of every pair of words, every triple of words, every four words.</li>
</ul>
<ul>
<li>Where as the tree recursive neural network is saying well some of those representations don’t correspond to a phrase and so we’re gonna delete them out. <strong>So that for the convolultional neural network, you have a representation for every bigram</strong>. So you have a representation for there speak and trigram there speak slowly. <strong>Whereas for the recursive neural network, you only have representations for the sort of semantically meaningful phrases</strong> like people there and speaks slowly going together to give a representation for the whole sentence.</li>
</ul>
<h3 id="Recursive-Neural-Networks-for-Structure-Prediction"><a href="#Recursive-Neural-Networks-for-Structure-Prediction" class="headerlink" title="Recursive    Neural    Networks    for    Structure    Prediction"></a>Recursive    Neural    Networks    for    Structure    Prediction</h3><p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn25.png"></p>
<ul>
<li><p>输入： 两个子节点的向量表示</p>
</li>
<li><p>输出： 两个子节点合并后的新节点语义表示，以及新节点成立的分值</p>
</li>
</ul>
<h3 id="Recursive-Neural-Network-Definition"><a href="#Recursive-Neural-Network-Definition" class="headerlink" title="Recursive    Neural    Network    Definition"></a>Recursive    Neural    Network    Definition</h3><p>可以同时得到句法树和向量表示的一种任务。通过socre来得到句法树。</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn26.png"></p>
<p>顺便提一下assignment3:</p>
<p>在 assignment3 是这样的tree-RNN</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn27.png"></p>
<p>$$h=relu([h^{(1)}<em>{left},h^{(1)}</em>{right}]W+b^{(1)})$$</p>
<p>$$\hat y = softmax(h^{(1)}U+b^{(s)})$$</p>
<p>$L\in R^{|V|\times d},W^{(1)}\in R^{2d\times d}, b^{(1)}\in R^{1\times d}, U\in R^{(d\times 5)}, b^{(s)}\in R^{1\times 5}$</p>
<p>在assignment3中用tree-rnn进行情感分析，是已经通过句法分析得到了句法树的，所以只需要从根节点开始，递归找到子节点，并计算出对应的向量表示，并归一化softmax，然后与每个节点（包括叶节点）真实标签对比，计算得到损失值。然后用梯度下降优化得到模型参数。</p>
<p>那么这里的参数怎么理解？在传统的rnn中 $W_{hh}$ 可以看做是隐藏状态转移矩阵，这里呢？？？关于 $W_{hh}$ 的理解，可以看知乎</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/57396443/answer/263019702">HMM和RNN是什么关系？功效上两者有冲突重叠？</a></p>
<h3 id="Parsing-a-sentence-with-an-RNN"><a href="#Parsing-a-sentence-with-an-RNN" class="headerlink" title="Parsing    a    sentence    with    an    RNN"></a>Parsing    a    sentence    with    an    RNN</h3><p>greedily incrementally building up parse structure.</p>
<p>计算任意两个单词合并的得分（虽然下图是相邻两个，但我觉得那只是绘图方便；就算是我第一次写的玩具级别的依存句法分析器，也是任意两个单词之间计算）：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn05.png"></p>
<p>然后贪心地选择得分最大的一对合并：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn06.png"></p>
<p>重复这一过程:计算任意两个节点，合并得分最大的一对</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn07.png"></p>
<p>直到得到根节点：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn08.png"></p>
<p>模型中只有一个合成函数，使用同一个权值矩阵W处理NP、VP、PP……这明显是不合理的。</p>
<h4 id="Max-Margin-Framework-Details"><a href="#Max-Margin-Framework-Details" class="headerlink" title="Max-Margin Framework-Details"></a>Max-Margin Framework-Details</h4><p>损失函数使用最大间隔</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn09.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn10.png"></p>
<p>再回顾一下多分类支持向量机损失 Multiclass Support Vector Machine Loss</p>
<p><a target="_blank" rel="noopener" href="https://github.com/PanXiebit/cs231n/blob/master/myNotes_cnn/02.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8-SVM-Softmax.ipynb">我的cs231n笔记</a></p>
<p>对于单个节点，可以看做单个样本损失</p>
<p>$$L_i=\sum_{j\ne y_j}^N max(0, s_j-(s_{y_i}-\Delta))$$</p>
<p>其中 $s_{y_j}$ 表示真实标签对应的值，非真实分类的得分不能超过 $s_{y_j}-\Delta$，凡是超过的都会对 $L_i$ 产生影响。比这个值就没事～</p>
<p>对于整个sentence：</p>
<p>$$J=\sum_imax(0, s(x_i,y_j)-max_{y\in A(x_i)}(s(x_i,y)+\Delta(y,y_i)))$$</p>
<ul>
<li><p>$\Delta$ 表示对所有非正确分类的惩罚</p>
</li>
<li><p>max 表示贪心搜索得到的syntactic tree的得分</p>
<ul>
<li>有时候也可用beam search</li>
</ul>
</li>
</ul>
<h4 id="使用-tree-RNN-进行分类任务"><a href="#使用-tree-RNN-进行分类任务" class="headerlink" title="使用 tree-RNN 进行分类任务"></a>使用 tree-RNN 进行分类任务</h4><p>这里的rnn指的是 递归recursive neural networks.空间结构上的递归，而以前学的RNN也是递归，不过是时间序列上的递归。</p>
<p>以assignment3中的情感分类任务为例，进行前向、反向传播推导。</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn27.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn11.png"></p>
<p>由于前向传播时每个节点的信号来自所有子节点，所以梯度也来自所有子节点。并且前向传播时父节点的信号是利用子节点信号的拼接计算的，所以梯度需要针对子节点的信号计算：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/bpts03.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/bpts04.png"></p>
<p>这个问题其实在TensorFlow那一课已经讲过了，图计算：前向传播信号流入某节点，反向传播误差就得从某节点分流到所有源节点。树只是图的一个特例：</p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/bpts05.png"></p>
<p>Richard Socher 的代码比如softmax之类的可真熟练～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forwardProp</span>(<span class="params">self, node</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Recursive</span></span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is node&#x27;s hidden activation</span></span><br><span class="line"></span><br><span class="line">  node.h = np.dot(self.W, np.hstack([node.left.h, node.right.h])) + self.b <span class="comment"># [1,d]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Relu</span></span><br><span class="line"></span><br><span class="line">  node.h[node.h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Softmax</span></span><br><span class="line"></span><br><span class="line">  node.score = np.dot(self.Ws, node.h) + self.bs <span class="comment"># [1, 5]</span></span><br><span class="line"></span><br><span class="line">  node.score -= np.<span class="built_in">max</span>(node.probs)</span><br><span class="line"></span><br><span class="line">  node.probs = np.exp(node.score)/np.<span class="built_in">sum</span>(np.exp(node.score))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backProp</span>(<span class="params">self.node, error=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># softmax`grad</span></span><br><span class="line"></span><br><span class="line">  deltas = node.probs</span><br><span class="line"></span><br><span class="line">  deltas[node.label] -= <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">  self.dWs = np.outer(deltas, node.h) <span class="comment"># Compute the outer product of two vectors. 训练时一个batch只有一个sentence，所有h是向量。</span></span><br><span class="line"></span><br><span class="line">  self.dbs += deltas</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对隐藏状态求导</span></span><br><span class="line"></span><br><span class="line">  dh = np.dot(self.Ws.T, deltas)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Add deltas from above</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> error <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">    dh += error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># f&#x27;(z) now:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># relu 反向传播</span></span><br><span class="line"></span><br><span class="line">  dh *= (node.h != <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Updata word vector if leaf node:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 如果是叶节点，h 就是 L，词向量.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> node.isLeaf:</span><br><span class="line"></span><br><span class="line">    self.dL[node.word] += deltas</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Recursively backProp</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 如果当前节点不是叶节点，那么需要更新权重W和b，同时将error</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> node.isLeaf:</span><br><span class="line"></span><br><span class="line">    self.dW += np.outer(deltas, np.hstack([node.left.h, node.right.h]))</span><br><span class="line"></span><br><span class="line">    self.db += deltas</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Error signal to children</span></span><br><span class="line"></span><br><span class="line">    dh = np.dot(self.W.T, dh)  <span class="comment"># 就是公式 h=relu([h^&#123;(1)&#125;_&#123;left&#125;,h^&#123;(1)&#125;_&#123;right&#125;]W+b^&#123;(1)&#125;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 递归计算左子节点，node.lef用来计算左子节点自身的损失， dh[:self.hiddenDim]用来计算父节点传递下来的损失</span></span><br><span class="line"></span><br><span class="line">    self.backProp(node.left, dh[:self.hiddenDim])</span><br><span class="line"></span><br><span class="line">    self.backProp(node.right, dh[self.hiddenDim:])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>ok！完全弄懂了吧？！～</p>
<h3 id="Syntactically-Untied-RNN"><a href="#Syntactically-Untied-RNN" class="headerlink" title="Syntactically-Untied RNN"></a>Syntactically-Untied RNN</h3><p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn12.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn13.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn14.png"></p>
<h3 id="Version3"><a href="#Version3" class="headerlink" title="Version3"></a>Version3</h3><p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn15.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn16.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn17.png"></p>
<p><img src="/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/trnn18.png"></p>
<h3 id="Presentation"><a href="#Presentation" class="headerlink" title="Presentation"></a>Presentation</h3><p>[Deep reinforcement learning for dialogue generation]</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="http://www.hankcs.com/nlp/cs224n-tree-recursive-neural-networks-and-constituency-parsing.html">CS224n笔记14 Tree RNN与短语句法分析</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.gitxiv.com/posts/m2KWs4jX9uB57PeeX/recursive-neural-networks-can-learn-logical-semantics">Recursive Neural Networks Can Learn Logical Semantics </a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>cs224d-lecture14 Tree-RNN and Constituency Parsing</p><p><a href="http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/">http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-05-16</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/cs224d/">cs224d</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/05/21/cs224d-lecture16-dynamic-neural-network/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">cs224d lecture16 dynamic Memory network</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/05/14/cs224d-lecture13-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-item">cs224d-lecture13 卷积神经网络</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/';
            this.page.identifier = '2018/05/16/cs224d-lecture14-Tree-RNN-Constituency-Parsing/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">116</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>