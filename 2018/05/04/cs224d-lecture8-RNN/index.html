<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>cs224d-lecture8-RNN - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="主要内容：  语言模型 Language model  循环神经网络 recurrent neural network  梯度消失和梯度爆炸问题的原因以及解决方法  双向rnn， deep bi-RNNs  关于依存分析的presentation"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="主要内容：  语言模型 Language model  循环神经网络 recurrent neural network  梯度消失和梯度爆炸问题的原因以及解决方法  双向rnn， deep bi-RNNs  关于依存分析的presentation"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2018-05-04T08:44:44.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:08.540Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="cs224d"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/"},"headline":"cs224d-lecture8-RNN","image":["http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn15.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn16.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn3.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn17.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn5.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn7.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn8.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn9.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn10.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn13.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn19.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn12.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn14.png","http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/rnn6.png"],"datePublished":"2018-05-04T08:44:44.000Z","dateModified":"2021-06-29T08:12:08.540Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"主要内容：  语言模型 Language model  循环神经网络 recurrent neural network  梯度消失和梯度爆炸问题的原因以及解决方法  双向rnn， deep bi-RNNs  关于依存分析的presentation"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-05-04T08:44:44.000Z" title="2018/5/4 下午4:44:44">2018-05-04</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.540Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/cs224d/">cs224d</a></span><span class="level-item">11 分钟读完 (大约1628个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">cs224d-lecture8-RNN</h1><div class="content"><p>主要内容：</p>
<ul>
<li><p>语言模型 Language model</p>
</li>
<li><p>循环神经网络 recurrent neural network</p>
</li>
<li><p>梯度消失和梯度爆炸问题的原因以及解决方法</p>
</li>
<li><p>双向rnn， deep bi-RNNs</p>
</li>
<li><p>关于依存分析的presentation</p>
</li>
</ul>
<span id="more"></span>

<h3 id="语言模型-Language-Model"><a href="#语言模型-Language-Model" class="headerlink" title="语言模型 Language Model"></a>语言模型 Language Model</h3><p>语言模型是计算一系列词以特定序列出现的概率。传统的语言模型是基于频率，计算在前n个词的条件下生成下一个词 $w_i$ 的概率。</p>
<p>$$P(w_1,…,w_m)=\prod_{i=1}^{i=m}P(w_i|w_1,…,w_i-1)\approx\prod_{i=1}^{i=m}P(w_i|w_{i-n},…,w_{i-1})$$</p>
<p>其中：</p>
<p>$$P(w_2|w_1)=\dfrac{count(w_1,w_2)}{count(w_1)}$$</p>
<p>$$P(w_3|w_1,w_2)=\dfrac{count(w_1,w_2,w_3)}{count(w_1,w_2)}$$</p>
<p>但基于概率的语言模型并不能捕捉到一些语义信息。</p>
<blockquote>
<p>For instance, consider a case where an article discusses the history of Spain and France and somewhere later in the text, it reads “The two countries went on a battle”; clearly the information presented in this sentence alone is not sufficient to identify the name of the two countries.</p>
</blockquote>
<p>于是出现了第一个神经网络的语言模型， <code>learning a distributed representation of words</code></p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn15.png"></p>
<p>$$\hat y=softmax(W^{(2)}tanh(w^{(1)}x+b^{(1)})+w^{(3)}x+b^{(3)})$$</p>
<ul>
<li><p>W^{(1)} 应用于词向量（solid green arrows）</p>
</li>
<li><p>W^{(2)} 应用于隐藏层</p>
</li>
<li><p>W^{(3)} 应用于词向量（dashed green arrows）</p>
</li>
</ul>
<p>但如果要记忆更多的词，必须要增大windows size n，这会造成计算量太大而无法计算。</p>
<h3 id="循环神经网络-Recurrent-Neural-Network-language-model"><a href="#循环神经网络-Recurrent-Neural-Network-language-model" class="headerlink" title="循环神经网络 Recurrent Neural Network language model"></a>循环神经网络 Recurrent Neural Network language model</h3><p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn16.png"></p>
<p>$$h_t = \sigma(W_{hh}h_{t-1}+W_{hx}x_{|t|})$$</p>
<p>其中+表示concatenate还是直接相加？通过作业实现，是相加～</p>
<p>shapes:</p>
<ul>
<li>$h_0\in R^{D_h}$ is some initialization vector for the hidden layer at time step 0,</li>
</ul>
<ul>
<li>$x\in R^{d}$ is the column vector for L at index [t] at time step t</li>
</ul>
<ul>
<li>$W^{hh}\in R^{D_h\times D_h}$</li>
</ul>
<ul>
<li>$W^{hx}\in R^{D_h\times d}$</li>
</ul>
<ul>
<li>$W^{(S))}\in R^{|V|\times D_h}$</li>
</ul>
<p>当前时间步的输出：</p>
<p>$\hat y \in R^{|V|}$ 通过softmax得到的在词表V上的概率分布。</p>
<p>那么当前时间步的损失值：</p>
<p>$$J^{(t)}(\theta) = -\sum_{j=1}^{|V|}y_{t,j}log\hat y_{t,j}$$</p>
<p>$y_{t,j}$ 表示当前时间步的actual word,是 one-hot vector.</p>
<blockquote>
<p>在训练模型时，$\hat y_t$ 用来计算当前时间步的损失值,从而训练参数。而在测试集中时，也就是生成sentence时，用来作为下一个时间步的输入。</p>
</blockquote>
<p>那么对整个sentence的预测的损失值：</p>
<p>$$J=\dfrac{1}{T}\sum_{t=1}^T(\theta)=-\dfrac{1}{T}\sum_{t=1}^T\sum_{j=1}^{|V|}y_{t,j}\times log(\hat y_{t,j})$$</p>
<p>困惑度：</p>
<p>$$Perplexity=2^J$$</p>
<h4 id="梯度消失和梯度爆炸问题"><a href="#梯度消失和梯度爆炸问题" class="headerlink" title="梯度消失和梯度爆炸问题"></a>梯度消失和梯度爆炸问题</h4><p>Training RNNs is incredibly hard! Buz of gradient vanishing and explosion problems.</p>
<p>这篇文章对rnn中梯度消失的问题说的比较清楚，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28687529">RNN梯度消失和爆炸的原因</a></p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn3.png"></p>
<p>这里将rnn简化了,原本应该是：</p>
<p>$$h_t=\sigma (Wf(h_{t-1})+W^{(hx)}x_{|t|})$$</p>
<p>$$\hat y = softmax(W^{(S)}f(h_t))$$</p>
<p>这里就按照简化的来推导吧，t时间步的损失值对</p>
<p>$$\dfrac{\partial E_t}{\partial W} = \sum_{k=1}^t\dfrac{\partial E_t}{\partial y_t}\dfrac{\partial y_t}{\partial h_t}\dfrac{\partial h_t}{\partial h_k}\dfrac{\partial h_k}{\partial W}$$</p>
<p>其实主要是这个式子的问题 $\dfrac{\partial h_t}{\partial h_k}$, $h_t$ 是W 和 $h_t-1$ 的函数， $h_{t-1}$ 又是 W 和 $h_{t-2}$ 的函数…..</p>
<p>也就是说 $h_t$ 是之前所有时刻 $h_k$ 的函数，而 $h_k$ 也是权重 W 的函数</p>
<p>$$\dfrac{\partial h_t}{\partial h_k} = \prod_{j=k+1}^k\dfrac{\partial h_j}{\partial h_{j-1}}=\prod_{j=k+1}^tW^T\times diag[f’(j_{j-1})]$$</p>
<p>其中 $h\in R^{D_h}$, 因此其导数 $\partial h_j/\partial h_{j-1}$ 是一个 $D_h \times D_h$ 的雅克比矩阵。</p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn17.png"></p>
<p>所以有：</p>
<p>$$\dfrac{\partial E_t}{\partial W} = \sum_{k=1}^t\dfrac{\partial E_t}{\partial y_t}\dfrac{\partial y_t}{\partial h_t}(\prod_{j=k+1}^t\dfrac{\partial h_j}{\partial h_{j-1}})\dfrac{\partial h_k}{\partial W}$$</p>
<p>定义 $\beta$ 为范式的下界，那么 $||\dfrac{\partial h_j}{\partial h_{j-1}}||$ 很容易变得很小或很大。</p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn5.png"></p>
<h3 id="解决梯度爆炸或消失的一些tricks"><a href="#解决梯度爆炸或消失的一些tricks" class="headerlink" title="解决梯度爆炸或消失的一些tricks"></a>解决梯度爆炸或消失的一些tricks</h3><h4 id="梯度裁剪-gradient-clipping"><a href="#梯度裁剪-gradient-clipping" class="headerlink" title="梯度裁剪 gradient clipping"></a>梯度裁剪 gradient clipping</h4><p>对于gradient exploding，有个很简单的trick:gradient clipping</p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn7.png"></p>
<p>可以动手实践下，也许对梯度会有更深的理解～</p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn8.png"></p>
<ul>
<li><p>实线Solid lines表示 standard gradient descent trajectories</p>
</li>
<li><p>虚线Dashed lines表示 gradients rescaled to fixed size</p>
</li>
</ul>
<p>将error看作是很多维参数空间的函数,如果是二维的话，那error surface就是一个曲面。在曲面上高曲率的地方(high curvature walls)，其梯度也就很大。</p>
<p>详细的还是看文献吧<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v28/pascanu13.pdf">On the difficulty of training Recurrent Neural Networks, Pascanu</a></p>
<h4 id="对于梯度消失-vanishing-gradients"><a href="#对于梯度消失-vanishing-gradients" class="headerlink" title="对于梯度消失 vanishing gradients"></a>对于梯度消失 vanishing gradients</h4><p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn9.png"></p>
<ul>
<li><p>参数初始化 Initialization</p>
</li>
<li><p>relus, Rectified Relus</p>
</li>
</ul>
<p>很难理解为啥用relu能很好的解决梯度消失的问题，的确relu的梯度为1，但它的非线性也太简单了吧。。。所以得看看原论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1504.00941">A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</a></p>
<h4 id="softmax计算量太大的问题"><a href="#softmax计算量太大的问题" class="headerlink" title="softmax计算量太大的问题"></a>softmax计算量太大的问题</h4><p>对于每个时间步，从隐藏层到输出 $W^{(S)} \in R^{D_h, V}$ ,如果词表很大的话，这个矩阵也就很大了～</p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn10.png"></p>
<h3 id="序列模型的一些其他任务"><a href="#序列模型的一些其他任务" class="headerlink" title="序列模型的一些其他任务"></a>序列模型的一些其他任务</h3><p>Classify each word into:</p>
<ul>
<li><p>NER</p>
</li>
<li><p>Entity level sentiment in context</p>
</li>
<li><p>opinion expression extraction <a target="_blank" rel="noopener" href="http://www.cs.cornell.edu/~oirsoy/drnt.htm">Opinion Mining with Deep Recurrent Neural Networks</a></p>
</li>
</ul>
<h4 id="双向-RNNs"><a href="#双向-RNNs" class="headerlink" title="双向 RNNs"></a>双向 RNNs</h4><p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn13.png"></p>
<p>其实跟rnn没有太多变化，有两个隐藏层，并且隐藏层的递归分别是从语料库的两个不同的方向。</p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn19.png"></p>
<p><strong>Deep bidirectional RNNs</strong></p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn12.png"></p>
<p>$$\overrightarrow {h_t^{(i)}}=f(\overrightarrow{W^{(i)}}h_t^{(i-1)}+\overrightarrow{V^{(i)}}h_{t-1}^{(i)}+\overrightarrow{b^{(i)}})$$</p>
<p>其中 $h_t^{(i-1)}$ 表示上一层隐藏层的输入， $h_{t-1}^{(i)}$ 表示当前隐藏层层的上一个时间步的输入。</p>
<p>$$\overleftarrow {h_t^{(i)}}=f(\overleftarrow{W^{(i)}}h_t^{(i-1)}+\overleftarrow{V^{(i)}}h_{t-1}^{(i)}+\overleftarrow{b^{(i)}})$$</p>
<p>需要训练的参数有：$\overrightarrow{W^{(i)}},\overleftarrow{W^{(i)}}$ $\overrightarrow{V^{(i)}},\overleftarrow{V^{(i)}}$</p>
<p>$$\hat y_t=g(Uh_t+c)=g(U[\overrightarrow{h_t^{(L)}};\overleftarrow{h_t^{(L)}}]+c)$$</p>
<h4 id="data-evalution"><a href="#data-evalution" class="headerlink" title="data evalution"></a>data evalution</h4><p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn14.png"></p>
<h3 id="Presentation"><a href="#Presentation" class="headerlink" title="Presentation"></a>Presentation</h3><p><a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/P15-1032">Structured Training for Neural Network Transition-Based Parsing, David Weiss, Chris Alberti, Michael Collins, Slav Petrov</a></p>
<p><img src="/2018/05/04/cs224d-lecture8-RNN/rnn6.png"></p>
<p>表示根本听不懂，只知道使用deeplearning做依存分析。。用state-of-art的SyntaxNet和前人几篇有影响力的进行了对比～</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>cs224d-lecture8-RNN</p><p><a href="http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/">http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-05-04</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/cs224d/">cs224d</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/05/07/cs224d-lecture9-machine-translation/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">cs224d-lecture9 机器翻译</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/04/30/cs224d-lecture3-Word-Window%E5%88%86%E7%B1%BB%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-item">cs224d-lecture3 基于Window的分类与神经网络</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2018/05/04/cs224d-lecture8-RNN/';
            this.page.identifier = '2018/05/04/cs224d-lecture8-RNN/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>