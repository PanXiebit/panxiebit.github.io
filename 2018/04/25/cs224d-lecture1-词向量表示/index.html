<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>cs224d-lecture1-词向量表示 - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Word Vectors  Skip-gram  Continuous Bag of words(CBOW)  Negative Sampling  Hierarchical SoftmaxM  Word2Vec"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="Word Vectors  Skip-gram  Continuous Bag of words(CBOW)  Negative Sampling  Hierarchical SoftmaxM  Word2Vec"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2018-04-25T11:26:40.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:08.539Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="cs224d"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"},"headline":"cs224d-lecture1-词向量表示","image":["http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/06.png","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/07.png","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/CBOW.png","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/12.jpg","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/13.jpg","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/14.jpg","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/Skip-Gram.png","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/15.jpg","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/17.jpg","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/16.jpg","http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/19.jpg"],"datePublished":"2018-04-25T11:26:40.000Z","dateModified":"2021-06-29T08:12:08.539Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"Word Vectors  Skip-gram  Continuous Bag of words(CBOW)  Negative Sampling  Hierarchical SoftmaxM  Word2Vec"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-04-25T11:26:40.000Z" title="2018/4/25 下午7:26:40">2018-04-25</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:08.539Z" title="2021/6/29 下午4:12:08">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/cs224d/">cs224d</a></span><span class="level-item">10 分钟读完 (大约1428个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">cs224d-lecture1-词向量表示</h1><div class="content"><ul>
<li><p>Word Vectors</p>
</li>
<li><p>Skip-gram</p>
</li>
<li><p>Continuous Bag of words(CBOW)</p>
</li>
<li><p>Negative Sampling</p>
</li>
<li><p>Hierarchical SoftmaxM</p>
</li>
<li><p>Word2Vec</p>
</li>
</ul>
<span id="more"></span>



<h3 id="1-How-to-represent-words"><a href="#1-How-to-represent-words" class="headerlink" title="1. How to represent words?"></a>1. How to represent words?</h3><p>With word vectors, we can quite easily encode this ability in the vectors themselves (using distance measures such as Jaccard, Cosine, Eu-clidean, etc).</p>
<h3 id="2-Word-Vectors"><a href="#2-Word-Vectors" class="headerlink" title="2. Word Vectors"></a>2. Word Vectors</h3><p>encode word tokens into some vector(N-dimensional space, N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfre using speech.</p>
<p><strong>one-hot vector:</strong></p>
<p>V is the size of vocabulary.</p>
<p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/06.png"></p>
<p>each word is a completely independent entity.</p>
<p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/07.png"></p>
<h3 id="3-Iteration-Based-Methods-Word2Vec"><a href="#3-Iteration-Based-Methods-Word2Vec" class="headerlink" title="3. Iteration Based Methods - Word2Vec"></a>3. Iteration Based Methods - Word2Vec</h3><ul>
<li>2 algorithms: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</li>
</ul>
<ul>
<li>2 training methods: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary.</li>
</ul>
<p><strong>language model</strong></p>
<p>Unigram model :</p>
<p>$$P(w_1,w_2,…,w_n) = \prod_{i=1}^nP(w_i)$$</p>
<p>bigram model:</p>
<p>$$P(w_1,w_2,…,w_n) = \prod_{i=1}^nP(w_i|w_{i-1})$$</p>
<h3 id="4-Continuous-bag-of-words-model-CBOW"><a href="#4-Continuous-bag-of-words-model-CBOW" class="headerlink" title="4. Continuous bag of words model(CBOW)"></a>4. Continuous bag of words model(CBOW)</h3><p><strong>predict center word from the context.</strong></p>
<p>$$ \prod_{c=1}^{n}P(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$</p>
<p><strong>negative log likelihood:</strong></p>
<p>$$J(\theta)= -\sum_{c=1}^{n}logP(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$</p>
<p>the words of context to generate the center word is dependent:</p>
<p>$$J(\theta) = \dfrac{1}{n}\sum_{c=1}^T\sum_{-m\le j\le m}logp(w_c|w_{c+j})$$</p>
<p><strong>how to present this probability???</strong></p>
<p>To one sentence:</p>
<p>$$</p>
<p>\begin{align}</p>
<p>minimize J &amp;= -logP(w_c|w_{c-m},..,w_{c-1},w_{c+1},…,w_{c+m})\</p>
<p>&amp;= -log P(u_c|\hat v)\tag{1}\</p>
<p>&amp;= -log \dfrac{exp(u_c^T\hat v)}{\sum_{j=1}^{|V|}exp(u_j^T\hat v)}\tag{2}\</p>
<p>&amp;= -u_c^T\hat v + log\sum_{j=1}^{|V|}exp(u_j^T\hat v)</p>
<p>\end{align}</p>
<p>$$</p>
<p><strong>important: from word to vector</strong> the (1) to (2), using the <strong>softmax</strong> to present the probability</p>
<p> $$P(u_c|\hat v) = \dfrac{exp(u_c^T\hat v)}{\sum_{j=1}^{|V|}exp(u_j^T\hat v)}\tag{* }$$</p>
<p>其实word2vec可以理解为两个word，他们的上下文越相似，那么他们俩的词向量表示也就越相似.比如 he 和 she 大多数情况下他们的语境，也就是上下文出现的单词v都是很接近的，那么同样与这些词内积得到的概率就会差不多～说到底，也是个频率统计的方法，只不过用了无监督学习这个方式来得到distribution vector了～从这个角度理解就很合理了。错误的理解是 u 和v 出现在同一个窗口，他们的内积的概率就越大，这无法解释任何东西。</p>
<h4 id="4-1-We-can-use-an-simple-neural-networt-to-train-this-matrix-weights"><a href="#4-1-We-can-use-an-simple-neural-networt-to-train-this-matrix-weights" class="headerlink" title="4.1 We can use an simple neural networt to train this matrix weights"></a>4.1 We can use an simple neural networt to train this matrix weights</h4><p><strong>input:</strong> $x^{(c)}\in R^{|V|\times 1}$, the input one-hot vector of context</p>
<p><strong>labels:</strong> $y^{(c)}\in R^{|V|\times 1}$, the one hot vector of the known center word.</p>
<p><strong>parameters:</strong></p>
<ul>
<li><p>$w_i$: word i from vocabulary V</p>
</li>
<li><p>$V \in R^{n\times |V|}$ input word matrix</p>
</li>
<li><p>$v_i$:i-th column of $V$, the input vector representation of word $w_i$</p>
</li>
<li><p>$U\in R^{|V|\times n}$: output word matrix</p>
</li>
<li><p>$u_i$: i-th row of $U$, the output vector representation of word $w_i$</p>
</li>
<li><p>n is an arbitrary size which defines the size of our embedding space</p>
</li>
</ul>
<p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/CBOW.png"></p>
<p>there are some differences with the figure….$W_1^{n\times |V|}$, $W_2^{|V|\times n}$</p>
<ul>
<li>input : $x_1.shape = (|V|, 1)$, $x_2.shape = (|V|, 1)$,…,$x_{2m}.shape = (|V|, 1)$</li>
</ul>
<ul>
<li>$W_1$ : input matrix V, $W_1.shape = (n, |V|)$ each column is the representation of $w_i$</li>
</ul>
<ul>
<li>hidden layer: $\hat v = \dfrac{V.dot(x_1)+…+V.dot(x_{2m})}{2m}$, $\hat v.shape = (n,1)$</li>
</ul>
<ul>
<li>$W_2$ : output matrix U, $W_2.shape = (|V|, n)$ each row is the representation of $w_i$</li>
</ul>
<ul>
<li>score: $u.shape = (|V|, 1)$</li>
</ul>
<ul>
<li>output: $\hat y = softmax(u)$, $\hat y.shape=(|V|,1)$</li>
</ul>
<ul>
<li>cross entropy:</li>
</ul>
<p>$$H(\hat y, y) = -\sum_{j=1}^{|V|}y_jlog(\hat y_j)$$</p>
<p>Because y is the one hot vector, and i is the index whose value is 1.</p>
<p>$$H(\hat y, y) = -y_ilog(\hat y_i)$$</p>
<p>look at the paper <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1411.2738">word2vec Parameter Learning Explained</a>, it is very cautious, very wonderful!!! The symbols are different from the above.</p>
<h4 id="4-2-one-word-context-inference"><a href="#4-2-one-word-context-inference" class="headerlink" title="4.2 one word context inference"></a>4.2 one word context inference</h4><p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/12.jpg"></p>
<h4 id="4-3-one-word-context-backpropagation"><a href="#4-3-one-word-context-backpropagation" class="headerlink" title="4.3 one word context backpropagation"></a>4.3 one word context backpropagation</h4><p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/13.jpg"></p>
<h4 id="4-4-multi-words-context"><a href="#4-4-multi-words-context" class="headerlink" title="4.4 multi-words context"></a>4.4 multi-words context</h4><p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/14.jpg"></p>
<h3 id="5-Skip-gram"><a href="#5-Skip-gram" class="headerlink" title="5. Skip-gram"></a>5. Skip-gram</h3><p>$$</p>
<p>\begin{align}</p>
<p>minimize J &amp;=-logP(w_{c-m},…,w_{c-1},w_{c+1},..,w_{c+m}|w_c)\</p>
<p>&amp;=-log\prod_{j=0,j\neq m }^{2m}P(w_{c-m+j}|w_c)\</p>
<p>&amp;=-\sum_{j=0,j\neq m}^{2m}logP(w_{c-m+j}|w_c)\tag{3}\</p>
<p>&amp;=-\sum_{j=0,j\neq m}^{2m}log\dfrac{exp(u_{c-m+j}^Tv_c)}{\sum_{k=1}^{|V|}exp(u_{k}^Tv_c)}\tag{4}\</p>
<p>&amp;=-\sum_{j=0,j\neq m}^{2m}u_{c-m+j}^Tv_c+2m\ log\sum_{k=1}^{|V|}exp(u_{k}^Tv_c)</p>
<p>\end{align}</p>
<p>$$</p>
<blockquote>
<p><strong>important: from word to vector</strong> the (1) to (2), using the <strong>softmax</strong> to present the probability</p>
</blockquote>
<blockquote>
<p>$$P(u_{c-m+j}|w_c) = \dfrac{exp(u_{c-m+j}^Tv_c)}{\sum_{j=1}^{|V|}exp(u_j^Tv_c)}\tag{* }$$</p>
</blockquote>
<blockquote>
<p>V is the input matrix, U is the output matrix</p>
</blockquote>
<h4 id="5-1-We-can-use-the-simple-neural-networks-to-train-matrix-weights"><a href="#5-1-We-can-use-the-simple-neural-networks-to-train-matrix-weights" class="headerlink" title="5.1 We can use the simple neural networks to train matrix weights"></a>5.1 We can use the simple neural networks to train matrix weights</h4><p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/Skip-Gram.png"></p>
<h4 id="5-2-inference-and-backpropagation"><a href="#5-2-inference-and-backpropagation" class="headerlink" title="5.2 inference and backpropagation"></a>5.2 inference and backpropagation</h4><p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/15.jpg"></p>
<blockquote>
<p>Skip-gram treats each context word equally: the models computes the probability for each word of appearing in the context independently of its distance to the center word.</p>
</blockquote>
<h3 id="6-Optimizing-Computational-Efficiency"><a href="#6-Optimizing-Computational-Efficiency" class="headerlink" title="6. Optimizing Computational Efficiency"></a>6. Optimizing Computational Efficiency</h3><h4 id="6-1-Hirarchical-Softmax"><a href="#6-1-Hirarchical-Softmax" class="headerlink" title="6.1 Hirarchical Softmax"></a>6.1 Hirarchical Softmax</h4><p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/17.jpg"></p>
<p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/16.jpg"></p>
<h4 id="6-2-Negative-Sampling"><a href="#6-2-Negative-Sampling" class="headerlink" title="6.2 Negative Sampling"></a>6.2 Negative Sampling</h4><p><strong>loss function:</strong></p>
<p>$$E = -log\sigma(v’^T_{w_O}h)-\sum_{w_j\in W_{neg}}log\sigma(-v’^T_{w_j}h)$$</p>
<ul>
<li><p>in the CBOW, $h=\dfrac{1}{C}\sum_{c=1}^Cv_{w_c^T}$</p>
</li>
<li><p>in the skip-gram, $h=v_{w_I}^T$</p>
</li>
<li><p>how to choose the K negative samples?</p>
<ul>
<li>As described in (Mikolov et al., 2013b), word2vec uses a unigram distribution raised to the 3/4th power for the best quality of results.</li>
</ul>
</li>
</ul>
<p>关于负采样的原理的理解：</p>
<p><a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa">Intuitive explanation of Noise Contrastive Estimation (NCE) loss?</a></p>
<blockquote>
<p>The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead.    </p>
</blockquote>
<p>For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of kk randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.     </p>
<p>This is important: instead of predicting the next word (the “standard” training technique), the optimized classifier simply predicts whether a pair of words is good or bad.</p>
<p><img src="/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/19.jpg"></p>
<p>reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1411.2738">word2vec Parameter Learning Explained</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.hankcs.com/nlp/word2vec.html">word2vec原理推导与代码分析</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>cs224d-lecture1-词向量表示</p><p><a href="http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-词向量表示/">http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-词向量表示/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-04-25</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/cs224d/">cs224d</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/04/30/cs224d-lecture2-%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E9%AB%98%E7%BA%A7%E8%A1%A8%E7%A4%BA/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">cs224d-lecture2-词向量的高级表示</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/04/23/chapter14-dependency-Parsing/"><span class="level-item">chapter14 dependency Parsing1</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/';
            this.page.identifier = '2018/04/25/cs224d-lecture1-词向量表示/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">117</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>