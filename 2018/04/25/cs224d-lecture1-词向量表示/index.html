<!doctype html>
<html lang="de"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>cs224d-lecture1-词向量表示 - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="潘小榭"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="潘小榭"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Word Vectors  Skip-gram  Continuous Bag of words(CBOW)  Negative Sampling  Hierarchical SoftmaxM  Word2Vec"><meta property="og:type" content="blog"><meta property="og:title" content="cs224d-lecture1-词向量表示"><meta property="og:url" content="http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"><meta property="og:site_name" content="潘小榭"><meta property="og:description" content="Word Vectors  Skip-gram  Continuous Bag of words(CBOW)  Negative Sampling  Hierarchical SoftmaxM  Word2Vec"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/06.png"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/07.png"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/CBOW.png"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/12.jpg"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/13.jpg"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/14.jpg"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/Skip-Gram.png"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/15.jpg"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/17.jpg"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/16.jpg"><meta property="og:image" content="http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/19.jpg"><meta property="article:published_time" content="2018-04-25T11:26:40.000Z"><meta property="article:modified_time" content="2021-06-29T05:19:29.428Z"><meta property="article:author" content="Xie Pan"><meta property="article:tag" content="cs224d"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/06.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"},"headline":"cs224d-lecture1-词向量表示","image":["http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/06.png","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/07.png","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/CBOW.png","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/12.jpg","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/13.jpg","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/14.jpg","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/Skip-Gram.png","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/15.jpg","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/17.jpg","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/16.jpg","http://www.panxiaoxie.cn/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/19.jpg"],"datePublished":"2018-04-25T11:26:40.000Z","dateModified":"2021-06-29T05:19:29.428Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/logo.svg"}},"description":"Word Vectors  Skip-gram  Continuous Bag of words(CBOW)  Negative Sampling  Hierarchical SoftmaxM  Word2Vec"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Suche" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Gepostet vor&nbsp;<time dateTime="2018-04-25T11:26:40.000Z" title="2018/4/25 下午7:26:40">2018-04-25</time></span><span class="level-item">Aktualisiert vor&nbsp;<time dateTime="2021-06-29T05:19:29.428Z" title="2021/6/29 下午1:19:29">2021-06-29</time></span><span class="level-item"><a class="link-muted" href="/categories/cs224d/">cs224d</a></span><span class="level-item">10 minutes lesen (Über 1428 Wörter)</span></div></div><h1 class="title is-3 is-size-4-mobile">cs224d-lecture1-词向量表示</h1><div class="content"><ul>
<li><p>Word Vectors</p>
</li>
<li><p>Skip-gram</p>
</li>
<li><p>Continuous Bag of words(CBOW)</p>
</li>
<li><p>Negative Sampling</p>
</li>
<li><p>Hierarchical SoftmaxM</p>
</li>
<li><p>Word2Vec</p>
</li>
</ul>
<span id="more"></span>



<h3 id="1-How-to-represent-words"><a href="#1-How-to-represent-words" class="headerlink" title="1. How to represent words?"></a>1. How to represent words?</h3><p>With word vectors, we can quite easily encode this ability in the vectors themselves (using distance measures such as Jaccard, Cosine, Eu-clidean, etc).</p>
<h3 id="2-Word-Vectors"><a href="#2-Word-Vectors" class="headerlink" title="2. Word Vectors"></a>2. Word Vectors</h3><p>encode word tokens into some vector(N-dimensional space, N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfre using speech.</p>
<p><strong>one-hot vector:</strong></p>
<p>V is the size of vocabulary.</p>
<p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/06.png"></p>
<p>each word is a completely independent entity.</p>
<p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/07.png"></p>
<h3 id="3-Iteration-Based-Methods-Word2Vec"><a href="#3-Iteration-Based-Methods-Word2Vec" class="headerlink" title="3. Iteration Based Methods - Word2Vec"></a>3. Iteration Based Methods - Word2Vec</h3><ul>
<li>2 algorithms: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</li>
</ul>
<ul>
<li>2 training methods: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary.</li>
</ul>
<p><strong>language model</strong></p>
<p>Unigram model :</p>
<p>$$P(w_1,w_2,…,w_n) = \prod_{i=1}^nP(w_i)$$</p>
<p>bigram model:</p>
<p>$$P(w_1,w_2,…,w_n) = \prod_{i=1}^nP(w_i|w_{i-1})$$</p>
<h3 id="4-Continuous-bag-of-words-model-CBOW"><a href="#4-Continuous-bag-of-words-model-CBOW" class="headerlink" title="4. Continuous bag of words model(CBOW)"></a>4. Continuous bag of words model(CBOW)</h3><p><strong>predict center word from the context.</strong></p>
<p>$$ \prod_{c=1}^{n}P(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$</p>
<p><strong>negative log likelihood:</strong></p>
<p>$$J(\theta)= -\sum_{c=1}^{n}logP(w^{(c)}|w^{(c-m)},…,w^{(c-1)},w^{(c+1)},…,w^{(c+m)})$$</p>
<p>the words of context to generate the center word is dependent:</p>
<p>$$J(\theta) = \dfrac{1}{n}\sum_{c=1}^T\sum_{-m\le j\le m}logp(w_c|w_{c+j})$$</p>
<p><strong>how to present this probability???</strong></p>
<p>To one sentence:</p>
<p>$$</p>
<p>\begin{align}</p>
<p>minimize J &amp;= -logP(w_c|w_{c-m},..,w_{c-1},w_{c+1},…,w_{c+m})\</p>
<p>&amp;= -log P(u_c|\hat v)\tag{1}\</p>
<p>&amp;= -log \dfrac{exp(u_c^T\hat v)}{\sum_{j=1}^{|V|}exp(u_j^T\hat v)}\tag{2}\</p>
<p>&amp;= -u_c^T\hat v + log\sum_{j=1}^{|V|}exp(u_j^T\hat v)</p>
<p>\end{align}</p>
<p>$$</p>
<p><strong>important: from word to vector</strong> the (1) to (2), using the <strong>softmax</strong> to present the probability</p>
<p> $$P(u_c|\hat v) = \dfrac{exp(u_c^T\hat v)}{\sum_{j=1}^{|V|}exp(u_j^T\hat v)}\tag{* }$$</p>
<p>其实word2vec可以理解为两个word，他们的上下文越相似，那么他们俩的词向量表示也就越相似.比如 he 和 she 大多数情况下他们的语境，也就是上下文出现的单词v都是很接近的，那么同样与这些词内积得到的概率就会差不多～说到底，也是个频率统计的方法，只不过用了无监督学习这个方式来得到distribution vector了～从这个角度理解就很合理了。错误的理解是 u 和v 出现在同一个窗口，他们的内积的概率就越大，这无法解释任何东西。</p>
<h4 id="4-1-We-can-use-an-simple-neural-networt-to-train-this-matrix-weights"><a href="#4-1-We-can-use-an-simple-neural-networt-to-train-this-matrix-weights" class="headerlink" title="4.1 We can use an simple neural networt to train this matrix weights"></a>4.1 We can use an simple neural networt to train this matrix weights</h4><p><strong>input:</strong> $x^{(c)}\in R^{|V|\times 1}$, the input one-hot vector of context</p>
<p><strong>labels:</strong> $y^{(c)}\in R^{|V|\times 1}$, the one hot vector of the known center word.</p>
<p><strong>parameters:</strong></p>
<ul>
<li><p>$w_i$: word i from vocabulary V</p>
</li>
<li><p>$V \in R^{n\times |V|}$ input word matrix</p>
</li>
<li><p>$v_i$:i-th column of $V$, the input vector representation of word $w_i$</p>
</li>
<li><p>$U\in R^{|V|\times n}$: output word matrix</p>
</li>
<li><p>$u_i$: i-th row of $U$, the output vector representation of word $w_i$</p>
</li>
<li><p>n is an arbitrary size which defines the size of our embedding space</p>
</li>
</ul>
<p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/CBOW.png"></p>
<p>there are some differences with the figure….$W_1^{n\times |V|}$, $W_2^{|V|\times n}$</p>
<ul>
<li>input : $x_1.shape = (|V|, 1)$, $x_2.shape = (|V|, 1)$,…,$x_{2m}.shape = (|V|, 1)$</li>
</ul>
<ul>
<li>$W_1$ : input matrix V, $W_1.shape = (n, |V|)$ each column is the representation of $w_i$</li>
</ul>
<ul>
<li>hidden layer: $\hat v = \dfrac{V.dot(x_1)+…+V.dot(x_{2m})}{2m}$, $\hat v.shape = (n,1)$</li>
</ul>
<ul>
<li>$W_2$ : output matrix U, $W_2.shape = (|V|, n)$ each row is the representation of $w_i$</li>
</ul>
<ul>
<li>score: $u.shape = (|V|, 1)$</li>
</ul>
<ul>
<li>output: $\hat y = softmax(u)$, $\hat y.shape=(|V|,1)$</li>
</ul>
<ul>
<li>cross entropy:</li>
</ul>
<p>$$H(\hat y, y) = -\sum_{j=1}^{|V|}y_jlog(\hat y_j)$$</p>
<p>Because y is the one hot vector, and i is the index whose value is 1.</p>
<p>$$H(\hat y, y) = -y_ilog(\hat y_i)$$</p>
<p>look at the paper <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1411.2738">word2vec Parameter Learning Explained</a>, it is very cautious, very wonderful!!! The symbols are different from the above.</p>
<h4 id="4-2-one-word-context-inference"><a href="#4-2-one-word-context-inference" class="headerlink" title="4.2 one word context inference"></a>4.2 one word context inference</h4><p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/12.jpg"></p>
<h4 id="4-3-one-word-context-backpropagation"><a href="#4-3-one-word-context-backpropagation" class="headerlink" title="4.3 one word context backpropagation"></a>4.3 one word context backpropagation</h4><p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/13.jpg"></p>
<h4 id="4-4-multi-words-context"><a href="#4-4-multi-words-context" class="headerlink" title="4.4 multi-words context"></a>4.4 multi-words context</h4><p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/14.jpg"></p>
<h3 id="5-Skip-gram"><a href="#5-Skip-gram" class="headerlink" title="5. Skip-gram"></a>5. Skip-gram</h3><p>$$</p>
<p>\begin{align}</p>
<p>minimize J &amp;=-logP(w_{c-m},…,w_{c-1},w_{c+1},..,w_{c+m}|w_c)\</p>
<p>&amp;=-log\prod_{j=0,j\neq m }^{2m}P(w_{c-m+j}|w_c)\</p>
<p>&amp;=-\sum_{j=0,j\neq m}^{2m}logP(w_{c-m+j}|w_c)\tag{3}\</p>
<p>&amp;=-\sum_{j=0,j\neq m}^{2m}log\dfrac{exp(u_{c-m+j}^Tv_c)}{\sum_{k=1}^{|V|}exp(u_{k}^Tv_c)}\tag{4}\</p>
<p>&amp;=-\sum_{j=0,j\neq m}^{2m}u_{c-m+j}^Tv_c+2m\ log\sum_{k=1}^{|V|}exp(u_{k}^Tv_c)</p>
<p>\end{align}</p>
<p>$$</p>
<blockquote>
<p><strong>important: from word to vector</strong> the (1) to (2), using the <strong>softmax</strong> to present the probability</p>
</blockquote>
<blockquote>
<p>$$P(u_{c-m+j}|w_c) = \dfrac{exp(u_{c-m+j}^Tv_c)}{\sum_{j=1}^{|V|}exp(u_j^Tv_c)}\tag{* }$$</p>
</blockquote>
<blockquote>
<p>V is the input matrix, U is the output matrix</p>
</blockquote>
<h4 id="5-1-We-can-use-the-simple-neural-networks-to-train-matrix-weights"><a href="#5-1-We-can-use-the-simple-neural-networks-to-train-matrix-weights" class="headerlink" title="5.1 We can use the simple neural networks to train matrix weights"></a>5.1 We can use the simple neural networks to train matrix weights</h4><p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/Skip-Gram.png"></p>
<h4 id="5-2-inference-and-backpropagation"><a href="#5-2-inference-and-backpropagation" class="headerlink" title="5.2 inference and backpropagation"></a>5.2 inference and backpropagation</h4><p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/15.jpg"></p>
<blockquote>
<p>Skip-gram treats each context word equally: the models computes the probability for each word of appearing in the context independently of its distance to the center word.</p>
</blockquote>
<h3 id="6-Optimizing-Computational-Efficiency"><a href="#6-Optimizing-Computational-Efficiency" class="headerlink" title="6. Optimizing Computational Efficiency"></a>6. Optimizing Computational Efficiency</h3><h4 id="6-1-Hirarchical-Softmax"><a href="#6-1-Hirarchical-Softmax" class="headerlink" title="6.1 Hirarchical Softmax"></a>6.1 Hirarchical Softmax</h4><p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/17.jpg"></p>
<p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/16.jpg"></p>
<h4 id="6-2-Negative-Sampling"><a href="#6-2-Negative-Sampling" class="headerlink" title="6.2 Negative Sampling"></a>6.2 Negative Sampling</h4><p><strong>loss function:</strong></p>
<p>$$E = -log\sigma(v’^T_{w_O}h)-\sum_{w_j\in W_{neg}}log\sigma(-v’^T_{w_j}h)$$</p>
<ul>
<li><p>in the CBOW, $h=\dfrac{1}{C}\sum_{c=1}^Cv_{w_c^T}$</p>
</li>
<li><p>in the skip-gram, $h=v_{w_I}^T$</p>
</li>
<li><p>how to choose the K negative samples?</p>
<ul>
<li>As described in (Mikolov et al., 2013b), word2vec uses a unigram distribution raised to the 3/4th power for the best quality of results.</li>
</ul>
</li>
</ul>
<p>关于负采样的原理的理解：</p>
<p><a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa">Intuitive explanation of Noise Contrastive Estimation (NCE) loss?</a></p>
<blockquote>
<p>The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead.    </p>
</blockquote>
<p>For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of kk randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.     </p>
<p>This is important: instead of predicting the next word (the “standard” training technique), the optimized classifier simply predicts whether a pair of words is good or bad.</p>
<p><img src="/cs224d-lecture1-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/19.jpg"></p>
<p>reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1411.2738">word2vec Parameter Learning Explained</a></li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.hankcs.com/nlp/word2vec.html">word2vec原理推导与代码分析</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>cs224d-lecture1-词向量表示</p><p><a href="http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-词向量表示/">http://www.panxiaoxie.cn/2018/04/25/cs224d-lecture1-词向量表示/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2018-04-25</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/cs224d/">cs224d</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Gefällt Ihnen der Artikel? Unterstützen Sie den Autor mit</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Kauf mir einen Kaffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/04/30/cs224d-lecture2-%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E9%AB%98%E7%BA%A7%E8%A1%A8%E7%A4%BA/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">cs224d-lecture2-词向量的高级表示</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/04/23/chapter14-dependency-Parsing/"><span class="level-item">chapter14 dependency Parsing1</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Kommentare</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Seiten</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Kategorien</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">8</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Folgen</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Kategorien</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Letzte Einträge</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-05T02:03:16.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-contrastive-learning/">论文笔记-contrastive learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:07:08.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-video-transformer/">论文笔记-video transformer</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-29T01:05:10.000Z">2021-04-29</time></p><p class="title"><a href="/2021/04/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-dynamic-convolution-and-involution/">论文笔记-dynamic convolution and involution</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-16T07:48:48.000Z">2021-04-16</time></p><p class="title"><a href="/2021/04/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-unlikelihood-training/">论文笔记-unlikelihood training</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-12T08:16:35.000Z">2021-04-12</time></p><p class="title"><a href="/2021/04/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deformable-DETR/">论文笔记-DETR and Deformable DETR</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archive</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai-challenger/"><span class="tag">ai challenger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs224d/"><span class="tag">cs224d</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language-model/"><span class="tag">language model</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vision-transformer/"><span class="tag">vision transformer</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Abonnieren Sie Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Abonnieren"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("default");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Zurück nach oben" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "Diese Website verwendet Cookies, um Ihre Erfahrung zu verbessern.",
          dismiss: "Verstanden!",
          allow: "Cookies zulassen",
          deny: "Ablehnen",
          link: "Mehr erfahren",
          policy: "Cookie-Richtlinie",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Tippen Sie etwas..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Tippen Sie etwas...","untitled":"(Ohne Titel)","posts":"Seiten","pages":"Pages","categories":"Kategorien","tags":"Tags"});
        });</script></body></html>