<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>论文笔记-batch,layer,weights normalization - 潘小榭</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="black"><meta name="application-name" content="panxiaoxie"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="msapplication-TileColor" content="black"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="panxiaoxie"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="paper:    Batch Normalization    Layer Normalization    weights Normalization     Batch Normalization在之前的笔记已经详细看过了:深度学习-Batch Normalization Layer NormalizationMotivation batch normalization uses the d"><meta property="og:type" content="blog"><meta property="og:title" content="panxiaoxie"><meta property="og:url" content="https://github.com/PanXiebit"><meta property="og:site_name" content="panxiaoxie"><meta property="og:description" content="paper:    Batch Normalization    Layer Normalization    weights Normalization     Batch Normalization在之前的笔记已经详细看过了:深度学习-Batch Normalization Layer NormalizationMotivation batch normalization uses the d"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.qt86.com/cache/1625298592_187938.png"><meta property="article:published_time" content="2018-10-01T01:50:57.000Z"><meta property="article:modified_time" content="2021-06-29T08:12:09.140Z"><meta property="article:author" content="panxiaoxie"><meta property="article:tag" content="DL"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://www.qt86.com/cache/1625298592_187938.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/"},"headline":"论文笔记-batch,layer,weights normalization","image":["http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/01.png","http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/02.png","http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/03.png","http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/04.jpg","http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/05.png","http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/06.png"],"datePublished":"2018-10-01T01:50:57.000Z","dateModified":"2021-06-29T08:12:09.140Z","author":{"@type":"Person","name":"Xie Pan"},"publisher":{"@type":"Organization","name":"潘小榭","logo":{"@type":"ImageObject","url":"http://www.panxiaoxie.cn/img/panxiaoxie.png"}},"description":"paper:    Batch Normalization    Layer Normalization    weights Normalization     Batch Normalization在之前的笔记已经详细看过了:深度学习-Batch Normalization Layer NormalizationMotivation batch normalization uses the d"}</script><link rel="canonical" href="http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-10-01T01:50:57.000Z" title="2018/10/1 上午9:50:57">2018-10-01</time>发表</span><span class="level-item"><time dateTime="2021-06-29T08:12:09.140Z" title="2021/6/29 下午4:12:09">2021-06-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/">DL</a></span><span class="level-item">16 分钟读完 (大约2451个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">论文笔记-batch,layer,weights normalization</h1><div class="content"><p>paper:  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">Batch Normalization</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>  </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1602.07868.pdf">weights Normalization</a>  </p>
</li>
</ul>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>在之前的笔记已经详细看过了:<a target="_blank" rel="noopener" href="https://panxiaoxie.cn/2018/07/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Batch-Normalization/">深度学习-Batch Normalization</a></p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.  </p>
</blockquote>
<p>关于 batch normalisztion.</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/01.png"></p>
<p>从 Ng 的课上截来的一张图，全链接层相比卷积层更容易理解点，但形式上是一样的.  </p>
<p>样本数量是 m，第 l 层经过激活函数输出是第 l+1 层的输入，其中第 i 个神经元的值:  </p>
<p>线性输出： $z_i^l={w_i^l}^Th^l$.  </p>
<p>非线性输出： $h_i^{l+1} = a_i^l=f(z_i^l+b_i^l)$</p>
<p>其中 f 是非线性激活函数，$a_i^l$ 是下一层的 summed inputs. 如果 $a_i^l$ 的分布变化较大（change in a highly correlated way）,下一层的权重 $w^{l+1}$ 的梯度也会相应变化很大（反向传播中 $w^{l+1}$ 的梯度就是 $a_i^l$）。</p>
<p>Batch Normalization 就是将线性输出归一化。  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/02.png"></p>
<p>其中 $u_i^l$ 是均值，$\sigma_i^l$ 是方差。 $\overline a_i^l$ 是归一化之后的输出。 $g_i^l$ 是需要学习的参数，也就是 scale.</p>
<blockquote>
<p>有个疑问？为什么 BN 要在激活函数之前进行，而不是之后进行呢？</p>
</blockquote>
<p>上图中是单个样本，而所有的样本其实是共享层与层之间的参数的。样本与样本之间也存在差异，所以在某一个特征维度上进行归一化，（每一层其中的一个神经元可以看作一个特征维度）。</p>
<blockquote>
<p>batch normalization requires running averages of the summed input statistics. In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps.  </p>
</blockquote>
<p>BN 不是用于 RNN 是因为 batch 中的 sentence 长度不一致。我们可以把每一个时间步看作一个维度的特征提取，如果像 BN 一样在这个维度上进行归一化，显然在 RNN 上是行不通的。比如这个 batch 中最长的序列的最后一个时间步，他的均值就是它本身了，岂不是出现了 BN 在单个样本上训练的情况。</p>
<blockquote>
<p>In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case.  </p>
</blockquote>
<p>所以作者在这篇 paper 中提出了 Layer Normalization. 在单个样本上计算均值和方差进行归一化。然而是怎么进行的呢？</p>
<h3 id="Layer-Normalization-1"><a href="#Layer-Normalization-1" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>layer normalization 并不是在样本上求平均值和方差，而是在 hidden units 上求平均值和方差。</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/03.png"></p>
<p>其中 H 是 hidden units 的个数。</p>
<p>BN 和 LN 的差异：  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/04.jpg"></p>
<p>Layer normalisztion 在单个样本上取均值和方差，所以在训练和测试阶段都是一致的。</p>
<p>并且，尽管求均值和方差的方式不一样，但是在转换成 beta 和 gamma 的方式是一样的，都是在 channels 或者说 hidden_size 上进行的。</p>
<h3 id="Layer-normalized-recurrent-neural-networks"><a href="#Layer-normalized-recurrent-neural-networks" class="headerlink" title="Layer normalized recurrent neural networks"></a>Layer normalized recurrent neural networks</h3><blockquote>
<p>RNN is common among the NLP tasks to have different sentence lengths for different training cases. This is easy to deal with in an RNN because the same weights are used at every time-step. But when we apply batch normalization to an RNN in the obvious way, we need to to compute and store separate statistics for each time step in a sequence. This is problematic if a test sequence is longer than any of the training sequences. Layer normalization does not have such problem because its normalization terms depend only on the summed inputs to a layer at the current time-step. It also has only one set of gain and bias parameters shared over all time-steps.  </p>
</blockquote>
<p>这一部分也解释了 BN 不适用于 RNN 的原因，从 test sequence longer 的角度。RNN 的每个时间步计算共享参数权重.</p>
<p>$a^t=W_{hh}h^{t-1}+W_{xh}x^t$</p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/05.png"></p>
<p>其中 b 和 g 是可学习的参数。</p>
<p><strong>layer normalize 在 LSTM 上的使用：</strong>  </p>
<p><img src="/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/06.png"></p>
<h2 id="tensorflow-实现"><a href="#tensorflow-实现" class="headerlink" title="tensorflow 实现"></a>tensorflow 实现</h2><h3 id="batch-Normalization"><a href="#batch-Normalization" class="headerlink" title="batch Normalization"></a>batch Normalization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.training.moving_averages <span class="keyword">import</span> assign_moving_average</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.layers <span class="keyword">import</span> batch_norm</span><br><span class="line"></span><br><span class="line"><span class="comment">### batch normalization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span>(<span class="params">inputs, decay=<span class="number">0.9</span>, is_training=<span class="literal">True</span>, epsilon=<span class="number">1e-6</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param inputs:  [batch, length, width, channels]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param is_training:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param eplison:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    pop_mean = tf.Variable(tf.zeros(inputs.shape[-<span class="number">1</span>]), trainable=<span class="literal">False</span>, name=<span class="string">&quot;pop_mean&quot;</span>)</span><br><span class="line"></span><br><span class="line">    pop_var = tf.Variable(tf.ones(inputs.shape[-<span class="number">1</span>]), trainable=<span class="literal">False</span>, name=<span class="string">&quot;pop_variance&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mean_and_var</span>():</span></span><br><span class="line"></span><br><span class="line">        axes = <span class="built_in">list</span>(<span class="built_in">range</span>(inputs.shape.ndims))</span><br><span class="line"></span><br><span class="line">        batch_mean, batch_var = tf.nn.moments(inputs, axes=axes)</span><br><span class="line"></span><br><span class="line">        moving_average_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (<span class="number">1</span>-decay))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 也可用 assign_moving_average(pop_mean, batch_mean, decay)</span></span><br><span class="line"></span><br><span class="line">        moving_average_var = tf.assign(pop_var, pop_var * decay + batch_var * (<span class="number">1</span>-decay))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 也可用 assign_moving_average(pop_var, batch_var, decay)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.control_dependencies([moving_average_mean, moving_average_var]):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> tf.identity(batch_mean), tf.identity(batch_var)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    mean, variance = tf.cond(tf.equal(is_training, <span class="literal">True</span>), update_mean_and_var,</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">lambda</span>: (pop_mean, pop_var))</span><br><span class="line"></span><br><span class="line">    beta = tf.Variable(initial_value=tf.zeros(inputs.get_shape()[-<span class="number">1</span>]), name=<span class="string">&quot;shift&quot;</span>)</span><br><span class="line"></span><br><span class="line">    gamma = tf.Variable(initial_value=tf.ones(inputs.get_shape()[-<span class="number">1</span>]), name=<span class="string">&quot;scale&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="layer-normalization"><a href="#layer-normalization" class="headerlink" title="layer normalization"></a>layer normalization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">hidden_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">whh = tf.random_normal(shape=[batch, hidden_size], mean=<span class="number">5.0</span>, stddev=<span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">whh_norm = tf.contrib.layers.layer_norm(inputs=whh, center=<span class="literal">True</span>, scale=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(whh)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(whh_norm)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh[<span class="number">0</span>]), tf.reduce_mean(whh[<span class="number">1</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh_norm[<span class="number">0</span>]), tf.reduce_mean(whh_norm[<span class="number">5</span>]), tf.reduce_mean(whh_norm[<span class="number">59</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sess.run([tf.reduce_mean(whh_norm[:,<span class="number">0</span>]), tf.reduce_mean(whh_norm[:,<span class="number">1</span>]), tf.reduce_mean(whh_norm[:,<span class="number">63</span>])]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(var)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(sess.run(var))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Tensor(&quot;random_normal:0&quot;, shape=(60, 64), dtype=float32)</span><br><span class="line"></span><br><span class="line">Tensor(&quot;LayerNorm/batchnorm/add_1:0&quot;, shape=(60, 64), dtype=float32)</span><br><span class="line"></span><br><span class="line">[5.3812757, 4.607581]</span><br><span class="line"></span><br><span class="line">[-1.4901161e-08, -2.9802322e-08, -3.7252903e-09]</span><br><span class="line"></span><br><span class="line">[-0.22264712, 0.14112064, -0.07268284]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;tf.Variable &#x27;LayerNorm/beta:0&#x27; shape=(64,) dtype=float32_ref&gt;</span><br><span class="line"></span><br><span class="line">[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"></span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span><br><span class="line"></span><br><span class="line"> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"></span><br><span class="line">&lt;tf.Variable &#x27;LayerNorm/gamma:0&#x27; shape=(64,) dtype=float32_ref&gt;</span><br><span class="line"></span><br><span class="line">[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.</span><br><span class="line"></span><br><span class="line"> 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.</span><br><span class="line"></span><br><span class="line"> 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p> 发现一个很奇怪的问题， layer norm 是在每一个训练样本上求均值和方差，为啥 beta 和 gamma 的shape却是 [hidden_size]. 按理说不应该是 [batch,] 吗？ 带着疑问去看了源码，原来是这样的。。</p>
<p> 将源码用简介的方式写出来了：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm_mine</span>(<span class="params">inputs, epsilon=<span class="number">1e-12</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    inputs: [batch, sequence_len, hidden_size] or [batch, hidden_size]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    inputs_shape = inputs.shape</span><br><span class="line"></span><br><span class="line">    inputs_rank = inputs_shape.ndims</span><br><span class="line"></span><br><span class="line">    params_shape = inputs_shape[-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    beta, gamma = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> center:</span><br><span class="line"></span><br><span class="line">        beta = tf.get_variable(</span><br><span class="line"></span><br><span class="line">            name=<span class="string">&quot;beta&quot;</span>,</span><br><span class="line"></span><br><span class="line">            shape=params_shape,</span><br><span class="line"></span><br><span class="line">            initializer=tf.zeros_initializer(),</span><br><span class="line"></span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> scale:</span><br><span class="line"></span><br><span class="line">        gamma = tf.get_variable(</span><br><span class="line"></span><br><span class="line">            name=<span class="string">&quot;gamma&quot;</span>,</span><br><span class="line"></span><br><span class="line">            shape=params_shape,</span><br><span class="line"></span><br><span class="line">            initializer=tf.ones_initializer(),</span><br><span class="line"></span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    norm_axes = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, inputs_rank))</span><br><span class="line"></span><br><span class="line">    mean, variance = tf.nn.moments(inputs, norm_axes, keep_dims=<span class="literal">True</span>)      <span class="comment"># [batch]</span></span><br><span class="line"></span><br><span class="line">    inv = tf.rsqrt(variance + epsilon)</span><br><span class="line"></span><br><span class="line">    inv *= gamma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inputs*inv + ((beta-mean)*inv <span class="keyword">if</span> beta <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> - mean * inv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">hidden_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">whh = tf.random_normal(shape=[batch, hidden_size], mean=<span class="number">5.0</span>, stddev=<span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">whh_norm = layer_norm_mine(whh)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>layer_norm_mine 得到的结果与源码一致。可以发现 计算均值和方差时， <code>tf.nn.moments</code> 中 <code>axes=[1:-1]</code>. （tf.nn.moments 中 axes 的含义是在这些维度上求均值和方差）. 也就是说得到的均值和方差确实是 [batch,]. 只是在转换成 beta 和 gamma 的分布时，依旧是在最后一个维度上进行的。有意思，所以最终的效果应该和 batch normalization 效果是一致的。只不过是否符合图像或文本的特性就另说了。</p>
<h3 id="LayerNormBasicLSTMCell"><a href="#LayerNormBasicLSTMCell" class="headerlink" title="LayerNormBasicLSTMCell"></a>LayerNormBasicLSTMCell</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNormBasicLSTMCell</span>(<span class="params">rnn_cell_impl.RNNCell</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;LSTM unit with layer normalization and recurrent dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This class adds layer normalization and recurrent dropout to a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  basic LSTM unit. Layer normalization implementation is based on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1607.06450.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Layer Normalization&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  and is applied before the internal nonlinearities.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Recurrent dropout is base on:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1603.05118</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;Recurrent Dropout without Memory Loss&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               num_units,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               forget_bias=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               input_size=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               activation=math_ops.tanh,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               layer_norm=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               norm_gain=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               norm_shift=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_keep_prob=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               dropout_prob_seed=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">               reuse=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initializes the basic LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      num_units: int, The number of units in the LSTM cell.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      forget_bias: float, The bias added to forget gates (see above).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      input_size: Deprecated and unused.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      activation: Activation function of the inner states.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      layer_norm: If `True`, layer normalization will be applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      norm_gain: float, The layer normalization gain initial value. If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `layer_norm` has been set to `False`, this argument will be ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      norm_shift: float, The layer normalization shift initial value. If</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `layer_norm` has been set to `False`, this argument will be ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_keep_prob: unit Tensor or float between 0 and 1 representing the</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        recurrent dropout probability value. If float and 1.0, no dropout will</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        be applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      dropout_prob_seed: (optional) integer, the randomness seed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        the given variables, an error is raised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">super</span>(LayerNormBasicLSTMCell, self).__init__(_reuse=reuse)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">      logging.warn(<span class="string">&quot;%s: The input_size parameter is deprecated.&quot;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self._num_units = num_units</span><br><span class="line"></span><br><span class="line">    self._activation = activation</span><br><span class="line"></span><br><span class="line">    self._forget_bias = forget_bias</span><br><span class="line"></span><br><span class="line">    self._keep_prob = dropout_keep_prob</span><br><span class="line"></span><br><span class="line">    self._seed = dropout_prob_seed</span><br><span class="line"></span><br><span class="line">    self._layer_norm = layer_norm</span><br><span class="line"></span><br><span class="line">    self._norm_gain = norm_gain</span><br><span class="line"></span><br><span class="line">    self._norm_shift = norm_shift</span><br><span class="line"></span><br><span class="line">    self._reuse = reuse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rnn_cell_impl.LSTMStateTuple(self._num_units, self._num_units)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_norm</span>(<span class="params">self, inp, scope, dtype=dtypes.float32</span>):</span></span><br><span class="line"></span><br><span class="line">    shape = inp.get_shape()[-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    gamma_init = init_ops.constant_initializer(self._norm_gain)</span><br><span class="line"></span><br><span class="line">    beta_init = init_ops.constant_initializer(self._norm_shift)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(scope):</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Initialize beta and gamma for use by layer_norm.</span></span><br><span class="line"></span><br><span class="line">      vs.get_variable(<span class="string">&quot;gamma&quot;</span>, shape=shape, initializer=gamma_init, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      vs.get_variable(<span class="string">&quot;beta&quot;</span>, shape=shape, initializer=beta_init, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    normalized = layers.layer_norm(inp, reuse=<span class="literal">True</span>, scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> normalized</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_linear</span>(<span class="params">self, args</span>):</span></span><br><span class="line"></span><br><span class="line">    out_size = <span class="number">4</span> * self._num_units</span><br><span class="line"></span><br><span class="line">    proj_size = args.get_shape()[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dtype = args.dtype</span><br><span class="line"></span><br><span class="line">    weights = vs.get_variable(<span class="string">&quot;kernel&quot;</span>, [proj_size, out_size], dtype=dtype)</span><br><span class="line"></span><br><span class="line">    out = math_ops.matmul(args, weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      bias = vs.get_variable(<span class="string">&quot;bias&quot;</span>, [out_size], dtype=dtype)</span><br><span class="line"></span><br><span class="line">      out = nn_ops.bias_add(out, bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;LSTM cell with layer normalization and recurrent dropout.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    c, h = state</span><br><span class="line"></span><br><span class="line">    args = array_ops.concat([inputs, h], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    concat = self._linear(args)</span><br><span class="line"></span><br><span class="line">    dtype = args.dtype</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=<span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      i = self._norm(i, <span class="string">&quot;input&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      j = self._norm(j, <span class="string">&quot;transform&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      f = self._norm(f, <span class="string">&quot;forget&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">      o = self._norm(o, <span class="string">&quot;output&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    g = self._activation(j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">not</span> <span class="built_in">isinstance</span>(self._keep_prob, <span class="built_in">float</span>)) <span class="keyword">or</span> self._keep_prob &lt; <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">      g = nn_ops.dropout(g, self._keep_prob, seed=self._seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_c = (</span><br><span class="line"></span><br><span class="line">        c * math_ops.sigmoid(f + self._forget_bias) + math_ops.sigmoid(i) * g)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._layer_norm:</span><br><span class="line"></span><br><span class="line">      new_c = self._norm(new_c, <span class="string">&quot;state&quot;</span>, dtype=dtype)</span><br><span class="line"></span><br><span class="line">    new_h = self._activation(new_c) * math_ops.sigmoid(o)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_h, new_state</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div><div class="article-licensing box"><div class="licensing-title"><p>论文笔记-batch,layer,weights normalization</p><p><a href="http://www.panxiaoxie.cn/2018/10/01/论文笔记-batch-layer-weights-normalization/">http://www.panxiaoxie.cn/2018/10/01/论文笔记-batch-layer-weights-normalization/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Xie Pan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-10-01</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/DL/">DL</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CNN%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">论文笔记-CNN与自然语言处理</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-character-embedding-and-ELMO/"><span class="level-item">论文笔记-character embedding and ELMO</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.panxiaoxie.cn/2018/10/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-batch-layer-weights-normalization/';
            this.page.identifier = '2018/10/01/论文笔记-batch-layer-weights-normalization/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'panxie' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="panxiaoxie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">panxiaoxie</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">120</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/PanXiebit" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/PanXiebit"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/ftdpanxie@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/PanXiebit" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="ftdpanxie@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">email</span></span><span class="level-right"><span class="level-item tag">ftdpanxie@gmail.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CSAPP/"><span class="level-start"><span class="level-item">CSAPP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/DRL/"><span class="level-start"><span class="level-item">DRL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN-RL/"><span class="level-start"><span class="level-item">GAN, RL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cs224d/"><span class="level-start"><span class="level-item">cs224d</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/generation/"><span class="level-start"><span class="level-item">generation</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/generative-models/"><span class="level-start"><span class="level-item">generative models</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch/"><span class="level-start"><span class="level-item">pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/reinforcement-learning/"><span class="level-start"><span class="level-item">reinforcement learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/sign-language-recognition/"><span class="level-start"><span class="level-item">sign language recognition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/transfer-learning/"><span class="level-start"><span class="level-item">transfer learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/transformer/"><span class="level-start"><span class="level-item">transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构与算法</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">文本分类</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文笔记</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DL/"><span class="level-start"><span class="level-item">DL</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ESA/"><span class="level-start"><span class="level-item">ESA</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/MRC-and-QA/"><span class="level-start"><span class="level-item">MRC and QA</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Machine-Translation/"><span class="level-start"><span class="level-item">Machine Translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/capsules/"><span class="level-start"><span class="level-item">capsules</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/computer-vision/"><span class="level-start"><span class="level-item">computer vision</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/constrast-learning/"><span class="level-start"><span class="level-item">constrast learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/data-augmentation/"><span class="level-start"><span class="level-item">data augmentation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dialogue-system/"><span class="level-start"><span class="level-item">dialogue system</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/language-model/"><span class="level-start"><span class="level-item">language model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/machine-translation/"><span class="level-start"><span class="level-item">machine translation</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/open-set-recognition/"><span class="level-start"><span class="level-item">open set recognition</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/sentence-embedding/"><span class="level-start"><span class="level-item">sentence embedding</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/text-matching/"><span class="level-start"><span class="level-item">text matching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/vision-language/"><span class="level-start"><span class="level-item">vision-language</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/panxiaoxie.png" alt="潘小榭" height="28"></a><p class="is-size-7"><span>&copy; 2022 Xie Pan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>